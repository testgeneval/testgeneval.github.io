{"raw_generations": {"sample_200": ["def test_send_messages_one_sent(self):\n    \"\"\"A message is sent if it has at least one recipient.\"\"\"\n    backend = smtp.EmailBackend()\n    backend.connection = True\n    email = EmailMessage('Subject', 'Content', 'from@example.com', to=['to@example.com'])\n    sent = backend.send_messages([email])\n    self.assertEqual(sent, 1)\n", "def test_send_messages_non_ascii_addresses(self):\n    backend = smtp.EmailBackend()\n    backend.connection = True\n    email = EmailMessage('Subject', 'Content', 'from@example.com', ['t\u00f6st@example.com'])\n    sent = backend.send_messages([email])\n    self.assertEqual(sent, 1)\n", "def test_send_messages_invalid_recipient(self):\n    \"\"\"A message isn't sent if it has an invalid recipient.\"\"\"\n    backend = smtp.EmailBackend()\n    backend.connection = True\n    email = EmailMessage('Subject', 'Content', 'from@example.com', to=['invalid'])\n    with self.assertRaisesMessage(SMTPException, 'Invalid recipient'):\n        backend.send_messages([email])\n", "def test_send_messages_invalid_recipients(self):\n    \"\"\"send_messages() should raise an exception for invalid recipients.\"\"\"\n    backend = smtp.EmailBackend()\n    backend.connection = True\n    email = EmailMessage('Subject', 'Content', 'from@example.com', to=['invalid_email'])\n    with self.assertRaises(SMTPRecipientsRefused):\n        backend.send_messages([email])\n", "    def setUp(self):\n        super().setUp()\n        self.server.flush_sink()\n"], "sample_1007": ["def test_binomial_is_zero():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, -k).is_zero is True\n    assert binomial(n, k + n + 1).is_zero is True\n    assert binomial(n, k).is_zero is False\n    assert binomial(n, k - n - 1).is_zero is False\n", "def test_factorial_mod():\n    n = Symbol('n', integer=True, nonnegative=True)\n    q = Symbol('q', integer=True, positive=True)\n\n    assert factorial(n)._eval_Mod(q) == 0\n    assert factorial(n - 1)._eval_Mod(q) == 0\n    assert factorial(n - 2)._eval_Mod(q) is None\n    assert factorial(n - 3)._eval_Mod(q) == -1 % q\n    assert factorial(n - 6)._eval_Mod(q) == 0\n\n    q_prime = Symbol('q_prime', integer=True, positive=True, prime=True)\n\n    assert factorial(q_prime - 1)._eval_Mod(q_prime) == -1 % q_prime\n    assert factorial(q_prime - 5)._eval_Mod(q_prime) == 0\n", "def test_factorial_Mod():\n    n = Symbol('n', integer=True, nonnegative=True)\n    q = Symbol('q', integer=True)\n\n    assert factorial(n) % q == 0\n    assert factorial(n + q) % q == 0\n    assert factorial(n + q - 1) % q == 0\n    assert factorial(n + q - 2) % q != 0\n    assert factorial(n + q - 3) % q != 0\n\n    aq = Symbol('aq', integer=True, positive=True)\n    assert factorial(n) % aq == 0\n    assert factorial(n + aq) % aq == 0\n    assert factorial(n + aq - 1) % aq == 0\n    assert factorial(n + aq - 2) % aq != 0\n    assert factorial(n + aq - 3) % aq != 0\n\n    ap = Symbol('ap', integer=True, positive=True, prime=True)\n    assert factorial(n) % ap != 0\n    assert factorial(n + ap) % ap == 0\n    assert factorial(n + ap - 1) % ap == 0\n    assert factorial(n + ap - 2) % ap != 0\n    assert factorial(n + ap - 3) % ap != 0\n\n    ac = Symbol('ac', integer=True, positive=True, composite=True)\n    assert factorial(n) % ac != 0\n    assert factorial(n + ac) % ac == 0\n    assert factorial(n + ac - 1) % ac == 0\n    assert factorial(n + ac - 2) % ac != 0\n    assert factorial(n + ac - 3) % ac != 0\n    assert factorial(n + ac - 6) % ac == 0\n", "def test_factorial_Mod():\n    x = Symbol('x', integer=True, nonnegative=True)\n    q = Symbol('q', integer=True)\n\n    assert factorial(x)._eval_Mod(q) is None\n    assert factorial(x)._eval_Mod(0) is None\n    assert factorial(x)._eval_Mod(1) == 0\n    assert factorial(x)._eval_Mod(2) == 0\n    assert factorial(x)._eval_Mod(q).is_integer\n\n    assert factorial(x - 1)._eval_Mod(q) is None\n    assert factorial(x - 1)._eval_Mod(0) is None\n    assert factorial(x - 1)._eval_Mod(1) == 0\n    assert factorial(x - 1)._eval_Mod(2) == 0\n    assert factorial(x - 1)._eval_Mod(q) == 0\n\n    assert factorial(x - 2)._eval_Mod(q) == 0\n    assert factorial(x - 3)._eval_Mod(q) == 0\n\n    assert factorial(x + 2)._eval_Mod(q) is None\n    assert factorial(x + 3)._eval_Mod(q) is None\n\n    assert factorial(4)._eval_Mod(3) == 0\n    assert factorial(5)._eval_Mod(3) == 0\n    assert factorial(6)._eval_Mod(3) == 0\n    assert factorial(7)._eval_Mod(3) == 1\n    assert factorial(8)._eval_Mod(3) == 2\n    assert factorial(9)._eval_Mod(3) == 0\n\n    assert factorial(11)._eval_Mod(5) == 0\n    assert factorial(12)._eval_Mod(5) == 0\n    assert factorial(13)._eval_Mod(5) == 3\n    assert factorial(14)._eval_Mod(5) == 1\n    assert factorial(15).", "def test_factorial_Mod():\n    x = Symbol('x', integer=True, positive=True)\n    y = Symbol('y', integer=True, negative=True)\n    z = Symbol('z', integer=True, zero=True)\n    p = Symbol('p', integer=True, prime=True)\n    cp = Symbol('cp', integer=True, composite=True)\n\n    assert factorial(x)._eval_Mod(p) == 0\n    assert factorial(x)._eval_Mod(cp) == 0\n    assert factorial(y)._eval_Mod(p) == 0\n    assert factorial(y)._eval_Mod(cp) == 0\n    assert factorial(z)._eval_Mod(p) == 0\n    assert factorial(z)._eval_Mod(cp) == 0\n\n    assert factorial(x + p)._eval_Mod(p) == 0\n    assert factorial(x + p)._eval_Mod(cp) == 0\n    assert factorial(x + cp)._eval_Mod(cp) == 0\n\n    assert factorial(p)._eval_Mod(p) == 0\n    assert factorial(p - 1)._eval_Mod(p) == -1 % p\n    assert factorial(p - 2)._eval_Mod(p) == 0\n\n    assert factorial(cp)._eval_Mod(cp) == 0\n    assert factorial(cp - 1)._eval_Mod(cp) == 0\n    assert factorial(cp - 2)._eval_Mod(cp) == 0\n    assert factorial(cp - 3)._eval_Mod(cp) == 0\n    assert factorial(cp - 4)._eval_Mod(cp) == 0\n    assert factorial(cp - 5)._eval_Mod(cp) == 0\n    assert factorial(cp - 6)._eval_Mod(cp) == 0\n    assert factorial(cp - 7)._eval_Mod(cp) == 0\n"], "sample_55": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.obj = GetFormsetsArgumentChecking.objects.create(name='Test')\n", "def test_check_inline_view_on_site(self):\n    \"The value of 'view_on_site' for inlines must be a callable or a boolean value\"\n    try:\n        class InlineWithViewOnSite(admin.StackedInline):\n            model = City\n            view_on_site = True\n\n        class InlineWithViewOnSiteFalse(admin.StackedInline):\n            model = City\n            view_on_site = False\n\n        class InlineWithViewOnSiteCallable(admin.StackedInline):\n            model = City\n            view_on_site = lambda obj: obj.get_absolute_url()\n\n        class InlineWithViewOnSiteInvalid(admin.StackedInline):\n            model = City\n            view_on_site = []\n\n        class TestAdmin(admin.ModelAdmin):\n            model = State\n            inlines = [InlineWithViewOnSite, InlineWithViewOnSiteFalse, InlineWithViewOnSiteCallable, InlineWithViewOnSiteInvalid]\n\n        test_admin = TestAdmin(State, AdminSite())\n        self.assertEqual(test_admin.check(), [\n            Error(\n                \"The value of 'view_on_site' must be a callable or a boolean value.\",\n                obj=InlineWithViewOnSiteInvalid,\n                id='admin.E025',\n            ),\n        ])\n    finally:\n        # Restore the original values for the benefit of other tests.\n        InlineWithViewOnSite.view_on_site = True\n        InlineWithViewOnSiteFalse.view_on_site = False\n        InlineWithViewOnSiteCallable.view_on_site = lambda obj: obj.get_absolute_url()\n        InlineWithViewOnSiteInvalid.view_on_site = []\n", "def test_custom_save_related(self):\n    \"\"\"\n    #23934 - Custom save_related should receive the correct obj argument\n    \"\"\"\n    parent = ParentWithCustomSaveRelated.objects.create(name='Parent')\n    post_data = {\n        'childwithcustomsaverelated_set-TOTAL_FORMS': '1',\n        'childwithcustomsaverelated_set-INITIAL_FORMS': '0',\n        'childwithcustomsaverelated_set-MAX_NUM_FORMS': '0',\n        'childwithcustomsaverelated_set-0-name': 'Child',\n        'childwithcustomsaverelated_set-0-id': '',\n        'childwithcustomsaverelated_set-0-parent': str(parent.id),\n    }\n    response = self.client.post(reverse('admin:admin_views_parentwithcustomsaverelated_change', args=(parent.id,)), post_data)\n    self.assertEqual(response.status_code, 302)\n    child = ChildWithCustomSaveRelated.objects.get(parent=parent)\n    self.assertEqual(child.name, 'Child')\n    self.assertEqual(child.custom_save_related_called, True)\n", "def test_custom_save_related(self):\n    \"\"\"\n    Test the custom save_related method in the admin.\n    \"\"\"\n    parent = Parent.objects.create(name='Test Parent')\n    child_data = {\n        'child_set-TOTAL_FORMS': '1',\n        'child_set-INITIAL_FORMS': '0',\n        'child_set-MAX_NUM_FORMS': '',\n        'child_set-0-name': 'Test Child',\n        'child_set-0-id': '',\n    }\n    response = self.client.post(reverse('admin:admin_views_parent_change', args=(parent.pk,)), child_data)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(Child.objects.filter(parent=parent, name='Test Child').count(), 1)\n", "def test_queryset_argument(self):\n    # Create a new instance of the GetFormsetsArgumentCheckingAdmin class\n    admin = GetFormsetsArgumentCheckingAdmin(ExplicitlyProvidedPK, AdminSite())\n\n    # Test the get_queryset() method with and without the queryset argument\n    self.assertEqual(list(admin.get_queryset(ExplicitlyProvidedPK.objects.all())), [])\n    self.assertEqual(list(admin.get_queryset()), [])\n\n    # Create a new instance of the ExplicitlyProvidedPK model\n    obj = ExplicitlyProvidedPK.objects.create(name='test')\n\n    # Test the get_queryset() method with the queryset argument and an object\n    self.assertEqual(list(admin.get_queryset(ExplicitlyProvidedPK.objects.filter(pk=obj.pk))), [obj])\n\n    # Test the get_queryset() method with an object that doesn't exist\n    self.assertEqual(list(admin.get_queryset(ExplicitlyProvidedPK.objects.filter(pk=999))), [])\n"], "sample_744": ["def test_power_transformer_yeo_johnson():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_sparse_input():\n    X = sparse.csr_matrix(np.abs(X_2d))\n    pt = PowerTransformer(method='box-cox', standardize=True)\n\n    X_trans = pt.fit_transform(X)\n    X_trans_func = power_transform(X, standardize=True)\n\n    for X_trans_ in [X_trans, X_trans_func]:\n        X_trans_ = X_trans_.toarray()\n        for j in range(X_trans_.shape[1]):\n            X_expected, lmbda = stats.boxcox(X[:, j].toarray().flatten())\n            X_expected = scale(X_expected)\n\n            assert_almost_equal(X_trans_[:, j], X_expected)\n            assert_almost_equal(lmbda, pt.lambdas_[j])\n\n        # Test inverse transformation\n        X_inv = pt.inverse_transform(X_trans)\n        assert_array_almost_equal(X_inv.toarray(), X.toarray())\n\n    assert len(pt.lambdas_) == X.shape[1]\n    assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_partial_fit():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt_batch = PowerTransformer(method='box-cox', standardize=standardize)\n        pt_incr = PowerTransformer(method='box-cox', standardize=standardize)\n\n        pt_batch.fit(X)\n        X_trans_batch = pt_batch.transform(X)\n\n        for batch in gen_batches(X.shape[0], chunk_size=1):\n            pt_incr.partial_fit(X[batch])\n\n        X_trans_incr = pt_incr.transform(X)\n\n        assert_array_almost_equal(X_trans_batch, X_trans_incr)\n", "def test_power_transformer_yellow_brick_method():\n    pt = PowerTransformer(method='yeo-johnson')\n    X = np.abs(X_2d)\n\n    X_trans = pt.fit_transform(X)\n    X_trans_func = power_transform(X, method='yeo-johnson')\n\n    assert_array_almost_equal(X_trans, X_trans_func)\n\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n\n    assert len(pt.lambdas_) == X.shape[1]\n    assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_standardize_exception():\n    pt = PowerTransformer(method='box-cox', standardize=True)\n    X = np.abs(X_2d)[:, 0:1]\n\n    # Test the case where standardize=True, but the data is constant\n    pt.fit(X)\n    assert_raise_message(ValueError, 'standardize=True but the transformed data is constant',\n                         pt.transform, X)\n"], "sample_908": ["def test_unparse_annotation():\n    source = \"def func(x: int, y: str = 'hello') -> bool:\"\n    expected = \"def func(x: int, y: str = 'hello') -> bool: ...\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_argument_annotations():\n    source = \"def f(x: int, y: 'str' = 'default'): pass\"\n    expected = \"x: int, y: str = 'default'\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_annotated_arguments(source, expected):\n    source = f\"def func({source}): ...\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_additional(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    func_def = module.body[0]\n    assert ast.unparse_arguments(func_def.args) == expected\n"], "sample_1060": ["def test_NumPyPrinter_print_MatMul():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 3)\n    B = MatrixSymbol(\"B\", 3, 2)\n    assert n.doprint(A*B) == '(A).dot(B)'\n", "def test_NumPyPrinter_print_Min():\n    p = NumPyPrinter()\n    assert p.doprint(Min(x, y)) == 'numpy.amin((x, y))'\n", "def test_NumPyPrinter_print_DotProduct():\n    n = NumPyPrinter()\n\n    a = MatrixSymbol('a', 1, 2)\n    b = MatrixSymbol('b', 2, 1)\n    dp = DotProduct(a, b)\n    assert n.doprint(dp) == \"numpy.dot(a, b)\"\n\n    a = MatrixSymbol('a', 2, 1)\n    b = MatrixSymbol('b', 1, 2)\n    dp = DotProduct(a, b)\n    assert n.doprint(dp) == \"numpy.dot(a.T, b.T)\"\n", "def test_NumPyPrinter_print_Min_Max():\n    p = NumPyPrinter()\n    assert p.doprint(Min(x, y, z)) == 'numpy.amin((x, y, z))'\n    assert p.doprint(Max(x, y, z)) == 'numpy.amax((x, y, z))'\n", "def test_NumPyPrinter_print_Pow():\n    p = NumPyPrinter()\n    assert p.doprint(x**0.5) == 'numpy.sqrt(x)'\n    assert p.doprint(x**2) == 'x**2'\n"], "sample_693": ["def test_setup_class_teardown_class_order(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n            @classmethod\n                cls.values.append(\"setUpClass\")\n            @classmethod\n                cls.values.append(\"tearDownClass\")\n                self.values.append(\"setUp\")\n                self.values.append(\"tearDown\")\n                pass\n            assert MyTestCase.values == [\"setUpClass\", \"setUp\", \"tearDown\", \"tearDownClass\"]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 0\n    assert passed == 2\n", "def test_unittest_expected_failure_for_passing_test_is_fail_without_message(\n    pytester: Pytester,\n    runner: str,", "def test_unittest_skip_class(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        @unittest.skip(\"skipping class\")\n        class MyTestCase(unittest.TestCase):\n                pass\n                pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    reprec.assertoutcome(skipped=2)\n", "def test_do_cleanups_on_both_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n                assert False\n                pass\n            assert MyTestCase.values == [1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "def test_unittest_skip_conversion(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        test_foo=\"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                raise unittest.SkipTest('skipping due to reasons')\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"-rs\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *SKIP*[1]*skipping due to reasons*\n        *1 skipped*\n    \"\"\"\n    )\n    # Check if the exception was converted to a pytest.skip exception\n    call = result.getcalls('pytest_runtest_makereport')[0]\n    assert isinstance(call.excinfo.value, pytest.skip.Exception)\n"], "sample_488": ["    def test_get_registered_model(self):\n        \"\"\"\n        Tests apps.get_registered_model().\n        \"\"\"\n        self.assertEqual(apps.get_registered_model(\"admin\", \"LogEntry\"), LogEntry)\n        with self.assertRaises(LookupError):\n            apps.get_registered_model(\"admin\", \"LogExit\")\n\n        # App label is case-sensitive, Model name is case-insensitive.\n        self.assertEqual(apps.get_registered_model(\"admin\", \"loGentrY\"), LogEntry)\n        with self.assertRaises(LookupError):\n            apps.get_registered_model(\"Admin\", \"LogEntry\")\n\n        # get_registered_model() should work even if apps.models_ready isn't True.\n        apps.models_ready = False\n        try:\n            self.assertEqual(apps.get_registered_model(\"admin\", \"LogEntry\"), LogEntry)\n        finally:\n            apps.models_ready = True\n", "def test_get_registered_model(self):\n    \"\"\"\n    Tests apps.get_registered_model().\n    \"\"\"\n    from django.contrib.auth.models import User\n\n    self.assertEqual(apps.get_registered_model(\"auth\", \"User\"), User)\n    with self.assertRaises(LookupError):\n        apps.get_registered_model(\"auth\", \"NonExistentModel\")\n", "    def test_set_available_apps(self):\n        \"\"\"\n        Tests apps.set_available_apps().\n        \"\"\"\n        apps.set_available_apps([\"django.contrib.admin\"])\n        self.assertEqual(\n            [app_config.name for app_config in apps.get_app_configs()],\n            [\"django.contrib.admin\"],\n        )\n        apps.unset_available_apps()\n        self.assertEqual(\n            [app_config.name for app_config in apps.get_app_configs()],\n            SOME_INSTALLED_APPS_NAMES,\n        )\n", "def test_get_swappable_settings_name(self):\n    \"\"\"\n    Tests apps.get_swappable_settings_name().\n    \"\"\"\n    self.assertIsNone(apps.get_swappable_settings_name(\"admin.LogEntry\"))\n    self.assertEqual(apps.get_swappable_settings_name(\"sites.Site\"), \"SITE_ID\")\n", "    def test_set_available_apps(self):\n        \"\"\"\n        Tests apps.set_available_apps().\n        \"\"\"\n        all_apps = [app_config.name for app_config in apps.get_app_configs()]\n        available_apps = all_apps[:-1]  # Exclude the last app\n        apps.set_available_apps(available_apps)\n        self.assertEqual(\n            [app_config.name for app_config in apps.get_app_configs()], available_apps\n        )\n        apps.unset_available_apps()\n        self.assertEqual(\n            [app_config.name for app_config in apps.get_app_configs()], all_apps\n        )\n\n        with self.assertRaisesMessage(\n            ValueError, \"Available apps isn't a subset of installed apps\"\n        ):\n            apps.set_available_apps([\"nonexistent_app\"])\n"], "sample_572": ["def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1)\n", "def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == 1\n", "def test_bivariate_probability_stat_with_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == 1\n    assert (heights * weights[:, None] * weights[None, :]).sum() == 1\n", "def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1)\n\n    expected_heights, _ = h(x, y)\n    assert np.allclose(heights, expected_heights * weights[:, None] / weights.sum())\n", "def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1)\n"], "sample_416": ["def test_no_dbname_or_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"USER\": \"someuser\"}),\n        ([\"psql\", \"-U\", \"someuser\", \"postgres\"], None),\n    )\n", "def test_no_dbname_or_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"USER\": \"someuser\"}),\n        ([\"psql\", \"-U\", \"someuser\", \"postgres\"], None),\n    )\n", "def test_no_database_name(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"PASSWORD\": \"somepassword\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n", "def test_no_name_or_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"USER\": \"someuser\", \"HOST\": \"somehost\", \"PORT\": \"444\"}),\n        ([\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"], None),\n    )\n", "def test_no_dbname_with_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n                \"OPTIONS\": {\n                    \"service\": \"django_test\",\n                },\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\"],\n            {\"PGSERVICE\": \"django_test\"},\n        ),\n    )\n"], "sample_1114": ["def test_issue_17860():\n    r = Range(1, 10)\n    assert r[::2] == Range(1, 10, 2)\n    assert r[::-2] == Range(9, 1, -2)\n    r = Range(10, 1, -1)\n    assert r[::2] == Range(10, 1, -2)\n    assert r[::-2] == Range(1, 10, 2)\n", "def test_complex_region_expr():\n    r, theta = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True).variables\n    assert ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True).expr == r*(cos(theta) + S.ImaginaryUnit*sin(theta))\n    x, y = CartesianComplexRegion(Interval(1, 3) * Interval(4, 6)).variables\n    assert CartesianComplexRegion(Interval(1, 3) * Interval(4, 6)).expr == x + S.ImaginaryUnit*y\n", "def test_range_interval_intersection_with_symbols():\n    p = Symbol('p', positive=True)\n    assert isinstance(Range(3).intersect(Interval(p, p + 2)), Intersection)\n    assert Range(4).intersect(Interval(0, 3)) == Range(4)\n    assert Range(4).intersect(Interval(-oo, oo)) == Range(4)\n    assert Range(4).intersect(Interval(1, oo)) == Range(1, 4)\n    assert Range(4).intersect(Interval(1.1, oo)) == Range(2, 4)\n    assert Range(4).intersect(Interval(0.1, 3)) == Range(1, 4)\n    assert Range(4).intersect(Interval(0.1, 3.1)) == Range(1, 4)\n    assert Range(4).intersect(Interval.open(0, 3)) == Range(1, 3)\n    assert Range(4).intersect(Interval.open(0.1, 0.5)) is S.EmptySet\n\n    # Null Range intersections\n    assert Range(0).intersect(Interval(0.2, 0.8)) is S.EmptySet\n    assert Range(0).intersect(Interval(-oo, oo)) is S.EmptySet\n\n    # Symbolic Range intersections\n    x = Symbol('x')\n    assert Range(x).intersect(Interval(0, 3)) == Range(Max(0, x), Min(3, x))\n    assert Range(x).intersect(Interval(1, oo)) == Range(Max(1, x), oo)\n    assert Range(x).intersect(Interval(-oo, 3)) == Range(-oo, Min(3, x))\n    assert Range(x).intersect(Interval(1.1, oo)) == Range(Max(1.1, x), oo)\n    assert Range(x).intersect(Interval(-oo, 3.1)) == Range(-oo, Min(3.1, x))\n    assert Range(x).intersect(Interval.open(0, 3)) == Range(Max(", "def test_issue_18270():\n    assert imageset(x, sqrt(x), S.Naturals).doit() == Range(0, oo)\n    assert imageset(x, sqrt(x), S.Naturals0).doit() == Range(0, oo)\n    assert imageset(x, sqrt(x), S.Integers).doit() == Interval(-oo, oo)\n", "def test_complex_region_cartesian():\n    # Test cases for CartesianComplexRegion\n    c1 = ComplexRegion(Interval(1, 3) * Interval(4, 6))\n    assert 2 + 5*I in c1\n    assert 5*I in c1\n    assert 2 - 5*I not in c1\n\n    c2 = ComplexRegion(FiniteSet(1, 2) * FiniteSet(3, 4))\n    assert c2 == FiniteSet(1 + 3*I, 1 + 4*I, 2 + 3*I, 2 + 4*I)\n\n    # Test cases for PolarComplexRegion\n    c3 = ComplexRegion(Interval(0, 1) * Interval(0, 2*S.Pi), polar=True)\n    assert (0.5 + 0.5*I) in c3\n    assert (1 + I) not in c3\n\n    c4 = ComplexRegion(FiniteSet(0, 1) * FiniteSet(0, S.Pi), polar=True)\n    assert c4 == FiniteSet(0, 1)\n"], "sample_5": ["def test_models_evaluate_with_different_units(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n    m = model['class'](**model['parameters'])\n    for args in model['evaluation']:\n        if len(args) == 2:\n            x, y = args\n            x_diff_unit = x.to(x.unit * 2)\n            y_diff_unit = y.to(y.unit * 2)\n            args_diff_unit = (x_diff_unit, y_diff_unit)\n        else:\n            x, y, z = args\n            x_diff_unit = x.to(x.unit * 2)\n            y_diff_unit = y.to(y.unit * 2)\n            z_diff_unit = z.to(z.unit * 2)\n            args_diff_unit = (x_diff_unit, y_diff_unit, z_diff_unit)\n        assert_quantity_allclose(m(*args_diff_unit), m(*args))\n", "def test_models_evaluate_with_units_multiple_args():\n    m = Gaussian2D(amplitude=3 * u.Jy, x_mean=2 * u.m, y_mean=1 * u.m, x_stddev=3 * u.m, y_stddev=2 * u.m, theta=45 * u.deg)\n    x = np.array([412.1320343]) * u.cm\n    y = np.array([3.121320343]) * u.m\n    expected = np.array([3 * u.Jy * np.exp(-0.5)])\n    assert_quantity_allclose(m(x, y), expected)\n", "def test_models_evaluate_with_units_param_array_out_of_bounds():\n    model = {\n        'class': Gaussian1D,\n        'parameters': {'amplitude': 3 * u.Jy, 'mean': 2 * u.m, 'stddev': 30 * u.cm},\n        'evaluation': [(2600 * u.mm, 3 * u.Jy * np.exp(-2))],\n    }\n    params = {key: np.repeat(value, 2) for key, value in model['parameters'].items()}\n    params['n_models'] = 2\n    params['mean'][-1] = 10 * u.m  # This is out of bounds for a reasonable input\n    m = model['class'](**params)\n    x = np.linspace(1, 3, 100) * u.m\n    with pytest.raises(InputParameterError):\n        m(x)\n", "def test_models_evaluate_with_quantity_array(model):\n\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n\n    for args in model['evaluation']:\n        if len(args) == 2:\n            x, y = args\n            x_arr = u.Quantity([x, x], subok=True)\n            result = m(x_arr)\n            assert_quantity_allclose(result, u.Quantity([y, y], subok=True))\n        else:\n            x, y, z = args\n            x_arr = u.Quantity([x, x])\n            y_arr = u.Quantity([y, y])\n            z_arr = u.Quantity([z, z])\n            result = m(x_arr, y_arr)\n            assert_quantity_allclose(result, z_arr)\n", "def test_models_evaluate_with_arrays():\n    m = Gaussian1D(amplitude=3 * u.Jy, mean=2 * u.m, stddev=30 * u.cm)\n    x = np.array([2600, 2700]) * u.mm\n    y = np.array([3 * np.exp(-2), 3 * np.exp(-2.1)]) * u.Jy\n    assert_quantity_allclose(m(x), y)\n\n    m = Sersic1D(amplitude=3 * u.MJy / u.sr, r_eff=2 * u.arcsec, n=4)\n    x = np.array([3, 4]) * u.arcsec\n    y = np.array([1.3237148119468918, 0.29496130151863255]) * u.MJy / u.sr\n    assert_quantity_allclose(m(x), y)\n"], "sample_1029": ["def test_Cycle():\n    from sympy import Cycle\n    sT(Cycle(1, 2, 3), \"Cycle((1, 2, 3))\")\n", "def test_Cycle():\n    from sympy import Cycle\n    sT(Cycle(1, 2, 3), \"Cycle((1, 2, 3))\")\n", "def test_Cycle():\n    from sympy import Cycle\n    c = Cycle((1, 2, 3))\n    assert srepr(c) == \"Cycle((1, 2, 3))\"\n", "def test_Cycle():\n    from sympy import Cycle\n    sT(Cycle(0, 1, 2), \"Cycle((0, 1, 2))\")\n", "def test_Pow():\n    sT(x**Rational(1, 2), \"Pow(Symbol('x'), Rational(1, 2))\")\n    sT(x**-1, \"Pow(Symbol('x'), -1)\")\n"], "sample_738": ["def test_tfidfvectorizer_sublinear_tf():\n    # Non-regression test: TfidfVectorizer used to ignore its \"sublinear_tf\" param.\n    v = TfidfVectorizer(sublinear_tf=True, use_idf=False, norm=None)\n    assert_true(v.sublinear_tf)\n\n    X = v.fit_transform(['hello world', 'hello hello']).toarray()\n    expected = np.log(np.array([[1, 1], [2, 1]]) + 1)\n    assert_array_almost_equal(X, expected)\n    X2 = v.transform(['hello world', 'hello hello']).toarray()\n    assert_array_almost_equal(X2, expected)\n", "def test_vectorizer_with_stop_words_list():\n    # Test the vectorizer with a custom stop words list\n    stop_words = ['beer', 'the']\n    vectorizer = CountVectorizer(stop_words=stop_words)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert 'beer' not in vectorizer.get_feature_names()\n    assert 'the' not in vectorizer.get_feature_names()\n", "def test_vectorizer_custom_analyzer():\n    custom_analyzer = lambda x: x.split('-')\n    vec = CountVectorizer(analyzer=custom_analyzer)\n    X = vec.fit_transform([\"hello-world\", \"hello-hello\"])\n    assert_array_equal(X.toarray(), [[1, 1, 0], [2, 0, 1]])\n    assert_equal(vec.get_feature_names(), ['hello', 'world'])\n", "def test_vectorizer_get_stop_words():\n    # Test that get_stop_words method returns correct stop words\n    stop_words = ['the', 'and', 'is']\n    vectorizer = CountVectorizer(stop_words=stop_words)\n    assert vectorizer.get_stop_words() == frozenset(stop_words)\n", "def test_countvectorizer_custom_tokenizer():\n        return s.split('_')\n\n    cv = CountVectorizer(tokenizer=custom_tokenizer)\n    X = cv.fit_transform([\"this_is_a_test\", \"another_test\"])\n    expected_vocab = {\"this\": 0, \"is\": 1, \"a\": 2, \"test\": 3, \"another\": 4}\n    assert_equal(cv.vocabulary_, expected_vocab)\n    expected_X = np.array([[1, 1, 1, 1, 0], [0, 0, 0, 1, 1]])\n    assert_array_equal(X.toarray(), expected_X)\n"], "sample_272": ["def test_minimize_rollbacks_leaf_node(self):\n    r\"\"\"\n    Minimize rollbacks when target is a leaf node.\n\n    a: 1 <---- 2 <--- 3\n    b:    \\- 1 <--- 2 <--- 3\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    b2_impl = FakeMigration('b2')\n    b2 = ('b', '2')\n    b3_impl = FakeMigration('b3')\n    b3 = ('b', '3')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(b2, b2_impl)\n    graph.add_node(b3, b3_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a2)\n    graph.add_dependency(None, b2, b1)\n    graph.add_dependency(None, b3, b2)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        b1: b1_impl,\n        a2: a2_impl,\n        b2: b2_impl,\n        a3: a3_impl,\n    })\n\n    plan = executor.migration_plan({a3})\n\n    should_be_rolled_back = [b3_impl, a3_impl]\n", "def test_minimize_rollbacks_with_replacements(self):\n    \"\"\"\n    Minimize rollbacks when target is a replacement migration.\n\n    a: 1 <---- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n    b:      \\- 1 <--- 2\n\n    a2 and a3 are replaced by a123.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    a4_impl = FakeMigration('a4')\n    a4 = ('a', '4')\n    a123_impl = FakeMigration('a123')\n    a123 = ('a', '123')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    b2_impl = FakeMigration('b2')\n    b2 = ('b', '2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(a4, a4_impl)\n    graph.add_node(a123, a123_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(b2, b2_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a1)\n    graph.add_dependency(None, a4, a2)\n    graph.add_dependency(None, a4, a3)\n    graph.add_dependency(None, b2, b1)\n    graph.add_dependency(None, b1, a1)\n    graph", "def test_minimize_rollbacks_with_multiple_targets(self):\n    r\"\"\"\n    Minimize rollbacks when there are multiple targets.\n\n    a: 1 <---- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n    b:     1 <-- 2 <---- 3\n\n    If we're asked to migrate to a2 and b3, rollback a4 and b2, but not a3 or a1.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    a4_impl = FakeMigration('a4')\n    a4 = ('a', '4')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    b2_impl = FakeMigration('b2')\n    b2 = ('b', '2')\n    b3_impl = FakeMigration('b3')\n    b3 = ('b', '3')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(a4, a4_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(b2, b2_impl)\n    graph.add_node(b3, b3_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a1)\n    graph.add_dependency(None, a4, a2)\n    graph.add_dependency(None, a4, a3)\n    graph.add_dependency(None, b2, b1)\n    graph.add_dependency(None, b", "def test_minimize_rollbacks_unrelated_app(self):\n    r\"\"\"\n    Minimize rollbacks in unrelated apps.\n\n    a: 1 <- 2 <- 3\n    b: 1\n    c:     1 <- 2\n\n    If we're asked to migrate to a2, don't apply or unapply b1, even though\n    it's a leaf node in the graph.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    c1_impl = FakeMigration('c1')\n    c1 = ('c', '1')\n    c2_impl = FakeMigration('c2')\n    c2 = ('c', '2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(c1, c1_impl)\n    graph.add_node(c2, c2_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a2)\n    graph.add_dependency(None, c2, c1)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        b1: b1_impl,\n        a2: a2_impl,\n        c1: c1_impl,\n    })\n\n    plan = executor.migration_plan({a2})\n\n    self.assertEqual(plan, [(a3", "    def test_atomic_operation_in_atomic_migration(self):\n        \"\"\"\n        An atomic operation is properly rolled back inside an atomic\n        migration.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        with self.assertRaisesMessage(RuntimeError, \"Abort migration\"):\n            executor.migrate([(\"migrations\", \"0001_atomic\")])\n        migrations_apps = executor.loader.project_state((\"migrations\", \"0001_atomic\")).apps\n        Editor = migrations_apps.get_model(\"migrations\", \"Editor\")\n        self.assertFalse(Editor.objects.exists())\n        # Record previous migration as successful.\n        executor.migrate([(\"migrations\", \"0001_atomic\")], fake=True)\n        # Rebuild the graph to reflect the new DB state.\n        executor.loader.build_graph()\n        # Migrating backwards is also atomic.\n        with self.assertRaisesMessage(RuntimeError, \"Abort migration\"):\n            executor.migrate([(\"migrations\", None)])\n        self.assertFalse(Editor.objects.exists())\n"], "sample_234": ["def test_get_with_ordering_unsupported_on_combined_qs(self):\n    qs = Number.objects.all()\n    msg = 'Cannot reorder a query once a slice has been taken.'\n    combinators = ['union']\n    if connection.features.supports_select_difference:\n        combinators.append('difference')\n    if connection.features.supports_select_intersection:\n        combinators.append('intersection')\n    for combinator in combinators:\n        with self.subTest(combinator=combinator):\n            with self.assertRaisesMessage(TypeError, msg):\n                getattr(qs, combinator)(qs).order_by('num')\n", "def test_count_with_filters_on_combined_qs(self):\n    qs = Number.objects.all()\n    msg = 'Calling QuerySet.count() with filters after %s() is not supported.'\n    combinators = ['union']\n    if connection.features.supports_select_difference:\n        combinators.append('difference')\n    if connection.features.supports_select_intersection:\n        combinators.append('intersection')\n    for combinator in combinators:\n        with self.subTest(combinator=combinator):\n            with self.assertRaisesMessage(NotSupportedError, msg % combinator):\n                getattr(qs, combinator)(qs).filter(num=2).count()\n", "def test_union_with_values_and_ordering(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=5),\n        ReservedName(name='rn0', order=6),\n        ReservedName(name='rn9', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=6).values('name')\n    qs2 = ReservedName.objects.filter(order__lte=5).values('name')\n    union_qs = qs1.union(qs2)\n    self.assertQuerysetEqual(\n        union_qs.order_by('name'),\n        ['rn0', 'rn1', 'rn2', 'rn9'],\n        operator.itemgetter('name'),\n    )\n    self.assertQuerysetEqual(\n        union_qs.order_by('-name'),\n        ['rn9', 'rn2', 'rn1', 'rn0'],\n        operator.itemgetter('name'),\n    )\n", "def test_difference_with_annotations(self):\n    qs1 = Number.objects.annotate(doubled_num=F('num') * 2)\n    qs2 = Number.objects.annotate(doubled_num=F('num') * 3)\n    self.assertNumbersEqual(qs1.difference(qs2).values('doubled_num'), [2, 4, 6, 8, 10, 12, 14, 16, 18, 20], ordered=False)\n", "def test_order_by_on_annotated_values(self):\n    qs1 = Number.objects.filter(num__lt=5).annotate(count=Value(1, IntegerField()))\n    qs2 = Number.objects.filter(num__gte=5).annotate(count=Value(2, IntegerField()))\n    self.assertNumbersEqual(qs1.union(qs2).order_by('-count'), [9, 8, 7, 6, 4, 3, 2, 1, 0])\n"], "sample_312": ["    def test_add_squash(self):\n        node = Node(['a', 'b'], 'AND')\n        node.add(Node(['c', 'd'], 'AND'), 'AND')\n        self.assertEqual(node, Node(['a', 'b', 'c', 'd'], 'AND'))\n", "def test_add_squash_node(self):\n    node = Node(['a', 'b'], 'AND')\n    node_to_add = Node(['c', 'd'], 'AND')\n    node.add(node_to_add, 'AND')\n    self.assertEqual(node, Node(['a', 'b', 'c', 'd'], 'AND'))\n", "def test_add_squash(self):\n    # Test squashing of nodes with the same connector\n    node = Node(['a', 'b'], 'OR')\n    node.add(Node(['c', 'd'], 'OR'), 'OR')\n    self.assertEqual(node, Node(['a', 'b', 'c', 'd'], 'OR'))\n", "    def test_add_squash(self):\n        node = Node(['a'], 'AND')\n        node.add(Node(['b'], 'AND'), 'AND')\n        self.assertEqual(node, Node(['a', 'b'], 'AND'))\n", "def test_add_sqash(self):\n    node = Node(['a', 'b'], 'AND')\n    other_node = Node(['b', 'c'], 'AND')\n    node.add(other_node, 'AND')\n    self.assertEqual(node, Node(['a', 'b', 'c'], 'AND'))\n"], "sample_584": ["def test_auto_combine_with_explicit_none_concat_dim(self):\n    objs = [Dataset({'x': [0]}), Dataset({'x': [1]})]\n    with pytest.warns(FutureWarning, match=\"supplied do not have global\"):\n        auto_combine(objs, concat_dim=None)\n", "def test_combine_by_coords_with_fill_value(self):\n    datasets = [Dataset({'a': ('x', [2, 3]), 'x': [1, 2]}),\n                Dataset({'a': ('x', [1, 2]), 'x': [0, 1]})]\n    fill_value = 99\n    expected = Dataset({'a': (('t', 'x'),\n                              [[fill_value, 2, 3], [1, 2, fill_value]])},\n                       {'x': [0, 1, 2]})\n    actual = combine_by_coords(datasets, fill_value=fill_value)\n    assert_identical(expected, actual)\n", "def test_auto_combine_with_different_data_vars(self):\n    objs = [Dataset({'foo': ('x', [0])}),\n            Dataset({'bar': ('x', [1])})]\n    with pytest.warns(FutureWarning, match=\"require both concatenation\"):\n        auto_combine(objs)\n", "def test_auto_combine_with_different_data_vars(self):\n    objs = [Dataset({'x': [0]}), Dataset({'y': [1]})]\n    with pytest.raises(ValueError, match=\"cannot be combined\"):\n        auto_combine(objs)\n", "def test_auto_combine_with_none_concat_dim(self):\n    objs = [Dataset({'x': [0]}), Dataset({'x': [1]})]\n    with pytest.warns(FutureWarning, match=\"`concat_dim=None`\"):\n        auto_combine(objs, concat_dim=None)\n"], "sample_1138": ["def test_TR111():\n    assert TR111(tan(x)**-2) == cot(x)**2\n    assert TR111(sin(x)**-2) == csc(x)**2\n    assert TR111(cos(x)**-2) == sec(x)**2\n", "def test_TR111():\n    assert TR111(tan(x)**-2) == cot(x)**2\n    assert TR111(sin(x)**-1) == csc(x)\n    assert TR111(cos(x)**-1) == sec(x)\n    assert TR111(x**-2) == x**-2\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n", "def test_TR18():\n    assert TR18(sin(x)**2 + cos(x)**2) == 1\n", "def test_TR18():\n    assert TR18(sin(x)*cos(x)) == sin(2*x)/2\n    assert TR18(cos(x)*sin(x)) == sin(2*x)/2\n    assert TR18(cos(x)**2*sin(x)) == sin(3*x)/4 - sin(x)/4\n    assert TR18(sin(x)**2*cos(x)) == sin(3*x)/4 + sin(x)/4\n    assert TR18(cos(x)*sin(x)**2) == sin(3*x)/4 + sin(x)/4\n"], "sample_329": ["def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex((1+2j))\", set())\n    )\n", "def test_serialize_custom_deconstructible(self):\n    class CustomDeconstructible:\n            self.value = value\n\n            return ('migrations.test_writer.CustomDeconstructible', [self.value], {})\n\n    obj = CustomDeconstructible(42)\n    self.assertSerializedEqual(obj)\n    self.assertSerializedResultEqual(\n        obj,\n        (\"migrations.test_writer.CustomDeconstructible(42)\", {'import migrations.test_writer'})\n    )\n", "def test_serialize_empty_dictionary(self):\n    self.assertSerializedEqual({})\n    self.assertSerializedResultEqual({}, (\"{}\", set()))\n", "def test_serialize_decimal_with_special_values(self):\n    self.assertSerializedEqual(decimal.Decimal('NaN'))\n    self.assertSerializedEqual(decimal.Decimal('Infinity'))\n    self.assertSerializedEqual(decimal.Decimal('-Infinity'))\n", "def test_serialize_datetime_aware_with_default_timezone(self):\n    tzinfo = datetime.timezone(datetime.timedelta(hours=1))\n    with self.settings(TIME_ZONE='UTC'):\n        dt = datetime.datetime(2012, 1, 1, 1, 1, tzinfo=tzinfo)\n        self.assertSerializedResultEqual(\n            dt,\n            (\n                \"datetime.datetime(2012, 1, 1, 0, 1, tzinfo=utc)\",\n                {'import datetime', 'from django.utils.timezone import utc'},\n            )\n        )\n"], "sample_1170": ["def test_AppliedBinaryRelation():\n    assert str(Q.eq(x, y)) == \"Q.eq(x, y)\"\n    assert str(Q.ne(x, y)) == \"Q.ne(x, y)\"\n", "def test_issue_21823_partition():\n    assert str(Partition([1, 1, 2, 2])) == 'Partition({1, 2})'\n", "def test_issue_21823():\n    assert str(Partition([1, 2])) == 'Partition({1, 2})'\n    assert str(Partition({1, 2})) == 'Partition({1, 2})'\n", "def test_issue_21950():\n    expr = Sum(Subs(Sum(1, (i, 1, j)), (j, k)), (k, 1, n))\n    assert str(expr) == \"Sum(Subs(Sum(1, (i, 1, j)), (j, k)), (k, 1, n))\"\n", "def test_issue_21656():\n    assert str(S('-0.1') + S('0.2')) == '0.1'\n    assert str(S('-0.1') + S('0.2') + S('0.0')) == '0.1'\n    assert str(S('-0.1') + S('0.2') + S('0.00')) == '0.1'\n    assert str(S('-0.1') + S('0.2') + S('0.000')) == '0.100'\n"], "sample_18": ["def test_binary_op_structured(self):\n    q2 = u.Quantity(np.array([(5.0, 6.0), (7.0, 8.0)], dtype=[(\"p\", \"f8\"), (\"v\", \"f8\")]), \"m, m/s\")\n    q = self.q + q2\n    assert_no_info(q)\n", "def test_binary_op_structured(self):\n    q2 = u.Quantity([(3.0, 6.0), (5.0, 8.0)], \"m, m/s\")\n    q = self.q + q2\n    assert_no_info(q)\n", "def test_binary_op_with_structured_quantity(self):\n    q = self.q + self.q\n    assert_no_info(q)\n", "def test_structured_binary_op(self):\n    q2 = u.Quantity(np.array([(5.0, 6.0), (7.0, 8.0)]), \"m, m/s\")\n    q = self.q + q2\n    assert_no_info(q)\n", "def test_binary_op_with_scalar(self):\n    q = self.q + 1 * u.m\n    assert_info_equal(q, self.q)\n"], "sample_184": ["def test_unique_constraint_include_pointing_to_joined_fields(self):\n    class Model(models.Model):\n        age = models.SmallIntegerField()\n        parent = models.ForeignKey('self', models.CASCADE)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    name='name',\n                    fields=['age'],\n                    include=['parent__age'],\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"'constraints' refers to the joined field 'parent__age'.\",\n            obj=Model,\n            id='models.E041',\n        )\n    ])\n", "    def test_unique_constraint_include_pointing_to_joined_fields(self):\n        class Model(models.Model):\n            age = models.SmallIntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        name='name',\n                        fields=['age'],\n                        include=['parent__age'],\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the joined field 'parent__age'.\",\n                obj=Model,\n                id='models.E041',\n            )\n        ])\n", "def test_unique_constraint_include_pointing_to_joined_fields(self):\n    class Model(models.Model):\n        age = models.SmallIntegerField()\n        parent = models.ForeignKey('self', models.CASCADE)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    name='name',\n                    fields=['age'],\n                    include=['parent__age'],\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"'constraints' refers to the joined field 'parent__age'.\",\n            obj=Model,\n            id='models.E041',\n        )\n    ])\n", "def test_check_constraint_with_include(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n\n        class Meta:\n            constraints = [\n                models.CheckConstraint(\n                    name='name',\n                    check=models.Q(age__gte=18),\n                    include=['id'],\n                ),\n            ]\n\n    errors = Model.check(databases=self.databases)\n    expected = [\n        Error(\n            \"CheckConstraint doesn't support the 'include' argument.\",\n            obj=Model,\n            id='models.E042',\n        ),\n    ] if connection.features.supports_table_check_constraints else []\n    self.assertEqual(errors, expected)\n", "def test_unique_constraint_include_pointing_to_joined_fields(self):\n    class Target(models.Model):\n        field = models.IntegerField()\n\n    class Model(models.Model):\n        target = models.ForeignKey(Target, models.CASCADE)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['target__field'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"'constraints' refers to the joined field 'target__field'.\",\n            obj=Model,\n            id='models.E041',\n        ),\n    ])\n"], "sample_39": ["def test_to_fits_2():\n    \"\"\"\n    Test to_fits() with SIP distortion.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    wfits = w.to_fits()\n    assert isinstance(wfits, fits.HDUList)\n    assert isinstance(wfits[0], fits.PrimaryHDU)\n    assert 'A_ORDER' in wfits[0].header\n", "def test_naxis1_naxis2_deprecated():\n    w = wcs.WCS(naxis=2)\n    with pytest.warns(AstropyDeprecationWarning):\n        w.naxis1 = 100\n        w.naxis2 = 200\n    assert w._naxis == [100, 200]\n", "def test_wcs_with_no_axes():\n    w = wcs.WCS(naxis=0)\n    assert w.naxis == 0\n    assert w.wcs.naxis == 0\n    assert w.pixel_shape == ()\n    assert w.array_shape == ()\n", "def test_wcs_naxis():\n    w = wcs.WCS(naxis=2)\n    w.wcs.crval = [1, 1]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crpix = [1, 1]\n    w._naxis = [1000, 500]\n    assert w.wcs.naxis == 2\n", "def test_slice_with_step():\n    \"\"\"\n    Test slicing with a step.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.crval = [1, 1]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crpix = [1, 1]\n    w._naxis = [1000, 500]\n\n    w_sliced = w.slice([slice(None, None, 2), slice(None, None, 3)])\n    assert w_sliced.wcs.crpix == [0.6, 0.7]\n    assert w_sliced.wcs.cdelt == [0.2, 0.3]\n    assert w_sliced._naxis == [500, 167]\n"], "sample_45": ["def test_extract_time_func_with_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n    melb = pytz.timezone('Australia/Melbourne')\n\n    qs = DTModel.objects.annotate(\n        hour=ExtractHour('start_datetime'),\n        hour_melb=ExtractHour('start_datetime', tzinfo=melb),\n        minute=ExtractMinute('start_datetime'),\n        minute_melb=ExtractMinute('start_datetime', tzinfo=melb),\n        second=ExtractSecond('start_datetime'),\n        second_melb=ExtractSecond('start_datetime', tzinfo=melb),\n    ).order_by('start_datetime')\n\n    utc_model = qs.get()\n    self.assertEqual(utc_model.hour, 14)\n    self.assertEqual(utc_model.hour_melb, 20)\n    self.assertEqual(utc_model.minute, 30)\n    self.assertEqual(utc_model.minute_melb, 30)\n    self.assertEqual(utc_model.second, 50)\n    self.assertEqual(utc_model.second_melb, 50)\n\n    with timezone.override(melb):\n        melb_model = qs.get()\n\n    self.assertEqual(melb_model.hour, 20)\n    self.assertEqual(melb_model.hour_melb, 20)\n    self.assertEqual(melb_model.", "def test_trunc_quarter_boundaries_with_timezone(self):\n    melb = pytz.timezone('Australia/Melbourne')\n\n    last_quarter_2015 = datetime(2015, 12, 31, 14, 10, 50, 123)\n    first_quarter_2016 = datetime(2016, 1, 1, 14, 10, 50, 123)\n    last_quarter_2015 = timezone.make_aware(last_quarter_2015, is_dst=False)\n    first_quarter_2016 = timezone.make_aware(first_quarter_2016, is_dst=False)\n\n    self.create_model(last_quarter_2015, last_quarter_2015)\n    self.create_model(first_quarter_2016, first_quarter_2016)\n\n    qs = DTModel.objects.annotate(\n        melb_quarter=TruncQuarter('start_datetime', tzinfo=melb),\n    ).order_by('start_datetime')\n\n    self.assertQuerysetEqual(qs, [\n        (last_quarter_2015, truncate_to(last_quarter_2015, 'quarter', melb)),\n        (first_quarter_2016, truncate_to(first_quarter_2016, 'quarter', melb)),\n    ], lambda m: (m.start_datetime, m.melb_quarter))\n", "    def test_extract_quarter_func_with_timezone(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 8, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n\n        qs = DTModel.objects.annotate(\n            quarter=ExtractQuarter('start_datetime'),\n            quarter_melb=ExtractQuarter('start_datetime', tzinfo=melb),\n        ).order_by('start_datetime')\n\n        utc_model_1, utc_model_2 = qs\n        self.assertEqual(utc_model_1.quarter, 2)\n        self.assertEqual(utc_model_1.quarter_melb, 2)\n        self.assertEqual(utc_model_2.quarter, 3)\n        self.assertEqual(utc_model_2.quarter_melb, 3)\n\n        with timezone.override(melb):\n            melb_model_1, melb_model_2 = qs\n\n        self.assertEqual(melb_model_1.quarter, 2)\n        self.assertEqual(melb_model_1.quarter_melb, 2)\n        self.assertEqual(melb_model_2.quarter, 3)\n        self.assertEqual(melb_model_2.quarter_melb, 3)\n", "def test_trunc_week_func_with_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            extracted=TruncWeek('start_datetime', tzinfo=melb)\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, truncate_to(start_datetime.astimezone(melb), 'week', melb)),\n            (end_datetime, truncate_to(end_datetime.astimezone(melb), 'week', melb))\n        ],\n        lambda m: (m.start_datetime, m.extracted)\n    )\n    self.assertEqual(DTModel.objects.filter(start_datetime=TruncWeek('start_datetime', tzinfo=melb)).count(), 1)\n", "def test_trunc_time_with_timezone(self):\n    \"\"\"\n    Test TruncTime function with timezone argument for time fields.\n    \"\"\"\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            truncated=TruncTime('start_datetime', tzinfo=melb)\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, truncate_to(start_datetime.astimezone(melb).time(), 'second')),\n            (end_datetime, truncate_to(end_datetime.astimezone(melb).time(), 'second'))\n        ],\n        lambda m: (m.start_datetime, m.truncated)\n    )\n    self.assertEqual(DTModel.objects.filter(start_datetime__time=TruncTime('start_datetime', tzinfo=melb)).count(), 2)\n"], "sample_686": ["def test_fixture_positional_arguments_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them \"\n            \"as a keyword argument instead.\"\n        ]\n    )\n", "def test_fixture_positional_arguments_warning(testdir: Testdir) -> None:\n    \"\"\"Test the deprecation warning for passing positional arguments to pytest.fixture().\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(\"function\", \"arg1\", \"arg2\")\n            return request.param\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-v\")\n    result.stdout.fnmatch_lines([\"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\"])\n", "def test_warning_captured_hook(testdir: Testdir) -> None:\n    \"\"\"Check that pytest_warning_captured hook is deprecated (#7716)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import warnings\n            warnings.warn('Test warning', DeprecationWarning)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*The pytest_warning_captured is deprecated*\",\n         \"*Please use pytest_warning_recorded instead.*\"]\n    )\n", "def test_warning_captured_hook(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import warnings\n\n            warnings.warn(\"This is a deprecated warning\", DeprecationWarning)\n\n            assert isinstance(warning.message, DeprecationWarning)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated*\"])\n", "def test_warning_captured_hook(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import warnings\n        import pytest\n\n        @pytest.fixture\n            warnings.warn('Deprecated usage', DeprecationWarning)\n\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest('-rw')\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated*\"])\n"], "sample_391": ["def test_create_model_with_initial_data(self):\n    \"\"\"\n    CreateModel with initial data should not optimize away.\n    \"\"\"\n    self.assertDoesNotOptimize(\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [(\"name\", models.CharField(max_length=255))],\n                initial_data=[\n                    {\"name\": \"Initial data\"},\n                ],\n            ),\n            migrations.DeleteModel(\"Foo\"),\n        ],\n    )\n", "def test_create_model_with_default_manager(self):\n    \"\"\"\n    CreateModel should create a model with a default manager if one is specified.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=[(\"objects\", models.Manager())],\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=[(\"objects\", models.Manager())],\n            ),\n        ],\n    )\n", "def test_create_model_with_custom_manager(self):\n    \"\"\"\n    CreateModel should preserve the custom manager if it's not the default.\n    \"\"\"\n    managers = [(\"custom_objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_create_model_alter_unique_together_add_field(self):\n    \"\"\"\n    AlterUniqueTogether and AddField should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [\n                    (\"a\", models.IntegerField()),\n                    (\"b\", models.IntegerField()),\n                ],\n            ),\n            migrations.AlterUniqueTogether(\"Foo\", [[\"a\", \"b\"]]),\n            migrations.AddField(\"Foo\", \"c\", models.IntegerField()),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [\n                    (\"a\", models.IntegerField()),\n                    (\"b\", models.IntegerField()),\n                    (\"c\", models.IntegerField()),\n                ],\n                options={\"unique_together\": {(\"a\", \"b\")}},\n            ),\n        ],\n    )\n", "def test_create_model_with_indexes(self):\n    \"\"\"\n    CreateModel with indexes should optimize to CreateModel with indexes.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [(\"name\", models.CharField(max_length=255))],\n                options={\n                    \"indexes\": [\n                        models.Index(fields=[\"name\"], name=\"index_name\")\n                    ]\n                },\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [(\"name\", models.CharField(max_length=255))],\n                options={\n                    \"indexes\": [\n                        models.Index(fields=[\"name\"], name=\"index_name\")\n                    ]\n                },\n            ),\n        ],\n    )\n"], "sample_688": ["def test_get_lock_path(testdir):\n    path = Path('/test/path')\n    lock_path = get_lock_path(path)\n    assert lock_path == Path('/test/path/.lock')\n", "def test_collect_sub_with_symlinks_package(self, testdir):\n    \"\"\"Collection works with symlinked directories\"\"\"\n    sub = testdir.mkpydir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.join(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Symlink that gets collected.\n    symlink_or_skip(\"sub\", testdir.tmpdir.join(\"test_symlink\"))\n\n    result = testdir.runpytest(\"-v\", \"test_symlink\")\n    result.stdout.fnmatch_lines([\"test_symlink/test_file.py::test_file PASSED*\", \"*1 passed in*\"])\n", "def test_module_import_error(self, testdir):\n    \"\"\"Test that ImportError is raised when importing a module with errors.\"\"\"\n    testdir.makepyfile(\"import asdfasdfasdf\")\n    result = testdir.runpytest()\n    assert result.ret == pytest.ExitCode.INTERRUPTED\n    result.stdout.fnmatch_lines([\"*ImportError: No module named 'asdfasdfasdf'\"])\n", "def test_does_not_crash_on_error_from_decorated_function_with_assertion(testdir: Testdir) -> None:\n    \"\"\"Regression test for an issue around bad exception formatting due to\n    assertion rewriting mangling lineno's (#4984). This test includes an assertion\n    inside the decorated function to further test error handling.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture\n            assert False, \"This should fail\"\n            return 4\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    # Check for the assertion error message in the output\n    result.stdout.fnmatch_lines([\"*AssertionError: This should fail*\"])\n    # Check that the test failed due to an error\n    assert result.ret == ExitCode.TESTS_FAILED\n", "def test_import_path_importlib_with_custom_meta_importer(self, testdir):\n    \"\"\"Custom meta_importers are used by import_path when mode=importlib.\"\"\"\n    testdir.makepyfile(\n        **{\n            \"tests/custom_module.py\": \"\"\"\n                import sys\n                class CustomImporter:\n                        if fullname == \"custom_module\":\n                            return importlib.machinery.ModuleSpec(fullname, None)\n                        return None\n                        mod = importlib.util.module_from_spec(spec)\n                        mod.value = 42\n                        return mod\n                sys.meta_path.insert(0, CustomImporter())\n            \"\"\",\n            \"tests/test_custom_module.py\": \"\"\"\n                    from custom_module import value\n                    assert value == 42\n            \"\"\",\n        }\n    )\n    result = testdir.runpytest(\"-v\", \"--import-mode=importlib\")\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n"], "sample_888": ["def test_iforest_feature_importances(global_random_seed):\n    \"\"\"Check feature importances are zero for Isolation Forest.\"\"\"\n    X_train = np.array([[0, 1], [1, 2]])\n    clf = IsolationForest(random_state=global_random_seed).fit(X_train)\n    assert_array_equal(clf.feature_importances_, np.zeros(2))\n", "def test_iforest_feature_importances():\n    \"\"\"Test feature importances attribute.\"\"\"\n    X = iris.data\n    clf = IsolationForest().fit(X)\n    feature_importances = clf.feature_importances_\n    assert feature_importances.shape == (X.shape[1],)\n    assert np.all(feature_importances >= 0)\n    assert np.all(feature_importances <= 1)\n", "def test_iforest_with_contamination_boundary_values():\n    \"\"\"Test Isolation Forest with contamination at boundary values\"\"\"\n\n    # Contamination = 0.5\n    X = np.array([[1, 1], [1, 2], [2, 1], [2, 2], [3, 3], [4, 4]])\n    clf = IsolationForest(contamination=0.5)\n    clf.fit(X)\n    pred = clf.predict(X)\n    assert np.all(pred == -1), \"All samples should be predicted as outliers\"\n\n    # Contamination = 0\n    clf = IsolationForest(contamination=0)\n    clf.fit(X)\n    pred = clf.predict(X)\n    assert np.all(pred == 1), \"All samples should be predicted as inliers\"\n\n    # Contamination = 1\n    clf = IsolationForest(contamination=1)\n    clf.fit(X)\n    pred = clf.predict(X)\n    assert np.all(pred == -1), \"All samples should be predicted as outliers\"\n", "def test_iforest_random_state():\n    \"\"\"Check Isolation Forest stability across different random states.\"\"\"\n    X = np.array([[0, 1], [1, 2], [2, 1], [1, 1]])\n\n    # Generate predictions for different random states\n    pred1 = IsolationForest(random_state=1).fit(X).predict(X)\n    pred2 = IsolationForest(random_state=2).fit(X).predict(X)\n    pred3 = IsolationForest(random_state=3).fit(X).predict(X)\n\n    # Check that the results are the same regardless of the random state\n    assert_array_equal(pred1, pred2)\n    assert_array_equal(pred1, pred3)\n", "def test_iforest_feature_importances_raise_error():\n    \"\"\"Test that feature importances raise NotImplementedError.\"\"\"\n    X = iris.data\n    clf = IsolationForest().fit(X)\n\n    with pytest.raises(NotImplementedError):\n        clf.feature_importances_\n"], "sample_1148": ["def test_matrixsymbol_as_mutable():\n    Z = MatrixSymbol('Z', 2, 3)\n    assert Z.as_mutable() == Matrix([\n        [Z[0, 0], Z[0, 1], Z[0, 2]],\n        [Z[1, 0], Z[1, 1], Z[1, 2]],\n    ])\n    raises(ValueError, lambda: A.as_mutable())\n", "def test_matrix_determinant():\n    A = MatrixSymbol('A', 2, 2)\n    det_A = A[0, 0]*A[1, 1] - A[0, 1]*A[1, 0]\n    assert A.det() == det_A\n\n    A = MatrixSymbol('A', 3, 3)\n    det_A = A[0, 0]*A[1, 1]*A[2, 2] + A[0, 1]*A[1, 2]*A[2, 0] + A[0, 2]*A[1, 0]*A[2, 1] - A[0, 2]*A[1, 1]*A[2, 0] - A[0, 1]*A[1, 0]*A[2, 2] - A[0, 0]*A[1, 2]*A[2, 1]\n    assert A.det() == det_A\n", "def test_MatrixSymbol_commutative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    assert A*B != B*A\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    assert A*B == B*A\n", "def test_matrixsymbol_inverse():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    assert A.inv() == Inverse(A)\n    assert (A * A.inv()).simplify() == Identity(n)\n    assert (A.inv() * A).simplify() == Identity(n)\n    assert (A * B).inv() == B.inv() * A.inv()\n    assert (A * B * A.inv()).simplify() == B * A.inv()\n", "def test_MatrixElement_diff_with_matrix_symbol():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    expr = (A*B)[k, p].diff(A[q, r])\n    delta_kr = KroneckerDelta(k, q, (0, n-1))\n    delta_rp = KroneckerDelta(r, p, (0, m-1))\n    assert expr == B[r, p]*delta_kr\n    assert expr.doit() == B[r, p]*delta_kr\n"], "sample_802": ["def test_pipeline_memory_disabled():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Test with Transformer + SVC\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n    cached_pipe = Pipeline([('transf', transf), ('svc', clf)], memory=None)\n\n    # Memoize the transformer at the first fit\n    cached_pipe.fit(X, y)\n    pipe.fit(X, y)\n\n    # Check that cached_pipe and pipe yield identical results\n    assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n    assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n    assert_array_equal(pipe.predict_log_proba(X), cached_pipe.predict_log_proba(X))\n    assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n    assert_array_equal(pipe.named_steps['transf'].means_, cached_pipe.named_steps['transf'].means_)\n    assert not hasattr(transf, 'means_')\n", "def test_pipeline_wrong_memory_path():\n    # Test that an error is raised when memory is a non-existent path\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    # Define memory as a non-existent path\n    memory = \"/non/existent/path\"\n    cached_pipe = Pipeline([('transf', DummyTransf()),\n                            ('svc', SVC())], memory=memory)\n    assert_raises_regex(OSError, \"Couldn't create directory /non/existent/path\", cached_pipe.fit, X, y)\n", "def test_pipeline_memory_no_cache():\n    # Test that memory is not used when disabled\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y)\n        pipe.fit(X, y)\n        # Get the time stamp of the transformer in the cached pipeline\n        ts = cached_pipe.named_steps['transf'].timestamp_\n        # Disable caching and check that transformer is fitted again\n        cached_pipe.memory = None\n        cached_pipe.fit(X, y)\n        assert_not_equal(ts, cached_pipe.named_steps['transf'].timestamp_)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_none_estimator():\n    # Test that a pipeline with None estimator can be created and returns identity transform\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    pipe = Pipeline([('transf', Transf()), ('none_estimator', None)])\n    pipe.fit(X)\n    X_transformed = pipe.transform(X)\n    assert_array_equal(X, X_transformed)\n", "def test_pipeline_memory_disabled():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    # Test with Transformer + SVC with memory disabled\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n    cached_pipe = Pipeline([('transf', transf), ('svc', clf)], memory=None)\n\n    # Fit both pipelines\n    cached_pipe.fit(X, y)\n    pipe.fit(X, y)\n\n    # Check that cached_pipe and pipe yield identical results\n    assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n    assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n    assert_array_equal(pipe.predict_log_proba(X),\n                       cached_pipe.predict_log_proba(X))\n    assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n    assert_array_equal(pipe.named_steps['transf'].means_,\n                       cached_pipe.named_steps['transf'].means_)\n\n    # Check that the transformer was not cloned\n    assert not hasattr(transf, 'means_')\n"], "sample_1089": ["def test_issue_18059():\n    x = symbols('x')\n    assert factor_terms(x**(2*sqrt(2))) == (x**sqrt(2))**2\n", "def test_issue_17256_doit():\n    from sympy import Symbol, Range, Sum\n    x = Symbol('x')\n    s1 = Sum(x + 1, (x, 1, 9))\n    s2 = Sum(x + 1, (x, Range(1, 10)))\n    a = Symbol('a')\n    r1 = s1.doit()\n    r2 = s2.doit()\n\n    assert r1 == r2\n    s1 = Sum(x + 1, (x, 0, 9))\n    s2 = Sum(x + 1, (x, Range(10)))\n    a = Symbol('a')\n    r1 = s1.doit()\n    r2 = s2.doit()\n    assert r1 == r2\n", "def test_issue_7903_factor_terms():\n    a = symbols(r'a', real=True)\n    t = exp(I*cos(a)) + exp(-I*sin(a))\n    f = factor_terms(t)\n    assert f == exp(I*cos(a)) + exp(-I*sin(a))\n", "def test_issue_8921():\n    x = symbols('x')\n    assert factor_terms(exp(x + 1) - exp(x), sign=False) == exp(x)*(exp(1) - 1)\n", "def test_issue_19851():\n    from sympy import Symbol, diff\n    x = Symbol('x')\n    f = x**(1/2)\n    assert diff(f, x) == S.One/2*x**(-S.One/2)\n    f = x**(1/3)\n    assert diff(f, x) == S.One/3*x**(-S.Two/3)\n    f = x**(-1/2)\n    assert diff(f, x) == -S.One/2*x**(-S.Three/2)\n    f = x**(-1/3)\n    assert diff(f, x) == -S.One/3*x**(-S.Four/3)\n"], "sample_647": ["def test_unformatted_warning():\n    \"\"\"Make sure UnformattedWarning formats messages correctly.\"\"\"\n    warning = warning_types.UnformattedWarning(warning_types.PytestWarning, \"test {value}\")\n    formatted_warning = warning.format(value=\"warning\")\n    assert str(formatted_warning) == \"test warning\"\n", "def test_unformatted_warning_format(warning_class, expected_message):\n    \"\"\"Test the format method of UnformattedWarning.\"\"\"\n    test_value = \"test\"\n    unformatted_warning = warning_types.UnformattedWarning(warning_class, \"{}\")\n    warning = unformatted_warning.format(test_value)\n    assert str(warning) == expected_message.format(test_value)\n", "def test_unformatted_warning(warning_category, warning_template, format_kwargs, expected_message):\n    \"\"\"Test the formatting of UnformattedWarning instances.\"\"\"\n    if warning_template:\n        unformatted_warning = warning_types.UnformattedWarning(warning_category, warning_template)\n        warning = unformatted_warning.format(**format_kwargs)\n    else:\n        warning = warning_category.simple(**format_kwargs)\n\n    assert str(warning) == expected_message\n    assert isinstance(warning, warning_category)\n", "def test_unformatted_warning_format(warning_category, expected_message):\n    \"\"\"Test the format method of UnformattedWarning class.\"\"\"\n    warning = warning_types.UnformattedWarning(warning_category, \"{apiname} is an experimental api that may change over time\")\n    formatted_warning = warning.format(apiname=\"some_api\")\n    assert str(formatted_warning) == expected_message\n    assert isinstance(formatted_warning, warning_category)\n", "def test_unformatted_warning_format():\n    \"\"\"Test the format method of UnformattedWarning.\"\"\"\n    template = \"Test {value} formatting\"\n    kwargs = {\"value\": \"successful\"}\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=template\n    )\n    formatted_warning = warning.format(**kwargs)\n    assert str(formatted_warning) == \"Test successful formatting\"\n"], "sample_359": ["    def test_references_field_by_alter(self):\n        operation = migrations.AlterField('Model', 'field', models.ForeignKey('Other', models.CASCADE, to_field='field'))\n        self.assertIs(operation.references_field('Other', 'field', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n        self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "    def test_references_field_by_foreign_key_to_field(self):\n        operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE, to_field='field'))\n        self.assertIs(operation.references_field('Other', 'field', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n        self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "    def test_rename_field_with_through_fields(self):\n        operation = migrations.RenameField('Model', 'old_name', 'new_name')\n        operation.model_name = 'Through'\n        operation.field_name = 'through_fields'\n        operation.new_name = 'new_name'\n        self.assertIs(operation.references_field('Model', 'old_name', 'migrations'), True)\n        self.assertIs(operation.references_field('Model', 'new_name', 'migrations'), False)\n        self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "def test_reference_field_by_through_fields_missing(self):\n    operation = FieldOperation(\n        'Model', 'field', models.ManyToManyField('Other', through='Through', through_fields=('first', 'second'))\n    )\n    self.assertIs(operation.references_field('Through', 'missing', 'migrations'), False)\n", "    def test_no_options(self):\n        operation = migrations.AlterModelOptions('Model', {})\n        self.assertIs(operation.describe(), 'Change Meta options on Model')\n        self.assertIs(operation.migration_name_fragment, 'alter_model_options')\n"], "sample_14": ["def test_angle_to_string_format():\n    \"\"\"\n    Test the format parameter of the to_string method.\n    \"\"\"\n    a = Angle(45.0, u.deg)\n    assert a.to_string(format='unicode') == '45\u00b000\u203200\u2033'\n    assert a.to_string(format='latex') == '$45^\\circ00{}^\\prime00{}^{\\prime\\prime}$'\n    assert a.to_string(format='latex_inline') == '$45^\\circ00{}^\\prime00{}^{\\prime\\prime}$'\n    with pytest.raises(ValueError):\n        a.to_string(format='invalid_format')\n", "def test_angle_from_quantity():\n    # Test creating an Angle from a Quantity\n    q = Quantity(5, unit=u.degree)\n    a = Angle(q)\n    assert isinstance(a, Angle)\n    assert a.value == 5\n    assert a.unit == u.degree\n\n    # Test creating an Angle from a Quantity with an invalid unit\n    q = Quantity(5, unit=u.meter)\n    with pytest.raises(u.UnitsError):\n        Angle(q)\n\n    # Test creating an Angle from a Quantity with no unit\n    q = Quantity(5)\n    with pytest.raises(u.UnitsError):\n        Angle(q)\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test precision in to_string for Angle objects\n    \"\"\"\n\n    angle = Angle(\"54.12345678\", unit=u.degree)\n\n    assert angle.to_string(precision=2) == \"54d07m22.71s\"\n    assert angle.to_string(precision=4) == \"54d07m22.7059s\"\n    assert angle.to_string(precision=8) == \"54d07m22.70588235s\"\n", "def test_angle_to_sexagesimal_string_with_unicode_characters():\n    \"\"\"\n    Test the to_string method with the 'unicode' format and check if it returns the correct string\n    representation with unicode characters for sexagesimal notation.\n    \"\"\"\n    angle = Angle(\"54.12412\", unit=u.degree)\n    expected_result = \"54\u00b007\u203226.832\u2033\"\n    assert angle.to_string(format='unicode') == expected_result\n\n    angle = Angle(\"3:36:29.7888000120\", unit=u.hour)\n    expected_result = \"3\u02b036\u203229.7888\u2033\"\n    assert angle.to_string(format='unicode') == expected_result\n", "def test_angle_to_string_format():\n    \"\"\"\n    Test the to_string method with the format parameter.\n    \"\"\"\n    # test latex format\n    a = Angle(1.23456789, unit=u.degree)\n    assert a.to_string(format='latex') == r'${1^\\circ14{}^\\prime17.1938{}^{\\prime\\prime}}$'\n    assert a.to_string(format='latex_inline') == r'${1^\\circ14{}^\\prime17.1938{}^{\\prime\\prime}}$'\n\n    # test unicode format\n    assert a.to_string(format='unicode') == '1\u00b014\u203217.1938\u2033'\n\n    # test invalid format\n    with pytest.raises(ValueError):\n        a.to_string(format='invalid')\n"], "sample_465": ["def test_inline_has_change_permission_uses_obj(self):\n    class ConcertInline(TabularInline):\n        model = Concert\n\n            return bool(obj)\n\n    class BandAdmin(ModelAdmin):\n        inlines = [ConcertInline]\n\n    ma = BandAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockChangeUser()\n    self.assertEqual(ma.get_inline_instances(request), [])\n    band = Band(name=\"The Doors\", bio=\"\", sign_date=date(1965, 1, 1))\n    inline_instances = ma.get_inline_instances(request, band)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(inline_instances[0], ConcertInline)\n", "def test_get_inline_instances(self):\n    \"\"\"\n    get_inline_instances returns instances of InlineModelAdmin subclasses.\n    \"\"\"\n    class ConcertInline(TabularInline):\n        model = Concert\n\n    class BandAdmin(ModelAdmin):\n        inlines = [ConcertInline]\n\n    ma = BandAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockAddUser()\n    inline_instances = ma.get_inline_instances(request)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(inline_instances[0], ConcertInline)\n", "def test_get_formset_kwargs(self):\n    class ConcertInline(TabularInline):\n        model = Concert\n        fk_name = \"main_band\"\n\n    class BandAdmin(ModelAdmin):\n        inlines = [ConcertInline]\n\n    ma = BandAdmin(Band, self.site)\n    formset_kwargs = ma.get_formset_kwargs(request, self.band, ConcertInline, \"concerts\")\n    self.assertEqual(formset_kwargs[\"instance\"], self.band)\n    self.assertEqual(formset_kwargs[\"prefix\"], \"concerts\")\n    self.assertEqual(formset_kwargs[\"queryset\"], Concert.objects.none())\n    self.assertNotIn(\"data\", formset_kwargs)\n    self.assertNotIn(\"files\", formset_kwargs)\n    self.assertNotIn(\"save_as_new\", formset_kwargs)\n\n    request.method = \"POST\"\n    request.POST = {\"concerts-TOTAL_FORMS\": \"1\", \"concerts-INITIAL_FORMS\": \"0\"}\n    request.FILES = {}\n    formset_kwargs = ma.get_formset_kwargs(request, self.band, ConcertInline, \"concerts\")\n    self.assertEqual(formset_kwargs[\"instance\"], self.band)\n    self.assertEqual(formset_kwargs[\"prefix\"], \"concerts\")\n    self.assertEqual(formset_kwargs[\"queryset\"], Concert.objects.none())\n    self.assertEqual(formset_kwargs[\"data\"], request.POST)\n    self.assertEqual(formset_kwargs[\"files\"], request.FILES)\n    self.assertFalse(formset_kwargs[\"save_as_new\"])\n\n    request.POST = {\"concerts-TOTAL_FORMS\": \"1\", \"concerts-INITIAL_FORMS\": \"0\", \"_saveasnew\": \"1\"}\n    formset_kwargs = ma.get_formset_kwargs(request, self.band, ConcertInline, \"concerts\")\n    self.assertTrue(formset_kwargs[\"save_as_new\"])\n", "def test_modeladmin_get_queryset(self):\n    \"\"\"\n    ModelAdmin.get_queryset() should respect the ModelAdmin.has_view_or_change_permission().\n    \"\"\"\n    mock_request = MockRequest()\n    mock_request.user = User.objects.create_user(username=\"john\", password=\"secret\")\n    self.site.register(Band, ModelAdmin)\n    ma = self.site._registry[Band]\n\n    # User with no permissions.\n    original_method = ma.has_view_or_change_permission\n    ma.has_view_or_change_permission = lambda request: False\n    queryset = ma.get_queryset(mock_request)\n    self.assertEqual(queryset.count(), 0)\n\n    # User with permissions.\n    ma.has_view_or_change_permission = original_method\n    queryset = ma.get_queryset(mock_request)\n    self.assertEqual(queryset.count(), Band.objects.count())\n", "def test_get_formset_kwargs(self):\n    \"\"\"\n    Test that the get_formset_kwargs() method returns the correct parameters\n    for creating a formset.\n    \"\"\"\n    class ConcertInline(TabularInline):\n        model = Concert\n        fk_name = \"main_band\"\n        can_delete = False\n\n    ma = ConcertInline(Band, AdminSite())\n    request = MockRequest()\n    obj = Band.objects.create(name=\"Test Band\", bio=\"\", sign_date=date(1965, 1, 1))\n    prefix = \"concert\"\n    formset_params = ma.get_formset_kwargs(request, obj, prefix)\n    self.assertEqual(formset_params[\"instance\"], obj)\n    self.assertEqual(formset_params[\"prefix\"], prefix)\n    self.assertEqual(formset_params[\"queryset\"], Concert.objects.filter(main_band=obj))\n    self.assertFalse(\"data\" in formset_params)\n    self.assertFalse(\"files\" in formset_params)\n    self.assertFalse(\"save_as_new\" in formset_params)\n"], "sample_273": ["def test_default_auto_field_setting_overridden_in_model(self):\n    class Model(models.Model):\n        id = models.AutoField(primary_key=True)\n\n    self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n        Warning(self.msg, hint=self.hint, obj=Model, id='models.W042'),\n    ])\n", "    def test_custom_pk_name(self):\n        class Model(models.Model):\n            my_id = models.AutoField(primary_key=True)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_index_name_start_with_underscore_or_number(self):\n        class Model(models.Model):\n            class Meta:\n                app_label = 'check_framework.apps.CheckIndexesConfig'\n                indexes = [\n                    models.Index(fields=['id'], name='_invalid_name'),\n                    models.Index(fields=['id'], name='1invalid_name'),\n                ]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The index name '_invalid_name' cannot start with an underscore or a number.\",\n                obj=Model,\n                id='models.E033',\n            ),\n            Error(\n                \"The index name '1invalid_name' cannot start with an underscore or a number.\",\n                obj=Model,\n                id='models.E033',\n            ),\n        ])\n", "    def test_check_constraint_joined_field(self):\n        class Model(models.Model):\n            related_model = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(related_model__id__gt=0), name='foo'),\n                ]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\"'constraints' refers to the joined field 'related_model__id'.\", id='models.E041'),\n        ])\n", "    def test_check_constraint_with_joined_field(self):\n        class RelatedModel(models.Model):\n            value = models.IntegerField()\n\n        class Model(models.Model):\n            related = models.ForeignKey(RelatedModel, on_delete=models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(related__value__gt=0), name='check_constraint'),\n                ]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"'constraints' refers to the joined field 'related__value'.\",\n                obj=Model,\n                id='models.E041',\n            ),\n        ])\n"], "sample_1050": ["def test_NumPyPrinter_print_MatMul():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 3)\n    B = MatrixSymbol(\"B\", 3, 2)\n    assert n.doprint(A*B) == '(A).dot(B)'\n", "def test_NumPyPrinter_print_Min_Max():\n    p = NumPyPrinter()\n\n    assert p.doprint(Min(x, y)) == 'numpy.amin((x, y))'\n    assert p.doprint(Max(x, y)) == 'numpy.amax((x, y))'\n", "def test_NumPyPrinter_print_MatMul():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert n.doprint(A*B) == \"(A).dot(B)\"\n", "def test_NumPyPrinter_print_DotProduct():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 1, 3)\n    B = MatrixSymbol(\"B\", 3, 1)\n    assert n.doprint(A.dot(B)) == \"numpy.dot(A, B.T)\"\n", "def test_NumPyPrinter_print_MatMul():\n    p = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert p.doprint(A * B) == \"A.dot(B)\"\n"], "sample_793": ["def test_iforest_threshold():\n    X_train = [[1, 1], [1, 2], [2, 1], [10, 10]]  # the last sample is an outlier\n    clf = IsolationForest(contamination=0.1, behaviour='old').fit(X_train)\n    assert clf.threshold_ < clf.decision_function([[10, 10]])\n    assert clf.threshold_ > clf.decision_function([[1, 1]])\n", "def test_contamination_parameter():\n    X_train = [[1, 1], [1, 2], [2, 1], [10, 10]]\n    clf1 = IsolationForest(contamination=0.25).fit(X_train)\n    clf2 = IsolationForest(contamination='auto').fit(X_train)\n    assert clf1.offset_ != clf2.offset_\n    assert clf1.predict([[2., 2.]]) != clf2.predict([[2., 2.]])\n", "def test_contamination_param():\n    X_train = [[1, 1], [1, 2], [2, 1], [3, 3]]\n    clf1 = IsolationForest(contamination=0.25).fit(X_train)\n    clf2 = IsolationForest(contamination=0.5).fit(X_train)\n    assert_array_equal(clf1.decision_function([[3., 3.]]),\n                       clf2.decision_function([[3., 3.]]))\n", "def test_iforest_predict_probability():\n    \"\"\"Test the predict_proba method of IsolationForest.\"\"\"\n    X_train = np.array([[0, 1], [1, 2], [2, 1], [3, 3], [4, 4]])\n    X_test = np.array([[2, 1], [1, 1], [4, 5]])\n    clf = IsolationForest(random_state=rng).fit(X_train)\n    proba = clf.predict_proba(X_test)\n\n    # Check that the output shape is correct\n    assert proba.shape == (3, 2)\n\n    # Check that the sum of probabilities for each sample is 1\n    assert np.allclose(proba.sum(axis=1), 1)\n\n    # Check that the probability of outliers is greater for the sample [4, 5]\n    assert proba[2, 0] > proba[0, 0]\n    assert proba[2, 0] > proba[1, 0]\n", "def test_iforest_contamination_parameter():\n    \"\"\"Test Isolation Forest with different contamination parameters.\"\"\"\n    X = np.array([[0, 1], [1, 2], [2, 1], [3, 0], [4, 1], [5, 2]])\n    y = np.array([1, 1, 1, -1, -1, -1])\n\n    clf = IsolationForest(contamination=0.2).fit(X)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n\n    clf = IsolationForest(contamination=0.3).fit(X)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n\n    clf = IsolationForest(contamination='auto').fit(X)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n"], "sample_52": ["def test_choices_with_to_field_name(self):\n    f = forms.ModelChoiceField(Category.objects.all(), to_field_name='slug')\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.slug, 'Entertainment'),\n        (self.c2.slug, 'A test'),\n        (self.c3.slug, 'Third'),\n    ])\n    self.assertEqual(f.clean(self.c1.slug), self.c1)\n    self.assertEqual(f.clean(self.c1), self.c1)\n", "def test_modelmultiplechoicefield_custom_widget(self):\n    class CustomCheckboxSelectMultiple(CheckboxSelectMultiple):\n            # Add a custom attribute to each checkbox\n            output = super().render(name, value, attrs, renderer)\n            return output.replace('<input', '<input custom-attribute=\"value\"')\n\n    class CustomModelMultipleChoiceField(forms.ModelMultipleChoiceField):\n        widget = CustomCheckboxSelectMultiple\n\n    field = CustomModelMultipleChoiceField(Category.objects.all())\n    self.assertIn('custom-attribute=\"value\"', field.widget.render('name', []))\n", "def test_modelchoicefield_with_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name__startswith': 'E'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n    ])\n    # Add a Category object that doesn't match limit_choices_to\n    c4 = Category.objects.create(name='Fourth', slug='4th', url='4th')\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n    ])\n    # Update the queryset with a new limit_choices_to\n    f.limit_choices_to = {'name__startswith': 'T'}\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n        (c4.pk, 'Fourth'),  # Now this matches the new limit_choices_to\n    ])\n", "def test_empty_label_none_with_required(self):\n    f = forms.ModelChoiceField(Category.objects.all(), empty_label=None, required=True)\n    with self.assertRaises(ValidationError):\n        f.clean('')\n", "def test_custom_widget_attrs(self):\n    class CustomWidget(forms.RadioSelect):\n            context = super().get_context(name, value, attrs)\n            context['widget']['attrs']['custom_attribute'] = 'custom_value'\n            return context\n\n    class CustomModelChoiceField(forms.ModelChoiceField):\n        widget = CustomWidget\n\n    field = CustomModelChoiceField(Category.objects.all())\n    widget = field.widget\n    self.assertEqual(widget.get_context('name', '', {})['widget']['attrs']['custom_attribute'], 'custom_value')\n"], "sample_726": ["def test_label_binarize_multilabel_indicator():\n    y = [[1, 2], [0, 1, 2], [0]]\n    classes = [0, 1, 2]\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([[0, 1, 1], [1, 1, 1], [1, 0, 0]])\n    check_binarized_results(y, classes, pos_label, neg_label, expected)\n", "def test_label_binarize_multilabel_indicator():\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n    expected = pos_label * y_ind\n    y_sparse = [sparse_matrix(y_ind)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    for y in [y_ind] + y_sparse:\n        yield (check_binarized_results, y, classes, pos_label, neg_label,\n               expected)\n\n    # Test case for multilabel-indicator with pos_label != 1\n    y_ind = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n    classes = [0, 1, 2]\n    pos_label = 3\n    neg_label = 0\n    expected = pos_label * y_ind\n    y_sparse = [sparse_matrix(y_ind)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    for y in [y_ind] + y_sparse:\n        yield (check_binarized_results, y, classes, pos_label, neg_label,\n               expected)\n", "def test_label_binarize_multilabel_mismatch():\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1]  # Missing class 2\n    pos_label = 2\n    neg_label = 0\n    assert_raises(ValueError, label_binarize, y_ind, classes, pos_label, neg_label)\n", "def test_label_binarize_multilabel_unseen_labels():\n    y = csr_matrix([[0, 1, 0], [1, 1, 0], [0, 0, 1]])\n    classes = [0, 1]\n    pos_label = 1\n    neg_label = 0\n    assert_raises(ValueError, label_binarize, y, classes, pos_label=pos_label, neg_label=neg_label)\n", "def test_label_binarize_multilabel_errors():\n    # Test for errors in label_binarize with multilabel input\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1]\n    pos_label = 2\n    neg_label = 0\n\n    # Test for mismatch between number of classes and labels\n    assert_raises(ValueError, label_binarize, y_ind, classes, pos_label=pos_label, neg_label=neg_label)\n\n    # Test for pos_label less than or equal to neg_label\n    assert_raises(ValueError, label_binarize, y_ind, [0, 1, 2], pos_label=0, neg_label=0)\n    assert_raises(ValueError, label_binarize, y_ind, [0, 1, 2], pos_label=-1, neg_label=0)\n\n    # Test for multilabel with sparse_output=True and non-zero neg_label\n    assert_raises(ValueError, label_binarize, y_ind, [0, 1, 2], pos_label=3, neg_label=1, sparse_output=True)\n"], "sample_1028": ["def test_divmod_with_symbols():\n    assert divmod(x, 0) == (zoo, x)\n    assert divmod(0, x) == (0, 0)\n    assert divmod(x, 1) == (x, 0)\n    assert divmod(1, x) == (0, 1)\n", "def test_divmod_with_symbols():\n    x, y = symbols('x y', positive=True)\n    assert divmod(x, y) == (x//y, x % y)\n    assert divmod(x, 3) == (x//3, x % 3)\n    assert divmod(3, x) == (3//x, 3 % x)\n", "def test_divmod_symbols():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, positive=True)\n    assert divmod(n, 3) == (n//3, Mod(n, 3))\n    assert divmod(n, k) == (n//k, Mod(n, k))\n", "def test_div_eval():\n    assert (x/y)._eval_is_zero() is None\n    assert (x/3)._eval_is_zero() is None\n    assert (3/x)._eval_is_zero() is False\n    assert (0/x)._eval_is_zero() is True\n    assert (x/1)._eval_is_zero() is False\n    assert (0/1)._eval_is_zero() is True\n    assert (0/x)._eval_is_nonzero() is False\n    assert (x/y)._eval_is_nonzero() is None\n", "def test_divmod_is_integer():\n    p = Symbol('p', integer=True)\n    q1 = Symbol('q1', integer=True)\n    q2 = Symbol('q2', integer=True, nonzero=True)\n    assert divmod(x, y).is_integer is None\n    assert divmod(p, q1).is_integer is None\n    assert divmod(x, q2).is_integer is None\n    assert divmod(p, q2).is_integer is None\n"], "sample_441": ["def test_username_field_max_length_custom_user(self):\n    class CustomUserWithLongUsernameField(CustomUser):\n        username = models.CharField(max_length=200)\n\n    with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserWithLongUsernameField'):\n        CustomUserWithLongUsernameField.objects.create_user(username='u' * 200, password='password')\n        form = AuthenticationForm(None, {'username': 'u' * 200, 'password': 'password'})\n        self.assertEqual(form.fields['username'].max_length, 200)\n        self.assertEqual(form.fields['username'].widget.attrs.get('maxlength'), 200)\n        self.assertEqual(form.errors, {})\n", "    def test_help_text_translation(self):\n        french_help_texts = [\n            \"Votre mot de passe ne peut pas \u00eatre enti\u00e8rement num\u00e9rique.\",\n            \"Votre mot de passe doit contenir au moins 8 caract\u00e8res.\",\n            \"Votre mot de passe ne peut pas \u00eatre un mot de passe courant.\",\n            \"Votre mot de passe ne peut pas \u00eatre similaire \u00e0 vos autres informations personnelles.\",\n        ]\n        user = User.objects.get(username=\"testclient\")\n        form = AdminPasswordChangeForm(user)\n        with translation.override(\"fr\"):\n            html = form.as_p()\n            for french_text in french_help_texts:\n                self.assertIn(french_text, html)\n", "def test_unicode_email(self):\n    email = \"test@example.com\"\n    user = User.objects.create_user(\"testuser\", \"password\", \"b\u00e3o@example.com\")\n    data = {\"email\": \"b\u00e3o@example.com\"}\n    form = PasswordResetForm(data)\n    self.assertTrue(form.is_valid())\n    form.save(domain_override=\"example.com\")\n    self.assertEqual(form.cleaned_data[\"email\"], \"b\u00e3o@example.com\")\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertEqual(mail.outbox[0].to, [\"b\u00e3o@example.com\"])\n", "def test_unicode_username_in_admin_password_change_form(self):\n    User.objects.create_user(username=' uniquetestusername ', password='oldpassword')\n    user = User.objects.get(username=' uniquetestusername ')\n    data = {\n        'password1': 'newpassword',\n        'password2': 'newpassword',\n    }\n    form = AdminPasswordChangeForm(user, data)\n    self.assertTrue(form.is_valid())\n    form.save()\n    # Check that the username is still normalized\n    self.assertEqual(user.username, 'uniquetestusername')\n", "def test_custom_email_field_max_length(self):\n    max_length = CustomEmailField._meta.get_field(\"email\").max_length\n    data = {\"email\": \"a\" * (max_length + 1)}\n    form = PasswordResetForm(data)\n    self.assertFalse(form.is_valid())\n    self.assertEqual(\n        form.errors[\"email\"],\n        [\"Ensure this value has at most {} characters (it has {}).\".format(max_length, len(data[\"email\"]))],\n    )\n"], "sample_521": ["def test_scatter_spiral_modification():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    th = np.linspace(0, 2 * np.pi * 6, 256)\n    sc = ax.scatter(np.sin(th), np.cos(th), th, s=(1 + th * 5), c=th ** 2)\n    sc.set_edgecolor('C2')\n    sc.set_alpha(0.7)\n    fig.canvas.draw()\n    assert mcolors.same_color(sc.get_edgecolor(), (0.4, 0.6, 0.8, 0.7))\n", "def test_view_init_vertical_axis_inverse_roll():\n    \"\"\"\n    Test the actual projection, axis lines and ticks matches expected values.\n\n    Parameters\n    ----------\n    vertical_axis : str\n        Axis to align vertically.\n    proj_expected : ndarray\n        Expected values from ax.get_proj().\n    axis_lines_expected : tuple of arrays\n        Edgepoints of the axis line. Expected values retrieved according\n        to ``ax.get_[xyz]axis().line.get_data()``.\n    tickdirs_expected : list of int\n        indexes indicating which axis to create a tick line along.\n    \"\"\"\n    rtol = 2e-06\n    ax = plt.subplot(1, 1, 1, projection=\"3d\")\n    ax.view_init(elev=0, azim=0, roll=180, vertical_axis='z')\n    ax.figure.canvas.draw()\n\n    # Assert the projection matrix:\n    proj_expected = [\n        [0.0, -1.142857, 0.0, 0.571429],\n        [0.0, 0.0, -0.857143, 0.428571],\n        [0.0, 0.0, 0.0, -10.0],\n        [1.142857, 0.0, 0.0, -10.571429],\n    ]\n    proj_actual = ax.get_proj()\n    np.testing.assert_allclose(proj_expected, proj_actual, rtol=rtol)\n\n    # Assert black lines are correctly aligned:\n    axis_lines_expected = [\n        ([0.06329114, -0.06329114], [-0.04746835, 0.04746835]),\n        ([0.05617978, 0.06329114], [-0", "def test_view_init_vertical_axis_roll():\n    \"\"\"Test the actual projection with roll.\"\"\"\n    rtol = 2e-06\n    ax = plt.subplot(1, 1, 1, projection=\"3d\")\n    ax.view_init(elev=0, azim=0, roll=90, vertical_axis='z')\n    ax.figure.canvas.draw()\n\n    # Expected values from ax.get_proj() with roll=90\n    proj_expected = [\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, -1.0, 0.0, 0.0],\n        [0.0, 0.0, -1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0]\n    ]\n\n    proj_actual = ax.get_proj()\n    np.testing.assert_allclose(proj_expected, proj_actual, rtol=rtol)\n", "def test_view_init_vertical_axis_with_negative_roll():\n    \"\"\"\n    Test the actual projection, axis lines and ticks matches expected values\n    when the roll is negative.\n    \"\"\"\n    rtol = 2e-06\n    ax = plt.subplot(1, 1, 1, projection=\"3d\")\n    ax.view_init(elev=0, azim=0, roll=-45, vertical_axis='z')\n    ax.figure.canvas.draw()\n\n    proj_expected = [\n        [0.70710678, 0, 0.70710678, -0.5],\n        [0, 1, 0, 0],\n        [-0.70710678, 0, 0.70710678, 10.5],\n        [0, 0, 0, -10],\n    ]\n\n    axis_lines_expected = [\n        ([-0.06329114, -0.06329114], [0.04746835, -0.04746835]),\n        ([0.05617978, 0.06329114], [-0.04213483, -0.04746835]),\n        ([0.06329114, 0.06329114], [-0.04746835, -0.04746835]),\n    ]\n\n    tickdirs_expected = [1, 0, 0]\n\n    # Assert the projection matrix:\n    proj_actual = ax.get_proj()\n    np.testing.assert_allclose(proj_expected, proj_actual, rtol=rtol)\n\n    for i, axis in enumerate([ax.get_xaxis(), ax.get_yaxis(), ax.get_zaxis()]):\n        # Assert black lines are correctly aligned:\n        axis_", "def test_view_init_roll():\n    ax = plt.subplot(1, 1, 1, projection=\"3d\")\n    ax.view_init(elev=0, azim=0, roll=90)\n    ax.figure.canvas.draw()\n    # Check that the rolled view matches the expected rotation\n    np.testing.assert_allclose(ax.get_proj(), expected_proj, rtol=1e-6)\n"], "sample_490": ["def test_validate_nulls_distinct(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"],\n        name=\"name_nulls_distinct_uniq\",\n        nulls_distinct=True,\n    )\n    # Null values are considered distinct.\n    UniqueConstraintProduct.objects.create(name=None)\n    UniqueConstraintProduct.objects.create(name=None)\n    # Existing instances have their existing row excluded.\n    instance = UniqueConstraintProduct.objects.get(name=None)\n    constraint.validate(UniqueConstraintProduct, instance)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=None),\n        exclude={\"name\"},\n    )\n", "def test_validate_nullable_field_with_empty_string(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"price\"],\n        name=\"unique_price\",\n        condition=models.Q(price__isnull=False),\n    )\n    msg = \"Constraint \u201cunique_price\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(Product, Product(price=\"\"))\n", "def test_validate_nulls_distinct(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"name\"],\n        name=\"name_nulls_distinct_unique\",\n        nulls_distinct=True,\n    )\n    # Two NULL values are considered unique.\n    UniqueConstraintProduct.objects.create(name=None)\n    UniqueConstraintProduct.objects.create(name=None)\n    # A NULL value and a non-NULL value are considered unique.\n    UniqueConstraintProduct.objects.create(name=\"unique\")\n    # NULL values are not considered equal to empty strings.\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name=\"\")\n", "def test_nulls_distinct_validation(self):\n    UniqueConstraintProduct.objects.create(name=\"p1\", color=None)\n    UniqueConstraintProduct.objects.create(name=\"p1\", color=\"red\")\n\n    constraint = models.UniqueConstraint(\n        fields=[\"name\", \"color\"],\n        name=\"unique_fields\",\n        nulls_distinct=False,\n    )\n\n    # Validating a model with the same name but null color should not raise an error\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"p1\", color=None),\n    )\n", "def test_deferrable_initially_immediate_database_constraint(self):\n    obj_1 = UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"front\", price=10)\n    obj_2 = UniqueConstraintDeferrable.objects.create(name=\"p2\", shelf=\"back\", price=20)\n    obj_1.price, obj_2.price = obj_2.price, obj_1.price\n    with self.assertRaises(IntegrityError), atomic():\n        obj_1.save()\n    # Behavior can be changed with SET CONSTRAINTS.\n    with connection.cursor() as cursor:\n        constraint_name = connection.ops.quote_name(\"price_init_immediate_uniq\")\n        cursor.execute(\"SET CONSTRAINTS %s DEFERRED\" % constraint_name)\n        obj_1.save()\n        obj_2.save()\n"], "sample_141": ["def test_helpful_error_message_for_many2many_foreign_key(self):\n    \"\"\"\n    Invalid many-to-many keys for foreign keys should throw a helpful error message.\n    \"\"\"\n    test_string = \"\"\"[{\n        \"pk\": 1,\n        \"model\": \"serializers.article\",\n        \"fields\": {\n            \"author\": 1,\n            \"headline\": \"Unknown many to many foreign key\",\n            \"pub_date\": \"2014-09-15T10:35:00\",\n            \"authors\": [1, \"doesnotexist\"]\n        }\n    }, {\n        \"pk\": 1,\n        \"model\": \"serializers.author\",\n        \"fields\": {\n            \"name\": \"Agnes\"\n        }\n    }]\"\"\"\n    expected = \"(serializers.article:pk=1) field_value was 'doesnotexist'\"\n    with self.assertRaisesMessage(DeserializationError, expected):\n        list(serializers.deserialize('json', test_string))\n", "def test_helpful_error_message_for_many2many_forward_references(self):\n    \"\"\"\n    Invalid many-to-many keys with a forward reference should throw a helpful\n    error message.\n    \"\"\"\n    test_string = \"\"\"[{\n        \"pk\": 1,\n        \"model\": \"serializers.article\",\n        \"fields\": {\n            \"author\": 1,\n            \"headline\": \"Unknown many to many with forward reference\",\n            \"pub_date\": \"2014-09-15T10:35:00\",\n            \"categories\": [1, 2]\n        }\n    }, {\n        \"pk\": 1,\n        \"model\": \"serializers.author\",\n        \"fields\": {\n            \"name\": \"Agnes\"\n        }\n    }, {\n        \"pk\": 1,\n        \"model\": \"serializers.category\",\n        \"fields\": {\n            \"name\": \"Reference\"\n        }\n    }]\"\"\"\n    expected = \"(serializers.article:pk=1) field_value was '2'\"\n    with self.assertRaisesMessage(DeserializationError, expected):\n        for obj in serializers.deserialize('json', test_string):\n            obj.save()\n", "def test_helpful_error_message_for_many2many_not_iterable_fk(self):\n    \"\"\"\n    Not iterable many-to-many FK throws a helpful error message.\n    \"\"\"\n    test_string = \"\"\"[{\n        \"pk\": 1,\n        \"model\": \"serializers.article\",\n        \"fields\": {\n            \"author\": 1,\n            \"headline\": \"Not iterable many to many FK\",\n            \"pub_date\": \"2014-09-15T10:35:00\",\n            \"categories\": \"doesnotexist\"\n        }\n    }, {\n        \"pk\": 1,\n        \"model\": \"serializers.author\",\n        \"fields\": {\n            \"name\": \"Agnes\"\n        }\n    }]\"\"\"\n\n    expected = \"(serializers.article:pk=1) field_value was 'doesnotexist'\"\n    with self.assertRaisesMessage(DeserializationError, expected):\n        for obj in serializers.deserialize('json', test_string, ignore=False):\n            obj.save()\n", "def test_helpful_error_message_for_many2many_self_reference(self):\n    \"\"\"\n    Invalid many-to-many self-reference keys should throw a helpful error message.\n    \"\"\"\n    test_string = \"\"\"[{\n        \"pk\": 1,\n        \"model\": \"serializers.selfref\",\n        \"fields\": {\n            \"name\": \"A\",\n            \"children\": [2]\n        }\n    }, {\n        \"pk\": 2,\n        \"model\": \"serializers.selfref\",\n        \"fields\": {\n            \"name\": \"B\",\n            \"children\": []\n        }\n    }]\"\"\"\n    expected = \"(serializers.selfref:pk=1) field_value was '2'\"\n    with self.assertRaisesMessage(DeserializationError, expected):\n        list(serializers.deserialize('json', test_string))\n", "def test_helpful_error_message_for_many2many_non_natural_forward_reference(self):\n    \"\"\"\n    Invalid many-to-many keys with a forward reference should throw a helpful error message.\n    \"\"\"\n    test_string = \"\"\"[{\n        \"pk\": 1,\n        \"model\": \"serializers.article\",\n        \"fields\": {\n            \"author\": 1,\n            \"headline\": \"Unknown many to many with forward reference\",\n            \"pub_date\": \"2014-09-15T10:35:00\",\n            \"categories\": [1, 2]\n        }\n    }, {\n        \"pk\": 1,\n        \"model\": \"serializers.author\",\n        \"fields\": {\n            \"name\": \"Agnes\"\n        }\n    }]\"\"\"\n    expected = \"(serializers.article:pk=1) field_value was '2'\"\n    with self.assertRaisesMessage(DeserializationError, expected):\n        for obj in serializers.deserialize('json', test_string):\n            obj.save()\n"], "sample_626": ["def test_create_variables_multiindex(self) -> None:\n    foo_data = np.array([0, 0, 1], dtype=\"int64\")\n    bar_data = np.array([1.1, 1.2, 1.3], dtype=\"float64\")\n    pd_idx = pd.MultiIndex.from_arrays([foo_data, bar_data], names=(\"foo\", \"bar\"))\n    index_vars = {\n        \"foo\": IndexVariable(\"x\", foo_data, attrs={\"unit\": \"m\"}),\n        \"bar\": IndexVariable(\"x\", bar_data, encoding={\"fill_value\": 0}),\n    }\n\n    index = PandasMultiIndex(pd_idx, \"x\")\n    actual = index.create_variables(index_vars)\n\n    for k, expected in index_vars.items():\n        assert_identical(actual[k], expected)\n        assert actual[k].dtype == expected.dtype\n        assert actual[k].dtype == index.level_coords_dtype[k]\n", "def test_safe_cast_to_index_pandas_multiindex():\n    midx = pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=(\"one\", \"two\"))\n    expected = midx\n    actual = safe_cast_to_index(midx)\n    assert_array_equal(expected, actual)\n    assert isinstance(actual, pd.MultiIndex)\n", "def test_safe_cast_to_index_none():\n    array = None\n    expected = pd.Index([])\n    actual = safe_cast_to_index(array)\n    assert_array_equal(expected, actual)\n    assert isinstance(actual, pd.Index)\n", "def test_stack_dtype_preservation(indexes_and_vars) -> None:\n    _, variables = indexes_and_vars\n    variables[\"foo\"] = xr.Variable(\"foo\", np.array([0, 1], dtype=\"int32\"))\n\n    index = PandasMultiIndex.stack(variables, \"z\")\n    assert index.level_coords_dtype[\"foo\"] == np.int32\n", "def test_sel_boolean_multiindex(self) -> None:\n    # index should be ignored and indexer dtype should not be coerced\n    # see https://github.com/pydata/xarray/issues/5727\n    midx = pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=(\"one\", \"two\"))\n    index = PandasMultiIndex(midx, \"x\")\n    actual = index.sel({\"x\": [(False, 1), (True, 2), (False, 1), (True, 2)]})\n    expected_dim_indexers = {\"x\": [(False, 1), (True, 2), (False, 1), (True, 2)]}\n    np.testing.assert_array_equal(\n        actual.dim_indexers[\"x\"], expected_dim_indexers[\"x\"]\n    )\n"], "sample_204": ["def test_circular_dependencies(self):\n    \"\"\"\n    Makes sure the loader raises an error when circular dependencies are found.\n    \"\"\"\n    with self.assertRaises(CircularDependencyError):\n        MigrationLoader(connection)\n", "def test_missing_migration_class(self):\n    \"\"\"\n    MigrationLoader raises BadMigrationError if a migration module is missing a\n    Migration class.\n    \"\"\"\n    msg = \"Migration 0001_initial in app migrations has no Migration class\"\n    with self.assertRaisesMessage(BadMigrationError, msg):\n        MigrationLoader(connection)\n", "def test_first_dependency(self):\n    \"\"\"\n    Makes sure the '__first__' migrations build correctly with dependencies.\n    \"\"\"\n    migration_loader = MigrationLoader(connection)\n    self.assertEqual(\n        migration_loader.graph.forwards_plan((\"migrations\", \"second\")),\n        [\n            (\"migrations\", \"thefirst\"),\n            (\"dependency\", \"0001_initial\"),\n            (\"migrations\", \"second\"),\n        ],\n    )\n", "def test_detect_conflicts(self):\n    \"\"\"\n    Tests the detection of conflicting migrations.\n    \"\"\"\n    migration_loader = MigrationLoader(connection)\n    conflicts = migration_loader.detect_conflicts()\n    self.assertEqual(conflicts, {})\n\n    # Add a conflicting migration\n    migration_loader.disk_migrations[(\"migrations\", \"0003_conflicting\")] = \"conflicting_migration\"\n    migration_loader.graph.add_node((\"migrations\", \"0003_conflicting\"), \"conflicting_migration\")\n\n    conflicts = migration_loader.detect_conflicts()\n    self.assertEqual(conflicts, {\"migrations\": [\"0001_initial\", \"0002_second\", \"0003_conflicting\"]})\n", "def test_run_before_conflict(self):\n    \"\"\"\n    Makes sure run_before conflicts raise an appropriate exception.\n    \"\"\"\n    with self.assertRaisesMessage(NodeNotFoundError, \"Migration migrations.0002_second has a run_before dependency on nonexistent parent node ('migrations', 'nonexistent')\"):\n        MigrationLoader(connection)\n"], "sample_984": ["def test_ZeroMatrix():\n    Z = ZeroMatrix(3, 3)\n    assert str(Z) == \"0\"\n", "def test_Inverse():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    assert str(A.inv()) == \"A**-1\"\n", "def test_HadamardProduct():\n    from sympy import HadamardProduct, MatrixSymbol\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = HadamardProduct(A, B)\n    assert str(C) == \"A.*B\"\n", "def test_MatrixPow():\n    A = Matrix([[1, 2], [3, 4]])\n    B = A**2\n    assert str(B) == \"Matrix([[7, 10], [15, 22]])\"\n\n    C = MatrixSymbol(\"C\", 2, 2)\n    D = C**3\n    assert str(D) == \"C**3\"\n", "def test_ZeroMatrix():\n    M = ZeroMatrix(2, 2)\n    assert str(M) == sstr(M) == \"0\"\n    assert str(M + x) == sstr(M + x) == \"Matrix([[x, x], [x, x]])\"\n"], "sample_422": ["def test_foreignkey_forward(self):\n    authors = Author.objects.order_by(\"name\")\n    with self.assertNumQueries(3):\n        books = list(\n            Book.objects.prefetch_related(\n                Prefetch(\"first_book__authors\", authors),\n                Prefetch(\"first_book__authors\", authors[1:], to_attr=\"authors_sliced\"),\n            )\n        )\n    for book in books:\n        with self.subTest(book=book):\n            self.assertEqual(\n                book.authors_sliced,\n                list(book.first_book.authors.all())[1:],\n            )\n", "def test_foreignkey_forward(self):\n    books = Book.objects.all()\n    with self.assertNumQueries(3):\n        authors = list(\n            Author.objects.prefetch_related(\n                Prefetch(\"first_book\", books),\n                Prefetch(\"first_book\", books[1:], to_attr=\"first_book_sliced\"),\n            )\n        )\n    for author in authors:\n        with self.subTest(author=author):\n            self.assertEqual(\n                author.first_book_sliced,\n                list(Book.objects.filter(first_time_authors=author))[1:],\n            )\n", "def test_foreignkey_forward(self):\n    authors = Author.objects.all()  # Meta.ordering\n    with self.assertNumQueries(3):\n        books = list(\n            Book.objects.prefetch_related(\n                Prefetch(\"first_book__authors\", authors),\n                Prefetch(\"first_book__authors\", authors[1:], to_attr=\"authors_sliced\"),\n            )\n        )\n    for book in books:\n        with self.subTest(book=book):\n            if book.first_book:\n                self.assertEqual(book.authors_sliced, list(book.first_book.authors.all())[1:])\n            else:\n                self.assertIsNone(book.authors_sliced)\n", "def test_foreignkey_forward(self):\n    with self.assertNumQueries(2):\n        first_books = Author.objects.prefetch_related(\n            Prefetch('first_book', Book.objects.all()[1:])\n        ).values_list('first_book', flat=True)\n    self.assertNotIn(self.book1.id, first_books)\n    self.assertIn(self.book2.id, first_books)\n", "def test_foreignkey_forward(self):\n    with self.assertNumQueries(2):\n        first_books = list(Author.objects.values_list('first_book', flat=True))\n        books = Book.objects.prefetch_related(\n            Prefetch('authors', Author.objects.all(), to_attr='authors_prefetched')\n        )\n        for book in books:\n            self.assertEqual(\n                list(book.authors_prefetched.values_list('first_book', flat=True)),\n                [book.id] if book.id in first_books else []\n            )\n"], "sample_1100": ["def test_issue_18507_b():\n    assert Mul(zoo, 0, zoo) is nan\n", "def test_divmod_symbolic():\n    a, b = symbols('a b', integer=True)\n    assert divmod(a, b) == (a//b, a % b)\n    assert divmod(a, 3) == (a//3, a % 3)\n    assert divmod(3, a) == (3//a, 3 % a)\n", "def test_is_finite():\n    assert (x*y).is_finite is None\n    assert (x*2).is_finite is None\n    assert (sqrt(2)*Rational(1, 3)).is_finite is True\n", "def test_divmod_with_float():\n    assert divmod(1.5, 0.5) == (3, 0.0)\n    assert divmod(1.5, 2.5) == (0, 1.5)\n    assert divmod(2.5, 1.5) == (1, 1.0)\n", "def test_div_zero_handling():\n    # Test division by zero\n    with raises(ZeroDivisionError):\n        _ = 1/0\n    with raises(ZeroDivisionError):\n        _ = x/0\n    with raises(ZeroDivisionError):\n        _ = 0/x\n    with raises(ZeroDivisionError):\n        _ = x/y\n    y = Symbol('y', zero=True)\n    with raises(ZeroDivisionError):\n        _ = x/y\n"], "sample_226": ["    def test_migrate_test_setting_not_specified(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        # TEST['MIGRATE'] is not specified in the settings\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Migrations run by default when TEST['MIGRATE'] is not specified\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n            # App is not synced.\n            mocked_sync_apps.assert_not_called()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test", "    def test_reuse_test_database(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        # create_test_db() and _create_test_db() can reuse the test database\n        # when keepdb is True.\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        test_connection.settings_dict['TEST']['KEEPDB'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db', return_value=old_database_name):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            # Migrations run.\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n            # App is not synced.\n            mocked_sync_apps.assert_not_called", "    def test_keepdb_true(self, mocked_sync_apps):\n        # _create_test_db() should not recreate the database if keepdb is True\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        creation._execute_create_test_db = mock.Mock()\n        with mock.patch('builtins.input', return_value='yes'):\n            creation._create_test_db(verbosity=0, autoclobber=False, keepdb=True)\n            creation._execute_create_test_db.assert_not_called()\n", "    def test_serialize_deserialize_cycle(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handles\n        # cyclic references and correctly restores the data.\n        obj_a = CircularA.objects.create(key='A')\n        obj_b = CircularB.objects.create(key='B', obj=obj_a)\n        obj_a.obj = obj_b\n        obj_a.save()\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        CircularA.objects.all().delete()\n        CircularB.objects.all().delete()\n        # Deserialize objects.\n        connection.creation.deserialize_db_from_string(data)\n        obj_a_deserialized = CircularA.objects.get()\n        obj_b_deserialized = CircularB.objects.get()\n        # Check if the deserialized objects match the original ones.\n        self.assertEqual(obj_a_deserialized.key, 'A')\n        self.assertEqual(obj_b_deserialized.key, 'B')\n        self.assertEqual(obj_a_deserialized.obj, obj_b_deserialized)\n        self.assertEqual(obj_b_deserialized.obj, obj_a_deserialized)\n", "    def test_non_migrated_app(self):\n        # serialize_db_to_string() does not serialize non-migrated apps.\n        Object.objects.create(key='test')\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            loader_instance = loader.return_value\n            # Mark the backends app as not migrated.\n            loader_instance.migrated_apps = {'other_app'}\n            data = connection.creation.serialize_db_to_string()\n        self.assertEqual(data, '[]')\n"], "sample_727": ["def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    imputer = Imputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n    imputer = Imputer(missing_values=0, strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_axis():\n    # Test imputation with an invalid axis value.\n    X = np.random.randn(10, 2)\n    X[::2] = np.nan\n    imputer = Imputer(strategy='mean', axis=2)\n    with assert_raises(ValueError):\n        imputer.fit(X)\n", "def test_imputation_with_different_missing_values():\n    # Test imputation with different missing values.\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, 'NaN', 4],\n        [5, 6, ''],\n        ['', 8, 9],\n    ])\n\n    X_imputed_mean = np.array([\n        [4.5, 1, 2],\n        [3, 4.5, 4],\n        [5, 6, 5.5],\n        [4.5, 8, 9],\n    ])\n    statistics_mean = [4.5, 4.5, 5.5]\n\n    X_imputed_median = np.array([\n        [5, 1, 2],\n        [3, 6, 4],\n        [5, 6, 6],\n        [5, 8, 9],\n    ])\n    statistics_median = [5, 6, 6]\n\n    X_imputed_most_frequent = np.array([\n        [5, 1, 2],\n        [3, 'NaN', 4],\n        [5, 6, 2],\n        [5, 8, 9],\n    ])\n    statistics_most_frequent = [5, 'NaN', 2]\n\n    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, 'NaN')\n    _check_statistics(X, X_imputed_median, \"median\", statistics_median, 'NaN')\n    _check_statistics(X, X_imputed_most_frequent, \"most_frequent\", statistics_most_frequent, 'NaN')\n", "def test_imputation_with_non_numeric_values():\n    # Test imputation with non-numeric values (strings)\n    X = np.array([\n        ['a', 'b', 'NaN'],\n        ['c', 'NaN', 'e'],\n        ['f', 'g', 'h']\n    ], dtype=object)\n\n    X_imputed_most_frequent = np.array([\n        ['a', 'b', 'b'],\n        ['c', 'b', 'e'],\n        ['f', 'g', 'h']\n    ], dtype=object)\n\n    statistics_most_frequent = ['a', 'b', 'b']\n\n    _check_statistics(X, X_imputed_most_frequent, \"most_frequent\",\n                      statistics_most_frequent, 'NaN')\n\n    # Test that an error is raised for mean or median strategies\n    for strategy in [\"mean\", \"median\"]:\n        imputer = Imputer(missing_values=\"NaN\", strategy=strategy)\n        assert_raises(ValueError, imputer.fit, X)\n"], "sample_855": ["def test_constant_strategy_multioutput_with_2d_input():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = np.array([[2, 3],\n                  [1, 3],\n                  [2, 3],\n                  [2, 0]])\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0,\n                          constant=[[1, 0]])\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, np.tile([1, 0], (len(X), 1)))\n", "def test_dummy_classifier_sparse_target_prediction_error():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[2, 1],\n                                [2, 2],\n                                [1, 4],\n                                [4, 2],\n                                [1, 1]]))\n\n    clf = DummyClassifier(strategy=\"uniform\", random_state=0)\n    clf.fit(X, y)\n\n    with pytest.raises(ValueError, match=\"Sparse target prediction is not supported\"):\n        clf.predict(X)\n", "def test_constant_strategy_multioutput_with_sparse_target():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = sp.csc_matrix(np.array([[2, 0, 5, 4, 3],\n                                [2, 0, 1, 2, 5],\n                                [1, 0, 4, 5, 2],\n                                [1, 3, 3, 2, 0]]))\n\n    n_samples = len(X)\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=[1, 0, 0, 0, 0])\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert sp.issparse(y_pred)\n    assert_array_equal(y_pred.toarray(), np.hstack([np.ones((n_samples, 1)),\n                                                    np.zeros((n_samples, 4))]))\n", "def test_constant_strategy_with_invalid_constant(y, constant):\n    X = [[0], [0], [0], [0]]\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=constant)\n\n    err_msg = \"The constant target value must be present in the training data.*Possible values are.*\"\n    with pytest.raises(ValueError, match=err_msg):\n        clf.fit(X, y)\n", "def test_constant_strategy_multioutput_classifier():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = np.array([[1, 0, 5, 4, 3],\n                  [2, 0, 1, 2, 5],\n                  [1, 0, 4, 5, 2],\n                  [1, 3, 3, 2, 0]])\n    n_samples = len(X)\n    constant = [1, 2, 3, 4, 0]\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=constant)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    y_expected = np.hstack([np.ones((n_samples, 1)),\n                            np.full((n_samples, 1), 2),\n                            np.full((n_samples, 1), 3),\n                            np.full((n_samples, 1), 4),\n                            np.zeros((n_samples, 1))])\n    assert_array_equal(y_pred, y_expected)\n"], "sample_953": ["def test_quickstart_conflict_imgmath_mathjax(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n        'imgmath': 'y',\n        'mathjax': 'y',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n\n    assert 'sphinx.ext.imgmath' not in d['extensions']\n    assert 'sphinx.ext.mathjax' in d['extensions']\n", "def test_quickstart_conflicting_extensions(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n        'imgmath': 'y',\n        'mathjax': 'y',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n\n    assert 'sphinx.ext.imgmath' in d['extensions']\n    assert 'sphinx.ext.mathjax' in d['extensions']\n\n    qs.generate(d)\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n\n    assert 'sphinx.ext.imgmath' not in ns['extensions']\n    assert 'sphinx.ext.mathjax' in ns['extensions']\n", "def test_quiet_mode_validation(tempdir):\n    with pytest.raises(SystemExit) as pytest_wrapped_e:\n        qs.main(['-q', tempdir])\n    assert pytest_wrapped_e.type == SystemExit\n    assert pytest_wrapped_e.value.code == 1\n\n    with pytest.raises(SystemExit) as pytest_wrapped_e:\n        qs.main(['-q', '-p', 'project_name', tempdir])\n    assert pytest_wrapped_e.type == SystemExit\n    assert pytest_wrapped_e.value.code == 1\n\n    with pytest.raises(SystemExit) as pytest_wrapped_e:\n        qs.main(['-q', '-a', 'author', tempdir])\n    assert pytest_wrapped_e.type == SystemExit\n    assert pytest_wrapped_e.value.code == 1\n", "def test_quickstart_with_custom_template(tempdir, tmpdir):\n    custom_template = tmpdir.mkdir(\"custom_templates\")\n    custom_template.join(\"conf.py_t\").write(\"Custom {{ project }}\")\n\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Custom Project',\n        'Author name': 'Custom Author',\n        'Project version': '1.0',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d, templatedir=str(custom_template))\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    content = conffile.read_text()\n    assert content == 'Custom Custom Project'\n", "def test_quickstart_with_existing_masterfile(tempdir, monkeypatch):\n    existing_masterfile = tempdir / 'contents.rst'\n    existing_masterfile.touch()\n\n        return path == str(existing_masterfile)\n\n    monkeypatch.setattr(path, 'isfile', mock_isfile)\n\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n        'Name of your master document': 'contents',\n        'Please enter a new file name, or rename the existing file and press Enter': 'new_contents'\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    new_masterfile = tempdir / 'new_contents.rst'\n    assert new_masterfile.isfile()\n"], "sample_1062": ["def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 - tan(x)**2) == -tan(x)**2 + 1\n    assert TR22(1 - cot(x)**2) == -cot(x)**2 + 1\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 - cot(x)**2) == -tan(x)**2\n    assert TR22(1 - tan(x)**2) == -cot(x)**2\n", "def test_TR12i_edge_cases():\n    # Test cases with simple expressions\n    assert TR12i(tan(x)) == tan(x)\n    assert TR12i(1) == 1\n\n    # Test cases with more complex expressions\n    assert TR12i((tan(a) + tan(b))/(1 - tan(a)*tan(b))) == tan(a + b)\n    assert TR12i((tan(a) - tan(b))/(1 + tan(a)*tan(b))) == tan(a - b)\n\n    # Test cases with expressions that should not change\n    eq = (tan(x) + cos(2))/(1 - tan(x)*tan(y))\n    assert TR12i(eq) == eq\n    eq = (tan(x) + 2)**2/(1 - tan(x)*tan(y))\n    assert TR12i(eq) == eq\n    eq = tan(x)/(1 - tan(x)*tan(y))\n    assert TR12i(eq) == eq\n\n    # Test cases with expressions containing powers\n    eq = (((tan(a) + tan(b))*(a + 1)).expand())**2/(1 - tan(a)*tan(b))\n    assert TR12i(eq) == -(a + 1)**2*tan(a + b)\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n"], "sample_300": ["def test_multiple_transforms(self):\n    query = Query(Author, alias_cols=False)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower__exact='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "def test_complex_transform(self):\n    query = Query(Author, alias_cols=False)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower__exact='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "def test_transform_with_filtered_relation(self):\n    query = Query(Author, alias_cols=False)\n    query.add_filtered_relation(FilteredRelation('author_book', condition=Q(book__isnull=False)), 'author_book')\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(author_book__book__title__lower='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('book').related_model._meta.get_field('title'))\n", "def test_query_with_transform_and_lookup(self):\n    query = Query(Author, alias_cols=False)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower__exact='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "def test_filter_transform_function(self):\n    query = Query(Author, alias_cols=False)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower='foo'))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertIsNone(lookup.lhs.lhs.alias)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n"], "sample_1045": ["def test_Float_precision():\n    # Make sure Float inputs for keyword args work\n    assert Float('1.0', dps=Float(15.0))._prec == 15\n    assert Float('1.0', precision=Float(15.0))._prec == 15\n    assert type(Float('1.0', precision=Float(15.0))._prec) == int\n    assert sympify(srepr(Float('1.0', precision=15.0))) == Float('1.0', precision=15)\n", "def test_Float_eq_numpy():\n    from sympy.utilities.pytest import skip\n    from sympy.external import import_module\n    np = import_module('numpy')\n    if not np:\n        skip('numpy not installed. Abort numpy tests.')\n\n    assert Float(.12, 3) != np.float64(.12)\n    assert Float(.12, 3) == np.float32(.12)\n    assert np.float64(.12) == Float(.12, 3)\n    assert np.float32(.12) == Float(.12, 3)\n    assert Float('.12', 22) != np.float64(.12)\n    assert Float('.12', 22) != np.float32(.12)\n", "def test_Float_sqrt():\n    # Test that the square root of a Float is also a Float\n    a = Float(4.0)\n    b = sqrt(a)\n    assert isinstance(b, Float)\n    assert b == 2.0\n\n    # Test that the square root of a negative Float raises a ValueError\n    a = Float(-4.0)\n    with raises(ValueError):\n        sqrt(a)\n", "def test_issue_10785():\n    assert S(0)**S(-1) == S.ComplexInfinity\n", "def test_issue_11987():\n    assert sqrt(Rational(2, 3)).as_coeff_Mul() == (S.One, sqrt(Rational(2, 3)))\n    assert sqrt(Rational(2, 3)).as_coeff_Mul()[1] == sqrt(Rational(2, 3))\n    assert sqrt(Rational(2, 3)).as_coeff_Mul(rational=True) == (S.One, sqrt(Rational(2, 3)))\n    assert sqrt(Rational(2, 3)).as_coeff_Mul(rational=True)[1] == sqrt(Rational(2, 3))\n    assert sqrt(Rational(2, 3)).as_coeff_Mul(rational=False) == (S.One, sqrt(Rational(2, 3)))\n    assert sqrt(Rational(2, 3)).as_coeff_Mul(rational=False)[1] == sqrt(Rational(2, 3))\n"], "sample_1071": ["def test_convert_to_incompatible_units():\n    assert convert_to(meter, second) == meter\n", "def test_convert_to_incompatible_units():\n    assert convert_to(meter, second) == meter\n    assert convert_to(second, meter) == second\n", "def test_convert_to_incompatible_units():\n    assert convert_to(mile, kelvin) == mile\n    assert convert_to(kelvin, mile) == kelvin\n", "def test_convert_to_additional_quantities():\n    assert convert_to(kelvin, degree) == 1.8*degree\n    assert convert_to(degree, kelvin) == 5/9*kelvin\n    assert convert_to(kelvin, [meter, degree]) == 1.8*degree\n    assert convert_to(degree, [meter, kelvin]) == 5/9*kelvin\n    assert convert_to(minute, second) == 60*second\n    assert convert_to(hour, second) == 3600*second\n    assert convert_to(inch, centimeter) == 2.54*centimeter\n    assert convert_to(gram, kilogram) == 0.001*kilogram\n    assert convert_to(joule, kilogram*meter**2/second**2) == kilogram*meter**2/second**2\n", "def test_convert_to_additional_scenarios():\n    assert convert_to(2*meter + 3*centimeter, kilometer) == 0.00203*kilometer\n    assert convert_to(sqrt(meter) + 1, meter) == sqrt(meter) + 1\n    assert convert_to(2*joule + 3*kilogram*meter**2/second**2, kilogram*meter**2/second**2) == 5*kilogram*meter**2/second**2\n"], "sample_467": ["def test_render_attrs(self):\n    widget = SelectDateWidget(years=(\"2022\",), attrs={\"class\": \"test-class\"})\n    self.assertInHTML(\n        '<select name=\"mydate_year\" id=\"id_mydate_year\" class=\"test-class\">',\n        widget.render(\"mydate\", \"\"),\n    )\n", "def test_render_with_attrs(self):\n    widget = SelectDateWidget(\n        years=(\"2007\",),\n        attrs={\"class\": \"custom-class\", \"data-test\": \"test-value\"},\n    )\n    self.check_html(\n        widget,\n        \"mydate\",\n        \"\",\n        html=(\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" class=\"custom-class\" data-test=\"test-value\">\n                <option selected value=\"\">---</option>\n                <!-- Rest of the options here -->\n            </select>\n            <!-- Rest of the select elements here -->\n            \"\"\"\n        ),\n    )\n", "def test_render_attrs(self):\n    widget = SelectDateWidget(years=(2007,), attrs={'class': 'test-class'})\n    self.check_html(\n        widget,\n        \"mydate\",\n        \"\",\n        html=(\n            \"\"\"\n        <select name=\"mydate_month\" class=\"test-class\" id=\"id_mydate_month\">\n            <option selected value=\"\">---</option>\n            <!-- rest of the options here -->\n        </select>\n        <!-- rest of the select tags here -->\n        \"\"\"\n        ),\n    )\n", "def test_selectdate_localized_date_format(self):\n    class LocalizedDateForm(Form):\n        date = DateField(widget=SelectDateWidget)\n\n    with translation.override(\"de-at\"):\n        form = LocalizedDateForm()\n        self.assertHTMLEqual(\n            form.as_p(),\n            \"\"\"\n            <p><label for=\"id_date_day\">Date:</label> <select name=\"date_day\" id=\"id_date_day\">\n            <!-- Options for days 1-31 here -->\n            </select> <select name=\"date_month\" id=\"id_date_month\">\n            <option value=\"1\">J\u221a\u00a7nner</option>\n            <option value=\"2\">Februar</option>\n            <!-- More month options here -->\n            </select> <select name=\"date_year\" id=\"id_date_year\">\n            <!-- Options for years here -->\n            </select></p>\n            \"\"\",\n        )\n", "def test_selectdate_invalid_date_value_from_datadict(self):\n    data = {\n        \"field_year\": \"2010\",\n        \"field_month\": \"02\",\n        \"field_day\": \"30\",\n    }\n    self.assertEqual(\n        self.widget.value_from_datadict(data, {}, \"field\"), \"2010-02-30\"\n    )\n"], "sample_593": ["def test_summarize_variable_with_unsafe_name():\n    var = xr.Variable([\"<time>\", \"x\"], [[1, 2, 3], [4, 5, 6]], {\"foo\": \"bar\"})\n    formatted = fh.summarize_variable(\"<var>\", var)\n    assert \"&lt;time&gt;\" in formatted\n    assert \"&lt;var&gt;\" in formatted\n", "def test_summarize_variable_with_unsafe_name_and_dtype():\n    variable = xr.Variable((\"<x>\", \"y\"), np.random.randn(3, 2), {\"foo\": \"bar\"})\n    formatted = fh.summarize_variable(\"<name>\", variable, dtype=\"<dtype>\")\n    assert \"&lt;name&gt;\" in formatted\n    assert \"&lt;dtype&gt;\" in formatted\n    assert \"&lt;x&gt;\" in formatted\n", "def test_short_data_repr_html_large_data():\n    large_data = np.random.rand(1000, 1000)\n    dataarray = xr.DataArray(large_data)\n    data_repr = fh.short_data_repr_html(dataarray)\n    assert data_repr.startswith(\"<pre>array\")\n    assert \"...\" in data_repr\n", "def test_summarize_variable_with_unsafe_name_and_preview():\n    name = \"<x>\"\n    var = xr.DataArray(np.random.rand(3, 4), dims=(\"dim1\", \"dim2\"))\n    dtype = \"float64\"\n    preview = \"<preview>\"\n    formatted = fh.summarize_variable(name, var, dtype=dtype, preview=preview)\n    assert \"&lt;x&gt;\" in formatted\n    assert \"dim1, dim2\" in formatted\n    assert \"float64\" in formatted\n    assert \"&lt;preview&gt;\" in formatted\n", "def test_array_section_with_large_preview(dataarray):\n    # Make a large array to test the preview truncation\n    large_array = xr.DataArray(np.random.RandomState(0).randn(100, 100))\n    formatted = fh.array_section(large_array)\n    # Check that the preview is truncated\n    assert \"...\" in formatted\n"], "sample_712": ["def test_ordinal_encoder_specified_categories():\n    X = [['abc', 2], ['def', 1]]\n    cats = [['abc', 'def', 'ghi'], [1, 2, 3]]\n    enc = OrdinalEncoder(categories=cats)\n    exp = np.array([[0, 1], [1, 0]], dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp.astype('float64'))\n    enc = OrdinalEncoder(categories=cats, dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp)\n", "def test_one_hot_encoder_unseen_categories():\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='ignore', categories=[['Male', 'Female'], [1, 2, 3]])\n    enc.fit(X)\n    X_test = [['Unknown', 1], ['Male', 4]]\n    X_tr = enc.transform(X_test)\n    exp = np.array([[0, 0, 1], [1, 0, 0]], dtype=np.float64)\n    assert_array_equal(X_tr.toarray(), exp)\n", "def test_ordinal_encoder_specified_categories():\n    X = np.array([['a', 'b'], ['b', 'a']], dtype=object)\n    enc = OrdinalEncoder(categories=[['b', 'a']])\n    exp = np.array([[1, 0], [0, 1]], dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp.astype('float64'))\n    assert enc.categories_[0].tolist() == ['b', 'a']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # when specifying categories manually, unknown categories should already\n    # raise when fitting\n    X2 = np.array([['a', 'c'], ['d', 'a']], dtype=object)\n    enc = OrdinalEncoder(categories=[['b', 'a'], ['b', 'a']])\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit(X2)\n", "def test_one_hot_encoder_handle_unknown(X, exp):\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    assert_array_equal(enc.inverse_transform(enc.transform(X)), np.array(exp, dtype=object))\n", "def test_one_hot_encoder_unspecified_categories():\n    X = np.array([['abc', 2, 55], ['def', 1, 55]])\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n    exp = np.array([[1., 0., 1., 0., 1.], [0., 1., 0., 1., 1.]])\n    assert_array_equal(X_tr.toarray(), exp)\n\n    X_test = np.array([['ghi', 3, 55], ['abc', 1, 56]])\n    X_test_tr = enc.transform(X_test)\n    exp_test = np.array([[0., 0., 0., 0., 1.], [1., 0., 1., 0., 0.]])\n    assert_array_equal(X_test_tr.toarray(), exp_test)\n"], "sample_108": ["def test_path_lookup_with_empty_parameter(self):\n    match = resolve('/articles/')\n    self.assertEqual(match.url_name, 'articles-empty')\n    self.assertEqual(match.args, ())\n    self.assertEqual(match.kwargs, {})\n    self.assertEqual(match.route, 'articles/')\n", "def test_converter_with_unexpected_parameters(self):\n    url = '/base64/aGVsbG8=/subpatterns/d29ybGQ=/unexpected/'\n    with self.assertRaises(Resolver404):\n        resolve(url)\n", "def test_path_reverse_with_parameter_missing(self):\n    with self.assertRaises(NoReverseMatch):\n        reverse('articles-year-month-day', kwargs={'year': 2015, 'day': 12})\n", "def test_path_reverse_with_parameter_and_app_name(self):\n    url = reverse('app_namespace:articles-year-month-day', kwargs={'year': 2015, 'month': 4, 'day': 12})\n    self.assertEqual(url, '/app_namespace/articles/2015/4/12/')\n", "def test_path_lookup_with_multiple_optional_parameters(self):\n    match = resolve('/optional/2015/04/')\n    self.assertEqual(match.url_name, 'articles-year-month-opt-day')\n    self.assertEqual(match.args, ())\n    self.assertEqual(match.kwargs, {'year': 2015, 'month': 4})\n    self.assertEqual(match.route, 'optional/<int:year>/<int:month>(/<int:day>)?/')\n"], "sample_531": ["def test_pickle_with_device_pixel_ratio():\n    fig = Figure(dpi=42)\n    fig.canvas._set_device_pixel_ratio(7)\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert fig2.dpi == 42*7\n", "def test_subplot_mosaic_with_gridspec_kw(fig_test, fig_ref):\n    fig_ref.subplot_mosaic('AB', width_ratios=[1, 2], gridspec_kw={'left': 0.1})\n    fig_test.subplot_mosaic('AB', width_ratios=[1, 2], gridspec_kw={'left': 0.1})\n", "def test_deepcopy_with_constraints():\n    fig1, ax = plt.subplots(constrained_layout=True)\n    ax.plot([0, 1], [2, 3])\n    ax.set_yscale('log')\n\n    fig2 = copy.deepcopy(fig1)\n\n    # Make sure it is a new object\n    assert fig2.axes[0] is not ax\n    # And that the constraint layout got propagated\n    assert fig2.get_constrained_layout() is True\n    # And that the axis scale got propagated\n    assert fig2.axes[0].get_yscale() == 'log'\n\n    # Update the deepcopy and check the original isn't modified\n    fig2.axes[0].set_yscale('linear')\n    assert ax.get_yscale() == 'log'\n", "def test_tightbbox_box_aspect_multi():\n    fig, axs = plt.subplots(2, 2)\n    axs[0, 0].set_box_aspect(.5)\n    axs[1, 0].set_box_aspect((2, 1, 1))\n    axs[0, 1].set_box_aspect((1, 2, 1))\n    axs[1, 1].set_box_aspect((1, 1, 2))\n", "def test_constrained_layout_with_subfigures(fig_test, fig_ref):\n    fig_test.set_constrained_layout(True)\n    subfigs = fig_test.subfigures(2, 2)\n    axs = subfigs.flat\n    for ax in axs:\n        ax.plot([0, 1], [0, 1])\n\n    fig_ref.set_constrained_layout(True)\n    subfigs = fig_ref.subfigures(2, 2)\n    axs = subfigs.flat\n    for ax in axs:\n        ax.plot([0, 1], [0, 1])\n"], "sample_928": ["def test_escape_dot():\n    assert escape('A.B') == r'A\\.B'\n    assert escape('.A') == r'\\.A'\n", "def test_default_role(app):\n    with default_role('dummy.rst', 'ref'):\n        from docutils.parsers.rst import roles\n        assert roles._roles[''].name == 'ref'\n\n    # check the role is unregistered\n    with default_role('dummy.rst', 'unknown_role'):\n        pass\n    assert 'unknown_role' not in roles._roles\n", "def test_default_role(app):\n    docname = 'dummy'\n    name = 'ref'\n    with default_role(docname, name):\n        # Here you can add some assertions to check if the role is registered correctly.\n        # For example, you can check if the role is in the roles dictionary of the docutils.\n        pass\n", "def test_default_role(app):\n    from sphinx.util import docutils\n\n    with default_role('dummy.rst', 'role'):\n        role_fn, _ = docutils.get_role_info('', 'role')\n        assert role_fn is not None\n\n    role_fn, _ = docutils.get_role_info('', 'role')\n    assert role_fn is None\n", "def test_default_role(app, monkeypatch):\n    from sphinx.util import docutils\n\n        assert name == ''\n        assert callable(role_fn)\n\n        assert name == ''\n\n    monkeypatch.setattr(docutils, 'register_role', mock_register_role)\n    monkeypatch.setattr(docutils, 'unregister_role', mock_unregister_role)\n\n    from sphinx.util.rst import default_role\n\n    with default_role('dummy', 'existing_role'):\n        pass\n\n    with default_role('dummy', 'nonexistent_role'):\n        pass\n"], "sample_590": ["def test_concat_coords_not_in_all_datasets(self):\n    # coordinates present in some datasets but not others\n    ds1 = Dataset(data_vars={\"a\": (\"y\", [0.1])}, coords={\"x\": 0.1})\n    ds2 = Dataset(data_vars={\"a\": (\"y\", [0.2])})\n    with raises_regex(ValueError, \"coordinates in some datasets but not others\"):\n        concat([ds1, ds2], dim=\"y\", coords=\"minimal\")\n", "    def test_concat_different_data_vars(self):\n        ds1 = Dataset({\"a\": (\"x\", [0.1])})\n        ds2 = Dataset({\"b\": (\"x\", [0.2])})\n        actual = concat([ds1, ds2], dim=\"x\", data_vars=\"different\")\n        expected = Dataset({\"a\": (\"x\", [0.1, np.nan]), \"b\": (\"x\", [np.nan, 0.2])}, coords={\"x\": [0, 1]})\n        assert_identical(expected, actual)\n\n        ds1 = Dataset({\"a\": (\"x\", [0.1])})\n        ds2 = Dataset({\"a\": (\"x\", [0.2])})\n        actual = concat([ds1, ds2], dim=\"x\", data_vars=\"different\")\n        expected = Dataset({\"a\": (\"x\", [0.1, 0.2])})\n        assert_identical(expected, actual)\n\n        ds1 = Dataset({\"a\": (\"x\", [0.1]), \"b\": (\"x\", [0.2])})\n        ds2 = Dataset({\"b\": (\"x\", [0.3])})\n        actual = concat([ds1, ds2], dim=\"x\", data_vars=\"different\")\n        expected = Dataset({\"a\": (\"x\", [0.1, np.nan]), \"b\": (\"x\", [0.2, 0.3])})\n        assert_identical(expected, actual)\n", "def test_concat_no_conflicts(self):\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]]), \"b\": 1}, coords={\"x\": [0, 1], \"y\": [0, 1]})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[5, np.nan], [7, 8]]), \"b\": 1}, coords={\"x\": [0, 1], \"y\": [0, 1]})\n    expected = Dataset({\"a\": ((\"x\", \"y\"), [[5, 2], [7, 8]]), \"b\": 1}, coords={\"x\": [0, 1], \"y\": [0, 1]})\n    actual = concat([ds1, ds2], dim=\"z\", compat=\"no_conflicts\")\n    assert_identical(expected, actual)\n", "def test_concat_dim_coord_precedence(self, data):\n    # verify that the dimension coordinate takes precedence over\n    # the dim argument when they are both provided\n    dim = \"dim1\"\n    coord = (2 * data[dim]).rename(dim)\n    datasets = [g for _, g in data.groupby(dim, squeeze=False)]\n    expected = data.copy()\n    expected[dim] = coord\n    assert_identical(expected, concat(datasets, coord))\n", "def test_concat_data_vars_and_coords(self):\n    ds1 = Dataset({\"foo\": ((\"x\", \"y\"), np.random.random((2, 3)))}, coords={\"x\": [0, 1], \"z\": 1})\n    ds2 = Dataset({\"foo\": ((\"x\", \"y\"), np.random.random((2, 3)))}, coords={\"x\": [0, 1], \"z\": 2})\n\n    with raises_regex(ValueError, \"some variables in data_vars are not data variables\"):\n        concat([ds1, ds2], dim=\"x\", data_vars=[\"foo\", \"bar\"])\n\n    with raises_regex(ValueError, \"some variables in coords are not coordinates\"):\n        concat([ds1, ds2], dim=\"x\", coords=[\"x\", \"w\"])\n"], "sample_550": ["def test_toolbar_save_figure():\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.save_figure()\n    # Add assertions here to verify the expected behavior, e.g.:\n    # assert os.path.exists('image.png')\n", "def test_get_tightbbox_for_layout_only():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3])\n    ax.set_xlabel(\"x label\")\n    ax.set_ylabel(\"y label\")\n    ax.set_title(\"title\")\n\n    bbox = ax.get_tightbbox(for_layout_only=True)\n    bbox_layout_only = ax.get_tightbbox(for_layout_only=True)\n\n    # The y-extent of the bbox_layout_only should not include the y-extent of the ylabel.\n    assert bbox.height > bbox_layout_only.height\n    # The x-extent of the bbox_layout_only should not include the x-extent of the title or xlabel.\n    assert bbox.width > bbox_layout_only.width\n", "def test_pan_with_scroll_event():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10))\n    assert ax.get_navigate()\n\n    # Simulate scroll events\n    scroll_event_up = MouseEvent(\"scroll_event\", fig.canvas, 0, 0, button=MouseButton.UP)\n    scroll_event_down = MouseEvent(\"scroll_event\", fig.canvas, 0, 0, button=MouseButton.DOWN)\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.pan()\n\n    # Scroll up\n    tb.scroll(scroll_event_up)\n    expected_xlim_up = (ax.get_xlim()[0] + 0.1 * (ax.get_xlim()[1] - ax.get_xlim()[0]),\n                        ax.get_xlim()[1] - 0.1 * (ax.get_xlim()[1] - ax.get_xlim()[0]))\n    expected_ylim_up = (ax.get_ylim()[0] + 0.1 * (ax.get_ylim()[1] - ax.get_ylim()[0]),\n                        ax.get_ylim()[1] - 0.1 * (ax.get_ylim()[1] - ax.get_ylim()[0]))\n    assert tuple(ax.get_xlim()) == pytest.approx(expected_xlim_up, abs=0.02)\n    assert tuple(ax.get_ylim()) == pytest.approx(expected_ylim_up, abs=0.02)\n\n    # Scroll down\n    tb.scroll(scroll_event_down)\n    expected_xlim_down = (ax.get_xlim()[0] - 0.1 * (expected_xlim_up[1] - expected_xlim_up[0]),\n                          ax.get_xlim()[1] + 0.1 * (expected_xlim_up[1] - expected_xlim_up[0]))\n    expected_ylim_down = (ax.get_ylim()[0]", "def test_interactive_zoom_mode_twinx_twiny(mode, twinx, twiny, expected):\n    fig, ax = plt.subplots()\n    if twinx:\n        ax2 = ax.twinx()\n        ax2.set_ylim(0, 1)\n    if twiny:\n        ax3 = ax.twiny()\n        ax3.set_xlim(0, 1)\n    ax.set_xlim(1, 2)\n    ax.set_ylim(3, 4)\n\n    # Mouse from (1.1, 3.1) to (1.9, 3.9) (data coordinates, \"d\").\n    d0 = (1.1, 3.1)\n    d1 = (1.9, 3.9)\n    # Convert to screen coordinates (\"s\").  Events are defined only with pixel\n    # precision, so round the pixel values, and below, check against the\n    # corresponding xdata/ydata, which are close but not equal to d0/d1.\n    s0 = ax.transData.transform(d0).astype(int)\n    s1 = ax.transData.transform(d1).astype(int)\n\n    # Zoom in.\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, *s0, MouseButton.LEFT)\n    fig.canvas.callbacks.process(start_event.name,", "def test_toolbar_back_forward():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n\n    xlim0, ylim0 = ax.get_xlim(), ax.get_ylim()\n    ax.set_xlim(0.5, 1)\n    ax.set_ylim(0.5, 1)\n\n    tb._nav_stack.back()\n    assert ax.get_xlim() == xlim0\n    assert ax.get_ylim() == ylim0\n\n    tb._nav_stack.forward()\n    assert ax.get_xlim() == (0.5, 1)\n    assert ax.get_ylim() == (0.5, 1)\n"], "sample_1151": ["def test_issue_21034_expand():\n    e = -I*log((re(asin(5)) + I*im(asin(5)))/sqrt(re(asin(5))**2 + im(asin(5))**2))/pi\n    expanded_e = e.expand(complex=True)\n    assert expanded_e.round(2)\n", "def test_issue_21034_complex_log():\n    e = -I*log((re(asin(5)) + I*im(asin(5)))/sqrt(re(asin(5))**2 + im(asin(5))**2))/pi\n    assert e.round(2) == complex_round(e, 2)\n", "def test_issue_21034_expanded():\n    e = -I*log((re(asin(5)) + I*im(asin(5)))/sqrt(re(asin(5))**2 + im(asin(5))**2))/pi\n    expanded_e = e.expand(complex=True)\n    assert expanded_e.round(2)\n", "def test_issue_21034_rounding():\n    e = -I*log((re(asin(5)) + I*im(asin(5)))/sqrt(re(asin(5))**2 + im(asin(5))**2))/pi\n    assert e.round(2) == -0.2j  # Check that the expression is correctly rounded to 2 decimal places\n", "def test_issue_21373():\n    x_r, y_r = symbols('x_r y_r', real=True)\n    expr = exp(sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) / z))\n    assert expr.subs({1: 1.0})\n    assert sinh(Piecewise((x_r, y_r > x_r), (y_r, True)) * z ** -1.0).is_zero\n"], "sample_1099": ["def test_eval_partial_derivative_single_higher_rank_tensors_by_tensor():\n\n    expr1 = PartialDerivative(H(i, j, k), H(m, m1, m2))\n    assert expr1._perform_derivative() - L.delta(i, -m) * L.delta(j, -m1) * L.delta(k, -m2) == 0\n\n    expr2 = PartialDerivative(H(i, j, k), H(-m, m1, m2))\n    assert expr2._perform_derivative() - L.metric(i, L_0) * L.delta(-L_0, m) * L.delta(j, -m1) * L.delta(k, -m2) == 0\n\n    expr3 = PartialDerivative(H(i, j, k), H(m, -m1, m2))\n    assert expr3._perform_derivative() - L.delta(i, -m) * L.metric(j, L_0) * L.delta(-L_0, m1) * L.delta(k, -m2) == 0\n\n    expr4 = PartialDerivative(H(i, j, k), H(m, m1, -m2))\n    assert expr4._perform_derivative() - L.delta(i, -m) * L.delta(j, -m1) * L.metric(k, L_0) * L.delta(-L_0, m2) == 0\n\n    expr5 = PartialDerivative(H(i, j, k), H(-m, -m1, m2))\n    assert expr5._perform_derivative() - L.metric(i, L_0) * L.delta(-L_0, m) * L.metric(j, L_1) * L.delta(-L_1, m1) * L.delta(k, -m2) == 0\n\n    expr6 = PartialDerivative(H(i, j, k), H(-m, m1, -m2))\n    assert expr6._perform_deriv", "def test_eval_partial_derivative_higher_order_tensor_derivative():\n    expr = PartialDerivative(H(i, j, k), H(m, n, p))\n    assert expr._perform_derivative() - L.delta(i, -m) * L.delta(j, -n) * L.delta(k, -p) == 0\n\n    expr = PartialDerivative(H(i, j, k), H(-m, n, p))\n    assert expr._perform_derivative() - L.metric(i, L_0) * L.delta(-L_0, m) * L.delta(j, -n) * L.delta(k, -p) == 0\n\n    expr = PartialDerivative(H(i, j, k), H(m, -n, p))\n    assert expr._perform_derivative() - L.delta(i, -m) * L.metric(j, L_0) * L.delta(-L_0, n) * L.delta(k, -p) == 0\n\n    expr = PartialDerivative(H(i, j, k), H(m, n, -p))\n    assert expr._perform_derivative() - L.delta(i, -m) * L.delta(j, -n) * L.metric(k, L_0) * L.delta(-L_0, p) == 0\n\n    expr = PartialDerivative(H(i, j, k), H(-m, -n, p))\n    assert expr._perform_derivative() - L.metric(i, L_0) * L.delta(-L_0, m) * L.metric(j, L_1) * L.delta(-L_1, n) * L.delta(k, -p) == 0\n\n    expr = PartialDerivative(H(i, j, k), H(-m, n, -p))\n    assert expr._perform_derivative() - L.metric(i, L_0) * L.delta(-L_0, m) * L.delta(j, -n) * L.metric", "def test_eval_partial_derivative_mixed_vector_tensor_expr3():\n\n    tau, alpha = symbols(\"tau alpha\")\n\n    base_expr3 = A(i)*B(j) + tau**3*C(k)\n\n    vector_expression = PartialDerivative(base_expr3, A(m))._perform_derivative()\n    assert (vector_expression - (L.delta(j, -m)*C(k) + tau**3*L.delta(k, -m))).expand() == 0\n\n    tensor_expression = PartialDerivative(base_expr3, B(n))._perform_derivative()\n    assert (tensor_expression - (L.delta(i, -n)*C(k) + tau**3*L.delta(k, -n))).expand() == 0\n\n    scalar_expression = PartialDerivative(base_expr3, tau)._perform_derivative()\n    assert scalar_expression == 3*tau**2*C(k)\n", "def test_eval_partial_derivative_mixed_tensor_expr3():\n\n    base_expr3 = H(i, j) * A(-j) + B(i) * C(j)\n\n    tensor_expression = PartialDerivative(base_expr3, H(k, m))._perform_derivative()\n    assert (tensor_expression - (A(-m) * L.delta(i, -k) + H(i, -m) * A(-k))).expand() == 0\n\n    vector_expression = PartialDerivative(base_expr3, A(k))._perform_derivative()\n    assert (vector_expression - (H(i, -k) + B(L_0) * L.metric(-L_0, -L_1) * L.delta(L_1, -k) * C(i))).expand() == 0\n", "def test_eval_partial_derivative_mixed_tensor_vector_expr3():\n\n    base_expr3 = H(i, j)*A(-i)*B(j) + A(i)*A(-i)\n\n    tensor_expression = PartialDerivative(base_expr3, H(k, m))._perform_derivative()\n    assert (tensor_expression - (A(L_0)*B(L_1)*L.delta(L_0, -k)*L.delta(L_1, -m) +\n        H(L_0, L_1)*L.delta(L_0, -k)*A(-L_1)*L.metric(-L_1, -m) +\n        H(L_0, L_1)*A(-L_0)*L.delta(L_1, -m))).expand() == 0\n\n    vector_expression = PartialDerivative(base_expr3, A(k))._perform_derivative()\n    assert (vector_expression - (B(L_0)*H(L_0, -k) +\n        2*A(L_0)*L.delta(L_0, -k) +\n        H(L_0, L_1)*B(L_1)*L.metric(-L_0, -k) +\n        H(L_0, -k)*A(-L_0))).expand() == 0\n"], "sample_863": ["def test_pipeline_score_samples_without_score_samples():\n    X = np.array([[1, 2]])\n    y = np.array([1, 2])\n    # Test that a pipeline does not have score_samples method when the final\n    # step of the pipeline does not have score_samples defined.\n    pipe = make_pipeline(LinearRegression())\n    pipe.fit(X, y)\n    with pytest.raises(AttributeError,\n                       match=\"'LinearRegression' object has no attribute \"\n                             \"'score_samples'\"):\n        pipe.score_samples(X)\n", "def test_pipeline_fit_predict_with_final_estimator_fit_params():\n    # tests that Pipeline passes fit_params to the final estimator\n    # when fit_predict is invoked\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    pipe.fit_predict(X=None, y=None, clf__should_succeed=True)\n    assert pipe.named_steps['clf'].successful\n    assert not hasattr(pipe.named_steps['transf'], 'fit_params')\n", "def test_feature_union_empty():\n    # test feature union with no transformers\n    X = iris.data\n    y = iris.target\n    ft = FeatureUnion([])\n    ft.fit(X, y)\n    X_transformed = ft.transform(X)\n    assert X_transformed.shape == (X.shape[0], 0)\n", "def test_pipeline_with_fit_transform_only_transformer():\n    # Test the case where the pipeline ends with a transformer that does not implement fit.\n    X = iris.data\n    transf = Transf()\n    pipeline = Pipeline([('mock', transf)])\n\n    # test fit_transform:\n    X_trans = pipeline.fit_transform(X)\n    X_trans2 = transf.transform(X)\n    assert_array_almost_equal(X_trans, X_trans2)\n\n    # Check that pipeline does not have fit_predict method\n    assert_raises(AttributeError, getattr, pipeline, 'fit_predict')\n\n    # Check that pipeline does not have score method\n    assert_raises(AttributeError, getattr, pipeline, 'score')\n", "def test_pipeline_get_feature_names():\n    # Test that a pipeline with a feature union and a transformer\n    # has the correct feature names\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='randomized', random_state=0)\n    select = SelectKBest(k=1)\n    ft = FeatureUnion([(\"pca\", pca), (\"select\", select)])\n    pipeline = Pipeline([('ft', ft)])\n    pipeline.fit(X, y)\n    feature_names = pipeline.named_steps['ft'].get_feature_names()\n    expected_feature_names = ['ft__pca__0', 'ft__pca__1', 'ft__select__0']\n    assert feature_names == expected_feature_names\n"], "sample_206": ["def test_file_descriptor_get(self):\n    \"\"\"\n    Test the __get__ method of FileDescriptor.\n    \"\"\"\n    d = Document(myfile='something.txt')\n    field = d._meta.get_field('myfile')\n    descriptor = field.descriptor_class(field)\n    file = descriptor.__get__(d)\n    self.assertIsInstance(file, field.attr_class)\n    self.assertEqual(file.name, 'something.txt')\n", "def test_file_url(self):\n    \"\"\"\n    FileField.url returns the correct URL for the uploaded file.\n    \"\"\"\n    with open(__file__, 'rb') as fp:\n        file1 = File(fp, name='test_file.py')\n        document = Document(myfile='test_file.py')\n        document.myfile.save('test_file.py', file1)\n        try:\n            expected_url = settings.MEDIA_URL + 'unused/test_file.py'\n            self.assertEqual(document.myfile.url, expected_url)\n        finally:\n            document.myfile.delete()\n", "    def test_file_equality(self):\n        \"\"\"\n        FileField should be equal to another FileField with the same name and\n        not equal to a FileField with a different name.\n        \"\"\"\n        d1 = Document(myfile='something.txt')\n        d2 = Document(myfile='something.txt')\n        d3 = Document(myfile='something_else.txt')\n        self.assertEqual(d1.myfile, d2.myfile)\n        self.assertNotEqual(d1.myfile, d3.myfile)\n", "def test_save_form_data_none(self):\n    \"\"\"\n    FileField.save_form_data() sets an empty string when passed None.\n    \"\"\"\n    d = Document(myfile='something.txt')\n    self.assertEqual(d.myfile, 'something.txt')\n    field = d._meta.get_field('myfile')\n    field.save_form_data(d, None)\n    self.assertEqual(d.myfile, '')\n", "def test_save_form_data_with_file_object(self):\n    \"\"\"\n    FileField.save_form_data() can accept a File object.\n    \"\"\"\n    d = Document()\n    field = d._meta.get_field('myfile')\n    file_object = File(open(__file__, 'rb'))\n    field.save_form_data(d, file_object)\n    self.assertIsInstance(d.myfile, File)\n    file_object.close()\n"], "sample_532": ["def test_contour_linestyles_iterable():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    linestyles = ['solid', 'dashed', 'dashdot', 'dotted']\n\n    # Change linestyles using iterable linestyles kwarg\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z, 6, colors='k', linestyles=linestyles)\n    ax.clabel(CS, fontsize=9, inline=True)\n    ax.set_title('Single color - positive contours with iterable linestyles')\n    assert CS.linestyles == linestyles\n", "def test_negative_linestyles_image():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z, 6, colors='k', negative_linestyles='dashdot')\n    ax.clabel(CS, fontsize=9, inline=True)\n    ax.set_title('Single color - negative contours dashdot')\n", "def test_contour_linestyles_iterable():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    # Test with linestyles iterable\n    styles = ['solid', 'dashed', 'dashdot', 'dotted']\n    fig4, ax4 = plt.subplots()\n    CS4 = ax4.contour(X, Y, Z, 6, colors='k', linestyles=styles)\n    ax4.clabel(CS4, fontsize=9, inline=True)\n    ax4.set_title('Single color - linestyles iterable')\n    assert CS4.linestyles == styles\n\n    # Test with negative_linestyles iterable\n    fig5, ax5 = plt.subplots()\n    CS5 = ax5.contour(X, Y, Z, 6, colors='k', negative_linestyles=styles)\n    ax5.clabel(CS5, fontsize=9, inline=True)\n    ax5.set_title('Single color - negative_linestyles iterable')\n    assert CS5.negative_linestyles == styles\n", "def test_contour_transform():\n    x = np.linspace(0, 1, 10)\n    y = np.linspace(0, 1, 10)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z, transform=ax.transAxes)\n    assert_array_almost_equal(cs.collections[0].get_transform().transform([0.5, 0.5]), [0.5, 0.5])\n", "def test_contour_transform(algorithm):\n    fig, ax = plt.subplots()\n    x = np.arange(0, 10, 1)\n    y = np.arange(0, 20, 2)\n    X, Y = np.meshgrid(x, y)\n    Z = np.cos(X) * np.sin(Y)\n\n    # Create a transform\n    transform = ax.transAxes + ax.transData.inverted()\n\n    # Call contour with the transform\n    cs = ax.contour(X, Y, Z, transform=transform, algorithm=algorithm)\n\n    # Assert that the transform was applied correctly\n    assert cs.get_transform() == transform\n\n    # Optional: Add additional assertions to validate the output\n"], "sample_566": ["def test_set_constrained_layout_pads():\n    fig = plt.figure(constrained_layout=True)\n    with pytest.warns(PendingDeprecationWarning, match=\"will be deprecated\"):\n        fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, wspace=0.03, hspace=0.04)\n    with pytest.warns(PendingDeprecationWarning, match=\"will be deprecated\"):\n        assert fig.get_constrained_layout_pads() == (0.01, 0.02, 0.03, 0.04)\n", "def test_get_constrained_layout_pads_relative():\n    params = {'w_pad': 10, 'h_pad': 20, 'wspace': 0.03, 'hspace': 0.04}\n    fig = plt.figure(layout=mpl.layout_engine.ConstrainedLayoutEngine(**params))\n    fig.dpi = 100\n    fig.set_size_inches(6, 4)\n    expected = (1.0, 2.0, 0.03, 0.04)\n    with pytest.warns(PendingDeprecationWarning, match=\"will be deprecated\"):\n        assert fig.get_constrained_layout_pads(relative=True) == expected\n", "def test_add_subplot_invalid_kwargs():\n    fig = plt.figure()\n    with pytest.raises(ValueError, match=\"'projection' argument must be either 'rectilinear' or 'polar'\"):\n        fig.add_subplot(1, 1, 1, projection='invalid')\n\n    with pytest.raises(ValueError, match=\"'polar' argument must be either True or False\"):\n        fig.add_subplot(1, 1, 1, polar='invalid')\n", "def test_add_axes_zorder():\n    # check that adding a new axes changes zorder\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    assert ax1.get_zorder() == 0\n    ax2 = fig.add_subplot(111)\n    assert ax2.get_zorder() > ax1.get_zorder()\n", "def test_savefig_metadata_non_default(fmt):\n    fig = Figure()\n    metadata = {'Title': 'Test Figure', 'Author': 'Test Author'}\n    with io.BytesIO() as buf:\n        fig.savefig(buf, format=fmt, metadata=metadata)\n        buf.seek(0)\n        if fmt in ['png', 'jpg', 'jpeg', 'tiff', 'tif', 'webp']:\n            image = Image.open(buf)\n            assert image.info['Title'] == metadata['Title']\n            assert image.info['Author'] == metadata['Author']\n        elif fmt in ['eps', 'pdf', 'ps']:\n            pdf = PyPDF2.PdfReader(buf)\n            assert pdf.metadata.title == metadata['Title']\n            assert pdf.metadata.author == metadata['Author']\n        elif fmt in ['svg', 'svgz']:\n            svg = BeautifulSoup(buf.getvalue(), 'xml')\n            assert svg.find('title').text == metadata['Title']\n            assert svg.find('desc').text == f\"Author: {metadata['Author']}\"\n"], "sample_990": ["def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True).expand() == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - 3*tanh(x)**3) / (1 - 3*tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - 3*tanh(x)**3) / (1 - 3*tanh(x)**2 + tanh(x)**4)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y))/(1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True).expand() == 2*tanh(x)/(1 + tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - tanh(x)**3) / (1 - 3*tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == \\\n        (3*tanh(x) - 3*tanh(x)**3) / (1 + 3*tanh(x)**2 - tanh(x)**4)\n"], "sample_831": ["def test_graphviz_class_names_bool(pyplot):\n    # Check correctness of export_graphviz with class_names=True\n    clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    contents1 = export_graphviz(clf, class_names=True, out_file=None)\n    contents2 = 'digraph Tree {\\n' \\\n                'node [shape=box] ;\\n' \\\n                '0 [label=\"X[0] <= 0.0\\\\ngini = 0.5\\\\nsamples = 6\\\\n' \\\n                'value = [3, 3]\\\\nclass = y[0]\"] ;\\n' \\\n                '1 [label=\"gini = 0.0\\\\nsamples = 3\\\\nvalue = [3, 0]\\\\n' \\\n                'class = y[0]\"] ;\\n' \\\n                '0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\\n' \\\n                '2 [label=\"gini = 0.0\\\\nsamples = 3\\\\nvalue = [0, 3]\\\\n' \\\n                'class = y[1]\"] ;\\n' \\\n                '0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\\n' \\\n                '}'\n\n    assert_equal(contents1, contents2)\n", "def test_plot_tree_regressor(pyplot):\n    # Check correctness of plot_tree for DecisionTreeRegressor\n    reg = DecisionTreeRegressor(max_depth=3,\n                                min_samples_split=2,\n                                random_state=2)\n    reg.fit(X, y)\n\n    # Test export code\n    feature_names = ['feature_1', 'feature_2']\n    nodes = plot_tree(reg, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"feature_1 <= 0.0\\nmse = 1.0\\nsamples = 6\\nvalue = 0.0\")\n    assert nodes[1].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = -1.0\"\n    assert nodes[2].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = 1.0\"\n", "def test_plot_tree_multi_output(pyplot):\n    # Test for multi-output decision tree\n    clf = DecisionTreeClassifier(max_depth=2,\n                                 min_samples_split=2,\n                                 random_state=2)\n    clf.fit(X, y2)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 5\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\nsamples = 6\\n\"\n                                   \"value = [[3.0, 1.5, 0.0]\\n\"\n                                   \" [3.0, 1.0, 0.5]]\")\n    assert nodes[1].get_text() == \"samples = 3\\nvalue = [[3, 0, 0]\\n\"\n                                  \" [3, 0, 0]]\"\n    assert nodes[2].get_text() == (\"sepal_width <= 1.5\\nsamples = 3\\n\"\n                                   \"value = [[0.0, 1.5, 0.0]\\n\"\n                                   \" [0.0, 1.0, 0.5]]\")\n    assert nodes[3].get_text() == \"samples = 2\\nvalue = [[0, 1, 0]\\n\"\n                                  \" [0, 1, 0]]\"\n    assert nodes[4].get_text() == \"samples = 1\\nvalue = [[0.0, 0.5, 0.0]\\n\"\n                                  \" [0.0, 0.0, 0.5]]\"\n", "def test_plot_tree_options(pyplot):\n    # Test plot_tree with different options\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test with class_names\n    feature_names = ['first feat', 'sepal_width']\n    class_names = ['negative', 'positive']\n    nodes = plot_tree(clf, feature_names=feature_names, class_names=class_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"\n                                   \"samples = 6\\nvalue = [3, 3]\\nclass = positive\")\n\n    # Test with filled=True and impurity=False\n    nodes = plot_tree(clf, feature_names=feature_names, filled=True, impurity=False)\n    assert len(nodes) == 3\n    assert \"fillcolor\" in nodes[0].get_text()\n\n    # Test with proportion=True\n    nodes = plot_tree(clf, feature_names=feature_names, proportion=True)\n    assert len(nodes) == 3\n    assert \"samples = 100.0%\" in nodes[0].get_text()\n\n    # Test with rotate=True\n    nodes = plot_tree(clf, feature_names=feature_names, rotate=True)\n    assert len(nodes) == 3\n\n    # Test with rounded=True\n    nodes = plot_tree(clf, feature_names=feature_names, rounded=True)\n    assert len(nodes) == 3\n\n    # Test with max_depth=0\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=0)\n    assert len(nodes) == 1\n    assert nodes[0].get_text() == \"(...)\"\n", "def test_plot_tree_multiclass(pyplot):\n    # Test plot_tree with multiclass classification\n    X = [[0], [1], [2], [3], [4], [5]]\n    y = [0, 1, 2, 0, 1, 2]\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n    nodes = plot_tree(clf)\n    assert len(nodes) == 7\n    assert nodes[0].get_text() == (\"X[0] <= 2.5\\ngini = 0.6667\\nsamples = 6\\nvalue = [2, 2, 2]\")\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 2\\nvalue = [2, 0, 0]\"\n    assert nodes[2].get_text() == \"gini = 1.0\\nsamples = 4\\nvalue = [2, 2, 0]\"\n    assert nodes[3].get_text() == \"gini = 0.0\\nsamples = 1\\nvalue = [1, 0, 0]\"\n    assert nodes[4].get_text() == \"gini = 0.5\\nsamples = 3\\nvalue = [0, 2, 1]\"\n    assert nodes[5].get_text() == \"gini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]\"\n    assert nodes[6].get_text() == \"gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]\"\n"], "sample_8": ["def test_masked_array_from_numpy_ma(self):\n    \"\"\"Check that we can initialize a MaskedArray from a numpy.ma.MaskedArray.\"\"\"\n    np_ma = np.ma.MaskedArray(self.a, mask=self.mask_a)\n    ma = Masked(np_ma)\n    assert type(ma) is Masked\n    assert type(ma.unmasked) is self._data_cls\n    assert type(ma.mask) is np.ndarray\n    assert_array_equal(ma.unmasked, self.a)\n    assert_array_equal(ma.mask, self.mask_a)\n", "def test_masked_array_from_numpy_ma(self):\n    \"\"\"Check that numpy.ma.MaskedArray can initialize a MaskedArray properly.\"\"\"\n    ma = Masked(np.ma.MaskedArray(self.a, mask=self.mask_a))\n    assert type(ma) is Masked\n    assert type(ma.unmasked) is self._data_cls\n    assert type(ma.mask) is np.ndarray\n    assert_array_equal(ma.unmasked, self.a)\n    assert_array_equal(ma.mask, self.mask_a)\n", "def test_masked_array_initialization_with_numpy_ma(self):\n    np_ma = np.ma.MaskedArray(self.a, mask=self.mask_a)\n    ma = Masked(np_ma)\n    assert type(ma) is Masked\n    assert type(ma.unmasked) is self._data_cls\n    assert type(ma.mask) is np.ndarray\n    assert_array_equal(ma.unmasked, self.a)\n    assert_array_equal(ma.mask, self.mask_a)\n", "def test_masked_array_to_masked_array(self):\n    \"\"\"Test that we can convert to a MaskedArray properly.\"\"\"\n    np_ma = self.ma.to_masked_array()\n    assert type(np_ma) is np.ma.MaskedArray\n    assert type(np_ma.data) is self._data_cls\n    assert type(np_ma.mask) is np.ndarray\n    assert_array_equal(np_ma.data, self.a)\n    assert_array_equal(np_ma.mask, self.mask_a)\n", "def test_masked_array_fill_value(self):\n    \"\"\"Check that fill_value is handled correctly.\"\"\"\n    np_ma = np.ma.MaskedArray(self.ma, fill_value=0.0)\n    assert type(np_ma) is np.ma.MaskedArray\n    assert type(np_ma.data) is self._data_cls\n    assert type(np_ma.mask) is np.ndarray\n    expected_data = self.a.copy()\n    expected_data[self.mask_a] = 0.0\n    assert_array_equal(np_ma.data, expected_data)\n    assert_array_equal(np_ma.mask, self.mask_a)\n"], "sample_914": ["def test_unparse_dict_variable_value(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a: int = 1, b: str, *args, c: float = 3.0, d, **kwargs): pass\"\n    expected = \"a: int = 1, b: str, *args, c: float = 3.0, d, **kwargs\"\n    module = ast.parse(source)\n    assert ast.unparse_arguments(module.body[0].args) == expected\n", "def test_unparse_index_subscript_syntax(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_annotation():\n    source = \"def func(a: int, b: str = 'default'): pass\"\n    expected = \"def func(a: int, b: str = 'default'): ...\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_annotations():\n    source = \"def func(a: int, b: str = 'default', c: List[int] = None) -> bool:\"\n    expected = \"def func(a: int, b: str = 'default', c: List[int] = None) -> bool:\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n"], "sample_161": ["def test_foreign_object_to_fields_exist(self):\n    class Parent(models.Model):\n        pass\n\n    class Child(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('a', 'b'),\n            to_fields=('x', 'y'),\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(), [\n        Error(\n            \"The to_field 'x' doesn't exist on the related model 'invalid_models_tests.Parent'.\",\n            obj=field,\n            id='fields.E312',\n        ),\n        Error(\n            \"The to_field 'y' doesn't exist on the related model 'invalid_models_tests.Parent'.\",\n            obj=field,\n            id='fields.E312',\n        ),\n    ])\n", "def test_foreign_object_to_non_unique_subset_of_fields(self):\n    class Person(models.Model):\n        country_id = models.IntegerField()\n        city_id = models.IntegerField()\n\n        class Meta:\n            unique_together = (('country_id', 'city_id'),)\n\n    class MMembership(models.Model):\n        person_country_id = models.IntegerField()\n        person_city_id = models.IntegerField()\n        person = models.ForeignObject(\n            Person,\n            on_delete=models.CASCADE,\n            from_fields=['person_country_id'],\n            to_fields=['country_id'],\n        )\n\n    field = MMembership._meta.get_field('person')\n    self.assertEqual(field.check(), [\n        Error(\n            \"No subset of the fields 'country_id' on model 'Person' is unique.\",\n            hint=(\n                'Mark a single field as unique=True or add a set of '\n                'fields to a unique constraint (via unique_together or a '\n                'UniqueConstraint (without condition) in the model '\n                'Meta.constraints).'\n            ),\n            obj=field,\n            id='fields.E310',\n        ),\n    ])\n", "def test_m2m_through_foreign_key_to_non_unique_field(self):\n    class Target(models.Model):\n        bad = models.IntegerField()  # No unique=True\n\n    class Intermediate(models.Model):\n        model = models.ForeignKey('Model', models.CASCADE)\n        target = models.ForeignKey('Target', models.CASCADE, to_field='bad')\n\n    class Model(models.Model):\n        m2m = models.ManyToManyField('Target', through='Intermediate')\n\n    field = Model._meta.get_field('m2m')\n    self.assertEqual(field.check(from_model=Model), [\n        Error(\n            \"'Target.bad' must be unique because it is referenced by a foreign key.\",\n            hint=(\n                'Add unique=True to this field or add a UniqueConstraint '\n                '(without condition) in the model Meta.constraints.'\n            ),\n            obj=field,\n            id='fields.E311',\n        ),\n    ])\n", "def test_foreign_object_from_fields(self):\n    class Parent(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n\n    class Child(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('a', 'c'),  # 'c' is not a field on Child\n            to_fields=('a', 'b'),\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(from_model=Child), [\n        Error(\n            \"The from_field 'c' doesn't exist on the model 'Child'.\",\n            obj=field,\n            id='fields.E313',\n        ),\n    ])\n", "def test_foreign_object_to_non_unique_fields_with_intermediate_model(self):\n    class Parent(models.Model):\n        country_id = models.IntegerField()\n        city_id = models.IntegerField()\n\n    class Child(models.Model):\n        person_country_id = models.IntegerField()\n        person_city_id = models.IntegerField()\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('person_country_id', 'person_city_id'),\n            to_fields=('country_id', 'city_id'),\n            related_name='children',\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(from_model=Child), [\n        Error(\n            \"No subset of the fields 'country_id', 'city_id' on model 'Parent' is unique.\",\n            hint=(\n                'Mark a single field as unique=True or add a set of '\n                'fields to a unique constraint (via unique_together or a '\n                'UniqueConstraint (without condition) in the model '\n                'Meta.constraints).'\n            ),\n            obj=field,\n            id='fields.E310',\n        ),\n    ])\n"], "sample_504": ["def test_proportional_colorbars_extended():\n\n    x = y = np.arange(-3.0, 3.01, 0.025)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    levels = [-1.25, -0.5, -0.125, 0.125, 0.5, 1.25]\n    cmap = mcolors.ListedColormap(\n        ['0.3', '0.5', 'white', 'lightblue', 'steelblue'])\n    cmap.set_under('darkred')\n    cmap.set_over('crimson')\n    norm = mcolors.BoundaryNorm(levels, cmap.N)\n\n    extends = ['min', 'max']\n    spacings = ['uniform', 'proportional']\n    fig, axs = plt.subplots(2, 2)\n    for i in range(2):\n        for j in range(2):\n            CS3 = axs[i, j].contourf(X, Y, Z, levels, cmap=cmap, norm=norm,\n                                     extend=extends[i])\n            fig.colorbar(CS3, spacing=spacings[j], ax=axs[i, j])\n", "def test_colorbar_ticks_format():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    im = ax.contourf(data, levels=levels)\n    cbar = fig.colorbar(im, ticks=[0, 600, 1200], format=\"%.0f\")\n    labels = [label.get_text() for label in cbar.ax.yaxis.get_ticklabels()]\n    assert labels == ['0', '600', '1200']\n", "def test_colorbar_ticklabels():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    plt.contourf(data, levels=levels)\n    cb = plt.colorbar(orientation='horizontal')\n    ticklabels = ['Low', 'Medium', 'High']\n    cb.set_ticklabels(ticklabels)\n    assert cb.ax.get_xticklabels()[0].get_text() == 'Low'\n", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.arange(100).reshape((10, 10)), alpha=0.5)\n    fig.colorbar(im)\n    assert im.colorbar.alpha == 0.5\n", "def test_colorbar_minorticks_on_off_boundarynorm():\n    # test for github issue #11510 and PR #11584 for BoundaryNorm\n    np.random.seed(seed=12345)\n    data = np.random.randn(20, 20)\n    with rc_context({'_internal.classic_mode': False}):\n        fig, ax = plt.subplots()\n        norm = mcolors.BoundaryNorm(np.linspace(-2.3, 3.3, 11), 10)\n        im = ax.pcolormesh(data, norm=norm)\n\n        cbar = fig.colorbar(im)\n        # testing after minorticks_on()\n        cbar.minorticks_on()\n        np.testing.assert_almost_equal(\n            cbar.ax.yaxis.get_minorticklocs(),\n            [-2.2, -1.9, -1.7, -1.4, -1.2, -0.9, -0.7, -0.4, -0.2,\n             0.1, 0.3, 0.6, 0.8, 1.1, 1.3, 1.6, 1.8, 2.1, 2.3, 2.6, 2.8])\n        # testing after minorticks_off()\n        cbar.minorticks_off()\n        np.testing.assert_equal(cbar.ax.yaxis.get_minorticklocs(), [])\n"], "sample_1171": ["def test_issue_17860():\n    r = Range(-oo, oo)\n    raises(ValueError, lambda: r[-1])\n    r = Range(oo, -oo, -1)\n    raises(ValueError, lambda: r[-1])\n", "def test_Range_infinite_step():\n    assert Range(0, oo, 2) == Range(0, oo, 2)\n    assert Range(oo, 0, -2) == Range(oo, 0, -2)\n    assert Range(0, oo, -2) == Range(0, 0, 1)\n    assert Range(oo, 0, 2) == Range(0, 0, 1)\n    assert Range(0, oo, oo) == Range(0, oo, oo)\n    assert Range(oo, 0, oo) == Range(oo, 0, oo)\n    assert Range(0, oo, -oo) == Range(0, 0, 1)\n    assert Range(oo, 0, -oo) == Range(0, 0, 1)\n", "def test_Range_reversed():\n    assert Range(1, 10, 1).reversed == Range(9, 0, -1)\n    assert Range(10, 1, -1).reversed == Range(1, 10, 1)\n    assert Range(1, 10, 2).reversed == Range(9, 0, -2)\n    assert Range(10, 1, -2).reversed == Range(1, 10, 2)\n    assert Range(0, 10, 2).reversed == Range(10, -2, -2)\n    assert Range(10, 0, -2).reversed == Range(0, 10, 2)\n    assert Range(-10, 0, 2).reversed == Range(0, -12, -2)\n    assert Range(0, -10, -2).reversed == Range(-10, 2, 2)\n    assert Range(1, -10, -2).reversed == Range(-10, 3, 2)\n    assert Range(-10, 1, 2).reversed == Range(1, -12, -2)\n    assert Range(oo, -oo, -1).reversed == Range(-oo, oo, 1)\n    assert Range(-oo, oo, 1).reversed == Range(oo, -oo, -1)\n", "def test_range_intersection_symbolic():\n    a = Symbol('a', integer=True)\n    b = Symbol('b', integer=True, positive=True)\n    r1 = Range(a, b)\n    r2 = Range(a + 2, b + 2)\n    assert r1.intersect(r2) == Range(max(a, a + 2), min(b, b + 2))\n    assert r2.intersect(r1) == r1.intersect(r2)\n    r3 = Range(a, b, 2)\n    r4 = Range(a + 1, b + 1, 2)\n    assert r3.intersect(r4) == Range(max(a, a + 1), min(b, b + 1), 2)\n    assert r4.intersect(r3) == r3.intersect(r4)\n", "def test_issue_18516():\n    from sympy import cos, sin, atan2\n    r = symbols('r', real=True)\n    theta = symbols('theta', real=True)\n    cr = ComplexRegion(Interval(0, r) * Interval(0, 2*pi), polar=True)\n    assert cr.intersect(Interval(0, 1)) == Interval(0, 1)\n    assert cr.intersect(Interval(-1, 1)) == Interval(0, 1)\n    assert cr.intersect(Interval(0, r)) == Interval(0, r)\n    assert cr.intersect(Interval(r/2, r)) == Interval(r/2, r)\n    assert cr.intersect(Interval(-r, r/2)) == Interval(0, r/2)\n    assert cr.intersect(Interval(-r, r)) == Interval(0, r)\n    assert cr.intersect(Interval(-r, -r/2)) == Interval(0, 0)\n    assert cr.intersect(Interval(r, 2*r)) == Interval(r, r)\n    assert cr.intersect(Interval(-2*r, -r)) == Interval(0, 0)\n    assert cr.intersect(Interval(r, r)) == Interval(r, r)\n    assert cr.intersect(Interval(-r, -r)) == Interval(0, 0)\n    assert cr.intersect(Interval(0, 0)) == Interval(0, 0)\n    assert cr.intersect(Interval(-r, 0)) == Interval(0, 0)\n    assert cr.intersect(Interval(0, r)) == Interval(0, r)\n    cr = ComplexRegion(Interval(0, r) * Interval(0, atan2(r, r)), polar=True)\n    assert cr.intersect(Interval(-r, r)) == Interval(0, r*cos(atan2(1, 1)))\n"], "sample_472": ["def test_paginate_unordered_list(self):\n    \"\"\"\n    Paginating an unordered list does not raise a warning.\n    \"\"\"\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator([1, 2, 3, 4, 5], 2)\n    self.assertEqual(len(recorded), 0)\n", "def test_paginator_with_custom_page(self):\n    \"\"\"\n    Paginator should use custom Page class if it's provided.\n    \"\"\"\n    class CustomPage(Page):\n        pass\n\n    class CustomPaginator(Paginator):\n            return CustomPage(*args, **kwargs)\n\n    paginator = CustomPaginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertIsInstance(page, CustomPage)\n", "def test_paginating_ordered_queryset_does_not_warn(self):\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator(Article.objects.order_by(\"id\"), 5)\n    self.assertEqual(len(recorded), 0)\n", "def test_paginator_with_custom_page_class(self):\n    class CustomPage(Page):\n            return \"Custom method result\"\n\n    class CustomPaginator(Paginator):\n            return CustomPage(*args, **kwargs)\n\n    paginator = CustomPaginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertIsInstance(page, CustomPage)\n    self.assertEqual(page.custom_method(), \"Custom method result\")\n", "def test_paginating_ordered_object_list_does_not_warn(self):\n    \"\"\"\n    No warning is raised with an object that has an ordered attribute and\n    it is True.\n    \"\"\"\n\n    class OrderedObjectList:\n        ordered = True\n\n    object_list = OrderedObjectList()\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator(object_list, 5)\n    self.assertEqual(len(recorded), 0)\n"], "sample_898": ["def test_no_averaging_pos_label():\n    # test pos_label argument when not using averaging\n    # in binary classification case\n    y_true_binary = np.array([0, 1, 0, 1])\n    y_pred_binary = np.array([0, 1, 1, 0])\n    pos_label = 1\n\n    for name in METRICS_WITH_POS_LABEL:\n        metric = ALL_METRICS[name]\n\n        score_pos_label = metric(y_true_binary, y_pred_binary, pos_label=pos_label, average=None)\n        score = metric(y_true_binary, y_pred_binary, average=None)\n        assert_array_equal(score_pos_label, score)\n", "def test_pos_label_argument():\n    # test pos_label argument for metrics that support it\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([0, 1, 1, 0])\n    y_score = np.array([0.1, 0.9, 0.8, 0.2])\n\n    for name in METRICS_WITH_POS_LABEL:\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            score_pos_label_1 = metric(y_true, y_score, pos_label=1)\n            score_pos_label_0 = metric(y_true, y_score, pos_label=0)\n        else:\n            score_pos_label_1 = metric(y_true, y_pred, pos_label=1)\n            score_pos_label_0 = metric(y_true, y_pred, pos_label=0)\n\n        assert score_pos_label_1 != score_pos_label_0, f\"Metric {name} does not change with pos_label\"\n\n    # test that ValueError is raised for metrics that don't support pos_label\n    for name in set(ALL_METRICS) - set(METRICS_WITH_POS_LABEL):\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            with pytest.raises(ValueError):\n                metric(y_true, y_score, pos_label=1)\n        else:\n            with pytest.raises(ValueError):\n                metric(y_true, y_pred, pos_label=1)\n", "def test_pos_label_consistency():\n    y_true = np.array([0, 1, 0, 1, 0])\n    y_score = np.array([0.1, 0.6, 0.35, 0.8, 0.4])\n\n    for metric in THRESHOLDED_METRICS.values():\n        if metric.__name__ in METRICS_WITH_POS_LABEL:\n            # Test that pos_label is correctly handled\n            pos_label_true_value = metric(y_true, y_score, pos_label=1)\n            pos_label_false_value = metric(y_true, y_score, pos_label=0)\n            assert pos_label_true_value != pos_label_false_value, \"Positive label has no effect for {}\".format(metric.__name__)\n", "def test_sample_weight_normalization():\n    # Test that sample_weight is normalized\n    rng = np.random.RandomState(0)\n    y_true = rng.randint(0, 2, size=(50, ))\n    y_pred = rng.randint(0, 2, size=(50, ))\n    sample_weight = rng.randint(1, 10, size=50)\n\n    # Check that the score is invariant under scaling of the weights by a\n    # common factor\n    for scaling in [2, 0.3]:\n        scaled_sample_weight = sample_weight * scaling\n        weighted_score = accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n        scaled_weighted_score = accuracy_score(y_true, y_pred, sample_weight=scaled_sample_weight)\n        assert_almost_equal(weighted_score, scaled_weighted_score,\n                            err_msg=\"accuracy_score sample_weight is not invariant under scaling\")\n", "def test_log_loss_sample_weight_invariance():\n    # Test log_loss's sample_weight invariance\n    rng = np.random.RandomState(0)\n    y_true = rng.randint(0, 2, size=(50,))\n    y_score = rng.random_sample(size=(50,))\n    sample_weight = rng.randint(1, 10, size=len(y_true))\n\n    metric = log_loss\n    check_sample_weight_invariance('log_loss', metric, y_true, y_score)\n"], "sample_985": ["def test_as_real_imag():\n    assert (1 + I).as_real_imag() == (1, 1)\n    assert (1 - I).as_real_imag() == (1, -1)\n    assert (I).as_real_imag() == (0, 1)\n    assert (-I).as_real_imag() == (0, -1)\n    assert (2*I).as_real_imag() == (0, 2)\n    assert (2 + 3*I).as_real_imag() == (2, 3)\n    assert (2 - 3*I).as_real_imag() == (2, -3)\n    assert (-2 - 3*I).as_real_imag() == (-2, -3)\n    assert (-2 + 3*I).as_real_imag() == (-2, 3)\n", "def test_comparison():\n    x, y = symbols('x y')\n    assert (x + 1) > x\n    assert (x + 1) >= x\n    assert x < (x + 1)\n    assert x <= (x + 1)\n    assert (x + 1) != x\n    assert (x + 1) >= x + 1\n    assert (x + 1) <= x + 1\n    assert (x + 1) == x + 1\n    assert (x + 1) > x + 2 is False\n    assert (x + 1) < x + 2 is False\n", "def test_as_content_primitive():\n    x, y = symbols('x y')\n    assert (2*x).as_content_primitive() == (2, x)\n    assert (2*x*y).as_content_primitive() == (2, x*y)\n    assert (2*x + 2*y).as_content_primitive() == (2, x + y)\n    assert (2*x - 2*y).as_content_primitive() == (2, x - y)\n    assert (x/2).as_content_primitive() == (Rational(1, 2), x)\n    assert (x*y/2).as_content_primitive() == (Rational(1, 2), x*y)\n    assert (x/2 + y/2).as_content_primitive() == (Rational(1, 2), x + y)\n    assert (x/2 - y/2).as_content_primitive() == (Rational(1, 2), x - y)\n    assert (2*x**2).as_content_primitive() == (2, x**2)\n    assert (2*x**2*y).as_content_primitive() == (2, x**2*y)\n    assert (2*x**2 + 2*y**2).as_content_primitive() == (2, x**2 + y**2)\n    assert (2*x**2 - 2*y**2).as_content_primitive() == (2, x**2 - y**2)\n", "def test_eval_is_comparable():\n    x, y = symbols('x y')\n    assert (x + 1).is_comparable is True\n    assert (x > y).is_comparable is True\n    assert (x**2).is_comparable is False\n    assert (x + y).is_comparable is True\n    assert (x + I).is_comparable is False\n    assert (exp(x)).is_comparable is True\n", "def test_args_property():\n    x, y = symbols('x y')\n    expr = sin(x) + cos(y)\n    assert expr.args == (sin(x), cos(y))\n    assert expr.func(*expr.args) == expr\n\n    # Test args with a nested expression\n    expr = sin(x) + cos(sin(y))\n    assert expr.args == (sin(x), cos(sin(y)))\n    assert expr.func(*expr.args) == expr\n\n    # Test args with an Atom\n    expr = sin(x) + 2\n    assert expr.args == (sin(x), 2)\n    assert expr.func(*expr.args) == expr\n"], "sample_942": ["def test_pyfunction_signature_with_invalid_annotation(app):\n    text = \".. py:function:: hello(name: invalid_type) -> str\"\n    with pytest.raises(SyntaxError):\n        restructuredtext.parse(app, text)\n", "def test_pyfunction_signature_with_defaults(app):\n    text = \".. py:function:: hello(name='world', age=30)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, \"\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'world'\"])],\n                                      [desc_parameter, ([desc_sig_name, \"age\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"30\"])])])\n", "def test_py_meta_field(app):\n    text = (\".. py:function:: func\\n\"\n            \"   :meta:\\n\"\n            \"   :meta private:\\n\"\n            \"   :meta public:\\n\"\n            \"   :meta undocumented:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert len(doctree[1][1][1]) == 0  # :meta: field is removed\n", "def test_pyfunction_noindex(app):\n    text = \".. py:function:: f()\\n\"\n    text += \"   :noindex:\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[])\n", "def test_pymodule(app):\n    text = (\".. py:module:: example\\n\"\n            \"   :platform: Linux\\n\"\n            \"   :synopsis: This is an example module.\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,))\n    assert_node(doctree[0], nodes.target, **{\"ids\": [\"module-example\"]})\n    assert_node(doctree[1], addnodes.index,\n                entries=[('pair', 'module; example', 'module-example', '', None)])\n    assert_node(doctree[2], addnodes.index,\n                entries=[('single', 'This is an example module. (Linux, deprecated)',\n                          'module-example', '', None)])\n\n    assert 'example' in domain.modules\n    assert domain.modules['example'] == ('index', 'module-example',\n                                         'This is an example module.', 'Linux', True)\n"], "sample_818": ["def test_spectral_clustering_amg_solver(eigen_solver):\n    if amg_loaded:\n        S = np.array([[1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                      [1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                      [1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                      [0.2, 0.2, 0.2, 1.0, 1.0, 1.0, 1.0],\n                      [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],\n                      [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],\n                      [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]])\n\n        model = SpectralClustering(random_state=0, n_clusters=2,\n                                   affinity='precomputed',\n                                   eigen_solver=eigen_solver,\n                                   assign_labels='kmeans'\n                                   ).fit(S)\n        labels = model.labels_\n        if labels[0] == 0:\n            labels = 1 - labels\n\n        assert adjusted_rand_score(labels, [1, 1, 1, 0, 0, 0, 0]) == 1\n    else:\n        assert_raises(\n            ValueError, spectral_clustering,\n            S, n_clusters=2, eigen_solver='amg', random_state=0)\n", "def test_spectral_clustering_sparse_input():\n    # Test that spectral_clustering works correctly with sparse input\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    X_sparse = sparse.csr_matrix(X)\n    labels_dense = spectral_clustering(X, n_clusters=2, random_state=0)\n    labels_sparse = spectral_clustering(X_sparse, n_clusters=2, random_state=0)\n    assert_array_equal(labels_dense, labels_sparse)\n", "def test_spectral_clustering_parameters(n_clusters, eigen_solver, assign_labels):\n    # Test spectral_clustering with different parameters\n    X, y = make_blobs(n_samples=200, random_state=0,\n                      centers=n_clusters, cluster_std=1.0)\n    labels = spectral_clustering(X, n_clusters=n_clusters,\n                                eigen_solver=eigen_solver,\n                                assign_labels=assign_labels,\n                                random_state=0)\n    assert adjusted_rand_score(y, labels) >= 0.75\n", "def test_spectral_clustering_n_init():\n    # Test that spectral_clustering with different n_init gives same labels\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    labels_n_init_10 = SpectralClustering(n_clusters=2, random_state=0, n_init=10).fit(X).labels_\n    labels_n_init_20 = SpectralClustering(n_clusters=2, random_state=0, n_init=20).fit(X).labels_\n    assert adjusted_rand_score(labels_n_init_10, labels_n_init_20) == 1\n", "def test_spectral_clustering_multiple_clusters(n_samples, n_clusters):\n    # Test spectral clustering with multiple clusters\n    random_state = np.random.RandomState(seed=8)\n    X, y_true = make_blobs(n_samples=n_samples, centers=n_clusters, random_state=random_state)\n    labels = SpectralClustering(n_clusters=n_clusters, random_state=random_state).fit(X).labels_\n    assert len(np.unique(labels)) == n_clusters\n    assert adjusted_rand_score(y_true, labels) > 0.8\n"], "sample_435": ["    def test_custom_user_model(self):\n        data = {\n            \"email\": \"testclient@example.com\",\n            \"password1\": \"testclient\",\n            \"password2\": \"testclient\",\n            \"date_of_birth\": \"1988-02-24\",\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertTrue(CustomUser.objects.filter(email=\"testclient@example.com\").exists())\n", "def test_password_help_text(self):\n    form = AdminPasswordChangeForm(self.u1)\n    self.assertEqual(\n        form.fields[\"password1\"].help_text,\n        \"<ul><li>\"\n        \"Your password can\u2019t be too similar to your other personal information.\"\n        \"</li></ul>\",\n    )\n", "    def test_username_field_with_custom_user_model(self):\n        custom_user = CustomUser.objects.create_user(\n            email=\"customuser@example.com\", password=\"password\", date_of_birth=\"2000-01-01\"\n        )\n        data = {\"email\": \"customuser@example.com\", \"date_of_birth\": \"2000-01-01\"}\n        form = UserChangeForm(data, instance=custom_user)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data[\"email\"], \"customuser@example.com\")\n", "    def test_email_as_username(self):\n        user = CustomUser.objects.create_user(email=\"testclient@example.com\", password=\"password\")\n        data = {\n            \"email\": \"testclient@example.com\",\n            \"password\": \"password\",\n            \"date_of_birth\": \"1990-01-01\",\n        }\n        form = UserChangeForm(data, instance=user)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(form.cleaned_data[\"email\"], \"testclient@example.com\")\n        self.assertEqual(form.cleaned_data[\"date_of_birth\"], datetime.date(1990, 1, 1))\n", "    def test_custom_user_model_form_validation(self):\n        data = {\n            \"username\": \"testuser\",\n            \"password1\": \"testpassword\",\n            \"password2\": \"testpassword\",\n            \"date_of_birth\": \"1988-02-24\",\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.username, \"testuser\")\n        self.assertEqual(user.date_of_birth, datetime.date(1988, 2, 24))\n        self.assertTrue(user.check_password(\"testpassword\"))\n"], "sample_1136": ["def test_poly_with_zero_coefficient():\n    p = Poly(0*x, x)\n    assert p.is_zero\n    assert p.as_expr() == 0\n\n    p = Poly(0*x**2, x)\n    assert p.is_zero\n    assert p.as_expr() == 0\n\n    p = Poly(0*x**2 + 0*x, x)\n    assert p.is_zero\n    assert p.as_expr() == 0\n\n    p = Poly(0*x**2 + 0*x + 1, x)\n    assert not p.is_zero\n    assert p.as_expr() == 1\n", "def test_issue_19653():\n    f = Poly(x**2 - 2, x)\n    g = Poly(x**2 - 2, x, domain='ZZ')\n    h = Poly(x**2 - 2, x, domain='QQ')\n    assert f == g == h\n", "def test_issue_21022():\n    a, b = symbols('a b')\n    f = a*b**2 + a*b + a\n    assert factor(f) == a*(b**2 + b + 1)\n", "def test_issue_20054():\n    p = Poly(x + y, x, y, z)\n    assert p.as_coefficients_dict() == {(1, 1): 1}\n    assert p.as_coefficients_dict([x, y]) == {(1, 1): 1}\n    assert p.as_coefficients_dict([x, y, z]) == {(1, 1): 1}\n    assert p.as_coefficients_dict([y, x]) == {(1, 1): 1}\n    assert p.as_coefficients_dict([y, x, z]) == {(1, 1): 1}\n", "def test_issue_19462():\n    # Test for gcd, lcm, gcd_list, lcm_list with generators and coefficients not in the same field\n    x, y = symbols('x, y')\n    assert gcd(x + 1, y - 1) == 1\n    assert lcm(x + 1, y - 1) == (x + 1)*(y - 1)\n    assert gcd_list([x + 1, y - 1]) == 1\n    assert lcm_list([x + 1, y - 1]) == (x + 1)*(y - 1)\n"], "sample_705": ["def test_pytester_outcomes_xfailed(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(xfailed=1)\n", "def test_pytester_runpython_c(pytester: Pytester) -> None:\n    result = pytester.runpython_c(\"print('hello')\")\n    assert result.stdout.lines == ['hello']\n    assert result.stderr.str() == \"\"\n    assert result.ret == 0\n", "def test_pytester_run_with_stdin(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\n        \"\"\"\n        import sys\n            input_data = sys.stdin.read()\n            assert input_data == 'test input'\n        \"\"\"\n    )\n    result = pytester.runpytest(testfile, stdin='test input')\n    assert result.ret == ExitCode.OK\n", "def test_pytester_plugins_argument(pytester: Pytester) -> None:\n    pytester.plugins = [\"pytester\"]\n    pytester.makepyfile(\"def test_hello(pytester): assert 1\")\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_run_result_assert_outcomes_deselected(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-k\", \"test_one\")\n    result.assert_outcomes(passed=1, deselected=1)\n    assert result.parseoutcomes() == {\"passed\": 1, \"deselected\": 1}\n"], "sample_1047": ["def test_issue_9583():\n    x = Symbol('x', zero=True)\n    assert x.is_nonzero is False\n", "def test_issue_16313_extended():\n    x = Symbol('x', real=False, imaginary=True)\n    k = Symbol('k', real=True, nonzero=True)\n    assert (-x).is_real is False\n    assert (-x).is_imaginary is True\n    assert (k*x).is_real is False\n    assert (k*x).is_imaginary is True\n    assert (-x).is_positive is False\n    assert (-x).is_negative is False\n", "def test_issue_16313_addition():\n    x = Symbol('x', real=False)\n    k = Symbol('k', real=True)\n    l = Symbol('l', real=True, zero=False)\n    assert (-x + k).is_real is None  # k can be imaginary\n    assert (-x + l).is_real is None  # l*x can be real or imaginary\n    assert (-x + l*x).is_real is None  # since l*x can be a real number\n    assert (-x + k).is_positive is None  # k can be negative\n", "def test_issue_16314():\n    x = Symbol('x', real=True)\n    k = Symbol('k', real=False)\n    assert (k*x).is_real is None\n    assert (k*x*x).is_real is None\n    assert (-k).is_real is None\n", "def test_issue_16332():\n    p = Symbol('p', zero=True)\n    q = Symbol('q', zero=False, real=True)\n    j = Symbol('j', zero=False, even=True)\n    assert (p**q).is_negative is False\n    assert (p**j).is_negative is False\n"], "sample_1193": ["def test_are_coplanar_with_plane():\n    from sympy import Plane\n    p = Plane(Point3D(1, 0, 0), Point3D(0, 1, 0), Point3D(0, 0, 1))\n    a = Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))\n    b = Line3D(Point3D(3, 0, 0), Point3D(4, 0, 0))\n    assert are_coplanar(p, a) == True\n    assert are_coplanar(p, b) == True\n    c = Line3D(Point3D(1, 1, 0), Point3D(2, 2, 1))\n    assert are_coplanar(p, c) == False\n", "def test_are_coplanar_mixed_dimensions():\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line(Point2D(0, 3), Point2D(1, 5))\n    c = Point3D(0, 0, 0)\n    d = Point2D(0, 0)\n\n    assert are_coplanar(a, b, c, d) == True\n", "def test_are_similar():\n    from sympy import Point, Circle, Triangle, are_similar\n    c1, c2 = Circle(Point(0, 0), 4), Circle(Point(1, 4), 3)\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    assert are_similar(t1, t2) == True\n    assert are_similar(t1, t3) == False\n    assert are_similar(c1, c2) == True\n    raises(GeometryError, lambda: are_similar(t1, c1))\n", "def test_are_coplanar_with_plane():\n    from sympy import Plane\n\n    a = Point3D(1, 2, 3)\n    b = Point3D(4, 5, 6)\n    c = Point3D(7, 8, 9)\n    d = Point3D(10, 11, 12)\n    e = Point3D(13, 14, 15)\n\n    p = Plane(a, b, c)\n\n    assert are_coplanar(a, b, c, d) == True\n    assert are_coplanar(a, b, c, e) == False\n    assert are_coplanar(p, a, b, c) == True\n    assert are_coplanar(p, a, b, e) == False\n", "def test_are_similar():\n    # Test similar entities\n    t1 = Triangle(Point(0, 0), Point(1, 0), Point(0, 1))\n    t2 = Triangle(Point(0, 0), Point(2, 0), Point(0, 2))\n    t3 = Triangle(Point(0, 0), Point(3, 0), Point(0, 1))\n    assert are_similar(t1, t2) == True\n    assert are_similar(t1, t3) == False\n\n    # Test entities that are not comparable for similarity\n    p = Point(0, 0)\n    l = Line(Point(0, 0), Point(1, 1))\n    with raises(GeometryError):\n        are_similar(p, l)\n"], "sample_666": ["def test_logging_while_collecting_with_file_handler(testdir):\n    \"\"\"Issue #6240: Calls to logging.xxx() during collection causes all logging calls to be duplicated to the file handler\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import logging\n\n        logging.basicConfig(filename='test.log', level=logging.INFO)\n        logging.warning(\"during collection\")\n\n            logging.warning(\"during call\")\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    assert result.ret == ExitCode.TESTS_FAILED\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_*.py F*\",\n            \"====* FAILURES *====\",\n            \"____*____\",\n            \"*--- Captured log call*\",\n            \"WARNING * during call\",\n            \"*1 failed*\",\n        ]\n    )\n    with open('test.log', 'r') as f:\n        log_content = f.read()\n    assert \"during collection\" in log_content\n    assert \"during call\" in log_content\n", "def test_logging_while_collecting_no_duplicates(testdir):\n    \"\"\"Issue #6240: Calls to logging.xxx() during collection should not cause all logging calls to be duplicated to stderr\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import logging\n        import pytest\n\n        @pytest.hookimpl(tryfirst=True)\n            logging.warning(\"during collection\")\n\n            logging.warning(\"during call\")\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    assert result.ret == ExitCode.TESTS_FAILED\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_*.py F*\",\n            \"====* FAILURES *====\",\n            \"____*____\",\n            \"*--- Captured log call*\",\n            \"WARNING * during call\",\n            \"*1 failed*\",\n        ]\n    )\n    result.stdout.no_fnmatch_line(\"*Captured stderr call*\")\n    result.stdout.no_fnmatch_line(\"*during collection*\")\n", "def test_logging_level_for_setting(testdir):\n    config = testdir.parseconfig(\"--log-cli-level=WARNING\")\n    level = get_log_level_for_setting(config, \"log_cli_level\")\n    assert level == logging.WARNING\n\n    config = testdir.parseconfig(\"--log-cli-level=INFO\")\n    level = get_log_level_for_setting(config, \"log_cli_level\")\n    assert level == logging.INFO\n\n    config = testdir.parseconfig(\"--log-cli-level=DEBUG\")\n    level = get_log_level_for_setting(config, \"log_cli_level\")\n    assert level == logging.DEBUG\n", "def test_logging_during_call(testdir):\n    \"\"\"Ensure logging during the call phase is captured correctly.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import logging\n\n            logging.warning(\"during call\")\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_*.py .*\",\n            \"*--- Captured log call*\",\n            \"WARNING * during call\",\n            \"*1 passed*\",\n        ]\n    )\n    result.stdout.no_fnmatch_line(\"*Captured stderr call*\")\n    result.stdout.no_fnmatch_line(\"*during collection*\")\n", "def test_get_log_level_for_setting_with_int_log_level(testdir):\n    config = testdir.parseconfig(\"--log-level=20\")\n    level = get_log_level_for_setting(config, \"log_level\")\n    assert level == 20\n"], "sample_1115": ["def test_tensor_evaluate():\n    L = TensorIndexType(\"L\")\n    i0, i1, i2 = tensor_indices(\"i0:3\", L)\n    A = TensorHead(\"A\", [L])\n    expr = A(i0) + A(i1)\n    repl = {A(i0): 1, A(i1): 2}\n    assert expr.evaluate(repl) == 3\n", "def test_tensorhead_with_args():\n    with warns_deprecated_sympy():\n        A = tensorhead('A', [])\n        assert A.args == ('A', [])\n        assert A.name == 'A'\n        assert A.index_types == []\n", "def test_tensor_rewriting_with_indexed():\n    L = TensorIndexType(\"L\", dim=4)\n    A = TensorHead(\"A\", [L]*4)\n    B = TensorHead(\"B\", [L])\n\n    i0, i1, i2, i3 = symbols(\"i0:4\")\n    L_0, L_1 = symbols(\"L_0:2\")\n\n    # Test rewriting with Indexed for more complex expressions\n    expr = A(i0, -i0, i2, i3) + B(-i3)*A(i2, i3, i0, -i0)\n    expected = Sum(Indexed(Symbol(\"A\"), L_0, L_0, i2, i3), (L_0, 0, 3)) + Sum(Indexed(Symbol(\"B\"), L_1)*Indexed(Symbol(\"A\"), i2, i3, L_0, L_1), (L_0, 0, 3), (L_1, 0, 3))\n    assert expr.rewrite(Indexed) == expected\n", "def test_tensor_canon_bp():\n    # Test canonicalization with no symmetry\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    A, B = tensor_heads('A,B', [Lorentz]*2, TensorSymmetry.no_symmetry(2))\n    a, b, c, d = tensor_indices('a,b,c,d', Lorentz)\n    t = A(a, b)*B(-b, c)\n    t1 = t.canon_bp()\n    assert t1 == A(b, a)*B(-b, c)\n\n    # Test canonicalization with symmetry\n    sym2 = TensorSymmetry.fully_symmetric(2)\n    A_sym = TensorHead('A', [Lorentz]*2, sym2)\n    t_sym = A_sym(a, b)*B(-b, c)\n    t1_sym = t_sym.canon_bp()\n    assert t1_sym == B(c, b)*A_sym(-b, b)\n", "def test_tensorsymmetry_errors():\n    with raises(ValueError):\n        # passing a non-sequence to tensorsymmetry\n        tensorsymmetry(1)\n\n    with raises(ValueError):\n        # passing a sequence of non-integers to tensorsymmetry\n        tensorsymmetry([1, 'a'])\n\n    with raises(ValueError):\n        # passing a sequence of integers that doesn't add up to zero to tensorsymmetry\n        tensorsymmetry([1, 2, 3])\n"], "sample_466": ["def test_serialize_time(self):\n    self.assertSerializedEqual(datetime.time(12, 34, 56))\n    self.assertSerializedResultEqual(\n        datetime.time(12, 34, 56),\n        (\"datetime.time(12, 34, 56)\", {\"import datetime\"}),\n    )\n", "def test_serialize_deconstructible(self):\n    \"\"\"\n    Test serialization of deconstructible objects.\n    \"\"\"\n    obj = DeconstructibleInstances()\n    string, imports = MigrationWriter.serialize(obj)\n    self.assertEqual(\n        string, \"migrations.test_writer.DeconstructibleInstances()\"\n    )\n    self.assertIn(\"import migrations.test_writer\", imports)\n", "def test_serialize_custom_field(self):\n    custom_field = models.CharField(max_length=255, choices=[('A', 'Option A'), ('B', 'Option B')])\n    self.assertSerializedFieldEqual(custom_field)\n    self.assertSerializedResultEqual(\n        custom_field,\n        (\"models.CharField(max_length=255, choices=[('A', 'Option A'), ('B', 'Option B')])\", {\"from django.db import models\"}),\n    )\n", "def test_serialize_complex_default(self):\n    self.assertSerializedEqual(models.CharField(default=complex(1, 2)))\n    self.assertSerializedResultEqual(\n        models.CharField(default=complex(1, 2)),\n        (\n            \"models.CharField(default=complex('1+2j'))\",\n            {\"from django.db import models\"},\n        ),\n    )\n", "def test_serialize_model_with_exclude(self):\n    model = type('Model', (), {\n        'Meta': type('Meta', (), {\n            'managed': False,\n            'db_table': 'custom_table',\n            'app_label': 'testapp',\n            'exclude': ('exclude_field',),\n        }),\n    })\n    string, imports = MigrationWriter.serialize(model)\n    self.assertEqual(\n        string,\n        \"models.Model(name='Model', \"\n        \"options={'managed': False, 'db_table': 'custom_table', \"\n        \"'app_label': 'testapp', 'exclude': ('exclude_field',)}, \"\n        \"bases=(models.Model,))\",\n    )\n    self.assertEqual(imports, {\"from django.db import models\"})\n"], "sample_486": ["def test_inlineformset_factory_nulls_default_pks_auto_parent_auto_child(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n    the case of a parent object with an AutoField primary key and a child\n    object with an AutoField primary key.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        AutoPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet()\n    self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n", "def test_inlineformset_factory_nulls_default_pks_auto_child_auto_parent(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n    the case of a parent object with an AutoField primary key and a child\n    object with an AutoField primary key.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        AutoPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet()\n    self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n", "def test_inlineformset_factory_nulls_default_pks_uuid_parent_auto_child_submit(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_ignores_default_pks_on_submit\n    for the case of a parent object with a UUID primary key and a child object\n    with an AutoField primary key.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        UUIDPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet(\n        {\n            \"autopkchildofuuidpkparent_set-TOTAL_FORMS\": 3,\n            \"autopkchildofuuidpkparent_set-INITIAL_FORMS\": 0,\n            \"autopkchildofuuidpkparent_set-MAX_NUM_FORMS\": \"\",\n            \"autopkchildofuuidpkparent_set-0-name\": \"Foo\",\n            \"autopkchildofuuidpkparent_set-1-name\": \"\",\n            \"autopkchildofuuidpkparent_set-2-name\": \"\",\n        }\n    )\n    self.assertTrue(formset.is_valid())\n    self.assertIsNone(formset.instance.uuid)\n    self.assertIsNone(formset.forms[0].instance.parent_id)\n", "def test_inlineformset_factory_nulls_default_pks_uuid_parent_auto_child_data(self):\n    \"\"\"\n    If form data is provided, a parent's auto-generated UUID is set.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        UUIDPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\"\n    )\n    formset = FormSet(\n        {\n            \"autopkchildofuuidpkparent_set-TOTAL_FORMS\": 3,\n            \"autopkchildofuuidpkparent_set-INITIAL_FORMS\": 0,\n            \"autopkchildofuuidpkparent_set-MAX_NUM_FORMS\": \"\",\n            \"autopkchildofuuidpkparent_set-0-name\": \"Test\",\n            \"autopkchildofuuidpkparent_set-1-name\": \"\",\n            \"autopkchildofuuidpkparent_set-2-name\": \"\",\n        }\n    )\n    self.assertIs(formset.is_valid(), True)\n    self.assertIsNotNone(formset.instance.uuid)\n    self.assertEqual(formset.forms[0].instance.parent_id, formset.instance.uuid)\n", "def test_inlineformset_factory_nulls_default_pks_auto_child_alternate_key_relation(self):\n    \"\"\"\n    #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n    the case of a parent object with an AutoField primary key and a child\n    object that relates to an alternate key of the parent.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        AutoPKChildOfUUIDPKParent, ChildRelatedViaAK, fields=\"__all__\"\n    )\n    formset = FormSet()\n    self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n"], "sample_403": ["    def test_add_field(self):\n        operation = migrations.AddField(\"Model\", \"field\", models.IntegerField())\n        self.assertIs(operation.references_model(\"Model\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"Other\", \"migrations\"), False)\n        self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), True)\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\", models.CASCADE, limit_choices_to={\"field\": \"value\"}\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"field\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "    def test_references_field(self):\n        operation = migrations.RenameField(\"Model\", \"old_field\", \"new_field\")\n        self.assertIs(operation.references_field(\"Model\", \"old_field\", \"migrations\"), False)\n        self.assertIs(operation.references_field(\"Model\", \"new_field\", \"migrations\"), False)\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\",\n            models.CASCADE,\n            limit_choices_to={\"field__isnull\": True}\n        ),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, limit_choices_to={\"field\": \"value\"}),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n"], "sample_1140": ["def test_StrPrinter():\n    from sympy.printing.str import StrPrinter\n    from sympy.core.symbol import Str\n    p = StrPrinter()\n    assert p.doprint(Str('x')) == 'x'\n", "def test_issue_18343():\n    assert pretty(Sum(sin(k*x), (k, 0, 2))) == \\\n    '  2     \\n'\\\n    ' ___    \\n'\\\n    ' \\\\  `   \\n'\\\n    '  \\\\    k\\n'\\\n    '  /   sin(k*x)\\n'\\\n    ' /__,   \\n'\\\n    'k = 0   '\n\n    assert upretty(Sum(sin(k*x), (k, 0, 2))) == \\\n    '  2     \\n'\\\n    ' ___    \\n'\\\n    ' \u2572      \\n'\\\n    '  \u2572    k\\n'\\\n    '  \u2571   sin(k\u22c5x)\\n'\\\n    ' \u2571      \\n'\\\n    ' \u203e\u203e\u203e    \\n'\\\n    'k = 0   '\n", "def test_issue_17653():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    expr = HadamardProduct(A, B)\n    ascii_str = \\", "def test_issue_19874():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    assert pretty(Cylindrical(x, y, z)) == '(x*cos(y), x*sin(y), z)'\n", "def test_pretty_issue_19890():\n    from sympy import symbols, pretty, upretty, MatrixSymbol, sin, cos\n    a, b, c, d = symbols('a b c d')\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A*sin(a*B) + A*cos(c*B) + d*A\n    ascii_str = \\"], "sample_682": ["def test_skipif_with_invalid_syntax(self, testdir) -> None:\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid syntax\")\n            pass\n        \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_xfail_evaltrue_but_passes(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail('True')\n            assert 1\n    \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    callreport = reports[1]\n    assert callreport.passed\n    assert callreport.wasxfail == \"\"\n", "def test_skip_with_unknown_parameter(testdir):\n    \"\"\"\n    Verify that using pytest.skip() with unknown parameter raises an error\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.skip(\"skip_test\", unknown=1)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*TypeError:*['unknown']*\"])\n", "def test_xfail_not_run_xfail_reporting_no_reason(self, testdir):\n    p = testdir.makepyfile(\n        test_one=\"\"\"\n        import pytest\n        @pytest.mark.xfail(run=False)\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines(\n        [\"*test_one*test_this*\", \"*NOTRUN*\", \"*1 xfailed*\"]\n    )\n", "def test_module_level_skip_with_reason(testdir):\n    \"\"\"\n    Verify that using pytest.skip with a reason at module level is handled correctly\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.skip(\"skip_module_level\")\n\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*skip_module_level*\", \"*1 skipped*\"])\n"], "sample_679": ["def test_mark_closest_no_marker(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class Test:\n                pass\n        \"\"\"\n    )\n    items, rec = testdir.inline_genitems(p)\n    no_marker = items[0]\n    assert no_marker.get_closest_marker(\"missing\") is None\n", "def test_mark_with_parameters(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [\"foo\", \"bar\"])\n            assert arg in [\"foo\", \"bar\"]\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=2)\n", "def test_mark_option_custom_evaluation_error(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n            for item in items:\n                if \"error\" in item.nodeid:\n                    item.add_marker(pytest.mark.error(lambda x: x / 0))\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    rec = testdir.inline_run(\"-m\", \"error\")\n    passed, skipped, failed = rec.listoutcomes()\n    assert len(passed) == 0\n    assert len(skipped) == 0\n    assert len(failed) == 1\n    assert \"ZeroDivisionError\" in str(failed[0].longrepr)\n", "def test_mark_expression_in_parametrize(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"value\", [\n            pytest.param(1, marks=pytest.mark.skip(reason=\"skipping\")),\n            2,\n            3\n        ])\n            assert value != 2\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1, skipped=1, failed=1)\n", "def test_reevaluate_dynamic_expr_with_different_scope(testdir):\n    \"\"\"#7360\"\"\"\n    py_file1 = testdir.makepyfile(\n        test_reevaluate_dynamic_expr_scope1=\"\"\"\n        import pytest\n\n            skip = True\n            pytest.mark.skipif(\"skip\")\n            assert True\n    \"\"\"\n    )\n    py_file2 = testdir.makepyfile(\n        test_reevaluate_dynamic_expr_scope2=\"\"\"\n        import pytest\n\n            skip = False\n            pytest.mark.skipif(\"skip\")\n            assert True\n    \"\"\"\n    )\n\n    file_name1 = os.path.basename(py_file1.strpath)\n    file_name2 = os.path.basename(py_file2.strpath)\n    reprec = testdir.inline_run(file_name1, file_name2)\n    reprec.assertoutcome(passed=1, skipped=1)\n"], "sample_343": ["def test_bulk_related_objects(self):\n    question = Question.objects.create(text='Who?')\n    answer1 = Answer.objects.create(question=question)\n    answer2 = Answer.objects.create(question=question)\n\n    related_objects = Answer.question.field.bulk_related_objects([answer1, answer2])\n    self.assertCountEqual(related_objects, [question])\n", "def test_get_or_create_generic_relation(self):\n    question = Question.objects.create(text='test')\n    answer, created = question.answer_set.get_or_create(text='answer1')\n    self.assertTrue(created)\n    self.assertEqual(answer.question, question)\n    answer2, created = question.answer_set.get_or_create(text='answer1')\n    self.assertFalse(created)\n    self.assertEqual(answer2, answer)\n", "def test_cached_value(self):\n    question = Question.objects.create(text='Who?')\n    post = Post.objects.create(title='Answer', parent=question)\n\n    post = Post.objects.get(pk=post.pk)\n    with self.assertNumQueries(1):\n        self.assertEqual(post.parent, question)\n        self.assertEqual(post.parent, question)\n", "def test_get_object_cache_respects_changed_content_type(self):\n    question = Question.objects.create(text='Who?')\n    post = Post.objects.create(title='Answer', parent=question)\n\n    question.delete()\n    answer = Answer.objects.create(text='I don\\'t know')\n    answer.question = post\n    answer.save()\n\n    post = Post.objects.get(pk=post.pk)\n    with self.assertNumQueries(1):\n        self.assertEqual(post.object_id, answer.pk)\n        self.assertIsInstance(post.parent, Answer)\n        self.assertIsInstance(post.parent, Answer)\n", "def test_get_object_with_deleted_content_type(self):\n    question = Question.objects.create(text='Who?')\n    post = Post.objects.create(title='Answer', parent=question)\n\n    ContentType.objects.get_for_id(post.content_type_id).delete()\n\n    post = Post.objects.get(pk=post.pk)\n    with self.assertNumQueries(1):\n        self.assertEqual(post.object_id, question.pk)\n        self.assertIsNone(post.parent)\n        self.assertIsNone(post.parent)\n"], "sample_1059": ["def test_jacobi_fdiff_a_b():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n    _k = Dummy('k')\n\n    # Test fdiff for a and b\n    assert diff(jacobi(n, a, b, x), a).dummy_eq(Sum((jacobi(n, a, b, x) +\n        (2*_k + a + b + 1)*RisingFactorial(_k + b + 1, -_k + n)*jacobi(_k, a,\n        b, x)/((-_k + n)*RisingFactorial(_k + a + b + 1, -_k + n)))/(_k + a\n        + b + n + 1), (_k, 0, n - 1)))\n    assert diff(jacobi(n, a, b, x), b).dummy_eq(Sum(((-1)**(-_k + n)*(2*_k +\n        a + b + 1)*RisingFactorial(_k + a + 1, -_k + n)*jacobi(_k, a, b, x)/\n        ((-_k + n)*RisingFactorial(_k + a + b + 1, -_k + n)) + jacobi(n, a,\n        b, x))/(_k + a + b + n + 1), (_k, 0, n - 1)))\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    norm_factor = (S(2)**(a + b + 1) * (gamma(n + a + 1) * gamma(n + b + 1))\n                   / (2*n + a + b + 1) / (factorial(n) * gamma(n + a + b + 1)))\n\n    assert jacobi_normalized(n, a, b, x) == jacobi(n, a, b, x) / sqrt(norm_factor)\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi_normalized(n, a, b, x) == \\\n           (jacobi(n, a, b, x)/sqrt(2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)\n                                    /((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1))))\n\n    raises(ValueError, lambda: jacobi_normalized(-2.1, a, b, x))\n", "def test_orthogonal_polynomial_conjugate():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    # Test the _eval_conjugate method for different orthogonal polynomials\n    assert jacobi(n, a, b, x)._eval_conjugate() == jacobi(n, a.conjugate(), b.conjugate(), x.conjugate())\n    assert gegenbauer(n, a, x)._eval_conjugate() == gegenbauer(n, a.conjugate(), x.conjugate())\n    assert legendre(n, x)._eval_conjugate() == legendre(n, x.conjugate())\n    assert assoc_legendre(n, m, x)._eval_conjugate() == assoc_legendre(n, m.conjugate(), x.conjugate())\n    assert hermite(n, x)._eval_conjugate() == hermite(n, x.conjugate())\n    assert laguerre(n, x)._eval_conjugate() == laguerre(n, x.conjugate())\n    assert assoc_laguerre(n, alpha, x)._eval_conjugate() == assoc_laguerre(n, alpha.conjugate(), x.conjugate())\n    assert chebyshevt(n, x)._eval_conjugate() == chebyshevt(n, x.conjugate())\n    assert chebyshevu(n, x)._eval_conjugate() == chebyshevu(n, x.conjugate())\n", "def test_jacobi_normalized():\n    assert jacobi_normalized(n, a, b, x) == \\\n           jacobi(n, a, b, x)/sqrt(2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)\n                                    /((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)))\n    assert jacobi_normalized(n, a, b, 0) == sqrt(pi)/((2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1))\n                                                     /((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)*sqrt(gamma(a + b + n + 1))))\n    assert jacobi_normalized(n, a, b, 1) == sqrt(gamma(2*a + n))/((2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1))\n                                                                 /((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)*sqrt(gamma(2*a + n))))\n"], "sample_142": ["def test_readonly_field_exclusion(self):\n    class SongAdmin(admin.ModelAdmin):\n        readonly_fields = (\"title\",)\n        exclude = (\"title\",)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'readonly_fields' cannot contain a field that is also included in 'exclude'.\",\n            obj=SongAdmin,\n            id='admin.E036',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_exclude_non_model_field(self):\n    \"\"\"\n    Regression test for #12901 -- ModelAdmin.exclude can contain non-model fields\n    that are present in the form class.\n    \"\"\"\n    class SongForm(forms.ModelForm):\n        extra_data = forms.CharField()\n\n        class Meta:\n            model = Song\n            fields = '__all__'\n\n    class ExcludeNonModelFieldAdmin(admin.ModelAdmin):\n        form = SongForm\n        exclude = ['extra_data']\n\n    errors = ExcludeNonModelFieldAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_readonly_fields_with_callable(self):\n    class SongAdmin(admin.ModelAdmin):\n        readonly_fields = (lambda obj: \"test\" if obj.title == \"Test Song\" else \"\",)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_list_filter_works_on_foreign_key_even_when_apps_not_ready(self):\n    \"\"\"\n    Ensure list_filter can access foreign key fields even when the app registry\n    is not ready; refs #24146.\n    \"\"\"\n    class SongAdminWithListFilter(admin.ModelAdmin):\n        list_filter = ['album__title']\n\n    # Temporarily pretending apps are not ready yet. This issue can happen\n    # if the value of 'list_filter' refers to a 'foreign_key__field'.\n    Album._meta.apps.ready = False\n    try:\n        errors = SongAdminWithListFilter(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n    finally:\n        Album._meta.apps.ready = True\n", "def test_foreignkey_unique(self):\n    class SongInline(admin.TabularInline):\n        model = Song\n        extra = 1\n        max_num = 2\n\n    class AlbumAdmin(admin.ModelAdmin):\n        inlines = [SongInline]\n\n    errors = AlbumAdmin(Album, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'max_num' cannot be greater than 1 on an inline \"\n            \"model that has a unique ForeignKey to its parent model.\",\n            obj=SongInline,\n            id='admin.E203',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_124": ["def test_default_renderer_inheritance(self):\n    class ParentForm(Form):\n        default_renderer = CustomRenderer()\n\n    class ChildForm(ParentForm):\n        pass\n\n    form = ChildForm()\n    self.assertEqual(form.renderer, ParentForm.default_renderer)\n", "    def test_field_deep_copy_widget(self):\n        class CustomWidget(TextInput):\n            pass\n\n        field = CharField(widget=CustomWidget())\n        field_copy = copy.deepcopy(field)\n        self.assertIsInstance(field_copy, CharField)\n        self.assertIsInstance(field_copy.widget, CustomWidget)\n        self.assertIsNot(field_copy.widget, field.widget)\n", "def test_prefix_with_empty_form(self):\n    class EmptyForm(Form):\n        pass\n\n    form = EmptyForm(prefix='test')\n    self.assertEqual(form.prefix, 'test')\n    self.assertEqual(form.add_prefix('field'), 'test-field')\n", "def test_form_rendering_with_custom_renderer(self):\n    class CustomForm(Form):\n        field1 = CharField(max_length=50)\n        field2 = IntegerField()\n\n    class CustomRenderer(DjangoTemplates):\n            return f\"Custom rendered form: {str(context['form'])}\"\n\n    form = CustomForm()\n    form.renderer = CustomRenderer()\n    rendered = form.render()\n    self.assertEqual(rendered, \"Custom rendered form: <tr><th><label for=\\\"id_field1\\\">Field1:</label></th>\"\n                               \"<td><input type=\\\"text\\\" name=\\\"field1\\\" maxlength=\\\"50\\\" required id=\\\"id_field1\\\">\"\n                               \"</td></tr>\\n<tr><th><label for=\\\"id_field2\\\">Field2:</label></th>\"\n                               \"<td><input type=\\\"number\\\" name=\\\"field2\\\" required id=\\\"id_field2\\\"></td></tr>\")\n", "    def test_custom_widget_attributes(self):\n        class CustomTextInput(TextInput):\n                custom_attrs = {'class': 'custom-class', 'data-attr': 'custom-value'}\n                if attrs is not None:\n                    custom_attrs.update(attrs)\n                super().__init__(attrs=custom_attrs)\n\n        class CustomForm(Form):\n            custom_field = CharField(widget=CustomTextInput())\n\n        form = CustomForm()\n        self.assertIn('class=\"custom-class\"', str(form['custom_field']))\n        self.assertIn('data-attr=\"custom-value\"', str(form['custom_field']))\n\n        form = CustomForm(auto_id='custom_id_%s')\n        self.assertIn('id=\"custom_id_custom_field\"', str(form['custom_field']))\n"], "sample_1011": ["def test_DiracDelta_printing():\n    assert octave_code(DiracDelta(x, y)) == 'dirac(y, x)'\n    assert octave_code(DiracDelta(x)) == 'dirac(x)'\n", "def test_MatrixSlicing_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert mcode(A[:2, 1:]) == \"A(1:2, 2:end)\"\n    assert mcode(A[1:, :2]) == \"A(2:end, 1:2)\"\n    assert mcode(A[::2, ::2]) == \"A(1:2:end, 1:2:end)\"\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 4)\n    assert mcode(A[1:3, 0:2]) == \"A(2:3, 1:2)\"\n    assert mcode(A[1:, 0:2]) == \"A(2:end, 1:2)\"\n    assert mcode(A[:, 0:2]) == \"A(:, 1:2)\"\n    assert mcode(A[1:3, :]) == \"A(2:3, :)\"\n    assert mcode(A[:, :]) == \"A(:, :)\"\n    assert mcode(A[0, :]) == \"A(1, :)\"\n    assert mcode(A[:, 0]) == \"A(:, 1)\"\n    assert mcode(A[0, 0]) == \"A(1, 1)\"\n    assert mcode(A[::2, ::2]) == \"A(1:2:end, 1:2:end)\"\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 4)\n    B = MatrixSymbol(\"B\", 3, 4)\n\n    assert mcode(A[1:2, 0:3]) == \"A(2:2, 1:3)\"\n    assert mcode(A[1:2, :]) == \"A(2:2, :)\"\n    assert mcode(A[1:, 0:3]) == \"A(2:end, 1:3)\"\n    assert mcode(A[1:, :]) == \"A(2:end, :)\"\n    assert mcode(A[:, 0:3]) == \"A(:, 1:3)\"\n    assert mcode(A[:, :]) == \"A(:, :)\"\n\n    F = B[1:2, 0:3].subs(B, A - A)\n    assert mcode(F) == \"zeros(1, 3)\"\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 4)\n    B = MatrixSymbol(\"B\", 5, 5)\n    assert mcode(A[:2, :3]) == \"A(1:2, 1:3)\"\n    assert mcode(A[:, :]) == \"A(:, :)\"\n    assert mcode(A[:3, 1:]) == \"A(1:3, :)\"\n    assert mcode(A[2:, :2]) == \"A(3:end, 1:2)\"\n    assert mcode(B[::2, ::2]) == \"B(1:2:end, 1:2:end)\"\n"], "sample_186": ["def test_valid_date_hierarchy(self):\n    class SongAdmin(admin.ModelAdmin):\n        date_hierarchy = 'release_date'\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_list_display_invalid_attribute(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = ('title', 'invalid_attribute')\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_display[1]' refers to 'invalid_attribute', which is not a \"\n            \"callable, an attribute of 'SongAdmin', or an attribute or method on 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E108',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_display_links_valid_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = [\"pk\", \"title\"]\n        list_display_links = [\"title\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_date_hierarchy_valid_date_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        date_hierarchy = 'release_date'\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_check_date_hierarchy_with_invalid_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        date_hierarchy = \"invalid_field\"\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'date_hierarchy' refers to 'invalid_field', which does not refer to a Field.\",\n            obj=SongAdmin,\n            id='admin.E127',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_409": ["def test_trimmed_option(self, tag_name):\n    output = self.engine.render_to_string(\"template\")\n    self.assertEqual(output, \"Trimmed text\")\n", "def test_percent_formatting_in_blocktranslate_singular(self, tag_name):\n    with translation.override(\"de\"):\n        output = self.engine.render_to_string(\"template\", {\"percent\": 42})\n        self.assertEqual(output, \"42%\")\n", "def test_blocktranslate_with_context(self):\n    output = self.engine.render_to_string(\"template\")\n    self.assertEqual(output, \"Translated context string\")\n", "def test_context_is_str(self, tag_name):\n    with self.assertRaisesMessage(\n        TemplateSyntaxError,\n        \"Invalid argument '%s' provided to the '%s' tag for the context option\"\n        % (\"greeting\", tag_name),\n    ):\n        self.engine.render_to_string(\"template\", {\"message_context\": \"greeting\"})\n", "def test_i18n42(self):\n    output = self.engine.render_to_string(\"template\")\n    if self.engine.string_if_invalid:\n        self.assertEqual(output, \"INVALID\")\n    else:\n        self.assertEqual(output, \"\")\n"], "sample_709": ["def test_pytester_makefile_creates_directories(pytester: Pytester) -> None:\n    p1 = pytester.makepyfile(**{\"subdir/test\": \"\"})\n    assert str(p1) == str(pytester.path / \"subdir/test.py\")\n    assert (pytester.path / \"subdir\").is_dir()\n", "def test_pytester_subprocess_custom_command(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\"def test_custom_command(): pass\")\n\n    class CustomPytester(Pytester):\n            return sys.executable, \"-m\", \"custom_pytest_module\"\n\n    custom_pytester = CustomPytester(pytester._request, pytester._tmp_path_factory)\n    assert custom_pytester.runpytest_subprocess(testfile).ret == 0\n", "def test_pytester_makefile_multiple_args(pytester: Pytester) -> None:\n    p1 = pytester.makefile(\".txt\", \"line1\", \"line2\")\n    with open(p1, \"r\") as f:\n        content = f.read()\n    assert content == \"line1\\nline2\"\n", "def test_pytester_outcomes_warnings(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, warnings=0)\n", "def test_pytester_outcomes_with_no_tests(pytester: Pytester) -> None:\n    p1 = pytester.makepyfile(\"\")  # empty file\n    result = pytester.runpytest(str(p1))\n    result.assert_outcomes(passed=0)\n    assert result.parseoutcomes() == {\"passed\": 0}\n"], "sample_362": ["    def test_add_custom_fk_with_db_column(self):\n        class CustomFK(models.ForeignKey):\n                kwargs['db_column'] = 'custom_column'\n                super().__init__(*args, **kwargs)\n\n                name, path, args, kwargs = super().deconstruct()\n                del kwargs['db_column']\n                return name, path, args, kwargs\n\n        book_custom_fk_db_column = ModelState('testapp', 'Book', [\n            ('author', CustomFK('testapp.Author', on_delete=models.CASCADE)),\n        ])\n        changes = self.get_changes(\n            [self.author_empty],\n            [self.author_empty, book_custom_fk_db_column],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book')\n", "def test_add_model_with_field_removed_from_base_model_multiple_inheritance(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name, even in the presence of multiple\n    inheritance.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'bookable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('isbn', models.CharField(max_length=13)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'bookable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('isbn', models.CharField(max_length=13)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n            ('isbn', models.CharField(max_length=13)),\n        ], bases=('app.readable', 'app.bookable')),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_add_model_with_index_together_and_index_constraint_on_unmanaged_model(self):\n    author_unmanaged_with_index = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ], options={\n        'managed': False,\n        'index_together': {('name',)},\n        'indexes': [\n            models.Index(fields=['name'], name='author_name_idx'),\n        ],\n    })\n    changes = self.get_changes([], [author_unmanaged_with_index])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AddIndex'])\n    self.assertOperationAttributes(\n        changes, 'testapp', 0, 0, name='Author',\n        options={'managed': False, 'index_together': {('name',)}},\n    )\n    self.assertEqual(\n        changes['testapp'][0].operations[1].index,\n        models.Index(fields=['name'], name='author_name_idx'),\n    )\n", "def test_add_model_with_custom_base(self):\n    \"\"\"\n    Creating a model with a custom base model should correctly detect the base\n    model's fields.\n    \"\"\"\n    class CustomBase(models.Model):\n        base_field = models.CharField(max_length=100)\n\n        class Meta:\n            abstract = True\n\n    before = []\n    after = [\n        ModelState('app', 'custombase', [\n            ('id', models.AutoField(primary_key=True)),\n            ('base_field', models.CharField(max_length=100)),\n        ], options={'abstract': True}),\n        ModelState('app', 'childmodel', [\n            ('child_field', models.IntegerField()),\n        ], bases=('app.custombase',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='childmodel')\n    self.assertOperationFieldAttributes(changes, 'app', 0, 0, max_length=100)\n", "    def test_remove_field_with_fk_constraint(self):\n        changes = self.get_changes(\n            [self.author_with_book, self.book_with_author_fk],\n            [self.author_with_book, self.book_with_no_author_fk],\n        )\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='author')\n        field = changes['otherapp'][0].operations[0].field\n        self.assertEqual(field.remote_field.on_delete, models.DO_NOTHING)\n        self.assertEqual(field.remote_field.on_delete_source, models.CASCADE)\n"], "sample_659": ["def test_raises_function_with_match(self):\n    msg = r\"invalid literal for int\\(\\) with base 10: 'xyz'\"\n    with pytest.raises(ValueError, match=msg):\n        pytest.raises(ValueError, int, 'xyz')\n", "def test_raises_exception_looks_iterable_with_match(self):\n    from six import add_metaclass\n\n    class Meta(type(object)):\n            return 1 / 0\n\n            return 1\n\n    @add_metaclass(Meta)\n    class ClassLooksIterableException(Exception):\n            return \"ClassLooksIterableException message\"\n\n    match = \"ClassLooksIterableException message\"\n    with pytest.raises(ClassLooksIterableException, match=match):\n        raise ClassLooksIterableException()\n", "def test_raises_exception_info_match(self):\n    msg = \"invalid literal for int\\\\(\\\\) with base 10: 'asdf'\"\n    with pytest.raises(ValueError) as excinfo:\n        int(\"asdf\")\n    excinfo.match(msg)\n\n    msg = \"with base 16\"\n    expr = \"Pattern '{msg}' not found in {exc_msg}\".format(msg=msg, exc_msg=str(excinfo.value))\n    with pytest.raises(AssertionError, match=expr):\n        excinfo.match(msg)\n", "def test_raises_match_unicode_vs_bytes(self):\n    message = u\"Unicode snowman: \\u2603\"\n    match = u\"Unicode snowman: \\u2603\"\n    with pytest.raises(ValueError, match=match):\n        raise ValueError(message)\n\n    message = u\"Unicode snowman: \\u2603\"\n    match = message.encode(\"UTF-8\")\n    if six.PY2:\n        with pytest.raises(ValueError, match=match):\n            raise ValueError(message)\n", "def test_raises_match_unicode(self):\n    msg = u\"with base 16\"\n    with pytest.raises(ValueError, match=msg):\n        int(u\"asdf\")\n"], "sample_74": ["def test_no_user(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'host': 'somehost',\n            'port': '444',\n        }), (\n            ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n            {},\n        )\n    )\n", "def test_no_host_port(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n        }), (\n            ['psql', '-U', 'someuser', 'dbname'],\n            {},\n        )\n    )\n", "def test_no_user(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'host': 'somehost',\n            'port': '444',\n        }), (\n            ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n            {},\n        )\n    )\n", "def test_no_host(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'port': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-p', '444', 'dbname'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_no_host(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'port': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-p', '444', 'dbname'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n"], "sample_1180": ["def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0, -15)) == [-12, 0, -15]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [-1, -1, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0, -15)) == [-12, 0, -15]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0, -15)) == [-12, 0, -15]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_project():\n    assert Point.project((1, 1), (1, 0)) == Point(1, 0)\n    assert Point.project((2, 2), (1, 1)) == Point(2, 2)\n    assert Point.project((0, 0), (1, 1)) == Point(0, 0)\n    assert Point.project((1, 2), (2, 4)) == Point(1, 2)\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, 0, -15]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n"], "sample_385": ["    def test_render_multiple_options(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        album1 = Album.objects.create(name=\"Album 1\", band=beatles)\n        album2 = Album.objects.create(name=\"Album 2\", band=who)\n        form = AlbumForm(initial={\"featuring\": [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option1 = (\n            '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        )\n        selected_option2 = '<option value=\"%s\" selected>The Who</option>' % who.pk\n        self.assertIn(selected_option1, output)\n        self.assertIn(selected_option2, output)\n", "def test_render_options_multiple_selected(self):\n    \"\"\"Multiple selected options are correctly marked as selected.\"\"\"\n    beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n    who = Band.objects.create(name=\"The Who\", style=\"rock\")\n    beatles_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    who_option = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    form = AlbumForm(initial={\"featuring\": [beatles.pk, who.pk]})\n    output = form.as_table()\n    self.assertIn(beatles_option, output)\n    self.assertIn(who_option, output)\n", "def test_render_options_with_multiple_selected_values(self):\n    \"\"\"Multiple selected values are present in the rendered output.\"\"\"\n    beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n    who = Band.objects.create(name=\"The Who\", style=\"rock\")\n    form = NotRequiredBandForm(initial={\"band\": [beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option_beatles = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    selected_option_who = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    self.assertIn(selected_option_beatles, output)\n    self.assertIn(selected_option_who, output)\n", "def test_render_options_initial_not_in_queryset(self):\n    # Create a band that is not in the queryset\n    unrelated_band = Band.objects.create(name=\"Unrelated Band\", style=\"pop\")\n    # Use the unrelated band as initial value\n    form = NotRequiredBandForm(initial={\"band\": unrelated_band})\n    output = form.as_table()\n    # The output should not contain any option for the unrelated band\n    unrelated_option = f'<option value=\"{unrelated_band.pk}\">Unrelated Band</option>'\n    self.assertNotIn(unrelated_option, output)\n", "    def test_render_options_empty_queryset(self):\n        \"\"\"Empty queryset produces empty options.\"\"\"\n        form = EmptyQuerysetForm()\n        output = form.as_table()\n        self.assertNotIn(self.empty_option, output)\n"], "sample_631": ["def test_no_name_in_module(self):\n    \"\"\"Make sure that 'import ...' does not emit a 'no-name-in-module'\n    for a missing submodule.\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        import missing_submodule\n        \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"no-name-in-module\", node=node, args=(\"missing_submodule\", \"\"))\n    ):\n        self.checker.visit_import(node)\n", "def test_typing_import_unused(self):\n    \"\"\"Make sure that 'from typing import ...' does not emit a\n    'unused-import' message.\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    from typing import List\n        l: List[int] = [1, 2, 3]\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_importfrom(node)\n        self.checker.visit_functiondef(node[\"func\"])\n        self.checker.leave_functiondef(node[\"func\"])\n", "def test_assignment_in_function_annotation(self):\n    \"\"\"Make sure assignments in function annotations are not marked as unused variables.\n\n    https://github.com/PyCQA/pylint/issues/5234\n    \"\"\"\n    node = astroid.parse(\n        \"\"\"\n        pass\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.walk(node)\n", "def test_undefined_variable_in_lambda(self):\n    \"\"\"Make sure undefined variables in lambda are reported correctly.\n\n    https://github.com/PyCQA/pylint/issues/4530\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        x = lambda: undefined_variable  # @\n    \"\"\"\n    )\n    msg = Message(\"undefined-variable\", node=node[\"undefined_variable\"], args=\"undefined_variable\")\n    with self.assertAddsMessages(msg):\n        self.checker.visit_functiondef(node)\n        self.checker.leave_functiondef(node)\n", "def test_dummy_variable_regexp_config(self):\n    node = astroid.parse(\n        \"\"\"\n        pass\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.walk(node)\n"], "sample_919": ["def test_xref_consistency_cpp_role(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_xref_consistency_cpp_specific_roles(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n    cpp_roles = ['class', 'struct', 'union', 'func', 'member', 'var', 'type', 'concept', 'enum', 'enumerator']\n\n    for role in cpp_roles:\n        expect = f'`cpp:{role}` uses the `cpp-{role}` class'.format(role=role)\n        assert {'xref', f'cpp-{role}', 'cpp'} <= classes(role, 'a'), expect\n", "def test_pointer_to_member():\n    check('function', 'void f(int A::* p)', {2: '1fM1Ci'})\n    check('function', 'void f(int (A::* p)(float, double))', {2: '1fM1CFifdE'})\n    check('function', 'int A::* f(int, double)', {2: '1fid'})\n    check('function', 'void f(int A::* *p)', {2: '1fPM1Ci'})\n    check('function', 'void f(int A::*const *p)', {2: '1fPKM1Ci'})\n\n    check('function', 'void f(int A::*)', {2: '1fM1Ci'})\n    check('function', 'void f(int A::*const)', {2: '1fKM1Ci'})\n    check('function', 'void f(int A::*const&)', {2: '1fRKM1Ci'})\n    check('function', 'void f(int A::*volatile)', {2: '1fVM1Ci'})\n    check('function', 'void f(int A::*const volatile)', {2: '1fVKM1Ci'}, output='void f(int A::*volatile const)')\n    check('function', 'void f(int A::*volatile const)', {2: '1fVKM1Ci'})\n\n    check('function', 'void f(int A::**)', {2: '1fPM1Ci'})\n    check('function', 'void f(int A::*const*)', {2: '1fPKM1Ci'})\n", "def test_xref_roles_consistency():\n    role_classes = {}\n\n        classes = classes(role, tag)\n        role_classes[role] = {'root': classes}\n        for content_tag in content_tags:\n            role_classes[role][content_tag] = classes(role, content_tag)\n\n    check_role('cpp-class', 'a', ['span'])\n    check_role('cpp-struct', 'a', ['span'])\n    check_role('cpp-union', 'a', ['span'])\n    check_role('cpp-func', 'a', ['code'])\n    check_role('cpp-member', 'a', ['span'])\n    check_role('cpp-var', 'a', ['span'])\n    check_role('cpp-type', 'a', ['code'])\n    check_role('cpp-concept', 'a', ['span'])\n    check_role('cpp-enum', 'a', ['span'])\n    check_role('cpp-enumerator', 'a', ['span'])\n\n    # Check that all xref roles use the same reference classes\n    for role, classes in role_classes.items():\n        assert classes['root'] == classes['a'] or classes['root'] == classes['code'], f'Inconsistent reference classes for role `{role}`'\n", "def test_anon_name_generation():\n    # Check if anon name generation works correctly\n    check('class', '{{key}}A', {3: \"Ut1_a\"})\n    check('class', '{{key}}A::B', {3: \"NUt1_a1BE\"})\n    check('class', '{{key}}A::B::C', {3: \"NUt1_a1B1CE\"})\n    check('class', '{{key}}A<T>', {3: \"IE1AIT\"})\n    check('class', '{{key}}A<T>::B', {3: \"NIE1AIT1BE\"})\n    check('class', '{{key}}A<T>::B::C', {3: \"NIE1AIT1B1CE\"})\n    check('function', '{{key}}A()', {3: \"1a\"})\n    check('function', '{{key}}A::B()', {3: \"N1A1b\"})\n    check('function', '{{key}}A::B::C()', {3: \"N1A1B1c\"})\n    check('function', '{{key}}A<T>()', {3: \"IA1TE\"})\n    check('function', '{{key}}A<T>::B()', {3: \"NIA1TE1b\"})\n    check('function', '{{key}}A<T>::B::C()', {3: \"NIA1TE1B1c\"})\n"], "sample_967": ["def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'http://example.com/mathjax.js' in content\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://example.com/mathjax.js' in content\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'http://example.com/mathjax.js' in content\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://example.com/mathjax.js' in content\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'src=\"custom/mathjax/path\"' in content\n"], "sample_318": ["def test_include_3_tuple_app_name(self):\n    self.assertEqual(\n        include((self.url_patterns, 'app_name', None)),\n        (self.url_patterns, 'app_name', 'app_name')\n    )\n", "    def test_resolver_match_repr(self):\n        match = resolve('/no_kwargs/42/37/')\n        expected_repr = (\n            \"ResolverMatch(func=urlpatterns_reverse.views.empty_view, \"\n            \"args=('42', '37'), kwargs={}, url_name='no-kwargs', \"\n            \"app_names=[], namespaces=[], \"\n            \"route='^no_kwargs/([0-9]+)/([0-9]+)/$')\"\n        )\n        self.assertEqual(repr(match), expected_repr)\n", "    def test_include_app_name_app_urls(self):\n        app_urls = URLObject('inc-app-urls')\n        self.assertEqual(\n            include(app_urls, app_name='app_name'),\n            (app_urls, 'app_name', 'inc-app-urls')\n        )\n", "    def test_view_class(self):\n        test_urls = [\n            ('view-class', [], {}, views.view_class_instance),\n            ('view-class', [37, 42], {}, views.view_class_instance),\n            ('view-class', [], {'arg1': 42, 'arg2': 37}, views.view_class_instance),\n        ]\n        for name, args, kwargs, expected in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs):\n                match = resolve(reverse(name, args=args, kwargs=kwargs))\n                self.assertIsInstance(match.func, views.ViewClass)\n                self.assertEqual(match.func, expected)\n", "def test_resolve_with_args_and_kwargs(self):\n    test_urls = [\n        ('mixed-args-kwargs', [], {'arg1': '42', 'arg2': '37'}, '/mixed_args/42/37/'),\n        ('mixed-args-kwargs', ['42'], {'arg2': '37'}, '/mixed_args/42/37/'),\n        ('mixed-args-kwargs', ['42', '37'], {}, '/mixed_args/42/37/'),\n    ]\n    for name, args, kwargs, expected in test_urls:\n        with self.subTest(name=name, args=args, kwargs=kwargs):\n            self.assertEqual(reverse(name, args=args, kwargs=kwargs), expected)\n            match = resolve(expected)\n            self.assertEqual(match.kwargs, kwargs)\n            self.assertEqual(match.args, args)\n"], "sample_555": ["def test_default_capstyle_joinstyle():\n    patch = Patch()\n    patch.set_capstyle('round')\n    patch.set_joinstyle('round')\n    assert patch.get_capstyle() == 'round'\n    assert patch.get_joinstyle() == 'round'\n    patch.set_capstyle(None)\n    patch.set_joinstyle(None)\n    assert patch.get_capstyle() == 'butt'\n    assert patch.get_joinstyle() == 'miter'\n", "def test_arc_str():\n    arc = Arc((1, 2), 3, 4, angle=5, theta1=6, theta2=7)\n    assert str(arc) == 'Arc(xy=(1, 2), width=3, height=4, angle=5, theta1=6, theta2=7)'\n", "def test_arc_angle():\n    arc = Arc([.5, .5], .5, 1, theta1=0, theta2=90, angle=20)\n    assert arc.get_angle() == 20\n    arc.set_angle(30)\n    assert arc.get_angle() == 30\n", "def test_large_arc_2():\n    fig, ax = plt.subplots()\n    x = 210\n    y = -2115\n    diameter = 4261\n    a = Arc((x, y), diameter, diameter, lw=2, color='k')\n    ax.add_patch(a)\n    ax.set_axis_off()\n    ax.set_aspect('equal')\n    ax.set_xlim(-25000, 18000)\n    ax.set_ylim(-20000, 6600)\n", "def test_circle_polygon_with_resolution_1():\n    fig, ax = plt.subplots()\n    circle = CirclePolygon((0.5, 0.5), 0.5, resolution=1)\n    ax.add_patch(circle)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.set_aspect('equal')\n"], "sample_975": ["def test_nsolve_with_complex_numbers():\n    x = Symbol('x')\n    f = x**2 + 1\n    sol = nsolve(f, x, 1j)\n    assert abs(sol - 1j) < 1e-15\n", "def test_nsolve_system():\n    x, y = symbols('x y')\n    f1 = x**2 - y\n    f2 = y**2 - x\n    sol = nsolve((f1, f2), (x, y), (1, 1))\n    assert abs(sol[0] - sol[1]) < 1e-10\n", "def test_nsolve_complex():\n    x = Symbol('x')\n    sol = nsolve(x**2 + 1, x, 1 + 1j, prec=128)\n    assert abs(1j.evalf(128) - sol) < 1e-128\n    assert isinstance(sol, Complex)\n", "def test_nsolve_with_symbols():\n    x, y = symbols('x y')\n    f = sin(x) + cos(y)\n    solution = nsolve(f, (x, y), (1, 1))\n    assert isinstance(solution, Matrix)\n    assert solution.shape == (2, 1)\n    assert abs(f.subs({x: solution[0], y: solution[1]}).evalf()) < 1e-15\n", "def test_nsolve_options():\n    x = Symbol('x')\n    sol = nsolve(sin(x), x, 2, solver='muller')\n    assert abs(pi.evalf() - sol) < 1e-15\n\n    sol = nsolve(sin(x), x, 2, solver='halley')\n    assert abs(pi.evalf() - sol) < 1e-15\n\n    sol = nsolve(sin(x), x, 2, solver='anderson')\n    assert abs(pi.evalf() - sol) < 1e-15\n"], "sample_194": ["def test_no_fields_provided(self):\n    msg = 'At least one field is required to define a unique constraint.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(name='uniq', fields=[])\n", "def test_unique_constraint_without_fields(self):\n    msg = 'At least one field is required to define a unique constraint.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(name='uniq_no_fields', fields=[])\n", "def test_opclasses_database_constraint(self):\n    UniqueConstraintProduct.objects.create(name='p1', color='red')\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name='p1', color='Red')\n\n    constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\n    opclasses = constraints['name_color_opclasses'].get('opclasses')\n    self.assertEqual(opclasses, ['text_pattern_ops', 'varchar_pattern_ops'])\n", "    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(name='p1', color='red')\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', color='Red')\n", "def test_deferred_unique_constraint_behavior(self):\n    UniqueConstraintDeferrable.objects.create(name='p1', shelf='front')\n    obj_2 = UniqueConstraintDeferrable.objects.create(name='p2', shelf='back')\n\n        obj_1 = UniqueConstraintDeferrable.objects.get(name='p1')\n        obj_1.shelf, obj_2.shelf = obj_2.shelf, obj_1.shelf\n        obj_1.save()\n        obj_2.save()\n\n    swap_shelves()\n    # No error should be raised as the constraint is deferred.\n    self.assertEqual(UniqueConstraintDeferrable.objects.count(), 2)\n\n    # Behavior can be changed with SET CONSTRAINTS.\n    with self.assertRaises(IntegrityError), atomic(), connection.cursor() as cursor:\n        constraint_name = connection.ops.quote_name('name_init_deferred_uniq')\n        cursor.execute('SET CONSTRAINTS %s IMMEDIATE' % constraint_name)\n        swap_shelves()\n"], "sample_236": ["def test_fast_delete_with_select_related(self):\n    u = User.objects.create(avatar=Avatar.objects.create())\n    u_with_avatar = User.objects.select_related('avatar').get(pk=u.pk)\n    # With select_related, it should not be fast deletable.\n    collector = Collector(using='default')\n    self.assertFalse(collector.can_fast_delete(u_with_avatar))\n    # Deletion should still work correctly.\n    u_with_avatar.delete()\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n", "def test_fast_delete_no_signal_listeners(self):\n    # Fast-deleting should be possible if no signal listeners are connected.\n    u = User.objects.create(\n        avatar=Avatar.objects.create()\n    )\n    a = Avatar.objects.get(pk=u.avatar_id)\n    # Attach a signal to make sure we will not do fast_deletes.\n    calls = []\n\n        calls.append('')\n\n    models.signals.post_delete.connect(noop, sender=User)\n\n    # 1 query to delete the user\n    # 1 query to delete the avatar\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    self.assertEqual(len(calls), 1)\n\n    models.signals.post_delete.disconnect(noop, sender=User)\n", "def test_fast_delete_combined_relationships_multiple(self):\n    # The cascading fast-delete of SecondReferrer should be combined\n    # in a single DELETE WHERE referrer_id OR unique_field for multiple objects.\n    origin = Origin.objects.create()\n    referer1 = Referrer.objects.create(origin=origin, unique_field=42)\n    referer2 = Referrer.objects.create(origin=origin, unique_field=43)\n    with self.assertNumQueries(2):\n        Referrer.objects.filter(id__in=[referer1.id, referer2.id]).delete()\n", "def test_fast_delete_related_with_signals(self):\n    # Attach a signal to make sure we will not do fast_deletes.\n    calls = []\n\n        calls.append('')\n\n    models.signals.post_delete.connect(noop, sender=User)\n\n    u = User.objects.create(\n        avatar=Avatar.objects.create()\n    )\n    a = Avatar.objects.get(pk=u.avatar_id)\n    # Even though we can defer constraint checks, we can't do fast deletes\n    # because of the signal.\n    self.assertNumQueries(4, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    self.assertEqual(len(calls), 1)\n    models.signals.post_delete.disconnect(noop, sender=User)\n", "def test_fast_delete_non_empty_no_update_can_self_select(self):\n    \"\"\"\n    Fast deleting on backends that don't have the `no_update_can_self_select`\n    feature should work even if the specified filter matches some rows and not\n    others.\n    \"\"\"\n    User.objects.create()\n    User.objects.create(avatar=Avatar.objects.create(desc='a'))\n    with self.assertNumQueries(2):\n        self.assertEqual(\n            User.objects.filter(avatar__desc='a').delete(),\n            (1, {'delete.User': 1}),\n        )\n    self.assertEqual(User.objects.count(), 1)\n    self.assertIs(Avatar.objects.exists(), False)\n"], "sample_443": ["def test_invalid_with_version_key_length_memcached(self):\n    # make_key() adds a version to the key and exceeds the limit.\n    key = \"a\" * 248\n    expected_warning = (\n        \"Cache key will cause errors if used with memcached: \"\n        \"%r (longer than %s)\" % (key, 250)\n    )\n    self._perform_invalid_key_test(key, expected_warning)\n", "def test_invalid_key_length(self):\n    key = \"a\" * 251\n    expected_warning = (\n        \"Cache key will cause errors if used with memcached: \"\n        \"%r (longer than %s)\" % (key, 250)\n    )\n    self._perform_invalid_key_test(key, expected_warning)\n", "def test_cache_page_no_cache(self):\n    # A view decorated with no_cache should not be cached.\n    view = no_cache(hello_world_view)\n    request = self.factory.get(\"/view/\")\n    response = view(request, \"1\")\n    self.assertEqual(response.content, b\"Hello World 1\")\n    response = view(request, \"2\")\n    self.assertEqual(response.content, b\"Hello World 2\")\n", "    def test_patch_cache_control_no_existing_header(self):\n        response = HttpResponse()\n        patch_cache_control(response, private=True)\n        self.assertEqual(response.headers[\"Cache-Control\"], \"private\")\n", "def test_patch_cache_control_no_existing_headers(self):\n    response = HttpResponse()\n    patch_cache_control(response, private=True)\n    self.assertEqual(response.headers['Cache-Control'], 'private')\n"], "sample_212": ["    def test_session_middleware(self):\n        request = HttpRequest()\n        middleware = SessionMiddleware(lambda request: HttpResponse())\n        middleware.process_request(request)\n        self.assertIsNotNone(request.session)\n", "    def setUp(self):\n        self.middleware = SessionMiddleware()\n        self.request = HttpRequest()\n", "    def setUp(self):\n        self.middleware = SessionMiddleware(lambda req: HttpResponse())\n        self.request = HttpRequest()\n        self.request.COOKIES = {}\n", "    def setUp(self):\n        self.middleware = SessionMiddleware(get_response=lambda r: HttpResponse())\n        self.request = HttpRequest()\n", "def test_session_middleware_process_response(self):\n    \"\"\"\n    Test that the SessionMiddleware's process_response method correctly sets\n    and deletes session cookies based on session modification and expiry.\n    \"\"\"\n    request = HttpRequest()\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n    request.session = self.SessionStore('test_session_key')\n    request.session['test_key'] = 'test_value'\n    response = HttpResponse()\n    middleware = SessionMiddleware(lambda req: response)\n    middleware.process_response(request, response)\n\n    self.assertTrue(settings.SESSION_COOKIE_NAME in response.cookies)\n    self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME].value, 'test_session_key')\n\n    # Test session deletion\n    request.session.flush()\n    middleware.process_response(request, response)\n    self.assertTrue(response.cookies[settings.SESSION_COOKIE_NAME]['max-age'] == 0)\n"], "sample_297": ["    def test_ticket_23622_different_fields(self):\n        \"\"\"\n        Make sure __pk__in and __in work the same for related fields when\n        using a distinct on subquery with different fields.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        qx = (\n            Q(ticket23605b__pk__in=Ticket23605B.objects.order_by('modela_fk', '-field_b0').distinct('modela_fk')) &\n            Q(ticket23605b__field_b1=True)\n        )\n        qy = (\n            Q(ticket23605b__in=Ticket23605B.objects.order_by('modela_fk', '-field_b0').distinct('modela_fk')) &\n            Q(ticket23605b__field_b1=True)\n        )\n        self.assertEqual(\n            set(Ticket23605A.objects", "    def test_ticket_24605_subquery_annotation(self):\n        \"\"\"\n        Subquery annotations should be quoted correctly.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        qs = Individual.objects.annotate(\n            related_count=Subquery(\n                RelatedIndividual.objects.filter(related=OuterRef('pk')).values('pk').annotate(count=Count('pk')).values('count'),\n                output_field=IntegerField(),\n            )\n        )\n        self.assertEqual(qs.get(pk=i1).related_count, 1)\n        self.assertEqual(qs.get(pk=i2).related_count, 1)\n        self.assertEqual(qs.get(pk=i3).related_count, 0)\n        self.assertEqual(qs.get(pk=i4).related_count, 0)\n", "    def test_ticket_24785(self):\n        \"\"\"\n        Make sure __in works with an empty queryset.\n        \"\"\"\n        empty_qs = Annotation.objects.none()\n        a1 = Annotation.objects.create(name='a1')\n        a2 = Annotation.objects.create(name='a2')\n        self.assertSequenceEqual(Annotation.objects.filter(pk__in=empty_qs), [a1, a2])\n", "    def test_ticket_24708(self):\n        \"\"\"\n        __in should work with a non-tuple sequence.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        Ticket23605B.objects.create(modela_fk=a1)\n        Ticket23605B.objects.create(modela_fk=a2)\n\n        class NonTupleSequence:\n                yield a1.pk\n                yield a2.pk\n\n                return 2\n\n        self.assertSequenceEqual(Ticket23605A.objects.filter(pk__in=NonTupleSequence()), [a1, a2])\n", "    def test_ticket_24837(self):\n        \"\"\"\n        Ensure that queryset.query.alias_map is updated when related objects\n        are added to a queryset.\n        \"\"\"\n        # Create a related object that will be added to the queryset\n        related_obj = RelatedIndividual.objects.create(related=Individual.objects.create(alive=True))\n\n        # Create a base queryset\n        base_qs = Individual.objects.all()\n\n        # Ensure that alias_map does not contain the related object\n        self.assertNotIn('related_individual', base_qs.query.alias_map)\n\n        # Add the related object to the queryset\n        qs = base_qs.prefetch_related('related_individual')\n\n        # Ensure that alias_map now contains the related object\n        self.assertIn('related_individual', qs.query.alias_map)\n"], "sample_156": ["def test_add_error(self):\n    class CustomForm(Form):\n        field = CharField()\n\n    form = CustomForm()\n    form.add_error('field', 'Custom error message')\n    self.assertEqual(form.errors['field'], ['Custom error message'])\n", "    def test_renderer_attribute(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n", "    def test_custom_renderer(self):\n        class CustomRenderer(DjangoTemplates):\n            form_template_name = 'custom_form_template.html'\n            field_template_name = 'custom_field_template.html'\n\n        class CustomForm(Form):\n            username = CharField()\n\n        form = CustomForm(renderer=CustomRenderer())\n        self.assertEqual(form.render(), 'Rendered using custom_form_template.html')\n        self.assertEqual(str(form['username']), 'Rendered using custom_field_template.html')\n", "def test_field_with_custom_renderer(self):\n    class CustomRenderer(DjangoTemplates):\n            if template_name == 'django/forms/widgets/custom_widget.html':\n                return Template('Custom widget rendering')\n            return super().get_template(template_name)\n\n    class CustomWidget(Widget):\n        template_name = 'django/forms/widgets/custom_widget.html'\n\n    class CustomForm(Form):\n        custom_field = CharField(widget=CustomWidget())\n\n    form = CustomForm(renderer=CustomRenderer())\n    self.assertEqual(form.render('custom_field'), 'Custom widget rendering')\n", "def test_custom_renderer_in_fields(self):\n    class CustomForm(Form):\n        default_renderer = CustomRenderer()\n        custom_field = CharField(renderer=CustomRenderer())\n\n    form = CustomForm()\n    self.assertIsInstance(form.renderer, CustomForm.default_renderer)\n    self.assertIsInstance(form.fields['custom_field'].widget.renderer, CustomRenderer)\n"], "sample_452": ["def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, limit_choices_to={'field': 'value'}),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "def test_reference_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, limit_choices_to={\"field\": \"value\"}),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n", "def test_references_field_by_related_name(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related\"),\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"related\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n    )\n", "    def test_references_field_by_limit_choices_to(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, limit_choices_to={\"field\": \"value\"}),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n", "    def test_deconstruct_subclass(self):\n        class CustomAddField(AddField):\n                _, path, args, kwargs = super().deconstruct()\n                return ('migrations.CustomAddField', args, kwargs)\n\n        operation = CustomAddField('Model', 'field', models.BooleanField(default=False))\n        _, args, kwargs = operation.deconstruct()\n        self.assertEqual(args, ('Model', 'field', models.BooleanField(default=False)))\n        self.assertEqual(kwargs, {})\n"], "sample_1120": ["def test_matrix_element_symbol_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', n, m)\n    assert A[0, 0].subs(A, B) == B[0, 0]\n    assert A[0, 0].subs(A, C) == C[0, 0]\n    assert A[0, 0].subs(A[0, 0], B[0, 0]) == B[0, 0]\n    assert A[0, 0].subs(A[0, 0], C[0, 0]) == C[0, 0]\n", "def test_matrix_symbol_from_matrix():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A * B\n    new_expr = expr.subs(A, Identity(2))\n    assert new_expr == B\n", "def test_matrix_symbol_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, l)\n\n    assert A.subs(A, B) == B\n    assert A.subs(A, C) != A\n    assert (A*B).subs(B, C) == A*C\n    assert (A*B).subs(A, C) == C*B\n\n    D = MatrixSymbol('D', p, q)\n    assert A.subs(A, D) != A\n", "def test_matrix_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    X = MatrixSymbol('X', n, l)\n\n    # Test derivative of matrix multiplication\n    dAB = diff(A * B, X)\n    assert dAB == ZeroMatrix(n, l)\n\n    # Test derivative of matrix power\n    dAB2 = diff(A ** 2, X)\n    assert dAB2 == ZeroMatrix(n, n)\n\n    # Test derivative of matrix transpose\n    dAT = diff(A.T, X)\n    assert dAT == ZeroMatrix(m, n)\n\n    # Test derivative of matrix inverse\n    dAI = diff(A.inv(), X)\n    assert dAI == ZeroMatrix(n, n)\n\n    # Test derivative of matrix trace\n    dtrA = diff(A.trace(), X)\n    assert dtrA == ZeroMatrix(1, 1)\n", "def test_matrix_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    X = MatrixSymbol('X', n, l)\n\n    # Test derivative of a matrix expression with respect to a matrix\n    assert (A*B).diff(X) == ZeroMatrix(n, l)\n    assert (A*X*B).diff(X) == A*B.T\n    assert (X*A*B).diff(X) == A.T*B\n    assert (X*A*X*B).diff(X) == A.T*X + X*A\n\n    # Test derivative of a matrix expression with respect to a scalar\n    x = Symbol('x')\n    assert (x*A).diff(x) == A\n    assert (A*x).diff(x) == A\n    assert (x*A*x).diff(x) == 2*x*A\n\n    # Test derivative of a matrix expression with respect to itself\n    assert (A*A.T).diff(A) == 2*A\n    assert (A.T*A).diff(A) == 2*A*A.T\n"], "sample_34": ["def test_unit_conversion_with_floats():\n    assert_allclose(u.m.to(u.cm, 2.0), 200.0)\n    assert_allclose(u.cm.to(u.m, 200.0), 2.0)\n    assert_allclose(u.s.to(u.ms, 1.0), 1000.0)\n    assert_allclose(u.ms.to(u.s, 1000.0), 1.0)\n", "def test_unit_conversion_with_equivalencies():\n    with u.set_enabled_equivalencies(u.dimensionless_angles()):\n        angle = 0.5 * u.cycle\n        np.testing.assert_almost_equal(np.exp(1j*angle), 1-1j)\n", "def test_composite_unit_to_string():\n    \"\"\"\n    Test for converting a composite unit to a string.\n\n    Regression test for https://github.com/astropy/astropy/issues/3835\n    \"\"\"\n\n    composite_unit = u.m * u.s\n    string_representation = composite_unit.to_string()\n    assert string_representation == \"m s\"\n", "def test_unit_summary_prefixes_new():\n    \"\"\"\n    Test for a new unit that the unit summary table correctly reports\n    whether or not that unit supports prefixes.\n\n    Added for improving coverage of the existing unit test file.\n    \"\"\"\n\n    new_unit = u.def_unit('new', prefixes=True)\n    for summary in utils._iter_unit_summary(u.__dict__):\n        unit, _, _, _, prefixes = summary\n        if unit.name == 'new':\n            assert prefixes\n            break\n    else:\n        assert False, \"new unit not found in summary\"\n", "def test_unit_summary_format_specifier():\n    \"\"\"\n    Test for a few units that the unit summary table correctly reports\n    the format specifier for that unit.\n\n    Regression test for https://github.com/astropy/astropy/issues/4663\n    \"\"\"\n\n    from .. import astrophys\n\n    for summary in utils._iter_unit_summary(astrophys.__dict__):\n        unit, _, _, _, _, format_specifier = summary\n\n        if unit.name == 'u':\n            assert format_specifier == '\u00b5'\n        elif unit.name == 'angstrom':\n            assert format_specifier == '\u00c5'\n"], "sample_368": ["def test_minimize_rollbacks_forward(self):\n    r\"\"\"\n    Minimize rollbacks when target has multiple in-app parents.\n\n    a: 1 --> 2 --> 3\n    b:     \\- 1 --> 2\n    c:          \\- 1 --> 2\n\n    If b2 and c2 are applied already and a3 is not, and we're asked to migrate to\n    a3, don't unapply b2 or c2, regardless of their current state.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    b2_impl = FakeMigration('b2')\n    b2 = ('b', '2')\n    c1_impl = FakeMigration('c1')\n    c1 = ('c', '1')\n    c2_impl = FakeMigration('c2')\n    c2 = ('c', '2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(b2, b2_impl)\n    graph.add_node(c1, c1_impl)\n    graph.add_node(c2, c2_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a2)\n    graph.add_dependency(None, b2, b1)\n    graph.add_dependency(None, b1, a1)\n    graph.add_dependency(None, b2, a2)\n    graph.add_", "def test_minimize_rollbacks_cycle(self):\n    r\"\"\"\n    Minimize rollbacks when target is in a cycle.\n\n    a: 1 <-- 2 <-- 3 <-- 4\n        ^                 |\n        +-----------------+\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    a4_impl = FakeMigration('a4')\n    a4 = ('a', '4')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(a4, a4_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a2)\n    graph.add_dependency(None, a4, a3)\n    graph.add_dependency(None, a1, a4)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n        a3: a3_impl,\n        a4: a4_impl,\n    })\n\n    plan = executor.migration_plan({a2})\n\n    should_be_rolled_back = [a4_impl, a3_impl]\n    exp = [(m, True) for m in should_be_rolled_back]\n    self.assertEqual(plan, exp)\n", "def test_minimize_rollbacks_forward_to_child(self):\n    r\"\"\"\n    Minimize rollbacks when target is a child of the current migration.\n\n    a: 1 <--- 2\n    b:    \\- 1\n    c:     \\- 1\n\n    If a1 is applied, and we're asked to migrate to a2, don't apply or\n    unapply b1 or c1, regardless of their current state.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    c1_impl = FakeMigration('c1')\n    c1 = ('c', '1')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(c1, c1_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, b1, a1)\n    graph.add_dependency(None, c1, a1)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        b1: b1_impl,\n        c1: c1_impl,\n    })\n\n    plan = executor.migration_plan({a2})\n\n    self.assertEqual(plan, [(a2_impl, False)])\n", "def test_migrate_marks_replacement_unapplied_when_replaced_migrations_are_not_applied(self):\n    \"\"\"\n    A squash migration will be marked as unapplied if not all its replaced\n    migrations are applied (#24628).\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    executor.migrate([('migrations', '0001_squashed_0002')])\n    try:\n        self.assertIn(\n            ('migrations', '0001_squashed_0002'),\n            executor.recorder.applied_migrations(),\n        )\n        executor.recorder.record_unapplied('migrations', '0002_second')\n        executor.loader.build_graph()\n        executor.migrate([('migrations', None)])\n        self.assertNotIn(\n            ('migrations', '0001_squashed_0002'),\n            executor.recorder.applied_migrations(),\n        )\n    finally:\n        executor.recorder.record_applied('migrations', '0002_second')\n        executor.loader.build_graph()\n        executor.migrate([('migrations', None)])\n", "def test_migrate_to_replacement_migration(self):\n    \"\"\"\n    Tests migrating to a replacement migration.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    try:\n        # Make sure the tables are not there before\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n        # Migrate to the replacement migration\n        executor.migrate([('migrations', '0001_squashed_0002')])\n        # Make sure the tables are there now\n        self.assertTableExists('migrations_author')\n        self.assertTableExists('migrations_book')\n    finally:\n        # Unmigrate everything\n        executor = MigrationExecutor(connection)\n        executor.migrate([('migrations', None)])\n        # Make sure the tables are not there\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n"], "sample_994": ["def test_Float_precision():\n    # Make sure Float inputs for keyword args work\n    assert Float(1.0, dps=Integer(15))._prec == 15\n    assert Float(1.0, precision=Integer(15))._prec == 15\n    assert type(Float(1.0, precision=Integer(15))._prec) == int\n    assert sympify(srepr(Float(1.0, precision=15))) == Float(1.0, precision=15)\n", "def test_issue_10020_complex_infinity():\n    assert oo**(1 + I) == S.ComplexInfinity\n    assert oo**(-1 + I) == 0\n    assert (-oo)**I == S.NaN\n    assert (-oo)**(-1 + I) == 0\n", "def test_Float_hash():\n    assert hash(Float(3.14, 2)) == hash(Float(3.14, 2))\n    assert hash(Float(3.14, 2)) != hash(Float(3.14, 3))\n", "def test_Float_round_nearest():\n    assert Float('1.5').round(1) == Float('2.0')\n    assert Float('1.25').round(1) == Float('1.3')\n    assert Float('1.23456').round(3) == Float('1.235')\n    assert Float('1.23456').round(2) == Float('1.23')\n    assert Float('1.23456').round(1) == Float('1.2')\n    assert Float('1.23456').round(0) == Float('1.0')\n    assert Float('-1.23456').round(3) == Float('-1.235')\n    assert Float('-1.23456').round(2) == Float('-1.23')\n    assert Float('-1.23456').round(1) == Float('-1.2')\n    assert Float('-1.23456').round(0) == Float('-1.0')\n", "def test_NumberSymbol_fuzzy_equality():\n    assert fuzzy_not(pi, 3.14) == False\n    assert fuzzy_not(pi, 3.141) == True\n    assert fuzzy_not(pi, 3.1415) == True\n    assert fuzzy_not(pi, 3.14159) == True\n    assert fuzzy_not(pi, 3.1415926) == True\n    assert fuzzy_not(pi, 3.14159265) == True\n    assert fuzzy_not(pi, 3.141592653) == True\n    assert fuzzy_not(pi, 3.1415926535) == True\n    assert fuzzy_not(pi, 3.14159265358) == True\n    assert fuzzy_not(pi, 3.141592653589) == True\n    assert fuzzy_not(pi, 3.1415926535897) == True\n    assert fuzzy_not(pi, 3.14159265358979) == False\n    assert fuzzy_not(pi, 3.141592653589793) == True\n"], "sample_339": ["def test_modelformset_factory_disable_delete(self):\n    AuthorFormSet = modelformset_factory(\n        Author,\n        fields='__all__',\n        can_delete=False,\n        extra=2,\n    )\n    formset = AuthorFormSet()\n    self.assertEqual(len(formset), 2)\n    self.assertNotIn('DELETE', formset.forms[0].fields)\n    self.assertNotIn('DELETE', formset.forms[1].fields)\n", "def test_inlineformset_factory_passes_renderer(self):\n    from django.forms.renderers import Jinja2\n    renderer = Jinja2()\n    AuthorBooksFormSet = inlineformset_factory(Author, Book, can_delete=False, extra=2, fields=\"__all__\", renderer=renderer)\n    author = Author.objects.create(pk=1, name='Charles Baudelaire')\n    formset = AuthorBooksFormSet(instance=author)\n    self.assertEqual(formset.renderer, renderer)\n", "def test_modelformset_factory_with_custom_pk_and_to_field(self):\n    class CustomPrimaryKeyForm(forms.ModelForm):\n        class Meta:\n            model = CustomPrimaryKey\n            fields = \"__all__\"\n            to_field = \"my_pk\"\n\n    CustomPrimaryKeyFormSet = modelformset_factory(CustomPrimaryKey, form=CustomPrimaryKeyForm)\n    formset = CustomPrimaryKeyFormSet()\n    self.assertEqual(len(formset.forms), 1)\n    self.assertHTMLEqual(\n        formset.forms[0].as_p(),\n        '<p><label for=\"id_form-0-my_pk\">My pk:</label> <input id=\"id_form-0-my_pk\" type=\"text\" name=\"form-0-my_pk\" maxlength=\"10\"></p>'\n        '<p><label for=\"id_form-0-some_field\">Some field:</label>'\n        '<input id=\"id_form-0-some_field\" type=\"text\" name=\"form-0-some_field\" maxlength=\"100\"></p>'\n    )\n", "def test_modelformset_factory_with_custom_pk_and_inlineformset_factory(self):\n    # Test modelformset_factory and inlineformset_factory with a custom primary key\n    PlaceFormSet = modelformset_factory(Place, fields=\"__all__\")\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '',\n        'form-0-name': 'Test Place',\n        'form-0-city': 'Test City',\n    }\n    formset = PlaceFormSet(data)\n    self.assertTrue(formset.is_valid())\n    saved = formset.save()\n    self.assertEqual(len(saved), 1)\n    place = saved[0]\n    self.assertEqual(place.name, 'Test Place')\n\n    OwnerFormSet = inlineformset_factory(Place, Owner, fields=\"__all__\")\n    data = {\n        'owner_set-TOTAL_FORMS': '1',\n        'owner_set-INITIAL_FORMS': '0',\n        'owner_set-MAX_NUM_FORMS': '',\n        'owner_set-0-name': 'Test Owner',\n        'owner_set-0-place': place.pk,\n    }\n    formset = OwnerFormSet(data, instance=place)\n    self.assertTrue(formset.is_valid())\n    saved = formset.save()\n    self.assertEqual(len(saved), 1)\n    owner = saved[0]\n    self.assertEqual(owner.name, 'Test Owner')\n    self.assertEqual(owner.place, place)\n", "def test_modelformset_factory_widgets_with_custom_pk(self):\n    widgets = {\n        'tacos_are_yummy': forms.TextInput(attrs={'class': 'mexican_restaurant'})\n    }\n    MexicanRestaurantFormSet = modelformset_factory(ClassyMexicanRestaurant, fields=\"__all__\", widgets=widgets)\n    form = MexicanRestaurantFormSet.form()\n    self.assertHTMLEqual(\n        str(form['tacos_are_yummy']),\n        '<input id=\"id_tacos_are_yummy\" type=\"text\" class=\"mexican_restaurant\" name=\"tacos_are_yummy\" required>'\n    )\n"], "sample_598": ["def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\", \"10 days\", \"01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\", \"-3 days\", \"00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\", \"0 days\", \"03:00:00\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\", \"NaT\", \"NaT\"),\n    ]\n    for item, expected_default, expected_date, expected_time in cases:\n        actual_default = formatting.format_timedelta(item)\n        actual_date = formatting.format_timedelta(item, timedelta_format=\"date\")\n        actual_time = formatting.format_timedelta(item, timedelta_format=\"time\")\n        assert expected_default == actual_default\n        assert expected_date == actual_date\n        assert expected_time == actual_time\n", "def test_inline_variable_array_repr_dask_array():\n    import dask.array as da\n\n    dask_array = da.from_array(np.array([20, 40]), chunks=2)\n    variable = xr.Variable(\"x\", dask_array)\n\n    max_width = 25\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    expected = \"dask.array<chunksize=(2), meta=np.ndarray>\"\n\n    assert actual == expected\n", "def test_format_items_with_timedelta_format():\n    cases = [\n        (\n            pd.to_timedelta([\"1 day 1 hour\", \"2 days\", \"3 hours\"]),\n            \"datetime\",\n            \"1 days 01:00:00 2 days 00:00:00 0 days 03:00:00\",\n        ),\n        (\n            pd.to_timedelta([\"1 day 1 hour\", \"2 days\", \"3 hours\"]),\n            \"date\",\n            \"1 days 2 days 0 days\",\n        ),\n        (\n            pd.to_timedelta([\"1 day 1 hour\", \"2 days\", \"3 hours\"]),\n            \"time\",\n            \"01:00:00 00:00:00 03:00:00\",\n        ),\n    ]\n    for item, timedelta_format, expected in cases:\n        actual = \" \".join(formatting.format_items(item, timedelta_format=timedelta_format))\n        assert expected == actual\n", "def test_inline_variable_array_repr_in_memory():\n    array = np.random.randn(10, 10)\n    variable = xr.Variable(\"x\", array)\n\n    max_width = 20\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    expected = formatting.format_array_flat(array, max_width)\n    assert actual == expected\n", "def test_format_timestamp_precision():\n    cases = [\n        (pd.Timestamp(\"2000-01-01T12:34:56.789\"), \"2000-01-01T12:34:56.789000\"),\n        (pd.Timestamp(\"2000-01-01T12:34:56.789123456\"), \"2000-01-01T12:34:56.789123\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_timestamp(item)\n        assert expected == actual\n"], "sample_396": ["def test_related_lookup_type_parent_objects_queryset(self):\n    ob_qs = ObjectB.objects.filter(pk=self.ob.pk)\n    with self.assertRaisesMessage(\n        ValueError, self.error % (ob_qs, ObjectA._meta.object_name)\n    ):\n        ObjectC.objects.exclude(childobjecta__in=[self.coa, ob_qs])\n", "    def test_ticket_24628(self):\n        \"\"\"\n        If a values queryset is used, then the given values\n        will be used instead of forcing use of the relation's field.\n        \"\"\"\n        o1 = Order.objects.create(id=-2)\n        o2 = Order.objects.create(id=-1)\n        oi1 = OrderItem.objects.create(order=o1, status=0)\n        oi1.status = oi1.pk\n        oi1.save()\n        OrderItem.objects.create(order=o2, status=0)\n\n        # The query below should match o1 as it has related order_item\n        # with id == status.\n        self.assertSequenceEqual(\n            Order.objects.filter(items__in=OrderItem.objects.values_list(\"status\")),\n            [o1],\n        )\n\n        # Test with a different value in values_list\n        oi2 = OrderItem.objects.create(order=o2, status=1)\n        oi2.status = oi2.pk\n        oi2.save()\n\n        self.assertSequenceEqual(\n            Order.objects.filter(items__in=OrderItem.objects.values_list(\"pk\")),\n            [o1, o2],\n        )\n", "    def test_ticket_24800(self):\n        \"\"\"\n        The 'isnull' lookup on reverse m2m relations should not be promoted to\n        a join, as it may hide results.\n        \"\"\"\n        school = School.objects.create()\n        Student.objects.create(school=school)\n        Classroom.objects.create(school=school)\n\n        self.assertSequenceEqual(\n            School.objects.filter(classroom__students__isnull=True),\n            [school],\n        )\n", "    def test_ticket_25187(self):\n        \"\"\"\n        Verify that subquery in __in filter is not evaluated too early.\n        \"\"\"\n        Parent.objects.create(name=\"parent1\")\n        Child.objects.create(name=\"child1\")\n        Child.objects.create(name=\"child2\")\n        qs = Parent.objects.filter(children__in=Child.objects.filter(name=\"child1\"))\n        with CaptureQueriesContext(connection) as captured_queries:\n            self.assertEqual(qs.count(), 1)\n        self.assertEqual(len(captured_queries), 2)\n", "    def test_ticket_24896(self):\n        \"\"\"\n        #24896 - Ensure correct table aliasing when using raw query in\n        subquery and a filtered annotation.\n        \"\"\"\n        ObjectC.objects.create(objecta_id=1)\n        ObjectC.objects.create(objecta_id=2)\n        ObjectC.objects.create(objecta_id=3)\n        ObjectC.objects.create(objecta_id=4)\n\n        raw_qs = ObjectA.objects.raw(\"SELECT id FROM objecta\")\n        annotated_qs = ObjectC.objects.annotate(objecta_in_raw=Exists(raw_qs.filter(id=F(\"objecta_id\"))))\n\n        self.assertEqual(annotated_qs.filter(objecta_in_raw=True).count(), 4)\n"], "sample_998": ["def test_ExteriorProduct_printing():\n    from sympy.diffgeom.rn import R3\n    from sympy.diffgeom import ExteriorProduct\n    ep = ExteriorProduct(R3.dx, R3.dy, R3.dz)\n    assert latex(ep) == r\"\\mathrm{d}x \\wedge \\mathrm{d}y \\wedge \\mathrm{d}z\"\n", "def test_QuotientRing_latex_printing():\n    from sympy.polys.domains import QQ\n    R = QQ.old_poly_ring(x)/[x**2 + 1]\n\n    assert latex(R.one) == r\"{1} + {\\left< {x^{2} + 1} \\right>}\"\n    assert latex(R.zero) == r\"{\\left< {x^{2} + 1} \\right>}\"\n    assert latex(R.gen) == r\"x + {\\left< {x^{2} + 1} \\right>}\"\n", "def test_Quaternion_multiplication_printing():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q = q1 * q2\n    assert latex(q) == r\"-60 + 12 i + 24 j + 32 k\"\n", "def test_MatrixSymbol_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(A) == r\"A\"\n", "def test_intersection():\n    assert latex(Intersection(Interval(0, 1), Interval(2, 3))) == r\"\\left[0, 1\\right] \\cap \\left[2, 3\\right]\"\n"], "sample_1195": ["def test_gamma_matrix_simplification_with_coefficients():\n    i, j, k = tensor_indices('i,j,k', LorentzIndex)\n\n    t = 2 * G(i) * G(-i)\n    ts = simplify_gamma_expression(t)\n    assert _is_tensor_eq(ts, 8)\n\n    t = 3 * G(i) * G(j) * G(-j)\n    ts = simplify_gamma_expression(t)\n    assert _is_tensor_eq(ts, 6 * G(i))\n\n    t = 4 * G(i) * G(j) * G(k) * G(-k) * G(-j) * G(-i)\n    ts = simplify_gamma_expression(t)\n    assert _is_tensor_eq(ts, 384)\n", "def test_gamma_matrix_simplify_single_line():\n    i, j, k = tensor_indices('i,j,k', LorentzIndex)\n\n    # Test single line simplification\n    t = G(i)*G(-i)\n    ts = _simplify_single_line(t)\n    assert _is_tensor_eq(ts, 4*eye(4))\n\n    t = G(i)*G(j)*G(-i)*G(-j)\n    ts = _simplify_single_line(t)\n    assert _is_tensor_eq(ts, (4*eye(4))**2)\n\n    t = G(i)*G(j)*G(-j)*G(-i)\n    ts = _simplify_single_line(t)\n    assert _is_tensor_eq(ts, (4*eye(4))**2)\n\n    t = G(i)*G(j)*G(-j)*G(k)*G(-i)*G(-k)\n    ts = _simplify_single_line(t)\n    assert _is_tensor_eq(ts, 4*eye(4))\n", "def test_simplify_single_line():\n    m0, m1, m2, m3 = tensor_indices('m0:4', LorentzIndex)\n\n    t = G(m0)*G(m1)*G(m2)*G(-m0)*G(-m1)*G(-m2)\n    r = _simplify_single_line(t)\n    assert r.equals(16*eye(4))\n\n    t = G(m0)*G(m1)*G(-m1)*G(-m0)\n    r = _simplify_single_line(t)\n    assert r.equals(4*eye(4))\n\n    t = G(m0)*G(m1)*G(m2)*G(-m2)*G(-m0)*G(-m1)\n    r = _simplify_single_line(t)\n    assert r.equals(16*eye(4))\n\n    t = G(m0)*G(m1)*G(m2)*G(m3)*G(-m0)*G(-m1)*G(-m2)*G(-m3)\n    r = _simplify_single_line(t)\n    assert r.equals(16*eye(4))\n\n    t = G(m0)*G(m1)*G(m2)*G(-m2)*G(-m1)*G(-m0)*G(m3)\n    r = _simplify_single_line(t)\n    assert r.equals(16*eye(4)*G(m3))\n", "def test_simplify_single_line():\n    i0, i1, i2 = tensor_indices('i0:3', LorentzIndex)\n    t = G(i0)*G(i1)*G(-i0)*G(-i1)\n    r = _simplify_single_line(t)\n    assert r.equals(D**2*eye(4))\n\n    t = G(i0)*G(-i0)*G(i1)*G(-i1)\n    r = _simplify_single_line(t)\n    assert r.equals(16*eye(4))\n\n    t = G(i0)*G(i1)*G(-i1)*G(-i0)\n    r = _simplify_single_line(t)\n    assert r.equals(4*eye(4))\n\n    t = G(i0)*G(i1)*G(i2)*G(-i0)*G(-i1)\n    r = _simplify_single_line(t)\n    assert r.equals(4*G(i2))\n\n    t = G(i0)*G(i1)*G(-i0)*G(i2)*G(-i1)\n    r = _simplify_single_line(t)\n    assert r.equals(4*G(i2))\n\n    t = G(i0)*G(i1)*G(-i0)*G(-i1)*G(i2)\n    r = _simplify_single_line(t)\n    assert r.equals((-4*D + 4)*G(i2))\n", "def test_gamma_matrix_simplify_spinor_indices():\n    # Define some DiracSpinorIndex for this test\n    DiracSpinorIndex = TensorIndexType('DiracSpinorIndex', dim=4, dummy_name=\"S\")\n    a, b = tensor_indices('a, b', DiracSpinorIndex)\n\n    # Test with simple expressions involving DiracSpinorIndex\n    t = G(a)*G(-a)\n    r = kahane_simplify(t)\n    assert r.equals(4*eye(4))  # This should hold because of Dirac trace identity\n\n    t = G(a)*G(b)*G(-b)*G(-a)\n    r = kahane_simplify(t)\n    assert r.equals(16*eye(4))  # This should hold because of Dirac trace identity\n\n    # Test with more complex expressions involving DiracSpinorIndex\n    p, q = tensor_heads('p,q', [DiracSpinorIndex])\n    ps = p(a)*G(-a)\n    qs = q(a)*G(-a)\n    t = ps*qs\n    r = kahane_simplify(t)\n    assert r.equals(0)  # This should hold because of Dirac trace identity\n\n    t = ps*qs*ps*qs\n    r = kahane_simplify(t)\n    assert _is_tensor_eq(r, -4*p(a)*p(-a)*q(a)*q(-a))  # This should hold because of Dirac trace identity\n"], "sample_49": ["def test_media_dsl_extend(self):\n    ###############################################################\n    # DSL Class-based media definitions with extend\n    ###############################################################\n\n    # A widget can extend media if it needs to.\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1',)\n\n    class MyWidget2(MyWidget1):\n        class Media:\n            extend = True\n            css = {\n                'all': ('/path/to/css3',)\n            }\n            js = ('/path/to/js2', '/path/to/js3')\n\n    w2 = MyWidget2()\n    self.assertEqual(\n        str(w2.media),\n        \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_inheritance_overrides(self):\n    # A widget can override a parent's media by specifying 'extend=True'\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n    class MyWidget13(MyWidget1):\n        class Media:\n            extend = True\n            css = {\n                'all': ('/path/to/css3', 'path/to/css1')\n            }\n            js = ('/path/to/js4',)\n\n    w13 = MyWidget13()\n    self.assertEqual(\n        str(w13.media),\n        \"\"\"<link href=\"/path/to/css3\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_inheritance_complex(self):\n    # A widget can extend another widget's media, which in turn extends another widget's media\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2')\n\n    class MyWidget2(MyWidget1):\n        class Media:\n            extend = True\n            css = {\n                'all': ('/path/to/css3',)\n            }\n            js = ('/path/to/js3',)\n\n    class MyWidget3(MyWidget2):\n        class Media:\n            extend = True\n            css = {\n                'all': ('/path/to/css4',)\n            }\n            js = ('/path/to/js4',)\n\n    w3 = MyWidget3()\n    self.assertEqual(\n        str(w3.media),\n        \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_inheritance_ordering(self):\n    \"\"\"\n    Media inheritance and overriding works as expected with multiple levels of inheritance.\n    \"\"\"\n    class MyWidget1(TextInput):\n        class Media:\n            css = {'all': ['/path/to/css1', '/path/to/css2']}\n            js = ['/path/to/js1', '/path/to/js2']\n\n    class MyWidget2(MyWidget1):\n        class Media:\n            css = {'all': ['/path/to/css3', '/path/to/css4']}\n            js = ['/path/to/js3']\n\n    class MyWidget3(MyWidget2):\n        class Media:\n            css = {'all': ['/path/to/css5']}\n            js = ['/path/to/js4', '/path/to/js5']\n\n    w3 = MyWidget3()\n    self.assertEqual(\n        str(w3.media),\n        \"\"\"<link href=\"/path/to/css5\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_with_absolute_paths(self):\n    class MyWidget(TextInput):\n        class Media:\n            css = {'all': ['https://example.com/static/path/to/css1', 'https://example.com/static/path/to/css2']}\n            js = ['https://example.com/static/path/to/js1', 'https://example.com/static/path/to/js2']\n\n    w = MyWidget()\n    self.assertEqual(\n        str(w.media),\n        \"\"\"<link href=\"https://example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_987": ["def test_evalf_abs():\n    assert Abs(x).evalf(subs={x: 3}) == 3\n    assert Abs(x).evalf(subs={x: 3*I}) == 3\n    assert Abs(x).evalf(subs={x: 2 + 3*I}) == 5\n    assert Abs(x).evalf(subs={x: 3.}) == 3\n    assert Abs(x).evalf(subs={x: 3.*I}) == 3\n    assert Abs(x).evalf(subs={x: 2. + 3*I}) == 5\n", "def test_evalf_complex_parts():\n    assert NS('re(1+I)', 10) == '1.0'\n    assert NS('im(1+I)', 10) == '1.0'\n    assert NS('re(1)', 10) == '1.0'\n    assert NS('im(1)', 10) == '0.0'\n    assert NS('re(I)', 10) == '0.0'\n    assert NS('im(I)', 10) == '1.0'\n", "def test_evalf_piecewise():\n    assert Piecewise((x, x > 0), (0, True)).evalf(subs={x: 1.5}) == 1.5\n    assert Piecewise((x, x > 0), (0, True)).evalf(subs={x: -1.5}) == 0\n    assert Piecewise((x, x > 0), (0, True)).evalf(subs={x: 0}) == 0\n    assert Piecewise((x, x < 0), (0, True)).evalf(subs={x: -1.5}) == -1.5\n    assert Piecewise((x, x < 0), (0, True)).evalf(subs={x: 1.5}) == 0\n    assert Piecewise((x, x < 0), (0, True)).evalf(subs={x: 0}) == 0\n", "def test_evalf_strict():\n    # Test that strict=True raises PrecisionExhausted when the result is not accurate enough\n    expr = 1 / (1 + exp(-x))\n    raises(PrecisionExhausted, lambda: expr.evalf(prec=50, strict=True, subs={x: 100}))\n", "def test_issue_10756():\n    from sympy import Function\n    f = Function('f')\n    assert f(x).evalf(subs={x: 1}) == f(1)\n"], "sample_542": ["def test_update_from_text():\n    fig, ax = plt.subplots()\n    t1 = ax.text(0.5, 0.5, 'Original Text', fontsize=12, color='black')\n    t2 = ax.text(0.5, 0.6, 'New Text', fontsize=14, color='red')\n    t1.update_from(t2)\n    assert t1.get_text() == 'New Text'\n    assert t1.get_fontsize() == 14\n    assert t1.get_color() == 'red'\n", "def test_text_set_text_none():\n    fig, ax = plt.subplots()\n    t = ax.text(0.5, 0.5, \"original\")\n    fig.canvas.draw()\n    original_extent = t.get_window_extent()\n    t.set_text(None)\n    fig.canvas.draw()\n    new_extent = t.get_window_extent()\n    assert new_extent.width == 0 and new_extent.height == 0\n    assert original_extent != new_extent\n", "def test_text_bbox_padding():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Test\", bbox=dict(facecolor='none', edgecolor='black', pad=10.0))\n    fig.canvas.draw()\n    bbox = text.get_window_extent()\n    padding = (ax.transData.inverted().transform(bbox.get_points()[1]) - ax.transData.inverted().transform(bbox.get_points()[0])) / 2\n    assert np.allclose(padding, [10.0, 10.0], atol=1)\n", "def test_annotation_clip():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\"hello\", xy=(.5, 1.5), annotation_clip=True)\n    fig.canvas.draw()\n    # Check that the annotation was not drawn (i.e. it was clipped).\n    assert ann.get_window_extent(fig.canvas.renderer).width == 0\n", "def test_annotation_clipping():\n    fig, ax = plt.subplots()\n\n    ax.annotate('Clipped annotation', xy=(1.1, 0.5), xycoords='data',\n                xytext=(0.5, 0.5), textcoords='axes fraction',\n                arrowprops={'arrowstyle': '->'},\n                annotation_clip=True)\n\n    ax.annotate('Not clipped annotation', xy=(1.1, 0.5), xycoords='data',\n                xytext=(0.5, 0.7), textcoords='axes fraction',\n                arrowprops={'arrowstyle': '->'},\n                annotation_clip=False)\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n"], "sample_334": ["def test_attribute_override_class(self):\n    class CustomForm(Form):\n        default_renderer = DjangoTemplates()\n\n    form = CustomForm(renderer=CustomRenderer)\n    self.assertIsInstance(form.renderer, CustomRenderer)\n", "def test_form_init_with_prefix_and_data(self):\n    class PrefixForm(Form):\n        field1 = CharField()\n\n    data = {'field1': 'test'}\n    form = PrefixForm(data, prefix='prefix')\n    self.assertEqual(form.prefix, 'prefix')\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['field1'], 'test')\n", "def test_add_error_method(self):\n    class TestForm(forms.Form):\n        field1 = forms.CharField()\n\n    form = TestForm()\n    form.add_error('field1', 'Custom error message')\n    self.assertIn('Custom error message', form.errors['field1'])\n", "def test_render_errors_with_custom_renderer(self):\n    class CustomRenderer(DjangoTemplates):\n        form_error_template = 'custom_form_error.html'\n        error_template = 'custom_error.html'\n\n    class MyForm(Form):\n        foo = CharField()\n        bar = CharField()\n\n            raise ValidationError('Non-field error.')\n\n    form = MyForm({})\n    self.assertIs(form.is_valid(), False)\n\n    renderer = CustomRenderer()\n    form.renderer = renderer\n    errors = form.render_errors()\n    self.assertTemplateUsed(renderer.form_error_template)\n    self.assertTemplateUsed(renderer.error_template)\n", "def test_widget_class_names(self):\n    class MyWidget(TextInput):\n            context = super().get_context(name, value, attrs)\n            context['widget']['attrs']['class'] = 'test-class'\n            return context\n\n    class WidgetForm(Form):\n        widget_field = CharField(widget=MyWidget())\n\n    form = WidgetForm()\n    widget = form['widget_field']\n    self.assertHTMLEqual(\n        widget.as_widget(),\n        '<input type=\"text\" name=\"widget_field\" class=\"test-class\" required>'\n    )\n"], "sample_835": ["def test_adaboost_regressor_loss(loss):\n    # Check classification on a toy dataset with different loss functions.\n    clf = AdaBoostRegressor(loss=loss, random_state=0)\n    clf.fit(X, y_regr)\n    assert_array_equal(clf.predict(T), y_t_regr)\n", "def test_adaboost_regressor_loss_functions():\n    # Check that AdaBoostRegressor works with different loss functions\n    X, y = datasets.make_regression(n_samples=100, n_features=10, noise=0.5, random_state=42)\n\n    for loss in ['linear', 'square', 'exponential']:\n        reg = AdaBoostRegressor(n_estimators=50, loss=loss, random_state=42)\n        reg.fit(X, y)\n        score = reg.score(X, y)\n        assert score > 0.8, f\"Failed with loss function '{loss}' and score = {score}\"\n", "def test_adaboost_classifier_algorithm_error():\n    # Test that an error is raised when an unsupported algorithm is specified\n    with pytest.raises(ValueError, match=\"algorithm unsupported is not supported\"):\n        AdaBoostClassifier(algorithm=\"unsupported\")\n", "def test_adaboost_regressor_loss_function():\n    # Check that the loss function parameter works correctly\n    X, y = datasets.make_regression(n_samples=100, n_features=2, random_state=42)\n    reg = AdaBoostRegressor(random_state=42, loss='square')\n    reg.fit(X, y)\n    score_square = reg.score(X, y)\n    reg = AdaBoostRegressor(random_state=42, loss='exponential')\n    reg.fit(X, y)\n    score_exponential = reg.score(X, y)\n    assert score_square != score_exponential\n", "def test_adaboost_feature_importances():\n    # Check that feature_importances_ works\n    X, y = datasets.make_classification(n_samples=200, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=1)\n\n    clf = AdaBoostClassifier()\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert importances.shape[0] == 10\n    assert (importances[:3, np.newaxis] >= importances[3:]).all()\n\n    # Test with a base estimator that does not have feature_importances_\n    class DummyEstimator(BaseEstimator):\n            pass\n            return np.zeros(X.shape[0])\n\n    boost = AdaBoostClassifier(DummyEstimator(), n_estimators=3)\n    with pytest.raises(AttributeError, match=\"Unable to compute feature importances since base_estimator does not have a feature_importances_ attribute\"):\n        boost.fit(X, y)\n        boost.feature_importances_\n"], "sample_305": ["def test_annotate_ordering(self):\n    qs = Book.objects.annotate(avg_rating=Avg('rating')).order_by('-avg_rating')\n    self.assertQuerysetEqual(\n        qs,\n        ['Artificial Intelligence: A Modern Approach',\n         'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp',\n         'Practical Django Projects',\n         'Python Web Development with Django',\n         'The Definitive Guide to Django: Web Development Done Right',\n         'Sams Teach Yourself Django in 24 Hours'],\n        lambda b: b.name\n    )\n", "def test_annotate_with_distinct_filter(self):\n    # Regression test for #19105\n    qs = Book.objects.annotate(num_authors=Count('authors', distinct=True))\n    self.assertEqual(\n        qs.filter(num_authors__gt=1).count(),\n        1  # Only 'Python Web Development with Django' has more than 1 unique author\n    )\n", "def test_distinct_aggregate_ordering(self):\n    # Check if distinct aggregate works correctly with ordering.\n    qs = Book.objects.values('rating').distinct().order_by('-rating')\n    self.assertQuerysetEqual(\n        qs, [{'rating': 4.5}, {'rating': 4.0}, {'rating': 3.0}],\n        lambda x: x\n    )\n", "def test_annotation_ordering(self):\n    books = Book.objects.annotate(num_authors=Count('authors')).order_by('num_authors', 'name')\n    self.assertQuerysetEqual(\n        books,\n        ['Sams Teach Yourself Django in 24 Hours', 'Artificial Intelligence: A Modern Approach',\n         'The Definitive Guide to Django: Web Development Done Right', 'Python Web Development with Django',\n         'Practical Django Projects'],\n        lambda b: b.name\n    )\n", "def test_annotate_aggregate_filter(self):\n    # Regression for #15884 - Annotate with Aggregate followed by filter should work\n    qs = Book.objects.annotate(avg_price=Avg('price')).filter(avg_price__gt=F('price'))\n    self.assertQuerysetEqual(qs, [])\n\n    # Using the annotation in the filter should also work\n    qs = Book.objects.annotate(avg_price=Avg('price')).filter(price__lt=F('avg_price'))\n    self.assertEqual(len(qs), 6)\n"], "sample_964": ["def test_pyclassmethod_with_options(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:classmethod:: meth\\n\"\n            \"      :final:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class class method)', 'Class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, (\"final\", desc_sig_space,\n                                                                        \"classmethod\", desc_sig_space)],\n                                                     [desc_name, \"meth\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n    assert 'Class.meth' in domain.objects\n    assert domain.objects['Class.meth'] == ('index', 'Class.meth', 'method', False)\n", "def test_noindex(app):\n    text = (\".. py:function:: f()\\n\"\n            \"   :noindex:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, desc)\n    assert 'f' not in domain.objects\n", "def test_python_use_unqualified_type_names_disabled_annotation(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">foo.Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>foo.Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n    assert '<span class=\"n\"><a class=\"reference external\" href=\"https://python.org/\">str</a></span>' in content\n    assert '<span class=\"n\"><a class=\"reference external\" href=\"https://python.org/\">int</a></span>' in content\n", "def test_pyfunction_signature_async(app):\n    text = \".. py:function:: hello(name: str) -> str\\n   :async:\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"async\", desc_sig_space)],\n                                                    [desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      desc_sig_space,\n                                                      [nodes.inline, pending_xref, \"str\"])])\n", "def test_parse_annotation_with_none_generic(app):\n    doctree = _parse_annotation(\"List[None]\", app.env)\n    assert_node(doctree, ([pending_xref, \"List\"],\n                          [desc_sig_punctuation, \"[\"],\n                          [pending_xref, \"None\"],\n                          [desc_sig_punctuation, \"]\"]))\n    assert_node(doctree[3], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n"], "sample_774": ["def test_one_hot_encoder_dtype_error():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OneHotEncoder(dtype='invalid_dtype')\n    with pytest.raises(TypeError, match=\"'invalid_dtype' is not a valid dtype\"):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_dtype_numpy():\n    X = np.array([['a', 'b'], ['c', 'd']], dtype=object)\n    enc = OneHotEncoder(dtype=np.float32)\n    assert_array_equal(enc.fit_transform(X).toarray().dtype, np.float32)\n    assert_array_equal(enc.fit(X).transform(X).toarray().dtype, np.float32)\n\n    enc = OneHotEncoder(dtype=np.float32, sparse=False)\n    assert_array_equal(enc.fit_transform(X).dtype, np.float32)\n    assert_array_equal(enc.fit(X).transform(X).dtype, np.float32)\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2],\n         ['def', 12, 1],\n         ['ghi', 3, 2]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 0],\n           [1, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    exp_dropped = ['abc', 12, 0]\n    assert_array_equal(dropped_cats, exp_dropped)\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_ordinal_encoder_inverse_transform(X):\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(X_tr), np.array(X, dtype=object))\n", "def test_one_hot_encoder_transform(density, drop):\n    enc = OneHotEncoder(sparse=density, drop=drop)\n    X = [['a', 2, 'b'], ['c', 1, 'a'], ['d', 3, 'c']]\n    enc.fit(X)\n    exp = np.array([[0, 0], [1, 0], [0, 1]], dtype='int64')\n    assert_array_equal(enc.transform(np.array([['a', 2, 'b'], ['d', 3, 'c']]).tolist()).toarray(), exp)\n"], "sample_946": ["def test_pydata_with_default_value(app):\n    text = (\".. py:data:: version\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 1\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0],\n                ([desc_name, \"version\"],\n                 [desc_annotation, (\": \",\n                                    [pending_xref, \"int\"])],\n                 [desc_annotation, \" = 1\"]))\n", "def test_property_without_annotation(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:property:: prop\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'prop (Class property)', 'Class.prop', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"property \"],\n                                                     [desc_name, \"prop\"])],\n                                   [desc_content, ()]))\n    assert 'Class.prop' in domain.objects\n    assert domain.objects['Class.prop'] == ('index', 'Class.prop', 'property', False)\n", "def test_pyfunction_signature_with_ellipsis(app):\n    text = \".. py:function:: hello(name: str, ...) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [nodes.inline, pending_xref, \"str\"])],\n                                      [desc_parameter, ([desc_sig_punctuation, \"...\"],)])])\n", "def test_pydata_with_none_type(app):\n    text = (\".. py:data:: version\\n\"\n            \"   :type: None\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0],\n                ([desc_name, \"version\"],\n                 [desc_annotation, (\": \",\n                                    [pending_xref, \"None\"])],\n                 [desc_annotation, \" = None\"]))\n", "def test_pyfunction_with_unqualified_types(app):\n    text = \".. py:function:: hello(age: int, height: float)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"age\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"int\"])],\n                                      [desc_parameter, ([desc_sig_name, \"height\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"float\"])])])\n\n    app.config.python_use_unqualified_type_names = True\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"age\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, \"int\"])],\n                                      [desc_parameter, ([desc_sig_name, \"height\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, \"float\"])])])\n"], "sample_962": ["def test_restify_mock_object_attributes():\n    with mock(['unknown']):\n        import unknown\n        obj = unknown.secret.Class()\n        assert obj.attribute is not None\n        assert restify(obj.attribute) == ':py:class:`unknown.secret.Class.attribute`'\n", "def test_restify_undecorate():\n    from sphinx.ext.autodoc import undecorate\n    with mock(['unknown']):\n        import unknown\n        mock_obj = unknown.secret.Class()\n        undecorated_obj = undecorate(mock_obj)\n        assert restify(undecorated_obj) == ':py:class:`unknown.secret.Class`'\n        assert stringify(undecorated_obj) == 'unknown.secret.Class'\n", "def test_restify_type_hints_annotated():\n    from typing import Annotated\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":py:class:`str`\"\n", "def test_restify_mock_decorator():\n    with mock(['mock_decorator']):\n        import mock_decorator\n        @mock_decorator.decorator\n            pass\n\n        assert restify(my_function) == ':py:func:`tests.test_util_typing.my_function`'\n        assert stringify(my_function) == 'tests.test_util_typing.my_function'\n", "def test_restify_undecorate():\n    from sphinx.ext.autodoc import mock\n\n        return obj\n\n    with mock(['decorator']):\n        import decorator\n        decorator.dec = dec\n\n        @decorator.dec\n        class DecoratedClass:\n            pass\n\n        assert restify(DecoratedClass) == \":py:class:`tests.test_util_typing.DecoratedClass`\"\n        assert restify(undecorate(DecoratedClass)) == \":py:class:`tests.test_util_typing.DecoratedClass`\"\n\n        @decorator.dec\n            pass\n\n        assert restify(decorated_func) == \":py:func:`tests.test_util_typing.decorated_func`\"\n        assert restify(undecorate(decorated_func)) == \":py:func:`tests.test_util_typing.decorated_func`\"\n"], "sample_1013": ["def test_lambdify_with_custom_module():\n    class CustomModule:\n            return x ** 0.5\n\n    f = lambdify(x, sqrt(x), CustomModule())\n    assert f(4) == 2.0\n", "def test_lambdify_lambda_arg():\n    l = Lambda(x, x**2)\n    f = lambdify(x, l(x), 'sympy')\n    assert f(3) == 9\n", "def test_lambdify_tensorflow_dtype():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n    expr = Max(sin(x), Abs(1/(x+2)))\n    func = lambdify(x, expr, modules=\"tensorflow\", dtype=tensorflow.float64)\n    a = tensorflow.constant(0, dtype=tensorflow.float64)\n    s = tensorflow.Session()\n    assert func(a).eval(session=s) == 0.5\n", "def test_issue_15043():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    # Test for issue #15043\n    f = lambdify((x, y), (x, y))\n    numpy.testing.assert_array_equal(f(1, 2), numpy.array([1, 2]))\n    numpy.testing.assert_array_equal(f(numpy.array(1), numpy.array(2)), numpy.array([1, 2]))\n", "def test_lambdify_with_numpy_and_scipy():\n    if not numpy:\n        skip(\"numpy not installed\")\n    if not import_module('scipy'):\n        skip(\"scipy not installed\")\n    from scipy.special import jv\n    f = lambdify(x, jv(0, x), 'numpy')\n    assert abs(f(0.5) - jv(0, 0.5)) < 1e-15\n"], "sample_459": ["def test_integerfield_float_rounding(self):\n    obj = self.model.objects.create(value=10.5)\n    self.assertEqual(obj.value, 11)\n    obj = self.model.objects.get(value__gt=10.4)\n    self.assertEqual(obj.value, 11)\n    obj = self.model.objects.get(value__gte=10.5)\n    self.assertEqual(obj.value, 11)\n    obj = self.model.objects.get(value__lt=11.6)\n    self.assertEqual(obj.value, 11)\n    obj = self.model.objects.get(value__lte=11.5)\n    self.assertEqual(obj.value, 11)\n", "def test_integerfield_float_rounding(self):\n    float_value = 10.5\n    self.model.objects.create(value=float_value)\n    instance = self.model.objects.get(value=float_value)\n    self.assertEqual(instance.value, math.ceil(float_value))\n", "def test_integerfield_float_rounding(self):\n    obj = self.model.objects.create(value=1.5)\n    obj.refresh_from_db()\n    self.assertEqual(obj.value, 2)\n", "def test_integerfield_float_rounding(self):\n    instance = self.model(value=1.9)\n    instance.full_clean()\n    instance.save()\n    qs = self.model.objects.filter(value=2)\n    self.assertEqual(qs.count(), 1)\n    self.assertEqual(qs[0].value, 2)\n", "def test_integerfield_raises_error_with_invalid_choice(self):\n    f = models.IntegerField(choices=self.Choices.choices)\n    with self.assertRaises(ValidationError):\n        f.clean(\"2\", None)\n"], "sample_527": ["def test_canvas_print_figure():\n    fig = plt.figure()\n    canvas = FigureCanvasBase(fig)\n    with pytest.raises(NotImplementedError):\n        canvas.print_figure(\"test.png\")\n", "def test_widgetlock_zoompan_with_rubberband():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    fig.canvas.widgetlock(ax)\n    tb = NavigationToolbar2(fig.canvas)\n    toolmanager = fig.canvas.manager.toolmanager\n    toolmanager.trigger_tool('rubberband')\n    tb.zoom()\n    assert ax.get_navigate_mode() is None\n    tb.pan()\n    assert ax.get_navigate_mode() is None\n    toolmanager.trigger_tool('rubberband')\n", "def test_interactive_rubberband():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10))\n    assert ax.get_navigate()\n\n    # Mouse move starts from 2, 2 and ends at 6, 6\n    mousestart = (2, 2)\n    mouseend = (6, 6)\n    # Convert to screen coordinates (\"s\").  Events are defined only with pixel\n    # precision, so round the pixel values, and below, check against the\n    # corresponding xdata/ydata, which are close but not equal to d0/d1.\n    sstart = ax.transData.transform(mousestart).astype(int)\n    send = ax.transData.transform(mouseend).astype(int)\n\n    # Set up the mouse movements\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, *sstart, button=MouseButton.LEFT)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas, *send, button=MouseButton.LEFT)\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    tb.press_zoom(start_event)\n    tb.release_zoom(stop_event)\n\n    # Should be close, but won't be exact due to screen integer resolution\n    assert tuple(ax.get_xlim()) == pytest.approx((2, 6), abs=0.1)\n    assert tuple(ax.get_ylim()) == pytest.approx((2, 6), abs=0.1)\n", "def test_interactive_zoom_outside_axes():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    assert ax.get_navigate_mode() is None\n\n    # Mouse from (1, 1) to (2, 2) (data coordinates, \"d\").\n    d0 = (1, 1)\n    d1 = (2, 2)\n    # Convert to screen coordinates (\"s\").\n    s0 = ax.transData.transform(d0).astype(int)\n    s1 = ax.transData.transform(d1).astype(int)\n\n    # Set up the mouse movements\n    start_event = MouseEvent(\"button_press_event\", fig.canvas, *s0, button=MouseButton.LEFT)\n    stop_event = MouseEvent(\"button_release_event\", fig.canvas, *s1, button=MouseButton.LEFT)\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    tb.press_zoom(start_event)\n    tb.drag_zoom(stop_event)\n    tb.release_zoom(stop_event)\n\n    # Should be close, but won't be exact due to screen integer resolution\n    assert tuple(ax.get_xlim()) == pytest.approx((0.5, 1.5), abs=0.02)\n    assert tuple(ax.get_ylim()) == pytest.approx((0.5, 1.5), abs=0.02)\n", "def test_auto_scale_values(attr):\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    getter = getattr(ax, f\"get_{attr}\")\n    setter = getattr(ax, f\"set_{attr}\")\n\n    val = 0.5\n    setter(val)\n    assert getter() == val\n    ax.set_autoscalex_on(True)\n    ax.set_autoscaley_on(True)\n    ax.autoscale()\n    assert getter() != val\n"], "sample_786": ["def test_transform_multi_features():\n    X = np.array([[-2, 1],\n                  [-1, 2],\n                  [0, 3],\n                  [1, 4]])\n    est = KBinsDiscretizer(n_bins=[2, 2], encode='ordinal')\n    est.fit(X)\n    expected = np.array([[0, 0],\n                         [1, 0],\n                         [1, 1],\n                         [1, 1]])\n    assert_array_equal(expected, est.transform(X))\n", "def test_inverse_transform_outside_fit_range():\n    X = np.array([0, 1, 2, 3])[:, None]\n    kbd = KBinsDiscretizer(n_bins=4, strategy='uniform', encode='ordinal')\n    kbd.fit(X)\n\n    X2 = np.array([-2, 5])[:, None]\n    X2t = kbd.transform(X2)\n    X2inv = kbd.inverse_transform(X2t)\n    assert_array_equal(X2inv.max(axis=0), X.max(axis=0))\n    assert_array_equal(X2inv.min(axis=0), X.min(axis=0))\n", "def test_transform_invalid_n_features():\n    est = KBinsDiscretizer(n_bins=3).fit(X)\n    bad_X = np.arange(12).reshape(4, -1)\n    assert_raise_message(ValueError,\n                         \"Incorrect number of features. Expecting 4, \"\n                         \"received 3\", est.transform, bad_X)\n", "def test_transform_shape(n_bins):\n    est = KBinsDiscretizer(n_bins=n_bins)\n    est.fit(X)\n    Xt = est.transform(X)\n    assert Xt.shape == (len(X), n_bins)\n", "def test_transform_output_shape(encode):\n    X = np.random.RandomState(0).randn(100, 3)\n    kbd = KBinsDiscretizer(n_bins=3, strategy='uniform', encode=encode)\n    Xt = kbd.fit_transform(X)\n    expected_shape = (100, 3 * 3) if encode == 'onehot-dense' else (100, 3 * 3 - 2)\n    assert Xt.shape == expected_shape\n"], "sample_387": ["def test_add_view_with_no_permission(self):\n    \"\"\"\n    Test that the add view redirects to the login page if the user does not have\n    add permission for the model.\n    \"\"\"\n    self.admin_login(username=\"u2\", password=\"secret\", login_url=\"/\")\n    response = self.client.get(reverse(\"admin:admin_widgets_car_add\"))\n    self.assertRedirects(response, \"/admin/login/?next=/admin/admin_widgets/car/add/\")\n", "def test_ForeignKey_cannot_add_or_change(self):\n    from selenium.webdriver.common.by import By\n\n    # Create a new user without add or change permissions\n    limited_user = User.objects.create_user(username=\"limited\", password=\"secret\")\n    limited_user.user_permissions.add(Permission.objects.get(codename=\"view_user\"))\n\n    self.admin_login(username=\"limited\", password=\"secret\", login_url=\"/\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_profile_add\")\n    )\n\n    # The Add User button should not be present\n    self.assertFalse(self.is_element_present(By.ID, \"add_id_user\"))\n\n    # The Change User button should not be present\n    self.assertFalse(self.is_element_present(By.ID, \"change_id_user\"))\n", "def test_ForeignKey_with_invalid_uuid(self):\n    from selenium.webdriver.common.by import By\n\n    invalid_uuid = \"invalid-uuid\"\n    Band.objects.create(uuid=invalid_uuid, name=\"Invalid Band\")\n\n    self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n    )\n    main_window = self.selenium.current_window_handle\n\n    # No value has been selected yet\n    self.assertEqual(\n        self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"), \"\"\n    )\n\n    # Attempt to open the popup window with an invalid UUID\n    self.selenium.find_element(By.ID, \"id_main_band\").send_keys(invalid_uuid)\n    self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n    self.wait_for_and_switch_to_popup()\n\n    # Verify that the invalid UUID is cleared from the input field\n    self.assertEqual(\n        self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"), \"\"\n    )\n\n    self.selenium.switch_to.window(main_window)\n", "def test_add_button_for_fk_in_inlines(self):\n    from selenium.webdriver.common.by import By\n\n    band = Band.objects.create(name=\"Test Band\")\n    member = Member.objects.create(name=\"Test Member\")\n\n    self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_band_change\", args=(band.id,))\n    )\n    main_window = self.selenium.current_window_handle\n\n    # Open the inline and click on the Add Member button\n    self.selenium.find_element(By.LINK_TEXT, \"Add another Member\").click()\n    self.wait_for_and_switch_to_popup()\n    name_field = self.selenium.find_element(By.ID, \"id_name\")\n    name_field.send_keys(\"New Member\")\n\n    save_button_css_selector = \".submit-row > input[type=submit]\"\n    self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n    self.selenium.switch_to.window(main_window)\n\n    # The field now contains the new member\n    self.selenium.find_element(By.CSS_SELECTOR, \"#band_members-0-name option[value=New Member]\")\n\n    # Go ahead and submit the form to make sure it works\n    self.selenium.find_element(By.NAME, \"_save\").click()\n    self.wait_for_text(\"li.success\", \"The band \u201cTest Band\u201d was changed successfully.\")\n    band.refresh_from_db()\n    self.assertEqual(band.members.count(), 2)\n    self.assertEqual(band.members.last().name, \"New Member\")\n", "    def test_admin_widget_selenium_case(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_add'))\n        name_field = self.selenium.find_element(By.ID, 'id_name')\n        name_field.send_keys('Test User')\n        birthdate_field = self.selenium.find_element(By.ID, 'id_birthdate_0')\n        birthdate_field.send_keys('1990-01-01')\n        gender_select = Select(self.selenium.find_element(By.ID, 'id_gender'))\n        gender_select.select_by_visible_text('Male')\n        email_field = self.selenium.find_element(By.ID, 'id_email')\n        email_field.send_keys('test@example.com')\n        self.selenium.find_element(By.NAME, '_save').click()\n        self.wait_for_text('li.success', 'The member \u201cTest User\u201d was added successfully.')\n        members = Member.objects.all()\n        self.assertEqual(len(members), 1)\n        member = members[0]\n        self.assertEqual(member.name, 'Test User')\n        self.assertEqual(member.birthdate, datetime(1990, 1, 1))\n        self.assertEqual(member.gender, 'M')\n        self.assertEqual(member.email, 'test@example.com')\n"], "sample_669": ["def test_encodedfile_name_is_str(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert isinstance(ef.name, str)\n    tmpfile.close()\n", "def test_encodedfile_name_and_mode(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert str(ef.name) == str(tmpfile.name)\n    assert ef.mode == tmpfile.mode.replace(\"b\", \"\")\n", "def test_encodedfile_readline(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.write(\"line1\\nline2\")\n    ef.flush()\n    tmpfile.seek(0)\n    assert ef.readline() == \"line1\\n\"\n    assert ef.readline() == \"line2\"\n    assert ef.readline() == \"\"\n    tmpfile.close()\n    with pytest.raises(ValueError):\n        ef.write(\"line3\")\n", "def test_encoding_stdin(capsys):\n    original_stdin = sys.stdin\n    sys.stdin = io.TextIOWrapper(io.BytesIO(b'test input'), encoding='utf-8')\n    try:\n        result = sys.stdin.read()\n        assert result == 'test input'\n    finally:\n        sys.stdin = original_stdin\n", "def test_teesyscapture_writelines(capsysbinary, capsys):\n    # Test TeeSysCapture writelines method\n    captured = capsysbinary.readouterr()\n    assert captured.out == b\"\"\n    assert captured.err == b\"\"\n\n    tee_capture = capture.TeeSysCapture(1)\n    tee_capture.start()\n    tee_capture.writelines([\"line1\\n\", \"line2\\n\"])\n    tee_capture.done()\n\n    captured = capsysbinary.readouterr()\n    assert captured.out == b\"line1\\nline2\\n\"\n    assert captured.err == b\"\"\n\n    captured = capsys.readouterr()\n    assert captured.out == \"line1\\nline2\\n\"\n    assert captured.err == \"\"\n"], "sample_27": ["def test_rawdatadiff_diff_with_atol(tmp_path):\n    \"\"\"Regression test for https://github.com/astropy/astropy/issues/13330\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n    a = np.zeros((10, 2), dtype=\"float32\")\n    a[:, 0] = np.arange(10, dtype=\"float32\") + 10\n    a[:, 1] = np.arange(10, dtype=\"float32\") + 20\n    b = a.copy()\n    changes = [(3, 13.001, 23.001), (8, 20.005, 30.005)]\n    for i, v, w in changes:\n        b[i, 0] = v\n        b[i, 1] = w\n\n    ca = Column(\"A\", format=\"20E\", array=[a])\n    cb = Column(\"A\", format=\"20E\", array=[b])\n    hdu_a = BinTableHDU.from_columns([ca])\n    hdu_a.writeto(path1, overwrite=True)\n    hdu_b = BinTableHDU.from_columns([cb])\n    hdu_b.writeto(path2, overwrite=True)\n    with fits.open(path1) as fits1:\n        with fits.open(path2) as fits2:\n            diff = FITSDiff(fits1, fits2, atol=0.001, rtol=0)\n            str1 = diff.report(fileobj=None, indent=0)\n\n            diff = FITSDiff(fits1, fits2, atol=0.01, rtol=0)\n            str2 = diff.report(fileobj=None, indent=0)\n\n    assert \"...and at 1 more indices.\" in str1\n    assert \"...and at 1 more indices.\" not in str2\n", "def test_rawdatadiff_diff_with_atol(tmp_path):\n    \"\"\"Regression test for https://github.com/astropy/astropy/issues/13330\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n    a = np.zeros((10, 2), dtype=\"float32\")\n    a[:, 0] = np.arange(10, dtype=\"float32\") + 10\n    a[:, 1] = np.arange(10, dtype=\"float32\") + 20\n    b = a.copy()\n    changes = [(3, 13.1, 23.1), (8, 20.5, 30.5)]\n    for i, v, w in changes:\n        b[i, 0] = v\n        b[i, 1] = w\n\n    ca = Column(\"A\", format=\"20E\", array=[a])\n    cb = Column(\"A\", format=\"20E\", array=[b])\n    hdu_a = BinTableHDU.from_columns([ca])\n    hdu_a.writeto(path1, overwrite=True)\n    hdu_b = BinTableHDU.from_columns([cb])\n    hdu_b.writeto(path2, overwrite=True)\n    with fits.open(path1) as fits1:\n        with fits.open(path2) as fits2:\n            diff = FITSDiff(fits1, fits2, atol=0.001, rtol=0)\n            str1 = diff.report(fileobj=None, indent=0)\n\n            diff = FITSDiff(fits1, fits2, atol=0.1, rtol=0)\n            str2 = diff.report(fileobj=None, indent=0)\n\n    assert \"...and at 1 more indices.\" in str1\n    assert \"...and at 1 more indices.\" not in str2\n", "def test_rawdatadiff_diff_with_atol(tmp_path):\n    \"\"\"Regression test for https://github.com/astropy/astropy/issues/13330\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n    a = np.zeros((10, 2), dtype=\"float32\")\n    a[:, 0] = np.arange(10, dtype=\"float32\") + 10\n    a[:, 1] = np.arange(10, dtype=\"float32\") + 20\n    b = a.copy()\n    changes = [(3, 13.0001, 23.0001), (8, 20.0005, 30.0005)]\n    for i, v, w in changes:\n        b[i, 0] = v\n        b[i, 1] = w\n\n    ca = Column(\"A\", format=\"20E\", array=[a])\n    cb = Column(\"A\", format=\"20E\", array=[b])\n    hdu_a = BinTableHDU.from_columns([ca])\n    hdu_a.writeto(path1, overwrite=True)\n    hdu_b = BinTableHDU.from_columns([cb])\n    hdu_b.writeto(path2, overwrite=True)\n    with fits.open(path1) as fits1:\n        with fits.open(path2) as fits2:\n            diff = FITSDiff(fits1, fits2, atol=0.00001, rtol=0)\n            str1 = diff.report(fileobj=None, indent=0)\n\n            diff = FITSDiff(fits1, fits2, atol=0.001, rtol=0)\n            str2 = diff.report(fileobj=None, indent=0)\n\n    assert \"...and at 1 more indices.\" in str1\n    assert \"...and at 1 more indices.\" not in str2\n", "def test_floating_point_rtol_and_atol(self):\n    ha = Header([(\"A\", 1), (\"B\", 1.00001), (\"C\", 3.000001)])\n    hb = ha.copy()\n    hb[\"B\"] = 1.00002\n    hb[\"C\"] = 3.000002\n    diff = HeaderDiff(ha, hb, rtol=1e-6, atol=1e-5)\n    assert not diff.identical\n    assert diff.diff_keyword_values == {\"B\": [(1.00001, 1.00002)]}\n", "def test_fitsdiff_with_different_ver(tmp_path):\n    \"\"\"Make sure diff report reports HDU name and ver if not same in files\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\", ver=1)])\n    hdulist.writeto(path1)\n    hdulist[1].ver = 2\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension HDU 1 (SCI, 2):\" in diff.report()\n"], "sample_673": ["def test_doctest_inline_rst(self, testdir):\n    p = testdir.maketxtfile(\n        test_inline_rst=\"\"\"\n        Some text before the doctest.\n\n        .. doctest::\n\n            >>> 1 + 1\n            2\n\n        Some text after the doctest.\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_no_linedata_on_exception_in_property(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        class Sample(object):\n            @property\n                '''\n                >>> Sample().some_property\n                Traceback (most recent call last):\n                ...\n                Exception: Some error occurred\n                '''\n                raise Exception(\"Some error occurred\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*= FAILURES =*\",\n            \"*_ [[]doctest[]] test_no_linedata_on_exception_in_property.Sample.some_property _*\",\n            \"EXCEPTION LOCATION UNKNOWN, not showing all tests of that example\",\n            \"[?][?][?] >>> Sample().some_property\",\n            \"Traceback (most recent call last):\",\n            \"Exception: Some error occurred\",\n            \"\",\n            \"*/test_no_linedata_on_exception_in_property.py:None: UnexpectedException\",\n            \"*= 1 failed in *\",\n        ]\n    )\n", "def test_doctest_textfile_with_encoding(self, testdir):\n    encoding = \"iso-8859-1\"\n    test_string = \"\u00f6\u00e4\u00fc\"\n    testdir.makeini(f\"[pytest]\\ndoctest_encoding = {encoding}\")\n    doctest = f\"\"\"\n        >>> \"{test_string}\"\n        '{test_string}'\n    \"\"\"\n    testdir._makefile(\".txt\", [doctest], {}, encoding=encoding)\n\n    result = testdir.runpytest()\n\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_doctest_module_skipif(self, testdir):\n    \"\"\"Test that skipif works with doctest modules.\n    \"\"\"\n    testdir.makepyfile(\n        foo=\"\"\"\n            '''\n            >>> import pytest\n            >>> pytest.skip('skipping test')\n            '''\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n", "def test_doctest_report_unexpected_exception_traceback(self, testdir):\n    testdir.maketxtfile(\n        \"\"\"\n        >>> raise Exception(\"Boom!\")\n        >>>\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\n            \"test_doctest_report_unexpected_exception_traceback.txt F *\",\n            \"\",\n            \"*= FAILURES =*\",\n            \"*_ [[]doctest[]] test_doctest_report_unexpected_exception_traceback.txt _*\",\n            \"001 >>> raise Exception('Boom!')\",\n            \"UNEXPECTED EXCEPTION: Exception('Boom!')\",\n            \"Traceback (most recent call last):\",\n            '  File \"*/doctest.py\", line *, in __run',\n            \"    *\",\n            '  File \"<doctest test_doctest_report_unexpected_exception_traceback.txt[0]>\", line 1, in <module>',\n            \"Exception: Boom!\",\n            \"*/test_doctest_report_unexpected_exception_traceback.txt:1: UnexpectedException\",\n        ]\n    )\n"], "sample_710": ["def test_do_cleanups_on_teardown_failure_in_setup(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                    assert False\n                self.addCleanup(failing_cleanup)\n                pass\n            assert MyTestCase.values == [1, 1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 2  # one for the test, one for the failing cleanup\n    assert passed == 1\n", "def test_teardown_on_test_failure(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                self.values.append(None)\n                assert False\n                self.assertEqual(MyTestCase.values, [None])\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "def test_teardown_failure_is_shown_with_setup_error(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n        class TC(unittest.TestCase):\n                assert 0, \"setup error\"\n                assert 0, \"teardown error\"\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-s\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"*setUp*\", \"*assert 0*setup error*\", \"*tearDown*\", \"*assert 0*teardown error*\", \"*1 error*\"])\n", "def test_do_cleanups_on_unhandled_exception(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                raise ValueError(\"Unexpected exception\")\n            assert MyTestCase.values == [1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "def test_setup_failure_is_not_shown_when_no_other_tests_run(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n        class MyTestCase(unittest.TestCase):\n                assert 0, \"setup failure\"\n                pass\n        class NoTests(unittest.TestCase):\n                pytest.skip(\"skipping this test\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-s\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n    result.stdout.no_fnmatch_line(\"*setup failure*\")\n"], "sample_834": ["def test_transform_before_fit():\n    # Assert that transform raises NotFittedError when fit has not been called\n    nca = NeighborhoodComponentsAnalysis()\n    X = iris_data\n    assert_raises(NotFittedError, nca.transform, X)\n", "def test_non_numeric_input():\n    # Test that non-numeric input raises TypeError\n    X = np.array([[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"]])\n    y = [1, 2]\n    nca = NeighborhoodComponentsAnalysis()\n    assert_raises(TypeError, nca.fit, X, y)\n", "def test_n_components_none():\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(n_components=None)\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (X.shape[1], X.shape[1]))\n", "def test_transform_before_fit():\n    # Test that calling transform before fit raises a NotFittedError\n    nca = NeighborhoodComponentsAnalysis()\n    X = iris_data\n    assert_raises(NotFittedError, nca.transform, X)\n", "def test_nca_transform():\n    # Test the transform method after fitting the model\n    X = np.array([[0, 0], [0, 1], [2, 0], [2, 1]])\n    y = np.array([1, 0, 1, 0])\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='identity', random_state=42)\n    nca.fit(X, y)\n    X_transformed = nca.transform(X)\n\n    # Check the shape of the transformed data\n    assert X_transformed.shape == (4, 2)\n\n    # Check the data type of the transformed data\n    assert X_transformed.dtype == np.float64\n"], "sample_678": ["def test_symlink_or_skip(tmp_path, monkeypatch):\n    \"\"\"Ensure symlink_or_skip skips the test when symlinks are not supported.\"\"\"\n    src = tmp_path / \"src\"\n    dst = tmp_path / \"dst\"\n    src.write_text(\"test\")\n\n        raise OSError(\"symlinks not supported\")\n\n    with monkeypatch.context() as m:\n        m.setattr(os, \"symlink\", raise_os_error)\n        with pytest.raises(pytest.skip.Exception, match=\"symlinks not supported\"):\n            symlink_or_skip(src, dst)\n    assert not dst.exists()\n", "def test_cleanup_candidates(tmp_path):\n    \"\"\"Test that cleanup_candidates returns the correct candidates for removal.\"\"\"\n    from _pytest.pathlib import cleanup_candidates\n\n    root = tmp_path\n    prefix = \"test-\"\n    keep = 2\n\n    # Create directories\n    for i in range(1, 6):\n        (root / f\"{prefix}{i}\").mkdir()\n\n    # Test cleanup_candidates\n    candidates = list(cleanup_candidates(root, prefix, keep))\n    expected_candidates = [root / f\"{prefix}{i}\" for i in range(1, 4)]\n    assert candidates == expected_candidates\n", "def test_cleanup_lock_removal(tmp_path, monkeypatch):\n    \"\"\"Ensure that the cleanup lock is removed on program exit (#6882).\"\"\"\n    path = tmp_path / \"temp-1\"\n    path.mkdir()\n\n        pass\n\n    monkeypatch.setattr(\"atexit.unregister\", mock_unregister)\n\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n    assert lock_path.is_file()\n\n    register_cleanup_lock_removal(lock_path)\n\n    assert not lock_path.is_file()\n", "def test_extended_length_path_creation(tmp_path, monkeypatch):\n    \"\"\"Ensure that creating extended length paths works (particularly on Windows).\"\"\"\n    if sys.platform == \"win32\":\n        path = (tmp_path / (\"a\" * 250)).resolve()\n        extended_path = \"\\\\\\\\?\\\\\" + str(path)\n\n            return True\n\n        monkeypatch.setattr(Path, \"is_file\", mock_is_file)\n\n        assert str(ensure_extended_length_path(path)) == extended_path\n    else:\n        path = tmp_path / \"test\"\n        assert str(ensure_extended_length_path(path)) == str(path)\n", "def test_symlink_or_skip(tmp_path, monkeypatch):\n    \"\"\"Test that symlink_or_skip skips the test if symlinks are not supported.\"\"\"\n    src = tmp_path / \"source\"\n    dst = tmp_path / \"destination\"\n    src.write_text(\"content\")\n\n        raise OSError(\"symlinks not supported\")\n\n    monkeypatch.setattr(os, \"symlink\", raise_os_error)\n\n    with pytest.raises(pytest.skip.Exception):\n        symlink_or_skip(src, dst)\n\n    assert not dst.exists()\n"], "sample_635": ["def test_finds_compact_container_types_google(self):\n    node = astroid.extract_node(\n        '''\n        \"\"\"The docstring\n\n        Args:\n            named_arg (dict(str,str)): Returned\n\n        Returns:\n            dict(str,str): named_arg\n        \"\"\"\n        return named_arg\n    '''\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_finds_missing_yields_google(self) -> None:\n    \"\"\"Example of a function having missing yields documentation in\n    the Google style docstring\n    \"\"\"\n    node = astroid.extract_node(\n        '''\n        \"\"\"A generator function.\n\n        Args:\n            n (int): The number of values to generate.\n\n        Returns:\n            Generator[int, None, None]: A generator yielding integers.\n        \"\"\"\n        i = 0\n        while i < n:\n            yield i\n            i += 1\n    '''\n    )\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"missing-yields-doc\", node=node)\n    ):\n        self.checker.visit_functiondef(node)\n", "def test_finds_yield_documentation_sphinx(self) -> None:\n    \"\"\"Example of a generator function having yield documentation in\n    a Sphinx style docstring\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''A generator function\n\n        :yield: A value\n        :yieldtype: int\n        '''\n        yield 1\n        yield 2\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_ignores_return_in_abstract_method_numpy_3(self) -> None:\n    \"\"\"Example of an abstract method documenting the return type that an\n    implementation should return.\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    import abc\n    class Foo(object):\n        @abc.abstractmethod\n            '''docstring ...\n\n            Parameters\n            ----------\n            arg : int\n                An argument.\n\n            Returns\n            -------\n            int\n                Ten\n            '''\n            return 10\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_finds_multiple_types_with_parentheses_google(self) -> None:\n    node = astroid.extract_node(\n        '''\n        \"\"\"The docstring\n\n        Args:\n            named_arg (int or (str, float)): Returned\n\n        Returns:\n            int or (str, float): named_arg\n        \"\"\"\n        return named_arg\n    '''\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n"], "sample_1156": ["def test_coth_sign_assumptions():\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    assert coth(n).is_negative is True\n    assert coth(p).is_positive is True\n", "def test_tanh_positive():\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    assert tanh(n).is_negative is True\n    assert tanh(p).is_positive is True\n    assert tanh(0).is_zero is True\n", "def test_sinh_is_finite():\n    z = Symbol('z')\n    assert sinh(z).is_finite is True\n    assert sinh(zoo).is_finite is False\n    assert sinh(nan).is_finite is False\n    assert sinh(I*z).is_finite is True\n", "def test_tanh_positive():\n    # See issue 11721\n    # tanh(x) is positive for real values of x greater than 0\n    k = symbols('k', real=True)\n    n = symbols('n', integer=True)\n\n    assert tanh(k, evaluate=False).is_positive is None\n    assert tanh(k + pi*I, evaluate=False).is_positive is None\n    assert tanh(1, evaluate=False).is_positive is True\n    assert tanh(-1, evaluate=False).is_negative is True\n    assert tanh(S.Zero, evaluate=False).is_positive is False\n", "def test_sech_real_assumptions():\n    p = Symbol('p', extended_real=True)\n    assert sech(p).is_real is True\n"], "sample_741": ["def test_grid_search_cv_refit_true():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    for iid in (False, True):\n        search = GridSearchCV(SVC(gamma='scale'), cv=n_splits, iid=iid,\n                              param_grid=params, refit=True)\n        search.fit(X, y)\n        assert_equal(iid, search.iid)\n        assert_true(hasattr(search, 'best_estimator_'))\n", "def test_grid_search_error_score():\n    # Test that grid search handles error_score properly\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n    clf = LinearSVC()\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, error_score='raise')\n    assert_raises(ValueError, cv.fit, X_[:180], y_[:180])\n\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, error_score=0)\n    cv.fit(X_[:180], y_[:180])\n\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]}, error_score=float('nan'))\n    cv.fit(X_[:180], y_[:180])\n", "def test_grid_search_with_multimetric_scoring():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n    scoring = {'accuracy': make_scorer(accuracy_score), 'recall': make_scorer(recall_score)}\n\n    for iid in (False, True):\n        grid_search = GridSearchCV(SVC(gamma='scale'), cv=n_splits, iid=iid, param_grid=params, scoring=scoring)\n        grid_search.fit(X, y)\n        assert_equal(grid_search.iid, iid)\n        cv_results = grid_search.cv_results_\n\n        # Check if score and timing are reasonable\n        assert_true(all(cv_results['rank_test_accuracy'] >= 1))\n        assert_true(all(cv_results['rank_test_recall'] >= 1))\n        assert_true(all(cv_results[k] >= 0) for k in cv_results if 'test' in k and 'time' not in k and 'rank' not in k)\n        assert_true(all(cv_results[k] <= 1) for k in cv_results if 'test' in k and 'time' not in k and 'rank' not in k)\n\n        # Check cv_results structure\n        param_keys = ('param_C', 'param_degree', 'param_gamma', 'param_kernel')\n        score_keys = ('mean_test_accuracy', 'mean_train_accuracy', 'rank_test_accuracy',\n                      'split0_test_accuracy', 'split1_test_accuracy', 'split2_test_accuracy',\n                      'split0_train_accuracy', 'split1_train_accuracy', 'split2_train_accuracy',\n                     ", "def test_grid_search_with_single_sample_train_set():\n    # Test that grid search works when the training set has a single sample\n    X, y = make_classification(n_samples=1, n_features=10, random_state=0)\n\n    clf = SVC(gamma='scale')\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    cv.fit(X, y)\n\n    assert_true(cv.best_score_ >= 0)\n\n    y_pred = cv.predict(X)\n\n    assert_equal(y_pred, y)\n", "def test_grid_search_with_imbalanced_classes():\n    # Test grid-search with imbalanced classes\n    X, y = make_classification(n_samples=50, n_classes=2, weights=[0.9, 0.1], random_state=42)\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    grid_search = GridSearchCV(SVC(gamma='scale'), param_grid=params, cv=2)\n    grid_search.fit(X, y)\n\n    # Check if the best parameters found are not biased towards the majority class\n    assert_true(grid_search.best_params_[\"kernel\"] == \"rbf\")\n    assert_true(grid_search.best_params_[\"C\"] == 10)\n    assert_true(grid_search.best_params_[\"gamma\"] == 0.1)\n"], "sample_434": ["    def test_view_setup_assigns_request_args_kwargs(self):\n        request = RequestFactory().get('/')\n        args = ('arg1', 'arg2')\n        kwargs = {'key1': 'value1', 'key2': 'value2'}\n\n        view = SyncView()\n        view.setup(request, *args, **kwargs)\n\n        self.assertEqual(view.request, request)\n        self.assertEqual(view.args, args)\n        self.assertEqual(view.kwargs, kwargs)\n", "    def test_setup_initializes_attributes(self):\n        class TestView(View):\n                return HttpResponse()\n\n        request_factory = RequestFactory()\n        request = request_factory.get('/')\n        view = TestView()\n        view.setup(request, 'arg1', 'arg2', kwarg1='value1', kwarg2='value2')\n\n        self.assertEqual(view.request, request)\n        self.assertEqual(view.args, ('arg1', 'arg2'))\n        self.assertEqual(view.kwargs, {'kwarg1': 'value1', 'kwarg2': 'value2'})\n", "    def test_dispatch_method_calls_correct_handler(self):\n        request_factory = RequestFactory()\n        tests = [\n            (SyncView, False),\n            (AsyncView, True),\n        ]\n        for view_cls, is_coroutine in tests:\n            with self.subTest(view_cls=view_cls, is_coroutine=is_coroutine):\n                instance = view_cls()\n                instance.setup(request_factory.get(\"/\"), *[], **{})\n                response = instance.dispatch(request_factory.get(\"/\"), *[], **{})\n                self.assertIs(\n                    asyncio.iscoroutine(response),\n                    is_coroutine,\n                )\n                if is_coroutine:\n                    response = asyncio.run(response)\n\n                self.assertEqual(response.content, b\"Hello (sync) world!\")\n", "    def test_dispatch_calls_correct_handler(self):\n        request_factory = RequestFactory()\n        tests = [\n            (SyncView, False),\n            (AsyncView, True),\n        ]\n        for view_cls, is_coroutine in tests:\n            with self.subTest(view_cls=view_cls, is_coroutine=is_coroutine):\n                instance = view_cls()\n                instance.setup(request_factory.get(\"/\"))\n                response = instance.dispatch(request_factory.get(\"/\"))\n                self.assertIs(\n                    asyncio.iscoroutine(response),\n                    is_coroutine,\n                )\n                if is_coroutine:\n                    response = asyncio.run(response)\n\n                self.assertIsInstance(response, HttpResponse)\n                self.assertEqual(response.content, b\"Hello (sync) world!\" if not is_coroutine else b\"Hello (async) world!\")\n", "    def test_setup_method(self):\n        request = RequestFactory().get(\"/\")\n        view = SyncView()\n        view.setup(request, \"arg1\", \"arg2\", kwarg1=\"value1\")\n        self.assertEqual(view.request, request)\n        self.assertEqual(view.args, (\"arg1\", \"arg2\"))\n        self.assertEqual(view.kwargs, {\"kwarg1\": \"value1\"})\n        self.assertTrue(hasattr(view, \"head\"))\n"], "sample_529": ["def test_legend_multiple_handles_labels():\n    # test legend entries with multiple handles and labels\n    fig, ax = plt.subplots()\n    p1, = ax.plot([1, 2, 3], '-o')\n    p2, = ax.plot([2, 3, 4], '-x')\n    p3, = ax.plot([3, 4, 5], '-d')\n    ax.legend([p1, p2, p3], ['one', 'two', 'three'])\n", "def test_legend_markerfirst():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), '-o', label='test')\n    legend = ax.legend(loc='best', markerfirst=False)\n    assert legend.get_children()[0].get_children()[0].align == 'right'\n", "def test_legend_title_fontsize_rcparams():\n    # test the default title_fontsize value from rcParams\n    plt.plot(range(10))\n    leg = plt.legend(title='Aardvark')\n    assert leg.get_title().get_fontsize() == mpl.rcParams['legend.title_fontsize']\n", "def test_legend_multiple_columns():\n    # Test that the legend can handle multiple columns\n    fig, ax = plt.subplots()\n    for i in range(6):\n        ax.plot(np.arange(10), label=f'line {i}')\n    ax.legend(ncol=2)\n", "def test_legend_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6], label='Test')\n    leg = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n    assert leg.get_bbox_to_anchor() == ax.bbox\n"], "sample_1145": ["def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.real(x) & Q.positive(x)) == 1\n    assert refine(sign(x), Q.real(x) & Q.negative(x)) == -1\n    assert refine(sign(x), Q.imaginary(x) & Q.positive(im(x))) == S.ImaginaryUnit\n    assert refine(sign(x), Q.imaginary(x) & Q.negative(im(x))) == -S.ImaginaryUnit\n", "def test_refine_zero():\n    assert refine(0, Q.integer(x)) == 0\n    assert refine(0, Q.rational(x)) == 0\n    assert refine(0, Q.real(x)) == 0\n    assert refine(0, Q.imaginary(x)) == 0\n    assert refine(0, Q.complex(x)) == 0\n    assert refine(0, Q.positive(x)) == 0\n    assert refine(0, Q.negative(x)) == 0\n", "def test_refine_odd():\n    x = Symbol('x', integer=True)\n    assert refine(x, Q.odd(x)) == x\n    assert refine(x + 2, Q.odd(x)) == x + 2\n    assert refine(x - 1, Q.odd(x)) == x - 1\n    assert refine(x*3, Q.odd(x)) == x*3\n", "def test_refine_Abs_with_Mul():\n    assert refine(Abs(x*y), Q.positive(x) & Q.positive(y)) == x*y\n    assert refine(Abs(x*y), Q.positive(x) & Q.negative(y)) == x*-y\n    assert refine(Abs(x*y), Q.negative(x) & Q.positive(y)) == -x*y\n    assert refine(Abs(x*y), Q.negative(x) & Q.negative(y)) == x*y\n    assert refine(Abs(x*y), Q.positive(x)) != x*Abs(y)\n    assert refine(Abs(x*y), Q.negative(x)) != x*Abs(y)\n    assert refine(Abs(x*y), Q.positive(y)) != Abs(x)*y\n    assert refine(Abs(x*y), Q.negative(y)) != Abs(x)*y\n", "def test_refine_boolean():\n    assert refine(Q.real(x) & Q.positive(x)) == Q.positive(x)\n    assert refine(Q.real(x) & Q.negative(x)) == Q.negative(x)\n    assert refine(Q.real(x) | Q.positive(x)) == True\n    assert refine(Q.real(x) | Q.negative(x)) == Q.real(x)\n    assert refine(Q.positive(x) | Q.negative(x)) == Q.real(x)\n    assert refine(Q.positive(x) & Q.negative(x)) == False\n"], "sample_602": ["def test_open_dataset_decode_coords():\n    # Create a dataset with coordinates\n    ds = xr.Dataset(\n        {\"var\": (\"x\", np.arange(5))},\n        coords={\"coord\": (\"x\", np.arange(5), {\"coordinates\": \"var\"})},\n    )\n\n    # Open the dataset with decode_coords=True\n    opened_ds = xr.open_dataset(ds, decode_coords=True)\n\n    # Check that the coordinate variable is set as a coordinate\n    assert \"coord\" in opened_ds.coords\n", "def test_open_dataset_with_chunks():\n    data = np.random.rand(10, 10)\n    ds = xr.Dataset({'data': (('x', 'y'), data)})\n    ds.to_netcdf('test.nc')\n\n    # open dataset with chunks\n    chunks = {'x': 5, 'y': 5}\n    ds_chunked = xr.open_dataset('test.nc', chunks=chunks)\n\n    # check if chunks are applied correctly\n    assert ds_chunked.chunks == chunks\n\n    # check if data is the same\n    np.testing.assert_array_equal(ds_chunked.data.values, data)\n", "def test_open_dataset_with_decode_coords():\n    ds = xr.Dataset(\n        dict(a=np.arange(5), b=np.arange(5)),\n        coords=dict(x=(\"x\", np.arange(5), {\"coordinates\": \"a\"})),\n    )\n    with xr.set_options(file_cache_maxsize=0):  # don't want caching for this test\n        with xr.backends.api.open_dataset(ds.to_netcdf(), decode_coords=\"coordinates\") as actual:\n            assert \"a\" in actual.coords\n", "def test_open_dataset_decode_coords():\n    ds = xr.Dataset(\n        data_vars={\"var\": (\"x\", [1, 2, 3])},\n        coords={\"coord\": (\"x\", [4, 5, 6])},\n        attrs={\"coordinates\": \"coord\"},\n    )\n    store = xr.backends.InMemoryDataStore()\n    store.open = lambda: store\n    store.set_vars({\"var\": ds[\"var\"], \"coord\": ds[\"coord\"]}, attrs=ds.attrs)\n\n    # Test decode_coords=\"coordinates\" (default)\n    ds_decoded = xr.open_dataset(store, engine=\"memory\", decode_coords=\"coordinates\")\n    assert \"coord\" in ds_decoded.coords\n\n    # Test decode_coords=True (same as \"coordinates\")\n    ds_decoded = xr.open_dataset(store, engine=\"memory\", decode_coords=True)\n    assert \"coord\" in ds_decoded.coords\n\n    # Test decode_coords=False\n    ds_undecoded = xr.open_dataset(store, engine=\"memory\", decode_coords=False)\n    assert \"coord\" not in ds_undecoded.coords\n", "def test_open_dataset_decode_coords():\n    data = xr.Dataset(\n        data_vars=dict(temperature=(\"time\", np.random.random(size=5))),\n        coords=dict(\n            time=(\"time\", np.arange(5), dict(units=\"seconds since 2000-01-01\"))\n        ),\n    )\n\n    encoded_ds = xr.Dataset(\n        data_vars=dict(temperature=(\"time\", np.random.random(size=5))),\n        coords=dict(time=(\"time\", np.arange(5))),\n        attrs=dict(\n            coordinates=\"time\",\n            time_units=\"seconds since 2000-01-01\",\n        ),\n    )\n\n    store = xr.backends.memory.MemoryBackend(data_vars=encoded_ds.data_vars, attrs=encoded_ds.attrs)\n    actual = xr.open_dataset(store, decode_coords=\"coordinates\")\n    assert_identical(data, actual)\n"], "sample_1161": ["def test_MatrixDeterminant():\n    A = Matrix([[1, 2], [3, 4]])\n    assert str(A.det()) == \"-2\"\n", "def test_Differential():\n    from sympy.diffgeom import Differential\n    assert sstr(Differential(b)) == 'd(x)'\n", "def test_AppliedBinaryRelation_printing():\n    rel = Equivalent\n    assert sstr(rel(x, y)) == 'Equivalent(x, y)'\n    assert sstr(rel(x + 1, y - 2)) == 'Equivalent(x + 1, y - 2)'\n", "def test_Transpose():\n    assert str(Matrix([[1, 2], [3, 4]]).T) == \"Matrix([\\n[1, 3],\\n[2, 4]])\"\n    assert str(Matrix([[1, 2, 3], [4, 5, 6]]).T) == \"Matrix([\\n[1, 4],\\n[2, 5],\\n[3, 6]])\"\n    assert str(Matrix([[1, 2, 3]]).T) == \"Matrix([\\n[1],\\n[2],\\n[3]])\"\n    assert str(Matrix([[1], [2], [3]]).T) == \"Matrix([[1, 2, 3]])\"\n    assert str(Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).T) == \"Matrix([\\n[1, 4, 7],\\n[2, 5, 8],\\n[3, 6, 9]])\"\n", "def test_issue_22126():\n    e = (-1 + x)**y\n    assert str(e) == '(-1 + x)**y'\n    assert str(expand(e)) == 'x**y - y*x**(y - 1) + 1'\n"], "sample_70": ["def test_fast_delete_signals(self):\n    calls = []\n\n        calls.append('')\n\n    models.signals.pre_delete.connect(noop, sender=User)\n    models.signals.post_delete.connect(noop, sender=User)\n\n    u = User.objects.create(\n        avatar=Avatar.objects.create()\n    )\n    a = Avatar.objects.get(pk=u.avatar_id)\n    # 1 query to fast-delete the user\n    # 1 query to delete the avatar\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    self.assertEqual(len(calls), 1)\n\n    models.signals.pre_delete.disconnect(noop, sender=User)\n    models.signals.post_delete.disconnect(noop, sender=User)\n", "    def test_collector_add(self):\n        collector = Collector(using='default')\n        a = create_a('collector_test')\n        new_objs = collector.add([a])\n        self.assertEqual(len(new_objs), 1)\n        self.assertEqual(new_objs[0], a)\n        self.assertEqual(collector.data[A], {a})\n", "def test_fast_delete_with_post_delete_signals(self):\n    post_delete_calls = []\n\n        post_delete_calls.append(instance)\n\n    models.signals.post_delete.connect(log_post_delete, sender=User)\n\n    try:\n        u = User.objects.create()\n        u.delete()\n        self.assertEqual(len(post_delete_calls), 1)\n        self.assertIs(type(post_delete_calls[0]), User)\n    finally:\n        models.signals.post_delete.disconnect(log_post_delete)\n", "    def test_fast_delete_with_signals(self):\n        # Test that fast delete doesn't occur if there are signals attached\n        # to the model.\n            pass\n\n        models.signals.pre_delete.connect(noop, sender=Avatar)\n\n        a = Avatar.objects.create()\n        u = User.objects.create(avatar=a)\n\n        # Since there is a signal attached, there should be 1 query to fetch\n        # the User instance, and then 1 to delete it, and then 1 to delete the\n        # Avatar.\n        self.assertNumQueries(3, a.delete)\n\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n\n        models.signals.pre_delete.disconnect(noop, sender=Avatar)\n", "def test_fast_delete_on_delete_set(self):\n    a = create_a('setnull')\n    setnull_id = a.setnull_id\n    # 1 for self delete, 1 for fast delete of setnull\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(A.objects.exists())\n    self.assertTrue(R.objects.filter(pk=setnull_id).exists())\n"], "sample_811": ["def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    X = [[0], [0]]\n    Y = [[1], [2]]\n    D = paired_cosine_distances(X, Y)\n    # The cosine distance is equivalent to the half the squared\n    # euclidean distance if each sample is normalized to unit norm\n    assert_array_almost_equal(D, [0.5, 1.0])\n", "def test_pairwise_distances_chunked_no_sparse():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    Y = rng.random_sample((200, 4))\n    # Test with sparse X and Y\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    assert_raises(TypeError, check_pairwise_distances_chunked, X_sparse, Y_sparse, working_memory=1, metric='cityblock')\n", "def test_pairwise_distances_data_derived_params_manual():\n    # test data-derived parameters when manually specified\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((1000, 10))\n    Y = rng.random_sample((1000, 10))\n    metric = \"seuclidean\"\n    V = np.ones(10)\n    dist_manual_params = pairwise_distances(X, Y, metric=metric, V=V)\n    dist_default_params = pairwise_distances(X, Y, metric=metric)\n    assert_allclose(dist_manual_params, dist_default_params)\n", "def test_pairwise_distances_chunked_error_handling():\n    # Test that an error is raised when working_memory is too small\n    X = np.random.RandomState(0).random_sample((400, 4))\n    with pytest.raises(ValueError):\n        check_pairwise_distances_chunked(X, None, working_memory=2 ** -31, metric='euclidean')\n\n    # Test that an error is raised when metric is not valid\n    with pytest.raises(ValueError):\n        next(pairwise_distances_chunked(X, None, working_memory=1, metric='invalid_metric'))\n", "def test_pairwise_distances_invalid_data_derived_params(metric):\n    # check that pairwise_distances raises a ValueError when the shape of\n    # the data-derived parameter is invalid\n    X = np.random.random((10, 10))\n    Y = np.random.random((10, 10))\n    invalid_params = {'V': np.random.random((9,)), 'VI': np.random.random((9, 9))}\n\n    for param_name, invalid_param in invalid_params.items():\n        with pytest.raises(ValueError):\n            pairwise_distances(X, Y, metric=metric, **{param_name: invalid_param})\n"], "sample_483": ["def test_list_filter_field_not_exists(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = (\"nonexistent\",)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent', which does not refer to a Field.\",\n            obj=SongAdmin,\n            id=\"admin.E116\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_ordering_with_random_marker(self):\n    class SongAdmin(admin.ModelAdmin):\n        ordering = (\"title\", \"?\")\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering' has the random ordering marker '?', \"\n            \"but contains other fields as well.\",\n            hint='Either remove the \"?\", or remove the other fields.',\n            obj=SongAdmin,\n            id=\"admin.E032\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_duplicate_fields_in_fieldsets(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        fieldsets = [\n            (None, {\"fields\": [\"title\", \"title\"]}),\n        ]\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'fieldsets[0][1]['fields']' contains duplicate field(s).\",\n            obj=MyModelAdmin,\n            id=\"admin.E012\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_autocomplete_fields_valid_input(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"title\"]\n\n    admin.site.register(Song, SongAdmin)\n    admin.site.register(Author, admin.ModelAdmin)\n    try:\n        errors = checks.run_checks()\n        self.assertEqual(errors, [])\n    finally:\n        admin.site.unregister(Song)\n        admin.site.unregister(Author)\n", "def test_autocomplete_fields(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = (\"album\",)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_10": ["def test_table_init_from_dict_of_arrays():\n    data = {'a': [1, 2, 3], 'b': [4, 5, 6]}\n    t = table.Table(data)\n    assert t.colnames == ['a', 'b']\n    assert np.all(t['a'] == data['a'])\n    assert np.all(t['b'] == data['b'])\n", "def test_iterrows_with_types():\n    dat = [(1, 2.0, 'x'),\n           (4, 5.0, 'y'),\n           (5, 8.2, 'z')]\n    t = table.Table(rows=dat, names=('a', 'b', 'c'), dtype=('i4', 'f8', 'S1'))\n    c_s = []\n    a_s = []\n    b_s = []\n    for c, a, b in t.iterrows('c', 'a', 'b'):\n        a_s.append(a)\n        b_s.append(b)\n        c_s.append(c)\n    assert np.all(t['a'] == a_s)\n    assert np.allclose(t['b'], b_s)\n    assert t['c'] == c_s\n", "def test_rename_column_invalid_name_message():\n    t = table.table_helpers.simple_table(1)\n    with pytest.raises(ValueError, match='invalid name'):\n        t.rename_column('a', 'a new name with spaces')\n", "def test_table_with_nested_columns():\n    # Test for #12529\n    t = table.Table()\n    t['a'] = [[1, 2], [3, 4]]\n    t['b'] = [5, 6]\n    t.keep_columns(['a', 'b'])\n    assert t.colnames == ['a', 'b']\n    assert np.all(t['a'] == [[1, 2], [3, 4]])\n    assert np.all(t['b'] == [5, 6])\n", "def test_write_csv_with_delimiter_parameter():\n    t = table.table_helpers.simple_table(1)\n    test_file = \"test.csv\"\n    t.write(test_file, format='csv', delimiter=';')\n    with open(test_file, 'r') as f:\n        lines = f.readlines()\n    assert lines[0] == 'a;b;c\\n'\n    assert lines[1] == '1;2;3\\n'\n    os.remove(test_file)\n"], "sample_717": ["def test_load_lfw_pairs_invalid_subset():\n    assert_raises(ValueError, fetch_lfw_pairs, subset='invalid', data_home=SCIKIT_LEARN_DATA, download_if_missing=False)\n", "def test_load_fake_lfw_people_resize():\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=0.7,\n                                  download_if_missing=False)\n\n    # The data is resized according to the resize parameter\n    assert_equal(lfw_people.images.shape, (10, 43, 33))\n    assert_equal(lfw_people.data.shape, (10, 1429))\n\n    # the target and class names remain the same\n    assert_array_equal(lfw_people.target, [2, 0, 1, 0, 2, 0, 2, 1, 1, 2])\n    expected_classes = ['Abdelatif Smith', 'Abhati Kepler', 'Onur Lopez']\n    assert_array_equal(lfw_people.target_names, expected_classes)\n", "def test_load_fake_lfw_pairs_test_subset():\n    lfw_pairs_test = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, subset='test',\n                                     download_if_missing=False)\n\n    # Check that the test subset is loaded correctly\n    assert_equal(lfw_pairs_test.DESCR, \"'test' segment of the LFW pairs dataset\")\n    assert_equal(lfw_pairs_test.pairs.shape, (0, 2, 62, 47))  # Assuming no data in test subset\n    assert_array_equal(lfw_pairs_test.target, [])  # Assuming no data in test subset\n", "def test_load_fake_lfw_pairs_different_subsets():\n    lfw_pairs_test = fetch_lfw_pairs(subset='test', data_home=SCIKIT_LEARN_DATA, download_if_missing=False)\n    lfw_pairs_10_folds = fetch_lfw_pairs(subset='10_folds', data_home=SCIKIT_LEARN_DATA, download_if_missing=False)\n\n    # Check if the data shapes are correct for each subset\n    assert_equal(lfw_pairs_test.pairs.shape, (expected_test_pairs_shape,))\n    assert_equal(lfw_pairs_10_folds.pairs.shape, (expected_10_folds_pairs_shape,))\n\n    # Check if the target lengths are correct for each subset\n    assert_equal(len(lfw_pairs_test.target), expected_test_target_length)\n    assert_equal(len(lfw_pairs_10_folds.target), expected_10_folds_target_length)\n\n    # Check if the target names are the same for each subset and as expected\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs_test.target_names, expected_classes)\n    assert_array_equal(lfw_pairs_10_folds.target_names, expected_classes)\n", "def test_load_fake_lfw_pairs_subset():\n    # Test loading a specific subset of LFW pairs\n    lfw_pairs_test = fetch_lfw_pairs(subset='test', data_home=SCIKIT_LEARN_DATA, download_if_missing=False)\n\n    # The data shape and target shape should match the subset\n    assert_equal(lfw_pairs_test.pairs.shape, (expected_shape_for_test, 2, 62, 47))\n    assert_equal(lfw_pairs_test.target.shape, (expected_shape_for_test,))\n\n    # The target should be a binary array with 0 and 1 values\n    assert_array_equal(np.unique(lfw_pairs_test.target), [0, 1])\n\n    # The target_names should be the same as expected\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs_test.target_names, expected_classes)\n"], "sample_140": ["    def test_sensitive_post_parameters(self):\n        request = self.client.post('/sensitive-post/', {'password': 'secret', 'username': 'user'})\n        self.assertEqual(request.sensitive_post_parameters, ['password'])\n", "    def test_sensitive_post_parameters(self):\n        \"\"\"\n        Sensitive POST parameters cannot be seen in the default error reports.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_post_parameters('sausage-key', 'bacon-key')(sensitive_view))\n            self.verify_unsafe_email(sensitive_post_parameters('sausage-key', 'bacon-key')(sensitive_view))\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_post_parameters('sausage-key', 'bacon-key')(sensitive_view))\n            self.verify_safe_email(sensitive_post_parameters('sausage-key', 'bacon-key')(sensitive_view))\n", "    def test_sensitive_variables_with_arguments(self):\n        @sensitive_variables('password', 'credit_card')\n            password = user.pass_word\n            credit_card = user.credit_card_number\n\n        self.assertTrue(hasattr(my_function, 'sensitive_variables'))\n        self.assertEqual(my_function.sensitive_variables, ('password', 'credit_card'))\n", "    def test_sensitive_view(self):\n        request = RequestFactory().post('/sensitive/', self.breakfast_data)\n        response = sensitive_view(request)\n        self.assertContains(response, 'sausage-key', status_code=500)\n        self.assertNotContains(response, 'sausage-value', status_code=500)\n        self.assertContains(response, 'bacon-key', status_code=500)\n        self.assertNotContains(response, 'bacon-value', status_code=500)\n", "    def test_sensitive_post_parameters(self):\n        \"\"\"\n        Sensitive POST parameters should not be shown in the default error reports.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_view)\n            self.verify_unsafe_email(sensitive_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_view)\n            self.verify_safe_email(sensitive_view)\n"], "sample_971": ["def test_warning_prefix_filter(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    prefix_filter = logging.MessagePrefixFilter(\"PREFIX:\")\n    logging.getLogger(logging.NAMESPACE).handlers[1].addFilter(prefix_filter)\n\n    logger.warning('message1')\n    assert 'WARNING: PREFIX: message1' in warning.getvalue()\n\n    prefix_filter.prefix = \"Another PREFIX:\"\n    logger.warning('message2')\n    assert 'WARNING: Another PREFIX: message2' in warning.getvalue()\n\n    logging.getLogger(logging.NAMESPACE).handlers[1].removeFilter(prefix_filter)\n    logger.warning('message3')\n    assert 'WARNING: message3' in warning.getvalue()\n", "def test_warning_log_format(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message %s', 'argument')\n    assert 'WARNING: message argument' in warning.getvalue()\n\n    logger.warning('message without argument')\n    assert 'WARNING: message without argument' in warning.getvalue()\n", "def test_warning_prefix_filter(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    prefix = \"CUSTOM_PREFIX: \"\n    with prefixed_warnings(prefix):\n        logger.warning('message1')\n        assert prefix + 'WARNING: message1' in warning.getvalue()\n\n    # check that the prefix is removed after the context manager\n    logger.warning('message2')\n    assert 'WARNING: message2' in warning.getvalue()\n\n    # check that nested prefixed_warnings work as expected\n    with prefixed_warnings('PREFIX1:'):\n        logger.warning('message3')\n        with prefixed_warnings('PREFIX2:'):\n            logger.warning('message4')\n        logger.warning('message5')\n\n    assert 'WARNING: PREFIX1: message3' in warning.getvalue()\n    assert 'WARNING: PREFIX2: message4' in warning.getvalue()\n    assert 'WARNING: PREFIX1: message5' in warning.getvalue()\n", "def test_warning_location_with_tuple_length_3(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1', location=('index', 10, 'extra'))\n    assert 'index.txt:10: WARNING: message1' in warning.getvalue()\n    # Verify that 'extra' is not included in the log message\n    assert 'extra' not in warning.getvalue()\n", "def test_warning_suppressor_type_subtype(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    app._warncount = 0  # force reset\n    app.config.suppress_warnings = ['test.logging']\n\n    logger.warning('message', type='test')\n    assert app._warncount == 1\n\n    logger.warning('message', type='test', subtype='logging')\n    assert app._warncount == 1  # should be suppressed\n\n    logger.warning('message', type='test', subtype='crash')\n    assert app._warncount == 2  # not suppressed as subtype is different\n"], "sample_382": ["    def test_no_template_directories(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set()\n        )\n", "def test_reset_cached_loader(self, mock_reset):\n    autoreload.reset_loaders()\n    self.assertEqual(mock_reset.call_count, 1)\n", "def test_reset_jinja2_loaders(self, mock_reset):\n    autoreload.reset_loaders()\n    mock_reset.assert_called_once()\n", "def test_non_django_template_loader_with_get_dirs(self, mock_is_django_path):\n    class MockLoaderWithGetDirs:\n            return ['loader_dir']\n\n    with mock.patch('django.template.engines.all') as mock_all:\n        mock_all.return_value = [\n            mock.MagicMock(engine=mock.MagicMock(template_loaders=[MockLoaderWithGetDirs()]))\n        ]\n\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                Path.cwd() / 'loader_dir',\n            }\n        )\n        mock_is_django_path.assert_called_once()\n", "def test_django_path_template_included(self, mock_is_django_path):\n    self.assertTrue(autoreload.template_changed(None, EXTRA_TEMPLATES_DIR / 'index.html'))\n    mock_is_django_path.assert_called_once()\n"], "sample_642": ["def test_enable_all_extensions(capsys: CaptureFixture) -> None:\n    \"\"\"Test that the --enable-all-extensions option enables all extensions.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            with pytest.raises(SystemExit):\n                Run([\"--enable-all-extensions\"])\n            out = capsys.readouterr()\n            # This output only exists when all extensions are enabled\n            assert \"All extensions have been enabled.\" in out.err\n", "def test_enable_all_extensions(capsys: CaptureFixture) -> None:\n    \"\"\"Test that we enable all extensions correctly.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            with pytest.raises(SystemExit):\n                Run([\"--enable-all-extensions\"])\n            out = capsys.readouterr()\n            # Check that some extensions are enabled\n            assert \"pylint.extensions.bad_builtin\" in out.err\n", "def test_init_hook_option() -> None:\n    \"\"\"Test that the init-hook option is processed correctly.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            run = Run([\"--init-hook\", 'import sys; sys.path.append(\".\")'])\n            assert \".\" in sys.path\n", "def test_preprocess_options() -> None:\n    \"\"\"Test the _preprocess_options function.\"\"\"\n    run = Run([\"--output\", \"result.txt\", \"--rcfile\", \"pylintrc\"])\n    args = [\"--lo\", \"pylint_plugin\", \"--in\", \"init_value\"]\n    processed_args = config._preprocess_options(run, args)\n    assert run._output == \"result.txt\"\n    assert run._rcfile == \"pylintrc\"\n    assert \"--load-plugins\" in processed_args\n    assert \"pylint_plugin\" in processed_args\n    assert \"--init-hook\" in processed_args\n    assert \"init_value\" in processed_args\n", "def test_enable_all_extensions_abbreviation(capsys: CaptureFixture) -> None:\n    \"\"\"Test that we correctly handle an abbreviated pre-processable option for --enable-all-extensions.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            with pytest.raises(SystemExit):\n                Run([\"--enable-a\"])\n            out = capsys.readouterr()\n            # This output only exists when all extensions are enabled\n            assert \"All extensions have been enabled.\" in out.err\n"], "sample_420": ["def test_model_to_dict_with_exclude(self):\n    item = ColourfulItem.objects.create(name=\"Test Item\")\n    data = model_to_dict(item, exclude=[\"colours\"])\n    self.assertEqual(data, {\"id\": item.id, \"name\": \"Test Item\"})\n", "def test_many_to_many_add(self):\n    \"\"\"Adding items to a ManyToManyField updates the list.\"\"\"\n    blue = Colour.objects.create(name=\"blue\")\n    red = Colour.objects.create(name=\"red\")\n    item = ColourfulItem.objects.create()\n    data = model_to_dict(item)[\"colours\"]\n    self.assertEqual(data, [])\n    item.colours.add(blue)\n    data = model_to_dict(item)[\"colours\"]\n    self.assertEqual(data, [blue])\n    item.colours.add(red)\n    data = model_to_dict(item)[\"colours\"]\n    self.assertEqual(data, [blue, red])\n", "    def test_many_to_many_with_fields(self):\n        \"\"\"Data for a ManyToManyField with fields is a list of dicts rather than a lazy QuerySet.\"\"\"\n        blue = Colour.objects.create(name=\"blue\", rgb=\"0000FF\")\n        red = Colour.objects.create(name=\"red\", rgb=\"FF0000\")\n        item = ColourfulItem.objects.create()\n        item.colours.set([blue])\n        data = model_to_dict(item, fields=[\"colours\"], m2m_fields=[\"colours\"])[\"colours\"]\n        self.assertEqual(data, [{\"name\": \"blue\", \"rgb\": \"0000FF\"}])\n        item.colours.set([red])\n        # If data were a QuerySet, it would be reevaluated here and give \"red\"\n        # instead of the original value.\n        self.assertEqual(data, [{\"name\": \"blue\", \"rgb\": \"0000FF\"}])\n", "    def test_many_to_many_field_with_prefetch_related(self):\n        blue = Colour.objects.create(name=\"blue\")\n        red = Colour.objects.create(name=\"red\")\n        item = ColourfulItem.objects.create()\n        item.colours.set([blue, red])\n\n        field = forms.ModelMultipleChoiceField(\n            ColourfulItem.objects.prefetch_related(\"colours\"),\n            to_field_name=\"id\"\n        )\n        with self.assertNumQueries(3):  # would be 4 if prefetch is ignored\n            self.assertEqual(\n                tuple(field.choices),\n                ((item.pk, str(item)),)\n            )\n", "    def test_exclude_many_to_many(self):\n        \"\"\"Exclude a ManyToManyField in model_to_dict.\"\"\"\n        blue = Colour.objects.create(name=\"blue\")\n        item = ColourfulItem.objects.create()\n        item.colours.set([blue])\n        data = model_to_dict(item, exclude=[\"colours\"])\n        self.assertNotIn(\"colours\", data)\n"], "sample_31": ["def test_write_latex_with_kwargs(self, write, tmp_path, format):\n    \"\"\"Test writing a LaTeX file with additional kwargs\"\"\"\n    fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n    write(fp, format=format, latex_names=False, format=\"ascii.latex\", overwrite=True)\n    tbl = QTable.read(fp)\n    # asserts each column name has not been renamed to LaTeX format\n    for column_name in tbl.colnames[2:]:\n        assert column_name not in _FORMAT_TABLE.values()\n", "def test_write_latex_no_latex_names(self, write, tmp_path, format):\n    \"\"\"Test to write a LaTeX file without latex names\"\"\"\n    fp = tmp_path / \"test_write_latex_no_latex_names.tex\"\n    write(fp, format=format, latex_names=False)\n    tbl = QTable.read(fp)\n    # asserts each column name has not been converted to LaTeX\n    for column_name in tbl.colnames[2:]:\n        assert column_name not in _FORMAT_TABLE.values()\n", "def test_write_latex_kwargs(self, write, tmp_path, format):\n    \"\"\"Test passing additional kwargs to table.write\"\"\"\n    fp = tmp_path / \"test_write_latex_kwargs.tex\"\n    write(fp, format=format, latex_names=True, latexdict={'units': {u.km/u.s/u.Mpc: r'\\\\,km\\\\,s^{-1}\\\\,Mpc^{-1}'}})\n    with open(fp, 'r') as f:\n        content = f.read()\n    assert r'\\\\,km\\\\,s^{-1}\\\\,Mpc^{-1}' in content\n", "def test_write_latex_custom_kwargs(self, write, tmp_path, format):\n    \"\"\"Test passing custom kwargs to QTable.write\"\"\"\n    fp = tmp_path / \"test_write_latex_custom_kwargs.tex\"\n    write(fp, format=format, latex_names=True, overwrite=True, format_kwargs={'escape': False})\n    with open(fp, 'r') as f:\n        content = f.read()\n    assert \"\\\\textbackslash\" not in content  # check that escape characters are not present\n", "def test_write_latex_cosmology_subclass(self, write, tmp_path, format):\n    \"\"\"Test writing LaTeX for a custom Cosmology subclass\"\"\"\n    class CustomCosmology(Cosmology):\n        name = \"Custom Cosmology\"\n        H0 = Parameter(70, unit=\"km/s/Mpc\")\n        Om0 = Parameter(0.3)\n\n    cosmo = CustomCosmology()\n    fp = tmp_path / \"test_write_latex_cosmology_subclass.tex\"\n    write(cosmo, fp, format=format)\n\n    tbl = QTable.read(fp)\n    assert \"Custom Cosmology\" in tbl.meta[\"cosmology_name\"]\n    assert tbl[\"$H_0$\"].unit == \"km / (Mpc s)\"\n    assert tbl[\"$\\\\Omega_{m,0}$\"].data == 0.3\n"], "sample_64": ["def test_file_response(self):\n    filename = os.path.join(os.path.dirname(__file__), 'abc.txt')\n    with open(filename, 'rb') as file:\n        response = FileResponse(file)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertIn('Content-Length', response)\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(filename)))\n        self.assertEqual(response.getvalue(), file.read())\n", "    def test_stream_interface_with_encoding(self):\n        r = HttpResponse()\n        r['Content-Encoding'] = 'gzip'\n        r.writelines(['foo\\n', 'bar\\n', 'baz\\n'])\n        self.assertNotEqual(r.content, b'foo\\nbar\\nbaz\\n')\n        self.assertEqual(r.getvalue(), b'foo\\nbar\\nbaz\\n')\n", "    def test_redirect_with_iris(self):\n        \"\"\"Make sure HttpResponseRedirect works with IRIs.\"\"\"\n        r = HttpResponseRedirect('/r\u00e9direct\u00e9/')\n        self.assertEqual(r.url, '/r\u00e9direct\u00e9/')\n        self.assertEqual(r['Location'], '/r%C3%A9direct%C3%A9/')\n\n        r = HttpResponseRedirect('http://example.com/r\u00e9direct\u00e9/')\n        self.assertEqual(r.url, 'http://example.com/r\u00e9direct\u00e9/')\n        self.assertEqual(r['Location'], 'http://example.com/r%C3%A9direct%C3%A9/')\n\n        r = HttpResponseRedirect('https://example.com/r\u00e9direct\u00e9/')\n        self.assertEqual(r.url, 'https://example.com/r\u00e9direct\u00e9/')\n        self.assertEqual(r['Location'], 'https://example.com/r%C3%A9direct%C3%A9/')\n", "    def test_unicode_headers(self):\n        r = HttpResponse()\n        r['Content-Disposition'] = 'attachment; filename=\"Resum\u00e9.txt\"'\n        self.assertEqual(r['Content-Disposition'], 'attachment; filename=\"Resum\u00e9.txt\"')\n        self.assertIn(b'Resum\\xc3\\xa9.txt', r.serialize_headers())\n", "def test_set_signed_cookie(self):\n    response = HttpResponse()\n    response.set_signed_cookie('name', 'value', salt='salt')\n    cookie = response.cookies['name']\n    self.assertEqual(cookie.value, 'value:1qE6H34vS2qXmffg70UjZ7VMEqEKZGwVVeLmQrHK0qw')\n    self.assertEqual(cookie['httponly'], True)\n    self.assertEqual(cookie['samesite'], None)\n\n    response = HttpResponse()\n    response.set_signed_cookie('name', 'value', salt='salt', httponly=False, samesite='lax')\n    cookie = response.cookies['name']\n    self.assertEqual(cookie.value, 'value:1qE6H34vS2qXmffg70UjZ7VMEqEKZGwVVeLmQrHK0qw')\n    self.assertEqual(cookie['httponly'], False)\n    self.assertEqual(cookie['samesite'], 'lax')\n"], "sample_694": ["def test_hookimpl_via_markers_are_deprecated():\n    from _pytest.config import PytestPluginManager\n\n    pm = PytestPluginManager()\n\n    class DeprecatedHookMarkerImpl:\n        @pytest.mark.hookimpl(tryfirst=True)\n            pass\n\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=r\"Please use the pytest.hookimpl\\(tryfirst=True\\) decorator\",\n    ) as recorder:\n        pm.register(DeprecatedHookMarkerImpl())\n    (record,) = recorder\n    assert (\n        record.lineno\n        == DeprecatedHookMarkerImpl.pytest_runtest_call.__code__.co_firstlineno\n    )\n    assert record.filename == __file__\n", "def test_argument_type_str_choice_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup(\"custom options\")\n            group.addoption(\n                '--custom-option',\n                type='str',\n                choices=['choice1', 'choice2'],\n                help='Custom option with deprecated type str and choices'\n            )\n\n            pass\n        \"\"\"\n    )\n    output = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    message = [\n        \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str'. \"\n        \"For choices this is optional and can be omitted, but when supplied should be a type (for example `str` or `int`).\"\n    ]\n    output.stdout.fnmatch_lines(message)\n    output.assert_outcomes(passed=1)\n\n    # Test with type as str and no choices\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup(\"custom options\")\n            group.addoption(\n                '--custom-option',\n                type='str',\n                help='Custom option with deprecated type str and no choices'\n            )\n\n            pass\n        \"\"\"\n    )\n    output = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    message = [\n        \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str', \"\n        \"but when supplied should be a type (for example `str` or `int`).\"\n    ]\n    output.stdout.fnmatch_lines(message)\n    output.assert_outcomes(passed=1)\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup(\"custom options\")\n            group.addoption(\n                \"--custom-option\",\n                type='str',\n                choices=['a', 'b'],\n                help=\"Custom option for testing\",\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str'.\"\n            \" For choices this is optional and can be omitted, \"\n            \" but when supplied should be a type (for example `str` or `int`).\"\n            \" (options: --custom-option)*\",\n        ]\n    )\n    result.assert_outcomes(warnings=1)\n", "def test_deprecated_options_in_addoption(pytester: Pytester) -> None:\n    \"\"\"Tests for deprecation of string types in addoption.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n            group = parser.getgroup(\"deprecated\")\n            group.addoption(\n                \"--option-str-type\",\n                type=\"str\",\n                action=\"store\",\n                dest=\"str_type\",\n                help=\"option with string type\",\n            )\n            group.addoption(\n                \"--option-str-choices\",\n                type=\"str\",\n                choices=[\"choice1\", \"choice2\"],\n                action=\"store\",\n                dest=\"str_choices\",\n                help=\"option with string choices\",\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str'.\",\n            \"*`type` argument to addoption() is the string 'str', but when supplied should be a type\",\n        ]\n    )\n", "def test_argument_type_str_choice(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup('test_options')\n            group.addoption(\n                '--custom-option',\n                type='str',\n                choices=['choice1', 'choice2'],\n                help='custom help'\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.assert_outcomes(warnings=0)\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup('test_options')\n            group.addoption(\n                '--custom-option',\n                type='choice1',\n                choices=['choice1', 'choice2'],\n                help='custom help'\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'choice1', \"\n            \"but when supplied should be a type (for example `str` or `int`). \"\n            \"(options: --custom-option)*\",\n        ]\n    )\n    result.assert_outcomes(warnings=1)\n"], "sample_159": ["def test_username_unique(self):\n    \"\"\"A unique USERNAME_FIELD should not raise any errors.\"\"\"\n    class CustomUserUniqueUsername(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n    errors = checks.run_checks()\n    self.assertEqual(errors, [])\n", "def test_username_unique(self):\n    \"\"\"\n    A unique USERNAME_FIELD should not raise an error or a warning.\n    \"\"\"\n    class CustomUserUniqueUsername(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n    errors = checks.run_checks()\n    self.assertEqual(errors, [])\n", "    def test_is_staff_method(self):\n        \"\"\"\n        <User Model>.is_staff must not be a method.\n        \"\"\"\n        class CustomUserWithMethodIsStaff(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n                return True\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"%s.is_staff must be an attribute or property rather than a method. \"\n                \"Ignoring this is a security issue as staff users will be treated as non-staff.\" % CustomUserWithMethodIsStaff,\n                obj=CustomUserWithMethodIsStaff,\n                id='auth.E009',\n            ),\n        ])\n", "def test_is_anonymous_authenticated_attributes(self):\n    \"\"\"\n    <User Model>.is_anonymous/is_authenticated must be attributes or properties.\n    \"\"\"\n    class CustomUserBadAttribute(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        @property\n            return True\n\n        @property\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "    def test_custom_permission_codename_length(self):\n        custom_permission_codename = 'x' * 101\n\n        class CustomUserWithInvalidCodename(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = []\n\n            class Meta:\n                permissions = [\n                    (custom_permission_codename, 'Custom permission'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed '%s' of model 'auth_tests.CustomUserWithInvalidCodename' \"\n                \"is longer than 100 characters.\" % custom_permission_codename,\n                obj=CustomUserWithInvalidCodename,\n                id='auth.E012',\n            ),\n        ])\n"], "sample_1082": ["def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x) * tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2 * tanh(x) / (1 + tanh(x)**2)\n", "def test_acsch_real_assumptions():\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    assert acsch(p).is_positive is True\n    assert acsch(n).is_negative is True\n", "def test_asinh_expansion():\n    x, y = symbols('x, y')\n    assert asinh(x+y).expand(trig=True) == log(sqrt(x**2 + 2*x*y + y**2) + x + y)\n    assert asinh(x-y).expand(trig=True) == log(sqrt(x**2 - 2*x*y + y**2) + x - y)\n    assert asinh(2*x).expand(trig=True) == log(2*x + sqrt(4*x**2 + 1))\n    assert asinh(3*x).expand(trig=True).expand() == log(3*x + sqrt(9*x**2 + 1))\n", "def test_hyperbolic_functions_as_real_imag():\n    x = Symbol('x')\n    assert sinh(x).as_real_imag() == (sinh(re(x))*cos(im(x)), cos(re(x))*sinh(im(x)))\n    assert cosh(x).as_real_imag() == (cosh(re(x))*cos(im(x)), sin(re(x))*sinh(im(x)))\n    assert tanh(x).as_real_imag() == (sinh(re(x))*cosh(re(x))/(cos(im(x))**2 + sinh(re(x))**2), sin(im(x))*cos(im(x))/(cos(im(x))**2 + sinh(re(x))**2))\n    assert coth(x).as_real_imag() == (sinh(re(x))*cosh(re(x))/(sin(im(x))**2 + sinh(re(x))**2), -sin(im(x))*cos(im(x))/(sin(im(x))**2 + sinh(re(x))**2))\n    assert csch(x).as_real_imag() == (cos(im(x))*sinh(re(x))/(sin(im(x))**2*cosh(re(x))**2 + cos(im(x))**2*sinh(re(x))**2), -sin(im(x))*cosh(re(x))/(sin(im(x))**2*cosh(re(x))**2 + cos(im(x))**2*sinh(re(x))**2))\n    assert sech(x).as_real_imag() == (cos(im(x))*cosh(re(x))/(sin(im(x))**2*sinh(re(x))**2 + cos(im(x))**2*cosh(re(x))**2), -sin(im(x))*sinh(re(x))/(sin(im(x))**2*sinh(re(x))**2 + cos(", "def test_atanh_expansion():\n    x, y = symbols('x,y')\n    assert atanh(x + y).expand(trig=True) == atanh(x) + atanh((y / (1 - x*y)))\n    assert atanh(2*x).expand(trig=True) == 2*atanh(x) / (1 - 2*x**2)\n    assert atanh(3*x).expand(trig=True).expand() == atanh(x) + 2*atanh((3*x - x) / (1 + 2*x - 3*x**2))\n"], "sample_848": ["def test_multi_target_regression_score():\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    rgr.fit(X_train, y_train)\n    score = rgr.score(X_test, y_test)\n\n    # R^2 score should be between -inf and 1.0\n    assert score >= -np.inf\n    assert score <= 1.0\n", "def test_multi_target_regression_score():\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, y_train = X[:50], y[:50]\n    X_test, y_test = X[50:], y[50:]\n\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    rgr.fit(X_train, y_train)\n    score = rgr.score(X_test, y_test)\n\n    # Compute R^2 of each target manually\n    r2_scores = []\n    for n in range(3):\n        y_pred = rgr.estimators_[n].predict(X_test)\n        y_true = y_test[:, n]\n        u = ((y_true - y_pred) ** 2).sum()\n        v = ((y_true - y_true.mean()) ** 2).sum()\n        r2_scores.append(1 - u / v)\n\n    # Check that the score is the uniform average of the individual R^2 scores\n    assert_almost_equal(score, np.mean(r2_scores))\n", "def test_multi_output_score():\n    # Test the score method of MultiOutputClassifier\n    X, y = datasets.make_classification(n_samples=100, n_features=20, n_classes=3, n_informative=2, random_state=0)\n    y = np.column_stack((y, y, y))  # creating a multilabel scenario\n    clf = MultiOutputClassifier(LogisticRegression(solver='liblinear', random_state=0))\n    clf.fit(X, y)\n    score = clf.score(X, y)\n    assert isinstance(score, float)\n    assert 0 <= score <= 1\n", "def test_multi_output_classification_sparse_input():\n    # Test if MultiOutputClassifier works with sparse input\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix(X)\n    multi_target_linear = MultiOutputClassifier(SGDClassifier(loss='log', random_state=1, max_iter=5))\n    multi_target_linear.fit(X_sparse, y)\n\n    predictions = multi_target_linear.predict(X_sparse)\n    assert (n_samples, n_outputs) == predictions.shape\n\n    predict_proba = multi_target_linear.predict_proba(X_sparse)\n    assert len(predict_proba) == n_outputs\n    for class_probabilities in predict_proba:\n        assert (n_samples, n_classes) == class_probabilities.shape\n\n    assert_array_equal(np.argmax(np.dstack(predict_proba), axis=1),\n                       predictions)\n", "def test_multi_target_classification_sample_weights_api():\n    X = [[1, 2, 3], [4, 5, 6]]\n    y = [[1, 2], [2, 1]]\n    w = [0.8, 0.6]\n\n    clf = MultiOutputClassifier(LogisticRegression(solver='liblinear'))\n    assert_raises_regex(ValueError, \"does not support sample weights\",\n                        clf.fit, X, y, w)\n\n    # no exception should be raised if the base estimator supports weights\n    clf = MultiOutputClassifier(LogisticRegression(solver='liblinear'))\n    clf.fit(X, y, w)\n"], "sample_473": ["    def test_run_get_response(self):\n        handler = ASGIHandler()\n        request = Mock(spec=ASGIRequest)\n        response = Mock()\n        handler.get_response_async = Mock(return_value=response)\n        result = asyncio.run(handler.run_get_response(request))\n        self.assertEqual(result, response)\n        self.assertEqual(response._handler_class, handler.__class__)\n", "compilation error", "            def __init__(self):\n                self.disconnected = False\n", "compilation error", "compilation error"], "sample_745": ["def test_function_transformer_sparse():\n    X_sparse = sparse.csr_matrix(np.random.randn(100, 10))\n    transformer = FunctionTransformer(accept_sparse=True, validate=True)\n    X_sparse_trans = transformer.fit_transform(X_sparse)\n    assert sparse.issparse(X_sparse_trans)\n", "def test_function_transformer_sparse_input():\n    X_sparse = sparse.csr_matrix([[1, 0, 2], [0, 0, 3], [4, 0, 5]])\n    transformer = FunctionTransformer(np.sqrt, accept_sparse=True)\n    X_sparse_trans = transformer.fit_transform(X_sparse)\n    X_dense_trans = np.sqrt(X_sparse.toarray())\n    assert_allclose_dense_sparse(X_sparse_trans, X_dense_trans)\n", "def test_function_transformer_sparse():\n    X_sparse = sparse.csr_matrix(np.random.randn(100, 10))\n    transformer = FunctionTransformer(accept_sparse=True)\n    X_sparse_trans = transformer.fit_transform(X_sparse)\n    assert sparse.issparse(X_sparse_trans)\n", "def test_function_transformer_sparse_matrix():\n    X_sparse = sparse.csr_matrix([[1, 0, 2], [0, 0, 3], [4, 0, 5]])\n    transformer = FunctionTransformer(func=np.sqrt, accept_sparse=True)\n    X_sparse_trans = transformer.fit_transform(X_sparse)\n    assert_allclose_dense_sparse(X_sparse_trans.toarray(), np.sqrt(X_sparse.toarray()))\n", "def test_function_transformer_with_sparse_input():\n    X_sparse = sparse.csr_matrix([[1, 0, 2], [0, 0, 3], [4, 5, 6]])\n\n    transformer = FunctionTransformer(np.log1p, validate=True, accept_sparse=True)\n    X_sparse_trans = transformer.fit_transform(X_sparse)\n\n    assert_array_equal(X_sparse_trans.toarray(), np.log1p(X_sparse.toarray()))\n"], "sample_1184": ["def test_matrix_multiplication():\n    A, B, C, D, E, F, G, H = symbols('A B C D E F G H')\n    mat1 = RayTransferMatrix(A, B, C, D)\n    mat2 = RayTransferMatrix(E, F, G, H)\n    product = mat1 * mat2\n    assert product == Matrix([[A*E + B*G, A*F + B*H], [C*E + D*G, C*F + D*H]])\n", "def test_beam_parameters_with_refractive_index():\n    l, z, w_0, n = symbols('l z w_0 n', positive=True)\n    p = BeamParameter(l, z, w=w_0, n=n)\n    assert p.z_r == waist2rayleigh(w_0, l, n)\n    assert p.radius == z * (1 + (p.z_r / z) ** 2)\n    assert p.w == w_0 * sqrt(1 + (z / p.z_r) ** 2)\n    assert p.divergence == l / (n * pi * w_0)\n    assert p.waist_approximation_limit == 2 * l / (n * pi)\n", "def test_gauss_opt_negative_distance():\n    wavelen = symbols('wavelen', positive=True)\n    z = symbols('z', negative=True)\n    w_0 = symbols('w_0', positive=True)\n    p = BeamParameter(wavelen, z, w=w_0)\n    assert p.radius == z * (1 + (z_r / z) ** 2)\n    assert p.w == w_0 * sqrt(1 + (z / z_r) ** 2)\n    assert p.w_0 == w_0\n    assert p.divergence == wavelen / (pi * w_0)\n    assert p.gouy == atan2(z, z_r)\n    assert p.waist_approximation_limit == 2 * wavelen / pi\n", "def test_beam_parameter_exceptions():\n    wavelen, z, z_r, w, n = symbols('wavelen z z_r w n')\n    # Test with no z_r or w specified\n    try:\n        BeamParameter(wavelen, z)\n    except ValueError:\n        pass\n    else:\n        assert False, \"Expected ValueError\"\n\n    # Test with both z_r and w specified\n    try:\n        BeamParameter(wavelen, z, z_r=z_r, w=w)\n    except ValueError:\n        pass\n    else:\n        assert False, \"Expected ValueError\"\n", "def test_gauss_opt_additional():\n    # Testing the interaction of Gaussian beams with thin lenses\n    f, l, w_i, w_o = symbols('f l w_i w_o', positive=True)\n    s_in, _, _ = conjugate_gauss_beams(l, w_i, w_o, f=f)\n    p_in = BeamParameter(l, s_in, w=w_i)\n    p_out = ThinLens(f) * p_in\n    assert p_out.z == geometric_conj_af(s_in, f)\n\n    # Testing the interaction of Gaussian beams with curved mirrors\n    R, s, l, w_0 = symbols('R s l w_0', positive=True)\n    p_in = BeamParameter(l, s, w=w_0)\n    p_out = CurvedMirror(R) * p_in\n    assert p_out.z == 2 * R - s\n\n    # Testing the interaction of Gaussian beams with curved refracting surfaces\n    n1, n2, R, s, l, w_0 = symbols('n1 n2 R s l w_0', positive=True)\n    p_in = BeamParameter(l, s, w=w_0)\n    p_out = CurvedRefraction(R, n1, n2) * p_in\n    assert p_out.z == (n2 * R * s) / (n1 * (R - s))\n"], "sample_360": ["def test_cache_versioning_delete_many(self):\n    cache.set('answer1', 37, version=1)\n    cache.set('answer1', 42, version=2)\n    cache.set('answer2', 37, version=1)\n    cache.set('answer2', 42, version=2)\n    self.assertEqual(cache.delete_many(['answer1', 'answer2']), 2)\n    self.assertIsNone(cache.get('answer1', version=1))\n    self.assertEqual(cache.get('answer1', version=2), 42)\n    self.assertIsNone(cache.get('answer2', version=1))\n    self.assertEqual(cache.get('answer2', version=2), 42)\n\n    cache.set('answer3', 37, version=1)\n    cache.set('answer3', 42, version=2)\n    cache.set('answer4', 37, version=1)\n    cache.set('answer4', 42, version=2)\n    self.assertEqual(caches['v2'].delete_many(['answer3', 'answer4']), 2)\n    self.assertEqual(cache.get('answer3', version=1), 37)\n    self.assertIsNone(cache.get('answer3', version=2))\n    self.assertEqual(cache.get('answer4', version=1), 37)\n    self.assertIsNone(cache.get('answer4', version=2))\n", "def test_cache_key_varies_by_host(self):\n    \"\"\"\n    get_cache_key keys differ by hostname instead of fully-qualified URL\n    \"\"\"\n    request1 = self.factory.get(self.path, HTTP_HOST='sub-1.example.com')\n    learn_cache_key(request1, HttpResponse())\n    request2 = self.factory.get(self.path, HTTP_HOST='sub-2.example.com')\n    learn_cache_key(request2, HttpResponse())\n    self.assertNotEqual(get_cache_key(request1), get_cache_key(request2))\n", "    def test_different_aliases(self):\n        \"\"\"\n        Attempting to retrieve different aliases should yield different instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertIsNot(cache1, cache2)\n", "def test_cache_incr_decr_float_values(self):\n    cache.set('float_value', 1.5)\n    self.assertEqual(cache.incr('float_value'), 2.5)\n    self.assertEqual(cache.decr('float_value'), 1.5)\n", "    def test_cache_with_timeout(self):\n        key = 'timeout_test'\n        value = 'test_value'\n        timeout = 1\n        cache.set(key, value, timeout)\n        self.assertEqual(cache.get(key), value)\n        time.sleep(timeout)\n        self.assertIsNone(cache.get(key))\n"], "sample_1143": ["def test_Float_floordiv():\n    assert Float(2.5)//Float(0.5) == 5\n", "def test_Float_floordiv():\n    assert Float(2.5) // Float(0.5) == 5\n    assert Float(2.5) // S.Half == 5\n", "def test_Rational_floor_ceiling_power():\n    assert Rational(3, 2).floor() == 1\n    assert Rational(3, 2).ceiling() == 2\n    assert Rational(3, 2)**S.Half == Rational(3**S.Half, 2**S.Half)\n", "def test_Float_floordiv():\n    assert Float(2.9) // Float(1.2) == 2\n    assert Float(2.9) // 1.2 == 2\n    assert 2.9 // Float(1.2) == 2\n    assert Float(2.9) // S.Half == 5\n    assert S.Half // Float(2.9) == 0\n    raises(TypeError, lambda: Float(2.9) // None)\n", "def test_issue_10754():\n    assert isinstance(Integer(2) // Float(3), Float)\n"], "sample_1009": ["def test_vector_magnitude_normalize():\n    N = ReferenceFrame('N')\n    v1 = x*N.x + y*N.y + z*N.z\n\n    magnitude = v1.magnitude()\n    assert magnitude == sqrt(x**2 + y**2 + z**2)\n\n    normalized_v1 = v1.normalize()\n    assert normalized_v1.magnitude() == 1\n    assert normalized_v1 == v1 / magnitude\n", "def test_vector_doit():\n    x, y, z = symbols('x, y, z')\n    N = ReferenceFrame('N')\n\n    test1 = (x + y).doit() * N.x\n    assert (test1 & N.x) == x + y\n\n    test2 = (sin(x) + cos(y)).doit() * N.y\n    assert (test2 & N.y) == sin(x) + cos(y)\n\n    test3 = (x**2 + y**2).doit() * N.z\n    assert (test3 & N.z) == x**2 + y**2\n\n    test4 = (sin(x)**2 + cos(y)**2).doit() * N.x\n    assert (test4 & N.x) == 1\n", "def test_vector_express():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.y])\n    B = N.orientnew('B', 'Axis', [q2, N.z])\n\n    v1 = q1 * A.x + q2 * B.y\n    v2 = v1.express(N)\n\n    assert v2 == q1 * cos(q2) * N.x - q1 * sin(q2) * N.z + q2 * N.y\n\n    v3 = v1.express(A)\n    assert v3 == v1\n", "def test_vector_magnitude():\n    x, y, z = symbols('x y z')\n    N = ReferenceFrame('N')\n\n    v1 = x*N.x + y*N.y + z*N.z\n    assert v1.magnitude() == sqrt(x**2 + y**2 + z**2)\n\n    v2 = 2*N.x - 3*N.y + N.z\n    assert v2.magnitude() == sqrt(2**2 + (-3)**2 + 1**2)\n", "def test_vector_doit():\n    t, a, b, c = symbols('t, a, b, c')\n    N = ReferenceFrame('N')\n\n    v = sin(t)*a*N.x + cos(t)*b*N.y + t*c*N.z\n    v_doit = v.doit()\n    assert v_doit == sin(t)*a*N.x + cos(t)*b*N.y + t*c*N.z\n\n    v = sin(t)*a*N.x + cos(t)*b*N.y + c*N.z\n    v_doit = v.doit()\n    assert v_doit == sin(t)*a*N.x + b*N.y + c*N.z\n\n    v = a*N.x + b*N.y + c*N.z\n    v_doit = v.doit()\n    assert v_doit == a*N.x + b*N.y + c*N.z\n"], "sample_250": ["def test_datetime_with_utc_tzinfo(self):\n    dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=utc)\n    self.assertEqual(datetime.fromtimestamp(int(format(dt, 'U')), utc), dt)\n    self.assertEqual(format(dt, 'O'), '+0000')\n    self.assertEqual(format(dt, 'T'), 'UTC')\n    self.assertEqual(format(dt, 'Z'), '0')\n", "def test_time_formats_with_single_digit_hour(self):\n    my_time = datetime(2022, 1, 1, 9, 30)\n\n    self.assertEqual(dateformat.format(my_time, 'g'), '9')\n    self.assertEqual(dateformat.format(my_time, 'G'), '09')\n    self.assertEqual(dateformat.format(my_time, 'h'), '09')\n    self.assertEqual(dateformat.format(my_time, 'H'), '09')\n", "def test_timezone_awareness(self):\n    naive_dt = datetime(2009, 5, 16, 5, 30, 30)\n    aware_dt = make_aware(datetime(2009, 5, 16, 5, 30, 30), get_default_timezone())\n\n    self.assertEqual(dateformat.format(naive_dt, 'T'), 'CET')\n    self.assertEqual(dateformat.format(aware_dt, 'T'), 'CET')\n\n    self.assertEqual(dateformat.format(naive_dt, 'e'), '')\n    self.assertEqual(dateformat.format(aware_dt, 'e'), '+0200')\n", "def test_time_formats_with_microseconds(self):\n    dt = datetime(1979, 7, 8, 22, 00, 30, 123456)\n    self.assertEqual(dateformat.format(dt, 'u'), '123456')\n", "def test_timezone_dst_transition(self):\n    # Test the DST transition period\n    ambiguous_dt = datetime(2015, 10, 25, 2, 30, 0)\n\n    self.assertEqual(format(ambiguous_dt, 'I'), '1')\n    self.assertEqual(format(ambiguous_dt, 'O'), '+0200')\n    self.assertEqual(format(ambiguous_dt, 'T'), 'CET')\n    self.assertEqual(format(ambiguous_dt, 'Z'), '7200')\n"], "sample_3": ["def test_ecsv_unit_column():\n    \"\"\"Test writing a column with a unit and reading it back.\"\"\"\n    t = Table()\n    t['a'] = np.array([1, 2, 3]) * u.m\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert np.all(t2['a'] == t['a'])\n    assert t2['a'].unit == u.m\n", "def test_guess_ecsv_with_delimiter():\n    \"\"\"Test guessing with delimiter other than space\"\"\"\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    # delimiter: \",\"\n    # datatype:\n    # - {name: col, datatype: string, description: hello}\n    # schema: astropy-2.0\n    col\n    1\n    2\n    \"\"\"\n    t = ascii.read(txt, guess=True)\n    assert t['col'].dtype.kind == 'U'  # would be int with basic format\n    assert t['col'].description == 'hello'\n", "def test_ecsv_with_no_header_line():\n    \"\"\"\n    Test that missing CSV header line raises the expected exception.\n    \"\"\"\n    lines = copy.copy(SIMPLE_LINES)\n    del lines[lines.index('a b c')]\n    with pytest.raises(ValueError) as err:\n        ascii.read(lines, format='ecsv')\n    assert \"CSV header line 'a b c' not found as last line.\" in str(err.value)\n", "def test_ecsv_subtype_mixed_type_array():\n    \"\"\"Test a 2-d array with mixed types.\"\"\"\n    t = Table()\n    t['a'] = np.array([[1, 'b'], [3, 4.5]], dtype=object)\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert np.all(t2['a'][0] == [1, 'b'])\n    assert np.all(t2['a'][1] == [3, 4.5])\n", "def test_ecsv_outputter_dtypes():\n    \"\"\"\n    Test the _convert_vals method of EcsvOutputter with different dtypes\n    and ensure it raises the appropriate exceptions.\n    \"\"\"\n    from astropy.io.ascii.ecsv import EcsvOutputter\n    from astropy.io.ascii.core import Column\n\n    # Create a mock column with some data\n    col = Column(name='col', data=['1', '2', '3'], dtype='str')\n\n    # Create an instance of EcsvOutputter\n    outputter = EcsvOutputter()\n\n    # Test with object dtype\n    col.subtype = 'object'\n    col.str_vals = ['{\"key\": \"value\"}', 'null']\n    outputter._convert_vals([col])\n    assert isinstance(col.data[0], dict)\n    assert np.ma.is_masked(col.data[1])\n\n    # Test with multidim dtype\n    col.subtype = 'int64'\n    col.shape = (2, 2)\n    col.str_vals = ['[1, 2]', '[3, 4]', 'null']\n    outputter._convert_vals([col])\n    assert isinstance(col.data[0], np.ndarray)\n    assert np.ma.is_masked(col.data[2])\n\n    # Test with invalid JSON in str_vals\n    col.str_vals = ['[1, 2', '[3, 4]']\n    with pytest.raises(ValueError) as e:\n        outputter._convert_vals([col])\n    assert \"column 'col' failed to convert: column value is not valid JSON\" in str(e.value)\n\n    # Test with dtype not equal to 'str'\n    col.dtype = 'int64'\n    with pytest.raises(ValueError) as e:\n        outputter._convert_vals([col])\n    assert \"datatype of column 'col' must be \\\"string\\\"\" in str(e.value)\n"], "sample_570": ["def test_bivariate_percent_stat(self, x, y):\n\n    h = Histogram(stat=\"percent\")\n    heights, _ = h(x, y)\n    assert heights.sum() == 100\n", "def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1)\n", "def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1, abs=1e-6)\n", "def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1, abs=1e-5)\n", "def test_bivariate_probability_stat_weights(self, x, y, weights):\n\n    h = Histogram(stat=\"probability\")\n    heights, _ = h(x, y, weights=weights)\n    assert heights.sum() == pytest.approx(1)\n"], "sample_797": ["def test_power_transformer_sparse_input(method):\n    # Check that the PowerTransformer works correctly with sparse input\n    X = sparse.csr_matrix(np.abs(X_2d))\n\n    pt = PowerTransformer(method=method)\n    X_trans = pt.fit_transform(X)\n    assert sparse.issparse(X_trans)\n    assert_array_almost_equal(X_trans.toarray(), power_transform(X.toarray(), method=method))\n\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert sparse.issparse(X_inv_trans)\n    assert_array_almost_equal(X_inv_trans.toarray(), X.toarray())\n", "def test_power_transform_sparse_input():\n    X = np.abs(X_2d)\n    X_sparse = sparse.csr_matrix(X)\n\n    pt = PowerTransformer(method='box-cox')\n    pt.fit(X_sparse)\n    X_trans_sparse = pt.transform(X_sparse)\n    X_trans_dense = pt.transform(X)\n\n    assert_array_almost_equal(X_trans_dense, X_trans_sparse.toarray())\n", "def test_power_transform_different_methods():\n    # Check that box-cox and yeo-johnson methods return different results\n    X = X_1col\n    X_boxcox = power_transform(X, method='box-cox')\n    X_yeojohnson = power_transform(X, method='yeo-johnson')\n    assert not np.allclose(X_boxcox, X_yeojohnson)\n", "def test_power_transform_valid_method():\n    X = np.abs(X_2d)\n    valid_methods = ['box-cox', 'yeo-johnson']\n    for method in valid_methods:\n        X_trans = power_transform(X, method=method)\n        assert X_trans is not None\n    invalid_method = 'invalid-method'\n    with pytest.raises(ValueError) as e_info:\n        power_transform(X, method=invalid_method)\n    assert str(e_info.value) == \"'method' must be one of {}\".format(valid_methods)\n", "def test_power_transformer_copy_and_standardize():\n    # check that when copy=False and standardize=True, fit doesn't change X\n    # inplace but transform, fit_transform and inverse_transform do.\n    X = np.abs(X_1col)\n    X_original = X.copy()\n    assert X is not X_original  # sanity checks\n    assert_array_almost_equal(X, X_original)\n\n    pt = PowerTransformer(method='box-cox', standardize=True, copy=False)\n\n    pt.fit(X)\n    assert_array_almost_equal(X, X_original)  # fit didn't change X\n\n    X_trans = pt.transform(X)\n    assert X_trans is not X\n    assert_array_almost_equal(X_trans, scale(stats.boxcox(X.flatten())[0].reshape(-1, 1)))\n\n    X = np.abs(X_1col)  # reset X\n    X_trans = pt.fit_transform(X)\n    assert X_trans is not X\n    assert_array_almost_equal(X_trans, scale(stats.boxcox(X.flatten())[0].reshape(-1, 1)))\n\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert X_inv_trans is not X_trans\n    assert_array_almost_equal(X_inv_trans, X)\n"], "sample_530": ["def test_packers_empty_children(mode):\n    # Test packers with no children\n    fig = plt.figure(dpi=72)\n    hpacker = HPacker(children=[], mode=mode)\n    vpacker = VPacker(children=[], mode=mode)\n    renderer = fig.canvas.get_renderer()\n\n    # HPacker\n    *extents, offset_pairs = hpacker.get_extent_offsets(renderer)\n    # width, height, xdescent, ydescent\n    assert_allclose((0, 0, 0, 0), extents)\n    assert offset_pairs == []\n\n    # VPacker\n    *extents, offset_pairs = vpacker.get_extent_offsets(renderer)\n    # width, height, xdescent, ydescent\n    assert_allclose((0, 0, 0, 0), extents)\n    assert offset_pairs == []\n", "def test_packers_with_children(mode):\n    fig, ax = plt.subplots()\n    r1 = DrawingArea(10, 30)\n    r2 = DrawingArea(20, 60)\n    hpacker = HPacker(children=[r1, r2], pad=0, sep=0, align=\"center\", mode=mode)\n    ax.add_artist(hpacker)\n    fig.canvas.draw()\n    # Add assertions here to check the layout of children based on the mode\n", "def test_get_packed_offsets_modes(mode, expected):\n    widths = [1, 2, 1, 2]\n    total = 10\n    sep = 2\n    result = _get_packed_offsets(widths, total, sep, mode=mode)\n    assert result[0] == expected[0]\n    assert_allclose(result[1], expected[1])\n", "def test_offsetbox_set_figure():\n    fig, ax = plt.subplots()\n    ob = OffsetBox()\n    ob.set_figure(fig)\n    assert ob.figure == fig\n\n    child_ob = OffsetBox()\n    ob.add_artist(child_ob)\n    assert child_ob.figure == fig\n\n    ax_ob = OffsetBox()\n    ax_ob.axes = ax\n    assert ax_ob.figure == fig\n\n    child_ax_ob = OffsetBox()\n    ax_ob.add_artist(child_ax_ob)\n    assert child_ax_ob.figure == fig\n    assert child_ax_ob.axes == ax\n", "def test_offsetbox_multiline_baseline():\n    # Test that multiline TextArea with multilinebaseline=True is centered\n    # around the baseline of single-line text.\n    text = \"single\\nmultiline\"\n    text_single = \"single\"\n\n    fig, ax = plt.subplots()\n    ta = TextArea(text, multilinebaseline=True)\n    ta_single = TextArea(text_single)\n    da = DrawingArea(50, 50)\n    da.add_artist(ta)\n    da.add_artist(ta_single)\n\n    ax.add_artist(da)\n    fig.canvas.draw()\n\n    renderer = fig.canvas.get_renderer()\n    bb_multi = ta.get_window_extent(renderer)\n    bb_single = ta_single.get_window_extent(renderer)\n\n    assert_allclose(bb_multi.y0, bb_single.y0, atol=1)\n"], "sample_996": ["def test_product_with_zero_factor():\n    assert product(0, (k, 1, n)) == 0\n", "def test_product_symmetry():\n    x, y, a, b = symbols('x y a b', integer=True)\n\n    # Test the symmetry property of products\n    assert Product(x*y, (x, a, b), (y, a, b)) == Product(y*x, (y, a, b), (x, a, b))\n", "def test_issue_14661():\n    n = Symbol('n', integer=True, positive=True)\n    p = Product(1 - 1/n, (n, 1, oo))\n    assert p.doit() == S(1)/2\n", "def test_issue_14827():\n    a, n = symbols('a n', integer=True)\n    assert product(2*a, (n, 1, oo)) == oo\n    assert product(2**n, (n, 1, oo)) == oo\n    assert product(2*a*n, (n, 1, oo)) == oo\n    assert product(2*a*n**2, (n, 1, oo)) == oo\n    assert product(2*a**n, (n, 1, oo)) == oo\n", "def test_issue_17052():\n    a = Symbol('a', positive=True)\n    assert Product(a + Rational(1, 2)**k, (k, 0, oo)).doit() == a * sqrt(1 - a)\n"], "sample_901": ["def test_k_means_sparse_data():\n    # test k_means with sparse data\n    X_sparse = sp.csr_matrix(X)\n    cluster_centers, labels, inertia = k_means(X_sparse, n_clusters=n_clusters,\n                                               sample_weight=None,\n                                               verbose=False)\n    centers = cluster_centers\n    assert centers.shape == (n_clusters, n_features)\n\n    labels = labels\n    assert np.unique(labels).shape[0] == n_clusters\n\n    # check that the labels assignment are perfect (up to a permutation)\n    assert v_measure_score(true_labels, labels) == 1.0\n    assert inertia > 0.0\n", "def test_k_means_with_different_n_jobs():\n    # check that the results are the same with different n_jobs\n    rnd = np.random.RandomState(0)\n    X = rnd.normal(size=(50, 10))\n\n    result_1 = KMeans(n_clusters=3, random_state=0, n_jobs=1).fit(X).labels_\n    result_2 = KMeans(n_clusters=3, random_state=0, n_jobs=2).fit(X).labels_\n    assert_array_equal(result_1, result_2)\n", "def test_k_means_sparse_init():\n    # Check k_means with sparse initialization works as expected\n    sparse_init = sp.csr_matrix(centers)\n    km = KMeans(init=sparse_init, n_clusters=n_clusters, random_state=42, n_init=1)\n    km.fit(X)\n    _check_fitted_model(km)\n", "def test_kmeans_empty_cluster(algo):\n    # check that an error is raised when an empty cluster is found\n    X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n    km = KMeans(n_clusters=3, random_state=0, algorithm=algo)\n    with pytest.raises(ValueError, match='Empty cluster found'):\n        km.fit(X)\n", "def test_k_means_partial_fit_int_data():\n    # Test if partial_fit works with integer data and\n    # the cluster centers are of float type\n    X = np.array([[-1], [1]], dtype=np.int)\n    km = MiniBatchKMeans(n_clusters=2)\n    km.partial_fit(X)\n    assert km.cluster_centers_.dtype.kind == \"f\"\n"], "sample_1137": ["def test_quantity_simplify():\n    assert quantity_simplify(kilo*foot*inch) == 250*foot**2/3\n    assert quantity_simplify(foot - 6*inch) == foot/2\n", "def test_convert_to_with_multiple_units():\n    expr = joule * second / meter\n    target_units = [kilogram, meter, second]\n    converted_expr = convert_to(expr, target_units)\n    expected_expr = kilogram * meter / second**2\n    assert converted_expr == expected_expr\n", "def test_convert_to_with_decimal():\n    from decimal import Decimal\n    q = Quantity(\"q2\")\n    q.set_global_relative_scale_factor(Decimal('5000'), meter)\n\n    assert q.convert_to(m) == Decimal('5000')*m\n", "def test_quantity_simplify():\n    # Test simplification with a prefix\n    assert quantity_simplify(kilo*foot) == 3048*meter\n\n    # Test simplification with a combination of quantities and prefixes\n    assert quantity_simplify(kilo*foot*inch) == 250*foot**2/3\n\n    # Test simplification with addition of quantities\n    assert quantity_simplify(foot - 6*inch) == foot/2\n", "def test_conversion_with_prefixes():\n    assert convert_to(kilo*foot, meter) == 304.8*meter\n    assert convert_to(kilo*foot, centimeter) == 30480*centimeter\n"], "sample_285": ["def test_default_storage_finder_with_no_base_location(self):\n    with mock.patch('django.contrib.staticfiles.finders.default_storage.base_location', None):\n        with self.assertRaisesMessage(ImproperlyConfigured, \"The storage backend of the staticfiles finder 'django.contrib.staticfiles.finders.DefaultStorageFinder' doesn't have a valid location.\"):\n            get_finder('django.contrib.staticfiles.finders.DefaultStorageFinder')\n", "def test_finder_not_subclass_of_base_finder(self):\n    \"\"\"get_finder() raises ImproperlyConfigured if the finder is not a subclass of BaseFinder.\"\"\"\n    with self.assertRaisesMessage(ImproperlyConfigured, 'Finder \"<class \\'str\\'>\" is not a subclass of \"<class \\'django.contrib.staticfiles.finders.BaseFinder\\'>\"'):\n        get_finder('str')\n", "def test_default_storage_finder_without_storage(self):\n    with self.assertRaisesMessage(ImproperlyConfigured, \"The staticfiles storage finder '<class 'django.contrib.staticfiles.finders.DefaultStorageFinder'>' doesn't have a storage class assigned.\"):\n        get_finder('django.contrib.staticfiles.finders.DefaultStorageFinder')\n", "def test_default_storage_finder_without_storage(self):\n    with self.assertRaisesMessage(ImproperlyConfigured,\n                                  \"The staticfiles storage finder '<class 'django.contrib.staticfiles.finders.DefaultStorageFinder'>' doesn't have a storage class assigned.\"):\n        get_finder('django.contrib.staticfiles.finders.DefaultStorageFinder')\n", "def test_default_storage_finder_with_invalid_storage(self):\n    class InvalidStorage:\n        pass\n\n    finder = get_finder('django.contrib.staticfiles.finders.DefaultStorageFinder')\n    finder.storage = InvalidStorage()\n\n    with self.assertRaisesMessage(ImproperlyConfigured, \"The storage backend of the staticfiles finder %r doesn't have a valid location.\" % finder.__class__):\n        finder.find('test_path')\n"], "sample_1150": ["def test_issue_18081():\n    assert ImageSet(Lambda(n, n*log(2)), S.Integers).intersection(S.Integers) == ImageSet(Lambda(n, n*log(2)), S.Integers)\n", "def test_ComplexRegion_contains_symbols():\n    r = Symbol('r', real=True)\n    theta = Symbol('theta', real=True)\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    assert c1.contains(r*exp(I*theta)) == Contains(r*exp(I*theta), c1, evaluate=False)\n", "def test_Range_eval_imageset_with_symbols():\n    a, b, c = symbols('a b c')\n    eq = a*(x + b) + c\n    r = Range(3, -3, -2)\n    imset = imageset(x, eq, r)\n    assert imset.lamda.expr != eq\n    assert list(imset) == [eq.subs(x, i).expand() for i in list(r)]\n\n    _x = symbols('x', negative=True)\n    eq = _x**2 - _x + 1\n    assert imageset(_x, eq, S.Integers).lamda.expr == _x**2 + _x + 1\n    eq = 3*_x - 1\n    assert imageset(_x, eq, S.Integers).lamda.expr == 3*_x + 2\n", "def test_Range_str():\n    assert str(Range(1, 10, 2)) == \"Range(1, 10, 2)\"\n    assert str(Range(1, 10)) == \"Range(1, 10)\"\n    assert str(Range(10)) == \"Range(10)\"\n    assert str(Range(0, oo, 2)) == \"Range(0, oo, 2)\"\n    assert str(Range(oo, -oo, -1)) == \"Range(oo, -oo, -1)\"\n    assert str(Range(x, x + 4, 5)) == \"Range(x, x + 4, 5)\"\n    assert str(Range(x, y, t)) == \"Range(x, y, t)\"\n    assert str(Range(i, i + 19, 2)) == \"Range(i, i + 19, 2)\"\n    assert str(Range(i, i*8, 3*i)) == \"Range(i, i*8, 3*i)\"\n", "def test_range_interval_intersection_with_symbols():\n    x = Symbol('x')\n    assert Range(0, x).intersect(Interval(1, 2)) == Range(1, x)\n    assert Range(0, x).intersect(Interval(x+1, oo)) == Range(x+1, x)\n    assert Range(x, oo).intersect(Interval(-oo, x-1)) == Range(x, x-1)\n    assert Range(x, oo).intersect(Interval(-oo, 0)) == Range(x, 0)\n    assert Range(-oo, x).intersect(Interval(0, x-1)) == Range(0, x-1)\n    assert Range(-oo, x).intersect(Interval(x+1, oo)) == Range(x+1, x)\n    assert Range(x, oo).intersect(Interval(0, oo)) == Range(x, oo)\n    assert Range(-oo, x).intersect(Interval(-oo, 0)) == Range(-oo, x)\n    assert Range(-oo, oo).intersect(Interval(x, oo)) == Range(x, oo)\n    assert Range(-oo, oo).intersect(Interval(-oo, x)) == Range(-oo, x)\n"], "sample_492": ["def test_serialize_custom_field(self):\n    custom_field = CustomField(max_length=200)\n    self.assertSerializedEqual(custom_field)\n    string, imports = MigrationWriter.serialize(custom_field)\n    self.assertEqual(\n        string,\n        \"migrations.test_writer.CustomField(max_length=200)\",\n    )\n    self.assertIn(\"import migrations.test_writer\", imports)\n", "def test_serialize_complex_number(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(3, 4),\n        (\"complex('3+4j')\", set()),\n    )\n", "def test_serialize_custom_objects(self):\n    obj = DeconstructibleInstances()\n    string, imports = MigrationWriter.serialize(obj)\n    self.assertEqual(string, \"migrations.test_writer.DeconstructibleInstances()\")\n    self.assertIn(\"import migrations.test_writer\", imports)\n", "def test_serialize_functions_in_models(self):\n    \"\"\"\n    Make sure functions defined in models can be serialized.\n    \"\"\"\n    class TestModel3:\n            return \"default\"\n\n        field = models.CharField(default=get_default)\n\n    self.assertSerializedEqual(TestModel3.field)\n\n    field = models.CharField(default=TestModel3.get_default)\n    with self.assertRaisesMessage(ValueError, \"Could not find function get_default in migrations.test_writer\"):\n        self.serialize_round_trip(field)\n", "def test_serialize_bytes(self):\n    self.assertSerializedEqual(b\"foo\\x00bar\")\n    string, imports = MigrationWriter.serialize(b\"foo\\x00bar\")\n    self.assertEqual(string, \"b'foo\\\\x00bar'\")\n"], "sample_940": ["def test_is_singledispatch_function():\n    @functools.singledispatch\n        pass\n\n    @functools.singledispatch\n        pass\n\n    class Foo:\n        @functools.singledispatchmethod\n            pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func_method) is False\n    assert inspect.is_singledispatch_function(Foo.method) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @func.register(int)\n        pass\n\n    class C:\n        @singledispatch\n            pass\n\n        @method.register(str)\n            pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(func.register) is False\n    assert inspect.is_singledispatch_function(C().method) is False\n    assert inspect.is_singledispatch_function(C.method) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @func.register\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(_) is False\n    assert inspect.is_singledispatch_function(str) is False\n", "def test_evaluate_signature():\n             *args: 'T', **kwargs: 'Dict[str, T]') -> 'List[T]':\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig, globals(), locals())\n\n    assert str(evaluated_sig.parameters['x'].annotation) == \"typing.List[~T]\"\n    assert str(evaluated_sig.parameters['y'].annotation) == \"typing.Callable[[int, str], ~T]\"\n    assert str(evaluated_sig.parameters['z'].annotation) == \"~T\"\n    assert str(evaluated_sig.parameters['args'].annotation) == \"~T\"\n    assert str(evaluated_sig.parameters['kwargs'].annotation) == \"typing.Dict[str, ~T]\"\n    assert str(evaluated_sig.return_annotation) == \"typing.List[~T]\"\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(f)\n    evaluated_sig = inspect.evaluate_signature(sig, globals(), locals())\n    assert evaluated_sig.parameters['a'].annotation == typing.TypeVar('T')\n    assert evaluated_sig.parameters['b'].annotation == typing.Union[typing.TypeVar('T'), None]\n    assert evaluated_sig.return_annotation == typing.Union[typing.TypeVar('T'), None]\n"], "sample_1176": ["def test_floordiv_with_Float():\n    assert S(2.5)//S.Half == 5.0\n", "def test_floordiv_Integer():\n    assert Integer(10) // Integer(3) == 3\n    assert Integer(10) // 3 == 3\n    assert 10 // Integer(3) == 3\n\n    assert Integer(10) // Integer(-3) == -4\n    assert Integer(10) // -3 == -4\n    assert 10 // Integer(-3) == -4\n\n    assert Integer(-10) // Integer(3) == -4\n    assert Integer(-10) // 3 == -4\n    assert -10 // Integer(3) == -4\n\n    assert Integer(-10) // Integer(-3) == 3\n    assert Integer(-10) // -3 == 3\n    assert -10 // Integer(-3) == 3\n\n    raises(TypeError, lambda: Integer(10) // 0.0)\n    raises(TypeError, lambda: 0.0 // Integer(10))\n    raises(ZeroDivisionError, lambda: Integer(10) // 0)\n    raises(ZeroDivisionError, lambda: 10 // Integer(0))\n", "def test_floordiv_Integer():\n    assert Integer(10) // Integer(3) == 3\n    assert Integer(10) // Integer(-3) == -4\n    assert Integer(-10) // Integer(3) == -4\n    assert Integer(-10) // Integer(-3) == 3\n\n    assert Integer(10) // 3 == 3\n    assert Integer(10) // -3 == -4\n    assert Integer(-10) // 3 == -4\n    assert Integer(-10) // -3 == 3\n\n    assert 10 // Integer(3) == 3\n    assert 10 // Integer(-3) == -4\n    assert -10 // Integer(3) == -4\n    assert -10 // Integer(-3) == 3\n\n    raises(TypeError, lambda: Integer(10) // 3.5)\n    raises(TypeError, lambda: 10 // Integer(3.5))\n\n    raises(ZeroDivisionError, lambda: Integer(10) // Integer(0))\n    raises(ZeroDivisionError, lambda: Integer(10) // 0)\n    raises(ZeroDivisionError, lambda: 10 // Integer(0))\n", "def test_floordiv_with_float():\n    assert S(2.0)//S.Half == 4.0\n    assert S(2.0)//S(1.5) == 1.0\n    assert S(-2.0)//S.Half == -4.0\n    assert S(-2.0)//S(-1.5) == 1.0\n", "def test_Float_floordiv():\n    assert Float(3.2)//Float(1.2) == 2.0\n    assert Float(3.2)//1.2 == 2.0\n    assert 3.2//Float(1.2) == 2.0\n    assert Float(3.2)//Float('1.2') == 2.0\n"], "sample_254": ["def test_inline_verbose_name_plural(self):\n    \"\"\"\n    The item added by the \"Add another XXX\" link must use the correct\n    verbose_name_plural in the inline form.\n    \"\"\"\n    self.admin_login(username='super', password='secret')\n    # Hide sidebar.\n    self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_course_add'))\n    toggle_button = self.selenium.find_element_by_css_selector('#toggle-nav-sidebar')\n    toggle_button.click()\n    # Each combination of horizontal/vertical filter with stacked/tabular\n    # inlines.\n    tests = [\n        'admin:admin_inlines_course_add',\n        'admin:admin_inlines_courseproxy_add',\n        'admin:admin_inlines_courseproxy1_add',\n        'admin:admin_inlines_courseproxy2_add',\n    ]\n    css_selector = '.dynamic-class_set#class_set-%s h2'\n\n    for url_name in tests:\n        with self.subTest(url=url_name):\n            self.selenium.get(self.live_server_url + reverse(url_name))\n            # First inline shows the verbose_name_plural.\n            available, chosen = self.selenium.find_elements_by_css_selector(css_selector % 0)\n            self.assertEqual(available.text, 'AVAILABLE ATTENDANTS')\n            self.assertEqual(chosen.text, 'CHOSEN ATTENDANTS')\n            # Added inline should also have the correct verbose_name_plural.\n            self.selenium.find_element_by_link_text('Add another Class').click()\n            available, chosen = self.selenium.find_elements_by_css_selector(css_selector % 1)\n            self.assertEqual(available.text, 'AVAILABLE ATTENDANTS')\n            self.assertEqual(chosen.text, 'CHOSEN ATTENDANTS')\n            # Third inline should also have the correct", "def test_inlines_plural_heading_many_to_many(self):\n    response = self.client.get(reverse('admin:admin_inlines_author_add'))\n    self.assertContains(response, '<h2>Non-autopk books</h2>', html=True)\n    self.assertContains(response, '<h2>Non-autopk book children</h2>', html=True)\n    self.assertContains(response, '<h2>Editablepk books</h2>', html=True)\n", "def test_inline_custom_get_extra(self):\n    holder = Holder.objects.create(dummy=13)\n    Inner.objects.create(dummy=42, holder=holder)\n    Inner.objects.create(dummy=43, holder=holder)\n\n    class CustomInline(StackedInline):\n        model = Inner\n\n            if obj:\n                return 1\n            return 0\n\n    modeladmin = ModelAdmin(Holder, admin_site)\n    modeladmin.inlines = [CustomInline]\n    request = self.factory.get(reverse('admin:admin_inlines_holder_change', args=(holder.id,)))\n    request.user = User(username='super', is_superuser=True)\n    response = modeladmin.changeform_view(request, object_id=str(holder.id))\n    self.assertContains(response, '<input type=\"hidden\" name=\"inner_set-TOTAL_FORMS\" value=\"3\" id=\"id_inner_set-TOTAL_FORMS\">')\n\n    request = self.factory.get(reverse('admin:admin_inlines_holder_add'))\n    request.user = User(username='super', is_superuser=True)\n    response = modeladmin.changeform_view(request)\n    self.assertContains(response, '<input type=\"hidden\" name=\"inner_set-TOTAL_FORMS\" value=\"1\" id=\"id_inner_set-TOTAL_FORMS\">')\n", "def test_inline_model_admin_verbose_name(self):\n    \"\"\"\n    The title of the inline form uses the correct verbose_name.\n    \"\"\"\n    self.admin_login(username='super', password='secret')\n    self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n    stacked_inline_title_selector = 'div#inner5stacked_set-group div.inline-group h2'\n    tabular_inline_title_selector = 'div#inner5tabular_set-group div.inline-group h2'\n    stacked_inline_title = self.selenium.find_element_by_css_selector(stacked_inline_title_selector)\n    tabular_inline_title = self.selenium.find_element_by_css_selector(tabular_inline_title_selector)\n    self.assertEqual(stacked_inline_title.text, 'Inner5 stacked')\n    self.assertEqual(tabular_inline_title.text, 'Inner5 tabular')\n", "def test_inline_form_can_delete(self):\n    \"\"\"\n    Test that the inline form's can_delete attribute is set correctly\n    based on the parent object's permission.\n    \"\"\"\n    holder = Holder.objects.create(dummy=1)\n    inner = Inner.objects.create(dummy=1, holder=holder)\n    response = self.client.get(reverse('admin:admin_inlines_holder_change', args=(holder.pk,)))\n    self.assertTrue(response.context['inline_admin_formset'].can_delete)\n    self.client.logout()\n    self.client.force_login(User.objects.create_user(username='noperms', password='password'))\n    response = self.client.get(reverse('admin:admin_inlines_holder_change', args=(holder.pk,)))\n    self.assertFalse(response.context['inline_admin_formset'].can_delete)\n"], "sample_665": ["def test_collect_subdir_with_init_and_symlinks(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Create a broken symlink.\n    sub.join(\"test_broken.py\").mksymlinkto(\"test_doesnotexist.py\")\n\n    # Symlink that gets collected.\n    sub.join(\"test_symlink.py\").mksymlinkto(\"test_file.py\")\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub/test_file.py::test_file PASSED*\",\n            \"sub/test_symlink.py::test_file PASSED*\",\n            \"*2 passed in*\",\n        ]\n    )\n", "def test_collect_with_tb_style_native(testdir):\n    \"\"\"Check that the 'native' tbstyle is respected during collection.\"\"\"\n    p1 = testdir.makepyfile(\"assert 0\")\n    result = testdir.runpytest(p1, \"--tb=native\")\n    assert result.ret == ExitCode.INTERRUPTED\n    result.stdout.fnmatch_lines(\n        [\n            \"*_ ERROR collecting test_collect_with_tb_style_native.py _*\",\n            \"Traceback (most recent call last):\",\n            '  File \"*/test_collect_with_tb_style_native.py\", line 1, in <module>',\n            \"    assert 0\",\n            \"AssertionError: assert 0\",\n            \"*! Interrupted: 1 errors during collection !*\",\n            \"*= 1 error in *\",\n        ]\n    )\n", "def test_collect_file_hook_with_custom_file_type(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        class MyCustomFile(pytest.File):\n                return [MyCustomItem(name=\"test_custom\", parent=self)]\n\n        class MyCustomItem(pytest.Item):\n                pass\n\n            if path.ext == \".custom\":\n                return MyCustomFile(path, parent)\n        \"\"\"\n    )\n    p = testdir.makefile(\".custom\", \"\")\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    res = testdir.runpytest(\"%s::test_custom\" % p.basename)\n    res.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_collect_handles_raising_on_dunder_init(testdir):\n    \"\"\"Handle classes that might raise on ``__init__`` (#4266).\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        class RaisesOnInit(object):\n                raise Exception\n\n        raises = RaisesOnInit()\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n    assert result.ret == 0\n", "def test_parametrized_fixtures(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=[\"param1\", \"param2\"])\n            return request.param\n\n            assert fixture_with_params in [\"param1\", \"param2\"]\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*2 passed*\"])\n"], "sample_57": ["    def test_custom_user_creation(self):\n        data = {\n            'username': 'customuser',\n            'password1': 'testpassword123',\n            'password2': 'testpassword123',\n            'email': 'customuser@example.com',\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.username, 'customuser')\n        self.assertEqual(user.email, 'customuser@example.com')\n        self.assertTrue(user.check_password('testpassword123'))\n", "def test_password_reset_form_custom_subject_template(self):\n    (user, username, email) = self.create_dummy_user()\n    data = {\"email\": email}\n    form = PasswordResetForm(data)\n    self.assertTrue(form.is_valid())\n    form.save(subject_template_name='registration/custom_password_reset_subject.txt')\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertEqual(mail.outbox[0].subject, 'Custom subject for password reset')\n", "    def test_widget_attrs(self):\n        field = UsernameField()\n        widget_attrs = field.widget_attrs(forms.TextInput())\n        self.assertEqual(widget_attrs['autocapitalize'], 'none')\n", "    def test_integer_username(self):\n        data = {\n            'username': 12345,\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(User.objects.filter(username=data['username']).count(), 1)\n", "    def test_unicode_normalization(self):\n        # Test that unicode normalization occurs in the form\n        ohm_username = 'test\\u2126'  # U+2126 OHM SIGN\n        omega_username = 'test\\u03A9'  # U+03A9 GREEK CAPITAL LETTER OMEGA\n        self.assertNotEqual(ohm_username, omega_username)\n        data = {\n            'username': ohm_username,\n            'password1': 'pwd1',\n            'password2': 'pwd1',\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.username, omega_username)\n"], "sample_569": ["def test_regplot_line_kws(self):\n\n    f, ax = plt.subplots()\n    color = 'r'\n    linewidth = 2\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df,\n                    line_kws={'color': color, 'linewidth': linewidth})\n    assert ax.lines[0].get_color() == color\n    assert ax.lines[0].get_linewidth() == linewidth\n", "def test_regplot_with_numpy_arrays(self):\n    x = self.df['x'].values\n    y = self.df['y'].values\n    ax = lm.regplot(x=x, y=y)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n    x_plot, y_plot = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, x_plot)\n    npt.assert_array_equal(y, y_plot)\n", "def test_regplot_line_kws(self):\n    # Test the line_kws argument for regplot\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, line_kws={'linestyle': '--'})\n    assert ax.lines[0].get_linestyle() == '--'\n", "def test_regplot_ylim(self):\n    f, ax = plt.subplots()\n    x, y1, y2 = np.random.randn(3, 50)\n    lm.regplot(x=x, y=y1, truncate=False)\n    lm.regplot(x=x, y=y2, truncate=False)\n    line1, line2 = ax.lines\n    assert np.array_equal(line1.get_ydata(), line2.get_ydata())\n", "def test_regplot_x_partial_y_partial(self):\n\n    x = self.rs.randn(100)\n    y = x + self.rs.randn(100)\n    z = x + self.rs.randn(100)\n\n    p = lm._RegressionPlotter(y, z)\n    _, r_orig = np.corrcoef(p.x, p.y)[0]\n\n    p = lm._RegressionPlotter(y, z, x_partial=x, y_partial=x)\n    _, r_partial = np.corrcoef(p.x, p.y)[0]\n    assert r_partial < r_orig\n\n    x = pd.Series(x)\n    y = pd.Series(y)\n    p = lm._RegressionPlotter(y, z, x_partial=x, y_partial=x)\n    _, r_partial = np.corrcoef(p.x, p.y)[0]\n    assert r_partial < r_orig\n"], "sample_482": ["def test_empty(self):\n    output = self.engine.render_to_string(\"escapeseq_empty\", {\"a\": []})\n    self.assertEqual(output, \"\")\n", "def test_safe_strings(self):\n    output = self.engine.render_to_string(\n        \"escapeseq_safe_strings\", {\"a\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]}\n    )\n    self.assertEqual(output, \"x&y, <p>\")\n", "def test_empty(self):\n    output = self.engine.render_to_string(\"escapeseq_empty\", {\"a\": []})\n    self.assertEqual(output, \"\")\n", "def test_empty(self):\n    output = self.engine.render_to_string(\"escapeseq_empty\", {\"a\": []})\n    self.assertEqual(output, \"\")\n", "def test_empty_sequence(self):\n    output = self.engine.render_to_string(\"escapeseq_empty\", {\"a\": []})\n    self.assertEqual(output, \"\")\n"], "sample_852": ["def test_make_classification_hypercube_reproducibility():\n    \"\"\"Test that the hypercube generation in make_classification is reproducible\"\"\"\n    X1, _ = make_classification(n_samples=100, n_features=20, n_informative=5,\n                                n_redundant=1, n_repeated=1, n_classes=3,\n                                n_clusters_per_class=1, hypercube=True,\n                                shift=None, scale=None, weights=None,\n                                random_state=0)\n\n    X2, _ = make_classification(n_samples=100, n_features=20, n_informative=5,\n                                n_redundant=1, n_repeated=1, n_classes=3,\n                                n_clusters_per_class=1, hypercube=True,\n                                shift=None, scale=None, weights=None,\n                                random_state=0)\n\n    assert_array_equal(X1, X2, \"Hypercube generation is not reproducible\")\n", "def test_make_classification_n_features():\n    \"\"\"Test the total number of features in make_classification\"\"\"\n    n_features = 20\n    n_informative = 5\n    n_redundant = 2\n    n_repeated = 1\n    n_useless = n_features - n_informative - n_redundant - n_repeated\n\n    X, y = make_classification(n_samples=100, n_features=n_features,\n                               n_informative=n_informative,\n                               n_redundant=n_redundant,\n                               n_repeated=n_repeated,\n                               n_classes=2, n_clusters_per_class=1,\n                               random_state=0)\n\n    assert X.shape == (100, n_features), \"X shape mismatch\"\n    assert np.sum(np.var(X, axis=0) == 0) == n_useless, \"Unexpected number of useless features\"\n", "def test_make_classification_error_conditions():\n    # Test error conditions for make_classification\n\n    # Test that n_informative + n_redundant + n_repeated > n_features raises a ValueError\n    with pytest.raises(ValueError):\n        make_classification(n_samples=10, n_features=5, n_informative=2, n_redundant=2, n_repeated=2)\n\n    # Test that n_informative < np.log2(n_classes * n_clusters_per_class) raises a ValueError\n    with pytest.raises(ValueError):\n        make_classification(n_samples=10, n_features=10, n_informative=1, n_classes=2, n_clusters_per_class=2)\n\n    # Test that shift is not None and not an array of shape [n_features] raises a ValueError\n    with pytest.raises(ValueError):\n        make_classification(n_samples=10, n_features=10, shift=[1, 2, 3])\n\n    # Test that scale is not None and not an array of shape [n_features] raises a ValueError\n    with pytest.raises(ValueError):\n        make_classification(n_samples=10, n_features=10, scale=[1, 2, 3])\n", "def test_make_classification_redundant_features():\n    \"\"\"Test the construction of redundant features in make_classification\"\"\"\n    # Create a dataset with redundant features; check that they are linear combinations\n    # of informative features\n    make = partial(make_classification, n_redundant=2, n_repeated=0, flip_y=0,\n                   shift=0, scale=1, shuffle=False, random_state=0)\n\n    n_informative = 3\n    n_samples = 100\n    n_features = n_informative + 2\n\n    X, y = make(n_samples=n_samples, n_features=n_features,\n                n_informative=n_informative)\n\n    assert X.shape == (n_samples, n_features)\n    assert y.shape == (n_samples,)\n\n    # Check that redundant features are linear combinations of informative features\n    for i in range(n_informative, n_features):\n        # Solve for coefficients of linear combination\n        coeffs, _, _, _ = np.linalg.lstsq(X[:, :n_informative], X[:, i], rcond=None)\n        # Check that redundant feature is close to linear combination of informative features\n        assert_array_almost_equal(X[:, i], np.dot(X[:, :n_informative], coeffs),\n                                  decimal=5, err_msg=\"Redundant feature is not \"\n                                                     \"a linear combination of \"\n                                                     \"informative features\")\n", "def test_make_classification_repeated_features():\n    \"\"\"Test the construction of repeated features in make_classification\"\"\"\n    n_informative = 2\n    n_redundant = 2\n    n_repeated = 2\n    n_features = n_informative + n_redundant + n_repeated\n    X, y = make_classification(n_samples=100, n_features=n_features,\n                               n_informative=n_informative,\n                               n_redundant=n_redundant, n_repeated=n_repeated,\n                               n_classes=2, n_clusters_per_class=1,\n                               hypercube=False, shift=0, scale=1,\n                               random_state=0)\n\n    assert X.shape == (100, n_features), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n\n    # Check that the repeated features are duplicates of the informative or\n    # redundant features\n    informative_features = X[:, :n_informative]\n    redundant_features = X[:, n_informative:n_informative + n_redundant]\n    repeated_features = X[:, n_informative + n_redundant:]\n\n    for rf in repeated_features.T:\n        assert (np.all(rf == informative_features[np.argmax(np.sum(informative_features == rf, axis=0)), :]) or\n                np.all(rf == redundant_features[np.argmax(np.sum(redundant_features == rf, axis=0)), :])), (\n            \"Repeated feature is not a duplicate of informative or redundant features\")\n"], "sample_436": ["def test_help_with_settings(self):\n    \"\"\"--help displays the correct usage with --settings.\"\"\"\n    args = [\"help\", \"--settings=test_project.settings\"]\n    out, err = self.run_django_admin(args)\n    self.assertNoOutput(err)\n    self.assertOutput(out, \"Usage: django-admin.py help [--settings=SETTINGS] [subcommand]\")\n", "def test_runserver_with_noreload(self):\n    with mock.patch(\"django.core.management.commands.runserver.run\") as mock_run:\n        call_command(\"runserver\", use_reloader=False)\n    mock_run.assert_called_once_with(\n        addrport=\"\",\n        ipv6=False,\n        threading=False,\n        wsgi_handler=None,\n        use_reloader=False,\n        static_handler=None,\n        insecure_serving=False,\n    )\n", "def test_unicode_name(self):\n    \"\"\"startapp creates the correct directory with Unicode characters.\"\"\"\n    args = [\"startapp\", \"\u3053\u3093\u306b\u3061\u306f\"]\n    app_path = os.path.join(self.test_dir, \"\u3053\u3093\u306b\u3061\u306f\")\n    out, err = self.run_django_admin(args, \"test_project.settings\")\n    self.assertNoOutput(err)\n    self.assertTrue(os.path.exists(app_path))\n    with open(os.path.join(app_path, \"apps.py\"), encoding=\"utf8\") as f:\n        content = f.read()\n        self.assertIn(\"class \u3053\u3093\u306b\u3061\u306fConfig(AppConfig)\", content)\n        self.assertIn('name = \"\u3053\u3093\u306b\u3061\u306f\"' if HAS_BLACK else \"name = '\u3053\u3093\u306b\u3061\u306f'\", content)\n", "def test_complex_project(self):\n    \"\"\"Make sure the startproject management command creates a complex project\"\"\"\n    template_path = os.path.join(custom_templates_dir, \"project_template\")\n    args = [\n        \"startproject\",\n        \"--template\",\n        template_path,\n        \"--extension=py,txt\",\n        \"complex_testproject\",\n    ]\n    testproject_dir = os.path.join(self.test_dir, \"complex_testproject\")\n\n    out, err = self.run_django_admin(args)\n    self.assertNoOutput(err)\n    self.assertTrue(os.path.isdir(testproject_dir))\n    for f in (\"complex_testproject\", \"additional_dir\", \"requirements.txt\"):\n        self.assertTrue(os.path.exists(os.path.join(testproject_dir, f)))\n", "    def test_exclude_app_labels_in_command_line(self):\n        \"\"\"makemessages exclude app labels in command line.\"\"\"\n        self.write_settings(\"settings.py\", apps=[\"admin_scripts.simple_app\"])\n        args = [\"makemessages\", \"-e\", \"simple_app\"]\n        out, err = self.run_manage(args)\n        self.assertNoOutput(err)\n        self.assertOutput(\n            out,\n            \"processing locale en\\n\"\n            \"no local paths found in locale en\\n\"\n            \"no messages to process in locale en\",\n        )\n"], "sample_15": ["def test_radian_invalid_units(self, function):\n    with pytest.raises(u.UnitsError):\n        function(3.0 * u.s, 2.0 * u.m, 1.0 * u.kg)\n", "def test_bessel_scalar(self, function):\n    q = function(3.0 * u.m / (6.0 * u.m))\n    assert q.unit == u.dimensionless_unscaled\n    assert q.value == function(0.5)\n", "def test_hypot_invalid_units(self):\n    q_i1 = 4.7 * u.m\n    q_i2 = 9.4 * u.s\n    with pytest.raises(u.UnitsError, match=\"incompatible units\"):\n        np.hypot(q_i1, q_i2)\n", "def test_scipy_registration_missing():\n    \"\"\"Check that missing scipy functions raise ImportError.\"\"\"\n    with pytest.raises(ImportError, match=\"scipy is required for this function\"):\n        sps.missing_function(1.0 * u.percent)\n", "def test_deg2rad(self, function):\n    q1 = function(180.0 * u.degree)\n    assert_allclose(q1.value, np.pi)\n    assert q1.unit == u.radian\n\n    q2 = function(0.0 * u.degree, degrees=True)\n    assert_allclose(q2.value, 0.0)\n    assert q2.unit == u.radian\n\n    q3 = function(360.0 * u.degree, degrees=False)\n    assert_allclose(q3.value, 2 * np.pi)\n    assert q3.unit == u.radian\n\n    with pytest.raises(TypeError):\n        function(3.0 * u.m, degrees=True)\n"], "sample_534": ["def test_contour_linestyles(style):\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    # Test contour linestyles\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z, 6, colors='k', linestyles=style)\n    ax.clabel(CS, fontsize=9, inline=True)\n    ax.set_title(f'Single color - linestyles {style}')\n    assert CS.linestyles == style\n", "def test_contour_auto_label_bool():\n    ax = plt.figure().add_subplot()\n    ax.contour(np.arange(16).reshape((4, 4)).astype(bool), colors='k')\n    ax.clabel()\n    assert {text.get_text() for text in ax.texts} == {\"0.5\"}\n", "def test_contour_filled_nchunk(algorithm):\n    x = np.arange(10)\n    y = np.arange(9)\n    xg, yg = np.meshgrid(x, y)\n    z = np.random.random((9, 10))\n\n    fig, ax = plt.subplots()\n    cs1 = ax.contourf(xg, yg, z, algorithm=algorithm, nchunk=1)\n    cs2 = ax.contourf(xg, yg, z, algorithm=algorithm, nchunk=2)\n\n    assert len(cs1.collections[0].get_paths()) > len(cs2.collections[0].get_paths())\n", "def test_nchunk(algorithm, nchunk):\n    z = np.array([[1.0, 2.0], [3.0, 4.0]])\n    plt.contourf(z, algorithm=algorithm, nchunk=nchunk)\n", "def test_contourf_hatches():\n    # Test hatches for filled contours\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n\n    hatches = ['/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*']\n    contourf = axs[0].contourf(x, y, z, levels=np.arange(0, 10, 1), hatches=hatches)\n    axs[0].set_title('All hatches')\n\n    # Test hatches with fewer levels\n    contourf = axs[1].contourf(x, y, z, levels=np.arange(0, 5, 1), hatches=hatches)\n    axs[1].set_title('Fewer levels')\n\n    # Test hatches with more levels\n    contourf = axs[2].contourf(x, y, z, levels=np.arange(0, 15, 1), hatches=hatches)\n    axs[2].set_title('More levels')\n"], "sample_271": ["def test_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "def test_tick_triggers_on_file_creation(self, mock_notify_file_changed):\n    new_file = self.tempdir / 'new_file.py'\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, new_file]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        self.ensure_file(new_file)\n        next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n        self.assertEqual(mock_notify_file_changed.call_args[0][0], new_file)\n", "def test_file_deleted(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        self.existing_file.unlink()\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "def test_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "def test_non_existing_file_in_sys_path(self, mocked_modules, notify_mock):\n    nonexistent_file = self.tempdir / 'nonexistent_file.py'\n    with extend_sys_path(str(nonexistent_file)):\n        with self.tick_twice():\n            pass\n    self.assertFalse(notify_mock.called)\n"], "sample_427": ["def test_formset_initial_data_with_extra(self):\n    \"\"\"\n    A FormSet can be prefilled with existing data by providing a list of\n    dicts to the `initial` argument. The `extra` argument specifies additional\n    empty forms to display.\n    \"\"\"\n    initial = [{\"choice\": \"Calexico\", \"votes\": 100}]\n    ChoiceFormSet = formset_factory(Choice, extra=3)\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-0-votes\" value=\"100\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-1-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-1-votes\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-2-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-2-votes\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-3-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-3-votes\"></li>',\n    )\n", "def test_custom_delete_widget(self):\n    class CustomDeleteFormSet(BaseFormSet):\n        deletion_widget = CheckboxInput(attrs={'class': 'custom-delete'})\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomDeleteFormSet, can_delete=True)\n    formset = ChoiceFormSet()\n    self.assertIn('custom-delete', formset.forms[0].as_p())\n", "def test_formset_with_custom_default_error_messages(self):\n    \"\"\"\n    Formsets can have custom default error messages.\n    \"\"\"\n    class CustomFormSet(BaseFormSet):\n        default_error_messages = {\n            \"missing_management_form\": \"Custom missing management form error message.\",\n        }\n\n    CustomChoiceFormSet = formset_factory(Choice, formset=CustomFormSet)\n    formset = CustomChoiceFormSet()\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"Custom missing management form error message.\"],\n    )\n", "def test_formset_with_invalid_initial_data(self):\n    \"\"\"\n    Formset handles invalid initial data.\n    \"\"\"\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": \"invalid\"},\n    ]\n    formset = self.make_choiceformset(initial=initial)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.errors,\n        [{\"votes\": [\"Enter a whole number.\"]}, {}],\n    )\n", "def test_default_template_name(self):\n    \"\"\"The formset uses the template_name attribute of the renderer.\"\"\"\n    formset = self.make_choiceformset()\n    self.assertEqual(formset.template_name, \"django/forms/formsets/div.html\")\n\n    class CustomRenderer(TemplatesSetting):\n        formset_template_name = \"custom_template.html\"\n\n    ChoiceFormSet = formset_factory(Choice, renderer=CustomRenderer)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"custom_template.html\")\n"], "sample_672": ["def test_nested_data_structure():\n    class Nested:\n            self.data = data\n\n            return f'Nested({self.data})'\n\n    nested_obj = Nested(Nested(Nested(1)))\n    assert saferepr(nested_obj) == 'Nested(Nested(Nested(1)))'\n", "def test_maxsize_error_on_class():\n    class A:\n            raise ValueError(\"...\")\n\n    s = saferepr(A, maxsize=25)\n    assert len(s) == 25\n    assert \"ValueError\" in s\n", "def test_large_custom_object():\n    class LargeCustomObject:\n            return 'x' * 1000\n\n    s = saferepr(LargeCustomObject(), maxsize=25)\n    assert len(s) == 25\n    assert s == \"'xxx...xxx'\"\n", "def test_large_maxsize():\n    # Test saferepr with a maxsize larger than the input length\n    s = saferepr(\"test\", maxsize=100)\n    assert s == \"'test'\"\n", "def test_nested_exceptions():\n    class InnerBrokenRepr:\n            raise ValueError(\"inner error\")\n\n    class OuterBrokenRepr:\n            return \"Outer({!r})\".format(InnerBrokenRepr())\n\n    s = saferepr(OuterBrokenRepr())\n    assert \"ValueError\" in s\n    assert \"InnerBrokenRepr\" in s\n    assert \"Outer\" in s\n"], "sample_1066": ["def test_print_acosh():\n    assert mathml(acosh(x), printer='presentation') == \\\n        '<mrow><mo>arccosh</mo><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mathml(acosh(x + 1), printer='presentation') == \\\n        '<mrow><mo>arccosh</mo><mfenced><mrow><mi>x</mi><mo>+</mo><mn>1</mn></mrow></mfenced></mrow>'\n", "def test_print_RootOf():\n    assert mathml(RootOf(x**2 - 1, 0), printer='presentation') == \\\n        '<mrow><msqrt><mrow><mn>1</mn><mo>+</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></msqrt><mfenced><mn>0</mn></mfenced></mrow>'\n", "def test_mathml_binary_functions():\n    # Test binary functions\n    assert mathml(gcd(x, y), printer='presentation') == \\\n        '<mrow><mo>gcd</mo><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mathml(lcm(x, y), printer='presentation') == \\\n        '<mrow><mo>lcm</mo><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow>'\n    assert mathml(Pow(x, y), printer='presentation') == \\\n        '<msup><mi>x</mi><mi>y</mi></msup>'\n    assert mathml(Pow(x, -y), printer='presentation') == \\\n        '<mfrac><mn>1</mn><msup><mi>x</mi><mi>y</mi></msup></mfrac>'\n    assert mathml(Pow(x, Rational(1, 2)), printer='presentation') == \\\n        '<msqrt><mi>x</mi></msqrt>'\n    assert mathml(Pow(x, Rational(1, 3)), printer='presentation') == \\\n        '<mroot><mi>x</mi><mn>3</mn></mroot>'\n", "def test_print_complex_matrix():\n    m = Matrix([[1+I, 2-I], [3, 4]])\n    assert mathml(m, printer='presentation') == '<mtable><mtr><mtd><mrow><mn>1</mn><mo>+</mo><mi>&ImaginaryI;</mi></mrow></mtd><mtd><mrow><mn>2</mn><mo>-</mo><mi>&ImaginaryI;</mi></mrow></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable>'\n", "def test_print_rotation_matrix():\n    theta = Symbol('theta')\n    assert mathml(rotation_matrix(2, theta), printer='presentation') == \\\n        '<mfenced close=\"]\" open=\"[\"><mtable><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>-</mo><mfrac><mi>&#x221B;3</mi><mn>2</mn></mfrac><mi>&#x3C0;</mi><mi>&#x2212;</mi><mi>theta</mi><mo>&#x2212;</mo><mi>&#x3C0;</mi></mtd></mtr><mtr><mtd><mfrac><mi>&#x221B;3</mi><mn>2</mn></mfrac><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>&#x3C0;</mi><mi>theta</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>&#x2212;</mo><mn>1</mn></mtd></mtr></mtable></mfenced>'\n"], "sample_1042": ["def test_Indexed_derivative_with_KroneckerDelta():\n    i, j, k = symbols('i j k', cls=Idx)\n    A = IndexedBase('A')\n    expr = A[i]*KroneckerDelta(i, j)\n    assert expr.diff(A[k]) == KroneckerDelta(i, k)*KroneckerDelta(i, j)\n    assert expr.diff(A[j]) == KroneckerDelta(i, j)\n    assert expr.diff(A[i]) == KroneckerDelta(i, j)\n", "def test_IndexedBase_with_array():\n    A = Array([1, 2, 3])\n    B = IndexedBase(A)\n    assert B[0] == 1\n    assert B[1] == 2\n    assert B[2] == 3\n", "def test_Indexed_str():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, j)\n    assert str(A) == 'A[i, j]'\n    assert repr(A) == 'Indexed(Symbol(\"A\"), i, j)'\n\n    B = IndexedBase('B')\n    assert str(B) == 'B'\n    assert repr(B) == 'IndexedBase(Symbol(\"B\"))'\n", "def test_IndexedBase_strides():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', strides=(2, 3))\n    assert a[i, j].strides == (2, 3)\n    assert a[i, j].offset == 0\n\n    b = IndexedBase('b', strides='C')\n    assert b[i, j].strides == 'C'\n    assert b[i, j].offset == 0\n\n    c = IndexedBase('c', strides='F')\n    assert c[i, j].strides == 'F'\n    assert c[i, j].offset == 0\n\n    d = IndexedBase('d', offset=5)\n    assert d[i, j].offset == 5\n", "def test_indexed_base_init():\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase('A')\n    assert A[i, j] == Indexed('A', i, j)\n    assert A[i, j, k] == Indexed('A', i, j, k)\n\n    A = IndexedBase('A', shape=(i, j, k))\n    assert A[i, j, k].shape == Tuple(i, j, k)\n\n    A = IndexedBase('A', real=True)\n    assert A[i, j].is_real\n    assert A[i, j].is_complex\n    assert not A[i, j].is_imaginary\n\n    A = IndexedBase('A', integer=True)\n    assert A[i, j].is_integer\n\n    A = IndexedBase('A', positive=True)\n    assert A[i, j].is_positive\n\n    A = IndexedBase('A', negative=True)\n    assert A[i, j].is_negative\n\n    A = IndexedBase('A', nonzero=True)\n    assert A[i, j].is_nonzero\n\n    A = IndexedBase('A', commutative=False)\n    assert not A[i, j].is_commutative\n"], "sample_1073": ["def test_sqrtdenest_issue_12420():\n    I = S.ImaginaryUnit\n    e = sqrt(2 + I) + sqrt(3 + 2*I) - sqrt(5 + 3*I)\n    assert sqrtdenest(e) == I\n", "def test_sqrtdenest_add_sub():\n    assert sqrtdenest(sqrt(9) - sqrt(4)) == 1\n    assert sqrtdenest(sqrt(9) + sqrt(4)) == 2*sqrt(2) + 3\n", "def test_issue_15822():\n    z = sqrt(r2 + sqrt(r2 + sqrt(r2)))\n    assert sqrtdenest(z) == sqrt(r2 + sqrt(r2 + sqrt(r2)))\n", "def test_sqrtdenest5():\n    z = sqrt(1 + r7 - r2*sqrt(5 + r7) + 5)\n    assert sqrtdenest(z) == sqrt(2) + r3\n", "def test_sqrtdenest_additive_identity():\n    z = sqrt(10)\n    assert sqrtdenest(z + sqrt(0)) == z\n    assert sqrtdenest(sqrt(0) + z) == z\n"], "sample_1027": ["def test_issue_14364_lcm():\n    assert lcm(S(2)/3*sqrt(3), S(5)/6*sqrt(3)) == S(10)*sqrt(3)/3\n    assert lcm(3*sqrt(3), S(4)/sqrt(3)) == 12*sqrt(3)\n    assert lcm(S(5)*(1 + 2**(S(1)/3))/6, S(3)*(1 + 2**(S(1)/3))/8) == S(15)/2 * (1 + 2**(S(1)/3))\n", "def test_issue_14596():\n    # Test for issue #14596\n    f = Poly(x**2 - y**2, x, y)\n    g = Poly(x - y, x, y)\n    h = Poly(x + y, x, y)\n    assert f.div(g) == (h, 0)\n", "def test_issue_16270():\n    assert gcd(x**2 - 2*x*y + y**2, x**2 - y**2) == x - y\n    assert lcm(x**2 - 2*x*y + y**2, x**2 - y**2) == (x - y)*(x + y)**2\n", "def test_issue_15124():\n    # Test for correct handling of symbols with assumptions\n    a = Symbol('a', real=True)\n    b = Symbol('b', positive=True)\n    f = Poly(a*x**2 + b*x + a, domain='ZZ[a,b]')\n    assert f.all_coeffs() == [a, b, a]\n", "def test_issue_14364_square_free():\n    # Test square-free factorization for expressions involving square roots\n    assert sqf(S(6)*(1 + sqrt(3))/5) == S(3)/5 * (1 + sqrt(3))\n    assert sqf(S(5)*(1 + sqrt(5))/3) == (1 + sqrt(5))\n    assert sqf(S(4)*sqrt(2)/3) == S(2)*sqrt(2)/3\n    assert sqf(S(7)*(2 + sqrt(6))/4) == S(7)/4 * (2 + sqrt(6))\n"], "sample_394": ["def test_url_with_trailing_slash_if_not_authenticated(self):\n    url = reverse(\"admin:article_extra_json\") + \"/\"\n    response = self.client.get(url)\n    self.assertRedirects(response, \"%s?next=%s\" % (reverse(\"admin:login\"), url))\n", "def test_missing_slash_append_slash_true_single_model_no_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username=\"staff\",\n        password=\"secret\",\n        email=\"staff@example.com\",\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse(\"admin9:admin_views_actor_changelist\")\n    response = self.client.get(known_url[:-1])\n    self.assertRedirects(\n        response, known_url, status_code=301, target_status_code=403\n    )\n", "    def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(\n        self,", "    def test_unknown_url_redirects_login_if_not_authenticated_with_admin_url(self):\n        unknown_url = \"/admin/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertRedirects(\n            response, \"%s?next=%s\" % (reverse(\"admin:login\"), unknown_url)\n        )\n", "def test_get_absolute_url(self):\n    model_admin = ModelAdmin(Worker, customadmin.site)\n    worker = Worker.objects.create(work_at=self.r1, name=\"John\", surname=\"Doe\")\n    url = model_admin.get_view_on_site_url(worker)\n    expected_url = reverse('worker_detail', kwargs={'name': worker.name, 'surname': worker.surname})\n    self.assertEqual(url, expected_url)\n"], "sample_84": ["    def test_parsing_invalid_date(self):\n        self.assertIsNone(parse_http_date_safe('Invalid Date'))\n", "    def test_limited_parse_qsl(self):\n        result = limited_parse_qsl('a=1&b=2&c=3', fields_limit=2)\n        self.assertEqual(result, [('a', '1'), ('b', '2')])\n", "    def test_parsing_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date format')\n", "    def test_urlencode_with_multivalue_dict(self):\n        mvd = MultiValueDict({'a': [1, 2, 3], 'b': [4, 5, 6]})\n        result = urlencode(mvd, doseq=True)\n        expected_results = ['a=1&a=2&a=3&b=4&b=5&b=6', 'a=1&a=2&b=4&a=3&b=5&b=6', 'a=1&b=4&a=2&b=5&a=3&b=6',\n                            'a=1&b=4&b=5&a=2&b=6&a=3', 'a=1&b=4&b=5&b=6&a=2&a=3', 'b=4&a=1&a=2&a=3&b=5&b=6']\n        self.assertIn(result, expected_results)\n", "    def test(self):\n        tests = (\n            ('http://example.com', 'http://example.com'),\n            ('/example.com', '/example.com'),\n            ('example.com', 'example.com'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n"], "sample_192": ["def test_formset_default_prefix(self):\n    \"\"\"The default prefix for a formset is 'form'.\"\"\"\n    formset = formsets.BaseFormSet(form=FavoriteDrinkForm)\n    self.assertEqual(formset.prefix, 'form')\n    self.assertEqual(formset.management_form.prefix, 'form')\n\n    formset = formsets.BaseFormSet(form=FavoriteDrinkForm, prefix='drinks')\n    self.assertEqual(formset.prefix, 'drinks')\n    self.assertEqual(formset.management_form.prefix, 'drinks')\n", "def test_formset_with_custom_min_num(self):\n    \"\"\"FormSet with custom min_num argument.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=0, min_num=2)\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(len(formset.forms), 2)\n    self.assertEqual(formset.forms[0].empty_permitted, False)\n    self.assertEqual(formset.forms[1].empty_permitted, False)\n", "def test_min_num_forms_with_empty_forms(self):\n    \"\"\"\n    Minimum number of forms is enforced even if some of them are empty.\n    \"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-0-choice': '',\n        'choices-0-votes': '',\n        'choices-1-choice': '',\n        'choices-1-votes': '',\n    }\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit 2 or more forms.'])\n", "def test_empty_formset_with_ordering(self):\n    \"\"\"\n    An empty formset with can_order=True should not have ordering fields on\n    the empty form.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, extra=0)\n    formset = ChoiceFormSet()\n    self.assertHTMLEqual(\n        formset.empty_form.as_ul(),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"form-__prefix__-choice\"></li>\n        <li>Votes: <input type=\"number\" name=\"form-__prefix__-votes\"></li>\"\"\"\n    )\n", "def test_formset_with_default_min_num(self):\n    \"\"\"\n    The default minimum number of forms in a formset is 0.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.min_num, 0)\n    self.assertEqual(len(formset.forms), 1)\n\n    data = {\n        'choices-TOTAL_FORMS': '0',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '1000',\n    }\n    formset = ChoiceFormSet(data, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.total_error_count(), 0)\n"], "sample_643": ["def test_template_option_with_symbol(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = '{{ \"Symbol\": \"{symbol}\" }}'\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\n        \"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4)\n    )\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1] == '{ \"Symbol\": \"missing-module-docstring\" }'\n    assert out_lines[2] == '{ \"Symbol\": \"line-too-long\" }'\n", "def test_colorize_ansi_with_message_style(linter):\n    msg = \"Test message\"\n    msg_style = MessageStyle(\"red\", (\"bold\", \"italic\"))\n    result = colorize_ansi(msg, msg_style)\n    expected = \"\\033[1;3;31mTest message\\033[0m\"\n    assert result == expected\n", "def test_colorize_ansi_deprecation(recwarn: WarningsRecorder) -> None:\n    \"\"\"Test the deprecation warning for colorize_ansi.\"\"\"\n    from pylint.reporters.text import colorize_ansi, MessageStyle\n\n    # Test with deprecated parameters\n    colorize_ansi(\"Deprecated\", \"red\", \"bold\")\n    warning = recwarn.pop()\n    assert \"In pylint 3.0, the colorize_ansi function of Text reporters will only accept a MessageStyle parameter\" in str(warning)\n\n    # Test with MessageStyle parameter\n    colorize_ansi(\"Non-deprecated\", MessageStyle(\"green\", (\"bold\",)))\n    assert not recwarn.list\n", "def test_colorize_ansi_deprecation(recwarn: WarningsRecorder) -> None:\n    \"\"\"Test the deprecation warning for colorize_ansi function.\"\"\"\n    from pylint.reporters.text import colorize_ansi, MessageStyle\n\n    # Using color and style parameters, which are deprecated\n    colorize_ansi(\"Test message\", color=\"red\", style=\"bold\")\n    warning = recwarn.pop()\n    assert \"In pylint 3.0, the colorize_ansi function of Text reporters will only accept a MessageStyle parameter\" in str(warning)\n\n    # Using MessageStyle parameter, which is the new way\n    colorize_ansi(\"Test message\", MessageStyle(\"green\", (\"bold\",)))\n    assert not recwarn.list, \"No deprecation warning should be raised when using MessageStyle parameter\"\n", "def test_colorized_text_reporter_default_color_mapping(linter):\n    output = StringIO()\n    linter.reporter = ColorizedTextReporter(output)\n    linter.open()\n    linter.set_current_module(\"my_module\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"E0602\", line=2, args=(\"undefined_variable\",))\n    linter.add_message(\"W0621\", line=3, args=(\"redefined_variable\",))\n    linter.add_message(\"R0201\", line=4, args=(\"no_self_use\", \"method_name\"))\n    linter.add_message(\"I0011\", line=5, args=(\"info_message\",))\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert \"************* Module my_module\" in out_lines[1]\n    assert \"\\033[32mLine too long (1/2)\\033[0m\" in out_lines[2]\n    assert \"\\033[31m\\033[1mUndefined variable 'undefined_variable'\\033[0m\" in out_lines[3]\n    assert \"\\033[35mName 'redefined_variable' defined from line 3\\033[0m\" in out_lines[4]\n    assert \"\\033[35m\\033[3m\\033[5mMethod 'method_name' is not method of an instance (no-method-argument)\\033[0m\" in out_lines[5]\n    assert \"\\033[33m\\033[7mInfo message\\033[0m\" in out_lines[6]\n"], "sample_1040": ["def test_print_function():\n    f = Function('f')\n    assert mpp.doprint(f(x)) == '<mrow><mi>f</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(f(x)) == '<apply><f/><ci>x</ci></apply>'\n", "def test_presentation_mathml_power_of_sum():\n    mml = mpp._print((x + 1)**2)\n    assert mml.nodeName == 'msup'\n    assert mml.childNodes[0].nodeName == 'mrow'\n    assert mml.childNodes[0].childNodes[0].nodeName == 'mo'\n    assert mml.childNodes[0].childNodes[0].childNodes[0].nodeValue == '('\n    assert mml.childNodes[0].childNodes[1].childNodes[0].nodeValue == 'x'\n    assert mml.childNodes[0].childNodes[2].nodeName == 'mo'\n    assert mml.childNodes[0].childNodes[2].childNodes[0].nodeValue == '+'\n    assert mml.childNodes[0].childNodes[3].childNodes[0].nodeValue == '1'\n    assert mml.childNodes[0].childNodes[4].nodeName == 'mo'\n    assert mml.childNodes[0].childNodes[4].childNodes[0].nodeValue == ')'\n    assert mml.childNodes[1].childNodes[0].nodeValue == '2'\n", "def test_presentation_mathml_matrix_symbol():\n    A = MatrixSymbol('A', 1, 2)\n    mml = mpp._print(A)\n    assert mml.nodeName == 'mi'\n    assert mml.childNodes[0].nodeValue == 'A'\n\n    mml = mpp._print(A, mat_symbol_style=\"bold\")\n    assert mml.nodeName == 'mi'\n    assert mml.childNodes[0].nodeValue == 'A'\n    assert mml.getAttribute('mathvariant') == 'bold'\n\n    mml = mp._print(A, mat_symbol_style=\"bold\")\n    assert mml.nodeName == 'ci'\n    assert mml.childNodes[0].nodeValue == 'A'\n    # No effect in content printer, so no change in attributes\n", "def test_presentation_mathml_power_with_negative_exponent():\n    mml = mpp._print(x**-1)\n    assert mml.nodeName == 'mrow'\n    nodes = mml.childNodes\n    assert nodes[0].nodeName == 'mfenced'\n    assert nodes[0].childNodes[0].childNodes[0].nodeValue == 'x'\n    assert nodes[1].nodeName == 'msup'\n    assert nodes[1].childNodes[0].childNodes[0].nodeValue == '-'\n    assert nodes[1].childNodes[1].childNodes[0].nodeValue == '1'\n", "def test_presentation_mathml_pow():\n    mml = mpp._print(x**(1/2))\n    assert mml.nodeName == 'msqrt'\n    assert mml.childNodes[0].childNodes[0].nodeValue == 'x'\n\n    mml = mpp._print(x**(-1))\n    assert mml.nodeName == 'msup'\n    assert mml.childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mml.childNodes[1].childNodes[0].nodeValue == '-1'\n\n    mml = mpp._print(x**(-1/2))\n    assert mml.nodeName == 'mrow'\n    assert mml.childNodes[0].nodeName == 'mo'\n    assert mml.childNodes[0].childNodes[0].nodeValue == '&InvisibleTimes;'\n    assert mml.childNodes[1].nodeName == 'msqrt'\n    assert mml.childNodes[1].childNodes[0].childNodes[0].nodeValue == 'x'\n    assert mml.childNodes[2].nodeName == 'mrow'\n    assert mml.childNodes[2].childNodes[0].nodeName == 'mo'\n    assert mml.childNodes[2].childNodes[0].childNodes[0].nodeValue == '\u207b'\n    assert mml.childNodes[2].childNodes[1].nodeName == 'mn'\n    assert mml.childNodes[2].childNodes[1].childNodes[0].nodeValue == '\u00bd'\n"], "sample_581": ["def test_blueprint_register_blueprint_options(app, client):\n    parent = flask.Blueprint(\"parent\", __name__)\n    child = flask.Blueprint(\"child\", __name__)\n\n    @child.route(\"/\")\n        return \"child\"\n\n    parent.register_blueprint(child, url_prefix=\"/child\", subdomain=\"api\")\n    app.register_blueprint(parent, subdomain=\"parent\")\n\n    client.allow_subdomain_redirects = True\n\n    domain_name = \"domain.tld\"\n    app.config[\"SERVER_NAME\"] = domain_name\n    response = client.get(\"/child/\", base_url=\"http://api.parent.\" + domain_name)\n\n    assert response.status_code == 200\n", "def test_blueprint_app_error_handling_with_custom_code(app, client):\n    errors = flask.Blueprint(\"errors\", __name__)\n\n    @errors.app_errorhandler(500)\n        return \"custom internal server error\", 500\n\n    @app.route(\"/error\")\n        flask.abort(500)\n\n    app.register_blueprint(errors)\n\n    assert client.get(\"/error\").data == b\"custom internal server error\"\n", "def test_blueprint_name_prefix(app, client):\n    parent = flask.Blueprint(\"parent\", __name__)\n    child = flask.Blueprint(\"child\", __name__)\n\n    @child.route(\"/\")\n        return \"child\"\n\n    parent.register_blueprint(child)\n    app.register_blueprint(parent, url_prefix=\"/parent\")\n\n    response = client.get(\"/parent/child/\")\n    assert response.status_code == 200\n\n    response = client.get(\"/parent/\")\n    assert response.status_code == 404\n", "def test_blueprint_register_blueprint_options(app, client):\n    parent = flask.Blueprint(\"parent\", __name__)\n    child = flask.Blueprint(\"child\", __name__)\n\n    @child.route(\"/\")\n        return \"child\"\n\n    parent.register_blueprint(child, url_prefix=\"/custom\", name=\"custom_child\")\n    app.register_blueprint(parent, url_prefix=\"/parent\")\n\n    response = client.get(\"/parent/custom/\")\n    assert response.status_code == 200\n    assert response.data == b\"child\"\n\n    # Test if the custom name works for url_for\n    with app.test_request_context():\n        assert flask.url_for(\"custom_child.index\") == \"/parent/custom/\"\n", "def test_blueprint_multiple_registration(app, client):\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/\")\n        return \"bp index\"\n\n    app.register_blueprint(bp, url_prefix=\"/a\")\n    app.register_blueprint(bp, url_prefix=\"/b\")\n\n    assert client.get(\"/a/\").data == b\"bp index\"\n    assert client.get(\"/b/\").data == b\"bp index\"\n"], "sample_993": ["def test_FreeGroupElm_cyclic_subword():\n    w = x**2*y**3*x**-1*y\n    assert w.cyclic_subword(1, 6) == y**3*x**-1*y*x**2\n    assert w.cyclic_subword(2, 7) == x**-1*y*x**2*y**3\n    assert w.cyclic_subword(3, 3) == F.identity\n", "def test_FreeGroupElm_cyclic_conjugates():\n    w = x*y*x*y*x\n    assert w.cyclic_conjugates() == {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x, x*y*x*y*x}\n\n    s = x*y*x**2*y*x\n    assert s.cyclic_conjugates() == {x**2*y*x**2*y, y*x**2*y*x**2, x*y*x**2*y*x}\n\n    assert w.is_cyclic_conjugate(x**2*y*x*y)\n    assert not w.is_cyclic_conjugate(x*y**2*x)\n", "def test_FreeGroupElm_cyclic_reduction():\n    w = x**2 * y**-1 * x**-1\n    assert not w.is_cyclically_reduced()\n    assert w.identity_cyclic_reduction() == y**-1\n    assert w.cyclic_reduction() == y**-1 * x**2\n\n    w2 = y * x**2 * y**2\n    assert w2.is_cyclically_reduced()\n    assert w2.identity_cyclic_reduction() == w2\n    assert w2.cyclic_reduction() == w2\n\n    w3 = x**-3 * y**-1 * x**5\n    assert not w3.is_cyclically_reduced()\n    assert w3.identity_cyclic_reduction() == x**2 * y**-1\n    assert w3.cyclic_reduction() == y**-1 * x**2\n", "def test_FreeGroupElm_is_dependent_and_is_independent():\n    w = x**4 * y**-3\n    assert (x**4 * y**-2).is_dependent(w)\n    assert not (x * y).is_dependent(w)\n    assert (x * y**2 * x * y**2).is_dependent(x * y**2)\n    assert (x**12).is_dependent(x**-4)\n\n    assert (x**2 * y**-1).is_independent(x * y)\n    assert not (x**4 * y**-3).is_independent(x * y)\n    assert not (x**12).is_independent(x**-4)\n", "def test_FreeGroup_cyclic_conjugates():\n    w1 = x*y*x*y*x\n    assert w1.cyclic_conjugates() == {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x, x*y*x*y*x}\n\n    w2 = x*y*x**2*y*x\n    assert w2.cyclic_conjugates() == {x**2*y*x**2*y, y*x**2*y*x**2, x*y*x**2*y*x}\n\n    w3 = x**2*y**5\n    assert w3.is_cyclic_conjugate(x*y**5*x)\n    assert not w3.is_cyclic_conjugate(x**-1*y**5*x**-1)\n"], "sample_187": ["def test_capfirst(self):\n    self.assertEqual(text.capfirst('hello world'), 'Hello world')\n    self.assertEqual(text.capfirst('Hello world'), 'Hello world')\n    self.assertEqual(text.capfirst('123 world'), '123 world')\n    self.assertEqual(text.capfirst(''), '')\n    self.assertEqual(text.capfirst(lazystr('hello world')), 'Hello world')\n", "def test_capfirst(self):\n    self.assertEqual(capfirst('hello'), 'Hello')\n    self.assertEqual(capfirst(None), '')\n    self.assertEqual(capfirst(''), '')\n    self.assertEqual(capfirst('123'), '123')\n    self.assertEqual(capfirst(lazystr('hello')), 'Hello')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseExample'), 'camel case example')\n    self.assertEqual(text.camel_case_to_spaces('AnotherExample'), 'another example')\n    self.assertEqual(text.camel_case_to_spaces('lazystrExample'), 'lazystr example')\n    self.assertEqual(text.camel_case_to_spaces('lazySTRExample'), 'lazy s t r example')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseString'), 'camel case string')\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseStringWithNoSpace'), 'camel case string with no space')\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseString WithSpace'), 'camel case string with space')\n    self.assertEqual(text.camel_case_to_spaces('lowercase'), 'lowercase')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces('A'), 'a')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('CamelCaseString')), 'camel case string')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseTest'), 'camel case test')\n    self.assertEqual(text.camel_case_to_spaces('CamelCase'), 'camel case')\n    self.assertEqual(text.camel_case_to_spaces('Camel'), 'camel')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces('camelCase'), 'camel case')\n    self.assertEqual(text.camel_case_to_spaces('camelCaseTest'), 'camel case test')\n    self.assertEqual(text.camel_case_to_spaces('Camel_Case_Test'), 'camel case test')\n    self.assertEqual(text.camel_case_to_spaces('CAMELCASETEST'), 'camel case test')\n"], "sample_103": ["def test_annotate_with_filter(self):\n    books = Book.objects.annotate(\n        num_expensive_authors=Count('authors__id', filter=Q(authors__book__price__gt=Decimal(\"50.0\")))\n    ).order_by('pk')\n    self.assertQuerysetEqual(\n        books, [\n            (\"The Definitive Guide to Django: Web Development Done Right\", 0),\n            (\"Sams Teach Yourself Django in 24 Hours\", 0),\n            (\"Practical Django Projects\", 0),\n            (\"Python Web Development with Django\", 1),\n            (\"Artificial Intelligence: A Modern Approach\", 0),\n            (\"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\", 0)\n        ],\n        lambda b: (b.name, b.num_expensive_authors)\n    )\n", "def test_aggregate_annotation_with_filter(self):\n    books = Book.objects.annotate(num_authors=Count(\"authors__id\")).filter(num_authors__gt=1).order_by(\"pk\")\n    self.assertQuerysetEqual(\n        books,\n        ['Artificial Intelligence: A Modern Approach', 'Python Web Development with Django'],\n        lambda b: b.name\n    )\n\n    authors = Author.objects.annotate(num_books=Count(\"book__id\", filter=Q(book__rating__gt=4.0))).filter(num_books__gt=0).order_by(\"pk\")\n    self.assertQuerysetEqual(\n        authors,\n        ['Jacob Kaplan-Moss', 'Jeffrey Forcier', 'James Bennett', 'Paul Bissex', 'Wesley J. Chun', 'Peter Norvig', 'Stuart Russell'],\n        lambda a: a.name\n    )\n", "def test_stddev_aggregate(self):\n    stddev = Book.objects.aggregate(stddev=StdDev('rating'))['stddev']\n    self.assertIsInstance(stddev, Decimal)\n    self.assertEqual(stddev, Approximate(Decimal('0.5'), places=1))\n\n    stddev_sample = Book.objects.aggregate(stddev_sample=StdDev('rating', sample=True))['stddev_sample']\n    self.assertIsInstance(stddev_sample, Decimal)\n    self.assertEqual(stddev_sample, Approximate(Decimal('0.6'), places=1))\n", "def test_aggregate_with_filter(self):\n    books = Book.objects.aggregate(\n        expensive_books=Count('id', filter=Q(price__gt=Decimal('50'))),\n        cheap_books=Count('id', filter=Q(price__lt=Decimal('50')))\n    )\n    self.assertEqual(books['expensive_books'], 2)\n    self.assertEqual(books['cheap_books'], 4)\n\n    authors = Author.objects.aggregate(\n        young_authors=Count('id', filter=Q(age__lt=40)),\n        old_authors=Count('id', filter=Q(age__gte=40))\n    )\n    self.assertEqual(authors['young_authors'], 5)\n    self.assertEqual(authors['old_authors'], 4)\n", "def test_aggregate_filter_clause(self):\n    books = Book.objects.filter(rating__gt=4.0).aggregate(average_price=Avg('price', filter=F('price') > 40))\n    self.assertEqual(books['average_price'], Decimal('75.00'))\n"], "sample_983": ["def test_sparse_scalar_multiply():\n    A = SparseMatrix([[1, 2], [3, 4]])\n    B = A.scalar_multiply(2)\n    assert B == SparseMatrix([[2, 4], [6, 8]])\n    C = A.scalar_multiply(0)\n    assert C == SparseMatrix([[0, 0], [0, 0]])\n", "def test_sparse_determinant():\n    A = SparseMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert A.det() == 0\n\n    A = SparseMatrix([[1, 2], [3, 4]])\n    assert A.det() == -2\n\n    A = SparseMatrix([[1, 0], [0, 1]])\n    assert A.det() == 1\n", "def test_sparse_multiply():\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = SparseMatrix(((5, 6), (7, 8)))\n    assert A*B == SparseMatrix(((19, 22), (43, 50)))\n    A = SparseMatrix(((1, 0), (0, 1)))\n    assert A.scalar_multiply(3) == SparseMatrix(((3, 0), (0, 3)))\n", "def test_sparse_solve_overdetermined():\n    A = SparseMatrix([[1, 2], [3, 4], [5, 6]])\n    b = Matrix([7, 8, 9])\n    raises(ValueError, lambda: A.solve(b))\n", "def test_sparse_matrix_element_assignment():\n    a = SparseMatrix(2, 2, {})\n\n    # test integer assignment\n    a[0, 1] = 3\n    assert a[0, 1] == 3\n    assert a._smat == {(0, 1): 3}\n\n    # test symbol assignment\n    x = Symbol('x')\n    a[1, 0] = x\n    assert a[1, 0] == x\n    assert a._smat == {(0, 1): 3, (1, 0): x}\n\n    # test zero assignment\n    a[1, 0] = 0\n    assert a[1, 0] == 0\n    assert a._smat == {(0, 1): 3}\n"], "sample_60": ["def test_get_form_with_exclude(self):\n    class MediaInline(GenericTabularInline):\n        model = Media\n        exclude = ['url']\n\n    ma = MediaInline(Media, self.site)\n    form = ma.get_formset(None).form\n    self.assertNotIn('url', form.base_fields)\n", "    def test_custom_model_form(self):\n        class CustomMediaForm(ModelForm):\n            class Meta:\n                model = Media\n                fields = '__all__'\n\n        class CustomMediaInline(InlineModelAdmin):\n            model = Media\n            form = CustomMediaForm\n            fk_name = 'episode'\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [\n                CustomMediaInline\n            ]\n\n        ma = EpisodeAdmin(Episode, admin_site)\n        inline = ma.get_inline_instances(request)[0]\n        self.assertIsInstance(inline.get_formset(request).form(), CustomMediaForm)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.episode = Episode.objects.create(name='This Week in Django')\n        self.modeladmin = admin.ModelAdmin(Episode, admin_site)\n        self.modeladmin.inlines = [MediaInline]\n        self.request = self.factory.get(reverse('admin:generic_inline_admin_episode_change', args=(self.episode.pk,)))\n        self.request.user = self.superuser\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "def test_has_view_or_change_permission(self):\n    \"\"\"\n    Test the has_view_or_change_permission method.\n    \"\"\"\n    class MediaInline(GenericTabularInline):\n        model = Media\n\n    class EpisodeAdmin(admin.ModelAdmin):\n        inlines = [\n            MediaInline\n        ]\n\n    ma = EpisodeAdmin(Episode, self.site)\n    inline = ma.get_inline_instances(request)[0]\n    # Test with view permission\n    with self.settings(AUTH_USER_MODEL='auth.User'):\n        request.user = User.objects.create_user(username='user', password='password')\n        self.assertFalse(inline.has_view_or_change_permission(request))\n        request.user.user_permissions.add(Permission.objects.get(codename='view_media'))\n        self.assertTrue(inline.has_view_or_change_permission(request))\n    # Test with change permission\n    with self.settings(AUTH_USER_MODEL='auth.User'):\n        request.user = User.objects.create_user(username='user2', password='password')\n        self.assertFalse(inline.has_view_or_change_permission(request))\n        request.user.user_permissions.add(Permission.objects.get(codename='change_media'))\n        self.assertTrue(inline.has_view_or_change_permission(request))\n"], "sample_1204": ["def test_symmetricpermutationgroup_contains():\n    G = SymmetricPermutationGroup(4)\n    assert Permutation(1, 2, 3) in G\n    assert Permutation(1, 2, 3, 4, 5) not in G\n    assert 1 not in G\n", "def test_contains():\n    a = Permutation([1, 2, 0, 3, 4, 5])\n    b = Permutation([2, 1, 3, 4, 5, 0])\n    g = PermutationGroup([a, b])\n    assert Permutation([0, 1, 2, 3, 4, 5]) in g\n    assert Permutation([5, 0, 1, 2, 3, 4]) in g\n    assert Permutation([1, 2, 3, 4, 5]) not in g\n    assert Permutation([0, 1, 2, 3, 4]) not in g\n    assert Permutation([0, 1, 2, 3, 4, 5, 6]) not in g\n", "def test_sylow_subgroup_alt_sym():\n    # Test sylow_subgroup for Alt(4) and Sym(4)\n    A = AlternatingGroup(4)\n    S = SymmetricGroup(4)\n    S_sylow = S.sylow_subgroup(2)\n    A_sylow = A.sylow_subgroup(2)\n    assert S_sylow.order() == 8\n    assert A_sylow.order() == 4\n    assert len(S_sylow.generators) == 2\n    assert len(A_sylow.generators) == 2\n\n    # Test sylow_subgroup for Alt(5)\n    A = AlternatingGroup(5)\n    A_sylow = A.sylow_subgroup(3)\n    assert A_sylow.order() == 9\n    assert len(A_sylow.generators) == 2\n", "def test_schreier_sims_incremental_with_initial_data():\n    S = SymmetricGroup(5)\n    base = [0, 1, 2]\n    strong_gens = [Permutation([2, 3, 4, 0, 1]), Permutation([1, 2, 3, 4, 0])]\n    new_base, new_strong_gens = S.schreier_sims_incremental(base=base, strong_gens=strong_gens)\n    assert _verify_bsgs(S, new_base, new_strong_gens) is True\n", "def test_coset_enumeration():\n    G = PermutationGroup(Permutation(1, 2, 3, 4), Permutation(2, 3, 4))\n    H = PermutationGroup(Permutation(1, 2, 3, 4))\n    table = G.coset_table(H)\n    assert table == [[0, 1, 2, 3], [3, 0, 1, 2], [2, 3, 0, 1], [1, 2, 3, 0]]\n    transversal = G.coset_transversal(H)\n    assert transversal == [Permutation(4), Permutation(1, 2, 3, 4), Permutation(2, 3, 4), Permutation(1, 4, 3, 2)]\n    coset_enum = G.coset_enumeration(H)\n    assert coset_enum == (table, transversal)\n"], "sample_432": ["def test_show_all_with_large_queryset(self):\n    parent = Parent.objects.create(name=\"anything\")\n    for i in range(1, 301):\n        Child.objects.create(name=\"name %s\" % i, parent=parent)\n\n    # Add \"show all\" parameter to request\n    request = self.factory.get(\"/child/\", data={ALL_VAR: \"\"})\n    request.user = self.superuser\n\n    # Test \"show all\" request with large queryset\n    m = ChildAdmin(Child, custom_site)\n    m.list_max_show_all = 200\n    # 200 is the max we'll pass to ChangeList\n    cl = m.get_changelist_instance(request)\n    cl.get_results(request)\n    self.assertGreater(len(cl.result_list), m.list_max_show_all)\n", "    def test_get_results_with_invalid_ordering(self):\n        m = BandAdmin(Band, custom_site)\n        request = self.factory.get(\"/band/\", data={\"o\": \"invalid_field\"})\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        cl.get_results(request)\n        # Queryset must be deletable.\n        self.assertIs(cl.queryset.query.distinct, False)\n        # No errors should be raised, and the queryset should not be empty.\n        self.assertTrue(cl.queryset.exists())\n", "def test_no_duplicates_for_fk_in_search_fields(self):\n    \"\"\"\n    When using a ForeignKey in search_fields, Exists() must be applied and\n    results shouldn't appear more than once.\n    \"\"\"\n    parent = Parent.objects.create(name=\"Mary\")\n    Child.objects.create(parent=parent, name=\"Danielle\")\n    Child.objects.create(parent=parent, name=\"Daniel\")\n\n    m = ChildAdmin(Child, custom_site)\n    m.search_fields = [\"parent__name\"]\n\n    request = self.factory.get(\"/child/\", data={SEARCH_VAR: \"Mary\"})\n    request.user = self.superuser\n\n    cl = m.get_changelist_instance(request)\n    # Exists() is applied.\n    self.assertEqual(cl.queryset.count(), 2)\n    # Queryset must be deletable.\n    self.assertIs(cl.queryset.query.distinct, False)\n    cl.queryset.delete()\n    self.assertEqual(cl.queryset.count(), 0)\n", "def test_pagination_overflow(self):\n    \"\"\"\n    Regression tests for ticket #18641: ensure the page number is not out\n    of range when using pagination.\n    \"\"\"\n    parent = Parent.objects.create(name=\"parent\")\n    for i in range(10):\n        Child.objects.create(name=\"child %s\" % i, parent=parent)\n\n    superuser = self._create_superuser(\"superuser\")\n    m = ChildAdmin(Child, custom_site)\n    m.list_per_page = 10\n\n    # Test with a page number greater than the total number of pages\n    request = self._mocked_authenticated_request(\"/child/?p=2\", superuser)\n    response = m.changelist_view(request)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, \"/child/\")\n\n    # Test with a page number less than 1\n    request = self._mocked_authenticated_request(\"/child/?p=0\", superuser)\n    response = m.changelist_view(request)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, \"/child/\")\n", "def test_changelist_with_unicode_characters(self):\n    parent = Parent.objects.create(name=\"parent with Unicode: \u03b1\u03b2\u03b3\u03b4\")\n    child = Child.objects.create(name=\"child with Unicode: \u03b1\u03b2\u03b3\u03b4\", parent=parent)\n\n    m = ChildAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request(\"/child/\", self.superuser)\n    cl = m.get_changelist_instance(request)\n    cl.get_results(request)\n\n    self.assertEqual(cl.queryset.count(), 1)\n    self.assertEqual(cl.queryset.first(), child)\n"], "sample_762": ["def test_clone_nested_estimators():\n    # Test that clone correctly handles nested estimators\n    estimator = Pipeline([('svc1', SVC()), ('svc2', SVC())])\n    clone_estimator = clone(estimator)\n    assert estimator is not clone_estimator\n    assert estimator.named_steps['svc1'] is not clone_estimator.named_steps['svc1']\n    assert estimator.named_steps['svc2'] is not clone_estimator.named_steps['svc2']\n", "def test_clone_estimator_in_dict():\n    # Check that clone works correctly when an estimator is a dictionary value\n    clf = MyEstimator()\n    dict_with_estimator = {'clf': clf}\n    dict_with_estimator_copy = clone(dict_with_estimator)\n    assert dict_with_estimator is not dict_with_estimator_copy\n    assert dict_with_estimator['clf'] is not dict_with_estimator_copy['clf']\n", "def test_get_params_deep_false():\n    test = T(K(), K())\n    params = test.get_params(deep=False)\n    assert_equal(params, {'a': test.a, 'b': test.b})\n    assert_true(isinstance(params['a'], K))\n    assert_true(isinstance(params['b'], K))\n", "def test_clone_classifier_pipeline():\n    # Test cloning a pipeline with a classifier\n    clf = SVC(kernel='linear')\n    pipe = Pipeline([('clf', clf)])\n    pipe_cloned = clone(pipe)\n    assert_equal(type(pipe.named_steps['clf']), type(pipe_cloned.named_steps['clf']))\n    assert_dict_equal(pipe.named_steps['clf'].get_params(), pipe_cloned.named_steps['clf'].get_params())\n", "def test_clone_with_nested_objects():\n    # Test that clone works when the object contains nested objects\n    nested_obj = K(c=3, d=4)\n    estimator = T(a=nested_obj, b=nested_obj)\n    cloned_estimator = clone(estimator)\n\n    assert estimator is not cloned_estimator\n    assert estimator.a is not cloned_estimator.a\n    assert estimator.b is not cloned_estimator.b\n    assert estimator.a.c == cloned_estimator.a.c\n    assert estimator.a.d == cloned_estimator.a.d\n    assert estimator.b.c == cloned_estimator.b.c\n    assert estimator.b.d == cloned_estimator.b.d\n"], "sample_536": ["def test_MultiCursor_clear():\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    # useblit=false to avoid having to draw the figure to cache the renderer\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=True, vertOn=True\n    )\n\n    # mock a motion_notify_event\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n    # force a draw + draw event to exercise clear\n    ax1.figure.canvas.draw()\n\n    # Clear the MultiCursor\n    multi.clear()\n\n    # After clearing, no lines should be visible\n    assert len([line for line in multi.vlines if line.get_visible()]) == 0\n    assert len([line for line in multi.hlines if line.get_visible()]) == 0\n", "def test_polygon_selector_box_props(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with custom box props\n    box_handle_props = {'color': 'r', 'markersize': 10}\n    box_props = {'edgecolor': 'r', 'linewidth': 2, 'fill': False}\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_handle_props=box_handle_props,\n                                   box_props=box_props)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the box props were set correctly\n    assert tool._box._handles_artists[0].get_color() == 'r'\n    assert tool._box._handles_artists[0].get_markersize() == 10\n    assert tool._box._selection_artist.get_edgecolor() == 'r'\n    assert tool._box._selection_artist.get_linewidth() == 2\n    assert tool._box._selection_artist.get_fill() is False\n", "def test_polygon_selector_box_handle_props(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector with custom handle props\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True,\n                                   box_handle_props={'color': 'r'})\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that handle props were set correctly\n    for artist in tool._box._corner_handles.artists:\n        assert artist.get_markeredgecolor() == 'r'\n        assert artist.get_markerfacecolor() == 'r'\n\n    # Check that setting handle props after creation works\n    tool.set_handle_props(color='b')\n    for artist in tool._box._corner_handles.artists:\n        assert artist.get_markeredgecolor() == 'b'\n        assert artist.get_markerfacecolor() == 'b'\n", "def test_polygon_selector_add_state():\n    tool = widgets.PolygonSelector(get_ax(), onselect=noop)\n    with pytest.raises(ValueError):\n        tool.add_state('unsupported_state')\n", "def test_polygon_selector_ignore_outside(ax, draw_bounding_box):\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    tool = widgets.PolygonSelector(ax, onselect, draw_bounding_box=draw_bounding_box)\n    # Create a polygon\n    click_and_drag(tool, start=(50, 50), end=(150, 50))\n    click_and_drag(tool, start=(150, 50), end=(150, 150))\n    click_and_drag(tool, start=(150, 150), end=(50, 150))\n    click_and_drag(tool, start=(50, 150), end=(50, 50))\n    onselect.assert_called_once()\n    assert tool.verts == [(50, 50), (150, 50), (150, 150), (50, 150)]\n\n    onselect.reset_mock()\n    # Trigger event outside of polygon\n    click_and_drag(tool, start=(200, 200), end=(250, 250))\n    onselect.assert_not_called()\n    assert tool.verts == [(50, 50), (150, 50), (150, 150), (50, 150)]\n"], "sample_619": ["def test_decode_cf_datetime_0d_array():\n    units = \"days since 2000-01-01\"\n    num_dates = np.array(0)\n    expected = np.array(np.datetime64(\"2000-01-01\", \"ns\"))\n    result = decode_cf_datetime(num_dates, units)\n    np.testing.assert_equal(result, expected)\n", "def test_decode_1size_datetime(use_cftime):\n    # Test for single-element array\n    if use_cftime and not has_cftime:\n        pytest.skip()\n\n    dtype = object if use_cftime else \"M8[ns]\"\n    num_dates = np.array([18262], dtype=np.int64)\n    units = \"days since 2000-01-01\"\n    calendar = \"proleptic_gregorian\"\n    expected = np.array([\"2016-01-01T00:00:00\"], dtype=dtype)\n    actual = decode_cf_datetime(num_dates, units, calendar, use_cftime)\n    np.testing.assert_equal(expected, actual)\n", "def test_decode_cf_datetime_with_numpy_dtype():\n    # Test case for decoding with numpy dtype\n    num_dates = np.array([0, 1, 2], dtype=np.int64)\n    units = \"days since 2000-01-01\"\n    calendar = \"standard\"\n    expected = np.array([np.datetime64(\"2000-01-01\"), np.datetime64(\"2000-01-02\"), np.datetime64(\"2000-01-03\")])\n    result = decode_cf_datetime(num_dates, units, calendar)\n    np.testing.assert_array_equal(result, expected)\n", "def test_decode_cf_datetime_uint64_with_cftime_and_large_value():\n    units = \"days since 1700-01-01\"\n    num_dates = np.uint64(182621)\n    result = decode_cf_datetime(num_dates, units, use_cftime=True)\n    expected = np.asarray(np.datetime64(\"2200-01-01\"), dtype=\"O\")\n    np.testing.assert_equal(result, expected)\n", "def test_decode_cf_datetime_large_numbers(num_dates, units, calendar, use_cftime, expected):\n    result = decode_cf_datetime(num_dates, units, calendar, use_cftime)\n    np.testing.assert_equal(result, np.asarray(expected))\n"], "sample_819": ["def test_none_estimators():\n    \"\"\"VotingClassifier should raise an error when all estimators are None\"\"\"\n    eclf = VotingClassifier(estimators=[('lr', None), ('rf', None)],\n                            voting='hard')\n    msg = 'All estimators are None. At least one is required!'\n    assert_raise_message(ValueError, msg, eclf.fit, X, y)\n", "def test_predict_when_all_estimators_are_none():\n    \"\"\"Check that an error is raised when all estimators are None.\"\"\"\n    eclf = VotingClassifier(estimators=[(None, None), (None, None)],\n                            voting='soft')\n    msg = 'All estimators are None. At least one is required!'\n    assert_raise_message(ValueError, msg, eclf.fit, X, y)\n", "def test_transform_hard_voting():\n    \"\"\"Check transform method for hard voting on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    expected_transform = np.array([[1, 1, 1], [1, 1, 1], [2, 2, 2], [2, 2, 2]])\n    assert_array_equal(eclf.transform(X), expected_transform)\n", "def test_empty_estimators():\n    \"\"\"Check that error is raised when estimators list is empty.\"\"\"\n    eclf = VotingClassifier(estimators=[], voting='hard')\n    msg = 'All estimators are None. At least one is required!'\n    assert_raise_message(ValueError, msg, eclf.fit, X, y)\n", "def test_sample_weight_shape():\n    \"\"\"Check that VotingClassifier raises ValueError if sample_weight shape is incorrect.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1)], voting='soft')\n    incorrect_sample_weight = np.ones((len(y) + 1,))\n    msg = \"sample_weight.shape == \\(151, \\) should be \\(150, \\)\"\n    assert_raise_message(ValueError, msg, eclf.fit, X, y, incorrect_sample_weight)\n"], "sample_446": ["def test_floatformat_force_grouping(self):\n    with translation.override(\"en\"):\n        output = self.engine.render_to_string(\n            \"floatformat_force_grouping\", {\"a\": 66666.666, \"b\": 10000}\n        )\n        self.assertEqual(output, \"66,666.67 10,000\")\n", "def test_floatformat03(self):\n    with translation.override(\"en\"):\n        output = self.engine.render_to_string(\n            \"floatformat03\", {\"a\": \"12345.6789\", \"b\": \"12345.6789\"}\n        )\n        self.assertEqual(output, \"12345.679 12,345.679\")\n", "    def test_invalid_input_for_arg(self):\n        self.assertEqual(floatformat(123.456, \"invalid\"), \"123.456\")\n        self.assertEqual(floatformat(123.456, None), \"123.456\")\n        self.assertEqual(floatformat(123.456, {}), \"123.456\")\n", "    def test_floatformat_with_commas(self):\n        with translation.override(\"en\"):\n            self.assertEqual(floatformat(10000, \"2,g\"), \"10,000.00\")\n            self.assertEqual(floatformat(66666.666, \"1,g\"), \"66,666.7\")\n            self.assertEqual(floatformat(10000, \"2,ug\"), \"10000.00\")\n            self.assertEqual(floatformat(66666.666, \"1,ug\"), \"66666.7\")\n        with translation.override(\"de\", deactivate=True):\n            self.assertEqual(floatformat(10000, \"2,g\"), \"10.000,00\")\n            self.assertEqual(floatformat(66666.666, \"1,g\"), \"66.666,7\")\n            self.assertEqual(floatformat(10000, \"2,ug\"), \"10000.00\")\n            self.assertEqual(floatformat(66666.666, \"1,ug\"), \"66666.7\")\n", "    def test_non_finite_values(self):\n        nan = float(\"nan\")\n        self.assertEqual(floatformat(nan), \"nan\")\n        self.assertEqual(floatformat(-nan), \"nan\")\n        self.assertEqual(floatformat(nan, 2), \"nan\")\n        self.assertEqual(floatformat(nan, -2), \"nan\")\n        self.assertEqual(floatformat(nan, \"2\"), \"nan\")\n        self.assertEqual(floatformat(nan, \"2g\"), \"nan\")\n        self.assertEqual(floatformat(nan, \"2u\"), \"nan\")\n"], "sample_350": ["def test_difference_with_subcompound_qs(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.intersection(Number.objects.filter(num__lt=5))\n    self.assertEqual(qs1.difference(qs2).count(), 5)\n", "def test_order_by_with_combined_qs(self):\n    qs1 = Number.objects.filter(num__lte=5).order_by('num')\n    qs2 = Number.objects.filter(num__gte=5).order_by('-num')\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 9, 8, 7, 6, 5])\n", "def test_union_with_distinct_on_fields(self):\n    Number.objects.create(num=1, other_num=1)\n    Number.objects.create(num=1, other_num=2)\n    qs1 = Number.objects.filter(num=1).values('num')\n    qs2 = Number.objects.filter(num=1).values('other_num')\n    self.assertEqual(list(qs1.union(qs2).distinct('num')), [{'num': 1}])\n", "def test_union_with_different_querysets(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=6)\n    qs3 = Number.objects.filter(num=5)\n    qs4 = Number.objects.filter(num=6)\n\n    # Union two querysets\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ordered=False)\n\n    # Union three querysets\n    self.assertNumbersEqual(qs1.union(qs3, qs4), [0, 1, 2, 3, 4, 5, 6], ordered=False)\n\n    # Union a queryset with itself\n    self.assertNumbersEqual(qs1.union(qs1), [0, 1, 2, 3, 4, 5], ordered=False)\n", "def test_combined_qs_with_raw_queryset(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.all()\n    raw_qs = Number.objects.raw('SELECT * FROM myapp_number WHERE num > 5')\n    msg = 'Calling QuerySet.%s() after %s() is not supported.'\n    combinators = ['union', 'difference', 'intersection']\n    for combinator in combinators:\n        with self.subTest(combinator=combinator):\n            with self.assertRaisesMessage(NotSupportedError, msg % ('raw', combinator)):\n                getattr(qs1, combinator)(qs2).raw('SELECT * FROM myapp_number')\n            with self.assertRaisesMessage(NotSupportedError, msg % ('raw', combinator)):\n                getattr(raw_qs, combinator)(qs2)\n            with self.assertRaisesMessage(NotSupportedError, msg % ('raw', combinator)):\n                getattr(qs1, combinator)(raw_qs)\n"], "sample_845": ["def test_vectorizer_pipeline_with_callable_analyzer(tmpdir):\n    # raw documents\n    data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS\n\n    # label junk food as -1, the others as +1\n    target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)\n\n    # split the dataset for model development and final evaluation\n    train_data, test_data, target_train, target_test = train_test_split(\n        data, target, test_size=.1, random_state=0)\n\n    f = tmpdir.join(\"file.txt\")\n    f.write(\"sample content\\n\")\n\n        return doc.split()\n\n    pipeline = Pipeline([('vect', TfidfVectorizer(analyzer=analyzer)),\n                         ('svc', LinearSVC())])\n\n    parameters = {\n        'vect__ngram_range': [(1, 1), (1, 2)],\n        'vect__norm': ('l1', 'l2'),\n        'svc__loss': ('hinge', 'squared_hinge'),\n    }\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)\n\n    # Check that the best model found by grid search is 100% correct on the\n    # held out evaluation set.\n    pred = grid_search.fit(train_data, target_train).predict(test_data)\n    assert_array_equal(pred, target_test)\n\n    # check if a custom exception from the analyzer is shown to the user\n        raise Exception(\"testing\")\n\n    pipeline = Pipeline([('vect', TfidfVectorizer(analyzer=bad_analyzer)),\n                         ('svc', LinearSVC())])\n\n    with pytest.raises(Exception, match=\"testing\"):\n        pipeline.fit", "def test_hashingvectorizer_with_custom_analyzer():\n        return doc.split()\n\n    data = [\"this is a test\", \"another test document\"]\n    vec = HashingVectorizer(analyzer=custom_analyzer)\n    X = vec.transform(data)\n    assert X.shape[1] == 2 ** 20\n", "def test_vectorizer_max_df_values(Estimator, max_df):\n    data = ['the quick brown fox jumps over the lazy dog',\n            'the brown dog is quick and smart',\n            'the lazy fox is slow and dumb']\n    vec = Estimator(max_df=max_df)\n    vec.fit_transform(data)\n    if isinstance(max_df, float):\n        assert len(vec.stop_words_) == 2 if max_df < 0.6 else 0\n    else:\n        assert len(vec.stop_words_) == 1 if max_df < 2 else 0\n", "def test_vectorizers_input_validation(Estimator, tmpdir):\n    # Test input validation for 'filename' and 'file'\n    data = ['this is text, not file or filename']\n    vectorizer = Estimator()\n\n    # 'filename' input validation\n    with pytest.raises(ValueError, match=\"Invalid input type. Expected a list of file paths, got <class 'str'>\"):\n        vectorizer.input = 'filename'\n        vectorizer.fit_transform(data)\n\n    # 'file' input validation\n    with pytest.raises(ValueError, match=\"Invalid input type. Expected a file-like object, got <class 'str'>\"):\n        vectorizer.input = 'file'\n        vectorizer.fit_transform(data)\n\n    # Test with valid 'filename' input\n    f = tmpdir.join(\"file.txt\")\n    f.write(\"sample content\\n\")\n    vectorizer.input = 'filename'\n    X = vectorizer.fit_transform([f])\n    assert X.shape == (1, len(vectorizer.vocabulary_))\n\n    # Test with valid 'file' input\n    with open(f, 'r') as file:\n        vectorizer.input = 'file'\n        X = vectorizer.fit_transform([file])\n        assert X.shape == (1, len(vectorizer.vocabulary_))\n", "def test_tfidf_transformer_pickle_compatibility():\n    \"\"\"Check that TfidfTransformer is pickle compatible with older versions.\"\"\"\n    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)\n    tfidf_transformer = TfidfTransformer()\n    tfidf_transformer.fit(X)\n\n    # Pickle the object with the current version\n    pickled_tfidf_transformer = pickle.dumps(tfidf_transformer)\n\n    # Unpickle the object with an older version of scikit-learn\n    # (This assumes that the pickle format is stable across versions.)\n    old_version_pickle = pickled_tfidf_transformer\n    unpickled_tfidf_transformer = pickle.loads(old_version_pickle)\n\n    # Check that the unpickled object works correctly\n    X_transformed = unpickled_tfidf_transformer.transform(X)\n    assert X_transformed.shape == X.shape\n"], "sample_484": ["def test_right_with_none_length(self):\n    Author.objects.create(name=\"Bob\")\n    authors = Author.objects.annotate(name_part=Right(\"name\", Length(\"alias\")))\n    self.assertQuerySetEqual(\n        authors.order_by(\"name\"),\n        [\n            \"\" if connection.features.interprets_empty_strings_as_nulls else None,\n            \"\",\n            \"\",\n        ],\n        lambda a: a.name_part,\n    )\n", "def test_expression_with_non_int(self):\n    with self.assertRaisesMessage(TypeError, \"arguments to Right() must be integers\"):\n        Author.objects.annotate(raises=Right(\"name\", \"invalid\"))\n", "    def test_substring_consistency(self):\n        authors = Author.objects.annotate(name_part=Right(\"name\", Length(\"name\") - 2))\n        for author in authors:\n            self.assertEqual(author.name_part, author.name[-2:])\n", "def test_null_expression(self):\n    Author.objects.create(name=None, alias=\"null\")\n    authors = Author.objects.annotate(\n        name_part=Right(Coalesce(\"name\", Value(\"\")), 5)\n    )\n    self.assertQuerySetEqual(\n        authors.order_by(\"alias\"),\n        [\"null\", \"\" if connection.features.interprets_empty_strings_as_nulls else None],\n        lambda a: a.name_part,\n    )\n", "def test_multiple_annotations(self):\n    authors = Author.objects.annotate(first_part=Right(\"name\", 5), last_part=Left(\"name\", 5))\n    self.assertQuerySetEqual(\n        authors.order_by(\"name\"), [\"Smith\", \"Rhond\"], lambda a: a.first_part\n    )\n    self.assertQuerySetEqual(\n        authors.order_by(\"name\"), [\"John \", \"Rhonda\"], lambda a: a.last_part\n    )\n"], "sample_81": ["    def test_resolver_cache_different_urlconf(self):\n        # resolver for different URLconfs are not the same cached object.\n        self.assertIsNot(get_resolver('urlpatterns.path_urls'), get_resolver('urlpatterns.path_dynamic_urls'))\n", "    def test_resolver_cache_different_urlconf(self):\n        # resolver for two different URLconf (passing different arguments) is not the same cached object.\n        self.assertIsNot(get_resolver('urlpatterns.path_urls'), get_resolver('urlpatterns.path_dynamic_urls'))\n", "def test_resolver_cache_different_urlconfs(self):\n    # resolver for different URLconf is not the same cached object\n    self.assertIsNot(get_resolver('urlpatterns.path_urls'), get_resolver('urlpatterns.path_dynamic_urls'))\n", "    def test_resolver_cache_different_urlconfs(self):\n        # resolver for different URLconf is not the same cached object.\n        self.assertIsNot(get_resolver('urlpatterns.path_urls'), get_resolver('urlpatterns.path_dynamic_urls'))\n", "    def test_resolver_cache_invalidation(self):\n        # resolver for a different URLconf invalidates the cache.\n        resolver1 = get_resolver('urlpatterns.path_urls')\n        resolver2 = get_resolver('urlpatterns.path_dynamic_urls')\n        self.assertIsNot(resolver1, resolver2)\n\n        # Modifying settings.ROOT_URLCONF invalidates the cache.\n        with self.settings(ROOT_URLCONF='urlpatterns.path_dynamic_urls'):\n            resolver3 = get_resolver()\n            self.assertIsNot(resolver1, resolver3)\n"], "sample_418": ["def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"dict\": {\"a\": 1, \"b\": 2}})\n    self.assertEqual(output, \"\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"var\": \"\"})\n    self.assertEqual(output, \"True\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {\"none_var\": None})\n    self.assertEqual(output, \"Length is 0\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {})\n    self.assertEqual(output, \"\")\n", "def test_length_is12(self):\n    output = self.engine.render_to_string(\"length_is12\", {})\n    self.assertEqual(output, \"True\")\n"], "sample_748": ["compilation error", "def test_grid_search_cv_splits_consistency():\n    # Check if a one time iterable is accepted as a cv parameter.\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=n_samples, random_state=0)\n\n    gs = GridSearchCV(LinearSVC(random_state=0),\n                      param_grid={'C': [0.1, 0.2, 0.3]},\n                      cv=OneTimeSplitter(n_splits=n_splits,\n                                         n_samples=n_samples))\n    gs.fit(X, y)\n\n    gs2 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits))\n    gs2.fit(X, y)\n\n    gs3 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits, shuffle=True,\n                                random_state=0).split(X, y))\n    gs3.fit(X, y)\n\n    gs4 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=KFold(n_splits=n_splits, shuffle=True,\n                                random_state=0))\n    gs4.fit(X, y)\n\n        for key in ('mean_fit_time', 'std_fit_time',\n                    'mean_score_time', 'std_score_time'):\n            cv_results.pop(key)\n        return cv_results\n\n   ", "compilation error", "def test_your_functionality():\n    # Arrange\n    # You might need to set up some initial data or mock objects here\n\n    # Act\n    # Call the function you want to test\n    result = your_function(arguments)\n\n    # Assert\n    # Make some assertions about the result\n    assert expected_result == result\n", "def test_grid_search_cv_splits_stratification():\n    # Check if a one time iterable is accepted as a cv parameter\n    n_samples = 100\n    n_splits = 5\n    X, y = make_classification(n_samples=n_samples, random_state=0)\n    y[n_samples // 2:] = 1\n\n    gs = GridSearchCV(LinearSVC(random_state=0),\n                      param_grid={'C': [0.1, 0.2, 0.3]},\n                      cv=StratifiedKFold(n_splits=n_splits,\n                                         shuffle=True))\n    gs.fit(X, y)\n\n    gs2 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=StratifiedKFold(n_splits=n_splits,\n                                          shuffle=True,\n                                          random_state=42))\n    gs2.fit(X, y)\n\n    # Give generator as a cv parameter\n    assert_true(isinstance(StratifiedKFold(n_splits=n_splits,\n                                           shuffle=True).split(X, y),\n                           GeneratorType))\n    gs3 = GridSearchCV(LinearSVC(random_state=0),\n                       param_grid={'C': [0.1, 0.2, 0.3]},\n                       cv=StratifiedKFold(n_splits=n_splits, shuffle=True,\n                                          random_state=0).split(X, y))\n    gs3.fit(X, y)\n\n    # Check if generators are supported as cv and\n    # that the splits are consistent\n    np.testing.assert_equal(_pop_time_keys(gs2.cv_results_),\n                            _pop_time_keys(gs3.cv_results_))\n"], "sample_753": ["def test_warm_start_converge_LR_CV():\n    # Test to see that the logistic regression CV converges on warm start,\n    # with multi_class='multinomial'. Non-regressive test for #10836\n\n    rng = np.random.RandomState(0)\n    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))\n    y = np.array([1] * 100 + [-1] * 100)\n    lr_no_ws = LogisticRegressionCV(multi_class='multinomial',\n                                    solver='sag', warm_start=False,\n                                    random_state=0)\n    lr_ws = LogisticRegressionCV(multi_class='multinomial',\n                                 solver='sag', warm_start=True,\n                                 random_state=0)\n\n    lr_no_ws_loss = log_loss(y, lr_no_ws.fit(X, y).predict_proba(X))\n    for i in range(5):\n        lr_ws.fit(X, y)\n    lr_ws_loss = log_loss(y, lr_ws.predict_proba(X))\n    assert_allclose(lr_no_ws_loss, lr_ws_loss, rtol=1e-5)\n", "def test_warm_start_converge_LRCV():\n    # Test to see that the logistic regression cross validation converges on warm start,\n    # with multi_class='multinomial'. Non-regressive test for #10836\n\n    rng = np.random.RandomState(0)\n    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))\n    y = np.array([1] * 100 + [-1] * 100)\n    lr_no_ws = LogisticRegressionCV(multi_class='multinomial',\n                                    solver='sag', warm_start=False,\n                                    random_state=0)\n    lr_ws = LogisticRegressionCV(multi_class='multinomial',\n                                 solver='sag', warm_start=True,\n                                 random_state=0)\n\n    lr_no_ws_loss = log_loss(y, lr_no_ws.fit(X, y).predict_proba(X))\n    for i in range(5):\n        lr_ws.fit(X, y)\n    lr_ws_loss = log_loss(y, lr_ws.predict_proba(X))\n    assert_allclose(lr_no_ws_loss, lr_ws_loss, rtol=1e-5)\n", "def test_multinomial_binary_with_class_weight():\n    # Test multinomial LR with class_weight for a binary problem.\n    X, y = make_classification(n_samples=100, random_state=0)\n    clf = LogisticRegression(multi_class='multinomial', solver='saga', class_weight={0: 0.5, 1: 0.5})\n    clf.fit(X, y)\n\n    assert_equal(clf.coef_.shape, (1, X.shape[1]))\n    assert_equal(clf.intercept_.shape, (1,))\n    assert_equal(sorted(np.unique(clf.predict(X))), [0, 1])\n", "def test_saga_elasticnet():\n    # Test SAGA solver with elasticnet penalty\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    lr_saga = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga', tol=1e-6, max_iter=1000, random_state=0)\n    lr_saga.fit(X, y)\n\n    # Check that coefficients are not all zero\n    assert_true(np.any(lr_saga.coef_ != 0))\n\n    # Check that L1 and L2 penalties are applied correctly\n    lr_l1 = LogisticRegression(penalty='l1', solver='saga', tol=1e-6, max_iter=1000, random_state=0)\n    lr_l2 = LogisticRegression(penalty='l2', solver='saga', tol=1e-6, max_iter=1000, random_state=0)\n    lr_l1.fit(X, y)\n    lr_l2.fit(X, y)\n    l1_norm = np.sum(np.abs(lr_l1.coef_))\n    l2_norm = np.sum(lr_l2.coef_**2)\n    elasticnet_norm = np.sum(np.abs(lr_saga.coef_)) + 0.5 * np.sum(lr_saga.coef_**2)\n    assert_greater(l1_norm, elasticnet_norm)\n    assert_greater(l2_norm, elasticnet_norm)\n", "def test_logreg_refit_false():\n    # Test that refit=False in LogisticRegressionCV averages C and coefs across folds\n\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    lr_cv = LogisticRegressionCV(Cs=[1.0, 2.0], refit=False, solver='liblinear')\n    lr_cv.fit(X, y)\n    avg_C = np.mean([np.mean(lr_cv.scores_[i]) for i in lr_cv.scores_])\n    avg_coef = np.mean([lr_cv.coefs_paths_[i][:, 0] for i in lr_cv.coefs_paths_], axis=0)\n    assert_array_almost_equal(lr_cv.C_[0], avg_C)\n    assert_array_almost_equal(lr_cv.coef_, avg_coef)\n"], "sample_1207": ["def test_issue_22822_local_dict_override():\n    data = {'x': 1}\n    assert parse_expr('x', data) == 1\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    f = parse_expr('lambda x: x**2', transformations=transformations)\n    assert f(x) == x**2\n    raises(TokenError, lambda: parse_expr('lambda: x**2', transformations=transformations))\n    raises(TokenError, lambda: parse_expr('lambda *x: x**2', transformations=transformations))\n    raises(TokenError, lambda: parse_expr('lambda x**: x**2', transformations=transformations))\n", "def test_lambda_notation():\n    x = Symbol('x')\n    f = parse_expr(\"lambda x: x**2\")\n    assert f(3) == 9\n    assert f(x) == x**2\n    raises(TokenError, lambda: parse_expr(\"lambda x: x**2 + *x\"))\n    raises(TokenError, lambda: parse_expr(\"lambda x y: x**2 + y**2\"))\n", "def test_parsing_starred_arguments():\n    try:\n        parse_expr('lambda *args: args')\n    except TokenError as e:\n        assert str(e) == \"Starred arguments in lambda not supported\"\n", "def test_lambda_notation():\n    inputs = {\n        \"lambda x: x**2\": Lambda(x, x**2),\n        \"lambda x, y: x + y\": Lambda((x, y), x + y),\n        \"lambda x: x[0]\": Lambda(x, x[0]),\n        \"lambda x: x.real\": Lambda(x, x.real),\n        \"lambda: 42\": Lambda((), 42),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n\n    raises(TokenError, lambda: parse_expr(\"lambda *args: args[0]\"))\n    raises(TokenError, lambda: parse_expr(\"lambda: lambda: 42\"))\n"], "sample_761": ["def test_imputation_add_indicator_constant_strategy():\n    X = np.array([\n        [\"NaN\", \"1\", \"5\", \"NaN\", \"1\"],\n        [\"2\", \"NaN\", \"1\", \"NaN\", \"2\"],\n        [\"6\", \"3\", \"NaN\", \"NaN\", \"3\"],\n        [\"1\", \"2\", \"9\", \"NaN\", \"4\"]\n    ], dtype=object)\n    X_true = np.array([\n        [\"constant\", \"1\", \"5\", \"1\", \"1\", \"1\", \"1\", \"1\"],\n        [\"2\", \"constant\", \"1\", \"2\", \"0\", \"1\", \"0\", \"1\"],\n        [\"6\", \"3\", \"5\", \"3\", \"0\", \"0\", \"1\", \"1\"],\n        [\"1\", \"2\", \"9\", \"4\", \"0\", \"0\", \"0\", \"1\"]\n    ], dtype=object)\n\n    imputer = SimpleImputer(missing_values=\"NaN\", strategy=\"constant\", fill_value=\"constant\", add_indicator=True)\n    X_trans = imputer.fit_transform(X)\n\n    assert_array_equal(X_trans, X_true)\n    assert_array_equal(imputer.indicator_.features_, np.array([0, 1, 2, 3]))\n", "def test_imputation_add_indicator_dense_matrix():\n    X_dense = np.array([\n        [np.nan, 1, 5],\n        [2, np.nan, 1],\n        [6, 3, np.nan],\n        [1, 2, 9]\n    ])\n    X_true = np.array([\n        [3., 1., 5., 1., 0., 0.],\n        [2., 2., 1., 0., 1., 0.],\n        [6., 3., 5., 0., 0., 1.],\n        [1., 2., 9., 0., 0., 0.],\n    ])\n\n    imputer = SimpleImputer(missing_values=np.nan, add_indicator=True)\n    X_trans = imputer.fit_transform(X_dense)\n\n    assert not sparse.issparse(X_trans)\n    assert X_trans.shape == X_true.shape\n    assert_allclose(X_trans, X_true)\n", "def test_imputation_add_indicator_different_shapes():\n    # Test that an error is raised when transforming data with different shape\n    X_fit = np.array([\n        [np.nan, 1, 5],\n        [2, np.nan, 1],\n        [6, 3, np.nan],\n        [1, 2, 9]\n    ])\n    X_transform = np.array([\n        [np.nan, 1],\n        [2, np.nan],\n        [6, 3],\n        [1, 2]\n    ])\n\n    imputer = SimpleImputer(missing_values=np.nan, add_indicator=True)\n    imputer.fit(X_fit)\n\n    with pytest.raises(ValueError, match=\"X has .* features per sample, expected .*\"):\n        imputer.transform(X_transform)\n", "def test_imputation_add_indicator_with_iterative_imputer():\n    X = np.array([\n        [np.nan, 1, 5, np.nan, 1],\n        [2, np.nan, 1, np.nan, 2],\n        [6, 3, np.nan, np.nan, 3],\n        [1, 2, 9, np.nan, 4]\n    ])\n    X_true = np.array([\n        [3., 1., 5., 1., 1., 0., 0., 1.],\n        [2., 2., 1., 2., 0., 1., 0., 1.],\n        [6., 3., 5., 3., 0., 0., 1., 1.],\n        [1., 2., 9., 4., 0., 0., 0., 1.]\n    ])\n\n    imputer = IterativeImputer(missing_values=np.nan, add_indicator=True)\n    X_trans = imputer.fit_transform(X)\n\n    assert_allclose(X_trans, X_true)\n    assert_array_equal(imputer.initial_imputer_.indicator_.features_, np.array([0, 1, 3]))\n", "def test_iterative_imputer_add_indicator():\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n    X[:, 0] = 1  # this column should not be discarded by IterativeImputer\n\n    imputer = IterativeImputer(missing_values=0,\n                               max_iter=2,\n                               n_nearest_features=5,\n                               sample_posterior=False,\n                               min_value=0,\n                               max_value=1,\n                               verbose=1,\n                               imputation_order='random',\n                               random_state=rng,\n                               add_indicator=True)\n    X_trans = imputer.fit_transform(X)\n\n    assert X_trans.shape == (n, d + imputer.n_features_with_missing_)\n    assert_array_equal(imputer.indicator_.features_, np.arange(d, d + imputer.n_features_with_missing_))\n"], "sample_675": ["def test_log_cli_format(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.formatter._fmt == '%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s'\n            logging.getLogger('catchlog').info(\"This log message will be shown with default format\")\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format = %(levelname)s - %(message)s\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"*INFO - This log message will be shown with default format*\"])\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_get_log_level_for_setting_invalid_level(testdir):\n    \"\"\"\n    Test that get_log_level_for_setting raises pytest.UsageError if an invalid logging level is provided.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            config = request.config\n            with pytest.raises(pytest.UsageError):\n                get_log_level_for_setting(config, 'invalid_log_level')\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('This is a single line log message.')\n            logger.info('This is a multi-line log message.\\\\nWith a second line.')\n            assert caplog.text == 'INFO     test_foo:This is a single line log message.\\\\nINFO     test_foo:This is a multi-line log message.\\\\n               With a second line.\\\\n'\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent\")\n    assert result.ret == 0\n", "def test_log_cli_enabled_without_terminal_reporter(testdir, monkeypatch):\n    msg = \"critical message logged by test\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.critical(\"{}\")\n    \"\"\".format(msg)\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n    \"\"\"\n    )\n\n    # Mock terminal reporter to be None\n    monkeypatch.setattr(testdir.config.pluginmanager, 'get_plugin', lambda name: None if name == 'terminalreporter' else testdir.config.pluginmanager.get_plugin(name))\n\n    result = testdir.runpytest()\n    assert msg not in result.stdout.str()\n", "def test_log_file_encoding(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level=INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger('catchlog').info(\"This is a test message with special characters: \u00e1\u00e9\u00ed\u00f3\u00fa\u00f1\")\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"This is a test message with special characters: \u00e1\u00e9\u00ed\u00f3\u00fa\u00f1\" in contents\n"], "sample_701": ["def test_warning_recorded_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            ...\n\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n            \"*Please use pytest_warning_recorded instead.*\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            ...\n\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n            \"*Please use pytest_warning_recorded instead.*\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            ...\n\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n            \"*Please use pytest_warning_recorded instead.*\",\n        ]\n    )\n", "def test_deprecated_options_type(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            group = parser.getgroup(\"mygroup\")\n            group.addoption('--myopt1', type='str')\n            group.addoption('--myopt2', type='str', choices=['foo', 'bar'])\n        \"\"\"\n    )\n    result = pytester.runpytest(p)\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str', \"\n            \"but when supplied should be a type (for example `str` or `int`).*\",\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str', \"\n            \"but for choices this is optional and can be omitted.*\",\n        ]\n    )\n    result.assert_outcomes(warnings=2)\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption('--my-choice', choices=['foo', 'bar'], type='str')\n\n            config.getoption('--my-choice')\n\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str', \"\n            \"but when supplied should be a type (for example `str` or `int`). \"\n            \"(options: --my-choice)*\",\n        ]\n    )\n"], "sample_1061": ["def test_Rational_floordiv():\n    assert Rational(5, 3) // S.Half == Rational(10, 3)\n", "def test_floordiv_with_float():\n    assert S(2)//Float(0.5) == 4\n", "def test_Float_floordiv():\n    a = Float('3.2')\n    b = Float('1.5')\n    assert a // b == 2.0\n", "def test_Float_floordiv():\n    a = Float('3.5')\n    b = Float('1.5')\n    assert a // b == 2.0\n    assert b // a == 0.0\n", "def test_Float_floordiv():\n    a = Float(3.5)\n    b = Float(2.0)\n\n    assert(a//b == Float(1.0))\n    assert(a//S.Half == Float(7.0))\n"], "sample_1133": ["def test_lens_makers_formula_convex_concave_lens():\n    n1, n2 = symbols('n1, n2')\n    m1 = Medium('m1', permittivity=e0, n=1)\n    m2 = Medium('m2', permittivity=e0, n=1.5)\n    assert ae(lens_makers_formula(m1, m2, 20, -10), -30.24, 2)\n    assert lens_makers_formula(n1, n2, 20, -10) == 20.0*n2/(2.0*n1 - n2)\n", "def test_lens_makers_formula_convex_convex_lens():\n    n1, n2 = symbols('n1, n2')\n    m1 = Medium('m1', permittivity=e0, n=1)\n    m2 = Medium('m2', permittivity=e0, n=1.5)\n    assert ae(lens_makers_formula(m1, m2, 20, 30), 45.00, 2)\n    assert lens_makers_formula(n1, n2, 20, 30) == 15.0*(n2 - n1)/(n1*((2.0/15.0) - (1.0/20.0) - (1.0/30.0)))\n", "def test_refraction_angle_exceptions():\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    normal_ray = Ray3D(Point3D(0, 0, 0), Point3D(0, 0, 1))\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    raises(ValueError, lambda: refraction_angle(2, 1, 1, plane=P))\n    raises(ValueError, lambda: refraction_angle(-0.5, 1, 1, plane=P))\n    raises(ValueError, lambda: refraction_angle(0.5, 1, 1, normal=normal_ray, plane=P))\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, normal=normal_ray, plane=P))\n    raises(TypeError, lambda: refraction_angle(r1, 1, 1, normal=\"not a normal\"))\n    raises(TypeError, lambda: refraction_angle(r1, 1, 1, plane=\"not a plane\"))\n", "def test_refraction_angle_exceptions():\n    n1, n2 = symbols('n1, n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    raises(ValueError, lambda: refraction_angle(r1, 1, 1, n, P))  # normal and plane should not be provided together\n    raises(ValueError, lambda: refraction_angle(2, 1, 1, n))  # incident angle out of range\n    raises(ValueError, lambda: refraction_angle(-0.5, 1, 1, n))  # incident angle out of range\n    raises(ValueError, lambda: refraction_angle(pi/2, 1, 1, n))  # incident angle out of range\n    raises(ValueError, lambda: refraction_angle(critical_angle(1, 1.33) + 0.01, 1, 1.33, n))  # total internal reflection\n", "def test_refraction_angle_exceptions():\n    n1, n2 = symbols('n1, n2')\n    m1 = Medium('m1')\n    m2 = Medium('m2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n\n    # Test for invalid angle of incidence\n    raises(ValueError, lambda: refraction_angle(pi/2, m1, m2))\n    raises(ValueError, lambda: refraction_angle(-0.1, m1, m2))\n\n    # Test for total internal reflection with angle of incidence\n    raises(ValueError, lambda: refraction_angle(pi/3, m2, m1))\n\n    # Test for normal and plane specified together\n    raises(ValueError, lambda: refraction_angle(r1, m1, m2, [0, 0, 1], P))\n\n    # Test for invalid incident type\n    raises(TypeError, lambda: refraction_angle('invalid', m1, m2))\n\n    # Test for invalid normal type\n    raises(TypeError, lambda: refraction_angle(r1, m1, m2, 'invalid'))\n\n    # Test for invalid plane type\n    raises(TypeError, lambda: refraction_angle(r1, m1, m2, plane='invalid'))\n\n    # Test for invalid medium type\n    raises(TypeError, lambda: refraction_angle(r1, 'invalid', m2))\n"], "sample_252": ["def test_key_transform_exact(self):\n    self.assertIs(NullableJSONModel.objects.filter(value__foo=KeyTransform('foo', 'value')).exists(), True)\n", "def test_key_exact(self):\n    tests = [\n        ('value__foo__exact', 'bar', self.objs[7]),\n        ('value__baz__exact', {'a': 'b', 'c': 'd'}, self.objs[7]),\n        ('value__bar__exact', ['foo', 'bar'], self.objs[7]),\n        ('value__bax__exact', {'foo': 'bar'}, self.objs[7]),\n        ('value__c__exact', 14, self.objs[3]),\n        ('value__p__exact', 4.2, self.objs[4]),\n        ('value__j__exact', None, self.objs[4]),\n        ('value__n__exact', [None], self.objs[4]),\n        (\n            'value__d__1__exact',\n            KeyTransform('f', KeyTransform('1', KeyTransform('d', 'value'))),\n            self.objs[4],\n        ),\n    ]\n    for lookup, value, expected in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertEqual(\n                NullableJSONModel.objects.filter(**{lookup: value}).get(),\n                expected,\n            )\n", "def test_key_transform_expression_wrapper(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__0__isnull=False).annotate(\n            expr=ExpressionWrapper(\n                KeyTransform('0', KeyTransform('d', 'value')),\n                output_field=models.CharField(),\n            ),\n        ).filter(expr='e'),\n        [self.objs[4]],\n    )\n", "def test_key_transform_expression_wrapper(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.annotate(\n            expr=ExpressionWrapper(\n                KeyTransform('a', 'value'),\n                output_field=CharField(),\n            ),\n        ).filter(expr='b'),\n        [self.objs[3], self.objs[4]],\n    )\n", "def test_contains_with_key_transform(self):\n    tests = [\n        ('value__baz__contains', 'd'),\n        ('value__baz__contains', {'a': 'b'}),\n        ('value__bar__contains', 'foo'),\n        ('value__contains', KeyTransform('bax', 'value')),\n        (\n            'value__contains',\n            KeyTransform('baz', RawSQL(\n                self.raw_sql,\n                ['{\"a\": \"b\", \"c\": \"d\"}'],\n            )),\n        ),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertIs(NullableJSONModel.objects.filter(\n                **{lookup: value},\n            ).exists(), True)\n"], "sample_357": ["def test_add_many_to_many_with_through(self):\n    \"\"\"#22435 - Adding a ManyToManyField with a through model should not prompt for a default.\"\"\"\n    changes = self.get_changes([self.author_empty, self.publisher], [self.author_with_m2m_through, self.publisher, self.contract])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"CreateModel\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"Contract\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"publishers\")\n", "    def test_add_model_with_field_removed_from_base_model_reverse_order(self):\n        \"\"\"\n        Removing a base field takes place before adding a new inherited model\n        that has a field with the same name, even if the order of operations\n        is reversed.\n        \"\"\"\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'book', [\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "    def test_add_model_with_custom_base(self):\n        before = [\n            ModelState('app', 'BaseModel', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'BaseModel', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'DerivedModel', [\n                ('name', models.CharField(max_length=200)),\n            ], bases=('app.BaseModel',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='DerivedModel')\n", "def test_index_suffix_length(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel('A' * 53, fields=[]),\n            migrations.CreateModel('B' * 53, fields=[]),\n        ]\n\n    migration = Migration('some_migration', 'test_app')\n    self.assertEqual(\n        migration.suggest_name(),\n        'a' * 53 + '_b' * 49 + '_0002',\n    )\n", "    def test_many_operations_with_max_length(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person1' * 10, fields=[]),\n                migrations.CreateModel('Person2' * 10, fields=[]),\n                migrations.CreateModel('Person3' * 10, fields=[]),\n                migrations.DeleteModel('Person4' * 10),\n                migrations.DeleteModel('Person5' * 10),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(\n            migration.suggest_name(),\n            'person1' * 10 + '_person2' * 10 + '_delete_person4' * 10 + '_and_more',\n        )\n"], "sample_266": ["def test_run_before_not_found(self):\n    \"\"\"\n    Makes sure the loader raises an error when a run_before migration is not found.\n    \"\"\"\n    # Load and test the plan\n    migration_loader = MigrationLoader(connection)\n    with self.assertRaises(NodeNotFoundError):\n        migration_loader.graph.forwards_plan((\"migrations\", \"0004_nonexistent\"))\n", "def test_run_before_with_unmigrated_app(self):\n    \"\"\"\n    Makes sure the loader raises a ValueError if run_before references an unmigrated app.\n    \"\"\"\n    with self.assertRaises(ValueError):\n        MigrationLoader(connection)\n", "def test_loading_replaces(self):\n    \"Tests loading a migration with replaces\"\n\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n        plan = set(loader.graph.forwards_plan(('migrations', '2_auto')))\n        return len(plan - loader.applied_migrations.keys())\n\n    # Empty database: use both migrations\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 2)\n\n    # Apply the first migration\n    recorder.record_applied(\"migrations\", \"1_auto\")\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 1)\n\n    # Apply the second migration (replaces the first)\n    recorder.record_applied(\"migrations\", \"2_auto\")\n    loader.build_graph()\n    self.assertEqual(num_nodes(), 0)\n", "def test_run_before_not_found(self):\n    \"\"\"\n    Makes sure the loader raises NodeNotFoundError when Migration.run_before references a non-existent node.\n    \"\"\"\n    # Load and test the plan\n    migration_loader = MigrationLoader(connection)\n    with self.assertRaises(NodeNotFoundError):\n        migration_loader.graph.forwards_plan((\"migrations\", \"0003_nonexistent\"))\n", "def test_loading_replaces(self):\n    \"Tests loading a migration with replaces\"\n\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Empty database: uses both migrations\n    loader.build_graph()\n    self.assertEqual(len(loader.graph.nodes), 2)\n\n    # Apply first migration: uses the replaced migration\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    loader.build_graph()\n    self.assertEqual(len(loader.graph.nodes), 1)\n    self.assertEqual(loader.graph.nodes[0][1], \"0002_replaces_0001\")\n\n    # Apply second migration: no migrations left\n    recorder.record_applied(\"migrations\", \"0002_replaces_0001\")\n    loader.build_graph()\n    self.assertEqual(len(loader.graph.nodes), 0)\n"], "sample_687": ["def test_caplog_captures_according_to_config_option_upon_success(testdir):\n    \"\"\"Test that upon success:\n    (1) `caplog` did NOT capture the DEBUG message => No assertion error is raised.\n    (2) The `DEBUG` message does NOT appear in the `Captured log call` report.\n    (3) The stdout, `INFO`, and `WARNING` messages DO appear in the test reports due to `--log-level=INFO`.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            assert 'DEBUG log ' + 'message' not in caplog.text\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-level=INFO\")\n    result.stdout.no_fnmatch_line(\"*AssertionError*\")\n    result.stdout.no_fnmatch_line(\"*DEBUG log message*\")\n    result.stdout.fnmatch_lines(\n        [\"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n    )\n    assert result.ret == 0\n", "def test_log_level_propagation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.WARNING\n\n            root_logger = logging.getLogger()\n            child_logger = logging.getLogger('child')\n\n            root_logger.setLevel(logging.INFO)\n\n            assert root_logger.level == logging.INFO\n            assert child_logger.level == logging.NOTSET\n\n            caplog.set_level(logging.DEBUG)\n\n            assert root_logger.level == logging.DEBUG\n            assert child_logger.level == logging.NOTSET\n\n            caplog.set_level(logging.WARNING, 'child')\n\n            assert root_logger.level == logging.WARNING\n            assert child_logger.level == logging.WARNING\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=WARNING\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_caplog_records_in_order(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"first log\")\n    logger.warning(\"second log\")\n    sublogger.error(\"third log\")\n\n    messages = [record.getMessage() for record in caplog.records]\n    assert messages == [\"first log\", \"second log\", \"third log\"]\n", "def test_log_level_format_parsing(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.CRITICAL\n\n            logger = logging.getLogger('catchlog')\n            logger.critical(\"CRITICAL message will be shown\")\n            logger.error(\"ERROR message will be shown\")\n\n            assert 'CRITICAL' in caplog.text\n            assert 'ERROR' in caplog.text\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=CRITICAL\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_log_file_rotation(testdir, tmpdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            log_file_path = str(tmpdir.join('test.log'))\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            plugin.set_log_path(log_file_path)\n            logger = logging.getLogger('filelog')\n            logger.info(\"This is a log message\")\n            with open(log_file_path, 'r') as f:\n                assert \"This is a log message\" in f.read()\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file = test.log\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n    log_file = tmpdir.join('test.log')\n    assert log_file.check(exists=1)\n    with open(str(log_file), 'r') as f:\n        assert \"This is a log message\" in f.read()\n\n    # Test log file rotation by running the test again\n    result = testdir.runpytest()\n    assert result.ret == 0\n    with open(str(log_file), 'r') as f:\n        assert \"This is a log message\" in f.read()\n"], "sample_274": ["def test_modelchoicefield_prepare_value(self):\n    obj = ChoiceModel.objects.create(pk=1, name='a')\n    f = ModelChoiceField(queryset=ChoiceModel.objects.all())\n    self.assertEqual(f.prepare_value(obj), 1)\n    self.assertEqual(f.prepare_value(obj.pk), 1)\n    self.assertEqual(f.prepare_value('1'), '1')\n\n    f.to_field_name = 'name'\n    self.assertEqual(f.prepare_value(obj), 'a')\n    self.assertEqual(f.prepare_value('a'), 'a')\n", "    def test_modelchoicefield_custom_to_field(self):\n        # Create choices for the model choice field test below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        # ModelChoiceField with custom to_field\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID NAME CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='name', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID NAME CHOICE'], f.clean, 'd')\n", "def test_modelform_error_messages(self):\n    class TestForm(ModelForm):\n        class Meta:\n            model = ChoiceModel\n            fields = ['name']\n            error_messages = {\n                'name': {\n                    'required': 'This field is required for ChoiceModel.',\n                    'invalid': 'Invalid value for ChoiceModel.',\n                },\n            }\n\n    # Testing overridden error messages\n    f = TestForm({'name': ''})\n    self.assertFormErrors(['This field is required for ChoiceModel.'], f.full_clean)\n\n    f = TestForm({'name': 'invalid_value'})\n    self.assertFormErrors(['Invalid value for ChoiceModel.'], f.full_clean)\n", "def test_modelform_defines_fields(self):\n    class TestForm(Form):\n        pass\n\n    class TestModelForm(ModelForm):\n        class Meta:\n            model = ChoiceModel\n            fields = ['name']\n\n    class TestModelFormWithoutMeta(ModelForm):\n        pass\n\n    self.assertFalse(modelform_defines_fields(TestForm))\n    self.assertTrue(modelform_defines_fields(TestModelForm))\n    self.assertFalse(modelform_defines_fields(TestModelFormWithoutMeta))\n", "def test_modelform_factory_validation(self):\n    class InvalidModelForm(ModelForm):\n        class Meta:\n            model = ChoiceModel\n            fields = ('invalid_field',)\n\n    with self.assertRaises(FieldError) as cm:\n        modelform_factory(ChoiceModel, form=InvalidModelForm)\n    self.assertEqual(str(cm.exception), \"Unknown field(s) (invalid_field) specified for ChoiceModel\")\n"], "sample_616": ["def test_cross_different_dim(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.", "def test_cross_dim_checks():\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"y\")\n    with pytest.raises(ValueError):\n        xr.cross(a, b)\n    with pytest.raises(KeyError):\n        xr.cross(a, b, dim=\"z\")\n    a = xr.DataArray([1, 2], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"x\")\n", "def test_cross_invalid_cases():\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n\n    # Test with invalid dim\n    with pytest.raises(KeyError):\n        xr.cross(a, b, dim=\"invalid_dim\")\n\n    # Test with invalid axis\n    with pytest.raises(np.AxisError):\n        xr.cross(a, b, dim=\"x\", axis=2)\n\n    # Test with mismatched shapes\n    a = xr.DataArray([1, 2], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"x\")\n", "def test_cross_errors(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError, match=\"Dimensions must be equal\"):\n        xr.cross(a, b, dim=dim)\n", "def test_cross_with_nans(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=dim, skipna=True)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n"], "sample_758": ["def test_check_array_shape_repr():\n    # Test _shape_repr function for different array shapes\n    assert_equal(_shape_repr((1, 2)), '(1, 2)')\n    one = 2 ** 64 / 2 ** 64  # force an upcast to `long` under Python 2\n    assert_equal(_shape_repr((one, 2 * one)), '(1, 2)')\n    assert_equal(_shape_repr((1,)), '(1,)')\n    assert_equal(_shape_repr(()), '()')\n", "def test_check_array_zero_dim():\n    # Test case for zero-dimensional arrays\n    X = np.array(42)\n    X_checked = check_array(X, ensure_2d=False)\n    assert_equal(X_checked.ndim, 0)\n", "def test_check_array_n_samples():\n    X = np.ones((3, 2))\n    n_samples = _num_samples(X)\n    assert_equal(n_samples, 3)\n\n    X = [1, 2, 3]\n    n_samples = _num_samples(X)\n    assert_equal(n_samples, 3)\n\n    X = np.array(42)\n    assert_raises_regex(TypeError, \"Singleton array array\\\\(42\\\\) cannot be considered a valid collection.\", _num_samples, X)\n\n    X = \"invalid\"\n    assert_raises_regex(TypeError, \"Expected sequence or array-like, got <class 'str'>\", _num_samples, X)\n", "def test_check_X_y_multi_output():\n    # Test check_X_y with multi_output=True\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[1, 2], [3, 4]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test check_X_y with multi_output=True and sparse matrix\n    X = sp.csr_matrix([[1, 2], [3, 4]])\n    y = sp.csr_matrix([[1, 2], [3, 4]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked.toarray(), X.toarray())\n    assert_array_equal(y_checked.toarray(), y.toarray())\n\n    # Test check_X_y with multi_output=True and non-finite values in y\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[1, 2], [3, np.nan]])\n    assert_raises(ValueError, check_X_y, X, y, multi_output=True)\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[1, 2], [3, np.inf]])\n    assert_raises(ValueError, check_X_y, X, y, multi_output=True)\n", "def test_check_array_warns_on_dtype_estimator_name():\n    # Check that the estimator name is correctly inserted in the warning message.\n    X = [[1, 2], [3, 4]]\n    estimator_name = \"DummyEstimator\"\n    estimator = type(estimator_name, (object,), {})()\n    assert_warns_message(DataConversionWarning, estimator_name, check_array, X, dtype=np.float64, warn_on_dtype=True, estimator=estimator)\n"], "sample_122": ["def test_cache_key_varies_by_accept_language(self):\n    request1 = self.factory.get(self.path, HTTP_ACCEPT_LANGUAGE='en-US')\n    learn_cache_key(request1, HttpResponse())\n    request2 = self.factory.get(self.path, HTTP_ACCEPT_LANGUAGE='es-ES')\n    learn_cache_key(request2, HttpResponse())\n    self.assertNotEqual(get_cache_key(request1), get_cache_key(request2))\n", "def test_memcached_options_with_binary(self):\n    params = {'BACKEND': self.base_params['BACKEND'], 'LOCATION': '127.0.0.1:11211', 'OPTIONS': {'binary': True}}\n    with self.settings(CACHES={'default': params}):\n        cache = caches['default']\n        self.assertTrue(cache._cache.binary)\n", "def test_set_many_with_timeout(self):\n    data = {\"key1\": \"value1\", \"key2\": \"value2\"}\n    timeout = 5\n    cache.set_many(data, timeout)\n    for key in data:\n        self.assertEqual(cache.get(key), data[key])\n    time.sleep(timeout)\n    for key in data:\n        self.assertIsNone(cache.get(key))\n", "    def test_different_instances(self):\n        \"\"\"\n        Attempting to retrieve different aliases should yield different instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']  # Assuming 'other' is a valid cache alias\n\n        self.assertIsNot(cache1, cache2)\n", "def test_cache_versioning_add_with_default_timeout(self):\n    cache.add('answer5', 42)\n    self.assertEqual(cache.get('answer5'), 42)\n    time.sleep(1)\n    self.assertIsNone(cache.get('answer5'))\n\n    cache.add('answer6', 42, timeout=None)\n    self.assertEqual(cache.get('answer6'), 42)\n    time.sleep(1)\n    self.assertEqual(cache.get('answer6'), 42)\n"], "sample_1012": ["def test_custom_functions_and_constants():\n    p = PythonCodePrinter({'user_functions': {'myfunc': 'mymodule.myfunc'},\n                           'user_constants': {'myconst': 'mymodule.myconst'}})\n    assert p.doprint(myfunc(x)) == 'mymodule.myfunc(x)'\n    assert p.doprint(myconst) == 'mymodule.myconst'\n", "def test_issue_14283_non_negative_inf():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(oo) == \"float('inf')\"\n", "def test_NumPyPrinter_Mod():\n    p = NumPyPrinter()\n    expr = Mod(x, y)\n    assert p.doprint(expr) == 'numpy.mod(x, y)'\n", "def test_issue_14283_extra():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(oo) == \"float('inf')\"\n", "def test_PythonCodePrinter_constants():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(pi) == 'math.pi'\n    assert prntr.module_imports == {'math': {'pi'}}\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.module_imports == {'math': {'pi'}, 'builtins': {'float'}}\n"], "sample_696": ["def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption('--custom', type='str', choices=['choice1', 'choice2'])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--custom=choice1\")\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str'.*\"]\n    )\n", "def test_argument_type_str_choice(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"str\", choices=[\"a\", \"b\"])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--foo\", \"a\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str'.*choices*\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n        ]\n    )\n", "def test_argument_type_str_is_deprecated(pytester: Pytester) -> None:\n    \"\"\"Test that using a string as the `type` argument to addoption() is deprecated.\"\"\"\n    pytester.makepyfile(\n        conftest=\"\"\"\n            group = parser.getgroup(\"my_plugin\")\n            group.addoption(\n                \"--my-option\",\n                action=\"store\",\n                type='str',\n                help=\"A deprecated option.\",\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str'*\"]\n    )\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    \"\"\"Test that using string types for choices in addoption is deprecated.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption(\n                \"--custom-option\",\n                type=\"str\",\n                choices=[\"choice1\", \"choice2\"],\n                default=\"choice1\",\n                help=\"Custom option for testing\",\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: `type` argument to addoption() is the string 'str'.*\"]\n    )\n\n    # Using the actual type 'str' should not raise a warning\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addoption(\n                \"--correct-option\",\n                type=str,\n                choices=[\"choice1\", \"choice2\"],\n                default=\"choice1\",\n                help=\"Correctly typed option\",\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    assert \"PytestDeprecationWarning\" not in result.stdout.str()\n"], "sample_689": ["def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    plugin = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            print(f\"Warning: {warning_message} - {when}\")\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"--capture=no\", plugin)\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\"]\n    )\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    \"\"\"pytest_warning_captured is deprecated and should raise a warning.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import warnings\n\n            print(f\"Warning captured: {warning_message}, {when}\")\n\n            warnings.warn(\"This is a deprecated warning\", DeprecationWarning)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated*\"])\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import warnings\n        import pytest\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-W\", \"always\")\n    result.stdout.fnmatch_lines(\n        [\"*The pytest_warning_captured is deprecated and will be removed in a future release.*\"]\n    )\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    plugin = testdir.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-p\", plugin)\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated*\"])\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    plugin_code = \"\"\"\n            pass\n        \"\"\"\n    testdir.makeconftest(plugin_code)\n\n    result = testdir.runpytest(\"-W\", \"ignore::DeprecationWarning\", \"-o\", \"python_functions=deprecated.WARNING_CAPTURED_HOOK\")\n\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated*\"])\n"], "sample_311": ["def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n    user = User.objects.create_user(\n        username='user',\n        password='secret',\n        email='user@example.com',\n        is_staff=False,\n    )\n    self.client.force_login(user)\n    known_url = reverse('admin10:admin_views_article_changelist')\n    response = self.client.get(known_url[:-1])\n    self.assertRedirects(response, '/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article')\n", "def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username='staff',\n        password='secret',\n        email='staff@example.com',\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse('admin10:admin_views_article_changelist')\n    response = self.client.get(known_url[:-1], SCRIPT_NAME='/prefix/')\n    self.assertRedirects(\n        response,\n        '/prefix' + known_url,\n        status_code=301,\n        fetch_redirect_response=False,\n    )\n", "def test_single_model_append_slash(self):\n    superuser = User.objects.create_user(\n        username='staff',\n        password='secret',\n        email='staff@example.com',\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse('admin11:admin_views_actor_changelist')\n    response = self.client.get(known_url[:-1])\n    self.assertRedirects(response, known_url, status_code=301, target_status_code=200)\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.staffuser = User.objects.create_user(username='staff', password='secret', is_staff=True)\n        cls.a1 = Article.objects.create(title=\"Test Article\", content=\"Test Content\", date=datetime.date.today())\n", "def test_url_with_query_parameters_if_not_authenticated(self):\n    url = reverse('admin:article_extra_json') + '?page=2'\n    response = self.client.get(url)\n    self.assertRedirects(response, '%s?next=%s' % (reverse('admin:login'), url))\n"], "sample_730": ["def test_enet_positive_constraint_with_intercept():\n    X = [[-1], [0], [1]]\n    y = [1, 0, -1]       # just a straight line with negative slope\n\n    enet = ElasticNet(alpha=0.1, max_iter=1000, positive=True, fit_intercept=True)\n    enet.fit(X, y)\n    assert_true(min(enet.coef_) >= 0)\n    assert_true(enet.intercept_ >= 0)\n", "def test_enet_alpha_zero():\n    # Test that an error message is raised if an estimator that\n    # uses _alpha_grid is called with alpha=0\n    msg = (\"Automatic alpha grid generation is not supported for alpha=0. \"\n           \"Please supply a grid by providing your estimator with the \"\n           \"appropriate `alphas=` argument.\")\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    assert_raise_message(ValueError, msg, ElasticNetCV(\n        alpha=0, random_state=42).fit, X, y)\n    assert_raise_message(ValueError, msg, MultiTaskElasticNetCV(\n        alpha=0, random_state=42).fit, X, y[:, None])\n\n    # Test that alpha=0 is allowed if we supply a grid manually\n    l1_ratios = [0.1, 0.5]\n    estkwds = {'l1_ratio': l1_ratios, 'random_state': 42}\n    est_desired = ElasticNetCV(alpha=0.00001, **estkwds)\n    est = ElasticNetCV(alpha=0, **estkwds)\n    with ignore_warnings():\n        est_desired.fit(X, y)\n        est.fit(X, y)\n    assert_array_almost_equal(est.coef_, est_desired.coef_, decimal=5)\n\n    est_desired = MultiTaskElasticNetCV(alpha=0.00001, **estkwds)\n    est = MultiTaskElasticNetCV(alpha=0, **estkwds)\n    with ignore_warnings():\n        est.fit(X, y[:, None])\n        est_desired.fit(X, y[:, None])\n    assert_array_almost_equal(est.coef_, est_desired.coef_, decimal=5)", "def test_enet_path_positive_multioutput():\n    # Test positive parameter\n\n    X, Y, _, _ = build_dataset(n_samples=50, n_features=50, n_targets=2)\n\n    # For multi output, positive parameter is not allowed\n    # Test that an error is raised\n    for path in [enet_path, lasso_path]:\n        assert_raises(ValueError, path, X, Y, positive=True)\n", "def test_enet_path_multi_output():\n    # Test multi output case for enet_path.\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20, n_targets=3)\n\n    _, coefs, _ = enet_path(X, y, l1_ratio=0.5)\n    assert coefs.shape == (3, 20, 100)  # 100 alphas by default\n\n    _, coefs, _ = enet_path(X, y, l1_ratio=0.5, n_alphas=50)\n    assert coefs.shape == (3, 20, 50)\n", "def test_enet_positive_l1_ratio():\n    # Test that an error message is raised if an estimator that\n    # uses _alpha_grid is called with l1_ratio<0 or l1_ratio>1\n    msg = (\"l1_ratio must be between 0 and 1, inclusive.\")\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    assert_raise_message(ValueError, msg, ElasticNetCV(\n        l1_ratio=-0.1, random_state=42).fit, X, y)\n    assert_raise_message(ValueError, msg, MultiTaskElasticNetCV(\n        l1_ratio=1.1, random_state=42).fit, X, y[:, None])\n\n    # Test that l1_ratio=0 and l1_ratio=1 are valid\n    alphas = [0.1, 10]\n    estkwds = {'alphas': alphas, 'random_state': 42}\n    est_l1 = ElasticNetCV(l1_ratio=1, **estkwds)\n    est_l2 = ElasticNetCV(l1_ratio=0, **estkwds)\n    with ignore_warnings():\n        est_l1.fit(X, y)\n        est_l2.fit(X, y)\n    # Verify that l1_ratio=1 (Lasso) and l1_ratio=0 (Ridge) give different results\n    assert not np.allclose(est_l1.coef_, est_l2.coef_)\n"], "sample_568": ["def test_surface3d_zsort_nan():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z[x.shape[0] // 2:, x.shape[1] // 2:] = np.nan\n\n    ax.plot_surface(x, y, z, cmap='jet')\n    ax.view_init(elev=45, azim=145)\n", "def test_Poly3DCollection_shade_zsort():\n    # Test that the shading is applied correctly when zsort is True\n    # GH#26420\n    poly = np.array([[0, 0, 1], [0, 1, 1], [0, 0, 0]], float)\n    c = art3d.Poly3DCollection([poly], shade=True, zsort=True)\n    assert c.get_zsort()\n    # Add more assertions to check the shading and sorting of the collection\n", "def test_quiver3D_color_and_length():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Define quiver data\n    x, y, z = np.meshgrid(np.arange(0, 1, 0.2), np.arange(0, 1, 0.2), np.arange(0, 1, 0.2))\n    u = np.sin(np.pi * x) * np.cos(np.pi * y) * np.cos(np.pi * z)\n    v = -np.cos(np.pi * x) * np.sin(np.pi * y) * np.cos(np.pi * z)\n    w = (2/3)**0.5 * np.cos(np.pi * x) * np.cos(np.pi * y) * np.sin(np.pi * z)\n\n    # Test with color as string\n    ax.quiver(x, y, z, u, v, w, color='r', length=0.1)\n\n    # Test with color as RGB tuple\n    ax.quiver(x.ravel(), y.ravel(), z.ravel(), u.ravel(), v.ravel(), w.ravel(), color=(1, 0, 0), length=0.1)\n\n    # Test with length as scalar\n    ax.quiver(x, y, z, u, v, w, color='b', length=0.2)\n\n    # Test with length as array\n    length = np.sqrt(u**2 + v**2 + w**2)\n    ax.quiver(x, y, z, u, v, w, color='g', length=length)\n", "def test_Poly3DCollection_init_facecolor_warning():\n    # test that a warning is raised when facecolor is a single color\n    # GH#26420\n    with pytest.warns(UserWarning, match='Argument \"facecolor\" of type <class \\'str\\'> is a single color, '\n                                         'and will result in an elementwise comparison to \"x\" in the collection'):\n        poly = np.array([[0, 0, 1], [0, 1, 1], [0, 0, 0]], float)\n        c = art3d.Poly3DCollection([poly], facecolor='r')\n", "def test_bar3d_shaded():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    x = np.arange(4)\n    y = np.arange(5)\n    x2d, y2d = np.meshgrid(x, y)\n    x2d, y2d = x2d.ravel(), y2d.ravel()\n    z = x2d + y2d + 1  # Avoid triggering bug with zero-depth boxes.\n    ax.bar3d(x2d, y2d, x2d * 0, 1, 1, z, shade=True)\n    fig.canvas.draw()\n"], "sample_398": ["def test_logout_csrf_rotate(self):\n    \"\"\"\n    Makes sure that a logout rotates the currently-used CSRF token.\n    \"\"\"\n\n        return HttpResponse()\n\n    # Do a GET to establish a CSRF token\n    req = HttpRequest()\n    CsrfViewMiddleware(get_response).process_view(req, LoginView.as_view(), (), {})\n    # get_token() triggers CSRF token inclusion in the response\n    get_token(req)\n    resp = CsrfViewMiddleware(LoginView.as_view())(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, None)\n    token1 = csrf_cookie.coded_value\n\n    # Log in to set another CSRF token\n    self.login()\n\n    # Prepare the POST request to log out\n    req = HttpRequest()\n    req.COOKIES[settings.CSRF_COOKIE_NAME] = self.client.session.session_key\n    req.method = \"POST\"\n    req.POST = {\"csrfmiddlewaretoken\": self.client.session.session_key}\n\n    # Use POST request to log out\n    SessionMiddleware(get_response).process_request(req)\n    CsrfViewMiddleware(get_response).process_view(req, LogoutView.as_view(), (), {})\n    resp = CsrfViewMiddleware(LogoutView.as_view())(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, None)\n    token2 = csrf_cookie.coded_value\n\n    # Check the CSRF token switched\n    self.assertNotEqual(token1, token2)\n", "    def setUpTestData(cls):\n        cls.u1 = UUIDUser.objects.create_user(\n            email=cls.user_email,\n            username=\"foo\",\n            password=cls.user_password,\n        )\n", "    def assertLogoutRedirectURLEqual(self, url):\n        self.logout()\n        response = self.client.post(\"/logout/\")\n        self.assertRedirects(response, url, fetch_redirect_response=False)\n", "    def test_user_change_different_user_email(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        original_email = u.email\n        new_email = \"new_\" + original_email\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_change\", args=(u.pk,)),\n            {\n                **self.get_user_data(u),\n                \"email\": new_email,\n            },\n        )\n        self.assertRedirects(response, reverse(\"auth_test_admin:auth_user_changelist\"))\n        row = LogEntry.objects.latest(\"id\")\n        self.assertEqual(row.user_id, self.admin.pk)\n        self.assertEqual(row.object_id, str(u.pk))\n        self.assertEqual(row.get_change_message(), \"Changed email address.\")\n        u.refresh_from_db()\n        self.assertEqual(u.email, new_email)\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.u1 = UUIDUser.objects.create_user(\n            username=\"testclient\", password=\"password\", email=\"testclient@example.com\"\n        )\n        cls.u3 = UUIDUser.objects.create_user(\n            username=\"staff\", password=\"password\", email=\"staffmember@example.com\"\n        )\n        # Make me a superuser before logging in.\n        UUIDUser.objects.filter(username=\"testclient\").update(\n            is_staff=True, is_superuser=True\n        )\n"], "sample_439": ["def test_custom_renderer_template_name_override(self):\n    class Person(Form):\n        first_name = CharField()\n\n    t = Template(\"{{ form }}\")\n    html = t.render(Context({\"form\": Person(renderer=CustomRenderer())}))\n    expected = \"\"\"\n    <div class=\"fieldWrapper\"><label for=\"id_first_name\">First name:</label>\n    <input type=\"text\" name=\"first_name\" required id=\"id_first_name\"></div>\n    \"\"\"\n    self.assertHTMLEqual(html, expected)\n", "def test_field_max_length(self):\n    class MyForm(Form):\n        field = CharField(max_length=10)\n\n    f = MyForm({\"field\": \"a\" * 11})\n    self.assertFalse(f.is_valid())\n    self.assertEqual(f.errors[\"field\"], [\"Ensure this value has at most 10 characters (it has 11).\"])\n", "def test_required_css_class_in_templates(self):\n    class CustomFrameworkForm(FrameworkForm):\n        template_name = \"forms_tests/required_css_class_test.html\"\n        required_css_class = \"required\"\n\n    f = CustomFrameworkForm()\n    self.assertHTMLEqual(\n        str(f),\n        '<div class=\"required\"><label for=\"id_name\" class=\"required\">Name:</label>'\n        '<input type=\"text\" name=\"name\" required id=\"id_name\"></div>'\n        '<div class=\"required\"><label for=\"id_language_0\" class=\"required\">Language:</label>'\n        '<input type=\"radio\" name=\"language\" value=\"P\" required id=\"id_language_0\">'\n        '<label for=\"id_language_0\">Python</label>'\n        '<input type=\"radio\" name=\"language\" value=\"J\" required id=\"id_language_1\">'\n        '<label for=\"id_language_1\">Java</label></div>',\n    )\n", "    def test_errorlist_template_name(self):\n        class CustomErrorList(ErrorList):\n            template_name = \"forms_tests/custom_error.html\"\n\n        class CommentForm(Form):\n            name = CharField(max_length=50, required=False)\n            email = EmailField()\n            comment = CharField()\n\n        data = {\"email\": \"invalid\"}\n        f = CommentForm(data, auto_id=False, error_class=CustomErrorList)\n        self.assertHTMLEqual(\n            f.errors.as_ul(),\n            '<ul class=\"errorlist\">'\n            '<li>Enter a valid email address.</li>'\n            '<li>This field is required.</li>'\n            '</ul>',\n        )\n", "def test_form_with_callable_initial_and_clean(self):\n    class Person(Form):\n        first_name = CharField(initial=lambda: \"John\")\n        last_name = CharField(initial=\"Doe\")\n\n            return self.cleaned_data['first_name'].upper()\n\n    p = Person(auto_id=False)\n    self.assertHTMLEqual(\n        p.as_ul(),\n        '<li><label for=\"id_first_name\">First name:</label> <input type=\"text\" name=\"first_name\" value=\"John\" required id=\"id_first_name\"></li>'\n        '<li><label for=\"id_last_name\">Last name:</label> <input type=\"text\" name=\"last_name\" value=\"Doe\" required id=\"id_last_name\"></li>',\n    )\n    p = Person({\"first_name\": \"jane\", \"last_name\": \"Doe\"}, auto_id=False)\n    self.assertTrue(p.is_valid())\n    self.assertEqual(p.cleaned_data, {\"first_name\": \"JANE\", \"last_name\": \"Doe\"})\n"], "sample_690": ["def test_marked_xfail_with_reason(self, pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason=\"this is an xfail\")\n            assert 0\n        \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    assert len(reports) == 3\n    callreport = reports[1]\n    assert callreport.skipped\n    assert callreport.wasxfail == \"this is an xfail\"\n", "def test_marked_skip_with_reason(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason=\"this is a skip\")\n            pass\n    \"\"\"\n    )\n    skipped = evaluate_skip_marks(item)\n    assert skipped\n    assert skipped.reason == \"this is a skip\"\n", "def test_xfail_imperative_condition(self, pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(\"True\")\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_this*\", \"*reason: True*\"])\n", "def test_evaluate_condition_error(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(\"syntax error\")\n            pass\n    \"\"\"\n    )\n    mark = item.get_closest_marker(\"xfail\")\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_condition(item, mark, \"syntax error\")\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'xfail' condition\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_xfail_condition_with_invalid_syntax(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(\"syntax error\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_xfail_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'xfail' condition\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n"], "sample_96": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n", "    def test_exclude_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 'hello'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n", "    def test_exclude_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n"], "sample_304": ["def test_email_validator_with_allowlist(self):\n    validator = EmailValidator(allowlist=['example.com'])\n    self.assertIsNone(validator('test@example.com'))\n    with self.assertRaises(ValidationError):\n        validator('test@invalid.com')\n", "compilation error", "def test_email_validator_with_allowlist(self):\n    validator = EmailValidator(allowlist=['localdomain'])\n    self.assertIsNone(validator('email@localdomain'))\n    with self.assertRaises(ValidationError):\n        validator('email@otherdomain')\n", "def test_email_validator_idn(self):\n    # Test email validation for IDN (Internationalized Domain Names)\n    validator = EmailValidator()\n    self.assertIsNone(validator('test@xn--hxajbheg2az3al.xn--p1ai'))  # Example for '\u0440\u0444' domain\n    self.assertRaises(ValidationError, validator, 'test@xn--hxajbheg2az3al.xn--p1a')  # Invalid IDN\n", "compilation error"], "sample_1052": ["def test_fcode_complex_output():\n    import sympy.utilities.codegen\n    sympy.utilities.codegen.COMPLEX_ALLOWED = True\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    result = codegen(('test', x+y), 'f95', 'test', header=False, empty=False)\n    source = (result[0][1])\n    expected = (\n        \"REAL*8 function test(x, y)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"REAL*8, intent(in) :: y\\n\"\n        \"test = x + y\\n\"\n        \"end function\\n\")\n    assert source == expected\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    result = codegen(('test', x+y), 'f95', 'test', header=False, empty=False)\n    source = (result[0][1])\n    expected = (\n        \"COMPLEX*16 function test(x, y)\\n\"\n        \"implicit none\\n\"\n        \"COMPLEX*16, intent(in) :: x\\n\"\n        \"REAL*8, intent(in) :: y\\n\"\n        \"test = x + y\\n\"\n        \"end function\\n\"\n        )\n    assert source == expected\n    sympy.utilities.codegen.COMPLEX_ALLOWED = False\n", "def test_fcode_complex_matrix():\n    import sympy.utilities.codegen\n    sympy.utilities.codegen.COMPLEX_ALLOWED = True\n    x = Symbol('x')\n    y = Symbol('y')\n    A = MatrixSymbol('A', 2, 2)\n    result = codegen(('test', A*Matrix([[x], [y]])), 'f95', 'test', header=False, empty=False)\n    source = (result[0][1])\n    expected = (\n        \"subroutine test(A, x, y, out_%(hash)s)\\n\"\n        \"implicit none\\n\"\n        \"COMPLEX*16, intent(in), dimension(1:2, 1:2) :: A\\n\"\n        \"COMPLEX*16, intent(in) :: x\\n\"\n        \"COMPLEX*16, intent(in) :: y\\n\"\n        \"COMPLEX*16, intent(out), dimension(1:2, 1:1) :: out_%(hash)s\\n\"\n        \"out_%(hash)s(1, 1) = A(1, 1)*x + A(1, 2)*y\\n\"\n        \"out_%(hash)s(2, 1) = A(2, 1)*x + A(2, 2)*y\\n\"\n        \"end subroutine\\n\"\n    )\n    # look for the magic number\n    a = source.splitlines()[5]\n    b = a.split('_')\n    out = b[1]\n    expected = expected % {'hash': out}\n    assert source == expected\n    sympy.utilities.codegen.COMPLEX_ALLOWED = False\n", "def test_c_complex():\n    import sympy.utilities.codegen\n    sympy.utilities.codegen.COMPLEX_ALLOWED = True\n    x = Symbol('x', real=True)\n    y = Symbol('y', complex=True)\n    result = codegen(('test', x + y), 'C', 'test', header=False, empty=False)\n    source = (result[0][1])\n    expected = (\n        \"#include \\\"test.h\\\"\\n\"\n        \"#include <math.h>\\n\"\n        \"double complex test(double x, double complex y) {\\n\"\n        \"   double complex test_result;\\n\"\n        \"   test_result = x + y;\\n\"\n        \"   return test_result;\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n    sympy.utilities.codegen.COMPLEX_ALLOWED = False\n", "def test_fcode_matrix_input_output():\n    x, y, z = symbols('x,y,z')\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 2, 3)\n    expr1 = Equality(A, Matrix([[1, 2, x], [3, 4, z]]))\n    expr2 = Equality(B, 2*A)\n    name_expr = (\"test\", [expr1, expr2])\n    result = codegen(name_expr, \"f95\", \"test\", header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        \"subroutine test(x, z, A, B)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"REAL*8, intent(in) :: z\\n\"\n        \"REAL*8, intent(out), dimension(1:2, 1:3) :: A\\n\"\n        \"REAL*8, intent(out), dimension(1:2, 1:3) :: B\\n\"\n        \"A(1, 1) = 1\\n\"\n        \"A(1, 2) = 2\\n\"\n        \"A(1, 3) = x\\n\"\n        \"A(2, 1) = 3\\n\"\n        \"A(2, 2) = 4\\n\"\n        \"A(2, 3) = z\\n\"\n        \"B(1, 1) = 2*A(1, 1)\\n\"\n        \"B(1, 2) = 2*A(1, 2)\\n\"\n        \"B(1, 3) = 2*A(1, 3)\\n\"\n        \"B(2, 1) = 2*A(2, 1)\\n\"\n        \"B(2, 2) = 2*A(2, 2)\\n\"\n        \"B(2, 3) = 2*A(2, 3)\\n\"", "def test_fcode_matrix_output_dim_names():\n    x, y, z = symbols('x,y,z')\n    e1 = x + y\n    e2 = Matrix([[x, y], [z, 16]])\n    A = MatrixSymbol('A', 1, 3)\n    name_expr = (\"test\", (e1, e2, A))\n    result = codegen(name_expr, \"f95\", \"test\", header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        \"subroutine test(x, y, z, A, out_%(hash)s, out_%(hash2)s)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"REAL*8, intent(in) :: y\\n\"\n        \"REAL*8, intent(in) :: z\\n\"\n        \"REAL*8, intent(out), dimension(1:1, 1:3) :: A\\n\"\n        \"REAL*8, intent(out), dimension(1:2, 1:2) :: out_%(hash)s\\n\"\n        \"REAL*8, intent(out) :: out_%(hash2)s\\n\"\n        \"out_%(hash)s(1, 1) = x\\n\"\n        \"out_%(hash)s(2, 1) = z\\n\"\n        \"out_%(hash)s(1, 2) = y\\n\"\n        \"out_%(hash)s(2, 2) = 16\\n\"\n        \"out_%(hash2)s = x + y\\n\"\n        \"end subroutine\\n\"\n    )\n    # look for the magic numbers\n    a = source.splitlines()[5]\n    b = a.split('_')\n    out = b[1]\n    c = source.splitlines()[6]\n    d = c.split('_')\n    out2 = d[1]\n"], "sample_197": ["def test_time_strings_parameter(self):\n    \"\"\"\n    The time_strings parameter allows customization of the output strings.\n    \"\"\"\n    custom_time_strings = {\n        'year': '%d yr',\n        'month': '%d mo',\n        'week': '%d wk',\n        'day': '%d d',\n        'hour': '%d hr',\n        'minute': '%d min',\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1 yr')\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1 mo')\n", "    def test_leap_year_with_time(self):\n        start_date = datetime.datetime(2016, 2, 28, 12, 0, 0)\n        end_date = datetime.datetime(2016, 3, 1, 12, 0, 0)\n        self.assertEqual(timesince(end_date, start_date), '24\\xa0hours')\n        self.assertEqual(timeuntil(start_date, end_date), '24\\xa0hours')\n", "def test_time_strings_override(self):\n    \"\"\" Custom time strings can be provided. \"\"\"\n    custom_strings = {\n        'year': 'Y%dY',\n        'month': 'M%dM',\n        'week': 'W%dW',\n        'day': 'D%dD',\n        'hour': 'H%dH',\n        'minute': 'm%d',\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_strings), 'Y1Y')\n    self.assertEqual(timeuntil(self.t, self.t - self.oneday, time_strings=custom_strings), 'D1D')\n", "def test_custom_time_strings(self):\n    \"\"\" Custom time strings are used correctly. \"\"\"\n    custom_strings = {\n        'year': '%d yr',\n        'month': '%d mo',\n        'week': '%d wk',\n        'day': '%d d',\n        'hour': '%d h',\n        'minute': '%d min',\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_strings), '1 yr')\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_strings), '1 mo')\n    self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_strings), '1 wk')\n    self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_strings), '1 d')\n    self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_strings), '1 h')\n    self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_strings), '1 min')\n", "def test_leap_year_subtraction(self):\n    start_date = datetime.date(2020, 2, 29)\n    end_date = datetime.date(2021, 2, 28)\n    self.assertEqual(timeuntil(end_date, start_date), '1\\xa0year')\n    self.assertEqual(timesince(start_date, end_date), '1\\xa0year')\n"], "sample_365": ["    def test_classproperty_multiple_getters(self):\n        class Foo:\n            @classproperty\n                return 123\n\n            @foo.getter\n                return 456\n\n            @foo.getter\n                return 789\n\n        self.assertEqual(Foo.foo, 789)\n        self.assertEqual(Foo().foo, 789)\n", "    def test_lazy_text_cast(self):\n        \"\"\"\n        __text_cast() method works correctly for Promises.\n        \"\"\"\n        original_object = 'Lazy translation text'\n        lazy_obj = lazy(lambda: original_object, str)\n        self.assertEqual(lazy_obj().__text_cast(), original_object)\n", "def test_lazy_pickle(self):\n    \"\"\"\n    Lazy objects can be pickled and unpickled.\n    \"\"\"\n    import pickle\n\n        return \"Lazy pickle test\"\n\n    lazy_obj = lazy(func, str)\n    pickled_obj = pickle.dumps(lazy_obj())\n    unpickled_obj = pickle.loads(pickled_obj)\n\n    self.assertEqual(str(lazy_obj()), str(unpickled_obj))\n", "def test_lazy_bytes_cast(self):\n    original_object = 'J\u00fcst a str\u00efng'\n    lazy_obj = lazy(lambda: original_object, str)\n    self.assertEqual(bytes(lazy_obj()), original_object.encode())\n", "    def test_lazy_comparison(self):\n        \"\"\"\n        Comparison operators work correctly for Promises.\n        \"\"\"\n        lazy_a = lazy(lambda: 4, int)\n        lazy_b = lazy(lambda: 4, int)\n        lazy_c = lazy(lambda: 5, int)\n\n        self.assertTrue(lazy_a() <= lazy_b())\n        self.assertTrue(lazy_b() >= lazy_a())\n        self.assertFalse(lazy_b() < lazy_c())\n        self.assertFalse(lazy_c() > lazy_a())\n"], "sample_183": ["def test_condition_with_q_object(self):\n    qs = CaseTestModel.objects.annotate(\n        test=Case(\n            When(Q(integer2=1), integer=2, then=Value(False)),\n            When(Q(integer2=1), integer=1, then=Value(True)),\n            default=Value(False),\n            output_field=BooleanField(),\n        ),\n    )\n    self.assertIs(qs.get(integer=1).test, True)\n", "def test_nested_case_expression(self):\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            nested_case=Case(\n                When(integer=1, then=Case(\n                    When(integer2=1, then=Value('integer1_1_integer2_1')),\n                    default=Value('integer1_1_default'),\n                    output_field=CharField(),\n                )),\n                When(integer=2, then=Case(\n                    When(integer2=2, then=Value('integer1_2_integer2_2')),\n                    default=Value('integer1_2_default'),\n                    output_field=CharField(),\n                )),\n                default=Value('outer_default'),\n                output_field=CharField(),\n            ),\n        ).order_by('pk'),\n        [\n            (1, 1, 'integer1_1_integer2_1'),\n            (2, 3, 'integer1_2_default'),\n            (3, 4, 'outer_default'),\n            (2, 2, 'integer1_2_integer2_2'),\n            (3, 4, 'outer_default'),\n            (3, 3, 'outer_default'),\n            (4, 5, 'outer_default')\n        ],\n        transform=attrgetter('integer', 'integer2', 'nested_case')\n    )\n", "def test_lookup_with_annotated_field(self):\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            integer_plus_1=F('integer') + 1,\n        ).annotate(\n            test=Case(\n                When(integer_plus_1=F('integer2'), then=Value('equal')),\n                When(integer_plus_1=F('integer2') + 1, then=Value('plus_one')),\n                default=Value('other'),\n                output_field=CharField(),\n            ),\n        ).order_by('pk'),\n        [(1, 'equal'), (2, 'plus_one'), (3, 'other'), (2, 'equal'), (3, 'plus_one'), (3, 'equal'), (4, 'other')],\n        transform=attrgetter('integer', 'test')\n    )\n", "def test_only_q_object_arguments(self):\n    msg = \"Only Q() objects are allowed in a When() condition.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        When(pk__in=[], then=Value(True))\n", "def test_lookup_multiple_conditions(self):\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            test=Case(\n                When(integer__lt=2, integer2__gt=2, then=Value('lt2 and gt2')),\n                When(integer__gt=2, integer2__lt=2, then=Value('gt2 and lt2')),\n                default=Value('other'),\n                output_field=CharField(),\n            ),\n        ).order_by('pk'),\n        [\n            (1, 1, 'other'), (2, 3, 'lt2 and gt2'), (3, 4, 'gt2 and lt2'), (2, 2, 'other'),\n            (3, 4, 'other'), (3, 3, 'other'), (4, 5, 'other')\n        ],\n        transform=attrgetter('integer', 'integer2', 'test')\n    )\n"], "sample_857": ["def test_feature_importances_sum_to_one():\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(iris.data, iris.target)\n    assert_almost_equal(np.sum(clf.feature_importances_), 1.0)\n", "def test_min_impurity_decrease_raises_negative_value():\n    clf = DecisionTreeClassifier()\n    msg = \"min_impurity_decrease must be greater than or equal to 0\"\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(min_impurity_decrease=-1.0)\n        clf.fit(X, y)\n\n    clf.set_params(min_impurity_decrease=0.0)\n    clf.fit(X, y)\n\n    with pytest.raises(ValueError, match=msg):\n        clf.set_params(min_impurity_decrease=-1.0)\n        clf._prune_tree()\n", "def test_prune_tree_with_small_ccp_alpha():\n    X, y = make_classification(n_samples=1000, n_features=20,\n                               n_informative=2, n_redundant=10,\n                               n_classes=2, random_state=1)\n    clf = DecisionTreeClassifier(random_state=1)\n    path = clf.cost_complexity_pruning_path(X, y)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n    # Test with a small ccp_alpha\n    clf_pruned = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alphas[-2])\n    clf_pruned.fit(X, y)\n\n    # Check that the pruned tree has fewer nodes\n    assert clf_pruned.tree_.node_count < clf.tree_.node_count\n\n    # Check that the pruned tree has higher accuracy\n    assert clf_pruned.score(X, y) >= clf.score(X, y)\n", "def test_prune_tree_classifier_pruned_trees_are_subtrees(criterion):\n    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n    est = DecisionTreeClassifier(criterion=criterion, max_leaf_nodes=20, random_state=0)\n    info = est.cost_complexity_pruning_path(X, y)\n    pruning_path = info.ccp_alphas\n\n    for ccp_alpha in pruning_path:\n        pruned_est = DecisionTreeClassifier(criterion=criterion, ccp_alpha=ccp_alpha, random_state=0)\n        pruned_est.fit(X, y)\n        assert_is_subtree(est.tree_, pruned_est.tree_)\n", "def test_prune_tree_negative_ccp_alpha_raises_exception():\n    X = iris.data\n    y = iris.target\n\n    # Try to prune with negative ccp_alpha\n    with pytest.raises(ValueError):\n        DecisionTreeClassifier(ccp_alpha=-1).fit(X, y)\n"], "sample_1201": ["def test_additional_conversions():\n    assert convert_to(statvolt, volt, cgs_gauss) == 299792458*volt/10**6\n    assert convert_to(statampere, coulomb/second, cgs_gauss) == statcoulomb/second\n    assert convert_to(erg, joule, cgs_gauss) == joule/10**7\n    assert convert_to(dyne, newton, cgs_gauss) == newton/10**5\n    assert convert_to(newton, dyne, cgs_gauss) == 10**5*dyne\n", "def test_cgs_gauss_additional_conversions():\n    assert convert_to(statvolt, volt, cgs_gauss) == volt*299792458/10**6\n    assert convert_to(volt, statvolt, cgs_gauss) == statvolt*10**6/299792458\n    assert convert_to(farad, henry, cgs_gauss) == henry*299792458**2/10**5\n    assert convert_to(henry, farad, cgs_gauss) == farad*10**5/299792458**2\n    assert convert_to(ohm, farad, cgs_gauss) == farad*299792458**2/10**5\n    assert convert_to(farad, ohm, cgs_gauss) == ohm*10**5/299792458**2\n", "def test_cgs_gauss_conversion_symmetry():\n    # Test symmetry of conversion factors\n    assert convert_to(convert_to(coulomb, statcoulomb, cgs_gauss), coulomb, cgs_gauss) == coulomb\n    assert convert_to(convert_to(statcoulomb, coulomb, cgs_gauss), statcoulomb, cgs_gauss) == statcoulomb\n    assert convert_to(convert_to(erg, joule, SI), erg, SI) == erg\n    assert convert_to(convert_to(erg, joule, cgs_gauss), erg, cgs_gauss) == erg\n    assert convert_to(convert_to(dyne, newton, SI), dyne, SI) == dyne\n    assert convert_to(convert_to(dyne, newton, cgs_gauss), dyne, cgs_gauss) == dyne\n", "def test_cgs_gauss_conversion_factors():\n    # Test conversion factors for some units\n    assert convert_to(statvolt, volt, cgs_gauss) == volt/2.99792458e8\n    assert convert_to(volt, statvolt, cgs_gauss) == 2.99792458e8*statvolt\n    assert convert_to(statampere, coulomb/second, cgs_gauss) == coulomb/(2.99792458e10*second)\n    assert convert_to(coulomb/second, statampere, cgs_gauss) == 2.99792458e10*second*statampere\n", "def test_cgs_gauss_unit_conversions():\n    # Test conversion from and to statvolt\n    assert convert_to(volt, statvolt, cgs_gauss) == convert_to(statvolt, volt, cgs_gauss).evalf(16)\n\n    # Test conversion from and to statampere\n    assert convert_to(ampere, statampere, cgs_gauss) == convert_to(statampere, ampere, cgs_gauss).evalf(16)\n\n    # Test conversion from and to gauss\n    assert convert_to(tesla, gauss, cgs_gauss) == convert_to(gauss, tesla, cgs_gauss).evalf(16)\n\n    # Test conversion from and to maxwell\n    assert convert_to(weber, maxwell, cgs_gauss) == convert_to(maxwell, weber, cgs_gauss).evalf(16)\n"], "sample_614": ["def test_dim_summary_limited(n_dims: int = 100) -> None:\n    expected = \"\"\"\\\n    dim_0: 3, dim_1: 3, dim_2: 3, dim_3: 3, dim_4: 3, dim_5: 3, dim_6: 3, dim_7: 3,\n    dim_8: 3, dim_9: 3, dim_10: 3, dim_11: 3, dim_12: 3, dim_13: 3, dim_14: 3, dim_15: 3,\n    dim_16: 3, dim_17: 3, dim_18: 3, dim_19: 3, dim_20: 3, dim_21: 3, dim_22: 3, dim_23: 3,\n    ...\n    dim_76: 3, dim_77: 3, dim_78: 3, dim_79: 3, dim_80: 3, dim_81: 3, dim_82: 3, dim_83: 3,\n    dim_84: 3, dim_85: 3, dim_86: 3, dim_87: 3, dim_88: 3, dim_89: 3, dim_90: 3, dim_91: 3,\n    dim_92: 3, dim_93: 3, dim_94: 3, dim_95: 3, dim_96: 3, dim_97: 3, dim_98: 3, dim_99: 3\"\"\"\n    expected = dedent(expected)\n\n    obj = xr.Dataset({f\"dim_{k}\": 3 for k in np.arange(n_dims)})\n    col_width = 10\n    max_rows = 12\n    actual = formatting.dim_summary_limited(obj, col_width, max_rows)\n    assert expected == actual\n", "def test_limit_lines() -> None:\n    short_str = \"This is a short string.\"\n    assert formatting.limit_lines(short_str, limit=3) == short_str\n\n    long_str = \"\\n\".join([\"Line \" + str(i) for i in range(10)])\n    expected = \"\\n\".join([\"Line 0\", \"...\", \"Line 9\"])\n    assert formatting.limit_lines(long_str, limit=3) == expected\n", "def test_inline_dask_repr(self) -> None:\n    import dask.array as da\n\n    # Test with meta attribute\n    array = da.from_array(np.random.randn(10, 10), chunks=(5, 5), meta=np.ndarray)\n    expected = \"dask.array<chunksize=(5, 5), meta=np.ndarray>\"\n    assert formatting.inline_dask_repr(array) == expected\n\n    # Test without meta attribute\n    array = da.from_array(np.random.randn(10, 10), chunks=(5, 5))\n    expected = \"dask.array<chunksize=(5, 5)>\"\n    assert formatting.inline_dask_repr(array) == expected\n", "def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\", \"date\", \"time\"),\n        (pd.Timedelta(\"10 days\"), \"10 days\", \"date\", \"date\"),\n        (pd.Timedelta(\"1 hour\"), \"01:00:00\", \"time\", \"time\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\", None, None),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\", \"date\", \"time\"),\n    ]\n    for item, expected, format_date, format_time in cases:\n        actual = formatting.format_timedelta(item, timedelta_format=\"datetime\")\n        assert expected == actual\n        actual = formatting.format_timedelta(item, timedelta_format=\"date\")\n        assert format_date == actual\n        actual = formatting.format_timedelta(item, timedelta_format=\"time\")\n        assert format_time == actual\n", "def test_unindexed_dims_repr():\n    ds = xr.Dataset(coords={\"foo\": [1, 2, 3]})\n    unindexed_dims = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    assert unindexed_dims is None\n\n    ds[(1, 2)] = xr.DataArray([0], dims=\"test\")\n    unindexed_dims = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    assert unindexed_dims == \"Dimensions without coordinates: test\"\n"], "sample_630": ["def test_get_annotation_annassign_complex(assign, label):\n    \"\"\"AnnAssign with complex types\"\"\"\n    node = astroid.extract_node(assign)\n    got = get_annotation(node.value).name\n    assert isinstance(node, astroid.AnnAssign)\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_functiondef(method, label):\n    \"\"\"FunctionDef\"\"\"\n    node = astroid.extract_node(method)\n    got = get_annotation(node.returns).name\n    assert isinstance(node, astroid.FunctionDef)\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_annassign_complex_types(assign, label):\n    \"\"\"AnnAssign with complex types\"\"\"\n    node = astroid.extract_node(assign)\n    got = get_annotation(node.value).name\n    assert isinstance(node, astroid.AnnAssign)\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_complex(assign, label):\n    \"\"\"Complex type annotations\"\"\"\n    node = astroid.extract_node(assign)\n    got = get_annotation(node.value).name\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_assign(assign, label):\n    \"\"\"Assign\"\"\"\n    node = astroid.extract_node(assign)\n    got = get_annotation(node.value).name\n    assert isinstance(node, astroid.Assign)\n    assert got == label, f\"got {got} instead of {label} for value {node}\"\n"], "sample_1113": ["def test_matrix_expression_from_index_summation_with_identities():\n    from sympy.abc import a, b, c, d, k\n    A = MatrixSymbol(\"A\", k, k)\n    I = Identity(k)\n    w1 = MatrixSymbol(\"w1\", k, 1)\n\n    expr = Sum(I[a, b]*A[b, c]*w1[c, d], (b, 0, k-1), (c, 0, k-1))\n    assert MatrixExpr.from_index_summation(expr, a) == A*w1\n\n    expr = Sum(A[a, b]*I[b, c]*w1[c, d], (b, 0, k-1), (c, 0, k-1))\n    assert MatrixExpr.from_index_summation(expr, a) == A*w1\n\n    expr = Sum(A[a, b]*w1[b, d]*I[c, d], (b, 0, k-1), (c, 0, k-1))\n    assert MatrixExpr.from_index_summation(expr, a) == A*w1\n", "def test_matrix_expression_to_indices_with_symbolic_shapes():\n    n, m, l = symbols('n m l', integer=True, positive=True)\n    i, j = symbols(\"i j\", integer=True, nonnegative=True)\n\n    expr = W*X*Z\n    assert expr._entry(i, j) == Sum(W[i, i1]*X[i1, i2]*Z[i2, j], (i1, 0, l-1), (i2, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr._entry(i, j)) == expr\n\n    expr = Z.T*X.T*W.T\n    assert expr._entry(i, j) == Sum(W[j, i2]*X[i2, i1]*Z[i1, i], (i1, 0, m-1), (i2, 0, l-1))\n    assert MatrixExpr.from_index_summation(expr._entry(i, j), i) == expr\n\n    expr = W*X*Z + W*Y*Z\n    assert expr._entry(i, j) == Sum(W[i, i1]*X[i1, i2]*Z[i2, j], (i1, 0, l-1), (i2, 0, m-1)) + Sum(W[i, i1]*Y[i1, i2]*Z[i2, j], (i1, 0, l-1), (i2, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr._entry(i, j)) == expr\n\n    expr = 2*W*X*Z + 3*W*Y*Z\n    assert expr._entry(i, j) == 2*Sum(W[i, i1]*X[i1, i2]*Z[i2, j], (i1, 0, l-1), (i2, 0, m-1)) + 3*Sum(W[i, i1]*Y[i1, i2]*Z[i2, j],", "def test_matrix_expression_multiplication():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n\n    expr = A * B\n    assert MatrixExpr.from_index_summation(expr[0, 0]) == (A * B)[0, 0]\n\n    expr = A * B * C\n    assert MatrixExpr.from_index_summation(expr[0, 0]) == (A * B * C)[0, 0]\n\n    expr = A * (B + C)\n    assert MatrixExpr.from_index_summation(expr[0, 0]) == (A * (B + C))[0, 0]\n\n    expr = (A + B) * C\n    assert MatrixExpr.from_index_summation(expr[0, 0]) == ((A + B) * C)[0, 0]\n", "def test_block_matrix_properties():\n    I = Identity(3)\n    Z = ZeroMatrix(3, 3)\n    B = BlockMatrix([[I, Z], [Z, I]])\n    assert B.is_Identity == True\n    assert B.is_structurally_symmetric == True\n\n    B = BlockMatrix([[I, I], [I, I]])\n    assert B.is_Identity == False\n    assert B.is_structurally_symmetric == True\n\n    B = BlockMatrix([[I, Z], [Z, Z]])\n    assert B.is_Identity == False\n    assert B.is_structurally_symmetric == False\n", "def test_matrix_expression_from_index_summation_irregular():\n    from sympy.abc import a, b, c, d, e, f\n    A = MatrixSymbol(\"A\", k, l)\n    B = MatrixSymbol(\"B\", l, m)\n    C = MatrixSymbol(\"C\", m, n)\n    D = MatrixSymbol(\"D\", k, n)\n\n    expr = Sum(A[a, b]*B[b, c]*C[c, d], (b, 0, l-1), (c, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr, a) == A*B*C\n\n    expr = Sum(A[a, b]*B[b, d], (b, 0, l-1))\n    assert MatrixExpr.from_index_summation(expr, a) == A*B\n\n    expr = Sum(A[a, b]*B[c, b]*D[a, c], (b, 0, l-1))\n    assert MatrixExpr.from_index_summation(expr, a) == A*B.T*D\n\n    expr = Sum(A[a, b]*B[c, b]*D[e, f], (b, 0, l-1))\n    raises(ValueError, lambda: MatrixExpr.from_index_summation(expr, a))\n"], "sample_175": ["def test_fast_delete_m2m_revm2m_combined(self):\n    t = M2MTo.objects.create()\n    f = M2MFrom.objects.create()\n    f.m2m.add(t)\n    # 1 to delete f, 1 to fast-delete m2m for f and reverse m2m for t\n    self.assertNumQueries(2, f.delete)\n    self.assertFalse(M2MTo.objects.exists())\n    self.assertFalse(M2MFrom.m2m.through.objects.exists())\n", "def test_fast_delete_with_signals(self):\n        pass\n\n    models.signals.pre_delete.connect(receiver, sender=User)\n    a = Avatar.objects.create()\n    User.objects.create(avatar=a)\n    # User can be fast-deleted, but there is a signal, so it shouldn't be\n    collector = Collector(using='default')\n    self.assertFalse(collector.can_fast_delete(a))\n    a.delete()\n    self.assertIsNone(a.pk)\n    self.assertFalse(User.objects.exists())\n    models.signals.pre_delete.disconnect(receiver, sender=User)\n", "def test_fast_delete_signals(self):\n    \"\"\"\n    Fast delete should not occur if signals are connected to the model.\n    \"\"\"\n        pass\n\n    u = User.objects.create(avatar=Avatar.objects.create())\n    models.signals.pre_delete.connect(receiver, sender=User)\n    with self.assertNumQueries(3):  # select, update, delete\n        u.avatar.delete()\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    models.signals.pre_delete.disconnect(receiver, sender=User)\n", "def test_fast_delete_signal(self):\n        pass\n\n    models.signals.pre_delete.connect(pre_delete_handler, sender=User)\n    u = User.objects.create(\n        avatar=Avatar.objects.create()\n    )\n    a = Avatar.objects.get(pk=u.avatar_id)\n    # 1 query to delete the user (signal handler is connected, so fast-delete is not possible)\n    # 1 query to delete the avatar\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    models.signals.pre_delete.disconnect(pre_delete_handler, sender=User)\n", "def test_fast_delete_gfk(self):\n    delete_top = DeleteTop.objects.create()\n    generic_delete_bottom = GenericDeleteBottom.objects.create(\n        generic_b1=GenericB1.objects.create(generic_delete_top=delete_top),\n        generic_b2=GenericB2.objects.create(generic_delete_top=delete_top),\n    )\n    # 1 to delete delete_top, 1 to fast-delete GenericB1 and GenericB2,\n    # 1 to fast-delete GenericDeleteBottom\n    self.assertNumQueries(3, delete_top.delete)\n    self.assertFalse(GenericDeleteBottom.objects.exists())\n    self.assertFalse(GenericB1.objects.exists())\n    self.assertFalse(GenericB2.objects.exists())\n    self.assertFalse(DeleteTop.objects.exists())\n"], "sample_864": ["def test_mean_shift_sparse_input():\n    # Test MeanShift algorithm with sparse input\n    X_sparse = sparse.csr_matrix(X)\n    bandwidth = 1.2\n    ms = MeanShift(bandwidth=bandwidth)\n    labels = ms.fit_predict(X_sparse)\n    assert len(np.unique(labels)) == n_clusters\n", "def test_mean_shift_bin_seeding():\n    # Test MeanShift algorithm with bin_seeding=True\n    ms = MeanShift(bandwidth=1.2, bin_seeding=True)\n    labels = ms.fit(X).labels_\n    labels_unique = np.unique(labels)\n    n_clusters_ = len(labels_unique)\n    assert n_clusters_ == 3  # We expect 3 clusters as in the original data\n    assert labels_unique[0] == 0  # The first cluster label should be 0\n", "def test_mean_shift_sparse_matrix():\n    # Test MeanShift algorithm with sparse matrix\n    X = sparse.lil_matrix((1000, 1000))\n    ms = MeanShift(bandwidth=2)\n    msg = \"A sparse matrix was passed, but dense data is required.\"\n    assert_raise_message(TypeError, msg, ms.fit, X)\n", "def test_mean_shift_with_seeds():\n    # Test MeanShift algorithm with provided seeds\n    seeds = np.array([[0, 0], [10, 10]])\n    ms = MeanShift(bandwidth=1.2, seeds=seeds)\n    labels = ms.fit(X).labels_\n    labels_unique = np.unique(labels)\n    n_clusters_ = len(labels_unique)\n    assert n_clusters_ == 3\n", "def test_meanshift_bin_seeding():\n    # Test MeanShift with bin_seeding\n    ms = MeanShift(bandwidth=1.2, bin_seeding=True)\n    labels = ms.fit_predict(X)\n    labels_unique = np.unique(labels)\n    n_clusters_ = len(labels_unique)\n    assert n_clusters_ == 3  # as we have 3 clusters in the data\n    assert labels_unique[0] == 0  # labels should start from 0\n"], "sample_82": ["def test_id_for_label(self):\n    self.assertEqual(self.widget.id_for_label('date'), 'date_month')\n    self.assertEqual(self.widget.id_for_label('date', 3), 'date_3')\n", "def test_selectdate_use_required_attribute(self):\n    class GetNotRequiredDate(Form):\n        mydate = DateField(widget=SelectDateWidget, required=False)\n\n    class GetRequiredDate(Form):\n        mydate = DateField(widget=SelectDateWidget, required=True)\n\n    # Test if 'required' attribute is not present in the output when the field is not required\n    self.assertNotIn('required', GetNotRequiredDate().as_p())\n\n    # Test if 'required' attribute is present in the output when the field is required\n    self.assertIn('required', GetRequiredDate().as_p())\n", "def test_id_for_label(self):\n    self.assertEqual(self.widget.id_for_label('mydate'), 'mydate_month')\n    self.assertEqual(self.widget.id_for_label('mydate', 0), 'mydate_month_0')\n", "def test_value_from_datadict_l10n(self):\n    widget = SelectDateWidget(years=('2007', '2008', '2009', '2010', '2011', '2012'))\n    with override_settings(USE_L10N=True), translation.override('nl'):\n        data = {'mydate_year': '2010', 'mydate_month': '8', 'mydate_day': '13'}\n        self.assertEqual(widget.value_from_datadict(data, {}, 'mydate'), '13-08-2010')\n", "def test_l10n_french(self):\n    w = SelectDateWidget(\n        years=('2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016')\n    )\n    self.assertEqual(\n        w.value_from_datadict({'date_year': '2010', 'date_month': '8', 'date_day': '13'}, {}, 'date'),\n        '13/08/2010',\n    )\n"], "sample_270": ["def test_unique_constraint_include_pointing_to_joined_fields(self):\n    class Model(models.Model):\n        age = models.SmallIntegerField()\n        parent = models.ForeignKey('self', models.CASCADE)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    name='name',\n                    fields=['age'],\n                    include=['parent__age'],\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"'constraints' refers to the joined field 'parent__age'.\",\n            obj=Model,\n            id='models.E041',\n        )\n    ])\n", "def test_unique_constraint_include_pointing_to_joined_fields(self):\n    class Target(models.Model):\n        field1 = models.IntegerField()\n\n    class Model(models.Model):\n        fk = models.ForeignKey(Target, models.CASCADE)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['fk__field1'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"'constraints' refers to the joined field 'fk__field1'.\",\n            obj=Model,\n            id='models.E041',\n        ),\n    ])\n", "def test_unique_constraint_with_include_pointing_to_missing_field(self):\n    class Model(models.Model):\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['id'],\n                    include=['missing_field'],\n                    name='name',\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"'constraints' refers to the nonexistent field 'missing_field'.\",\n            obj=Model,\n            id='models.E012',\n        ),\n    ])\n", "def test_unique_constraint_include_pointing_to_joined_fields(self):\n    class Model(models.Model):\n        age = models.SmallIntegerField()\n        parent = models.ForeignKey('self', models.CASCADE)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    name='name',\n                    fields=['age'],\n                    include=['parent__age'],\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"'constraints' refers to the joined field 'parent__age'.\",\n            obj=Model,\n            id='models.E041',\n        )\n    ])\n", "def test_unique_constraint_include_pointing_to_joined_fields(self):\n    class Model(models.Model):\n        age = models.SmallIntegerField()\n        parent = models.ForeignKey('self', models.CASCADE)\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    name='name',\n                    fields=['age'],\n                    include=['parent__age'],\n                ),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"'constraints' refers to the joined field 'parent__age'.\",\n            obj=Model,\n            id='models.E041',\n        ),\n    ])\n"], "sample_77": ["def test_urlize_nofollow(self):\n    value = 'Check out <a href=\"http://example.com\">this link</a>.'\n    output = 'Check out <a href=\"http://example.com\" rel=\"nofollow\">this link</a>.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_email_with_idn(self):\n    value = 'foo@\u00f6\u00e4\u00fc.com'\n    output = '<a href=\"mailto:foo@xn--4ca9at.com\">foo@\u00f6\u00e4\u00fc.com</a>'\n    self.assertEqual(urlize(value), output)\n", "def test_urlize_nofollow(self):\n    tests = (\n        ('google.com', '<a href=\"http://google.com\" rel=\"nofollow\">google.com</a>'),\n        ('http://google.com', '<a href=\"http://google.com\" rel=\"nofollow\">http://google.com</a>'),\n        ('www.google.com', '<a href=\"http://www.google.com\" rel=\"nofollow\">www.google.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_noescape(self):\n    text = '<script>alert(\"Hello, world!\");</script>'\n    output = '<a href=\"http://alert(&quot;Hello,%20world!&quot;);\">alert(&quot;Hello, world!&quot;);</a>'\n    self.assertEqual(urlize(text, autoescape=True), output)\n    self.assertEqual(urlize(text, autoescape=False), text)\n", "def test_urlize_wrapping_punctuation(self):\n    tests = (\n        ('(www.example.com)', '<a href=\"http://www.example.com\">(www.example.com)</a>'),\n        ('[www.example.com]', '<a href=\"http://www.example.com\">[www.example.com]</a>'),\n        ('(www.example.com]', '<a href=\"http://www.example.com\">(www.example.com)</a>]'),\n        ('[www.example.com)', '<a href=\"http://www.example.com\">[www.example.com)</a>'),\n        ('(www.example.com] and (www.example2.com)',\n         '<a href=\"http://www.example.com\">(www.example.com)</a>] and <a href=\"http://www.example2.com\">(www.example2.com)</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_352": ["    def test_ticket_24622(self):\n        \"\"\"\n        Make sure __in and __pk__in work the same for related fields.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        b1 = Ticket23605B.objects.create(modela_fk=a1, field_b0=123, field_b1=True, modelc_fk=c1)\n        b2 = Ticket23605B.objects.create(modela_fk=a2, field_b0=567, field_b1=True, modelc_fk=c1)\n        qx = Q(ticket23605b__pk__in=[b1.pk])\n        qy = Q(ticket23605b__in=[b1])\n        self.assertEqual(\n            set(Ticket23605A.objects.filter(qx).values_list('pk', flat=True)),\n            set(Ticket23605A.objects.filter(qy).values_list('pk', flat=True))\n        )\n        self.assertSequenceEqual(Ticket23605A.objects.filter(qx), [a1])\n", "    def test_ticket_23622_extra(self):\n        \"\"\"\n        Make sure extra query works with distinct on subquery.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        qs = Ticket23605A.objects.filter(\n            ticket23605b__in=Ticket23605B.objects.order_by('modela_fk', '-field_b1').distinct('modela_fk')\n        ).extra(select={'test': '1'})\n        self.assertEqual(len(qs), 2)\n", "    def test_ticket_24911(self):\n        \"\"\"\n        Ensure that nested annotation can be used in exclude clause.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        qs = Ticket23605A.objects.annotate(\n            max_b0=Max('ticket23605b__field_b0')\n        ).exclude(\n            max_b0__lt=300\n        )\n        self.assertSequenceEqual(qs, [a1])\n", "    def test_ticket_23735(self):\n        \"\"\"\n        Make sure the right table alias is used for filtering when combining\n        filters on the same relation.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=False,\n            modelc_fk=c1,\n        )\n        qx = (\n            Q(ticket23605b__field_b1=True) &\n            Q(ticket23605b__field_b0__gte=200)\n        )\n        self.assertEqual(\n            set(Ticket23605A.objects.filter(qx).values_list('pk', flat=True)),\n            {a1.pk}\n        )\n", "    def test_ticket_24816(self):\n        \"\"\"\n        Ensure foreign key related lookups work correctly when using\n        a queryset with a 'limit' clause.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        qs = Ticket23605A.objects.filter(\n            ticket23605b__in=Ticket23605B.objects.filter(\n                modelc_fk=c1,\n            ).values('modela_fk')[:1],\n        )\n        self.assertSequenceEqual(qs, [a1])\n"], "sample_840": ["def test_pls_inconsistent_length():\n    # Test that an error is raised when X and Y have inconsistent lengths\n    X = np.array([[0., 0., 1.],\n                   [1., 0., 0.],\n                   [2., 2., 2.]])\n    Y = np.array([[0.1, -0.2],\n                   [0.9, 1.1]])\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        with pytest.raises(ValueError):\n            clf.fit(X, Y)\n", "def test_pls_with_one_component():\n    # Test with only one component\n    X = np.array([[0., 0., 1.],\n                  [1., 0., 0.],\n                  [2., 2., 2.],\n                  [3., 5., 4.]])\n    Y = np.array([[0.1, -0.2],\n                  [0.9, 1.1],\n                  [6.2, 5.9],\n                  [11.9, 12.3]])\n    n_components = 1\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        clf.n_components = n_components\n        clf.fit(X, Y)\n        assert clf.x_weights_.shape[1] == n_components\n        assert clf.y_weights_.shape[1] == n_components\n        assert clf.x_loadings_.shape[1] == n_components\n        assert clf.y_loadings_.shape[1] == n_components\n        assert clf.x_scores_.shape[1] == n_components\n        assert clf.y_scores_.shape[1] == n_components\n", "def test_pls_regression_rank_deficient():\n    # Test PLSRegression when Y is rank-deficient\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    Y = np.array([[1, 1], [1, 1], [1, 1]])  # Rank-deficient matrix\n\n    pls = pls_.PLSRegression(n_components=2)\n    with pytest.warns(UserWarning, match=\"Y residual constant\"):\n        pls.fit(X, Y)\n", "def test_pls_input_validation():\n    # Test input validation for PLSCanonical and PLSRegression\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[1], [2], [3]])\n\n    # Test invalid number of components\n    with pytest.raises(ValueError, match=\"Invalid number of components\"):\n        pls_.PLSCanonical(n_components=3).fit(X, Y)\n        pls_.PLSRegression(n_components=3).fit(X, Y)\n\n    # Test invalid algorithm\n    with pytest.raises(ValueError, match=\"Got algorithm unknown\"):\n        pls_.PLSCanonical(algorithm=\"unknown\").fit(X, Y)\n\n    # Test invalid deflation mode\n    with pytest.raises(ValueError, match=\"The deflation mode is unknown\"):\n        pls_.PLSRegression(deflation_mode=\"unknown\").fit(X, Y)\n\n    # Test incompatible configuration: mode B is not implemented with svd algorithm\n    with pytest.raises(ValueError, match=\"Incompatible configuration: mode B\"):\n        pls_.PLSCanonical(algorithm=\"svd\", mode=\"B\").fit(X, Y)\n", "def test_pls_regression_scores():\n    # Test the scores calculated by PLSRegression\n    X = np.array([[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls_2 = pls_.PLSRegression(n_components=2)\n    pls_2.fit(X, Y)\n\n    # Expected scores calculated manually\n    expected_x_scores = np.array([[-0.47220244, -0.06857476],\n                                  [-0.07987921, -0.10440226],\n                                  [1.55208165, 0.83845702],\n                                  [1.35998989, 1.14376828]])\n    expected_y_scores = np.array([[-0.20351702, -0.41088705],\n                                  [1.62596058, 1.07345095],\n                                  [7.05812981, 6.41756157],\n                                  [13.28263204, 12.62187261]])\n\n    assert_array_almost_equal(pls_2.x_scores_, expected_x_scores, decimal=4)\n    assert_array_almost_equal(pls_2.y_scores_, expected_y_scores, decimal=4)\n"], "sample_968": ["def test_python_use_unqualified_type_names_in_signatures(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\"><span class=\"pre\">Name</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert '<p><strong>name</strong> (<em>Name</em>) \u2013 blah blah</p>' in content\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n", "def test_python_module_index_entry_without_synopsis(app):\n    text = \".. py:module:: sphinx\\n\"\n    doctree = restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )\n", "def test_python_use_unqualified_type_names_with_objects(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Class\" title=\"foo.Class\">'\n            '<span class=\"pre\">Class</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Method</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">foo.Attribute</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">foo.Property</span></span>' in content\n    assert ('<p><strong>cls</strong> (<a class=\"reference internal\" href=\"#foo.Class\" title=\"foo.Class\"><em>Class</em></a>) '\n            '\u2013 blah blah</p>' in content)\n    assert '<p><strong>meth</strong> (<em>foo.Method</em>) \u2013 blah blah</p>' in content\n    assert '<p><strong>attr</strong> (<em>foo.Attribute</em>) \u2013 blah blah</p>' in content\n    assert '<p><strong>prop</strong> (<em>foo.Property</em>) \u2013 blah blah</p>' in content\n", "def test_python_module_index_entry(app, status, warning):\n    text = (\".. py:module:: docutils\\n\"\n            \"   :synopsis: A synopsis for docutils.\\n\"\n            \"   :platform: Unix, Windows\\n\"\n            \"   :deprecated:\\n\")\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    assert 'docutils' in domain.modules\n    assert domain.modules['docutils'] == ('index', 'module-docutils', 'A synopsis for docutils.', 'Unix, Windows', True)\n    assert 'docutils' in domain.data['objects']\n    assert domain.data['objects']['docutils'] == ('index', 'module-docutils', 'module', False)\n", "def test_pyattribute_with_union_type_operator(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str] | int\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][1][1][0][1][2], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Optional\")\n    assert_node(doctree[1][1][1][0][1][4], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n    assert_node(doctree[1][1][1][0][1][6], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n"], "sample_791": ["def test_one_hot_encoder_drop_error_on_unknown():\n    enc = OneHotEncoder(handle_unknown='error', drop='first')\n    X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n    X_test = [['ghi', 1, 55]]\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, X_test)\n", "def test_one_hot_encoder_with_drop_and_fit_transform():\n    X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n    enc = OneHotEncoder(drop='first')\n    X_tr = enc.fit_transform(X)\n    exp = np.array([[0, 1], [1, 0], [1, 1]], dtype='int64')\n    assert_array_equal(X_tr.toarray(), exp)\n    assert_array_equal(enc.drop_idx_, np.array([0], dtype=np.int_))\n", "def test_one_hot_encoder_drop_first_with_unseen_categories():\n    enc = OneHotEncoder(drop='first')\n    X_train = [['abc', 2, 55], ['def', 1, 55], ['ghi', 3, 59]]\n    X_test = [['jkl', 4, 60]]\n    enc.fit(X_train)\n    enc.transform(X_test)\n    assert_array_equal(np.array(X_test, dtype=object),\n                       enc.inverse_transform(enc.transform(X_test)))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n    X_trans = enc.fit_transform(X).toarray()\n    expected_trans = [[0, 1, 1], [1, 0, 1], [0, 1, 0]]\n    assert_array_equal(X_trans, expected_trans)\n    assert_array_equal(enc.categories_, [np.array(['abc', 'def']), np.array([2, 1, 3]), np.array([55])])\n    assert_array_equal(enc.drop_idx_, np.array([0, 0, 0]))\n    X_inv = enc.inverse_transform(X_trans)\n    expected_inv = np.array(X, dtype=object)\n    assert_array_equal(X_inv, expected_inv)\n", "def test_one_hot_encoder_drop_categories(X, drop, cat_exp):\n    enc = OneHotEncoder(drop=drop)\n    enc.fit(X)\n    assert enc.categories_ == cat_exp\n"], "sample_597": ["def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n    ds2 = xr.Dataset({\"y\": 2, \"z\": 3})\n    expected = xr.Dataset({\"x\": 0, \"y\": 2, \"z\": 3})\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=\"y\"))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"y\"]))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"y\", \"z\"]))\n", "def test_merge_override(self):\n    ds1 = xr.Dataset({\"x\": (\"y\", [0, 0])})\n    ds2 = xr.Dataset({\"x\": (\"y\", [1, 1])})\n    expected = xr.Dataset({\"x\": (\"y\", [0, 0])})\n    actual = ds1.merge(ds2, compat=\"override\")\n    assert expected.identical(actual)\n\n    ds1 = xr.Dataset({\"x\": (\"y\", [0, 0])})\n    ds2 = xr.Dataset({\"x\": (\"z\", [1, 1])})\n    expected = xr.Dataset({\"x\": (\"y\", [0, 0]), \"y\": (\"y\", [0, 0]), \"z\": (\"z\", [1, 1])})\n    actual = ds1.merge(ds2, compat=\"override\")\n    assert expected.identical(actual)\n", "def test_merge_dataarray_with_coord(self):\n    ds = xr.Dataset({\"a\": 0}, coords={\"x\": (\"y\", [0, 1])})\n    da = xr.DataArray(data=1, name=\"b\", coords={\"y\": [0, 1]})\n\n    assert_identical(ds.merge(da), xr.merge([ds, da]))\n", "def test_merge_dataarray_with_coords(self):\n    da1 = xr.DataArray([1, 2, 3], coords={\"x\": [\"a\", \"b\", \"c\"]}, dims=\"x\", name=\"data\")\n    da2 = xr.DataArray([4, 5, 6], coords={\"y\": [\"d\", \"e\", \"f\"]}, dims=\"y\", name=\"data\")\n    result = xr.merge([da1, da2], join=\"outer\")\n    expected = xr.Dataset(\n        data_vars={\n            \"data\": (\n                (\"x\", \"y\"),\n                np.array([[1, 4], [2, 5], [3, 6], [np.nan, np.nan]]),\n            )\n        },\n        coords={\"x\": [\"a\", \"b\", \"c\", np.nan], \"y\": [\"d\", \"e\", \"f\"]},\n    )\n    xr.testing.assert_equal(result, expected)\n", "def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"a\": 1, \"b\": 2})\n    ds2 = xr.Dataset({\"b\": 3, \"c\": 4})\n    expected = xr.Dataset({\"a\": 1, \"b\": 3, \"c\": 4})\n\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=\"b\"))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"b\"]))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=(\"b\",)))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=set([\"b\"])))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars={\"b\"}))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=(\"b\", \"c\")))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"b\", \"c\"]))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=set([\"b\", \"c\"])))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars={\"b\", \"c\"}))\n\n    with pytest.raises(ValueError):\n        ds1.merge(ds2, overwrite_vars=5)\n"], "sample_1010": ["def test_TensorProduct_latex_printing():\n    from sympy.tensor.array import tensorproduct\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    tp = tensorproduct(A, B)\n    assert latex(tp) == r\"\\left[A\\right] \\otimes \\left[B\\right]\"\n", "def test_issue_14237_pow():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(A**2) == r\"A^{2}\"\n    assert latex(A**-1) == r\"A^{-1}\"\n    assert latex((-A)**2) == r\"\\left(-A\\right)^{2}\"\n", "def test_TensorContract_printing():\n    from sympy.tensor.functions import TensorContract\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert latex(TensorContract(A, (0, 1), B, (0, 1))) == r\"\\operatorname{contraction}_{0, 1}(A, B)\"\n", "def test_Issue13651_second_case():\n    expr = a + b + Mul(-1, c, evaluate=False)\n    assert latex(expr) == r\"a + b - c\"\n", "def test_issue_14865():\n    from sympy.parsing.sympy_parser import parse_expr\n    expr = parse_expr('I*I', evaluate=False)\n    assert latex(expr) == r\"I I\"\n"], "sample_812": ["def test_long_parameter_values():\n    # Test that long parameter values are correctly handled\n    long_string = 'a' * 1000\n    lr = LogisticRegression(penalty=long_string)\n    expected = f\"LogisticRegression(penalty='{long_string}')\"\n    assert lr.__repr__() == expected\n", "def test_param_grid_tuple():\n    # render a param_grid with tuples\n    param_grid = [{'kernel': ['rbf'], 'gamma': [(1e-3, 1e-4), 1e-5],\n                   'C': [1, 10, 100, 1000]},\n                  {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n    gs = GridSearchCV(SVC(), param_grid, cv=5)\n\n    expected = \"\"\"", "def test_ellipsis_for_long_strings():\n    # Test ellipsis for long strings\n    long_string = 'a' * 200\n    param_grid = {'custom_param': [long_string]}\n    gs = GridSearchCV(SVC(), param_grid)\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    repr_ = pp.pformat(gs)\n    assert '...' in repr_\n", "def test_n_max_elements_to_show_with_long_estimators():\n\n    n_max_elements_to_show = 2\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    # Test with an estimator that has more than n_max_elements_to_show parameters\n    lr = LogisticRegression(C=1, dual=True, tol=1e-3, fit_intercept=False, solver='liblinear', max_iter=1000)\n\n    expected = \"\"\"", "def test_nested_dicts():\n    # Render a deeply nested dictionary inside an estimator\n    pipeline = Pipeline([\n        ('feature_selection', SelectKBest(score_func=chi2)),\n        ('classification', LogisticRegression(C=999,\n                                              class_weight={0: 0.5, 1: 0.5}))\n    ])\n    expected = \"\"\""], "sample_770": ["def test_silhouette_sparse_input():\n    # Test silhouette score and samples with sparse input\n    X_csr = csr_matrix([[0, 0], [1, 1], [2, 2], [3, 3]])\n    labels = np.array([0, 0, 1, 1])\n\n    # Test silhouette score with sparse input\n    score_csr = silhouette_score(X_csr, labels, metric='euclidean')\n    pytest.approx(score_csr, 0.5)  # Expected score for this simple case\n\n    # Test silhouette samples with sparse input\n    samples_csr = silhouette_samples(X_csr, labels, metric='euclidean')\n    pytest.approx(samples_csr, [0.5, 0.5, 0.5, 0.5])  # Expected samples for this simple case\n", "def test_silhouette_sparse_matrix_equivalence():\n    # Assert that silhouette_score and silhouette_samples return the same\n    # results for dense and sparse input.\n    rng = np.random.RandomState(seed=0)\n    X_dense = rng.rand(10, 5)\n    X_csr = csr_matrix(X_dense)\n    labels = rng.randint(0, 2, size=10)\n\n    score_dense = silhouette_score(X_dense, labels)\n    score_csr = silhouette_score(X_csr, labels)\n    pytest.approx(score_dense, score_csr)\n\n    samples_dense = silhouette_samples(X_dense, labels)\n    samples_csr = silhouette_samples(X_csr, labels)\n    assert_array_equal(samples_dense, samples_csr)\n", "def test_silhouette_sparse_matrix():\n    # Test silhouette score and samples with sparse matrix input\n    from sklearn.utils.testing import assert_array_equal\n    from sklearn.utils.testing import assert_equal\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix([[0, 0], [1, 1], [2, 2]])\n    labels = [0, 1, 1]\n\n    score_sparse = silhouette_score(X_sparse, labels, metric='euclidean')\n    score_dense = silhouette_score(X_sparse.toarray(), labels, metric='euclidean')\n\n    assert_equal(score_sparse, score_dense)\n\n    samples_sparse = silhouette_samples(X_sparse, labels, metric='euclidean')\n    samples_dense = silhouette_samples(X_sparse.toarray(), labels, metric='euclidean')\n\n    assert_array_equal(samples_sparse, samples_dense)\n", "def test_silhouette_sparse_inputs():\n    # Test silhouette score and samples with sparse inputs\n    rng = np.random.RandomState(0)\n    X_sparse = sp.csr_matrix(rng.rand(10, 5))\n    labels = np.array([0] * 5 + [1] * 5)\n\n    score_sparse = silhouette_score(X_sparse, labels)\n    score_dense = silhouette_score(X_sparse.toarray(), labels)\n    pytest.approx(score_sparse, score_dense)\n\n    samples_sparse = silhouette_samples(X_sparse, labels)\n    samples_dense = silhouette_samples(X_sparse.toarray(), labels)\n    assert_array_equal(samples_sparse, samples_dense)\n", "def test_silhouette_with_sparse_input():\n    # Test that silhouette_score and silhouette_samples accept sparse inputs\n    rng = np.random.RandomState(42)\n    X_sparse = csr_matrix(rng.rand(50, 10))\n    labels = rng.randint(0, 2, size=50)\n\n    score_sparse = silhouette_score(X_sparse, labels)\n    score_dense = silhouette_score(X_sparse.toarray(), labels)\n    pytest.approx(score_sparse, score_dense)\n\n    samples_sparse = silhouette_samples(X_sparse, labels)\n    samples_dense = silhouette_samples(X_sparse.toarray(), labels)\n    assert_array_equal(samples_sparse, samples_dense)\n"], "sample_413": ["def test_template_tags_with_no_libraries(self):\n    with self.settings(\n        TEMPLATES=[\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"OPTIONS\": {},\n            },\n        ]\n    ):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "def test_template_tags_with_different_module_path(self):\n    with self.settings(\n        TEMPLATES=[\n            self.get_settings(\n                \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n            ),\n            self.get_settings(\n                \"same_tags\", \"same_tags_app_2.templatetags.different_tags\"\n            ),\n        ]\n    ):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_template_tags_with_different_path(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"different_tags_app.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def get_settings(module_name, module_path):\n        return {\n            \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n            \"OPTIONS\": {\n                \"libraries\": {\n                    module_name: f\"check_framework.template_test_apps.{module_path}\",\n                },\n            },\n        }\n", "    def get_settings(module_name, module_path):\n        return {\n            \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n            \"OPTIONS\": {\n                \"libraries\": {\n                    module_name: f\"check_framework.template_test_apps.{module_path}\",\n                },\n            },\n        }\n"], "sample_1203": ["def test_group_isomorphism():\n    # Test isomorphism between free group and permutation group\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**2, (a*b)**3])\n    H = PermutationGroup([Permutation(1, 2, 3), Permutation(2, 3, 4)])\n    assert is_isomorphic(G, H)\n\n    # Test isomorphism between two free groups with same generators and relators\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**2, b**3, (a*b)**2])\n    H = FpGroup(F, [a**2, b**3, (a*b)**2])\n    assert is_isomorphic(G, H)\n\n    # Test non-isomorphism between two free groups with different relators\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**2, b**3, (a*b)**2])\n    H = FpGroup(F, [a**2, b**2, (a*b)**3])\n    assert not is_isomorphic(G, H)\n", "def test_composition_and_restriction():\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    P = PermutationGroup(p)\n    T = homomorphism(P, D, [p], [p])\n    H = PermutationGroup([p**2])\n    T_restricted = T.restrict_to(H)\n    assert T_restricted.domain == H\n    assert T_restricted.codomain == D\n    assert T_restricted(p**2) == T(p**2)\n\n    T2 = homomorphism(D, P, D.generators, P.generators)\n    T_composed = T.compose(T2)\n    assert T_composed.domain == D\n    assert T_composed.codomain == D\n    assert T_composed(D.generators[0]) == D.generators[0]\n    assert T_composed(D.generators[1]) == D.generators[1]\n", "def test_kernel_computation():\n    # PermutationGroup -> FpGroup\n    D = DihedralGroup(8)\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**4, b**2, (a*b)**2])\n    T = homomorphism(D, G, D.generators, [a, b])\n    K = T.kernel()\n    assert K.order() == 2\n    assert K.is_subgroup(D)\n\n    # Check that the elements of the kernel are indeed mapped to the identity\n    for k in K.generators:\n        assert T(k).is_identity\n\n    # Check that the image order times the kernel order equals the domain order\n    assert T.image().order() * K.order() == D.order()\n", "def test_group_homomorphism_methods():\n    # PermutationGroup -> FpGroup\n    # Check that the homomorphism is not surjective\n    p = Permutation(0, 1, 2, 3)\n    P = PermutationGroup([p])\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**4])\n    T = homomorphism(P, G, [p], [a])\n    assert not T.is_surjective()\n\n    # Check that the homomorphism is not injective\n    p1 = Permutation(0, 1)\n    p2 = Permutation(2, 3)\n    P = PermutationGroup([p1, p2])\n    G = CyclicGroup(2)\n    T = homomorphism(P, G, [p1], [G.generators[0]])\n    assert not T.is_injective()\n\n    # Check that the homomorphism is not an isomorphism\n    p1 = Permutation(0, 1, 2)\n    p2 = Permutation(1, 2, 3)\n    P = PermutationGroup([p1, p2])\n    G = AlternatingGroup(4)\n    T = homomorphism(P, G, [p1, p2], G.generators)\n    assert not T.is_isomorphism()\n", "def test_group_isomorphism():\n    # FpGroup -> FpGroup\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = FpGroup(F, [b**3, a**2, (a*b)**2])\n    assert not is_isomorphic(G, H)\n\n    # PermutationGroup -> FpGroup\n    P = PermutationGroup([Permutation(0, 1, 2), Permutation(0, 2)])\n    G = FpGroup(F, [a**3, b**2, (a*b)**3])\n    check, T = group_isomorphism(P, G)\n    assert check\n    assert T(Permutation(0, 1, 2)) == a\n    assert T(Permutation(0, 2)) == b\n"], "sample_407": ["def test_add_then_set_after_prefetch(self):\n    c = City.objects.create(name=\"Musical City\")\n    District.objects.create(name=\"Ladida\", city=c)\n    d2 = District.objects.create(name=\"Ladidu\")\n    city = City.objects.prefetch_related(\"districts\").get(id=c.id)\n    self.assertEqual(city.districts.count(), 1)\n    city.districts.add(d2)\n    self.assertEqual(city.districts.count(), 2)\n    city.districts.set([d2])\n    self.assertSequenceEqual(city.districts.all(), [d2])\n", "def test_reverse_foreign_key_with_to_field_instance_caching(self):\n    parent = Parent.objects.create(name=\"a\")\n    ChildStringPrimaryKeyParent.objects.create(parent=parent)\n    child = parent.string_primary_key_children.get()\n    with self.assertNumQueries(0):\n        self.assertIs(child.parent, parent)\n", "def test_get_next_or_previous_by_FIELD(self):\n    # Create some articles with the same reporter\n    r = Reporter.objects.create(first_name=\"John\", last_name=\"Doe\", email=\"john.doe@example.com\")\n    a1 = Article.objects.create(headline=\"First Article\", pub_date=datetime.date(2005, 1, 1), reporter=r)\n    a2 = Article.objects.create(headline=\"Second Article\", pub_date=datetime.date(2005, 1, 2), reporter=r)\n    a3 = Article.objects.create(headline=\"Third Article\", pub_date=datetime.date(2005, 1, 3), reporter=r)\n\n    # Test get_next_by_pub_date\n    self.assertEqual(a1.get_next_by_pub_date(), a2)\n    self.assertEqual(a2.get_next_by_pub_date(), a3)\n    with self.assertRaises(Article.DoesNotExist):\n        a3.get_next_by_pub_date()\n\n    # Test get_previous_by_pub_date\n    self.assertEqual(a3.get_previous_by_pub_date(), a2)\n    self.assertEqual(a2.get_previous_by_pub_date(), a1)\n    with self.assertRaises(Article.DoesNotExist):\n        a1.get_previous_by_pub_date()\n", "def test_foreign_key_with_default_value(self):\n    default_category = Category.objects.create(name=\"Default\")\n    record_with_default = Record.objects.create()\n    self.assertEqual(record_with_default.category, default_category)\n\n    new_category = Category.objects.create(name=\"New\")\n    record_with_default.category = new_category\n    record_with_default.save()\n    self.assertEqual(record_with_default.category, new_category)\n", "def test_query_set_caching_after_prefetch(self):\n    c = City.objects.create(name=\"Musical City\")\n    d1 = District.objects.create(name=\"Ladida\", city=c)\n    d2 = District.objects.create(name=\"Ladidu\", city=c)\n    city = City.objects.prefetch_related(\"districts\").get(id=c.id)\n    districts = city.districts.all()\n    self.assertEqual(len(districts), 2)\n    # Simulate caching by storing the queryset in a local variable\n    cached_districts = districts\n    d3 = District.objects.create(name=\"New District\", city=c)\n    # The cached queryset should not include the newly created district\n    self.assertEqual(len(cached_districts), 2)\n"], "sample_117": ["    def test_widget_attrs(self):\n        field = UsernameField()\n        attrs = field.widget_attrs(forms.TextInput())\n        self.assertEqual(attrs['autocapitalize'], 'none')\n        self.assertEqual(attrs['autocomplete'], 'username')\n", "    def test_integer_username(self):\n        data = {\n            'username': 1234567,\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        u = form.save()\n        self.assertEqual(u.username, 1234567)\n", "def test_password_reset_form_validates_password(self):\n    user = User.objects.get(username='testclient')\n    data = {'email': user.email}\n    form = PasswordResetForm(data)\n    self.assertTrue(form.is_valid())\n    form.save(domain_override='example.com')\n    self.assertEqual(len(mail.outbox), 1)\n    reset_url = re.search(r'http://example.com/reset/(\\S+)/', mail.outbox[0].body).group(1)\n    uid, token = reset_url.split('/')[:2]\n    set_password_form = SetPasswordForm(user, {'new_password1': 'testclient', 'new_password2': 'testclient'})\n    self.assertFalse(set_password_form.is_valid())\n    self.assertEqual(len(set_password_form[\"new_password2\"].errors), 2)\n    self.assertIn('The password is too similar to the username.', set_password_form[\"new_password2\"].errors)\n    self.assertIn(\n        'This password is too short. It must contain at least 12 characters.',\n        set_password_form[\"new_password2\"].errors\n    )\n", "    def test_user_creation_form_uses_custom_user_model(self):\n        # The UserCreationForm should use the custom user model if specified.\n        class CustomUserCreationForm(UserCreationForm):\n            class Meta(UserCreationForm.Meta):\n                model = CustomUser\n\n        data = {\n            'username': 'testclient',\n            'password1': 'testclient',\n            'password2': 'testclient',\n            'email': 'testclient@example.com',\n            'date_of_birth': '1988-02-24',\n        }\n        form = CustomUserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertIsInstance(user, CustomUser)\n        self.assertEqual(user.username, 'testclient')\n        self.assertEqual(user.email, 'testclient@example.com')\n        self.assertEqual(user.date_of_birth, datetime.date(1988, 2, 24))\n", "    def test_select_related(self):\n        user = User.objects.get(username='testclient')\n        form = UserChangeForm(instance=user)\n        # Access the queryset to ensure it's cached.\n        user_permissions_qs = form.fields['user_permissions'].queryset\n        self.assertTrue(user_permissions_qs._prefetch_related_lookups)\n        self.assertEqual(user_permissions_qs._prefetch_related_lookups, {'permissions__content_type'})\n"], "sample_546": ["def test_toolmanager_add_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_len = len(fig.canvas.manager.toolmanager.tools)\n    assert 'custom_tool' not in fig.canvas.manager.toolmanager.tools\n    fig.canvas.manager.toolmanager.add_tool('custom_tool', None)\n    assert len(fig.canvas.manager.toolmanager.tools) == initial_len + 1\n    assert 'custom_tool' in fig.canvas.manager.toolmanager.tools\n", "def test_get_subplotspec():\n    fig = plt.figure()\n    gs = fig.add_gridspec(3, 3)\n    ax1 = fig.add_subplot(gs[0, :])\n    ax2 = fig.add_subplot(gs[1, :-1])\n    ax3 = fig.add_subplot(gs[1:, -1])\n    ax4 = fig.add_subplot(gs[-1, 0])\n    ax5 = fig.add_subplot(gs[-1, -2])\n\n    assert ax1._subplotspec == gs[0, :]\n    assert ax2._subplotspec == gs[1, :-1]\n    assert ax3._subplotspec == gs[1:, -1]\n    assert ax4._subplotspec == gs[-1, 0]\n    assert ax5._subplotspec == gs[-1, -2]\n", "def test_toolbar_trigger_tool(name):\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_mode = fig.canvas.manager.toolbar._active\n    fig.canvas.manager.toolmanager.trigger_tool(name)\n    if name == \"home\":\n        assert fig.canvas.manager.toolbar._active is None\n    else:\n        assert fig.canvas.manager.toolbar._active == name\n    fig.canvas.manager.toolbar._active = initial_mode\n", "def test_widgetlock_zoom_and_pan():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    fig.canvas.widgetlock(ax)\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    tb.pan()\n    assert ax.get_navigate_mode() is None\n", "def test_widgetlock_zoom():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    fig.canvas.widgetlock(ax)\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    assert ax.get_navigate_mode() is None\n"], "sample_296": ["def test_encode_decode_empty_messages(self):\n    \"\"\"\n    An empty list of messages is properly encoded/decoded by the custom JSON\n    encoder/decoder classes.\n    \"\"\"\n    messages = []\n    storage = self.get_storage()\n    encoded = storage._encode(messages, encode_empty=True)\n    decoded_messages = storage._decode(encoded)\n    self.assertEqual(messages, decoded_messages)\n", "def test_message_level(self):\n    \"\"\"\n    Messages with different levels are stored and retrieved correctly.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    messages = [\n        Message(constants.DEBUG, 'Debug message'),\n        Message(constants.INFO, 'Info message'),\n        Message(constants.SUCCESS, 'Success message'),\n        Message(constants.WARNING, 'Warning message'),\n        Message(constants.ERROR, 'Error message'),\n    ]\n    set_cookie_data(storage, messages)\n    retrieved_messages = list(storage)\n    self.assertEqual(len(retrieved_messages), len(messages))\n    for i in range(len(messages)):\n        self.assertEqual(retrieved_messages[i].level, messages[i].level)\n        self.assertEqual(retrieved_messages[i].message, messages[i].message)\n", "def test_empty_cookie(self):\n    \"\"\"\n    If no messages are added, no cookie should be set and no errors should occur.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertNotIn(CookieStorage.cookie_name, response.cookies)\n", "def test_empty_messages(self):\n    \"\"\"\n    Empty messages are encoded and decoded correctly.\n    \"\"\"\n    storage = self.get_storage()\n    messages = []\n    encoded = storage._encode(messages, encode_empty=True)\n    decoded = storage._decode(encoded)\n    self.assertEqual(decoded, messages)\n", "def test_empty_message_storage(self):\n    \"\"\"\n    Test that the _encode() method returns an empty string when the messages list is empty.\n    \"\"\"\n    storage = self.get_storage()\n    encoded_data = storage._encode([])\n    self.assertEqual(encoded_data, '')\n"], "sample_512": ["def test_pylab_integration_no_pylab():\n    pytest.importorskip(\"IPython\")\n    subprocess.run(\n        [\n            sys.executable,\n            \"-m\",\n            \"IPython\",\n            \"-c\",\n            \";\".join((\n                \"import matplotlib.pyplot as plt\",\n                \"assert plt._REPL_DISPLAYHOOK != plt._ReplDisplayHook.IPYTHON\",\n            )),\n        ],\n        env={**os.environ, \"SOURCE_DATE_EPOCH\": \"0\"},\n        timeout=60,\n        check=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True,\n    )\n", "def test_subplots_figsize():\n    fig, ax = plt.subplots(figsize=(6, 4))\n    assert fig.get_size_inches() == (6, 4)\n", "def test_subplot_mosaic_auto_sentinel():\n    fig, ax_dict = plt.subplot_mosaic('ABC;DEF;GHI')\n    for key in 'ABCDEFGHI':\n        assert key in ax_dict\n        assert isinstance(ax_dict[key], plt.Axes)\n    assert ax_dict['A'] is not ax_dict['B']\n    assert ax_dict['D'] is not ax_dict['E']\n    assert ax_dict['G'] is not ax_dict['H']\n    assert ax_dict['A'] is not ax_dict['D']\n    assert ax_dict['D'] is not ax_dict['G']\n    plt.close(fig)\n", "def test_empty_subplot_mosaic():\n    fig, ax_dict = plt.subplot_mosaic({})\n    assert not ax_dict\n", "def test_subplot_reuse_with_label():\n    # plt.subplot() searches for axes with the same subplot spec, and if one\n    # exists, and the kwargs match returns it, create a new one if they do not\n    fig = plt.figure()\n    ax = plt.subplot(1, 2, 1, label='ax1')\n    ax1 = plt.subplot(1, 2, 1, label='ax1')\n    ax2 = plt.subplot(1, 2, 2, label='ax2')\n    ax3 = plt.subplot(1, 2, 1, label='ax3')\n    assert ax is not None\n    assert ax1 is ax\n    assert ax2 is not ax\n    assert ax3 is not ax\n    assert ax3.get_label() == 'ax3'\n\n    assert ax not in fig.axes\n    assert ax2 in fig.axes\n    assert ax3 in fig.axes\n"], "sample_1": ["def test_separable_single_input():\n    # Test the case where the model has only one input but multiple outputs\n    compound_model = sh1 & sh2\n    assert_allclose(is_separable(compound_model), np.array([False, False]))\n    assert_allclose(separability_matrix(compound_model), np.ones((2, 1), dtype=np.bool_))\n", "def test_separable_single_output():\n    # Test the case where the model has only one output\n    result = is_separable(sh1)\n    assert_allclose(result, np.array([False]))\n    result = separability_matrix(sh1)\n    assert_allclose(result, np.array([[True]]))\n", "def test_separable_single_input_multiple_outputs():\n    model = sh1 | (sh2 & sh3)\n    assert_allclose(is_separable(model), np.array([True, True]))\n    assert_allclose(separability_matrix(model), np.array([[True, False, False], [True, False, False]]))\n", "def test_separable_single_input_model():\n    model = models.Shift(1) & models.Shift(2)\n    expected_is_separable = np.array([False, False])\n    expected_separability_matrix = np.ones((2, 1), dtype=np.bool_)\n    assert_allclose(is_separable(model), expected_is_separable)\n    assert_allclose(separability_matrix(model), expected_separability_matrix)\n", "def test_separable_custom_model_with_separability_matrix():\n    @custom_model\n        return x * y\n\n    model_d._calculate_separability_matrix = lambda: np.array([[False]])\n\n    assert not is_separable(model_d())\n    assert np.all(separability_matrix(model_d()) == np.array([[False]]))\n"], "sample_513": ["def test_legend_set_ncols():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n    leg.set_ncols(2)\n    assert leg._ncols == 2\n", "def test_legend_fontsize(fontsize):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(fontsize=fontsize)\n    assert leg.prop.get_size_in_points() == fontsize\n", "def test_legend_shadow_framealpha():\n    # Test that framealpha is set to 1 when shadow is True and framealpha is not provided\n    fig, ax = plt.subplots()\n    ax.plot(range(100), label=\"test\")\n    leg = ax.legend(shadow=True)\n    assert leg.get_frame().get_alpha() == 1\n", "def test_legend_rc_params():\n    # Test that rcParams are applied correctly\n    with mpl.rc_context(rc={'legend.fontsize': 12,\n                            'legend.title_fontsize': 14,\n                            'legend.borderpad': 1.5,\n                            'legend.labelspacing': 2.0,\n                            'legend.handlelength': 2.5,\n                            'legend.handleheight': 2.0,\n                            'legend.handletextpad': 1.0,\n                            'legend.borderaxespad': 2.0,\n                            'legend.columnspacing': 2.5}):\n        fig, ax = plt.subplots()\n        ax.plot(range(10), label='test')\n        leg = ax.legend(title=\"Aardvark\")\n        assert leg.prop.get_size() == 12\n        assert leg.get_title().get_size() == 14\n        assert leg.borderpad == 1.5\n        assert leg.labelspacing == 2.0\n        assert leg.handlelength == 2.5\n        assert leg.handleheight == 2.0\n        assert leg.handletextpad == 1.0\n        assert leg.borderaxespad == 2.0\n        assert leg.columnspacing == 2.5\n", "def test_legend_ncol_ncols_conflict():\n    # Test that both ncol and ncols raise an error\n    strings = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n    ncols = 3\n    with pytest.raises(TypeError):\n        plt.legend(strings, ncol=ncols, ncols=ncols)\n"], "sample_725": ["def test_check_X_y_complex_data_error():\n    X = np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]])\n    y = np.array([1, 2])\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # list of lists\n    X = [[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]]\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # tuple of tuples\n    X = ((1 + 2j, 3 + 4j, 5 + 7j), (2 + 3j, 4 + 5j, 6 + 7j))\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # list of np arrays\n    X = [np.array([1 + 2j, 3 + 4j, 5 + 7j]),\n         np.array([2 + 3j, 4 + 5j, 6 + 7j])]\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # tuple of np arrays\n    X = (np.array([1 + 2j, 3 + 4j, 5 + 7j]),\n         np.array([2 + 3j, 4 + 5j, 6 + 7j]))\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # dataframe\n    X = MockDataFrame(\n        np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + ", "def test_check_X_y_complex_data_error():\n    X = np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]])\n    y = np.array([1, 2])\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # list of lists\n    X = [[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]]\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # tuple of tuples\n    X = ((1 + 2j, 3 + 4j, 5 + 7j), (2 + 3j, 4 + 5j, 6 + 7j))\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # list of np arrays\n    X = [np.array([1 + 2j, 3 + 4j, 5 + 7j]),\n         np.array([2 + 3j, 4 + 5j, 6 + 7j])]\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # tuple of np arrays\n    X = (np.array([1 + 2j, 3 + 4j, 5 + 7j]),\n         np.array([2 + 3j, 4 + 5j, 6 + 7j]))\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # dataframe\n    X = MockDataFrame(\n        np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + ", "def test_check_X_y_complex_data_error():\n    X = np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]])\n    y = np.array([1, 2])\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # list of lists\n    X = [[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]]\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # tuple of tuples\n    X = ((1 + 2j, 3 + 4j, 5 + 7j), (2 + 3j, 4 + 5j, 6 + 7j))\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # list of np arrays\n    X = [np.array([1 + 2j, 3 + 4j, 5 + 7j]),\n         np.array([2 + 3j, 4 + 5j, 6 + 7j])]\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # tuple of np arrays\n    X = (np.array([1 + 2j, 3 + 4j, 5 + 7j]),\n         np.array([2 + 3j, 4 + 5j, 6 + 7j]))\n    assert_raises_regex(\n        ValueError, \"Complex data not supported\", check_X_y, X, y)\n\n    # dataframe\n    X = MockDataFrame(\n        np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + ", "def test_check_X_y_y_numeric():\n    y = [0, 1, '2']\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    with pytest.raises(ValueError):\n        check_X_y(X, y, y_numeric=True)\n\n    y_converted = check_X_y(X, y, y_numeric=False)[1]\n    assert_equal(y_converted, y)\n\n    y_numeric = np.array([0, 1, 2])\n    y_converted = check_X_y(X, y_numeric, y_numeric=True)[1]\n    assert_equal(y_converted.dtype, np.float64)\n", "def test_check_array_with_2d_sparse_matrix():\n    X_2d_sparse = sp.csr_matrix([[1, 0], [0, 1]])\n    X_checked = check_array(X_2d_sparse, accept_sparse=True)\n    assert_allclose_dense_sparse(X_checked, X_2d_sparse)\n"], "sample_181": ["def test_filtered_aggregate_ref_multiple_subquery_annotation_with_multiple_filters(self):\n    aggregate = Book.objects.values('publisher').annotate(\n        has_authors=Exists(\n            Book.authors.through.objects.filter(book=OuterRef('pk')),\n        ),\n        authors_have_other_books=Exists(\n            Book.objects.filter(\n                authors__in=Author.objects.filter(\n                    book_contact_set=OuterRef(OuterRef('pk')),\n                )\n            ).exclude(pk=OuterRef('pk')),\n        ),\n        has_pages_gt_500=Exists(\n            Book.objects.filter(pk=OuterRef('pk'), pages__gt=500),\n        ),\n    ).aggregate(\n        max_rating=Max(\n            'rating',\n            filter=Q(has_authors=True, authors_have_other_books=False, has_pages_gt_500=True),\n        )\n    )\n    self.assertEqual(aggregate, {'max_rating': 4.5})\n", "def test_filtered_aggregate_with_subquery(self):\n    subquery = Book.objects.filter(contact=OuterRef('pk')).values('price')\n    agg = Max('price', filter=Exists(subquery))\n    qs = Author.objects.annotate(max_price=agg).order_by('pk')\n    self.assertSequenceEqual([a.max_price for a in qs], [Decimal('30.00'), Decimal('23.09'), Decimal('29.69')])\n", "def test_filtered_aggregate_ref_annotation_with_annotation(self):\n    aggs = Author.objects.annotate(\n        double_age=F('age') * 2,\n        book_count=Count('book'),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(double_age__gt=F('book_count') * 100)),\n    )\n    self.assertEqual(aggs['cnt'], 1)\n", "def test_filtered_aggregate_with_annotation_in_filter(self):\n    q = Q(friends__book__pages=F('friends__book__publisher__num_awards') * 100)\n    agg = Sum('friends__book__rating', filter=q)\n    self.assertEqual(Author.objects.filter(name='test').aggregate(rating=agg)['rating'], 9.0)\n", "def test_filtered_aggregate_ref_multiple_subquery_annotation_multiple_filters(self):\n    aggregate = Book.objects.values('publisher').annotate(\n        has_authors=Exists(\n            Book.authors.through.objects.filter(book=OuterRef('pk')),\n        ),\n        authors_have_other_books=Exists(\n            Book.objects.filter(\n                authors__in=Author.objects.filter(\n                    book_contact_set=OuterRef(OuterRef('pk')),\n                )\n            ).exclude(pk=OuterRef('pk')),\n        ),\n    ).aggregate(\n        max_rating=Max(\n            'rating',\n            filter=Q(has_authors=True, authors_have_other_books=False, pages__gt=500),\n        )\n    )\n    self.assertEqual(aggregate, {'max_rating': 4.5})\n"], "sample_936": ["def test_stringify_type_hints_Generic():\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric) == \"test_util_typing.MyGeneric\"\n    assert stringify(MyGeneric[int]) == \"test_util_typing.MyGeneric[int]\"\n", "def test_stringify_type_hints_generic():\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric) == \"test_util_typing.MyGeneric\"\n    assert stringify(MyGeneric[int]) == \"test_util_typing.MyGeneric[int]\"\n", "def test_stringify_type_hints_Generic():\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric[int]) == \"test_util_typing.MyGeneric[int]\"\n    assert stringify(MyGeneric[str]) == \"test_util_typing.MyGeneric[str]\"\n", "def test_stringify_type_hints_Generic():\n    class GenericClass(Generic[T]):\n        pass\n\n    assert stringify(GenericClass[int]) == \"test_util_typing.GenericClass[int]\"\n", "def test_stringify_type_hints_Union_py36():\n    assert stringify(Union[int, Optional[str]]) == \"Optional[Union[int, str]]\"\n"], "sample_617": ["def test_cross_3d(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=dim)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_input_checks():\n    a = xr.DataArray([1, 2, 3])\n    b = xr.DataArray([4, 5, 6])\n    with pytest.raises(ValueError, match=\"dim must be provided\"):\n        xr.cross(a, b)\n    with pytest.raises(ValueError, match=\"dim not found in both inputs\"):\n        xr.cross(a, b, dim=\"nonexistent\")\n    with pytest.raises(ValueError, match=\"dim must be 1D\"):\n        xr.cross(xr.DataArray([[1, 2], [3, 4]]), b, dim=\"dim_0\")\n    with pytest.raises(ValueError, match=\"dim must be 1D\"):\n        xr.cross(a, xr.DataArray([[1, 2], [3, 4]]), dim=\"dim_0\")\n    with pytest.raises(ValueError, match=\"Both inputs must have the same length\"):\n        xr.cross(a, xr.DataArray([1, 2]), dim=\"dim_0\")\n", "def test_cross_errors(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError, match=\"Input arrays must have the same size\"):\n        xr.cross(a, b[1:], dim=dim)\n\n    with pytest.raises(ValueError, match=\"Input arrays must have the same dimension\"):\n        xr.cross(a, b.expand_dims(dim=\"new\"), dim=dim)\n\n    with pytest.raises(ValueError, match=\"dim must be a dimension of input arrays\"):\n        xr.cross(a, b, dim=\"nonexistent\")\n\n    with pytest.raises(ValueError, match=\"axis must be -1 for 1D input arrays\"):\n        xr.cross(a.expand_dims(dim=\"new\"), b.expand_dims(dim=\"new\"), dim=dim, axis=0)\n", "def test_cross_errors(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"test for dask.\")\n\n    # Test with incorrect dimension\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"incorrect_dim\")\n\n    # Test with non-matching shapes for broadcasting\n    with pytest.raises(ValueError):\n        xr.cross(xr.DataArray([1, 2]), xr.DataArray([1, 2, 3]), dim=\"dim_0\")\n\n    # Test with non-numeric data\n    with pytest.raises(TypeError):\n        xr.cross(xr.DataArray([\"a\", \"b\"]), xr.DataArray([1, 2]), dim=\"dim_0\")\n\n    # Test with non-existent dimension\n    with pytest.raises(KeyError):\n        xr.cross(a, b, dim=\"non_existent_dim\")\n\n    # Test with non-matching coordinates\n    with pytest.raises(ValueError):\n        xr.cross(\n            xr.DataArray([1, 2], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"z\"]))),\n            xr.DataArray([4, 5], dims=[\"cartesian\"], coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"]))),\n            dim=\"cartesian\",\n        )\n", "def test_cross_invalid_input() -> None:\n    a = xr.DataArray([1, 2, 3])\n    b = xr.DataArray([4, 5, 6])\n\n    # Test invalid axis value\n    with pytest.raises(ValueError, match=\"axis must be -1 or 0\"):\n        xr.cross(a, b, dim=\"dim_0\", axis=1)\n\n    # Test invalid dimension value\n    with pytest.raises(KeyError, match=\"Dimension 'invalid_dim' not found in dimensions\"):\n        xr.cross(a, b, dim=\"invalid_dim\")\n\n    # Test invalid dimension type\n    with pytest.raises(TypeError, match=\"dim must be a hashable type\"):\n        xr.cross(a, b, dim=[1, 2])\n\n    # Test mismatched dimensions\n    with pytest.raises(ValueError, match=\"operands could not be broadcast together\"):\n        xr.cross(a, b[:-1], dim=\"dim_0\")\n\n    # Test invalid input types\n    with pytest.raises(TypeError, match=\"Only xr.DataArray and xr.Variable are supported\"):\n        xr.cross(a.data, b, dim=\"dim_0\")\n"], "sample_425": ["def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex((1+2j))\", set()),\n    )\n", "def test_serialize_custom_function(self):\n        return x + y\n\n    value = functools.partial(custom_function, 1, y=2)\n    result = self.serialize_round_trip(value)\n    self.assertEqual(result.func, value.func)\n    self.assertEqual(result.args, value.args)\n    self.assertEqual(result.keywords, value.keywords)\n", "def test_serialize_timezone_aware_datetime(self):\n    \"\"\"\n    Test serialization of timezone-aware datetime objects.\n    \"\"\"\n    if pytz:\n        tz = pytz.timezone(\"Europe/Paris\")\n        dt = tz.localize(datetime.datetime(2022, 1, 1, 12, 0, 0))\n        self.assertSerializedEqual(dt)\n        self.assertSerializedResultEqual(\n            dt,\n            (\n                \"datetime.datetime(2022, 1, 1, 11, 0, tzinfo=datetime.timezone.utc)\",\n                {\"import datetime\"},\n            ),\n        )\n", "def test_serialize_custom_operations(self):\n    class CustomOperation(migrations.operations.Operation):\n            self.arg1 = arg1\n            self.arg2 = arg2\n\n            return (\"custom_operation\", [], {\"arg1\": self.arg1, \"arg2\": self.arg2})\n\n    operation = CustomOperation(\"test_arg1\", \"test_arg2\")\n    buff, imports = OperationWriter(operation, indentation=0).serialize()\n    self.assertEqual(imports, {\"import migrations\"})\n    self.assertEqual(\n        buff,\n        \"migrations.CustomOperation(\\n\"\n        \"    arg1='test_arg1',\\n\"\n        \"    arg2='test_arg2',\\n\"\n        \"),\",\n    )\n", "def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2),\n        (\"complex(1+2j)\", set()),\n    )\n"], "sample_655": ["def test_resume_global_capture_with_no_capture(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            capmanager = pytest.config.pluginmanager.getplugin(\"capturemanager\")\n            capmanager.resume_global_capture()\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p, \"--capture=no\")\n    assert result.ret == 0\n", "def test_capture_with_live_logging_disabled(testdir, capsys, capfd):\n    # Test the case when capture is disabled\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            captured = capsys.readouterr()\n            assert captured.out == \"\"\n            assert captured.err == \"\"\n\n            logging.info(\"something\")\n            print(\"next\")\n            logging.info(\"something\")\n\n            captured = capfd.readouterr()\n            assert captured.out == \"\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\", \"-s\")\n    assert result.ret == 0\n", "def test_py36_windowsconsoleio_workaround_encoding(monkeypatch, capsys):\n    \"\"\"\n    Ensure _py36_windowsconsoleio_workaround function works with objects that\n    have a non-default encoding, for example sys.stdout/sys.stderr.\n    \"\"\"\n    from _pytest.capture import _py36_windowsconsoleio_workaround\n\n    monkeypatch.setattr(sys.stdout, \"encoding\", \"utf-8\")\n    monkeypatch.setattr(sys.stderr, \"encoding\", \"utf-8\")\n\n    _py36_windowsconsoleio_workaround(sys.stdout)\n    _py36_windowsconsoleio_workaround(sys.stderr)\n\n    sys.stdout.write(\"hello\")\n    sys.stderr.write(\"world\")\n\n    out, err = capsys.readouterr()\n    assert out == \"hello\"\n    assert err == \"world\"\n", "def test_capsysbinary_write_non_string(capsysbinary):\n    import sys\n    if sys.version_info >= (3, 0):\n        with pytest.raises(TypeError):\n            capsysbinary.write(42)\n", "def test_capfd_with_invalid_fd(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.capture import FDCapture\n\n            invalid_fd = -1\n            with pytest.raises(OSError):\n                cap = FDCapture(invalid_fd)\n                cap.start()\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.assert_outcomes(passed=1)\n"], "sample_400": ["    def test_add_custom_fk_with_to_field(self):\n        class CustomForeignKey(models.ForeignKey):\n                kwargs[\"to_field\"] = \"custom_id\"\n                super().__init__(*args, **kwargs)\n\n                name, path, args, kwargs = super().deconstruct()\n                del kwargs[\"to_field\"]\n                return name, path, args, kwargs\n\n        book_custom_fk_to_field = ModelState(\n            \"testapp\",\n            \"Book\",\n            [\n                (\"author\", CustomForeignKey(\"testapp.Author\", on_delete=models.CASCADE)),\n            ],\n        )\n        changes = self.get_changes(\n            [self.author_empty],\n            [self.author_empty, book_custom_fk_to_field],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Book\")\n", "def test_create_model_with_constraints(self):\n    author = ModelState(\n        \"otherapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"constraints\": [\n                models.CheckConstraint(check=models.Q(name__isnull=False), name=\"name_not_null\")\n            ]\n        },\n    )\n    changes = self.get_changes([], [author])\n    added_constraint = models.CheckConstraint(check=models.Q(name__isnull=False), name=\"name_not_null\")\n    self.assertEqual(len(changes[\"otherapp\"]), 1)\n    self.assertEqual(len(changes[\"otherapp\"][0].operations), 2)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\", \"AddConstraint\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"Author\")\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 1, model_name=\"author\", constraint=added_constraint)\n", "def test_add_model_with_field_removed_from_base_model_multiple_inheritance(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name, even when there is multiple inheritance.\n    \"\"\"\n    before = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [],\n            bases=(\"app.readable\",),\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [],\n            bases=(\"app.readable\",),\n        ),\n        ModelState(\n            \"app\",\n            \"novel\",\n            [\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            bases=(\"app.book\",),\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RemoveField\", \"CreateModel\"])\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, name=\"title\", model_name=\"readable\"\n    )\n    self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"novel\")\n", "    def test_suggest_name_with_datetime(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n            ]\n\n        with mock.patch(\"django.utils.timezone.now\") as mock_now:\n            mock_now.return_value = datetime(2038, 1, 1, 11, 12, 0)\n            migration = Migration(\"some_migration\", \"test_app\")\n            self.assertEqual(migration.suggest_name(), \"person_20380101_1112\")\n", "    def test_add_field_with_deconstructible_default(self):\n        \"\"\"#24183 - Adding a field with a deconstructible default should work.\"\"\"\n        class DeconstructibleDefault:\n                pass\n\n                return (\n                    \"testapp.DeconstructibleDefault\",\n                    [],\n                    {},\n                )\n\n        author_with_deconstructible_default = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=DeconstructibleDefault())),\n            ],\n        )\n        changes = self.get_changes([self.author_empty], [author_with_deconstructible_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n"], "sample_816": ["def test_tfidfvectorizer_custom_analyzer():\n    # Test TfidfVectorizer with a custom analyzer\n        return doc.lower().split()\n\n    vectorizer = TfidfVectorizer(analyzer=custom_analyzer)\n    X = vectorizer.fit_transform([\"This is a sample document.\", \"Another document to analyze.\"])\n    assert X.shape == (2, 6)\n    assert_array_equal(vectorizer.get_feature_names(), ['another', 'document', 'is', 'sample', 'this', 'to'])\n", "def test_vectorizer_with_custom_analyzer_and_file_input(tmpdir):\n    # Test the case when a custom analyzer is used with file input\n        with open(doc, 'r') as f:\n            text = f.read()\n        return text.split()\n\n    f = tmpdir.join(\"file.txt\")\n    f.write(\"sample content\\n\")\n\n    vec = CountVectorizer(analyzer=custom_analyzer, input='filename')\n    X = vec.fit_transform([str(f)])\n\n    assert X.shape == (1, 2)\n    assert vec.get_feature_names() == ['content', 'sample']\n", "def test_vectorizer_stop_words_consistent():\n    stop_words = {\"and\", \"or\", \"but\"}\n    preprocess = lambda x: x.lower()\n    tokenize = lambda x: x.split()\n    is_consistent = CountVectorizer()._check_stop_words_consistency(stop_words, preprocess, tokenize)\n    assert is_consistent is True\n", "def test_vectorizer_inverse_transform_sparse():\n    # raw documents\n    data = ALL_FOOD_DOCS\n    vectorizer = CountVectorizer()\n    transformed_data = vectorizer.fit_transform(data)\n    # Test that inverse_transform also works with sparse matrices\n    transformed_data_sparse = sparse.csr_matrix(transformed_data)\n    inversed_data_sparse = vectorizer.inverse_transform(transformed_data_sparse)\n    analyze = vectorizer.build_analyzer()\n    for doc, inversed_terms in zip(data, inversed_data_sparse):\n        terms = np.sort(np.unique(analyze(doc)))\n        inversed_terms = np.sort(np.unique(inversed_terms))\n        assert_array_equal(terms, inversed_terms)\n", "def test_tfidf_vectorizer_with_custom_analyzer():\n        return doc.split()\n\n    vect = TfidfVectorizer(analyzer=custom_analyzer)\n    X = vect.fit_transform([\"this is a sample document\", \"another one\"])\n    assert_array_almost_equal(X.toarray(), [[0.57735027, 0.57735027, 0.57735027, 0.57735027],\n                                            [0.40824829, 0.40824829, 0.40824829, 0.81649658]])\n"], "sample_111": ["def test_non_unique_related_object_in_list_filter(self):\n    \"\"\"\n    Regression tests for #15819: If a field listed in list_filters\n    is a non-unique related object, distinct() must be called.\n    \"\"\"\n    parent = Parent.objects.create(name='Mary')\n    Child.objects.create(parent=parent, name='Daniel')\n    Child.objects.create(parent=parent, name='Daniel')\n\n    m = ParentAdmin(Parent, custom_site)\n    request = self.factory.get('/parent/', data={'child__name': 'Daniel'})\n    request.user = self.superuser\n\n    cl = m.get_changelist_instance(request)\n    # Make sure distinct() was called\n    self.assertEqual(cl.queryset.count(), 1)\n", "def test_no_add_permission_no_object_tools_block(self):\n    \"\"\"\n    When ModelAdmin.has_add_permission() returns False, the object-tools\n    block isn't displayed.\n    \"\"\"\n    superuser = self._create_superuser('superuser')\n    m = EventAdmin(Event, custom_site)\n    m.show_object_tools = False\n    request = self._mocked_authenticated_request('/event/', superuser)\n    self.assertFalse(m.has_add_permission(request))\n    response = m.changelist_view(request)\n    self.assertNotIn('<ul class=\"object-tools\">', response.rendered_content)\n", "def test_ordering_field_columns(self):\n    \"\"\"\n    Test that get_ordering_field_columns returns the correct dictionary of\n    ordering field column numbers and asc/desc.\n    \"\"\"\n    # Test with default ordering\n    m = BandAdmin(Band, custom_site)\n    request = self.factory.get('/band/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    ordering_fields = cl.get_ordering_field_columns()\n    self.assertEqual(ordering_fields, {0: 'asc'})\n\n    # Test with custom ordering\n    m.ordering = ['-nr_of_members', 'name']\n    cl = m.get_changelist_instance(request)\n    ordering_fields = cl.get_ordering_field_columns()\n    self.assertEqual(ordering_fields, {1: 'desc', 0: 'asc'})\n\n    # Test with query parameter ordering\n    request = self.factory.get('/band/', data={'o': '1.-2'})\n    cl = m.get_changelist_instance(request)\n    ordering_fields = cl.get_ordering_field_columns()\n    self.assertEqual(ordering_fields, {1: 'desc', 0: 'asc'})\n", "def test_custom_ordering_with_pk_shortcut(self):\n    self.assertEqual(CharPK._meta.pk.name, 'char_pk')  # Not equal to 'pk'.\n    m = admin.ModelAdmin(CustomIdUser, custom_site)\n\n    abc = CharPK.objects.create(char_pk='abc')\n    abcd = CharPK.objects.create(char_pk='abcd')\n    m.ordering = ['pk']\n\n    request = self.factory.get('/', data={ORDER_VAR: '0'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertCountEqual(cl.queryset, [abc, abcd])\n\n    request = self.factory.get('/', data={ORDER_VAR: '-0'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertCountEqual(cl.queryset, [abcd, abc])\n", "def test_empty_ordering(self):\n    \"\"\"\n    Regression test for #16376: If ordering is empty, use the default ordering.\n    \"\"\"\n    m = BandAdmin(Band, custom_site)\n    m.ordering = []\n    request = self.factory.get('/band/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {1: 'desc'})\n"], "sample_952": ["def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(inspect.is_singledispatch_function) is False\n", "def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b']\n    assert argspec.varargs == 'args'\n    assert argspec.keywords == 'kwargs'\n    assert argspec.defaults == (1,)\n    assert argspec.kwonlyargs == ['c', 'd']\n    assert argspec.kwonlydefaults == {'c': 2}\n    assert argspec.annotations == {}\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return \"base\"\n\n    @func.register(int)\n        return \"int\"\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(_.register) is False\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    sig = inspect.evaluate_signature(sig, globals())\n    assert str(sig) == \"(a: typing.List[int], b: typing.Callable[[str], int]) -> typing.Optional[str]\"\n\n    with pytest.raises(NameError):\n        # Unknown type\n            pass\n\n        sig = inspect.signature(func)\n        inspect.evaluate_signature(sig, globals())\n", "def test_signature_overloaded_functions():\n    from .typing_test_data import f21_overloaded\n\n    # overloaded functions with type hints\n    sig = inspect.signature(f21_overloaded)\n    assert stringify_signature(sig) == \"(*args: int, **kwargs: str)\"\n"], "sample_788": ["def test_transform_non_numeric_input():\n    X = np.array([[\"a\"], [\"b\"], [\"c\"]])\n    est = KBinsDiscretizer(n_bins=3)\n    assert_raise_message(ValueError, \"expected numeric array\", est.fit_transform, X)\n", "def test_kmeans_numeric_stability():\n    X = np.array([0.05, 0.05, 0.95]).reshape(-1, 1)\n    bin_edges = np.array([0.05, 0.5, 0.95])\n    Xt = np.array([0, 0, 1]).reshape(-1, 1)\n    kbd = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='kmeans')\n    kbd.fit(X)\n    assert_array_almost_equal(kbd.bin_edges_[0], bin_edges)\n    assert_array_almost_equal(kbd.transform(X), Xt)\n", "def test_transform_inverse_transform_consistency():\n    X = np.array([0, 1, 2, 3, 4, 5]).reshape(-1, 1)\n    kbd = KBinsDiscretizer(n_bins=3, strategy='uniform', encode='ordinal')\n    kbd.fit(X)\n    Xt = kbd.transform(X)\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_almost_equal(X, Xinv)\n", "def test_transform_with_nan():\n    X = np.array([[np.nan, 1.5, -4, -1],\n                  [-1, 2.5, np.nan, -0.5],\n                  [0, 3.5, -2, 0.5],\n                  [1, 4.5, -1, 2]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    assert_raises(ValueError, est.fit_transform, X)\n", "def test_kmeans_numeric_stability():\n    X = np.array([0.05, 0.05, 0.95]).reshape(-1, 1)\n    bin_edges = np.array([0.05, 0.5, 0.95])\n    Xt = np.array([0, 0, 1]).reshape(-1, 1)\n    kbd = KBinsDiscretizer(n_bins=2, encode='ordinal',\n                           strategy='kmeans')\n    kbd.fit(X)\n    assert_array_almost_equal(kbd.bin_edges_[0], bin_edges)\n    assert_array_almost_equal(kbd.transform(X), Xt)\n"], "sample_1081": ["def test_primefactors():\n    assert primefactors(123456) == [2, 3, 643]\n    assert primefactors(-5) == [5]\n    assert primefactors(123456) == [2, 3, 643]\n    assert primefactors(10000000001) == [101]\n    assert primefactors(0) == []\n", "def test_factorrat_visual_io():\n    fr = factorrat\n    # with factorrat\n    for th in [S(12)/1, Rational(25, 14), S(-25)/14/9]:\n        assert fr(th, visual=True) == fr(th, visual=1) == fr(th)\n        assert fr(th, visual=False) != fr(th)\n    for th in [S(12)/1, Rational(25, 14), S(-25)/14/9]:\n        assert fr(th, visual=None) == fr(th)\n        assert fr(th, visual=0) == fr(th)\n", "def test_pollard_pm1():\n    assert pollard_pm1(3) is None\n    assert pollard_pm1(4) is None\n    assert pollard_pm1(5) is None\n    assert pollard_pm1(15) == 3\n    assert pollard_pm1(21) == 3\n    assert pollard_pm1(35) == 5\n    assert pollard_pm1(105) == 5\n    assert pollard_pm1(2**100 + 1, B=10) == 274177\n", "def test_divisor_sigma_with_symbols():\n    m = Symbol(\"m\", integer=True, positive=True)\n    k = Symbol(\"k\", integer=True, positive=True)\n    assert divisor_sigma(m, k)\n    assert divisor_sigma(m, k).subs([(m, 3**10), (k, 2)]) == 19682\n    assert divisor_sigma(m, k).subs([(m, 5), (k, 3)]) == 345\n", "def test_factorrat_rational_input():\n    rat = Rational(25, 14)\n    factor_rat = factorrat(rat)\n    factor_num = factorint(rat.p)\n    factor_den = factorint(rat.q)\n    for key in factor_num.keys():\n        assert factor_rat[key] == factor_num[key] - factor_den.get(key, 0)\n"], "sample_773": ["def test_logistic_regression_path_l1_convergence():\n    # Test that logistic_regression_path with l1 penalty converges to the same\n    # solution as LogisticRegression with l1 penalty.\n    X, y = make_classification(n_samples=1000, random_state=0)\n    Cs = [0.1, 1.0, 10.0]\n    coefs_path, _, _ = _logistic_regression_path(X, y, penalty='l1', Cs=Cs, solver='saga')\n    for i, C in enumerate(Cs):\n        lr = LogisticRegression(penalty='l1', C=C, solver='saga', random_state=0)\n        lr.fit(X, y)\n        assert_array_almost_equal(lr.coef_, coefs_path[i], decimal=2)\n", "def test_logistic_regression_path_l1():\n    # Make sure that the returned coefs by logistic_regression_path when\n    # penalty='l1' have some zeros.\n    X, y = make_classification(n_samples=200, n_classes=2, n_informative=2,\n                               n_redundant=0, n_clusters_per_class=1,\n                               random_state=0, n_features=20)\n    Cs = [.00001, 1, 10000]\n    coefs, _, _ = _logistic_regression_path(X, y, penalty='l1', Cs=Cs,\n                                            solver='liblinear', random_state=0,\n                                            multi_class='ovr')\n\n    assert not np.allclose(coefs[0], 0)\n    assert np.any(coefs[-1] == 0)\n", "def test_logistic_regression_path_intercept_scaling():\n    # Test that intercept_scaling is used correctly by logistic_regression_path\n    X, y = make_classification(n_samples=100, n_features=2, random_state=0)\n    coefs, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=[1.],\n                                            solver='liblinear',\n                                            intercept_scaling=2.0)\n    # Check that intercept_scaling affects the intercept but not the coefficients\n    coefs_no_scaling, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=[1.],\n                                                       solver='liblinear')\n    assert_array_almost_equal(coefs[0, :-1], coefs_no_scaling[0, :-1])\n    assert_array_less(coefs[0, -1], coefs_no_scaling[0, -1])\n", "def test_LogisticRegression_multinomial_intercept_scaling(multi_class):\n    # Make sure intercept_scaling is ignored with multinomial logistic\n    # regression.\n    X, y = make_classification(n_samples=100, n_classes=3, n_informative=10,\n                               n_redundant=0, n_repeated=0, random_state=0)\n    msg = (\"intercept_scaling is not supported with multi_class='multinomial'.\"\n           \" Got (multi_class='multinomial')\")\n    lr = LogisticRegression(multi_class=multi_class, solver='saga',\n                            intercept_scaling=2)\n    assert_warns_message(UserWarning, msg, lr.fit, X, y)\n", "def test_logistic_regression_path_input_check():\n    # Test input validation for logistic_regression_path\n    X, y = make_classification(n_samples=1000, random_state=0)\n    Cs = np.logspace(-4, 4, 3)\n\n    # Test with invalid X input\n    msg = \"X has 2 dimensions, Pass 2D arrays only\"\n    assert_raise_message(ValueError, msg, _logistic_regression_path, [X], y, Cs=Cs)\n\n    # Test with invalid y input\n    msg = \"y has 2 dimensions, Pass 1D arrays only\"\n    assert_raise_message(ValueError, msg, _logistic_regression_path, X, [y], Cs=Cs)\n\n    # Test with invalid Cs input\n    msg = \"Cs has 2 dimensions, Pass 1D arrays only\"\n    assert_raise_message(ValueError, msg, _logistic_regression_path, X, y, Cs=[Cs])\n\n    # Test with invalid multi_class input\n    msg = (\"multi_class should be 'multinomial', 'ovr' or 'auto'. Got 'invalid'\")\n    assert_raise_message(ValueError, msg, _logistic_regression_path, X, y, Cs=Cs, multi_class='invalid')\n"], "sample_823": ["def test_pairwise_distances_callable_metric():\n    # Test the pairwise_distance helper function with the callable implementation\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((5, 4))\n\n        return np.abs(x - y).sum(axis=0)\n\n    S = pairwise_distances(X, Y, metric=custom_metric)\n    S2 = pairwise_distances(X, Y, metric=lambda x, y: np.abs(x - y).sum(axis=0))\n    assert_array_almost_equal(S, S2)\n\n    # Test that a value error is raised when the lengths of X and Y should not differ\n    Y = rng.random_sample((3, 4))\n    assert_raises(ValueError, pairwise_distances, X, Y, metric=custom_metric)\n", "def test_pairwise_distances_data_derived_params_dense():\n    # check that pairwise_distances give the same result in sequential and\n    # parallel, when metric has data-derived parameters, for dense input.\n    with config_context(working_memory=1):  # to have more than 1 chunk\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((1000, 10))\n        Y = rng.random_sample((1000, 10))\n        expected_dist_default_params = cdist(X, Y, metric=\"seuclidean\")\n        params = {'V': np.var(np.vstack([X, Y]), axis=0, ddof=1)}\n        expected_dist_explicit_params = cdist(X, Y, metric=\"seuclidean\", **params)\n        dist = np.vstack(tuple(pairwise_distances_chunked(X, Y, metric=\"seuclidean\", n_jobs=2)))\n\n        assert_allclose(dist, expected_dist_explicit_params)\n        assert_allclose(dist, expected_dist_default_params)\n", "def test_pairwise_boolean_distance_shape(metric):\n    # test that the output shape is correct for boolean distances\n    rng = np.random.RandomState(0)\n    X = rng.randn(5, 4)\n    Y = X.copy()\n    Y[0, 0] = 1 - Y[0, 0]\n\n    # ignore conversion to boolean in pairwise_distances\n    with ignore_warnings(category=DataConversionWarning):\n        for Z in [Y, None]:\n            res = pairwise_distances(X, Z, metric=metric)\n            assert res.shape == (5, 5 if Z is None else 4)\n", "def test_pairwise_distances_seuclidean_invalid_input():\n    # check that pairwise_distances raise ValueError when V is invalid\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    V = np.array([1, 2])\n    with pytest.raises(ValueError):\n        pairwise_distances(X, metric='seuclidean', V=V)\n", "def test_pairwise_distances_metric_params():\n    # Test pairwise_distances with different metric parameterizations\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n\n    # Test with scipy.spatial.distance metric and parameters as kwargs\n    S = pairwise_distances(X, Y, metric=\"minkowski\", p=2.0, w=None)\n    S2 = pairwise_distances(X, Y, metric=minkowski, p=2.0, w=None)\n    assert_array_almost_equal(S, S2)\n\n    # Test with scipy.spatial.distance metric and parameters as part of metric\n    S3 = pairwise_distances(X, Y, metric=\"minkowski\", p=1.5)\n    assert_raises(TypeError, minkowski, X, Y, p=1.5)\n\n    # Test with custom metric function and parameters\n        return np.sum(np.abs(x - y) ** alpha)\n\n    S4 = pairwise_distances(X, Y, metric=custom_metric, alpha=0.5)\n    S5 = pairwise_distances(X, Y, metric=custom_metric, alpha=1.0)\n    assert_raises(AssertionError, assert_array_almost_equal, S4, S5)\n"], "sample_202": ["def test_cookie_settings_with_samesite_none(self):\n    \"\"\"\n    CookieStorage honors SESSION_COOKIE_SAMESITE='None' with SESSION_COOKIE_SECURE=True (#30215).\n    \"\"\"\n    with override_settings(SESSION_COOKIE_SAMESITE='None', SESSION_COOKIE_SECURE=True):\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.add(constants.INFO, 'test')\n        storage.update(response)\n        self.assertIn('test', response.cookies['messages'].value)\n        self.assertEqual(response.cookies['messages']['samesite'], 'None')\n", "def test_store_and_remove_oldest(self):\n    \"\"\"\n    Test that the _store method removes the oldest message when max_cookie_size is exceeded\n    and remove_oldest is True.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Create messages larger than max_cookie_size\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        storage.add(constants.INFO, str(i) * msg_size)\n\n    # Store the messages with remove_oldest=True\n    unstored_messages = storage._store(list(storage), response, remove_oldest=True)\n\n    # Check that the oldest message was removed\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, '0' * msg_size)\n\n    # Check that the remaining messages were stored\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n", "def test_cookie_samesite_setting(self):\n    \"\"\"\n    CookieStorage honors SESSION_COOKIE_SAMESITE setting (#25000).\n    \"\"\"\n    # Test before the messages have been consumed\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.add(constants.INFO, 'test')\n    storage.update(response)\n    self.assertEqual(response.cookies['messages']['samesite'], 'Lax')\n\n    # Test deletion of the cookie (storing with an empty value) after the messages have been consumed\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.add(constants.INFO, 'test')\n    for m in storage:\n        pass  # Iterate through the storage to simulate consumption of messages.\n    storage.update(response)\n    self.assertEqual(response.cookies['messages']['samesite'], 'Lax')\n", "def test_extra_tags(self):\n    \"\"\"\n    Messages that include extra_tags are decoded correctly.\n    \"\"\"\n    # Encode the messages using the current encoder.\n    messages = [Message(constants.INFO, 'message %s' % i, extra_tags='tag%s' % i) for i in range(5)]\n    encoder = MessageEncoder(separators=(',', ':'))\n    encoded_messages = encoder.encode(messages)\n\n    # Decode the messages\n    decoded_messages = json.loads(encoded_messages, cls=MessageDecoder)\n\n    # Check that the messages and extra_tags match\n    for decoded, original in zip(decoded_messages, messages):\n        self.assertEqual(decoded.message, original.message)\n        self.assertEqual(decoded.extra_tags, original.extra_tags)\n", "def test_cookie_samesite_none_insecure(self):\n    \"\"\"\n    CookieStorage does not set the same-site attribute to 'None' when the cookie is not secure (#29967).\n    \"\"\"\n    with override_settings(SESSION_COOKIE_SECURE=False, SESSION_COOKIE_SAMESITE='None'):\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.add(constants.INFO, 'test')\n        storage.update(response)\n        self.assertEqual(response.cookies['messages'].get('samesite'), None)\n"], "sample_815": ["def test_balanced_accuracy_score_unseen_labels():\n    assert_warns_message(UserWarning, 'y_pred contains classes not in y_true',\n                         balanced_accuracy_score, ['a', 'b', 'b'], ['a', 'b', 'c'])\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = ['a', 'b', 'a', 'b']\n    y_pred = ['a', 'a', 'a', 'b']\n    sample_weight = [0.25, 0.25, 0.25, 0.25]\n    macro_recall = recall_score(y_true, y_pred, average='macro',\n                                labels=np.unique(y_true),\n                                sample_weight=sample_weight)\n    with ignore_warnings():\n        balanced = balanced_accuracy_score(y_true, y_pred,\n                                            sample_weight=sample_weight)\n    assert balanced == pytest.approx(macro_recall)\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = [0, 0, 1, 1]\n    y_pred = [0, 0, 1, 1]\n    sample_weight = [0.5, 0.5, 0.5, 0.5]\n    assert balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight) == 1.0\n    sample_weight = [1, 0, 0, 1]\n    assert balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight) == 0.5\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = [0, 1, 2, 2, 0, 1, 1, 2]\n    y_pred = [0, 0, 2, 2, 0, 1, 2, 2]\n    sample_weight = [1, 1, 2, 1, 1, 1, 2, 1]\n    balanced_weighted = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    balanced_unweighted = balanced_accuracy_score(y_true, y_pred)\n    assert balanced_weighted != balanced_unweighted\n", "def test_multilabel_jaccard_similarity_score_different_shapes():\n    # Dense label indicator matrix format\n    y1 = np.array([[0, 1, 1], [1, 0, 1]])\n    y2 = np.array([[0, 0, 1]])\n\n    msg = 'Found input variables with inconsistent numbers of samples: [2, 1]'\n    assert_raise_message(ValueError, msg, jaccard_similarity_score, y1, y2)\n"], "sample_65": ["def test_i18n_with_additional_package(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    when an additional package is requested.\n    \"\"\"\n    app6_trans_string = 'il faut traduire cette cha\\\\u00eene de caract\\\\u00e8res de app6'\n    with override('fr'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, app6_trans_string)\n", "    def test_jsi18n_with_invalid_package(self):\n        \"\"\"The javascript_catalog should raise ValueError if an invalid package is provided.\"\"\"\n        url = reverse('javascript-catalog', kwargs={'packages': 'invalid_package'})\n        with self.assertRaises(ValueError) as cm:\n            self.client.get(url)\n        self.assertEqual(str(cm.exception), 'Invalid package(s) provided to JavaScriptCatalog: invalid_package')\n", "def test_i18n_language_english_default_fallback(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    with English fallback if the default language is en-us, the selected\n    language has a partial translation available and a catalog composed by\n    djangojs domain translations of multiple Python packages is requested.\n    \"\"\"\n    base_trans_string = 'this string is to be translated from '\n    app1_trans_string = base_trans_string + 'app1'\n    app2_trans_string = base_trans_string + 'app2'\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n_multi_packages1/')\n        self.assertContains(response, app1_trans_string)\n        self.assertContains(response, app2_trans_string)\n        self.assertContains(response, 'il faut traduire cette cha\u00eene de caract\u00e8res de app1')\n        self.assertContains(response, 'il faut traduire cette cha\u00eene de caract\u00e8res de app2')\n", "def test_i18n_different_non_english_languages_with_plus_sign(self):\n    \"\"\"\n    Test the JavaScriptCatalog view with a plus sign in the URL.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='fr'), override('es-ar'):\n        response = self.client.get('/jsi18n/app3+app4/')\n        self.assertContains(response, 'este texto de app3 debe ser traducido')\n        self.assertContains(response, 'este texto de app4 debe ser traducido')\n", "def test_i18n_with_different_plural_formulas(self):\n    \"\"\"\n    Test the JavaScript i18n view with a language that uses a different plural formula.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('ru'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, 'plural formula for Russian')\n        self.assertContains(response, 'django.pluralidx = function(n) { return (n == 1) ? 0 : (n >= 2 && n <= 4) ? 1 : 2; };')\n"], "sample_806": ["def test_gradient_boosting_with_loss_quantile():\n    # Check that GradientBoostingRegressor works with loss='quantile'\n\n    X, y = make_regression(random_state=0)\n    gb = GradientBoostingRegressor(loss='quantile', alpha=0.5)\n    gb.fit(X, y)\n    y_pred = gb.predict(X)\n    assert y_pred.shape == y.shape\n", "def test_gradient_boosting_n_iter_no_change_none():\n    # Check that n_iter_no_change=None disables early stopping\n\n    X, y = make_regression(n_samples=1000, random_state=0)\n    gbr = GradientBoostingRegressor(n_estimators=1000, n_iter_no_change=None, learning_rate=0.1, max_depth=3, random_state=42)\n    gbr.fit(X, y)\n    assert gbr.n_estimators_ == 1000\n", "def test_gradient_boosting_with_init_prediction_shape():\n    # Check that the shape of the prediction of the init estimator is the\n    # same as the shape of the predictions of the gradient boosting estimator\n\n    X, y = make_classification(n_classes=3, n_clusters_per_class=1)\n    init = DummyClassifier()\n    gb = GradientBoostingClassifier(init=init)\n    gb.fit(X, y)\n\n    assert gb.init_.predict_proba(X).shape == gb.predict_proba(X).shape\n", "def test_gradient_boosting_with_custom_loss():\n    # Check that GradientBoostingRegressor works when using a custom loss function\n    X, y = make_regression(random_state=0)\n\n    # Define a custom loss function\n        return np.mean((y_true - y_pred) ** 2) + np.sum(np.abs(y_pred))\n\n    gb = GradientBoostingRegressor(loss='custom', random_state=0)\n    gb.loss_ = custom_loss\n    gb.fit(X, y)\n    assert gb.score(X, y) > 0.5  # Adjust the threshold based on the performance\n", "def test_gradient_boosting_with_loss_deviance():\n    # Check that GradientBoostingClassifier works with the 'deviance' loss function\n    # and the probability estimates are close to the true probabilities.\n\n    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n                               n_informative=2, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                    max_depth=3, loss='deviance',\n                                    random_state=42)\n    gb.fit(X_train, y_train)\n    y_pred_proba = gb.predict_proba(X_test)\n    y_true_proba = np.mean(y_test == 1)\n\n    # The true probability should be close to the predicted probability\n    assert_almost_equal(y_pred_proba[:, 1].mean(), y_true_proba, decimal=2)\n"], "sample_547": ["def test_offsetimage_properties():\n    data = np.random.rand(10, 10)\n    oi = OffsetImage(data)\n    assert_allclose(oi.get_data(), data)\n    assert oi.get_zoom() == 1\n\n    new_data = np.random.rand(20, 20)\n    oi.set_data(new_data)\n    oi.set_zoom(2)\n    assert_allclose(oi.get_data(), new_data)\n    assert oi.get_zoom() == 2\n", "def test_drawingarea_set_offset(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    da_ref = DrawingArea(10, 10)\n    da_ref.set_offset((20, 20))\n    ax_ref.add_artist(da_ref)\n\n    ax_test = fig_test.add_subplot()\n    da_test = DrawingArea(10, 10)\n    ax_test.add_artist(da_test)\n    da_test.set_offset((20, 20))\n", "def test_anchoredoffsetbox():\n    fig, ax = plt.subplots()\n\n    ta = TextArea(\"foo\")\n    ao = AnchoredOffsetbox('upper left', child=ta, pad=5, borderpad=1, frameon=True)\n    ax.add_artist(ao)\n\n    ta = TextArea(\"bar\")\n    ao = AnchoredOffsetbox('upper right', child=ta, pad=10, borderpad=2, frameon=False)\n    ax.add_artist(ao)\n\n    ta = TextArea(\"foobar\")\n    ao = AnchoredOffsetbox('lower right', child=ta, pad=15, borderpad=3)\n    ax.add_artist(ao)\n", "def test_offsetbox_invisible_children():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    bg = mpatches.Rectangle((0, 0), 100, 100, facecolor='#CCCCCC', edgecolor='None', linewidth=0)\n    line = mlines.Line2D([-50, 150], [50, 50], color='black', linewidth=10)\n    anchored_box = AnchoredOffsetbox(loc='center', child=da, pad=0., frameon=False, bbox_to_anchor=(.5, .5), bbox_transform=ax.transAxes, borderpad=0.)\n\n    da.add_artist(bg)\n    da.add_artist(line)\n    ax.add_artist(anchored_box)\n\n    # Make the child line invisible and check that it's not drawn\n    line.set_visible(False)\n    fig.canvas.draw()\n", "def test_packers_with_none_width_and_height(mode):\n    # Test Packer classes when width and height are None\n    fig = plt.figure()\n    renderer = fig.canvas.get_renderer()\n\n    r1 = DrawingArea(10, 20)\n    r2 = DrawingArea(20, 30)\n\n    packer = HPacker(children=[r1, r2], mode=mode, width=None, height=None)\n    packer.draw(renderer)\n    bbox = packer.get_bbox(renderer)\n    assert bbox.width == 30  # Sum of widths of r1 and r2\n    assert bbox.height == 30  # Max of heights of r1 and r2\n\n    packer = VPacker(children=[r1, r2], mode=mode, width=None, height=None)\n    packer.draw(renderer)\n    bbox = packer.get_bbox(renderer)\n    assert bbox.width == 20  # Max of widths of r1 and r2\n    assert bbox.height == 50  # Sum of heights of r1 and r2\n"], "sample_275": ["def test_delete_with_select_for_update(self):\n    \"\"\"\n    The delete() method should work correctly with select_for_update().\n    \"\"\"\n    Book.objects.create(pagecount=100)\n    Book.objects.create(pagecount=200)\n    with transaction.atomic():\n        books = Book.objects.select_for_update()\n        books.delete()\n    self.assertEqual(Book.objects.count(), 0)\n", "def test_delete_with_exclude(self):\n    \"\"\"\n    The exclude() method should work correctly with delete().\n    \"\"\"\n    Book.objects.create(title='Book 1')\n    Book.objects.create(title='Book 2')\n    Book.objects.create(title='Book 3')\n\n    Book.objects.exclude(title='Book 2').delete()\n\n    self.assertEqual(Book.objects.count(), 1)\n    self.assertEqual(Book.objects.first().title, 'Book 2')\n", "def test_delete_with_filtered_relation(self):\n    \"\"\"\n    Test that delete() with a filtered relation prefetching works as expected.\n    \"\"\"\n    person1 = Person.objects.create(name='Person 1')\n    person2 = Person.objects.create(name='Person 2')\n    Award.objects.create(name='Award 1', content_object=person1)\n    Award.objects.create(name='Award 2', content_object=person2)\n\n    Person.objects.filter(awards__name='Award 1').prefetch_related(\n        Prefetch('awards', queryset=Award.objects.filter(name='Award 1'))\n    ).delete()\n\n    self.assertEqual(Person.objects.count(), 1)\n    self.assertEqual(Award.objects.count(), 1)\n", "    def test_disallowed_delete_values(self):\n        msg = 'Cannot call delete() after .values() or .values_list()'\n        with self.assertRaisesMessage(TypeError, msg):\n            Book.objects.values().delete()\n        with self.assertRaisesMessage(TypeError, msg):\n            Book.objects.values_list().delete()\n", "    def test_custom_manager_with_use_for_related_fields(self):\n        \"\"\"\n        Custom managers with use_for_related_fields=True should be used when\n        resolving related objects for deletion.\n        \"\"\"\n        # Create a FooFileProxy instance and its related FooFile instance\n        image = Image.objects.create()\n        as_file = File.objects.get(pk=image.pk)\n        FooFileProxy.objects.create(my_file=as_file)\n\n        # Delete the Image instance through the custom FooImage manager\n        FooImage.objects.all().delete()\n\n        # The FooFile instance should also be deleted since it is related to\n        # the deleted FooImage instance through the File model\n        self.assertEqual(len(FooFileProxy.objects.all()), 0)\n"], "sample_1049": ["def test_plane_arbitrary_point_parameters():\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    u, v = symbols('u v')\n    pt = p.arbitrary_point(u, v)\n    params = p.parameter_value(pt, u, v)\n    assert pt.subs(params) == pt\n", "def test_is_coplanar():\n    p1 = Plane(Point3D(0, 0, 0), normal_vector=(1, 1, 1))\n    p2 = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    p3 = Plane(Point3D(0, 0, 0), normal_vector=(2, 2, 2))\n    assert p1.is_coplanar(p2)\n    assert p1.is_coplanar(p3)\n    assert p1.is_coplanar(Point3D(1, 1, 1))\n    assert not p1.is_coplanar(Point3D(2, 2, 2))\n    assert p1.is_coplanar(Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1)))\n    assert not p1.is_coplanar(Line3D(Point3D(2, 2, 2), Point3D(3, 3, 3)))\n    assert p1.is_coplanar(Segment3D(Point3D(0, 0, 0), Point3D(1, 1, 1)))\n    assert not p1.is_coplanar(Segment3D(Point3D(2, 2, 2), Point3D(3, 3, 3)))\n    assert p1.is_coplanar(Ray3D(Point3D(0, 0, 0), Point3D(1, 1, 1)))\n    assert not p1.is_coplanar(Ray3D(Point3D(2, 2, 2), Point3D(3, 3, 3)))\n", "def test_plane_coplanarity():\n    p1 = Plane((0, 0, 0), (1, 1, 1))\n    p2 = Plane((1, 1, 1), (1, 1, 1))\n    p3 = Plane((2, 2, 2), (1, 1, 1))\n    p4 = Plane((3, 3, 3), (1, 1, 1))\n    p5 = Plane((4, 4, 4), (1, 0, 0))\n\n    assert p1.is_coplanar(p2)\n    assert p1.is_coplanar(p3)\n    assert p1.is_coplanar(p4)\n    assert not p1.is_coplanar(p5)\n\n    l1 = Line3D((0, 0, 0), (1, 1, 1))\n    l2 = Line3D((1, 1, 1), (2, 2, 2))\n    l3 = Line3D((3, 3, 3), (4, 4, 4))\n\n    assert p1.is_coplanar(l1)\n    assert not p1.is_coplanar(l2)\n    assert not p1.is_coplanar(l3)\n\n    pt1 = Point3D(1, 1, 1)\n    pt2 = Point3D(2, 2, 2)\n    pt3 = Point3D(3, 3, 3)\n    pt4 = Point3D(4, 4, 5)\n\n    assert p1.is_coplanar(pt1)\n    assert p1.is_coplanar(pt2)\n    assert p1.is_coplanar(pt3)\n    assert not p1.is_coplanar(pt4)\n", "def test_is_coplanar():\n    p = Plane(Point3D(0, 0, 0), normal_vector=(1, 1, 1))\n    assert p.is_coplanar(Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))) is True\n    assert p.is_coplanar(Line3D(Point3D(1, 1, 1), Point3D(2, 3, 4))) is False\n    assert p.is_coplanar(Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))) is True\n    assert p.is_coplanar(Plane(Point3D(1, 1, 1), normal_vector=(2, 2, 2))) is False\n    assert p.is_coplanar(Point3D(1, 1, 1)) is True\n    assert p.is_coplanar(Point3D(1, 2, 3)) is False\n", "def test_plane_properties():\n    # Test is_coplanar method\n    p1 = Plane((0, 0, 0), (1, 0, 0), (0, 1, 0))\n    p2 = Plane((0, 0, 1), (1, 0, 0), (0, 1, 0))\n    p3 = Plane((0, 0, 0), (1, 1, 1))\n    assert p1.is_coplanar(p2) is False\n    assert p1.is_coplanar(p3) is True\n\n    # Test projection_line method\n    l1 = Line3D((1, 1, 1), (2, 2, 2))\n    l2 = Line3D((0, 0, 0), (1, 1, 0))\n    assert p1.projection_line(l1) == l1\n    assert p1.projection_line(l2) == Segment3D((0, 0, 0), (1, 1, 0))\n\n    # Test parallel_plane method\n    p4 = Plane((1, 1, 1), (1, 1, 1))\n    p5 = p4.parallel_plane((2, 2, 2))\n    assert p4.is_parallel(p5)\n    assert p4.normal_vector == p5.normal_vector\n    assert (2, 2, 2) in p5\n\n    # Test perpendicular_line method\n    l3 = p4.perpendicular_line((3, 3, 3))\n    assert p4.is_perpendicular(l3)\n    assert l3.direction_ratio == p4.normal_vector\n    assert (3, 3, 3) in l3\n"], "sample_165": ["def test_model_multiple_choice_field_invalid_pk_value(self):\n    ChoiceModel.objects.create(pk='invalid', name='d')\n    e = {\n        'invalid_pk_value': '\u201c%(pk)s\u201d is not a valid primary key value.',\n    }\n    f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n    self.assertFormErrors(['\u201cinvalid\u201d is not a valid primary key value.'], f.clean, ['invalid'])\n", "def test_splitdatetimewidget(self):\n    e = {\n        'date_required': 'DATE REQUIRED',\n        'time_required': 'TIME REQUIRED',\n        'invalid_date': 'INVALID DATE',\n        'invalid_time': 'INVALID TIME',\n    }\n    f = SplitDateTimeWidget(error_messages=e)\n    self.assertFormErrors(['DATE REQUIRED', 'TIME REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID DATE', 'INVALID TIME'], f.clean, ['a', 'b'])\n", "def test_modelform_error_messages(self):\n    class TestModelForm(ModelForm):\n        class Meta:\n            model = ChoiceModel\n            fields = ['name']\n            error_messages = {\n                'name': {\n                    'required': 'CUSTOM REQUIRED MESSAGE',\n                    'max_length': 'CUSTOM MAX LENGTH MESSAGE',\n                }\n            }\n\n    form = TestModelForm({'name': ''})\n    self.assertEqual(form.errors['name'], ['CUSTOM REQUIRED MESSAGE'])\n\n    form = TestModelForm({'name': 'a' * 51})\n    self.assertEqual(form.errors['name'], ['CUSTOM MAX LENGTH MESSAGE'])\n", "def test_filefield_max_length(self):\n    e = {\n        'required': 'REQUIRED',\n        'invalid': 'INVALID',\n        'missing': 'MISSING',\n        'empty': 'EMPTY FILE',\n        'max_length': 'FILE IS TOO LARGE',\n    }\n    f = FileField(error_messages=e, max_length=5)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID'], f.clean, 'abc')\n    self.assertFormErrors(['EMPTY FILE'], f.clean, SimpleUploadedFile('name', None))\n    self.assertFormErrors(['EMPTY FILE'], f.clean, SimpleUploadedFile('name', ''))\n    self.assertFormErrors(['FILE IS TOO LARGE'], f.clean, SimpleUploadedFile('name', '123456'))\n", "def test_modelform_defines_fields_function(self):\n    class TestForm(Form):\n        pass\n\n    self.assertFalse(modelform_defines_fields(TestForm))\n\n    class TestFormWithMeta(Form):\n        class Meta:\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(TestFormWithMeta))\n\n    class TestFormWithMetaAndFields(Form):\n        class Meta:\n            fields = ['field1', 'field2']\n\n    self.assertTrue(modelform_defines_fields(TestFormWithMetaAndFields))\n\n    class TestFormWithMetaAndExclude(Form):\n        class Meta:\n            exclude = ['field1', 'field2']\n\n    self.assertTrue(modelform_defines_fields(TestFormWithMetaAndExclude))\n"], "sample_759": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_feature_names_with_input():\n    enc = OneHotEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    input_features = ['Gender', 'Age']\n    feature_names = enc.get_feature_names(input_features)\n    assert_array_equal(['Gender_Female', 'Gender_Male', 'Age_1', 'Age_2', 'Age_3'], feature_names)\n", "def test_one_hot_encoder_categories_shape():\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(categories=[['Male', 'Female'], [1, 2, 3, 4]])\n    with pytest.raises(ValueError, match=\"Shape mismatch: if n_values is an array, it has to be of shape (n_features,).\"):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_unsorted_specified_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OneHotEncoder(categories=[['b', 'c', 'a']])\n    exp = np.array([[0., 0., 1.],\n                    [1., 0., 0.]])\n    assert_array_equal(enc.fit(X).transform(X).toarray(), exp)\n    assert enc.categories_[0].tolist() == ['b', 'c', 'a']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted specified categories should raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OneHotEncoder(categories=[[2, 3, 1]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_specified_categories_and_handle_unknown(X, cats, handle_unknown, expected_output):\n    enc = OneHotEncoder(categories=cats, handle_unknown=handle_unknown)\n    if isinstance(expected_output, np.ndarray):\n        assert_array_equal(enc.fit_transform(X).toarray(), expected_output)\n    else:\n        with pytest.raises(expected_output):\n            enc.fit_transform(X)\n"], "sample_859": ["def test_multi_task_elastic_net_cv_dtype():\n    n_samples, n_features = 10, 3\n    rng = np.random.RandomState(42)\n    X = rng.binomial(1, .5, size=(n_samples, n_features))\n    X = X.astype(int)  # make it explicit that X is int\n    y = X[:, [0, 0]].copy()\n    est = MultiTaskElasticNetCV(n_alphas=5, fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(est.coef_, [[1, 0, 0]] * 2, decimal=3)\n", "def test_elasticnet_cv_dtype():\n    n_samples, n_features = 10, 3\n    rng = np.random.RandomState(42)\n    X = rng.binomial(1, .5, size=(n_samples, n_features))\n    X = X.astype(int)  # make it explicit that X is int\n    y = X[:, 0].copy()\n    est = ElasticNetCV(n_alphas=5, fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(est.coef_, [1, 0, 0], decimal=3)\n", "def test_multi_task_lasso_cv_sparse():\n    n_samples, n_features = 10, 3\n    rng = np.random.RandomState(42)\n    X = rng.binomial(1, .5, size=(n_samples, n_features))\n    X_sparse = sparse.csr_matrix(X)\n    y = X[:, [0, 0]].copy()\n    est_sparse = MultiTaskLassoCV(n_alphas=5, fit_intercept=True).fit(X_sparse, y)\n    est_dense = MultiTaskLassoCV(n_alphas=5, fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(est_sparse.coef_, est_dense.coef_, decimal=3)\n", "def test_lasso_cv_with_l1_ratio_list():\n    X, y, X_test, y_test = build_dataset()\n    max_iter = 150\n    clf = LassoCV(n_alphas=10, eps=1e-3, max_iter=max_iter, cv=3, l1_ratio=[0.5, 0.7, 0.9])\n    clf.fit(X, y)\n    assert clf.l1_ratio_ in [0.5, 0.7, 0.9]\n", "def test_multi_task_enet_cv_dtype():\n    n_samples, n_features = 10, 3\n    rng = np.random.RandomState(42)\n    X = rng.binomial(1, .5, size=(n_samples, n_features))\n    X = X.astype(int)  # make it explicit that X is int\n    y = X[:, [0, 0]].copy()\n    est = MultiTaskElasticNetCV(n_alphas=5, fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(est.coef_, [[1, 0, 0]] * 2, decimal=3)\n"], "sample_522": ["def test_offset_text_loc_bottom():\n    plt.style.use('mpl20')\n    fig, ax = plt.subplots()\n    np.random.seed(seed=19680808)\n    pc = ax.pcolormesh(np.random.randn(10, 10)*1e6)\n    cb = fig.colorbar(pc, location='bottom', extend='max')\n    fig.draw_without_rendering()\n    # check that the offsetText is in the proper place below the\n    # colorbar axes.  In this case the colorbar axes is the same\n    # height as the parent, so use the parents bbox.\n    assert cb.ax.xaxis.offsetText.get_position()[1] < ax.bbox.y0\n", "def test_colorbar_label_fontsize():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, label='cbar')\n    cbar.set_label('new cbar', fontsize=16)\n    assert cbar.ax.get_ylabel() == 'new cbar'\n    assert cbar.ax.yaxis.label.get_fontsize() == 16\n", "def test_colorbar_extendfrac():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, extend='both', extendfrac=0.2)\n    fig.draw_without_rendering()\n    # check that the extendfrac is applied correctly\n    assert cb._extend_patches[0].get_extents().height == cb.ax.get_position().height * 0.2\n    assert cb._extend_patches[1].get_extents().height == cb.ax.get_position().height * 0.2\n", "def test_colorbar_set_alpha():\n    fig, ax = plt.subplots()\n    data = np.random.randn(10, 10)\n    pc = ax.pcolormesh(data)\n    cb = fig.colorbar(pc)\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n    assert cb.solids.get_alpha() == 0.5\n    assert cb.outline.get_alpha() == 0.5\n    for extpatch in cb._extend_patches:\n        assert extpatch.get_alpha() == 0.5\n", "def test_multiple_colorbars():\n    fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n    im1 = axs[0].imshow(np.random.random((10, 20)))\n    im2 = axs[1].imshow(np.random.random((10, 20)))\n    im3 = axs[2].imshow(np.random.random((10, 20)))\n    fig.colorbar(im1, ax=axs[0])\n    fig.colorbar(im2, ax=axs[1])\n    fig.colorbar(im3, ax=axs[2])\n"], "sample_814": ["def test_gbr_degenerate_feature_importances_with_sample_weight():\n    # growing an ensemble of single node trees with sample weights. See #13620\n    X = np.zeros((10, 10))\n    y = np.ones((10,))\n    sample_weight = np.ones((10,))\n    gbr = GradientBoostingRegressor().fit(X, y, sample_weight=sample_weight)\n    assert_array_equal(gbr.feature_importances_,\n                       np.zeros(10, dtype=np.float64))\n", "def test_gradient_boosting_feature_importances_shape():\n    # Test that feature_importances_ has the correct shape\n    X, y = make_classification(n_features=20, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=10, random_state=0)\n    gbc.fit(X, y)\n    assert_equal(gbc.feature_importances_.shape, (20,))\n\n    gbr = GradientBoostingRegressor(n_estimators=10, random_state=0)\n    gbr.fit(X, y)\n    assert_equal(gbr.feature_importances_.shape, (20,))\n", "def test_presort_auto_sparse():\n    # Test that presort='auto' works with sparse matrices.\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n    X_sparse = csr_matrix(X)\n    est = GradientBoostingClassifier(n_estimators=10, random_state=0, presort='auto')\n    est.fit(X_sparse, y)\n    assert_equal(est.estimators_.shape[0], 10)\n", "def test_gradient_boosting_with_presort():\n    # Check that presort='auto' works as expected\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               random_state=42)\n    X_sparse = csr_matrix(X)\n\n    gbc_auto = GradientBoostingClassifier(n_estimators=10, random_state=0,\n                                          presort='auto')\n    gbc_auto.fit(X_sparse, y)\n    assert gbc_auto.estimators_[0, 0].tree_.presort == \"auto\"\n\n    gbc_true = GradientBoostingClassifier(n_estimators=10, random_state=0,\n                                          presort=True)\n    gbc_true.fit(X, y)\n    assert gbc_true.estimators_[0, 0].tree_.presort == \"first\"\n\n    gbc_false = GradientBoostingClassifier(n_estimators=10, random_state=0,\n                                           presort=False)\n    gbc_false.fit(X, y)\n    assert gbc_false.estimators_[0, 0].tree_.presort == \"nothing\"\n", "def test_gbr_degenerate_estimators():\n    # growing an ensemble of zero estimators. See #13620\n    X = np.zeros((10, 10))\n    y = np.ones((10,))\n    gbr = GradientBoostingRegressor(n_estimators=0)\n    with pytest.raises(ValueError, match='n_estimators must be greater than 0'):\n        gbr.fit(X, y)\n"], "sample_903": ["def test_tsne_with_sparse_input():\n    \"\"\"Make sure that TSNE works with sparse input\"\"\"\n    random_state = check_random_state(0)\n    n_components = 2\n    X = random_state.randn(50, 10).astype(np.float32)\n    X_sparse = sp.csr_matrix(X)\n    X_transformed_tsne = TSNE(n_components=n_components, random_state=0, method='exact').fit_transform(X_sparse)\n    assert X_transformed_tsne.shape == (50, n_components)\n", "def test_tsne_with_precomputed_sparse_matrix():\n    \"\"\"Make sure that TSNE works with precomputed sparse matrix\"\"\"\n    random_state = check_random_state(0)\n    n_samples = 50\n    X = random_state.randn(n_samples, 3).astype(np.float32)\n    D = pairwise_distances(X)\n    D_sparse = sp.csr_matrix(D)\n    X_transformed_tsne = TSNE(metric='precomputed', random_state=0).fit_transform(D_sparse)\n    X_transformed_tsne_dense = TSNE(metric='precomputed', random_state=0).fit_transform(D)\n    assert_array_equal(X_transformed_tsne, X_transformed_tsne_dense)\n", "def test_tsne_with_different_perplexities():\n    \"\"\"Make sure that TSNE works for different perplexities\"\"\"\n    random_state = check_random_state(0)\n    n_components_original = 3\n    n_components_embedding = 2\n    X = random_state.randn(50, n_components_original).astype(np.float32)\n    perplexities = [5, 20, 50]\n    for perplexity in perplexities:\n        X_transformed = TSNE(n_components=n_components_embedding, perplexity=perplexity, random_state=0).fit_transform(X)\n        assert_array_equal(X_transformed.shape, (50, n_components_embedding))\n        assert np.isfinite(X_transformed).all()\n", "def test_tsne_with_different_perplexities():\n    \"\"\"Make sure that TSNE works for different perplexity values\"\"\"\n    random_state = check_random_state(0)\n    n_components_original = 3\n    n_components_embedding = 2\n    X = random_state.randn(50, n_components_original).astype(np.float32)\n    perplexities = [5, 20, 35]\n    for perplexity in perplexities:\n        X_transformed_tsne = TSNE(\n            n_components=n_components_embedding,\n            random_state=0,\n            perplexity=perplexity).fit_transform(X)\n        # Add assertion to check the output shape and type\n        assert X_transformed_tsne.shape == (50, n_components_embedding)\n        assert isinstance(X_transformed_tsne, np.ndarray)\n        assert X_transformed_tsne.dtype == np.float32\n", "def test_tsne_with_pca_initialization():\n    \"\"\"Make sure that TSNE works with PCA initialization\"\"\"\n    random_state = check_random_state(0)\n    n_components_original = 10\n    n_components_embedding = 2\n    X = random_state.randn(50, n_components_original).astype(np.float32)\n    X_transformed_tsne = TSNE(\n        init=\"pca\", n_components=n_components_embedding,\n        random_state=0).fit_transform(X)\n    pca = PCA(n_components=n_components_embedding, svd_solver='randomized',\n              random_state=0)\n    X_transformed_pca = pca.fit_transform(X)\n    X_transformed_tsne_from_pca = TSNE(\n        init=X_transformed_pca, n_components=n_components_embedding,\n        random_state=0).fit_transform(X)\n    assert_array_equal(X_transformed_tsne, X_transformed_tsne_from_pca)\n"], "sample_1084": ["def test_rationals_intersection():\n    assert S.Integers.intersect(S.Rationals) == S.Integers\n    assert S.Naturals.intersect(S.Rationals) == S.Naturals\n    assert S.Naturals0.intersect(S.Rationals) == S.Naturals0\n    assert S.Rationals.intersect(S.Reals) == S.Rationals\n    assert S.Rationals.intersect(Interval(0, 10)) == Intersection(S.Rationals, Interval(0, 10))\n    assert S.Rationals.intersect(FiniteSet(2, 3, 5, 7)) == FiniteSet(2, 3, 5, 7)\n", "def test_Rationals_intersection_Naturals():\n    assert S.Rationals.intersect(S.Naturals) == S.Naturals\n    assert S.Rationals.intersect(S.Naturals0) == S.Naturals0\n    assert S.Rationals.intersect(S.Integers) == S.Integers\n    assert S.Rationals.intersect(S.Reals) == S.Rationals\n", "def test_ComplexRegion_contains_issue_16867():\n    assert (1 + I) not in ComplexRegion(Interval(0, 1) * Interval(0, 2 * S.Pi), polar=True)\n", "def test_issue_17657():\n    assert ImageSet(Lambda(x, x), EmptySet()) == EmptySet()\n", "def test_issue_17471():\n    # This is a test for issue #17471, where the ImageSet constructor was not returning\n    # an unevaluated object when the base set was an empty set, and the lambda function\n    # was not fully simplified.\n    empty = Intersection(FiniteSet(log(2)/pi), S.Integers)\n    expected = ImageSet(Lambda(x, 1), empty)\n    actual = ImageSet(Lambda(x, 1), empty)\n    assert actual == expected\n"], "sample_1132": ["def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_has_variety():\n    assert has_variety((1, 2, 1))\n    assert not has_variety((1, 1, 1))\n", "def test_ordered_partitions_m_gt_n():\n    assert list(ordered_partitions(5, 7)) == []\n", "def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n    assert has_variety((1, 2, 3)) is True\n    assert has_variety(('a', 'a', 'b')) is True\n    assert has_variety(('a', 'a', 'a')) is False\n", "def test_multiset():\n    assert multiset([1, 2, 2]) == {1: 1, 2: 2}\n    assert multiset('aab') == {'a': 2, 'b': 1}\n    assert multiset([1, 2, 2], 2) == {1: 1, 2: 2}\n    assert multiset('aab', 2) == {'a': 2, 'b': 1}\n    assert multiset('aab', 1) == {'a': 1, 'b': 1}\n    raises(ValueError, lambda: multiset([1, 2, 2], 3))\n"], "sample_554": ["def test_annotation_antialiased_with_math():\n    annot = Annotation(r\"$e^{i\\pi} + 1 = 0$\", (.5, .5), antialiased=True)\n    assert annot._antialiased is True\n    assert annot.get_antialiased() == annot._antialiased\n\n    annot2 = Annotation(r\"$e^{i\\pi} + 1 = 0$\", (.5, .5), antialiased=False)\n    assert annot2._antialiased is False\n    assert annot2.get_antialiased() == annot2._antialiased\n\n    annot3 = Annotation(r\"$e^{i\\pi} + 1 = 0$\", (.5, .5), antialiased=False)\n    annot3.set_antialiased(True)\n    assert annot3.get_antialiased() is True\n    assert annot3._antialiased is True\n\n    annot4 = Annotation(r\"$e^{i\\pi} + 1 = 0$\", (.5, .5))\n    assert annot4._antialiased == mpl.rcParams['text.antialiased']\n", "def test_text_antialiased_with_usetex():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, r\"$\\sqrt{2}$\", antialiased=True, usetex=True)\n    fig.canvas.draw()\n    assert txt._antialiased is True\n    assert txt.get_antialiased() == txt._antialiased\n\n    fig2, ax2 = plt.subplots()\n    txt2 = ax2.text(0.5, 0.5, r\"$\\sqrt{2}$\", antialiased=False, usetex=True)\n    fig2.canvas.draw()\n    assert txt2._antialiased is False\n    assert txt2.get_antialiased() == txt2._antialiased\n", "def test_text_rotation_mode_anchor():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, 'Text', rotation=30, rotation_mode='anchor')\n    fig.canvas.draw()\n    # Add assertions to verify the expected behavior\n", "def test_text_antialiased_with_usetex():\n    # Test that antialiased parameter works with usetex\n    fig, ax = plt.subplots()\n    text_coords = ['offset points', 'offset fontsize']\n    # 10 points should be equal to 1 fontsize unit at fontsize=10\n    xy_text = [(10, 10), (1, 1)]\n    anns = [ax.annotate('$test$', xy=(0.5, 0.5),\n                        xytext=xy_text[i],\n                        fontsize='10',\n                        xycoords='data',\n                        textcoords=text_coords[i],\n                        usetex=True,\n                        antialiased=True) for i in range(2)]\n    fig.canvas.draw()\n    # Check that antialiased parameter has no effect with usetex\n    points_coords, fontsize_coords = [ann.get_window_extent() for ann in anns]\n    assert str(points_coords) == str(fontsize_coords)\n\n    # Test that antialiased parameter is ignored with usetex\n    anns = [ax.annotate('$test$', xy=(0.5, 0.5),\n                        xytext=xy_text[i],\n                        fontsize='10',\n                        xycoords='data',\n                        textcoords=text_coords[i],\n                        usetex=True,\n                        antialiased=False) for i in range(2)]\n    fig.canvas.draw()\n    # Check that antialiased parameter has no effect with usetex\n    points_coords, fontsize_coords = [ann.get_window_extent() for ann in anns]\n    assert str(points_coords) == str(fontsize_coords)\n", "def test_annotation_clip():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\n        \"hello\", xy=(.4, .4), xytext=(.6, .6), arrowprops={\"arrowstyle\": \"->\"},\n        annotation_clip=True)\n    fig.canvas.draw()  # Needed for the same reason as in test_contains.\n    event = MouseEvent(\n        \"button_press_event\", fig.canvas, *ax.transData.transform((1.5, 0.6)))\n    assert not ann.contains(event)[0]\n"], "sample_188": ["def test_expression_wrapper_source_expression(self):\n    expr = ExpressionWrapper(Lower(Value('f')), output_field=IntegerField())\n    self.assertIsInstance(expr.source_expression, Func)\n    self.assertEqual(expr.source_expression.function, 'LOWER')\n    self.assertEqual(expr.source_expression.source_expressions[0].value, 'f')\n", "    def test_as_sql(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        sql, params = expr.as_sql(compiler=None, connection=None)\n        self.assertEqual(sql, '3')\n        self.assertEqual(params, [])\n", "def test_expression_wrapper_with_annotated_expression(self):\n    Company.objects.create(name='Test Company', num_employees=50, num_chairs=25, ceo=self.max)\n    qs = Company.objects.annotate(\n        double_employees=ExpressionWrapper(F('num_employees') * 2, output_field=IntegerField())\n    ).filter(double_employees__gt=F('num_chairs'))\n    self.assertEqual(qs.count(), 1)\n", "def test_expression_wrapper_as_sql(self):\n    expr = ExpressionWrapper(Lower(Value('F')), output_field=CharField())\n    sql, params = expr.as_sql(mock.MagicMock(), connection)\n    self.assertEqual(sql, 'LOWER(%s)')\n    self.assertEqual(params, ['F'])\n", "def test_subquery_with_ordering(self):\n    # The order_by() clause of the subquery shouldn't affect the result.\n    numbers = Number.objects.all().order_by('pk')\n    inner = numbers.values('pk').order_by('integer')\n    outer = numbers.annotate(pk_subquery=Subquery(inner[:1], output_field=IntegerField()))\n    self.assertEqual(outer.values_list('pk_subquery', flat=True), numbers.values_list('pk', flat=True))\n"], "sample_478": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            list_editable = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'list_editable' must be a list or tuple.\",\n            \"admin.E120\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"not_a_tuple\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_invalid_view_on_site_url(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"not_a_callable_or_boolean\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n"], "sample_1102": ["def test_issue_18474():\n    p = poly(x**2 - y**2)\n    q = poly(x - y)\n    assert p.div(q) == (poly(x + y), poly(0))\n", "def test_issue_18679():\n    f = Poly(x**2 - 2, x)\n    g = Poly(x - sqrt(2), x)\n    assert f.compose(g) == 0\n", "def test_issue_18365():\n    p = poly(x - 1)\n    assert p.div(poly(x - 1)) == (Poly(1, x), Poly(0, x))\n", "def test_issue_18876():\n    p = Poly(x**2 - 2, x)\n    assert p.quo_rem(2) == (Poly(x**2/2 - 1, x), Poly(0, x))\n", "def test_issue_8695_edge_cases():\n    # Test with a polynomial that has no repeated factors\n    p = (x**2 + 1) * (x - 1) * (x - 2) * (x - 3)\n    result = (1, [(x**2 + 1, 1), (x - 1, 1), (x - 2, 1), (x - 3, 1)])\n    assert sqf_list(p) == result\n\n    # Test with a polynomial that is a constant\n    p = 5\n    result = (5, [(1, 1)])\n    assert sqf_list(p) == result\n\n    # Test with a polynomial that has a factor with a negative coefficient\n    p = (x**2 + 1) * (x + 1) * (x - 2) * (x - 3)\n    result = (1, [(x**2 + 1, 1), (x + 1, 1), (x - 2, 1), (x - 3, 1)])\n    assert sqf_list(p) == result\n"], "sample_462": ["def test_choicefield_integer_coercion(self):\n    f = ChoiceField(choices=[(1, \"One\"), (2, \"Two\")], coerce=int)\n    self.assertEqual(1, f.clean(1))\n    self.assertEqual(1, f.clean(\"1\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_with_int_choices(self):\n    f = ChoiceField(choices=[(1, \"One\"), (2, \"Two\")])\n    self.assertEqual(\"1\", f.clean(1))\n    self.assertEqual(\"1\", f.clean(\"1\"))\n    self.assertEqual(\"2\", f.clean(2))\n    self.assertEqual(\"2\", f.clean(\"2\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_enumeration_invalid_input(self):\n    class FirstNames(models.TextChoices):\n        JOHN = \"J\", \"John\"\n        PAUL = \"P\", \"Paul\"\n\n    f = ChoiceField(choices=FirstNames)\n    msg = \"'Select a valid choice. John is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"John\")\n", "def test_choicefield_with_custom_error_messages(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], error_messages={\"required\": \"Field is required.\", \"invalid_choice\": \"Invalid choice.\"})\n    with self.assertRaisesMessage(ValidationError, \"Field is required.\"):\n        f.clean(\"\")\n    msg = \"Invalid choice. 3 is not one of the available choices.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_with_iterable_choices(self):\n    iterable_choices = iter([(\"1\", \"One\"), (\"2\", \"Two\")])\n    f = ChoiceField(choices=iterable_choices)\n    self.assertEqual(\"1\", f.clean(1))\n    self.assertEqual(\"2\", f.clean(\"2\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n"], "sample_633": ["def test_hide_code_with_signatures() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\"] + 2 * [HIDE_CODE_WITH_SIGNATURES])\n    assert ex.value.code == 0\n    assert \"TOTAL lines=32 duplicates=16 percent=50.00\" not in output.getvalue()\n", "def test_ignore_signatures_async_functions_pass() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_signatures_empty_functions_with_docstring_fail() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''", "def test_ignore_signatures_with_empty_functions() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_hide_code_with_signatures() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", HIDE_CODE_WITH_IMPORTS, HIDE_CODE_WITH_IMPORTS])\n    assert ex.value.code == 0\n    assert \"TOTAL lines=32 duplicates=0 percent=0.00\" in output.getvalue()\n"], "sample_930": ["def test_create_index_with_multiple_entries(app):\n    text = (\".. index:: docutils\\n\"\n            \".. index:: docutils; reStructuredText\\n\"\n            \".. index:: Python\\n\"\n            \".. index:: Python; interpreter\\n\"\n            \".. index:: Sphinx\\n\"\n            \".. index:: Sphinx; documentation tool\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 3\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')],\n                                            [('reStructuredText', [('', '#index-1')])], None])])\n    assert index[1] == ('P', [('Python', [[('', '#index-2')],\n                                          [('interpreter', [('', '#index-3')])], None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-4')],\n                                          [('documentation tool', [('', '#index-5')])], None])])\n", "def test_create_index_without_grouping(app):\n    text = (\".. index:: func() (in module foo)\\n\"\n            \".. index:: func() (in module bar)\\n\"\n            \".. index:: another_func() (in module baz)\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 3\n    assert index[0] == ('F', [('func() (in module foo)', [[('', '#index-0')], [], None]),\n                              ('func() (in module bar)', [[('', '#index-1')], [], None])])\n    assert index[1] == ('A', [('another_func() (in module baz)', [[('', '#index-2')], [], None])])\n", "def test_create_index_without_subentry(app):\n    text = (\".. index:: single: docutils; \\n\"\n            \".. index:: single: Python; \\n\"\n            \".. index:: pip; \\n\"\n            \".. index:: Sphinx; \\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], {}, None])])\n    assert index[1] == ('P', [('pip', [[('', '#index-2')], {}, None]),\n                              ('Python', [[('', '#index-1')], {}, None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-3')], {}, None])])\n", "def test_create_index_without_uri(app, monkeypatch):\n    # Create a mock builder that raises NoUri error for get_relative_uri method\n    class MockBuilder:\n            raise NoUri('Mock NoUri error')\n\n    # Monkeypatch the builder for the app\n    monkeypatch.setattr(app, 'builder', MockBuilder())\n\n    text = (\".. index:: docutils\\n\"\n            \".. index:: Python\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # Check that the index is created correctly even when NoUri error is raised\n    assert len(index) == 2\n    assert index[0] == ('D', [('docutils', [[], [], None])])\n    assert index[1] == ('P', [('Python', [[], [], None])])\n", "def test_create_index_with_unicode_sorting(app):\n    text = (\".. index:: \u0415\u043b\u044c\\n\"\n            \".. index:: \u0417\u0435\u043c\u043b\u044f\\n\"\n            \".. index:: \u0437\u0435\u043c\u043b\u044f\\n\"\n            \".. index:: \u0451\u043b\u043a\u0430\\n\"\n            \".. index:: \u0401\u043b\u043a\u0430\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 2\n    assert index[0] == ('\u0415', [('\u0451\u043b\u043a\u0430', [[('', '#index-3')], [], None]),\n                              ('\u0401\u043b\u043a\u0430', [[('', '#index-4')], [], None]),\n                              ('\u0415\u043b\u044c', [[('', '#index-0')], [], None])])\n    assert index[1] == ('\u0417', [('\u0417\u0435\u043c\u043b\u044f', [[('', '#index-1')], [], None]),\n                              ('\u0437\u0435\u043c\u043b\u044f', [[('', '#index-2')], [], None])])\n"], "sample_317": ["def test_atom_feed_author_elements(self):\n    \"\"\"\n    Test the author elements in Atom feed.\n    \"\"\"\n    response = self.client.get('/syndication/atom/')\n    feed = minidom.parseString(response.content).firstChild\n    entries = feed.getElementsByTagName('entry')\n\n    author = entries[0].getElementsByTagName('author')[0]\n    self.assertChildNodes(author, ['name', 'email', 'uri'])\n    self.assertChildNodeContent(author, {\n        'name': 'Sally Smith',\n        'email': 'test@example.com',\n        'uri': 'http://example.com/blog/',\n    })\n", "def test_atom_feed_author_elements(self):\n    \"\"\"\n    Test the author elements in the Atom feed.\n    \"\"\"\n    response = self.client.get('/syndication/atom/')\n    feed = minidom.parseString(response.content).firstChild\n\n    entries = feed.getElementsByTagName('entry')\n    for entry in entries:\n        author = entry.getElementsByTagName('author')[0]\n        self.assertChildNodes(author, ['name', 'email', 'uri'])\n        self.assertChildNodeContent(author, {\n            'name': 'Sally Smith',\n            'email': 'test@example.com',\n            'uri': 'http://example.com/',\n        })\n\n    author = feed.getElementsByTagName('author')[0]\n    self.assertChildNodes(author, ['name', 'email', 'uri'])\n    self.assertChildNodeContent(author, {\n        'name': 'Sally Smith',\n        'email': 'test@example.com',\n        'uri': 'http://example.com/',\n    })\n", "def test_item_author_info(self):\n    \"\"\"\n    Test the author information in the feed items.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/')\n    doc = minidom.parseString(response.content)\n    chan = doc.getElementsByTagName('channel')[0]\n    items = chan.getElementsByTagName('item')\n\n    self.assertChildNodeContent(items[0], {\n        'author': 'Sally Smith (sally@example.com)',\n    })\n    self.assertChildNodeContent(items[1], {\n        'author': 'John Doe (john@example.com)',\n    })\n    self.assertChildNodeContent(items[2], {\n        'dc:creator': 'Jane Smith',\n    })\n", "def test_atom_feed_with_author(self):\n    \"\"\"\n    Test the author information in an Atom feed.\n    \"\"\"\n    response = self.client.get('/syndication/atom_with_author/')\n    feed = minidom.parseString(response.content).firstChild\n\n    author = feed.getElementsByTagName('author')[0]\n    self.assertChildNodes(author, ['name', 'email', 'uri'])\n    self.assertChildNodeContent(author, {\n        'name': 'Test Author',\n        'email': 'test@example.com',\n        'uri': 'http://example.com/author',\n    })\n\n    entries = feed.getElementsByTagName('entry')\n    for entry in entries:\n        author = entry.getElementsByTagName('author')[0]\n        self.assertChildNodes(author, ['name', 'email', 'uri'])\n        self.assertChildNodeContent(author, {\n            'name': 'Entry Author',\n            'email': 'entry@example.com',\n            'uri': 'http://example.com/entry_author',\n        })\n", "def test_rss2_feed_with_author_name(self):\n    \"\"\"\n    Test the author name is included in the RSS feed.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/author_name/')\n    doc = minidom.parseString(response.content)\n    chan = doc.getElementsByTagName('rss')[0].getElementsByTagName('channel')[0]\n    items = chan.getElementsByTagName('item')\n    for item in items:\n        self.assertEqual(item.getElementsByTagName('author')[0].firstChild.nodeValue, 'Sally Smith')\n"], "sample_216": ["def test_add_non_null_textfield_and_charfield(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Adding a NOT NULL `CharField` or `TextField` without default\n    should prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_biography_non_null])\n    self.assertEqual(mocked_ask_method.call_count, 2)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_add_model_with_field_renamed_from_base_model(self):\n    \"\"\"\n    Renaming a base field takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('new_title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='readable', old_name='title', new_name='new_title')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_create_model_with_field_removed_from_base_model_2(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name when base model is not the first\n    model to be created.\n    \"\"\"\n    before = [\n        ModelState('app', 'person', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.person',)),\n    ]\n    after = [\n        ModelState('app', 'person', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.person',)),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='person')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_rename_related_field_with_db_column(self):\n    \"\"\"\n    Tests renaming of a related field with db_column specified while\n    simultaneously renaming the model.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_book, self.book],\n        [self.author_renamed_with_book, self.book_with_field_and_author_renamed_db_column],\n        MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n    # Right number/type of migrations for related field rename?\n    # Alter is already taken care of.\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"RenameField\", \"AlterField\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, old_name=\"author\", new_name=\"writer\")\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, model_name=\"book\", name=\"writer\")\n", "def test_add_model_with_field_added_to_base_model(self):\n    \"\"\"\n    Adding a new inherited model with a field of the same name as a removed\n    base field takes place after removing the base field.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AddField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n"], "sample_1110": ["def test_NumPyPrinter_log2():\n    from sympy import log2\n\n    expr = log2(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(x)/numpy.log(2)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(x)/numpy.log(2)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log(x)/mpmath.log(2)'\n", "def test_lambda_printer():\n    from sympy import symbols, Lambda, lambdify\n\n    x, y = symbols('x y')\n    expr = Lambda((x, y), x**2 + y**2)\n    printer = PythonCodePrinter()\n    assert printer.doprint(expr) == 'Lambda((x, y), x**2 + y**2)'\n\n    f = lambdify((x, y), expr, \"numpy\")\n    assert f(2, 3) == 13\n", "def test_SymPyPrinter():\n    from sympy import Lambda, Function\n    p = SymPyPrinter()\n    assert p.doprint(Lambda(x, x**2)) == 'Lambda(x, x**2)'\n    assert p.doprint(Function('f')(x)) == 'f(x)'\n", "def test_log1p():\n    from sympy import log1p\n\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(x + 1)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x + 1)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log1p(x)'\n", "def test_log1p():\n    from sympy import log1p\n\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(x + 1)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x + 1)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log1p(x)'\n"], "sample_1032": ["def test_sqrt_cbrt_root_evaluate():\n    from sympy import sqrt, cbrt, root\n    assert sqrt(4, evaluate=True) == 2\n    assert cbrt(27, evaluate=True) == 3\n    assert root(8, 3, evaluate=True) == 2\n", "def test_root_evaluation():\n    from sympy.abc import x\n    assert sqrt(x**2) == x\n    assert sqrt(x**2, evaluate=False) == sqrt(x**2)\n    assert cbrt(x**3) == x\n    assert cbrt(x**3, evaluate=False) == cbrt(x**3)\n    assert root(x**4, 4) == x\n    assert root(x**4, 4, evaluate=False) == root(x**4, 4)\n", "def test_minmax_rewrite_as_Piecewise_complex():\n    from sympy import symbols, Piecewise, I\n    x, y, z, a, b = symbols('x y z a b')\n    assert Min(a, b).rewrite(Piecewise) == Min(a, b)\n    assert Max(x, y, a, b).rewrite(Piecewise) == Max(x, y, a, b)\n    assert Min(x, y, I).rewrite(Piecewise) == Min(x, y, I)\n    assert Max(x, y, I).rewrite(Piecewise) == Max(x, y, I)\n", "def test_minmax_with_numbers():\n    assert Min(3, 5) == 3\n    assert Max(3, 5) == 5\n    assert Min(-3, -5) == -5\n    assert Max(-3, -5) == -3\n    assert Min(0, 5) == 0\n    assert Max(0, 5) == 5\n    assert Min(3, 3) == 3\n    assert Max(3, 3) == 3\n    assert Min(3, 3, 5) == 3\n    assert Max(3, 3, 5) == 5\n    assert Min(3, -5, 0) == -5\n    assert Max(3, -5, 0) == 3\n", "def test_minmax_eval():\n    from sympy import symbols\n    x, y = symbols('x y')\n\n    assert Min(x, y).evalf(subs={x: 2, y: 3}) == 2\n    assert Min(x, y).evalf(subs={x: 3, y: 2}) == 2\n    assert Max(x, y).evalf(subs={x: 2, y: 3}) == 3\n    assert Max(x, y).evalf(subs={x: 3, y: 2}) == 3\n\n    # Testing with complex numbers\n    assert Min(x, y).evalf(subs={x: 2 + 3j, y: 3 + 4j}) == 2 + 3j\n    assert Max(x, y).evalf(subs={x: 2 + 3j, y: 3 + 4j}) == 3 + 4j\n"], "sample_363": ["def test_select_multiple_widget_can_change_related(self):\n    rel = Individual._meta.get_field('parent').remote_field\n    widget = forms.SelectMultiple()\n    wrapper = widgets.RelatedFieldWidgetWrapper(\n        widget, rel, widget_admin_site,\n        can_add_related=True,\n        can_change_related=False,\n        can_delete_related=True,\n    )\n    self.assertTrue(wrapper.can_add_related)\n    self.assertTrue(wrapper.can_change_related)\n    self.assertTrue(wrapper.can_delete_related)\n", "def test_proper_manager_for_foreign_key_label_lookup(self):\n    # see #9258\n    rel = Inventory._meta.get_field('parent').remote_field\n    w = widgets.ForeignKeyRawIdWidget(rel, widget_admin_site)\n\n    hidden = Inventory.objects.create(\n        barcode=93, name='Hidden', hidden=True\n    )\n    child_of_hidden = Inventory.objects.create(\n        barcode=94, name='Child of hidden', parent=hidden\n    )\n    self.assertHTMLEqual(\n        w.render('test', child_of_hidden.parent_id, attrs={}),\n        '<input type=\"text\" name=\"test\" value=\"93\" class=\"vForeignKeyRawIdAdminField\">'\n        '<a href=\"/admin_widgets/inventory/?_to_field=barcode\" '\n        'class=\"related-lookup\" id=\"lookup_id_test\" title=\"Lookup\"></a>'\n        '&nbsp;<strong><a href=\"/admin_widgets/inventory/%(pk)s/change/\">'\n        'Hidden</a></strong>' % {'pk': hidden.pk}\n    )\n", "    def test_render_with_uploaded_file(self):\n        album = Album.objects.create(name='Test Album')\n        upload_file = SimpleUploadedFile(\"test_file.txt\", b\"file_content\")\n        album.cover_art = upload_file\n        album.save()\n\n        w = widgets.AdminFileWidget()\n        self.assertHTMLEqual(\n            w.render('test', album.cover_art),\n            '<p class=\"file-upload\">Currently: <a href=\"%(STORAGE_URL)salbums/test_file.txt\">test_file.txt</a> '\n            '<span class=\"clearable-file-input\">'\n            '<input type=\"checkbox\" name=\"test-clear\" id=\"test-clear_id\"> '\n            '<label for=\"test-clear_id\">Clear</label></span><br>'\n            'Change: <input type=\"file\" name=\"test\"></p>' % {\n                'STORAGE_URL': default_storage.url(''),\n            },\n        )\n", "    def test_can_change_delete_related(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_add'))\n\n        # Check if Change and Delete links are present for related objects\n        self.assertIsNotNone(self.selenium.find_element(By.ID, 'change_id_user'))\n        self.assertIsNotNone(self.selenium.find_element(By.ID, 'delete_id_user'))\n", "    def test_copy_button(self):\n        \"\"\"\n        The copy button copies the ID to the clipboard.\n        \"\"\"\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.common.keys import Keys\n\n        band = Band.objects.create(name='Linkin Park')\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_band_change', args=(band.id,)))\n\n        # Click the copy button\n        self.selenium.find_element(By.CLASS_NAME, 'copy-button').click()\n\n        # Paste the ID into a text input\n        text_input = self.selenium.find_element(By.ID, 'text-input')\n        text_input.send_keys(Keys.CONTROL, 'v')\n\n        # Check that the ID was copied correctly\n        self.assertEqual(text_input.get_attribute('value'), str(band.id))\n"], "sample_979": ["def test_MatrixSymbol_simplify():\n    A = MatrixSymbol('A', n, m)\n    assert A._eval_simplify() == A\n", "def test_MatrixElement_sympify():\n    A = MatrixSymbol('A', n, m)\n    assert MatrixElement(A, 0, 0) == A[0, 0]\n    assert MatrixElement('A', 0, 0) == MatrixElement(A, 0, 0)\n    assert MatrixElement(A, n, m) == A[n, m]\n    assert MatrixElement('A', n, m) == MatrixElement(A, n, m)\n", "def test_MatrixSymbol_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, k)\n\n    assert A.subs({A: B}) == B\n    assert A.subs({A: C}) == C\n    assert (A*B).subs({A: C}) == C*B\n    assert (A*B).subs({B: C}) == A*C\n    assert (A*B*C).subs({A: C, B: A}) == C*A*C\n", "def test_MatrixElement_matrix_diff():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, p)\n    C = A * B\n    dC = diff(C[k, l], A[i, j])\n    assert dC == B[j, l].diff(A[i, j])\n", "def test_matrix_element_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    i, j, k, l = symbols('i j k l')\n\n    assert A[i, j].subs({i: k, j: l}) == A[k, l]\n    assert (A*B)[i, j].subs({i: k, j: l}) == (A*B)[k, l]\n    assert (A*B)[i, j].subs({A: MatrixSymbol('C', n, m), B: MatrixSymbol('D', m, l)}) == (MatrixSymbol('C', n, m)*MatrixSymbol('D', m, l))[i, j]\n"], "sample_263": ["    def test_dumpdata_with_compressed_file_output(self):\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": \"News Stories\"}}, '\n            '{\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, '\n            '{\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]',\n            filename='dumpdata.json.gz'\n        )\n        self.assertTrue(os.path.exists('dumpdata.json.gz'))\n        with gzip.open('dumpdata.json.gz', 'rt') as f:\n            content = f.read()\n        self.assertIn('\"pk\": 1', content)\n        self.assertIn('\"pk\": 2', content)\n        self.assertIn('\"pk\": 3', content)\n        os.remove('dumpdata.json.gz')\n", "def test_dumpdata_with_invalid_foreign_key(self):\n    m = PrimaryKeyUUIDModel.objects.create()\n    with self.assertRaises(ValueError):\n        self._dumpdata_assert(\n            ['fixtures.Article'],\n            '',\n            primary_keys='invalid_uuid'\n        )\n", "def test_loaddata_invalid_format(self):\n    msg = \"Unknown serialization format: unknownformat\"\n    with self.assertRaisesMessage(management.CommandError, msg):\n        management.call_command('loaddata', 'fixture1', format='unknownformat')\n", "def test_dumpdata_with_missing_foreign_key(self):\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    article = Article.objects.create(headline='Test Article', pub_date='2006-06-17T12:00:00', category_id=999)\n    with self.assertRaisesMessage(ObjectDoesNotExist, 'Category matching query does not exist.'):\n        self._dumpdata_assert(\n            ['fixtures.Article'],\n            '[{\"pk\": %d, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Test Article\", '\n            '\"pub_date\": \"2006-06-17T12:00:00\", \"category\": 999}}]' % article.pk,\n        )\n", "def test_dumpdata_with_multiple_format_specifiers(self):\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    with self.assertRaisesMessage(CommandError, \"Multiple --format specifiers.\"):\n        self._dumpdata_assert(\n            ['fixtures'],\n            '',\n            format='json json'\n        )\n"], "sample_19": ["def test_swapaxes_different_val():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.crpix = [32.5, 16.5, 1.0]\n    w.wcs.crval = [5.63, -72.05, 1.0]\n    w.wcs.pc = [[5.9e-06, 1.3e-05, 0.0], [-1.2e-05, 5.0e-06, 0.0], [0.0, 0.0, 1.0]]\n    w.wcs.cdelt = [1.0, 1.0, 1.0]\n    w.wcs.set()\n    axes_order = [3, 1, 2]\n    axes_order0 = list(i - 1 for i in axes_order)\n    ws = w.sub(axes_order)\n    imcoord = np.array([3, 5, 7])\n    imcoords = imcoord[axes_order0]\n    val_ref = w.wcs_pix2world([imcoord], 0)[0]\n    val_swapped = ws.wcs_pix2world([imcoords], 0)[0]\n\n    # check original axis and swapped give different results\n    assert not np.allclose(val_ref[axes_order0], val_swapped, rtol=0, atol=1e-8)\n", "def test_dropaxis():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.set()\n    w_dropped = w.dropaxis(1)\n    assert w_dropped.naxis == 2\n    assert w_dropped.wcs.ctype == [\"RA---TAN\", \"FREQ\"]\n    w_dropped = w.dropaxis(0)\n    assert w_dropped.naxis == 2\n    assert w_dropped.wcs.ctype == [\"DEC--TAN\", \"FREQ\"]\n    w_dropped = w.dropaxis(2)\n    assert w_dropped.naxis == 2\n    assert w_dropped.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\"]\n", "def test_inconsistent_axis_types():\n    w = wcs.WCS(naxis=2)\n    w.wcs.axis_types = [0, 1]  # Inconsistent axis types\n    with pytest.raises(wcs.InconsistentAxisTypesError):\n        w.has_celestial\n", "def test_all_world2pix_non_integer_input():\n    \"\"\"Test all_world2pix with non-integer input\"\"\"\n\n    # Open test FITS file:\n    fname = get_pkg_data_filename(\"data/j94f05bgq_flt.fits\")\n    h = fits.open(fname)\n    w = wcs.WCS(h[(\"SCI\", 1)].header, h)\n    h.close()\n    del h\n\n    crpix = w.wcs.crpix\n    ncoord = crpix.shape[0]\n\n    # Assume that CRPIX is at the center of the image and that the image has\n    # a power-of-2 number of pixels along each axis. Only use the central\n    # 1/64 for this testing purpose:\n    naxesi_l = list((7.0 / 16 * crpix).astype(int))\n    naxesi_u = list((9.0 / 16 * crpix).astype(int))\n\n    # Generate integer indices of pixels (image grid):\n    img_pix = np.dstack(\n        [i.flatten() for i in np.meshgrid(*map(range, naxesi_l, naxesi_u))]\n    )[0]\n\n    # Generage random data (in image coordinates):\n    with NumpyRNGContext(123456789):\n        rnd_pix = np.random.rand(100, ncoord)\n\n    # Scale random data to cover the central part of the image\n    mwidth = 2 * (crpix * 1.0 / 8)\n    rnd_pix = crpix - 0.5 * mwidth + (mwidth - 1) * rnd_pix\n\n    # Reference pixel coordinates in image coordinate system (CS):\n    test_pix = np.append(img_pix, rnd_pix, axis=0)\n    # Reference pixel coordinates in sky CS using forward transformation:\n    all_world = w.all_pix2world(test_pix, 0)\n\n    # Test with non-integer input\n", "def test_footprint_contains_array():\n    \"\"\"\n    Test WCS.footprint_contains(skycoord) with an array of sky coordinates.\n    \"\"\"\n\n    header = \"\"\"\n    WCSAXES =                    2 / Number of coordinate axes\n    CRPIX1  =               1045.0 / Pixel coordinate of reference point\n    CRPIX2  =               1001.0 / Pixel coordinate of reference point\n    PC1_1   =    -0.00556448550786 / Coordinate transformation matrix element\n    PC1_2   =   -0.001042120133257 / Coordinate transformation matrix element\n    PC2_1   =    0.001181477028705 / Coordinate transformation matrix element\n    PC2_2   =   -0.005590809742987 / Coordinate transformation matrix element\n    CDELT1  =                  1.0 / [deg] Coordinate increment at reference point\n    CDELT2  =                  1.0 / [deg] Coordinate increment at reference point\n    CUNIT1  = 'deg'                / Units of coordinate increment and value\n    CUNIT2  = 'deg'                / Units of coordinate increment and value\n    CTYPE1  = 'RA---TAN'           / TAN (gnomonic) projection + SIP distortions\n    CTYPE2  = 'DEC--TAN'           / TAN (gnomonic) projection + SIP distortions\n    CRVAL1  =      250.34971683647 / [deg] Coordinate value at reference point\n    CRVAL2  =      2.2808772582495 / [deg] Coordinate value at reference"], "sample_30": ["def test_get_infos_by_name_empty():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n          <RESOURCE type=\"results\">\n          </RESOURCE>\n        </VOTABLE>\"\"\"\n        )\n    )\n    infos = vot.get_infos_by_name(\"creator-name\")\n    assert list(infos) == []\n", "def test_get_infos_by_name_single_result():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n          <RESOURCE type=\"results\">\n            <INFO name=\"creator-name\" value=\"Cannon, A.\"/>\n          </RESOURCE>\n        </VOTABLE>\"\"\"\n        )\n    )\n    infos = vot.get_infos_by_name(\"creator-name\")\n    assert [i.value for i in infos] == [\"Cannon, A.\"]\n", "def test_timesys_from_scratch(tmp_path):\n    from astropy.io.votable.tree import TimeSys, VOTableFile\n\n    votable = VOTableFile()\n    timesys = TimeSys(ID=\"custom_timesys\", timeorigin=58421.5, timescale=\"TT\", refposition=\"TOPOCENTER\")\n    votable.time_systems.append(timesys)\n\n    out_path = tmp_path / \"timesys_from_scratch.xml\"\n    votable.to_xml(out_path)\n\n    votable_reloaded = parse(out_path)\n    timesys_reloaded = votable_reloaded.get_timesys_by_id(\"custom_timesys\")\n    assert timesys_reloaded.timeorigin == 58421.5\n    assert timesys_reloaded.timescale == \"TT\"\n    assert timesys_reloaded.refposition == \"TOPOCENTER\"\n", "def test_timesys_version_1_2():\n    votable = parse(get_pkg_data_filename(\"data/timesys_version_1_2.xml\"))\n    assert len(list(votable.iter_timesys())) == 0\n", "def test_parse_large_table():\n    # Test parsing a large table to make sure it doesn't run out of memory\n    # or take too long.\n    num_rows = 100000\n    num_cols = 10\n\n    # Create a large VOTable file with num_rows and num_cols\n    votable = tree.VOTableFile()\n    resource = tree.Resource()\n    votable.resources.append(resource)\n    table = tree.Table(votable)\n    resource.tables.append(table)\n    for i in range(num_cols):\n        table.fields.append(\n            tree.Field(votable, ID=f\"col{i}\", name=f\"col{i}\", datatype=\"double\", arraysize=\"1\")\n        )\n    table.create_arrays(num_rows)\n    table.array[:] = np.random.rand(num_rows, num_cols)\n\n    # Write the VOTable file to a BytesIO object\n    output = io.BytesIO()\n    votable.to_xml(output)\n    output.seek(0)\n\n    # Parse the VOTable file and check that the array has the correct shape and values\n    parsed_votable = parse(output)\n    parsed_table = parsed_votable.get_first_table()\n    assert parsed_table.array.shape == (num_rows, num_cols)\n    assert np.allclose(parsed_table.array, table.array)\n"], "sample_458": ["def test_floatformat_negative(self):\n    output = self.engine.render_to_string(\n        \"floatformat_negative\", {\"a\": \"-1.42\", \"b\": mark_safe(\"-1.42\")}\n    )\n    self.assertEqual(output, \"-1.4 -1.4\")\n", "    def test_unlocalize_with_force_grouping(self):\n        with translation.override(\"de\", deactivate=True):\n            self.assertEqual(floatformat(66666.666, \"2g\"), \"66.666,7\")\n            self.assertEqual(floatformat(66666.666, \"2ug\"), \"66666.67\")\n            # Invalid suffix.\n            self.assertEqual(floatformat(66666.666, \"gu\"), \"66666.666\")\n", "def test_float_with_decimal_point(self):\n    self.assertEqual(floatformat(123456.789, 2), \"123456.79\")\n", "    def test_suffix_and_unlocalize(self):\n        with translation.override(\"de\", deactivate=True):\n            self.assertEqual(floatformat(66666.666, \"2g\"), \"66.666,7\")\n            self.assertEqual(floatformat(66666.666, \"2ug\"), \"66666.67\")\n            # Invalid suffix.\n            self.assertEqual(floatformat(66666.666, \"gu2\"), \"66666.666\")\n", "def test_invalid_input(self):\n    self.assertEqual(floatformat(\"abc\"), \"\")\n    self.assertEqual(floatformat(None), \"\")\n"], "sample_925": ["def test_mock_module_special_attributes():\n    mock = _MockModule('mocked_module')\n    assert mock.__file__ == os.devnull\n    assert mock.__path__ == []\n    assert mock.__all__ == []\n    assert mock.__sphinx_mock__ == True\n", "def test_MockObject_iteration():\n    mock = _MockObject()\n    # __iter__ should return an empty iterator\n    assert len(list(iter(mock))) == 0\n    # __contains__ should always return False\n    assert 'some_attr' not in mock\n    # __len__ should always return 0\n    assert len(mock) == 0\n", "def test_MockObject_attributes():\n    attributes = {'attribute1': 'value1', 'attribute2': 'value2'}\n    mock = _MockObject(attributes=attributes)\n\n    assert mock.attribute1 == 'value1'\n    assert mock.attribute2 == 'value2'\n\n    # Attributes should not be shared between instances\n    new_mock = _MockObject()\n    assert not hasattr(new_mock, 'attribute1')\n    assert not hasattr(new_mock, 'attribute2')\n", "def test_mock_object_attributes():\n    attributes = {'__test__': 'attribute', 'method': lambda: 'result'}\n    mock = _MockObject(attributes=attributes)\n\n    assert mock.__test__ == 'attribute'\n    assert mock.method() == 'result'\n", "def test_mock_object_special_methods():\n    mock = _MockObject()\n    assert len(mock) == 0\n    assert 'key' not in mock\n    assert list(mock) == []\n    assert mock.__mro_entries__(()) == (mock.__class__,)\n    assert isinstance(mock['key'], _MockObject)\n    assert repr(mock) == '_MockObject'\n"], "sample_506": ["def test_spines_bounds():\n    fig, ax = plt.subplots()\n    ax.spines['left'].set_bounds(-1, 1)\n    ax.spines['bottom'].set_bounds(-2, 2)\n    ax.set_xlim([-3, 3])\n    ax.set_ylim([-4, 4])\n", "def test_spines_bounds():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10))\n    ax.spines['left'].set_bounds(2, 8)\n    ax.spines['bottom'].set_bounds(1, 9)\n", "def test_spines_bounds():\n    fig, ax = plt.subplots()\n    ax.spines['left'].set_bounds(-1, 1)\n    ax.spines['bottom'].set_bounds(0, 2*np.pi)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.set_xlim(0, 2*np.pi)\n    ax.set_ylim(-1, 1)\n    ax.plot(np.linspace(0, 2*np.pi, 100), np.sin(np.linspace(0, 2*np.pi, 100)))\n", "def test_spines_bounds():\n    fig, ax = plt.subplots()\n    ax.spines.left.set_bounds(-1, 1)\n    ax.spines.bottom.set_bounds(0, 2)\n    ax.set_xlim([-2, 2])\n    ax.set_ylim([-2, 2])\n", "def test_spines_bounds():\n    fig, ax = plt.subplots()\n    ax.spines['left'].set_bounds(0, 1)\n    ax.spines['bottom'].set_bounds(0, 1)\n    ax.set_xlim([-2, 2])\n    ax.set_ylim([-2, 2])\n"], "sample_255": ["def test_threaded_wsgi_server(self):\n    \"\"\"ThreadedWSGIServer sets daemon_threads to True.\"\"\"\n    server = ThreadedWSGIServer(('localhost', 0), WSGIRequestHandler)\n    self.assertTrue(server.daemon_threads)\n", "def test_threaded_wsgi_server(self):\n    \"\"\"ThreadedWSGIServer behaves correctly.\"\"\"\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    server = ThreadedWSGIServer(('localhost', 0), WSGIRequestHandler)\n    try:\n        self.assertTrue(server.daemon_threads)\n        self.assertIsInstance(server, socketserver.ThreadingMixIn)\n        # Additional assertions can be added to test specific behavior of ThreadedWSGIServer\n    finally:\n        server.server_close()\n", "def test_threaded_wsgi_server(self):\n    \"\"\"ThreadedWSGIServer daemonizes its threads.\"\"\"\n    server = ThreadedWSGIServer(('localhost', 0), WSGIRequestHandler)\n    self.assertTrue(server.daemon_threads)\n    server.server_close()\n", "    def test_threading_server(self):\n        \"\"\"ThreadedWSGIServer is a subclass of WSGIServer with ThreadingMixIn.\"\"\"\n        server = ThreadedWSGIServer(('localhost', 0), WSGIRequestHandler)\n        self.assertIsInstance(server, socketserver.ThreadingMixIn)\n        self.assertIsInstance(server, WSGIServer)\n        server.server_close()\n", "def test_threaded_server(self):\n    \"\"\"ThreadedWSGIServer handles multiple requests concurrently.\"\"\"\n        start_response('200 OK', [])\n        return [b'Hello World']\n\n    server = ThreadedWSGIServer(('localhost', 0), WSGIRequestHandler)\n    server.set_app(simple_app)\n    server_address = server.server_address\n\n    # Send multiple requests concurrently\n    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n        future_to_url = {executor.submit(requests.get, 'http://{}:{}/'.format(*server_address)): 'request1',\n                         executor.submit(requests.get, 'http://{}:{}/'.format(*server_address)): 'request2'}\n        for future in concurrent.futures.as_completed(future_to_url):\n            url = future_to_url[future]\n            response = future.result()\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.text, 'Hello World')\n\n    server.shutdown()\n    server.server_close()\n"], "sample_480": ["def test_key_transform_numeric_lookups(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__gt=4),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__gte=4.2),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__lt=5),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__lte=4.2),\n        [self.objs[4]],\n    )\n", "def test_key_transform_with_integer_key(self):\n    obj = NullableJSONModel.objects.create(value={\"123\": \"value\"})\n    self.assertEqual(\n        NullableJSONModel.objects.filter(value__123=\"value\").get(), obj\n    )\n", "def test_key_text_transform_from_lookup_numbers(self):\n    qs = NullableJSONModel.objects.annotate(c=KT(\"value__1__1\")).filter(\n        c__contains=\"2\",\n    )\n    self.assertSequenceEqual(qs, [self.objs[5]])\n", "def test_contains_with_key_transform(self):\n    tests = [\n        (\"value__d__contains\", KeyTransform(\"f\", \"value\")),\n        (\"value__contains\", KeyTransform(\"foo\", KeyTransform(\"bax\", \"value\"))),\n        (\"value__contains\", F(\"value__bax__foo\")),\n        (\"value__baz__contains\", {\"c\": \"d\"}),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertIs(\n                NullableJSONModel.objects.filter(\n                    **{lookup: value},\n                ).exists(),\n                True,\n            )\n", "def test_key_transform_numeric_lookups(self):\n    tests = [\n        (\"value__p__lt\", 5),\n        (\"value__p__lte\", 4.2),\n        (\"value__p__gt\", 4),\n        (\"value__p__gte\", 4.2),\n        (\"value__c__lt\", 10),\n        (\"value__c__lte\", 14),\n        (\"value__c__gt\", 13),\n        (\"value__c__gte\", 14),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertIs(\n                NullableJSONModel.objects.filter(\n                    **{lookup: value},\n                ).exists(),\n                True,\n            )\n"], "sample_661": ["def test_logging_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    assert len(node.find_by_tag(\"system-err\")) == 1\n    assert len(node.find_by_tag(\"system-out\")) == 1\n", "def test_record_testsuite_property_multiple_calls(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats1\", \"value1\")\n            record_testsuite_property(\"stats2\", \"value2\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p1_node.assert_attr(name=\"stats1\", value=\"value1\")\n    p2_node.assert_attr(name=\"stats2\", value=\"value2\")\n", "def test_escaped_parametrized_names_xml_with_non_ascii(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.parametrize('char', [\"\\u00e9\"])\n            assert char\n        \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    node.assert_attr(name=\"test_func[\\\\u00e9]\")\n", "def test_logging_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    systemout = node.find_first_by_tag(\"system-out\")\n    assert \"This is stdout\" in systemout.text\n    assert \"hello\" in systemout.text\n    systemerr = node.find_first_by_tag(\"system-err\")\n    assert \"This is stderr\" in systemerr.text\n", "def test_custom_properties_and_attributes_with_different_families(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_family = xunit2\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            record_property(\"bar\", 1)\n            record_xml_attribute(\"bar\", 1)\n            record_property(\"foo\", \"<1\")\n            record_xml_attribute(\"foo\", \"<1\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    psnode = tnode.find_first_by_tag(\"properties\")\n    pnodes = psnode.find_by_tag(\"property\")\n    pnodes[0].assert_attr(name=\"bar\", value=\"1\")\n    pnodes[1].assert_attr(name=\"foo\", value=\"<1\")\n    tnode.assert_attr(foo=\"<1\")\n    result.stdout.fnmatch_lines(\n        [\"*test_custom_properties_and_attributes_with_different_families.py:7:*record_xml_attribute is an experimental feature\"]\n    )\n"], "sample_837": ["def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_blas_info():\n    from sklearn.utils._show_versions import _get_blas_info\n\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_show_versions_without_blas(capsys, monkeypatch):\n    # Monkeypatch the _get_blas_info function to return an empty dictionary\n        return {}, {}\n\n    monkeypatch.setattr(\"sklearn.utils._show_versions._get_blas_info\", mock_get_blas_info)\n\n    show_versions()\n    out, err = capsys.readouterr()\n    assert 'python' in out\n    assert 'numpy' in out\n    assert 'BLAS' not in out\n"], "sample_469": ["def test_alias_forbidden_chars(self):\n    tests = [\n        'al\"ias',\n        \"a'lias\",\n        \"ali`as\",\n        \"alia s\",\n        \"alias\\t\",\n        \"ali\\nas\",\n        \"alias--\",\n        \"ali/*as\",\n        \"alias*/\",\n        \"alias;\",\n        # [] are used by MSSQL.\n        \"alias[\",\n        \"alias]\",\n    ]\n    msg = (\n        \"Column aliases cannot contain whitespace characters, quotation marks, \"\n        \"semicolons, or SQL comments.\"\n    )\n    for crafted_alias in tests:\n        with self.subTest(crafted_alias):\n            with self.assertRaisesMessage(ValueError, msg):\n                Book.objects.alias(**{crafted_alias: Value(1)})\n", "def test_alias_forbidden_chars(self):\n    tests = [\n        'al\"ias',\n        \"a'lias\",\n        \"ali`as\",\n        \"alia s\",\n        \"alias\\t\",\n        \"ali\\nas\",\n        \"alias--\",\n        \"ali/*as\",\n        \"alias*/\",\n        \"alias;\",\n        # [] are used by MSSQL.\n        \"alias[\",\n        \"alias]\",\n    ]\n    msg = (\n        \"Column aliases cannot contain whitespace characters, quotation marks, \"\n        \"semicolons, or SQL comments.\"\n    )\n    for crafted_alias in tests:\n        with self.subTest(crafted_alias):\n            with self.assertRaisesMessage(ValueError, msg):\n                Book.objects.alias(**{crafted_alias: Value(1)})\n", "def test_alias_forbidden_chars(self):\n    tests = [\n        'al\"ias',\n        \"a'lias\",\n        \"ali`as\",\n        \"alia s\",\n        \"alias\\t\",\n        \"ali\\nas\",\n        \"alias--\",\n        \"ali/*as\",\n        \"alias*/\",\n        \"alias;\",\n        # [] are used by MSSQL.\n        \"alias[\",\n        \"alias]\",\n    ]\n    msg = (\n        \"Column aliases cannot contain whitespace characters, quotation marks, \"\n        \"semicolons, or SQL comments.\"\n    )\n    for crafted_alias in tests:\n        with self.subTest(crafted_alias):\n            with self.assertRaisesMessage(ValueError, msg):\n                Book.objects.alias(**{crafted_alias: Value(1)})\n", "def test_alias_filter_chaining_with_m2m(self):\n    qs = (\n        Author.objects.filter(\n            name=\"Adrian Holovaty\",\n            friends__age=35,\n        )\n        .alias(\n            jacob_name=F(\"friends__name\"),\n        )\n        .filter(\n            friends__age=29,\n        )\n        .alias(\n            james_name=F(\"friends__name\"),\n        )\n        .values(\"jacob_name\", \"james_name\")\n    )\n    self.assertCountEqual(\n        qs,\n        [{\"jacob_name\": \"Jacob Kaplan-Moss\", \"james_name\": \"James Bennett\"}],\n    )\n", "def test_annotation_exists_aggregate_filter_chaining(self):\n    qs = (\n        Publisher.objects.annotate(\n            has_authors=Exists(\n                Book.authors.through.objects.filter(book__publisher=OuterRef(\"pk\"))\n            ),\n        )\n        .filter(has_authors=True)\n        .annotate(max_pages=Max(\"book__pages\"))\n        .values(\"name\", \"max_pages\")\n    )\n    self.assertCountEqual(\n        qs,\n        [\n            {\"name\": \"Apress\", \"max_pages\": 1132},\n            {\"name\": \"Morgan Kaufmann\", \"max_pages\": 946},\n            {\"name\": \"Prentice Hall\", \"max_pages\": 1132},\n        ],\n    )\n"], "sample_267": ["def test_disable_constraint_checking_with_multi_statement_transaction(self):\n    \"\"\"\n    SQLite schema editor is not usable within an outer transaction if\n    foreign key constraint checks are not disabled beforehand and a\n    multi-statement transaction is in progress.\n    \"\"\"\n    with transaction.atomic():\n        with connection.cursor() as cursor:\n            # Start a multi-statement transaction\n            cursor.execute('BEGIN')\n            msg = (\n                'SQLite schema editor cannot be used while foreign key '\n                'constraint checks are enabled. Make sure to disable them '\n                'before entering a transaction.atomic() context because '\n                'SQLite does not support disabling them in the middle of '\n                'a multi-statement transaction.'\n            )\n            with self.assertRaisesMessage(NotSupportedError, msg):\n                with connection.schema_editor(atomic=True):\n                    pass\n", "    def test_check_constraints_with_table_names(self):\n        Author.objects.create(name='John Doe')\n        Item.objects.create(author_id=1, name='Test Item')\n        with connection.constraint_checks_disabled():\n            with connection.cursor() as cursor:\n                cursor.execute(\"UPDATE backends_item SET author_id = 2 WHERE id = 1\")\n        with self.assertRaises(IntegrityError):\n            connection.check_constraints(['backends_item'])\n", "def test_disable_constraint_checking(self):\n    with connection.constraint_checks_disabled():\n        with connection.cursor() as cursor:\n            enabled = cursor.execute('PRAGMA foreign_keys').fetchone()[0]\n        self.assertFalse(bool(enabled))\n    with connection.cursor() as cursor:\n        enabled = cursor.execute('PRAGMA foreign_keys').fetchone()[0]\n    self.assertTrue(bool(enabled))\n", "def test_foreign_key_checking(self):\n    \"\"\"\n    Ensure foreign key constraints are checked during data insertion.\n    \"\"\"\n    author = Author.objects.create(name='Test Author')\n    with self.assertRaises(IntegrityError):\n        Item.objects.create(author_id=author.id + 1, name='Test Item')\n", "    def test_datetime_cast_date_function(self):\n        # Test the _sqlite_datetime_cast_date function with different input\n        test_cases = [\n            ('2022-01-01 12:34:56', '2022-01-01'),\n            ('2022-02-28 00:00:00', '2022-02-28'),\n            (None, None),\n            ('invalid', None),\n        ]\n        for input_dt, expected_output in test_cases:\n            with self.subTest(input_dt=input_dt):\n                self.assertEqual(_sqlite_datetime_cast_date(input_dt, None, None), expected_output)\n"], "sample_364": ["def test_path_inclusion_with_namespace(self):\n    url = reverse('namespaced-base64:base64', kwargs={'value': b'hello'})\n    self.assertEqual(url, '/base64/aGVsbG8=/')\n", "def test_path_reverse_with_inclusion(self):\n    url = reverse('inner-extra', args=['something'], kwargs={'extra': 'extra'})\n    self.assertEqual(url, '/included_urls/extra/something/extra/')\n", "    def test_path_inclusion_with_namespaces(self):\n        match = resolve('/included_urls/namespaced/d29ybGQ=/')\n        self.assertEqual(match.url_name, 'namespaced-base64')\n        self.assertEqual(match.kwargs, {'base': b'hello', 'value': b'world'})\n        self.assertEqual(match.namespaces, ['included_urls', 'namespaced-base64'])\n", "def test_invalid_regex(self):\n    msg = \"The route regex '^invalid_regex($' is not a valid regular expression: unbalanced parenthesis\"\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        re_path('^invalid_regex($', empty_view)\n", "    def test_path_inclusion_with_namespace(self):\n        match = resolve('/namespaced/included_urls/extra/something/')\n        self.assertEqual(match.url_name, 'namespaced:inner-extra')\n        self.assertEqual(match.kwargs, {'extra': 'something'})\n"], "sample_1091": ["def test_issue_18188_complex():\n    from sympy.sets.conditionset import ConditionSet\n    result1 = Eq(x*cos(x) - 3*I*sin(x), 0)\n    assert result1.as_set() == ConditionSet(x, Eq(x*cos(x) - 3*I*sin(x), 0), S.Complexes)\n\n    result2 = Eq(x**2 + I*sqrt(x*2) + sin(x), 0)\n    assert result2.as_set() == ConditionSet(x, Eq(I*sqrt(2)*sqrt(x) + x**2 + sin(x), 0), S.Complexes)\n", "def test_complex_infinity_relation():\n    assert Eq(zoo, zoo) is S.true\n    assert Ne(zoo, zoo) is S.false\n    assert Lt(zoo, zoo) is S.false\n    assert Le(zoo, zoo) is S.true\n    assert Gt(zoo, zoo) is S.false\n    assert Ge(zoo, zoo) is S.true\n", "def test_nonpolymonial_relations_simplify():\n    assert simplify(Eq(cos(x), 1)) == Eq(cos(x), 1)\n    assert simplify(Eq(sin(x), 0)) == Eq(sin(x), 0)\n    assert simplify(Eq(tan(x), 0)) == Eq(tan(x), 0)\n    assert simplify(Eq(cosh(x), 1)) == Eq(cosh(x), 1)\n    assert simplify(Eq(sinh(x), 0)) == Eq(sinh(x), 0)\n    assert simplify(Eq(tanh(x), 0)) == Eq(tanh(x), 0)\n", "def test_relation_simplification_with_noncommutative_symbols():\n    from sympy import NC\n    a, b = symbols('a b', commutative=False)\n    na, nb = NC(a), NC(b)\n\n    assert Eq(na + nb, nb + na).simplify() == False\n    assert (na + nb > nb + na).simplify() == False\n    assert (na + nb < nb + na).simplify() == False\n    assert (na + nb >= nb + na).simplify() == False\n    assert (na + nb <= nb + na).simplify() == False\n    assert Ne(na + nb, nb + na).simplify() == True\n", "def test_issue_18189():\n    # Test for simplification of relations involving functions\n    f = Function('f')\n    g = Function('g')\n\n    # Test simplification of equality with functions\n    eq = Eq(f(x), g(x))\n    assert eq.simplify() == Eq(f(x), g(x))\n\n    eq = Eq(f(x), f(x))\n    assert eq.simplify() is S.true\n\n    eq = Eq(f(x), f(y))\n    assert eq.simplify() == Eq(f(x), f(y))\n\n    # Test simplification of inequality with functions\n    ne = Ne(f(x), g(x))\n    assert ne.simplify() == Ne(f(x), g(x))\n\n    ne = Ne(f(x), f(x))\n    assert ne.simplify() is S.false\n\n    ne = Ne(f(x), f(y))\n    assert ne.simplify() == Ne(f(x), f(y))\n"], "sample_102": ["def test_union_with_filtered_relation(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.all()\n    qs2 = ReservedName.objects.filter(name='a')\n    reserved_name = qs1.union(qs2).filter(name='a').values('name', 'order', 'id').get()\n    self.assertEqual(reserved_name['name'], 'a')\n    self.assertEqual(reserved_name['order'], 2)\n", "def test_union_with_values_list_and_distinct(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=1),\n        ReservedName(name='rn2', order=1),\n        ReservedName(name='rn3', order=2),\n    ])\n    qs1 = ReservedName.objects.filter(order=1).values_list('order', flat=True)\n    qs2 = ReservedName.objects.filter(order=2).values_list('order', flat=True)\n    self.assertCountEqual(qs1.union(qs2), [1, 2])\n    self.assertCountEqual(qs1.union(qs2, all=True), [1, 1, 2])\n", "def test_union_with_unsupported_operations_in_subqueries(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.annotate(doubled_num=F('num') * 2).values('doubled_num')\n    msg = \"Selecting an annotation on a subquery is not supported.\"\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        list(qs1.union(qs2).order_by('doubled_num'))\n", "def test_union_with_values_list_and_order_desc(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=5),\n        ReservedName(name='rn0', order=6),\n        ReservedName(name='rn9', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=6)\n    qs2 = ReservedName.objects.filter(order__lte=5)\n    union_qs = qs1.union(qs2)\n    for qs, expected_result in (\n        # Order by a single column in descending order.\n        (union_qs.order_by('-pk').values_list('order', flat=True), [-1, 6, 5, 7]),\n        (union_qs.order_by('pk').reverse().values_list('order', flat=True), [-1, 6, 5, 7]),\n        (union_qs.values_list('order', flat=True).order_by('-pk'), [-1, 6, 5, 7]),\n        (union_qs.values_list('order', flat=True).order_by('pk').reverse(), [-1, 6, 5, 7]),\n    ):\n        with self.subTest(qs=qs):\n            self.assertEqual(list(qs), expected_result)\n", "def test_union_with_values_list_on_annotated_and_unannotated_querysets(self):\n    qs1 = Number.objects.annotate(\n        squared=F('num') * F('num'),\n    ).filter(squared__gt=10)\n    qs2 = Number.objects.filter(num=9)\n    self.assertCountEqual(qs1.union(qs2).values_list('num', flat=True), [3, 4, 9])\n"], "sample_487": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"hello\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "def test_autocomplete_with_through_model(self):\n    class ArtistAdmin(ModelAdmin):\n        search_fields = (\"name\",)\n\n    class SongAdmin(ModelAdmin):\n        autocomplete_fields = (\"artists\",)\n\n    site = AdminSite()\n    site.register(Artist, ArtistAdmin)\n    self.assertIsInvalid(\n        SongAdmin,\n        Song,\n        msg=(\n            'ArtistAdmin must define \"search_fields\", because it\\'s '\n            \"referenced by SongAdmin.autocomplete_fields.\"\n        ),\n        id=\"admin.E040\",\n        invalid_obj=SongAdmin,\n        admin_site=site,\n    )\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_invalid_callable(self):\n            pass\n\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = (random_obj,)\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_1183": ["def test_Domain_is_zero():\n    assert ZZ.is_zero(ZZ(0)) is True\n    assert QQ.is_zero(QQ(0)) is True\n    assert RR.is_zero(RR(0)) is True\n    assert CC.is_zero(CC(0)) is True\n    assert EX.is_zero(EX(0)) is True\n\n    assert ZZ.is_zero(ZZ(1)) is False\n    assert QQ.is_zero(QQ(1)) is False\n    assert RR.is_zero(RR(1)) is False\n    assert CC.is_zero(CC(1)) is False\n    assert EX.is_zero(EX(1)) is False\n\n    assert ZZ[x].is_zero(ZZ[x](0)) is True\n    assert QQ[x].is_zero(QQ[x](0)) is True\n    assert ZZ.frac_field(x).is_zero(ZZ.frac_field(x)(0)) is True\n    assert QQ.frac_field(x).is_zero(QQ.frac_field(x)(0)) is True\n\n    assert ZZ[x].is_zero(ZZ[x](1)) is False\n    assert QQ[x].is_zero(QQ[x](1)) is False\n    assert ZZ.frac_field(x).is_zero(ZZ.frac_field(x)(1)) is False\n    assert QQ.frac_field(x).is_zero(QQ.frac_field(x)(1)) is False\n", "def test_Domain_is_negative_zero():\n    assert not ZZ.is_negative(ZZ(0))\n    assert not QQ.is_negative(QQ(0))\n    assert not RR.is_negative(RR(0))\n    assert not CC.is_negative(CC(0))\n    assert not ZZ_I.is_negative(ZZ_I(0))\n    assert not QQ_I.is_negative(QQ_I(0))\n", "def test_Domain_is_exact():\n    assert ZZ.is_exact == True\n    assert QQ.is_exact == True\n    assert RR.is_exact == False\n    assert CC.is_exact == False\n    assert ZZ_I.is_exact == True\n    assert QQ_I.is_exact == True\n    assert EX.is_exact == False\n    assert EXRAW.is_exact == False\n    assert ZZ[x].is_exact == True\n    assert QQ[x].is_exact == True\n    assert ZZ.frac_field(x).is_exact == False\n    assert QQ.frac_field(x).is_exact == False\n", "def test_Domain_unify_exponential():\n    EX_E = EX.unify(ZZ[E])\n    assert EX_E == EX\n\n    EX_I = EX.unify(ZZ_I)\n    assert EX_I == EX\n", "def test_Domain_is_infinite():\n    assert not ZZ.is_infinite(ZZ(2))\n    assert not QQ.is_infinite(QQ(2))\n    assert not RR.is_infinite(RR(2))\n    assert not CC.is_infinite(CC(2))\n    assert not EX.is_infinite(EX(2))\n    assert not EXRAW.is_infinite(EXRAW(2))\n\n    assert not ZZ.is_infinite(ZZ(0))\n    assert not QQ.is_infinite(QQ(0))\n    assert not RR.is_infinite(RR(0))\n    assert not CC.is_infinite(CC(0))\n    assert not EX.is_infinite(EX(0))\n    assert not EXRAW.is_infinite(EXRAW(0))\n\n    assert ZZ.is_infinite(ZZ.oo)\n    assert QQ.is_infinite(QQ.oo)\n    assert RR.is_infinite(RR.oo)\n    assert CC.is_infinite(CC.oo)\n    assert EX.is_infinite(EX.oo)\n    assert EXRAW.is_infinite(EXRAW.oo)\n\n    assert ZZ.is_infinite(ZZ.neg_infinity)\n    assert QQ.is_infinite(QQ.neg_infinity)\n    assert RR.is_infinite(RR.neg_infinity)\n    assert CC.is_infinite(CC.neg_infinity)\n    assert EX.is_infinite(EX.neg_infinity)\n    assert EXRAW.is_infinite(EXRAW.neg_infinity)\n"], "sample_316": ["    def test_image_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            image_pil = Image.open(fh)\n            self.assertEqual(image.width, image_pil.width)\n            self.assertEqual(image.height, image_pil.height)\n", "    def test_image_file_dimensions(self):\n        \"\"\"\n        Test the width and height properties of the ImageFile class.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            image_pil = Image.open(fh)\n            self.assertEqual(image.width, image_pil.width)\n            self.assertEqual(image.height, image_pil.height)\n", "def test_file_iteration_with_mixed_newlines_at_chunk_boundary(self):\n    f = File(BytesIO(b'one\\rtwo\\nthree\\r\\nfour'))\n    # Set chunk size to create a boundary between \\r and \\n:\n    # b'one\\rtwo\\n...\n    #          ^\n    f.DEFAULT_CHUNK_SIZE = 6\n    self.assertEqual(list(f), [b'one\\rtwo\\n', b'three\\r\\n', b'four'])\n", "    def test_dimension_properties(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            image_pil = Image.open(fh)\n            self.assertEqual(image.width, image_pil.size[0])\n            self.assertEqual(image.height, image_pil.size[1])\n", "    def test_file_iteration_with_binary_newlines(self):\n        f = File(BytesIO(b'one\\r\\ntwo\\nthree\\r\\nfour'))\n        self.assertEqual(list(f), [b'one\\r\\n', b'two\\n', b'three\\r\\n', b'four'])\n"], "sample_524": ["def test_colorbar_set_formatter_locator_horizontal():\n    # check that the locator properties echo what is on the axis for horizontal colorbar:\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, orientation='horizontal')\n    cb.ax.xaxis.set_major_locator(FixedLocator(np.arange(10)))\n    cb.ax.xaxis.set_minor_locator(FixedLocator(np.arange(0, 10, 0.2)))\n    assert cb.locator is cb.ax.xaxis.get_major_locator()\n    assert cb.minorlocator is cb.ax.xaxis.get_minor_locator()\n    cb.ax.xaxis.set_major_formatter(LogFormatter())\n    cb.ax.xaxis.set_minor_formatter(LogFormatter())\n    assert cb.formatter is cb.ax.xaxis.get_major_formatter()\n    assert cb.minorformatter is cb.ax.xaxis.get_minor_formatter()\n\n    # check that the setter works as expected for horizontal colorbar:\n    loc = FixedLocator(np.arange(7))\n    cb.locator = loc\n    assert cb.ax.xaxis.get_major_locator() is loc\n    loc = FixedLocator(np.arange(0, 7, 0.1))\n    cb.minorlocator = loc\n    assert cb.ax.xaxis.get_minor_locator() is loc\n    fmt = LogFormatter()\n    cb.formatter = fmt\n    assert cb.ax.xaxis.get_major_formatter() is fmt\n    fmt = LogFormatter()\n    cb.minorformatter = fmt\n    assert cb.ax.xaxis.get_minor_formatter() is fmt\n", "def test_colorbar_extend_drawedges_horizontal():\n    params = [\n        ('both', 1, [[[0, 1.1], [1, 1.1]],\n                     [[0, 2], [1, 2]],\n                     [[0, 2.9], [1, 2.9]]]),\n        ('min', 0, [[[0, 1.1], [1, 1.1]],\n                    [[0, 2], [1, 2]]]),\n        ('max', 0, [[[0, 2], [1, 2]],\n                    [[0, 2.9], [1, 2.9]]]),\n        ('neither', -1, [[[0, 2], [1, 2]]]),\n    ]\n\n    plt.rcParams['axes.linewidth'] = 2\n\n    fig = plt.figure(figsize=(4, 10))\n    subfigs = fig.subfigures(2, 1)\n\n    for orientation, subfig in zip(['horizontal', 'vertical'], subfigs):\n        if orientation == 'horizontal':\n            axs = subfig.subplots(1, 4)\n        else:\n            axs = subfig.subplots(4, 1)\n        fig.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.95)\n\n        for ax, (extend, coloroffset, res) in zip(axs, params):\n            cmap = mpl.colormaps[\"viridis\"]\n            bounds = np.arange(5)\n            nb_colors = len(bounds) + coloroffset\n            colors = cmap(np.linspace(100, 255, nb_colors).astype(int))\n            cmap, norm = mcolors.from_levels_and_colors(bounds, colors,\n                                                        extend=extend)\n\n            cbar = Colorbar(ax, cmap=cmap, norm=norm, orientation=orientation,\n                            drawedges=True)\n            # Set limits such that only", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    np.random.seed(seed=19680808)\n    pc = ax.pcolormesh(np.random.randn(10, 10), alpha=0.5)\n    cb = fig.colorbar(pc, location='right', extend='max', alpha=0.8)\n    fig.draw_without_rendering()\n    assert cb.alpha == 0.8\n", "def test_colorbar_update_norm():\n    # Test updating the norm of a colorbar\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    im = ax.imshow(data, norm=LogNorm(vmin=data.min(), vmax=data.max()))\n    cb = fig.colorbar(im)\n    new_norm = PowerNorm(gamma=0.5)\n    im.set_norm(new_norm)\n    cb.update_normal(im)\n    assert cb.norm is new_norm\n", "def test_colorbar_ticks_scientific():\n    # test fix for #22121\n    fig, ax = plt.subplots()\n    x = np.arange(-3.0, 4.001)\n    y = np.arange(-4.0, 3.001)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    clevs = np.array([-1e-12, -1e-6, -1e-2, 1e-2, 1e-6, 1e-12], dtype=float)\n    colors = ['r', 'g', 'b', 'c']\n    cs = ax.contourf(X, Y, Z, clevs, colors=colors, extend='neither')\n    cbar = fig.colorbar(cs, orientation='horizontal', ticks=clevs, format='%.0e')\n    assert len(cbar.ax.xaxis.get_ticklocs()) == len(clevs)\n    assert all(['e' in t.get_text() for t in cbar.ax.xaxis.get_ticklabels()])\n"], "sample_1074": ["def test_simple():\n    A = AlternatingGroup(5)\n    assert A.is_simple()\n    S = SymmetricGroup(4)\n    assert not S.is_simple()\n", "def test_polycyclic_group():\n    G = PermutationGroup(Permutation(0,1,2), Permutation(0,2,3))\n    P = G.polycyclic_group()\n    assert P.is_polycyclic\n    assert P.is_isomorphic(G)\n", "def test_coset_rank():\n    gens_cube = [[1, 3, 5, 7, 0, 2, 4, 6], [1, 3, 0, 2, 5, 7, 4, 6]]\n    gens = [Permutation(p) for p in gens_cube]\n    G = PermutationGroup(gens)\n    i = 0\n    for h in G.generate(af=True):\n        rk = G.coset_rank(h)\n        assert rk == i\n        h1 = G.coset_unrank(rk, af=True)\n        assert h == h1\n        i += 1\n    assert G.coset_unrank(48) == None\n    assert G.coset_unrank(G.coset_rank(gens[0])) == gens[0]\n", "def test_composition_factors():\n    a = Permutation(1, 2, 3)\n    b = Permutation(1, 2)\n    G = PermutationGroup([a, b])\n    factors = G.composition_factors()\n    assert factors == [3, 2]\n    S = SymmetricGroup(4)\n    assert S.composition_factors() == [2, 2, 3]\n    A = AlternatingGroup(4)\n    assert A.composition_factors() == [2, 3]\n\n    # the composition factors for C_8 are [2, 2, 2]\n    G = CyclicGroup(8)\n    factors = G.composition_factors()\n    assert factors == [2, 2, 2]\n", "def test_sylow_subgroup_properties():\n    # Test that the Sylow subgroup of a group is a subgroup\n    G = SymmetricGroup(5)\n    p = 2\n    P = G.sylow_subgroup(p)\n    assert P.is_subgroup(G)\n\n    # Test that the Sylow subgroup has order equal to the largest power of p that divides the order of the group\n    assert P.order() == G.order() // (G.order() // p**(G.max_div))\n\n    # Test that any two Sylow subgroups of a group are conjugate\n    p = 3\n    P1 = G.sylow_subgroup(p)\n    P2 = G.sylow_subgroup(p)\n    assert G.is_conjugate(P1, P2)\n\n    # Test that the number of Sylow subgroups is equal to the index of the normalizer of a Sylow subgroup in the group\n    N = G.normalizer(P1)\n    assert len(G.sylow_subgroups(p)) == G.index(N)\n"], "sample_854": ["def test_svc_ovr_decision_function_shape():\n    # Test decision_function shape with OVR decision function shape\n    X, y = make_blobs(n_samples=80, centers=5, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovr').fit(X_train, y_train)\n    dec = clf.decision_function(X_test)\n    assert dec.shape == (len(X_test), 5)\n    assert_array_equal(clf.predict(X_test), np.argmax(dec, axis=1))\n", "def test_svc_one_label():\n    # Test SVC with only one label in the training set\n    X = np.array([[1, 1], [-1, -1], [2, 2], [-2, -2]])\n    y = np.array([1, 1, 1, 1])\n    clf = svm.SVC()\n    with pytest.raises(ValueError, match=\"The number of classes has to be greater than one; got 1 class\"):\n        clf.fit(X, y)\n", "def test_linear_svc_decision_function_shape():\n    # Test that decision_function_shape is handled correctly in LinearSVC\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, n_informative=2, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    clf = svm.LinearSVC(decision_function_shape='ovr')\n    clf.fit(X_train, y_train)\n    decision_ovr = clf.decision_function(X_test)\n\n    clf = svm.LinearSVC(decision_function_shape='ovo')\n    clf.fit(X_train, y_train)\n    decision_ovo = clf.decision_function(X_test)\n\n    assert decision_ovr.shape[1] == 3\n    assert decision_ovo.shape[1] == 3 * (3 - 1) // 2\n", "def test_svc_coef_one_vs_one():\n    # Test that SVC(kernel=\"linear\") has coef_ with the right sign and shape.\n    # Non-regression test for #15833.\n    X, y = make_classification(n_samples=4, n_features=2, n_informative=2,\n                               n_redundant=0, n_classes=3, n_clusters_per_class=1,\n                               random_state=42)\n\n    clf = svm.SVC(kernel='linear')\n    clf.fit(X, y)\n\n    assert clf.coef_.shape == (3, 2)\n    assert_array_almost_equal(clf.predict(X), clf.decision_function(X).argmax(axis=1))\n", "def test_linear_svc_sample_weights():\n    # Test that LinearSVC handles sample weights correctly.\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    sample_weight = np.array([1, 2, 1])\n\n    clf = svm.LinearSVC(random_state=0).fit(X, y, sample_weight=sample_weight)\n    coef_weighted = clf.coef_.copy()\n    intercept_weighted = clf.intercept_.copy()\n\n    # Compare with results without sample weights\n    clf.fit(X, y)\n    assert not np.allclose(clf.coef_, coef_weighted)\n    assert not np.allclose(clf.intercept_, intercept_weighted)\n\n    # Check that sample weights are applied correctly\n    clf.fit(X, y, sample_weight=np.ones_like(y))\n    assert np.allclose(clf.coef_, coef_weighted)\n    assert np.allclose(clf.intercept_, intercept_weighted)\n"], "sample_1101": ["def test_schur_partition_special_cases():\n    assert schur_partition(1) == [[1]]\n    assert schur_partition(3) == [[1, 2, 3]]\n    assert schur_partition(5) == [[3, 2], [5], [1, 4]]\n    assert schur_partition(8) == [[3, 2], [6, 5, 8], [1, 4, 7]]\n", "def test_schur_partition_lower_bound():\n    \"\"\"\n    Tests if the output of schur_partition is a valid lower bound for the Schur Number.\n    \"\"\"\n    for n in range(4, 20):\n        subsets = schur_partition(n)\n        k = len(subsets)\n        for subset in subsets:\n            _sum_free_test(subset)\n        assert SchurNumber(k).lower_bound() <= n\n", "def test_schur_partition_bounds():\n    assert len(schur_partition(1)) == 1\n    assert len(schur_partition(2)) == 1\n    assert len(schur_partition(3)) == 1\n    assert len(schur_partition(4)) <= 4\n    assert len(schur_partition(13)) <= 3\n    assert len(schur_partition(44)) <= 4\n", "def test_schur_partition_edge_cases():\n    assert schur_partition(1) == [[1]]\n    assert schur_partition(3) == [[1, 2, 3]]\n    assert schur_partition(4) == [[3, 2], [4], [1]]\n", "def test_schur_partition_large_n():\n    # Test schur_partition for large n\n    assert schur_partition(44) == [[1, 2, 5, 8, 11, 14, 22], [3, 4, 6, 9, 10, 12, 13, 15, 17, 18, 19, 20, 23, 24, 26, 27, 28, 29, 31, 32, 34, 35, 36, 38, 39, 40, 41, 43, 44], [7, 16, 21, 25, 30, 33, 37]]\n    assert schur_partition(100) == [[1, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98], [3, 4, 6, 9, 10, 12, 13, 15, 18, 21, 22, 24, 25, 27, 30, 33, 34, 36, 39, 42, 45, 46, 48, 51, 54, 55, 57, 60, 63, 64, 66, 69, 72, 75, 76, 78, 81, 84, 87, 88, 90, 93, 9"], "sample_53": ["def test_value_from_datadict(self):\n    data = {'band': '1'}\n    files = {}\n    name = 'band'\n    rel = Album._meta.get_field('band').remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    value = w.value_from_datadict(data, files, name)\n    self.assertEqual(value, '1')\n", "def test_render_options_with_custom_placeholder(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    form = NotRequiredBandForm()\n    form.fields['band'].widget.widget.attrs['placeholder'] = 'Select a band'\n    output = form.as_table()\n    self.assertIn('data-placeholder=\\'Select a band\\'', output)\n", "    def test_render_options_without_initial_value(self):\n        form = NotRequiredBandForm()\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n\n        form = RequiredBandForm()\n        output = form.as_table()\n        self.assertNotIn(self.empty_option, output)\n", "def test_render_options_with_l10n(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    with self.settings(USE_L10N=True, LANGUAGE_CODE='de'):\n        form = AlbumForm(initial={'band': beatles.pk})\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        option = '<option value=\"%s\">The Who</option>' % who.pk\n        self.assertIn(selected_option, output)\n        self.assertNotIn(option, output)\n        # Test with a language that needs a language file\n        with translation.override('de'):\n            form = AlbumForm(initial={'band': beatles.pk})\n            output = form.as_table()\n            self.assertIn(selected_option, output)\n            self.assertNotIn(option, output)\n", "    def test_build_attrs_with_custom_attrs(self):\n        custom_attrs = {'data-custom': 'value'}\n        form = AlbumForm()\n        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs=custom_attrs)['widget']['attrs']\n        expected_attrs = {\n            'class': 'my-class admin-autocomplete',\n            'data-ajax--cache': 'true',\n            'data-ajax--type': 'GET',\n            'data-ajax--url': '/admin_widgets/band/autocomplete/',\n            'data-theme': 'admin-autocomplete',\n            'data-allow-clear': 'false',\n            'data-placeholder': '',\n            'data-custom': 'value',\n        }\n        self.assertEqual(attrs, expected_attrs)\n"], "sample_650": ["def test_log_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('This is a\\\\nmultiline\\\\nmessage')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_auto_indent=True\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"WARNING  test_log_auto_indent:test_log_auto_indent.py:6 This is a\",\n            \"  multiline\",\n            \"  message\",\n        ]\n    )\n", "def test_log_file_ini_formats(pytester: Pytester) -> None:\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format=%(asctime)s; %(levelname)s; %(message)s\n        log_file_date_format=%Y-%m-%d %H:%M:%S.%f\n        \"\"\".format(\n            log_file\n        )\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"test_log_file_ini_formats.py PASSED\"])\n\n    # make sure that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert re.match(r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{6}; INFO; This log message will be shown\", contents)\n", "def test_log_cli_format(pytester: Pytester) -> None:\n    # Test custom log format for live logging\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logging.getLogger('test').warning(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format = %(levelname)s - %(message)s\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*WARNING - This log message will be shown\",\n            \"PASSED\",\n        ]\n    )\n\n    # make sure that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_disable_loggers_with_caplog(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        disabled_log = logging.getLogger('disabled')\n        test_log = logging.getLogger('test')\n            disabled_log.warning(\"no log; no stderr\")\n            test_log.info(\"Visible text!\")\n            assert \"no log; no stderr\" not in caplog.text\n            assert \"Visible text!\" in caplog.text\n         \"\"\"\n    )\n    result = pytester.runpytest(\"--log-disable=disabled\", \"-s\")\n    assert result.ret == ExitCode.OK\n    assert not result.stderr.lines\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('Multiline\\\\nmessage\\\\nwith\\\\nindentation')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\", \"--log-auto-indent\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"INFO    *test_foo:test_log_auto_indent.py:6 Multiline\",\n            \"          message\",\n            \"          with\",\n            \"          indentation\",\n        ]\n    )\n"], "sample_553": ["def test_save_count_override_warnings_has_length_cache_frame_data_false(anim):\n    save_count = 5\n    frames = list(range(2))\n    cache_frame_data = False\n    match_target = (\n        f'You passed in an explicit {cache_frame_data=} '\n        \"which is being ignored because \"\n        f\"{frames=!r} which we can infer the length of, \"\n        \"did not pass an explicit *save_count*.\"\n    )\n\n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames, 'save_count': save_count, 'cache_frame_data': cache_frame_data}\n        )\n    assert anim._save_count == len(frames)\n    assert anim._cache_frame_data is cache_frame_data\n    anim._init_draw()\n", "def test_extra_anim_saving(anim):\n    # Test saving an animation with additional animations\n    fig, ax = plt.subplots()\n    line1, = ax.plot([], [])\n    line2, = ax.plot([], [])\n\n        line1.set_data([], [])\n        line2.set_data([], [])\n        return line1, line2\n\n        x = np.linspace(0, 10, 100)\n        y1 = np.sin(x + i)\n        y2 = np.cos(x + i)\n        line1.set_data(x, y1)\n        line2.set_data(x, y2)\n        return line1, line2\n\n    anim1 = animation.FuncAnimation(fig, animate, init_func=init, frames=5)\n    anim2 = animation.FuncAnimation(fig, animate, init_func=init, frames=5)\n\n    with pytest.warns(None) as record:\n        anim1.save('test.mp4', writer='ffmpeg', extra_anim=[anim2])\n\n    assert len(record) == 0\n", "def test_animation_pause_resume(anim):\n    anim = animation.FuncAnimation(**anim)\n    anim.pause()\n    assert not anim.event_source.is_running()\n    anim.resume()\n    assert anim.event_source.is_running()\n", "def test_funcanimation_save_count_override(anim):\n    save_count = 10\n    frames = list(range(15))\n    with pytest.warns(UserWarning, match=f'You passed in an explicit {save_count=} which is being ignored in favor of {len(frames)=}.'):\n        anim = animation.FuncAnimation(**{**anim, 'frames': frames, 'save_count': save_count})\n    assert anim._save_count == len(frames)\n", "def test_save_count_override_warnings_no_length(anim):\n    save_count = 5\n    frames = lambda: (yield from range(2))\n    match_target = (\n        f'You passed in an explicit {save_count=} '\n        \"which is being ignored in favor of \"\n        \"the provided `save_count` value.\"\n    )\n\n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames(), 'save_count': save_count}\n        )\n    assert anim._save_count == save_count\n    anim._init_draw()\n"], "sample_670": ["def test_operator_precedence(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_precedence(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is eval(expr.replace(\"true\", \"True\").replace(\"false\", \"False\").replace(\"not\", \"not \").replace(\"and\", \"and \").replace(\"or\", \"or \"))\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_parentheses_and_order_of_operations(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n"], "sample_1096": ["def test_Indexed_free_symbols():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, j)\n    B = IndexedBase('B')\n    assert A.free_symbols == {i, j, A.base.label}\n    assert B[i, j].free_symbols == {i, j, B.label}\n    assert Indexed(B, i, j).free_symbols == {i, j, B.label}\n", "def test_Indexed_differentiate_unknown_base():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, j)\n    B = Indexed('B', i, j)\n    expr = A * B\n    assert expr.diff(A) == B\n    assert expr.diff(B) == A\n", "def test_IndexedBase_strides():\n    i, j, k = symbols('i j k', integer=True)\n    l, m, n, o = symbols('l m n o', integer=True)\n    A = IndexedBase('A', strides=(l, m, n), offset=o)\n    assert A.strides == (l, m, n)\n    assert A.offset == o\n    assert A[i, j, k].subs({l: 1, m: 2, n: 3, i: 4, j: 5, k: 6}) == A[1*4 + 2*5 + 3*6 + o]\n", "def test_IndexedBase_strides_offset():\n    i, j = symbols('i j', integer=True)\n    l, m, n, o = symbols('l m n o', integer=True)\n    A = IndexedBase('A', strides=(l, m, n), offset=o)\n    assert A.strides == (l, m, n)\n    assert A.offset == o\n    assert A[i, j, k] == Indexed(A, i, j, k)\n    assert A[i, j, k].subs({i: l, j: m, k: n}) == Indexed(A, l, m, n)\n    assert A[i, j, k].subs({i: l, j: m, k: n}).doit() == A[l*l + m*m + n*n + o]\n", "def test_Indexed_derivative_with_array():\n    from sympy import NDimArray\n\n    A = NDimArray([[1, 2], [3, 4]])\n    i, j = symbols('i j', integer=True)\n    A_indexed = Indexed(A, i, j)\n\n    # Test derivative with respect to an index\n    assert A_indexed.diff(i) == KroneckerDelta(i, i)\n    assert A_indexed.diff(j) == KroneckerDelta(j, j)\n\n    # Test derivative with respect to an element of the array\n    assert A_indexed.diff(A[0, 0]) == 0\n    assert A_indexed.diff(A[1, 1]) == 0\n    assert A_indexed.diff(A[0, 1]) == 1\n    assert A_indexed.diff(A[1, 0]) == 1\n"], "sample_871": ["def test_silhouette_score_non_numpy_input():\n    \"\"\"Check that silhouette_score works for non-numpy inputs.\"\"\"\n    X = [[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]]\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    output_with_numpy_input = silhouette_score(np.array(X), np.array(y))\n    output_with_list_input = silhouette_score(X, y)\n    assert_allclose(output_with_numpy_input, output_with_list_input)\n", "def test_silhouette_samples_non_euclidean_metric():\n    \"\"\"Check that silhouette_samples works with non-euclidean metric.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    output_with_euclidean = silhouette_samples(X, y, metric=\"euclidean\")\n    output_with_manhattan = silhouette_samples(X, y, metric=\"manhattan\")\n    assert_allclose(output_with_euclidean, output_with_manhattan, atol=1e-6)\n", "def test_silhouette_samples_sparse_input():\n    \"\"\"Check that silhouette_samples works for sparse input correctly.\"\"\"\n    X = csr_matrix([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]])\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    output_with_sparse_input = silhouette_samples(X, y)\n    output_with_dense_input = silhouette_samples(X.toarray(), y)\n    assert_allclose(output_with_sparse_input, output_with_dense_input)\n", "def test_silhouette_samples_with_numpy_array_input():\n    # Test silhouette_samples with numpy array input\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    labels = np.array([0, 0, 1, 1, 2])\n    result = silhouette_samples(X, labels)\n    expected = np.array([0.5, 0.5, -0.5, -0.5, 0.0])\n    np.testing.assert_allclose(result, expected)\n", "def test_silhouette_samples_different_sparse_formats():\n    \"\"\"Check that silhouette_samples gives same results for different sparse formats.\"\"\"\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    sparse_formats = [csr_matrix, csc_matrix, dok_matrix, lil_matrix]\n    results = [silhouette_samples(fmt(X), y) for fmt in sparse_formats]\n    for i in range(1, len(results)):\n        assert_allclose(results[i], results[0])\n"], "sample_493": ["def test_referenced_expression_kept(self):\n    with CaptureQueriesContext(connection) as ctx:\n        Book.objects.annotate(\n            price_pages_ratio=F(\"price\") / F(\"pages\"),\n        ).aggregate(Avg(\"price_pages_ratio\"))\n    sql = ctx.captured_queries[0][\"sql\"].lower()\n    self.assertEqual(sql.count(\"select\"), 2, \"Subquery wrapping required\")\n    self.assertEqual(sql.count(\"price_pages_ratio\"), 2)\n", "def test_annotation_pruning_with_filter(self):\n    with CaptureQueriesContext(connection) as ctx:\n        Book.objects.annotate(\n            authors_count=Count(\"authors\"),\n        ).filter(authors_count__gt=1).count()\n    sql = ctx.captured_queries[0][\"sql\"].lower()\n    self.assertEqual(sql.count(\"select\"), 1, \"No subquery wrapping required\")\n    self.assertIn(\"authors_count\", sql)\n", "def test_aggregation_with_float_field(self):\n    book = Book.objects.create(\n        isbn=\"123456789\",\n        name=\"Test book with float field\",\n        pages=200,\n        rating=3.5,\n        price=Decimal(\"19.99\"),\n        contact=self.a1,\n        publisher=self.p1,\n        pubdate=datetime.date(2019, 12, 6),\n        float_field=123.45,\n    )\n    book.authors.add(self.a1)\n    avg_float_field = Book.objects.aggregate(Avg(\"float_field\"))\n    self.assertAlmostEqual(avg_float_field[\"float_field__avg\"], 123.45)\n", "def test_annotate_sum_filter(self):\n    authors = Author.objects.annotate(\n        sum_books_pages=Sum(\"book__pages\", filter=Q(book__rating__gt=4))\n    ).order_by(\"name\")\n    self.assertQuerySetEqual(\n        authors,\n        [\n            (\"Adrian Holovaty\", 447),\n            (\"Brad Dayley\", 0),\n            (\"Jacob Kaplan-Moss\", 0),\n            (\"James Bennett\", 300),\n            (\"Jeffrey Forcier\", 350),\n            (\"Paul Bissex\", 350),\n            (\"Peter Norvig\", 0),\n            (\"Stuart Russell\", 0),\n            (\"Wesley J. Chun\", 350),\n        ],\n        lambda a: (a.name, a.sum_books_pages),\n    )\n", "def test_annotate_with_defer(self):\n    qs = (\n        Book.objects.annotate(page_sum=Sum(\"pages\"))\n        .defer(\"name\")\n        .filter(pk=self.b1.pk)\n    )\n\n    rows = [\n        (\n            self.b1.id,\n            \"159059725\",\n            447,\n            \"The Definitive Guide to Django: Web Development Done Right\",\n        )\n    ]\n    self.assertQuerySetEqual(\n        qs.order_by(\"pk\"), rows, lambda r: (r.id, r.isbn, r.page_sum, r.name)\n    )\n"], "sample_893": ["def test_plot_tree_max_depth(pyplot):\n    # Test max_depth parameter\n    clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=2)\n    clf.fit(X, y)\n\n    nodes = plot_tree(clf, max_depth=1)\n    assert len(nodes) == 2\n    assert nodes[0].get_text() == \"x[0] <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n", "def test_export_text_class_names(pyplot):\n    # Test export_text with custom class names\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- class: negative\n    |--- feature_1 >  0.00\n    |   |--- class: positive\n    \"\"\").lstrip()\n    assert export_text(clf, class_names=[\"negative\", \"positive\"]) == expected_report\n", "def test_plot_tree_errors(pyplot):\n    # Testing if plot_tree throws errors for invalid inputs\n    clf = DecisionTreeRegressor()\n    clf.fit(X, y)\n\n    # Test with invalid max_depth\n    with pytest.raises(ValueError):\n        plot_tree(clf, max_depth=-1)\n\n    # Test with invalid feature_names\n    with pytest.raises(ValueError):\n        plot_tree(clf, feature_names=[\"a\"])\n\n    # Test with invalid class_names\n    with pyplot.raises(ValueError):\n        plot_tree(clf, class_names=[\"1\"])\n\n    # Test with invalid label\n    with pytest.raises(ValueError):\n        plot_tree(clf, label=\"invalid\")\n\n    # Test with invalid precision\n    with pytest.raises(ValueError):\n        plot_tree(clf, precision=-1)\n", "def test_export_text_multi_output(pyplot):\n    # Testing export_text with multi-output trees\n    X_mo = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y_mo = [[-1, -1], [-1, -1], [-1, -1], [1, 1], [1, 1], [1, 1]]\n\n    reg = DecisionTreeRegressor(max_depth=2, random_state=0)\n    reg.fit(X_mo, y_mo)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.0\n    |   |--- value: [[-1.0, -1.0]\n    |                 [-1.0, -1.0]]\n    |--- feature_1 >  0.0\n    |   |--- value: [[1.0, 1.0]\n    |                 [1.0, 1.0]]\n    \"\"\").lstrip()\n    assert export_text(reg, decimals=1) == expected_report\n", "def test_plot_tree_multioutput(pyplot):\n    # Check correctness of plot_tree for multi-output\n    clf = DecisionTreeRegressor(\n        max_depth=3, min_samples_split=2, criterion=\"squared_error\", random_state=2\n    )\n    clf.fit(X, y2)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names, filled=True)\n    assert len(nodes) == 5\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\nsamples = 6\\nvalue = [[3.0, 1.5, 0.0],\\n[3.0, 1.0, 0.5]]\"\n    )\n    assert nodes[1].get_text() == \"samples = 3\\nvalue = [[3, 0, 0],\\n[3, 0, 0]]\"\n    assert nodes[2].get_text() == \"first feat <= 1.5\\nsamples = 3\\nvalue = [[0.0, 1.5, 0.0],\\n[0.0, 1.0, 0.5]]\"\n    assert nodes[3].get_text() == \"samples = 2\\nvalue = [[0, 1, 0],\\n[0, 1, 0]]\"\n    assert nodes[4].get_text() == \"samples = 1\\nvalue = [[0.0, 0.5, 0.0],\\n[0.0, 0.0, 0.5]]\"\n"], "sample_444": ["def test_stored_name(self):\n    \"\"\"\n    stored_name() returns the correct hashed name.\n    \"\"\"\n    name = \"cached/styles.css\"\n    expected_hashed_name = \"cached/styles.5e0040571e1a.css\"\n    self.assertEqual(\n        storage.staticfiles_storage.stored_name(name),\n        expected_hashed_name,\n    )\n", "    def setUp(self):\n        super().setUp()\n        storage.staticfiles_storage.manifest_version = \"2.0\"\n", "    def test_ignore_patterns(self):\n        # Add a file to be ignored.\n        ignored_file_name = \"cached/ignored.css\"\n        content = StringIO()\n        content.write(\"Ignored content\")\n        storage.staticfiles_storage.save(ignored_file_name, content)\n\n        # Collect the additional file.\n        self.run_collectstatic()\n\n        hashed_files = storage.staticfiles_storage.hashed_files\n        self.assertNotIn(ignored_file_name, hashed_files)\n\n        manifest_content, _ = storage.staticfiles_storage.load_manifest()\n        self.assertNotIn(ignored_file_name, manifest_content)\n", "def test_template_tag_double_slash(self):\n    relpath = self.hashed_file_path(\"cached/double_slash.css\")\n    self.assertEqual(relpath, \"cached/double_slash.6c0907565589.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"//styles.css\", content)\n        self.assertIn(b\"//styles.5e0040571e1a.css\", content)\n    self.assertPostCondition()\n", "def test_css_import_with_special_characters(self):\n    relpath = self.hashed_file_path(\"cached/styles_special_characters.css\")\n    self.assertEqual(relpath, \"cached/styles_special_characters.2f41c3c74732.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"cached/special%20chars.css\", content)\n        self.assertIn(b\"special%20chars.d41d8cd98f00.css\", content)\n    self.assertPostCondition()\n"], "sample_668": ["def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            assert my_fixture.funcargnames == []\n        \"\"\"\n    )\n\n    with pytest.warns(deprecated.FUNCARGNAMES):\n        testdir.runpytest()\n", "def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture\n            pass\n\n            assert 'some_fixture' in request.fixturenames\n            with pytest.warns(pytest.PytestDeprecationWarning):\n                assert 'some_fixture' in request.funcargnames\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`*\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_warning(testdir):\n    \"\"\"Show a warning when passing arguments to pytest.fixture() as positional arguments (#6445)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(\"function\", \"param1\", \"param2\")\n            pass\n\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them*\"\n        ]\n    )\n", "def test_fixture_positional_arguments_warning(testdir, fixture_args):\n    \"\"\"Check that warning is raised when passing arguments to pytest.fixture() as positional arguments (#issue_number)\"\"\"\n    test_code = f\"\"\"\n        import pytest\n\n        @pytest.fixture(*{fixture_args})\n            pass\n\n            pass\n    \"\"\"\n    testdir.makepyfile(test_code)\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them \"\n            \"as a keyword argument instead.*\",\n        ]\n    )\n", "def test_funcargnames_is_deprecated(pytestconfig):\n    \"\"\"Check that using funcargnames attribute is deprecated (#6180)\"\"\"\n    funcitem = pytestconfig.getoption(\"collectonly\")[0]\n    with pytest.warns(deprecated.FUNCARGNAMES):\n        assert funcitem.funcargnames == funcitem.fixturenames\n"], "sample_718": ["def test_check_estimator_sparse_data():\n    # check that check_estimator_sparse_data works on estimator\n    # that accept sparse data\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.utils.estimator_checks import check_estimator_sparse_data\n\n    est = LogisticRegression()\n    check_estimator_sparse_data('LogisticRegression', est)\n", "def test_check_estimator_dict_unchanged():\n    # check that estimator does not change dict during predict, transform, decision_function, and predict_proba\n    msg = 'Estimator changes __dict__ during'\n    assert_raises_regex(AssertionError, msg, check_estimator, ChangesDict)\n", "def test_check_estimator_transformer():\n    # check that transformer checks the shape of input in transform\n    msg = \"The transformer NoCheckinTransformer does not raise an error when the number of features in transform is different from the number of features in fit.\"\n    assert_raises_regex(AssertionError, msg, check_estimator, NoCheckinTransformer)\n    assert_raises_regex(AssertionError, msg, check_estimator, NoCheckinTransformer())\n", "def test_check_estimator_random_state():\n    # check that estimators with a random_state parameter are tested with\n    # a fixed random_state\n    from sklearn.utils import check_random_state\n\n    class RandomEstimator(BaseEstimator):\n            self.random_state = random_state\n\n            self.random_state_ = check_random_state(self.random_state)\n            return self\n\n    est = RandomEstimator()\n    est.fit(np.array([[1, 2], [3, 4]]), np.array([1, 2]))\n    assert_equal(est.random_state_, np.random.RandomState(0))\n", "def test_check_estimator_no_sample_weight_list():\n    # check that estimator accepts a 'sample_weight' parameter of\n    # type list in the 'fit' function.\n    class EstimatorWithSampleWeight(BaseEstimator):\n            X, y = check_X_y(X, y)\n            if not isinstance(sample_weight, list):\n                raise ValueError(\"'sample_weight' parameter should be of type list\")\n            return self\n\n            X = check_array(X)\n            return np.ones(X.shape[0])\n\n    est = EstimatorWithSampleWeight()\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    sample_weight = [0.5, 0.3, 0.2]\n    est.fit(X, y, sample_weight=sample_weight)\n    # No exception should be raised if sample_weight is of type list\n"], "sample_280": ["def test_aggregation_default_passed_another_aggregate_with_filter(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__lt=3.0), default=Avg('pages', filter=Q(rating__gt=4.0))),\n    )\n    self.assertAlmostEqual(result['value'], Decimal('23.09'), places=2)\n", "def test_aggregation_default_passed_another_aggregate_with_filter(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__lt=3.0), default=Sum('pages', filter=Q(rating__gt=4.0))),\n    )\n    self.assertEqual(result['value'], Decimal('23.09'))\n", "def test_aggregation_default_passed_another_aggregate_with_filter(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__lt=3.0), default=Avg('pages', filter=Q(rating__gt=4.0)) / 10.0),\n    )\n    self.assertAlmostEqual(result['value'], Decimal('47.80'), places=2)\n", "def test_annotate_values_list_distinct(self):\n    books = Book.objects.filter(rating=4).values_list('pages', flat=True).distinct()\n    self.assertEqual(sorted(books), [300, 350, 447])\n", "def test_aggregation_filter_compound_expression(self):\n    # Filter books by price less than the average price for each publisher\n    expr = Avg('book__price')\n    queryset = Publisher.objects.annotate(avg_price=expr).filter(\n        book__price__lt=OuterRef('avg_price')\n    ).values('name', 'avg_price').order_by('name')\n    self.assertSequenceEqual(queryset, [\n        {'name': 'Apress', 'avg_price': Approximate(Decimal('44.345'), places=2)},\n        {'name': 'Sams', 'avg_price': Decimal('23.09')},\n        {'name': 'Prentice Hall', 'avg_price': Approximate(Decimal('74.215'), places=2)},\n    ])\n"], "sample_949": ["def test_fieldlist(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'Field name: Field body\\n' in content\n", "def test_man_author_single(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert '.SH AUTHORS\\n.sp\\nSingle Author\\n' in content\n", "def test_image_in_man(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    # Check if the image reference is not present in the content\n    assert 'image_file.png' not in content\n", "def test_config_man_pages(app, status, warning):\n    app.build()\n    assert (app.outdir / 'custom.8').exists()\n    assert not (app.outdir / 'sphinxtests.1').exists()\n\n    content = (app.outdir / 'custom.8').read_text()\n    assert 'Custom Manual Page' in content\n    assert 'Custom Description' in content\n    assert 'Custom Author' in content\n", "def test_multiple_authors(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert '.TH PYTHON 1 \"2021-01-01\" \"1.0\" \"Authors: John Doe, Jane Smith\"\\n' in content\n"], "sample_367": ["def test_cache_control_decorator_headers(self):\n    @cache_control(max_age=60, private=True)\n        return HttpResponse()\n\n    response = a_view(HttpRequest())\n    self.assertEqual(response.headers['Cache-Control'], 'max-age=60, private')\n", "def test_cache_control_decorator(self):\n    @cache_control(max_age=3600, public=True)\n        return HttpResponse()\n\n    request = HttpRequest()\n    response = a_view(request)\n    self.assertEqual(response.headers['Cache-Control'], 'max-age=3600, public')\n", "def test_cache_control_decorator_headers(self):\n    @cache_control(max_age=60, public=True)\n        return HttpResponse()\n\n    response = a_view(HttpRequest())\n    self.assertEqual(response.headers['Cache-Control'], 'max-age=60, public')\n", "    def test_cache_control_decorator_kwargs(self):\n        @cache_control(max_age=60, private=True)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=60, private')\n", "def test_cache_control_decorator_overrides(self):\n    @cache_control(max_age=3600)\n        return HttpResponse()\n\n    response = a_view(HttpRequest())\n    self.assertEqual(response.headers['Cache-Control'], 'max-age=3600')\n\n    @cache_control(max_age=3600)\n        response = HttpResponse()\n        response['Cache-Control'] = 'no-cache'\n        return response\n\n    response = b_view(HttpRequest())\n    self.assertEqual(response.headers['Cache-Control'], 'no-cache')\n"], "sample_713": ["def test_ridge_classifier_cv_multi_output():\n    # Test RidgeClassifierCV with multiple outputs\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([[1, 2], [1, 2], [1, 2], [-1, -2], [-1, -2]])\n\n    reg = RidgeClassifierCV(alphas=[.01, .1, 1])\n    reg.fit(X, y)\n\n    assert_array_equal(reg.predict([[-.2, 2]]), np.array([[1, 2]]))\n", "def test_ridge_gcv_mode_auto():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 5\n    X_dense = rng.randn(n_samples, n_features)\n    X_sparse = sp.csr_matrix(X_dense)\n    y = rng.randn(n_samples)\n\n    ridge_gcv_auto_dense = _RidgeGCV(gcv_mode='auto')\n    ridge_gcv_auto_sparse = _RidgeGCV(gcv_mode='auto')\n\n    ridge_gcv_auto_dense.fit(X_dense, y)\n    ridge_gcv_auto_sparse.fit(X_sparse, y)\n\n    assert ridge_gcv_auto_dense.gcv_mode == 'svd'\n    assert ridge_gcv_auto_sparse.gcv_mode == 'eigen'\n", "def test_ridge_classifier_cv_multi_output_target():\n    X, y = make_regression(n_samples=10, n_features=5, n_targets=2, random_state=0)\n    ridge_cv = RidgeClassifierCV()\n    assert_raises(ValueError, ridge_cv.fit, X, y)\n", "def test_ridge_sag_intercept_sparse():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 50, 10\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    X_sparse = sp.csr_matrix(X)\n\n    ridge = Ridge(solver=\"sag\", fit_intercept=True)\n    ridge.fit(X_sparse, y)\n    assert_equal(ridge.coef_.shape[0], n_features)\n    assert_not_equal(ridge.intercept_, 0.0)\n\n    # Check that the intercept is computed correctly\n    X_with_intercept = sp.hstack([X_sparse, np.ones((n_samples, 1))])\n    y_pred_with_intercept = X_with_intercept.dot(np.concatenate([ridge.coef_, [ridge.intercept_]]))\n    assert_array_almost_equal(y_pred_with_intercept, ridge.predict(X_sparse))\n", "def test_ridge_classifier_cv_with_sample_weight():\n    X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=2, random_state=42)\n    sample_weight = np.random.RandomState(42).rand(y.size)\n    clf = RidgeClassifierCV()\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert hasattr(clf, \"coef_\")\n    assert hasattr(clf, \"intercept_\")\n    assert clf.predict(X).shape == y.shape\n"], "sample_281": ["def test_permission_denied_for_invalid_model(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid_app', 'model_name': 'invalid_model', 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_custom_to_field_m2m(self):\n    a1 = Author.objects.create(name='Author 1')\n    a2 = Author.objects.create(name='Author 2')\n    b = Book.objects.create(title='Test Book')\n    b.authors.add(a1, a2)\n    opts = {\n        'app_label': Book._meta.app_label,\n        'model_name': Book._meta.model_name,\n        'field_name': 'authors',\n    }\n    request = self.factory.get(self.url, {'term': 'Author', **opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data, {\n        'results': [{'id': str(a1.pk), 'text': a1.name}, {'id': str(a2.pk), 'text': a2.name}],\n        'pagination': {'more': False},\n    })\n", "    def test_filter_by_user(self):\n        \"\"\"\n        The autocomplete view should only show results that the user has permission to view.\n        \"\"\"\n        q1 = Question.objects.create(question='Question 1', created_by=self.superuser)\n        q2 = Question.objects.create(question='Question 2', created_by=self.user)\n        request = self.factory.get(self.url, {'term': 'Question', **self.opts})\n        request.user = self.user\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q2.pk), 'text': q2.question}],\n            'pagination': {'more': False},\n        })\n", "def test_filtered_results(self):\n    \"\"\"\n    Search results should be filtered according to limit_choices_to,\n    if defined on the foreign key field.\n    \"\"\"\n    Author.objects.create(name='Author 1', is_active=True)\n    Author.objects.create(name='Author 2', is_active=False)\n    book = Book.objects.create(title='Test Book')\n    request = self.factory.get(self.url, {'term': 'Author', **self.opts, 'field_name': 'author'})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    # Only the active author should be returned\n    self.assertEqual(len(data['results']), 1)\n    self.assertEqual(data['results'][0]['text'], 'Author 1')\n", "def test_admin_view_permissions(self):\n    \"\"\"\n    Users require the view permission for the related model to access\n    the autocomplete view for it.\n    \"\"\"\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.user\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    p = Permission.objects.get(\n        content_type=ContentType.objects.get_for_model(Question),\n        codename='view_question',\n    )\n    self.user.user_permissions.add(p)\n    request.user = User.objects.get(pk=self.user.pk)\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n"], "sample_905": ["def test_isasyncgenfunction(app):\n    from target.functions import asyncgenfunc, func, partial_asyncgenfunc\n    from target.methods import Base\n\n    assert inspect.isasyncgenfunction(func) is False                   # function\n    assert inspect.isasyncgenfunction(asyncgenfunc) is True           # async-gen function\n    assert inspect.isasyncgenfunction(partial_asyncgenfunc) is True   # partial-ed async-gen function\n    assert inspect.isasyncgenfunction(Base.meth) is False              # method\n    assert inspect.isasyncgenfunction(Base.asyncgenmeth) is True      # async-gen method\n\n    # partial-ed async-gen method\n    partial_asyncgenmeth = Base.__dict__['partial_asyncgenmeth']\n    assert inspect.isasyncgenfunction(partial_asyncgenmeth) is True\n", "def test_isabstractmethod(app):\n    from target.methods import Base, AbstractBase\n\n    assert inspect.isabstractmethod(AbstractBase.abstractmeth) is True\n    assert inspect.isabstractmethod(Base.meth) is False\n", "def test_signature_annotations_with_type_aliases():\n    type_aliases = {'alias.MyType': 'int', 'alias.AnotherType': 'str'}\n        pass\n\n    sig = inspect.signature(func, type_aliases=type_aliases)\n    assert stringify_signature(sig) == '(a: int, b: str) -> int'\n", "def test_evaluate_signature_forwardref():\n    from typing import ForwardRef\n\n    # Test evaluating a ForwardRef\n    type_aliases = {\"MyType\": \"int\"}\n    sig = inspect.signature_from_str(\"(x: 'MyType')\")\n    evaluated_sig = inspect.evaluate_signature(sig, type_aliases=type_aliases)\n    assert evaluated_sig.parameters['x'].annotation == int\n\n    # Test evaluating a nested ForwardRef\n    type_aliases = {\"MyType.NestedType\": \"str\"}\n    sig = inspect.signature_from_str(\"(x: 'MyType.NestedType')\")\n    evaluated_sig = inspect.evaluate_signature(sig, type_aliases=type_aliases)\n    assert evaluated_sig.parameters['x'].annotation == str\n", "def test_getdoc_partial():\n        \"\"\"\n        This is the docstring for the function.\n        \"\"\"\n        pass\n\n    partial_func = functools.partial(func, 1)\n\n    assert inspect.getdoc(partial_func) == func.__doc__\n"], "sample_683": ["def test_encodedfile_close(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.close()\n    with pytest.raises(ValueError):\n        ef.write(\"test\")\n", "def test_logging_while_collecting_after_teardown(testdir):\n    \"\"\"Issue #6240: Calls to logging.xxx() during collection causes all logging calls to be duplicated to stderr.\n    Ensure that logging calls after teardown are not duplicated.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import logging\n\n            logging.warning(\"during teardown\")\n\n            logging.warning(\"during call\")\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    assert result.ret == ExitCode.TESTS_FAILED\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_*.py F*\",\n            \"====* FAILURES *====\",\n            \"____*____\",\n            \"*--- Captured log call*\",\n            \"WARNING * during call\",\n            \"WARNING * during teardown\",\n            \"*1 failed*\",\n        ]\n    )\n    result.stdout.no_fnmatch_line(\"*Captured stderr call*\")\n", "def test_logging_in_conftest(testdir):\n    \"\"\"Test that logging works properly in conftest.py\"\"\"\n    testdir.makeconftest(\n        \"\"\"\\\n        import logging\n        logging.basicConfig(level=logging.INFO)\n        logging.info('Logging from conftest.py')\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\\\n        import logging\n            logging.info('Logging from test')\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(['*Logging from conftest.py*', '*Logging from test*'])\n", "def test_encodedfile_readline(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.write(\"line1\\nline2\\nline3\")\n    ef.flush()\n    tmpfile.seek(0)\n    assert ef.readline() == \"line1\\n\"\n    assert ef.readline() == \"line2\\n\"\n    assert ef.readline() == \"line3\"\n    assert ef.readline() == \"\"\n    tmpfile.close()\n    with pytest.raises(ValueError):\n        ef.write(\"line4\")\n", "def test_logging_during_setup(testdir):\n    \"\"\"Issue #6240: Calls to logging.xxx() during setup causes all logging calls to be duplicated to stderr\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import logging\n        import pytest\n\n        @pytest.fixture(scope=\"module\")\n            logging.warning(\"during setup\")\n\n            logging.warning(\"during call\")\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    assert result.ret == ExitCode.TESTS_FAILED\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_*.py F*\",\n            \"====* FAILURES *====\",\n            \"____*____\",\n            \"*--- Captured log setup*\",\n            \"WARNING * during setup\",\n            \"*--- Captured log call*\",\n            \"WARNING * during call\",\n            \"*1 failed*\",\n        ]\n    )\n    result.stdout.no_fnmatch_line(\"*Captured stderr call*\")\n"], "sample_1054": ["def test_ComplexRegion_from_real():\n    unit = Interval(0, 1)\n    assert ComplexRegion.from_real(unit) == ComplexRegion(unit * FiniteSet(0))\n    raises(ValueError, lambda: ComplexRegion.from_real(S.Complexes))\n", "def test_ComplexRegion_from_real():\n    unit = Interval(0, 1)\n    assert ComplexRegion.from_real(unit) == ComplexRegion(unit * FiniteSet(0))\n    assert ComplexRegion.from_real(S.Reals) == ComplexRegion(S.Reals * FiniteSet(0))\n    raises(ValueError, lambda: ComplexRegion.from_real(S.Complexes))\n", "def test_complex_region_from_real():\n    unit = Interval(0, 1)\n    expected_output = ComplexRegion(Interval(0, 1) * FiniteSet(0), False)\n    assert ComplexRegion.from_real(unit) == expected_output\n", "def test_ComplexRegion_difference():\n    # Polar form\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n\n    p1 = c1.difference(c2)\n    assert p1 == ComplexRegion(Interval(0, 1)*Interval(S.Pi, 2*S.Pi), polar=True)\n\n    # Rectangular form\n    c5 = ComplexRegion(Interval(2, 5)*Interval(6, 9))\n    c6 = ComplexRegion(Interval(4, 6)*Interval(7, 10))\n\n    p3 = c5.difference(c6)\n    assert p3 == ComplexRegion(Union(Interval(2, 4)*Interval(6, 9), Interval(4, 5)*Interval(6, 7), Interval(4, 5)*Interval(10, 9)), evaluate=False)\n", "def test_complexregion_symmetry():\n    a, b = Interval(0, 1), Interval(0, pi)\n    cp1 = ComplexRegion(a * b, polar=True)\n\n    # Test symmetry of ComplexRegion\n    assert cp1.intersect(Interval(0, 1)) == cp1.intersect(Interval(-1, 0))\n    assert cp1.intersect(Interval(1, 2)) == cp1.intersect(Interval(-2, -1))\n\n    cp2 = ComplexRegion(a * a)\n    assert cp2.intersect(Interval(0, 1)) == cp2.intersect(Interval(-1, 0))\n"], "sample_1182": ["def test_SymPyPrinter_print_seq():\n    s = SymPyPrinter()\n\n    assert s._print_seq(range(2)) == '(0, 1)'\n", "def test_issue_23672():\n    from sympy.functions.special.gamma_functions import loggamma\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(loggamma(x)) == 'math.lgamma(x)'\n", "def test_hyper():\n    from sympy.functions.special.hyper import hyper\n\n    expr = hyper([1, 2], [3, 4], x)\n\n    prntr = SciPyPrinter()\n    assert \"Not supported\" in prntr.doprint(expr)\n\n    prntr = NumPyPrinter()\n    assert \"Not supported\" in prntr.doprint(expr)\n\n    prntr = PythonCodePrinter()\n    assert \"Not supported\" in prntr.doprint(expr)\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.hyper([1, 2], [3, 4], x)'\n", "def test_hprint_Pow():\n    prntr = PythonCodePrinter()\n    assert prntr._hprint_Pow(sqrt(x), rational=False, sqrt='numpy.sqrt') == 'numpy.sqrt(x)'\n    assert prntr._hprint_Pow(sqrt(x), rational=True, sqrt='numpy.sqrt') == 'x**(1/2)'\n    assert prntr._hprint_Pow(1/sqrt(x), rational=False, sqrt='numpy.sqrt') == '1/numpy.sqrt(x)'\n    assert prntr._hprint_Pow(1/sqrt(x), rational=True, sqrt='numpy.sqrt') == 'x**(-1/2)'\n", "def test_log2():\n    from sympy.functions.elementary.exponential import log\n\n    expr = log(x, 2)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log2(x)'\n"], "sample_1160": ["def test_issue_17860():\n    r = Range(1, 10, 2)\n    assert r[:1] == Range(1, 3, 2)\n    assert r[::-1] == Range(9, 1, -2)\n    assert r[:-1] == Range(1, 9, 2)\n", "def test_range_slicing_with_step():\n    # Test slicing with a step on a Range\n    r = Range(1, 10, 2)\n    assert r[::2] == Range(1, 10, 4)\n    assert r[1::2] == Range(3, 10, 4)\n    assert r[::-2] == Range(9, 0, -4)\n    assert r[1::-2] == Range(7, 0, -4)\n", "def test_imageset_as_relational():\n    x = Symbol('x', real=True)\n    assert imageset(x, 2*x, S.Integers).as_relational(x) == Eq(Mod(x, 2), 0)\n    assert imageset(x, x**2, S.Integers).as_relational(x) == Eq(Mod(x, 1), 0)\n    y = Symbol('y', integer=True)\n    assert imageset(x, x + y, S.Integers).as_relational(x) == Eq(Mod(x + y, 1), 0)\n", "def test_range_intersection():\n    # Test intersection of Ranges with different start and end values\n    assert Range(1, 5).intersect(Range(3, 7)) == Range(3, 5)\n    assert Range(1, 5).intersect(Range(6, 10)) == S.EmptySet\n\n    # Test intersection of Ranges with same start and end values\n    assert Range(1, 5).intersect(Range(1, 5)) == Range(1, 5)\n\n    # Test intersection of Ranges with different step values\n    assert Range(1, 10, 2).intersect(Range(2, 11, 3)) == Range(2, 10, 6)\n\n    # Test intersection of Ranges with reverse step values\n    assert Range(10, 1, -1).intersect(Range(5, 15, 2)) == Range(6, 10, 2)\n\n    # Test intersection of Ranges with no overlap\n    assert Range(1, 5).intersect(Range(6, 10)) == S.EmptySet\n", "def test_imageset_complex_interval_intersection():\n    from sympy.abc import n\n    f1 = ImageSet(Lambda(n, I*n), Interval(0, 1))\n    f2 = ImageSet(Lambda(n, n*I + 2), S.Integers)\n    f3 = ImageSet(Lambda(n, n + I*n), S.Integers)\n    assert f1.intersect(Interval(-1, 1)) == Interval(-I, I)\n    assert f2.intersect(Interval(0, 1)) == S.EmptySet\n    assert f3.intersect(Interval(0, 1)) == Interval(0, sqrt(2)) * Interval(0, sqrt(2))\n"], "sample_1006": ["def test_rf_series():\n    x, n = symbols('x n')\n    assert rf(x, n).series(n, 0, 3) == x + O(n**3)\n", "def test_subfactorial_simplification():\n    n = Symbol('n', integer=True, nonnegative=True)\n    assert subfactorial(n + 1) == (n + 1)*subfactorial(n) + (-1)**n\n", "def test_factorial_Mod():\n    n = Symbol('n', integer=True, nonnegative=True)\n    q = Symbol('q', integer=True)\n    assert factorial(n)._eval_Mod(q) == 0\n    assert factorial(n+q)._eval_Mod(q) == 0\n    assert factorial(n+q+1)._eval_Mod(q) == 0\n    assert factorial(n+q+2)._eval_Mod(q) != 0\n    assert factorial(n+q-1)._eval_Mod(q) == -1 % q\n    assert factorial(n+q-2)._eval_Mod(q) == 0\n    assert factorial(n+q-3)._eval_Mod(q) == 0\n    assert factorial(n+q-4)._eval_Mod(q) == 0\n    assert factorial(n+q-5)._eval_Mod(q) != 0\n", "def test_factorial_eval_mod():\n    x = Symbol('x', integer=True, nonnegative=True)\n    y = Symbol('y', integer=True, nonnegative=True)\n    z = Symbol('z', integer=True)\n\n    assert factorial(x)._eval_Mod(3) is None\n    assert factorial(2)._eval_Mod(3) == 2\n    assert factorial(3)._eval_Mod(3) == 0\n    assert factorial(4)._eval_Mod(3) == 0\n    assert factorial(5)._eval_Mod(3) == 2\n    assert factorial(x)._eval_Mod(4) is None\n    assert factorial(2)._eval_Mod(4) == 2\n    assert factorial(3)._eval_Mod(4) == 6\n    assert factorial(4)._eval_Mod(4) == 0\n    assert factorial(5)._eval_Mod(4) == 0\n    assert factorial(z)._eval_Mod(y) is None\n    assert factorial(-1)._eval_Mod(y) is None\n", "def test_factorial_Mod():\n    x = Symbol('x', integer=True, nonnegative=True)\n    y = Symbol('y', integer=True, positive=True)\n    z = Symbol('z', integer=True, negative=True)\n\n    assert factorial(x)._eval_Mod(10) == 0\n    assert factorial(x)._eval_Mod(11) == 0\n    assert factorial(x)._eval_Mod(12) == 0\n    assert factorial(x + 1)._eval_Mod(11) is None\n    assert factorial(x)._eval_Mod(y) is None\n    assert factorial(x)._eval_Mod(z) is None\n    assert factorial(x)._eval_Mod(1) == 0\n    assert factorial(y)._eval_Mod(6) == 0\n    assert factorial(y)._eval_Mod(7) == 1\n    assert factorial(z)._eval_Mod(-6) == 0\n    assert factorial(z)._eval_Mod(-7) == 0\n"], "sample_208": ["def test_swappable_circular_multi_mti_with_fk(self):\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        parent = ModelState('a', 'Parent', [\n            ('user', models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE))\n        ])\n        child = ModelState('a', 'Child', [\n            ('parent', models.ForeignKey('a.Parent', models.CASCADE, related_name='children'))\n        ], bases=('a.Parent',))\n        user = ModelState('a', 'User', [], bases=(AbstractBaseUser, 'a.Child'))\n        changes = self.get_changes([], [parent, child, user])\n    self.assertNumberMigrations(changes, 'a', 1)\n    self.assertOperationTypes(changes, 'a', 0, ['CreateModel', 'CreateModel', 'CreateModel', 'AddField'])\n", "def test_add_unique_constraint(self):\n    \"\"\"Adding a unique constraint generates a migration.\"\"\"\n    changes = self.get_changes([self.author_name], [self.author_name_unique_constraint])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    added_constraint = models.UniqueConstraint(fields=['name'], name='unique_name')\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', constraint=added_constraint)\n", "def test_add_model_with_m2m_field_removed_from_base_model(self):\n    \"\"\"\n    Removing a base m2m field takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'Base', [\n            ('id', models.AutoField(primary_key=True)),\n            ('authors', models.ManyToManyField('app.Author')),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Base', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Book', [\n            ('authors', models.ManyToManyField('app.Author')),\n        ], bases=('app.Base',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='authors', model_name='base')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_add_field_with_through_model(self):\n    \"\"\"\n    #23439 - Adding a field with a through model first creates the\n    through model, then the field.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_m2m_through, self.publisher],\n        [self.author_with_m2m_through_and_field, self.publisher, self.contract],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AddField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Contract')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', name='extra_field')\n", "def test_add_field_with_default_function(self):\n    \"\"\"#22030 - Adding a field with a default function should work.\"\"\"\n        return \"Default Value\"\n\n    changes = self.get_changes([self.author_empty], [self.author_name_default_function])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n"], "sample_233": ["def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with an algorithm other than the default\n    by using the PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'sha224'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertEqual(p1.algorithm, settings.DEFAULT_HASHING_ALGORITHM)\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_changed_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepwuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_changed_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepwuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_496": ["def test_no_suggestions_for_ambiguous_commands(self):\n    args = ['help', '--settings=test_project.settings']\n    out, err = self.run_django_admin(args)\n    self.assertNoOutput(err)\n    self.assertNotInOutput(out, 'Did you mean')\n", "    def setUp(self):\n        self.write_settings('settings.py')\n", "def test_custom_project_template_with_empty_files(self):\n    \"\"\"\n    The startproject management command is able to render empty files.\n    \"\"\"\n    template_path = os.path.join(custom_templates_dir, 'project_template')\n    args = ['startproject', '--template', template_path, '--extension=txt', 'customtestproject']\n    testproject_dir = os.path.join(self.test_dir, 'customtestproject')\n    self.addCleanup(shutil.rmtree, testproject_dir, True)\n\n    out, err = self.run_django_admin(args)\n    self.assertNoOutput(err)\n    self.assertTrue(os.path.isdir(testproject_dir))\n    path = os.path.join(testproject_dir, 'ticket-18091-empty-template.txt')\n    with codecs.open(path, 'r', encoding='utf-8') as f:\n        self.assertEqual(f.read().splitlines(False), [])\n", "def test_base_command_with_invalid_option(self):\n    \"User BaseCommands outputs command usage when wrong option is specified\"\n    args = ['base_command', '--invalid']\n    out, err = self.run_manage(args)\n    self.assertNoOutput(out)\n    self.assertOutput(err, \"usage: manage.py base_command\")\n    self.assertOutput(err, \"error: unrecognized arguments: --invalid\")\n", "def test_no_database(self):\n    \"\"\"\n    Ensure runserver.check_migrations doesn't choke on empty DATABASES.\n    \"\"\"\n    tested_connections = ConnectionHandler({})\n    with mock.patch('django.core.management.base.connections', new=tested_connections):\n        self.cmd.check_migrations()\n"], "sample_190": ["def test_isnull_with_none_as_rhs(self):\n    \"\"\"Lookup.can_use_none_as_rhs=True allows None as a lookup value.\"\"\"\n    season = Season.objects.create(year=2012, nulled_text_field=None)\n    self.assertTrue(Season.objects.filter(pk=season.pk, nulled_text_field__isnull=None))\n", "def test_isnull_with_none_as_rhs(self):\n    \"\"\"Lookup.can_use_none_as_rhs=True allows None as a lookup value.\"\"\"\n    season = Season.objects.create(year=2012, nulled_text_field=None)\n    self.assertTrue(Season.objects.filter(pk=season.pk, nulled_text_field__isnull=None))\n", "def test_isnull_boolean_field(self):\n    # Test isnull lookup on BooleanField\n    Season.objects.create(year=2012, boolean_field=True)\n    Season.objects.create(year=2013, boolean_field=False)\n    Season.objects.create(year=2014, boolean_field=None)\n\n    self.assertQuerysetEqual(\n        Season.objects.filter(boolean_field__isnull=True),\n        ['<Season: 2014>']\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(boolean_field__isnull=False),\n        ['<Season: 2012>', '<Season: 2013>']\n    )\n", "def test_isnull_boolean_field(self):\n    # Test the isnull lookup for a BooleanField.\n    # Create some objects with boolean field values True and False.\n    Freebie.objects.create(name='Freebie 1', stock=True)\n    Freebie.objects.create(name='Freebie 2', stock=False)\n    Freebie.objects.create(name='Freebie 3')\n\n    # Test isnull lookup for True.\n    self.assertQuerysetEqual(\n        Freebie.objects.filter(stock__isnull=True),\n        ['<Freebie: Freebie 3>']\n    )\n\n    # Test isnull lookup for False.\n    self.assertQuerysetEqual(\n        Freebie.objects.filter(stock__isnull=False),\n        ['<Freebie: Freebie 1>', '<Freebie: Freebie 2>']\n    )\n", "def test_exact_query_rhs_with_multiple_selected_columns(self):\n    newest_author = Author.objects.create(name='Author 2')\n    authors_max_ids = Author.objects.filter(\n        name='Author 2',\n    ).values(\n        'name', 'id',\n    ).annotate(\n        max_id=Max('id'),\n    ).values('name', 'max_id')\n    authors = Author.objects.filter(id=authors_max_ids[:1])\n    self.assertEqual(authors.get(), newest_author)\n"], "sample_841": ["def test_ridge_classifier_cv_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n", "def test_ridge_classifier_dtype_stability(solver, seed):\n    random_state = np.random.RandomState(seed)\n    n_samples, n_features = 6, 5\n    X = random_state.randn(n_samples, n_features)\n    y = random_state.randint(0, 2, size=n_samples)\n    alpha = 1.0\n    results = dict()\n    # XXX: Sparse CG seems to be far less numerically stable than the\n    # others, maybe we should not enable float32 for this one.\n    atol = 1e-3 if solver == \"sparse_cg\" else 1e-5\n    for current_dtype in (np.float32, np.float64):\n        results[current_dtype] = RidgeClassifier(alpha=alpha, solver=solver).fit(X.astype(current_dtype), y).coef_\n\n    assert results[np.float32].dtype == np.float32\n    assert results[np.float64].dtype == np.float64\n    assert_allclose(results[np.float32], results[np.float64], atol=atol)\n", "def test_ridge_with_X_offset():\n    # check that Ridge can handle an offset in X\n    X, y, coef = _make_sparse_offset_regression(n_features=20, random_state=0, coef=True)\n    X_csr = sp.csr_matrix(X)\n\n    dense_ridge = Ridge(alpha=1., fit_intercept=True)\n    sparse_ridge = Ridge(alpha=1., fit_intercept=True)\n    dense_ridge.fit(X, y)\n    sparse_ridge.fit(X_csr, y)\n\n    assert np.allclose(dense_ridge.intercept_, sparse_ridge.intercept_)\n    assert np.allclose(dense_ridge.coef_, coef)\n    assert np.allclose(sparse_ridge.coef_, coef)\n", "def test_ridge_gcv_sparse_input():\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n    X_sparse = sp.csr_matrix(X)\n    ridge_cv = RidgeCV(fit_intercept=True, gcv_mode='svd')\n    ridge_cv.fit(X_sparse, y)\n    assert ridge_cv.score(X_sparse, y) > 0.8\n", "def test_ridge_classifier_multi_label():\n    X, y = make_multilabel_classification(n_samples=10, n_labels=2, random_state=0)\n    ridge = RidgeClassifierCV(multi_class='ovr')\n    ridge.fit(X, y)\n    assert ridge.coef_.shape == (2, X.shape[1])\n    y_pred = ridge.predict(X)\n    assert np.mean(y == y_pred) > 0.5\n"], "sample_876": ["def test_mlp_warm_start_with_early_stopping_and_convergence(MLPEstimator):\n    \"\"\"Check that early stopping works with warm start even when convergence is reached.\"\"\"\n    mlp = MLPEstimator(\n        max_iter=10, random_state=0, warm_start=True, early_stopping=True, tol=1e-6\n    )\n    mlp.fit(X_iris, y_iris)\n    n_validation_scores = len(mlp.validation_scores_)\n    mlp.set_params(max_iter=20)\n    mlp.fit(X_iris, y_iris)\n    assert len(mlp.validation_scores_) > n_validation_scores\n    assert mlp.n_iter_ == 10  # Convergence was reached before max_iter\n", "def test_mlp_classifier_multilabel_binarizer():\n    # Test that LabelBinarizer is used in multi-label classification\n    X, y = make_multilabel_classification(n_samples=50, random_state=0, return_indicator=True)\n    mlp = MLPClassifier(solver=\"lbfgs\", hidden_layer_sizes=50, alpha=1e-5, max_iter=150, random_state=0, activation=\"logistic\", learning_rate_init=0.2)\n    mlp.fit(X, y)\n    assert isinstance(mlp._label_binarizer, LabelBinarizer)\n    assert_array_equal(mlp._label_binarizer.classes_, np.unique(y))\n", "def test_mlp_predict_proba_shape_multilabel():\n    # Test that predict_proba returns a 2D array for multilabel\n    X, y = make_multilabel_classification(\n        n_samples=50, n_classes=3, random_state=0, return_indicator=True\n    )\n    mlp = MLPClassifier(solver=\"lbfgs\", random_state=0)\n    mlp.fit(X, y)\n    y_proba = mlp.predict_proba(X)\n    assert y_proba.ndim == 2\n", "def test_mlp_different_batch_sizes():\n    \"\"\"Check that the MLP estimator works with different batch sizes.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5, random_state=42)\n    batch_sizes = [5, 10, 20, 50, 100]\n    for batch_size in batch_sizes:\n        mlp = MLPClassifier(solver=\"sgd\", batch_size=batch_size, random_state=42)\n        mlp.fit(X, y)\n        score = mlp.score(X, y)\n        assert score > 0.95\n", "def test_batch_size_auto():\n    # Test batch size 'auto'\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n    mlp = MLPClassifier(batch_size='auto', solver='sgd', max_iter=2, random_state=1)\n    mlp.fit(X, y)\n    assert mlp.batch_size <= min(200, len(X))\n"], "sample_145": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n", "def test_exclude_in_inline(self):\n    class ValidationTestInline(TabularInline):\n        model = ValidationTestInlineModel\n        exclude = ('parent',)\n\n    class TestModelAdmin(ModelAdmin):\n        inlines = [ValidationTestInline]\n\n    self.assertIsInvalid(\n        TestModelAdmin, ValidationTestModel,\n        \"Cannot exclude the field 'parent', because it is the foreign key \"\n        \"to the parent model 'modeladmin.ValidationTestModel'.\",\n        'admin.E201',\n        invalid_obj=ValidationTestInline\n    )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            'admin.E014'\n        )\n"], "sample_479": ["def test_create_model_add_index(self):\n    \"\"\"\n    AddIndex should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n            ),\n            migrations.AddIndex(\n                \"Foo\",\n                models.Index(fields=[\"name\"], name=\"idx_foo_name\"),\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"indexes\": [models.Index(fields=[\"name\"], name=\"idx_foo_name\")]},\n            ),\n        ],\n    )\n", "def test_create_model_alter_table_comment(self):\n    \"\"\"\n    AlterModelTableComment should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AlterModelTableComment(\"Foo\", \"New comment\"),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [(\"name\", models.CharField(max_length=255))],\n                options={\"db_table_comment\": \"New comment\"},\n            ),\n        ],\n    )\n", "def test_create_alter_table_comment(self):\n    \"\"\"\n    AlterModelTableComment should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"db_table_comment\": \"Old comment\"},\n            ),\n            migrations.AlterModelTableComment(\n                name=\"Foo\", table_comment=\"New comment\"\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"db_table_comment\": \"New comment\"},\n            ),\n        ],\n    )\n", "def test_create_alter_table_comment(self):\n    \"\"\"\n    CreateModel and AlterModelTableComment should absorb each other.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))],\n                options={\"db_table_comment\": \"Initial comment\"},\n            ),\n            migrations.AlterModelTableComment(\"Foo\", \"New comment\"),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))],\n                options={\"db_table_comment\": \"New comment\"},\n            ),\n        ],\n    )\n", "def test_create_alter_table_comment_model(self):\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", fields=[]),\n            migrations.AlterModelTableComment(\n                name=\"Foo\", table_comment=\"A comment on the Foo table\"\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[],\n                options={\"db_table_comment\": \"A comment on the Foo table\"},\n            ),\n        ],\n    )\n\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[],\n                options={\"db_table_comment\": \"Old comment\"},\n            ),\n            migrations.AlterModelTableComment(\n                name=\"Foo\", table_comment=\"New comment\"\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[],\n                options={\"db_table_comment\": \"New comment\"},\n            ),\n        ],\n    )\n\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[],\n                options={\"db_table_comment\": \"Old comment\"},\n            ),\n            migrations.AlterModelTableComment(\n                name=\"Foo\", table_comment=\"Old comment\"\n            ),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[],\n                options={\"db_table_comment\": \"Old comment\"},\n            ),\n        ],\n    )\n\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", fields=[]),\n            migrations.AlterModelTableComment(name=\"Foo\", table_comment=None),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                fields=[],\n            ),\n        ],\n    )\n"], "sample_313": ["def test_get_template_directories_with_non_django_path(self, mock_is_django_path):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            ROOT / 'templates_extra',\n            ROOT / 'templates',\n        }\n    )\n    mock_is_django_path.assert_called()\n", "def test_to_path_called(self, mock_to_path):\n    mock_to_path.return_value = 'mock_dir'\n    autoreload.get_template_directories()\n    mock_to_path.assert_called()\n", "def test_django_path_filtered(self, mock_get_dirs):\n    mock_get_dirs.return_value = [\n        '/usr/lib/python3.8/site-packages/django/contrib/admin/templates',\n        str(EXTRA_TEMPLATES_DIR),\n    ]\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n        }\n    )\n", "def test_ignore_django_path(self, mock_is_django_path):\n    mock_is_django_path.side_effect = lambda x: 'django_path' in str(x)\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            Path.cwd() / 'non_django_path',\n        }\n    )\n", "    def test_django_path_not_added_to_template_directories(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                EXTRA_TEMPLATES_DIR,\n            }\n        )\n"], "sample_258": ["    def test_send_caching(self):\n        d_signal.connect(receiver_1_arg, sender=self)\n        result = d_signal.send(sender=self, val='test')\n        self.assertEqual(result, [(receiver_1_arg, 'test')])\n        # Check that the receivers are cached\n        self.assertEqual(d_signal.sender_receivers_cache[self], [weakref.ref(receiver_1_arg)])\n        d_signal.disconnect(receiver_1_arg, sender=self)\n        # Check that the cache is cleared\n        self.assertEqual(d_signal.sender_receivers_cache, {})\n        self.assertTestIsClean(d_signal)\n", "def test_send_robust_cache_invalidation(self):\n    d_signal.connect(receiver_1_arg)\n    result = d_signal.send(sender=self, val='test')\n    self.assertEqual(result, [(receiver_1_arg, 'test')])\n\n    # Invalidating the cache for the sender\n    d_signal.sender_receivers_cache.clear()\n\n    # The cache should be repopulated on the next send\n    result = d_signal.send(sender=self, val='test')\n    self.assertEqual(result, [(receiver_1_arg, 'test')])\n\n    d_signal.disconnect(receiver_1_arg)\n    self.assertTestIsClean(d_signal)\n", "    def test_cached_signal_no_receivers(self):\n        \"\"\"\n        Make sure cached signal sends don't return receivers that don't exist anymore.\n        \"\"\"\n        class sender:\n            pass\n        d_signal.connect(receiver_1_arg)\n        d_signal.send(sender, val='test1')\n        d_signal.disconnect(receiver_1_arg)\n        result = d_signal.send(sender, val='test2')\n        self.assertEqual(result, [])\n        self.assertTestIsClean(d_signal)\n", "    def test_send_with_specific_sender(self):\n        a_signal.connect(receiver_1_arg, sender=self)\n        a_signal.connect(receiver_1_arg, sender=object)\n        result = a_signal.send(sender=self, val='test')\n        self.assertEqual(result, [(receiver_1_arg, 'test')])\n        result = a_signal.send(sender=object, val='test')\n        self.assertEqual(result, [(receiver_1_arg, 'test')])\n        a_signal.disconnect(receiver_1_arg, sender=self)\n        a_signal.disconnect(receiver_1_arg, sender=object)\n        self.assertTestIsClean(a_signal)\n", "    def test_cached_receivers(self):\n        \"\"\"Test that cached receivers are returned when available.\"\"\"\n        d_signal.connect(receiver_1_arg)\n        result = d_signal.send(sender=self, val='test1')\n        self.assertEqual(result, [(receiver_1_arg, 'test1')])\n        # Ensure receivers are cached\n        self.assertEqual(d_signal.sender_receivers_cache[self], [receiver_1_arg])\n        # Send a second time and ensure cached receivers are returned\n        result = d_signal.send(sender=self, val='test2')\n        self.assertEqual(result, [(receiver_1_arg, 'test2')])\n        d_signal.disconnect(receiver_1_arg)\n        self.assertTestIsClean(d_signal)\n"], "sample_645": ["def test_log_report_captures_according_to_config_option_upon_success(\n    pytester: Pytester,", "def test_caplog_captures_according_to_config_option_upon_success(pytester: Pytester) -> None:\n    \"\"\"Test that upon success:\n    (1) `caplog` succeeded to capture the INFO and WARNING messages and assert on them => No `Exception` is raised.\n    (2) The `DEBUG` message does NOT appear in the `Captured log call` report.\n    (3) The `INFO` and `WARNING` messages DO appear in the test report due to `--log-level=INFO`.\n    (4) The `Print` message DOES appear in the test report as it's not captured by `caplog`.\n    \"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.INFO\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            assert 'INFO log message' in caplog.text\n            assert 'WARNING log message' in caplog.text\n            assert 'DEBUG log message' not in caplog.text\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"--log-level=INFO\")\n    result.stdout.no_fnmatch_line(\"*Exception*\")\n    result.stdout.no_fnmatch_line(\"*DEBUG log message*\")\n    result.stdout.fnmatch_lines(\n        [\"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"]\n    )\n    assert result.ret == 0\n", "def test_log_level_from_ini_overrides_option(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.ERROR\n            logger = logging.getLogger('catchlog')\n            logger.warning(\"WARNING message won't be shown\")\n            logger.error(\"ERROR message will be shown\")\n            assert 'WARNING' not in caplog.text\n            assert 'ERROR' in caplog.text\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_level=WARNING\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"--log-level=ERROR\")\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    # make sure that the log level from ini overrides the one from option\n    result.stdout.fnmatch_lines([\"*WARNING message won't be shown*\"])\n    result.stdout.fnmatch_lines([\"*ERROR message will be shown*\"])\n", "def test_log_cli_enabled(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin._log_cli_enabled() == False\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli = False\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli = True\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_caplog_captures_debug_messages(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logger = logging.getLogger('catchlog')\n            caplog.set_level(logging.DEBUG, logger.name)\n\n            logger.debug(\"DEBUG message will be shown\")\n            logger.info(\"INFO message will be shown\")\n\n            assert \"DEBUG message will be shown\" in caplog.text\n            assert \"INFO message will be shown\" in caplog.text\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n"], "sample_501": ["def test_legend_get_set_frame_on():\n    legend = plt.legend()\n    assert legend.get_frame_on() == mpl.rcParams[\"legend.frameon\"]\n    legend.set_frame_on(False)\n    assert legend.get_frame_on() is False\n    legend.set_frame_on(True)\n    assert legend.get_frame_on() is True\n", "def test_legend_edgecolor_inherit():\n    # Test edgecolor='inherit' keyword for Legend.\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(edgecolor='inherit')\n    assert leg.get_frame().get_edgecolor() == ax.spines['right'].get_edgecolor()\n", "def test_legend_shadow_framealpha(shadow, framealpha, expected_alpha):\n    fig, ax = plt.subplots()\n    ax.plot(range(100), label=\"test\")\n    kwargs = {\"shadow\": shadow}\n    if framealpha is not None:\n        kwargs[\"framealpha\"] = framealpha\n    leg = ax.legend(**kwargs)\n    assert leg.get_frame().get_alpha() == expected_alpha\n", "def test_legend_location_center():\n    # test that the 'center' location works properly\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='Aardvark')\n    leg = ax.legend(loc='center')\n    assert leg._loc_real == 'center'\n    assert leg._loc == 10  # 'center' corresponds to 10 in legend codes\n", "def test_legend_style_iter():\n    # test legend with different line styles as iterator\n    fig, ax = plt.subplots()\n    line_styles = iter(['-', '--', ':'])\n    for i in range(3):\n        ax.plot(range(i, i+3), label='Line {}'.format(i), linestyle=next(line_styles))\n    ax.legend(loc='best')\n"], "sample_1144": ["def test_requires_partial_with_integer_symbols():\n    x, y = symbols('x y', integer=True)\n    f = x * y\n    assert requires_partial(Derivative(f, x)) is False\n    assert requires_partial(Derivative(f, y)) is False\n", "def test_requires_partial_multiple_derivatives():\n    x, y, z = symbols('x y z')\n    f = Function('f')(x, y, z)\n    assert requires_partial(Derivative(f, x, y, z)) is True\n    assert requires_partial(Derivative(f, x, x, y, y, z, z)) is True\n", "def test_requires_partial_with_derivative_expression():\n    x, y = symbols('x y')\n    f = x * y\n    d = Derivative(f, x, y)\n    assert requires_partial(d) is True\n    d = Derivative(f, x)\n    assert requires_partial(d) is True\n    d = Derivative(f, y)\n    assert requires_partial(d) is True\n", "def test_requires_partial_with_integers_in_expr():\n    x, y, z = symbols('x y z')\n    n = symbols('n', integer=True)\n\n    # expression with integer in it\n    f = x + n\n    assert requires_partial(Derivative(f, x)) is False\n    assert requires_partial(Derivative(f, n)) is False\n\n    # expression with multiple free symbols and an integer\n    f = x * y + n\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, y)) is True\n    assert requires_partial(Derivative(f, n)) is False\n\n    # expression with integer and a function\n    f = Function('f')(x) + n\n    assert requires_partial(Derivative(f, x)) is False\n    assert requires_partial(Derivative(f, n)) is False\n", "def test_requires_partial_complex_variables():\n    x, y = symbols('x y', complex=True)\n    f = x * y\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, y)) is True\n"], "sample_991": ["def test_product_with_zero_factor():\n    assert product(k, (k, 1, n)) == factorial(n)\n    assert product(0, (k, 1, n)) == 0\n", "def test_product_of_sum():\n    x, n = symbols('x n', integer=True)\n    p = Product(Sum(x/k, (k, 1, n)), (n, 1, oo))\n    assert p.doit() == Product(x*harmonic(n), (n, 1, oo))\n", "def test_product_doit_deep():\n    p = Product(x**k, (k, 1, n))\n    assert p.doit(deep=False) == factorial(n)*x**(n*(n + 1)/2)\n    assert p.doit(deep=True) == x**(n*(n + 1)/2)\n", "def test_product_with_fractional_limit():\n    x = Symbol('x', real=True)\n    p = Product(x**(1/n), (n, 1, oo))\n    assert p.is_convergent() is S.false\n    assert product(x**(1/n), (n, 1, oo)) == p.doit()\n\n    p = Product(x**(-1/n), (n, 1, oo))\n    assert p.is_convergent() is S.false\n    assert product(x**(-1/n), (n, 1, oo)) == p.doit()\n", "def test_product_with_fractional_limits():\n    x = Symbol('x')\n    assert product(x, (x, 1, 3)) == x**3\n    assert product(x, (x, 1, 3.5)) != x**(3.5)\n    assert isinstance(product(x, (x, 1, 3.5)), Product)\n    assert isinstance(product(x, (x, 1, Rational(7, 2))), Product)\n"], "sample_144": ["def test_inheritance_update_fields(self):\n    r = Restaurant.objects.create(\n        name=\"Test Restaurant\",\n        address=\"123 Test St\",\n        serves_hot_dogs=True,\n        serves_pizza=True,\n    )\n    r.serves_hot_dogs = False\n    r.save(update_fields=['serves_hot_dogs'])\n    r.refresh_from_db()\n    self.assertFalse(r.serves_hot_dogs)\n    self.assertTrue(r.serves_pizza)\n", "def test_inherited_meta_options(self):\n    \"\"\"\n    Regression test for #12600\n    \"\"\"\n    # Meta options should be inherited from abstract base classes.\n    self.assertEqual(QualityControl._meta.verbose_name, 'quality control')\n    self.assertEqual(QualityControl._meta.verbose_name_plural, 'quality controls')\n    self.assertEqual(QualityControl._meta.ordering, ['pub_date'])\n", "def test_inheritance_with_related_name(self):\n    # Regression test for #21644\n    p1 = Person.objects.create(name='Alice')\n    p2 = Person.objects.create(name='Bob')\n    p3 = Person.objects.create(name='Charlie')\n\n    congressman = Congressman.objects.create(name='John', title='Senator', state='NY', person=p1)\n    politician = Politician.objects.create(name='Jane', title='Representative', person=p2)\n\n    self.assertEqual(p1.congressman, congressman)\n    self.assertEqual(p2.politician, politician)\n    self.assertFalse(hasattr(p3, 'congressman'))\n    self.assertFalse(hasattr(p3, 'politician'))\n\n    congressman.person = p3\n    congressman.save()\n\n    self.assertEqual(p3.congressman, congressman)\n    self.assertFalse(hasattr(p1, 'congressman'))\n    self.assertFalse(hasattr(p2, 'congressman'))\n\n    politician.person = p3\n    politician.save()\n\n    self.assertFalse(hasattr(p3, 'politician'))\n    self.assertEqual(p2.politician, politician)\n    self.assertFalse(hasattr(p1, 'politician'))\n", "def test_save_base_with_explicit_pk(self):\n    \"\"\"\n    Test save_base() method with an explicit primary key.\n    \"\"\"\n    r = Restaurant(pk=1, name='Test', address='Test Address', serves_pizza=True, serves_hot_dogs=False)\n    r.save_base(raw=True)\n    self.assertEqual(Restaurant.objects.get(pk=1), r)\n", "def test_inheritance_with_custom_pk(self):\n    profile = Profile.objects.create(username='testuser', extra='extra info')\n    self.assertEqual(profile.user_ptr_id, profile.pk)\n    self.assertEqual(profile.user_ptr.username, profile.username)\n\n    user = User.objects.get(pk=profile.user_ptr_id)\n    self.assertEqual(user.profile, profile)\n    self.assertEqual(user.profile.extra, 'extra info')\n"], "sample_749": ["def test_column_transformer_invalid_remainder_transformer():\n\n    class NoTrans(BaseEstimator):\n            return self\n\n            return X\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder=NoTrans())\n    assert_raise_message(TypeError, \"All estimators should implement fit\",\n                         ct.fit, X_array)\n", "def test_column_transformer_with_custom_transformer():\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            return self\n\n            return X * 2\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_both = X_array * 2\n\n    ct = ColumnTransformer([('custom', CustomTransformer(), [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 1\n", "def test_column_transformer_drop_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', 'drop', [0])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array).shape, (3, 0))\n    assert_array_equal(ct.fit(X_array).transform(X_array).shape, (3, 0))\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][0] == 'trans1'\n", "def test_column_transformer_with_no_transformers():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([], remainder='drop')\n    assert_raise_message(ValueError, \"No valid specification of the columns\", ct.fit, X_array)\n", "def test_column_transformer_with_different_input_types():\n    # Test the case when the input is a list of dictionaries\n    X_list_dict = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]\n    ct = ColumnTransformer([('dict', DictVectorizer(), ['a', 'b'])])\n    X_trans = ct.fit_transform(X_list_dict)\n    assert_true(sparse.issparse(X_trans))\n    assert_equal(X_trans.shape, (3, 3))\n\n    # Test the case when the input is a list of arrays\n    X_list_array = [np.array([1, 2]), np.array([3, 4]), np.array([5, 6])]\n    ct = ColumnTransformer([('array', StandardScaler(), [0, 1])])\n    X_trans = ct.fit_transform(X_list_array)\n    assert_equal(X_trans.shape, (3, 2))\n\n    # Test the case when the input is a DataFrame with mixed types\n    pd = pytest.importorskip('pandas')\n    X_mixed_df = pd.DataFrame({'a': [1, 2, 3], 'b': ['foo', 'bar', 'baz'], 'c': [4.0, 5.0, 6.0]})\n    ct = ColumnTransformer([('num', StandardScaler(), ['a', 'c']), ('cat', OneHotEncoder(), ['b'])])\n    X_trans = ct.fit_transform(X_mixed_df)\n    assert_true(sparse.issparse(X_trans))\n    assert_equal(X_trans.shape, (3, 5))\n"], "sample_1016": ["def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert mcode(A[0:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[:, 1:3]) == \"A(:, 2:3)\"\n    assert mcode(A[::2, ::2]) == \"A(1:2:end, 1:2:end)\"\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert mcode(A[:2, :2]) == \"A(1:2, 1:2)\"\n    assert mcode(A[:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[:, :2]) == \"A(:, 1:2)\"\n    assert mcode(A[::2, ::2]) == \"A(1:2:end, 1:2:end)\"\n", "def test_LambertW_special_cases():\n    assert octave_code(LambertW(0)) == '-inf'\n    assert octave_code(LambertW(-1)) == (\n        \"% Not supported in Octave:\\n\"\n        \"% LambertW\\n\"\n        \"LambertW(-1)\"\n    )\n    assert octave_code(LambertW(0, -1)) == (\n        \"% Not supported in Octave:\\n\"\n        \"% LambertW\\n\"\n        \"LambertW(0, -1)\"\n    )\n", "def test_Indexed():\n    from sympy import symbols, IndexedBase, Idx\n    i = Idx('i')\n    A = IndexedBase('A')\n    assert mcode(A[i]) == \"A(i)\"\n    assert mcode(A[i] + A[i+1]) == \"A(i) + A(i + 1)\"\n    assert mcode(A[i]*A[i+1]) == \"A(i).*A(i + 1)\"\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert mcode(A[:1, :]) == \"A(1:1, :)\"\n    assert mcode(A[:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[:2:2, :]) == \"A(1:2:2, :)\"\n    assert mcode(A[:-1, :]) == \"A(1:end-1, :)\"\n    assert mcode(A[1:, :]) == \"A(2:end, :)\"\n    assert mcode(A[1:2, :]) == \"A(2:2, :)\"\n    assert mcode(A[1:2:2, :]) == \"A(2:2:2, :)\"\n    assert mcode(A[1:, :1]) == \"A(2:end, 1:1)\"\n    assert mcode(A[1:, :2]) == \"A(2:end, 1:2)\"\n    assert mcode(A[1:, :2:2]) == \"A(2:end, 1:2:2)\"\n    assert mcode(A[1:, :-1]) == \"A(2:end, 1:end-1)\"\n"], "sample_131": ["    def test_serialize_db_to_string(self):\n        # Test that the serialize_db_to_string method correctly serializes the database\n        # into a JSON string\n        creation = connection.creation_class(connection)\n        serialized_data = creation.serialize_db_to_string()\n        self.assertIsInstance(serialized_data, str)\n        self.assertTrue(serialized_data.startswith('['))\n        self.assertTrue(serialized_data.endswith(']'))\n", "    def test_clone_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        suffix = 'clone'\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['NAME'] = 'existing_db'\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix, verbosity=0, keepdb=True)\n            creation._clone_test_db.assert_called_once_with(suffix, 0, True)\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(creation, '_clone_test_db'):\n            creation.clone_test_db(suffix='clone', verbosity=0, autoclobber=True, keepdb=True)\n        creation._clone_test_db.assert_called_once_with('clone', 0, True)\n", "    def test_clone_test_db_with_suffix(self):\n        # Test that clone_test_db() calls _clone_test_db() with the correct suffix.\n        test_connection = self.get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        suffix = 'clone'\n\n        with mock.patch.object(creation, '_clone_test_db') as mock_clone_test_db:\n            creation.clone_test_db(suffix, verbosity=0)\n\n        mock_clone_test_db.assert_called_once_with(suffix, verbosity=0, keepdb=False)\n", "    def test_createcachetable_called(self, mocked_createcachetable, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(creation, '_create_test_db'):\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        mocked_createcachetable.assert_called_once()\n"], "sample_256": ["def test_admin_password_change_form_validates_password(self):\n    user = User.objects.get(username='testclient')\n    data = {\n        'password1': 'testclient',\n        'password2': 'testclient',\n    }\n    form = AdminPasswordChangeForm(user, data)\n    self.assertFalse(form.is_valid())\n    self.assertEqual(len(form[\"password2\"].errors), 2)\n    self.assertIn('The password is too similar to the username.', form[\"password2\"].errors)\n    self.assertIn(\n        'This password is too short. It must contain at least 12 characters.',\n        form[\"password2\"].errors\n    )\n", "def test_password_help_text_in_admin_password_change_form(self):\n    user = User.objects.get(username='testclient')\n    form = AdminPasswordChangeForm(user)\n    self.assertEqual(\n        form.fields['password1'].help_text,\n        '<ul><li>Your password can\u2019t be too similar to your other personal information.</li></ul>'\n    )\n", "    def test_no_autofocus(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(user)\n        self.assertNotIn('autofocus', form.fields['password1'].widget.attrs)\n        self.assertNotIn('autofocus', form.fields['password2'].widget.attrs)\n", "    def test_unicode_email(self):\n        data = {\n            'username': 'testuser',\n            'password1': 'testpassword',\n            'password2': 'testpassword',\n            'email': 'test@\u00e9xample.com',\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        u = form.save()\n        self.assertEqual(u.email, 'test@\u00e9xample.com')\n", "    def test_validate_user_change_form(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'username': 'testclient',\n            'password': 'testclient',\n            'email': 'invalid_email',\n        }\n        form = UserChangeForm(data, instance=user)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form['email'].errors, [_('Enter a valid email address.')])\n"], "sample_331": ["    def test_parse_datetime_no_date(self):\n        invalid_inputs = (\n            '09:15:00',\n            '10:10',\n            '10:20:30.400',\n            '10:20:30,400',\n            '00:05:23+04:00',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n", "    def test_parse_datetime_with_spaces(self):\n        # Test parsing with multiple spaces\n        self.assertEqual(parse_datetime('2012-04-23  09:15:00'), datetime(2012, 4, 23, 9, 15))\n        self.assertEqual(parse_datetime('2012-4-9    4:8:16'), datetime(2012, 4, 9, 4, 8, 16))\n        self.assertEqual(parse_datetime('2012-04-23T09:15:00  Z'), datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(0)))\n", "    def test_invalid_inputs_for_datetime(self):\n        invalid_inputs = (\n            '2012-04-56T09:15:90',\n            '2012-04-23T10:20:30.400+25:00',\n            '2012-04-23T10:20:30.400+-02',\n            '2012-04-23T10:20:30.400--02',\n            '2012-04-23T10:20:30.400++02',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_datetime(source)\n", "    def test_parse_datetime_invalid_inputs(self):\n        invalid_inputs = [\n            '2012-04-56T09:15:90',\n            '2012-04-23T09:15:90Z',\n            '2012-04-23T09:15:90+01:00',\n            '2012-04-23T09:15:90.123456789',\n            '2012-04-23T09:15:90,123456789',\n            '2012-04-23T09:15:90+25:00',\n            '2012-04-23T09:15:90 +25:00',\n            '2012-04-23T09:15:90 -25:00',\n        ]\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_datetime(source)\n", "def test_parse_duration_invalid_inputs(self):\n    invalid_inputs = (\n        '15:',\n        '15:30:',\n        '15:30:60',\n        '15::30',\n        '15:30:30.1234567',\n        'P4Y1D',\n        'PT5',\n        'PT5HM',\n        'PT5H5',\n        'P4DTH',\n        'P4DT5M',\n        'P4DT5S',\n        '-P4DT5S',\n        'P4D-5S',\n        'P4D5MS',\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n"], "sample_217": ["def test_add_different_types(self):\n    widget1 = Media(css={'screen': ['a.css']}, js=['a'])\n    widget2 = Media(css={'all': ['b.css']}, js=['b'])\n    merged = widget1 + widget2\n    self.assertEqual(merged._css_lists, [{'screen': ['a.css']}, {'all': ['b.css']}])\n    self.assertEqual(merged._js_lists, [['a'], ['b']])\n", "def test_media_subtraction(self):\n    # Media objects can be subtracted. Any given media resource will be removed if it is present in both.\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2', '/path/to/js3')\n\n    class MyWidget2(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css2', '/path/to/css3')\n            }\n            js = ('/path/to/js1', '/path/to/js4')\n\n    w1 = MyWidget1()\n    w2 = MyWidget2()\n    self.assertEqual(\n        str(w1.media - w2.media),\n        \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_inheritance_multiple_parent(self):\n    ###############################################################\n    # Inheritance of media from multiple parents\n    ###############################################################\n\n    class MyWidget1(TextInput):\n        class Media:\n            css = {\n                'all': ('path/to/css1', '/path/to/css2')\n            }\n            js = ('/path/to/js1', 'http://media.other.com/path/to/js2')\n\n    class MyWidget2(TextInput):\n        class Media:\n            css = {\n                'all': ('/path/to/css3',)\n            }\n            js = ('/path/to/js3', 'https://secure.other.com/path/to/js4')\n\n    class MyWidget12(MyWidget1, MyWidget2):\n        pass\n\n    w12 = MyWidget12()\n    self.assertEqual(\n        str(w12.media),\n        \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_media_inheritance_multiwidget(self):\n    # If a MultiWidget extends another but defines media, it extends the parent\n    # MultiWidget's media by default.\n    class MyMultiWidget1(MultiWidget):\n            widgets = [MyWidget1, MyWidget2]\n            super().__init__(widgets, attrs)\n\n        class Media:\n            css = {'all': ('/path/to/css4',)}\n            js = ('/path/to/js5',)\n\n    class MyMultiWidget2(MyMultiWidget1):\n            widgets = [MyWidget3]\n            super().__init__(widgets, attrs)\n\n        class Media:\n            css = {'all': ('/path/to/css5',)}\n            js = ('/path/to/js6',)\n\n    w2 = MyMultiWidget2()\n    self.assertEqual(\n        str(w2.media),\n        \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_multi_widget_value_from_datadict(self):\n    class MyWidget1(TextInput):\n            return data.get(name, 'default1')\n\n    class MyWidget2(TextInput):\n            return data.get(name, 'default2')\n\n    class MyMultiWidget(MultiWidget):\n            widgets = [MyWidget1, MyWidget2]\n            super().__init__(widgets, attrs)\n\n    mymulti = MyMultiWidget()\n    data = {'0': 'value1', '1': 'value2'}\n    values = mymulti.value_from_datadict(data, {}, 'name')\n    self.assertEqual(values, ['value1', 'value2'])\n\n    data = {}\n    values = mymulti.value_from_datadict(data, {}, 'name')\n    self.assertEqual(values, ['default1', 'default2'])\n"], "sample_981": ["def test_commutes_with():\n    a = Permutation([1, 4, 3, 0, 2, 5])\n    b = Permutation([0, 1, 2, 3, 4, 5])\n    assert a.commutes_with(b)\n    b = Permutation([2, 3, 5, 4, 1, 0])\n    assert not a.commutes_with(b)\n", "def test_iter():\n    p = Permutation([0, 2, 1, 3])\n    assert list(p) == [0, 2, 1, 3]\n", "def test_commutes_with():\n    p = Permutation([1, 5, 2, 0, 3, 6, 4])\n    q = Permutation([[1, 2, 3, 5, 6], [0, 4]])\n    assert p.commutes_with(q) == False\n    assert q.commutes_with(p) == False\n    p2 = Permutation([0, 1, 2, 3])\n    q2 = Permutation([3, 2, 1, 0])\n    assert p2.commutes_with(q2) == False\n    assert q2.commutes_with(p2) == False\n", "def test_commutes_with():\n    p = Permutation([1, 5, 2, 0, 3, 6, 4])\n    q = Permutation([[1, 2, 3, 5, 6], [0, 4]])\n    assert p.commutes_with(q) is False\n    assert q.commutes_with(p) is False\n\n    a = Permutation(3)\n    b = Permutation(0, 6, 3)(1, 2)\n    assert a.commutes_with(b) is False\n    assert b.commutes_with(a) is False\n", "def test_permutation_from_sequence():\n    assert Permutation.from_sequence(['a', 'b', 'c']) == Permutation([0, 1, 2])\n    assert Permutation.from_sequence(['c', 'b', 'a']) == Permutation([2, 1, 0])\n    assert Permutation.from_sequence(['c', 'b', 'a'], key=lambda x: x) == Permutation([2, 1, 0])\n    assert Permutation.from_sequence(['c', 'b', 'a'], key=lambda x: -ord(x)) == Permutation([0, 1, 2])\n"], "sample_1003": ["def test_Composite_preprocess():\n    assert Composite.preprocess(False) is False\n    assert Composite.preprocess(True) is True\n\n    assert Composite.preprocess(0) is False\n    assert Composite.preprocess(1) is True\n\n    raises(OptionError, lambda: Composite.preprocess(x))\n", "def test_Composite_preprocess():\n    assert Composite.preprocess(False) is False\n    assert Composite.preprocess(True) is True\n\n    assert Composite.preprocess(0) is False\n    assert Composite.preprocess(1) is True\n\n    raises(OptionError, lambda: Composite.preprocess(x))\n", "def test_Gen_preprocess():\n    assert Gen.preprocess(x) == x\n    assert Gen.preprocess(10) == 10\n\n    raises(OptionError, lambda: Gen.preprocess(1.5))\n", "def test_Options_init():\n    opt = Options((x, y, z), {'domain': 'ZZ'})\n    assert opt['gens'] == (x, y, z)\n    assert opt['domain'] == ZZ\n    assert opt['expand'] == True\n    assert opt['auto'] == True\n\n    opt = Options((x, y, z), {'expand': False, 'auto': False})\n    assert opt['expand'] == False\n    assert opt['auto'] == False\n\n    opt = Options((x, y, z), {'domain': 'ZZ', 'expand': False, 'auto': False})\n    assert opt['expand'] == False\n    assert opt['auto'] == False\n    assert opt['domain'] == ZZ\n", "def test_Options_init():\n    # Test initializing Options with both *gens and keyword argument 'gens'\n    raises(OptionError, lambda: Options((x, y), {'gens': (z,)}))\n\n    # Test initializing Options with invalid option\n    raises(OptionError, lambda: Options((x, y), {'invalid_option': True}))\n\n    # Test initializing Options with option requiring another option\n    raises(OptionError, lambda: Options((x, y), {'modulus': 5}))\n\n    # Test initializing Options with options that exclude each other\n    raises(OptionError, lambda: Options((x, y), {'field': True, 'domain': ZZ}))\n"], "sample_997": ["def test_implicit_multiplication_with_parentheses():\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"(x+1)y\", transformations=transformations) == (x+1)*y\n", "def test_convert_equality_operators():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"1*2<=x\") == (2 <= x)\n    assert parse_expr(\"y >= x\") == (y >= x)\n    assert parse_expr(\"(2*y >= x) >= False\") == ((2*y >= x) >= False)\n", "def test_complex_numbers():\n    inputs = {\n        '1 + 2j': 1 + 2*I,\n        '1 - 2j': 1 - 2*I,\n        '2j * 3': 6*I,\n        '(1 + 2j) * (3 - 4j)': -5 + 10*I,\n        '(1 + 2j) / (3 - 4j)': Rational(11, 25) + Rational(22, 25)*I,\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text) == result\n", "def test_issue_11000():\n    inputs = {\n        '4**-3': '(4**-3)',\n        '4*-3': '(4)*(-3)',\n        '-4*3': '(-4)*3',\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, evaluate=False) == parse_expr(result, evaluate=False)\n", "def test_convert_equality_operators():\n    transformations = standard_transformations + (convert_equals_signs,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"x == y\", transformations=transformations) == Eq(x, y)\n    assert parse_expr(\"x != y\", transformations=transformations) == Ne(x, y)\n    assert parse_expr(\"x < y\", transformations=transformations) == Lt(x, y)\n    assert parse_expr(\"x > y\", transformations=transformations) == Gt(x, y)\n    assert parse_expr(\"x <= y\", transformations=transformations) == Le(x, y)\n    assert parse_expr(\"x >= y\", transformations=transformations) == Ge(x, y)\n"], "sample_558": ["def test_image_grid_each_bottom_label_mode_all():\n    imdata = np.arange(100).reshape((10, 10))\n\n    fig = plt.figure(1, (3, 3))\n    grid = ImageGrid(fig, (1, 1, 1), nrows_ncols=(2, 3), axes_pad=(0.5, 0.3),\n                     cbar_mode=\"each\", cbar_location=\"bottom\", cbar_size=\"15%\",\n                     label_mode=\"all\")\n    for ax, cax in zip(grid, grid.cbar_axes):\n        im = ax.imshow(imdata, interpolation='none')\n        cax.colorbar(im)\n", "def test_grid_with_axes_class_overriding_axis():\n    class CustomAxes(mpl_toolkits.axes_grid1.mpl_axes.Axes):\n            super().__init__(*args, **kwargs)\n            self.axis = \"custom\"\n\n    grid = Grid(plt.figure(), 111, (2, 2), axes_class=CustomAxes)\n    assert grid[0].axis == \"custom\"\n", "def test_grid_aspect(aspect):\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), aspect=aspect)\n    ax = grid[0]\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 1)\n    fig.canvas.draw()\n    assert ax.get_aspect() == \"auto\" if aspect else 2.0\n", "def test_grid_with_axes_class_overriding_axis():\n    class CustomAxes(mpl_toolkits.axes_grid1.axes_divider.Axes):\n            super().__init__(*args, **kwargs)\n            self.axis[\"bottom\"].set_visible(False)\n\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), axes_class=CustomAxes)\n    for ax in grid:\n        ax.plot([1, 2, 3], [1, 2, 3])\n", "def test_imagegrid_with_specified_aspect():\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, nrows_ncols=(1, 1), aspect=False)\n    assert not grid.get_aspect()\n"], "sample_1098": ["def test_appellf1_eval_nseries():\n    a, b1, b2, c, x, y, z = symbols('a b1 b2 c x y z')\n    assert appellf1(a, b1, b2, c, x, y)._eval_nseries(z, 7, None) == appellf1(a, b1, b2, c, x, y) + O(z**7)\n", "def test_appellf1_limit():\n    from sympy import limit\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    assert limit(appellf1(a, b1, b2, c, x, y), x, 0) == appellf1(a, b1, b2, c, 0, y)\n    assert limit(appellf1(a, b1, b2, c, x, y), y, 0) == appellf1(a, b1, b2, c, x, 0)\n", "def test_appellf1_eval():\n    from sympy import exp\n    a, b, c, x, y = symbols('a b c x y')\n    assert appellf1(a, b, b, c, x, y) == exp(x + y)\n\n    assert appellf1(a, b, b, c, S.Zero, S.Zero) is S.One\n\n    f = appellf1(a, b, b, c, S.Zero, S.Zero, evaluate=False)\n    assert f.func is appellf1\n    assert f.doit() is S.One\n", "def test_hyper_nseries():\n    a1, a2, b1, b2, b3 = symbols('a1:3, b1:4')\n    assert hyper((a1, a2), (b1, b2, b3), z)._eval_nseries(z, 3, None) == \\\n        a1*a2/(b1*b2*b3) * (1 + (a1 + a2 - b1 - b2 + b3)*z/(b1*b2*b3) +\n                            (a1*a2 - a1*b2 - a1*b3 - a2*b1 + a1*b1*b2 + a1*b1*b3 + a2*b1*b2 + a2*b1*b3)*z**2/\n                            (2*b1*b2*b3*(b1 + b2 - b3))) + O(z**3)\n", "def test_appellf1_expansion():\n    from sympy import sin, cos, pi, sqrt\n    a, b, c, x, y = symbols('a b c x y')\n    assert appellf1(a, b, b, a + 2*b, x, y) == (\n        sqrt(x)*sqrt(y)*sin(pi*a)/(2*sqrt(pi)*(a + b)) *\n        hyper((a/2, a/2 - b + 1/2), (a + 1/2), -x*y) +\n        cos(pi*a)/(2*sqrt(pi)*(a + b)) *\n        hyper((a/2, a/2 - b + 1/2), (a + 1/2), x*y)\n    )\n"], "sample_746": ["def test_brier_score_loss_with_pos_label():\n    # Test brier_score_loss function with pos_label argument\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    true_score = linalg.norm(y_true - y_pred) ** 2 / len(y_true)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=1), true_score)\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=0),\n                        linalg.norm(1 - y_true - y_pred) ** 2 / len(y_true))\n", "def test_brier_score_loss_with_sample_weights():\n    # Check brier_score_loss function with sample weights\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    weights = np.array([0.5, 0.5, 1.0, 1.0, 1.0, 2.0])\n    true_score = np.sum(weights * (y_true - y_pred) ** 2) / np.sum(weights)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, sample_weight=weights), true_score)\n", "def test_brier_score_loss_multiclass():\n    # Check brier_score_loss function for multiclass case\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.1, 0.2, 0.7], [0.3, 0.6, 0.1], [0.2, 0.5, 0.3],\n                       [0.4, 0.3, 0.3], [0.5, 0.4, 0.1], [0.6, 0.2, 0.2]])\n    true_score = np.mean(np.sum((label_binarize(y_true, classes=[0, 1, 2]) - y_pred) ** 2, axis=1))\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred[1:])\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred + 1.)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred - 1.)\n", "def test_brier_score_loss_multiclass():\n    # Check brier_score_loss function with multiclass input\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.2, 0.6, 0.2], [0.1, 0.8, 0.1], [0.1, 0.2, 0.7], [0.4, 0.3, 0.3], [0.2, 0.7, 0.1], [0.3, 0.3, 0.4]])\n    true_score = np.mean(np.sum((y_pred - np.eye(3)[y_true]) ** 2, axis=1))\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=0), np.mean(np.sum((y_pred - np.eye(3)[y_true]) ** 2, axis=1)[:, [0]]))\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=1), np.mean(np.sum((y_pred - np.eye(3)[y_true]) ** 2, axis=1)[:, [1]]))\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=2), np.mean(np.sum((y_pred - np.eye(3)[y_true]) ** 2, axis=1)[:, [2]]))\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred[1:])\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred + 1.)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred - 1.)\n    assert_", "def test_brier_score_loss_multiclass():\n    # Check brier_score_loss function with multiclass\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.1, 0.2, 0.7], [0.5, 0.3, 0.2], [0.3, 0.6, 0.1],\n                       [0.7, 0.1, 0.2], [0.2, 0.6, 0.2], [0.4, 0.3, 0.3]])\n    true_score = np.mean(np.sum((y_pred - np.eye(3)[y_true])**2, axis=1))\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=0),\n                        np.mean((y_pred[:, 0] - (y_true == 0))**2))\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=1),\n                        np.mean((y_pred[:, 1] - (y_true == 1))**2))\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=2),\n                        np.mean((y_pred[:, 2] - (y_true == 2))**2))\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred[1:])\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred + 1.)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred - 1.)\n"], "sample_244": ["def test_formset_with_file_field(self):\n    \"\"\"Formsets work with FileFields.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    FileFormSet = formset_factory(FileForm)\n    formset = FileFormSet()\n    self.assertTrue(formset.is_multipart())\n    self.assertIn('enctype=\"multipart/form-data\"', str(formset))\n", "def test_formset_with_empty_permitted(self):\n    \"\"\"FormSet should allow empty forms when empty_permitted is True.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=1)\n    formset = ChoiceFormSet(data={\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '1',\n        'form-0-choice': '',\n        'form-0-votes': '',\n    })\n    self.assertTrue(formset.is_valid())\n", "def test_formset_form_kwargs(self):\n    \"\"\"Formsets pass kwargs to their forms.\"\"\"\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n        'form-0-title': 'Test',\n        'form-0-pub_date': '1904-06-16',\n    }\n    formset = ArticleFormSet(data, form_kwargs={'custom_kwarg': 'value'})\n    self.assertEqual(formset.forms[0].custom_kwarg, 'value')\n", "    def test_empty_form_with_ordering(self):\n        \"\"\"Empty form can be submitted with ordering.\"\"\"\n        ChoiceFormSet = formset_factory(Choice, can_order=True)\n        initial = [{'choice': 'Calexico', 'votes': 100}]\n        formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n        data = {\n            'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '1',  # the number of forms with initial data\n            'choices-MIN_NUM_FORMS': '0',  # min number of forms\n            'choices-MAX_NUM_FORMS': '0',  # max number of forms\n            'choices-0-choice': 'Calexico',\n            'choices-0-votes': '100',\n            'choices-0-ORDER': '1',\n            'choices-1-choice': '',\n            'choices-1-votes': '',\n            'choices-1-ORDER': '',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(\n            [form.cleaned_data for form in formset.ordered_forms],\n            [\n                {'votes': 100, 'ORDER': 1, 'choice': 'Calexico'},\n            ],\n        )\n", "def test_formset_empty_permitted(self):\n    \"\"\"\n    empty_permitted is set to True for extra forms when they're not part of the\n    initial data.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=2)\n    formset = ChoiceFormSet()\n    self.assertFalse(formset.forms[0].empty_permitted)\n    self.assertFalse(formset.forms[1].empty_permitted)\n    formset = ChoiceFormSet(initial=[{'choice': 'Zero', 'votes': 0}])\n    self.assertFalse(formset.forms[0].empty_permitted)\n    self.assertTrue(formset.forms[1].empty_permitted)\n    self.assertTrue(formset.forms[2].empty_permitted)\n"], "sample_884": ["def test_property_deprecated():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n", "def test_property_deprecated():\n    # Test if deprecated decorator works for properties\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n", "def test_property_deprecation():\n    mock_class = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        _ = mock_class.n_features_\n", "def test_property_deprecated():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n", "def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        val = MockClass2().n_features_\n    assert val == 10\n"], "sample_264": ["def test_legacy_decode_invalid(self):\n    # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    # Encode/decode an invalid message using the pre-Django 3.1 hash.\n    encoder = MessageEncoder()\n    value = encoder.encode(messages)\n    encoded_messages = '%s$%s' % (storage._legacy_hash('invalid'), value)\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_legacy_decode_invalid(self):\n    # Test that the legacy decoding fails when the hash is invalid.\n    storage = self.storage_class(self.get_request())\n    messages = ['this', 'that']\n    # Encode a message using the pre-Django 3.1 hash.\n    encoder = MessageEncoder()\n    value = encoder.encode(messages)\n    encoded_messages = '%s$%s' % (storage._legacy_hash(value), value + 'tampered')\n    decoded_messages = storage._decode(encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_store_remove_oldest(self):\n    \"\"\"\n    When storing messages, if the encoded data is larger than max_cookie_size,\n    the oldest messages are removed first.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 4 messages into the cookie, but not 5.\n    # See also FallbackTest.test_session_fallback\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    messages = ['message {}'.format(i) for i in range(5)]\n    for msg in messages:\n        storage.add(constants.INFO, msg)\n    unstored_messages = storage.update(response)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, messages[0])\n", "def test_empty_messages(self):\n    \"\"\"\n    Test that encoding and decoding an empty list of messages works as expected.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    # Set initial (empty) data.\n    set_cookie_data(storage, [], encode_empty=True)\n    # The message contains what's expected.\n    self.assertEqual(list(storage), [])\n", "def test_empty_messages(self):\n    \"\"\"\n    Ensure that the cookie is set to expire if an empty list of messages is stored.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Store an empty list of messages\n    storage._store([], response)\n\n    # Check that the cookie is marked for deletion\n    self.assertEqual(response.cookies['messages'].value, '')\n    self.assertEqual(response.cookies['messages']['domain'], '.example.com')\n    self.assertEqual(response.cookies['messages']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n"], "sample_127": ["def test_large_batch_with_ignore_conflicts(self):\n    # Test inserting a large batch with ignore_conflicts=True.\n    objs = [TwoFields(f1=i, f2=i) for i in range(0, 1001)]\n    TwoFields.objects.bulk_create(objs, ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.count(), 1001)\n    # Inserting the same objects again should not create new records.\n    TwoFields.objects.bulk_create(objs, ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.count(), 1001)\n", "def test_ignore_conflicts_with_returning_fields(self):\n    data = [\n        TwoFields(f1=1, f2=1),\n        TwoFields(f1=2, f2=2),\n        TwoFields(f1=3, f2=3),\n    ]\n    TwoFields.objects.bulk_create(data)\n    self.assertEqual(TwoFields.objects.count(), 3)\n    # With ignore_conflicts=True and returning_fields, conflicts are ignored and new objects are created.\n    conflicting_objects = [\n        TwoFields(f1=2, f2=2),\n        TwoFields(f1=3, f2=3),\n        TwoFields(f1=4, f2=4),\n    ]\n    new_objects = TwoFields.objects.bulk_create(conflicting_objects, ignore_conflicts=True, returning_fields=['id'])\n    self.assertEqual(TwoFields.objects.count(), 4)\n    self.assertEqual(len(new_objects), 3)\n    self.assertIsNone(new_objects[0]['id'])\n    self.assertIsNone(new_objects[1]['id'])\n    self.assertIsNotNone(new_objects[2]['id'])\n", "def test_ignore_conflicts_conflict_with_returning_fields(self):\n    data = [\n        TwoFields(f1=1, f2=1),\n        TwoFields(f1=2, f2=2),\n        TwoFields(f1=3, f2=3),\n    ]\n    TwoFields.objects.bulk_create(data)\n    self.assertEqual(TwoFields.objects.count(), 3)\n    # With ignore_conflicts=True and returning fields, conflicts are ignored.\n    conflicting_objects = [\n        TwoFields(f1=2, f2=2),\n        TwoFields(f1=3, f2=3),\n    ]\n    message = 'Cannot specify returning fields when ignore_conflicts is True.'\n    with self.assertRaisesMessage(ValueError, message):\n        TwoFields.objects.bulk_create(conflicting_objects, ignore_conflicts=True, returning_fields=['id'])\n", "def test_bulk_create_with_returning_fields(self):\n    fields = ['name', 'iso_two_letter']\n    returned_fields = ['id', 'name']\n    data = [\n        {'name': 'Australia', 'iso_two_letter': 'AU'},\n        {'name': 'France', 'iso_two_letter': 'FR'},\n    ]\n    countries = Country.objects.bulk_create(\n        [Country(**fields) for fields in data],\n        fields=fields,\n        returning=returned_fields,\n    )\n    self.assertEqual(len(countries), 2)\n    self.assertEqual(countries[0].name, data[0]['name'])\n    self.assertIsNotNone(countries[0].id)\n    self.assertEqual(countries[1].name, data[1]['name'])\n    self.assertIsNotNone(countries[1].id)\n", "def test_bulk_insert_returns_values(self):\n    data = [\n        TwoFields(f1=i, f2=i + 1) for i in range(1000, 1010)\n    ]\n    with self.assertNumQueries(1):\n        created_objects = TwoFields.objects.bulk_create(data)\n    self.assertEqual(len(created_objects), 10)\n    self.assertEqual(created_objects[0].f1, 1000)\n    self.assertEqual(created_objects[0].f2, 1001)\n"], "sample_951": ["def test_signature_annotations_with_type_aliases():\n    from typing import List\n\n    type_aliases = {'MyList': 'List[int]'}\n\n        pass\n\n    sig = inspect.signature(func, type_aliases=type_aliases)\n    assert stringify_signature(sig) == '(x: List[int]) -> List[int]'\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n        pass\n\n    assert inspect.is_singledispatch_function(sdispatch_func) is True\n    assert inspect.is_singledispatch_function(not_singledispatch_func) is False\n", "def test_is_cython_function_or_method():\n    import cython\n\n    @cython.ccall\n        pass\n\n    class CythonClass:\n        @cython.ccall\n            pass\n\n    assert inspect.is_cython_function_or_method(cython_func) is True\n    assert inspect.is_cython_function_or_method(CythonClass().cython_method) is True\n    assert inspect.is_cython_function_or_method(test_is_cython_function_or_method) is False\n", "def test_isabstractmethod(app):\n    from target.abstractmethod import AbstractClass, ConcreteClass\n\n    assert inspect.isabstractmethod(AbstractClass.abstractmethod) is True\n    assert inspect.isabstractmethod(ConcreteClass.concretemethod) is False\n", "def test_is_singledispatch_function(app):\n    from target.singledispatch import func\n    from target.methods import Base\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(Base.meth) is False\n"], "sample_838": ["def test_column_transformer_reordered_column_names_no_remainder():\n    \"\"\"Regression test for issue #14223: 'Named col indexing fails with\n       ColumnTransformer remainder on changing DataFrame column ordering'\n\n       Should allow for changed order when no remainder.\n       Should allow for added columns in `transform` input DataFrame\n       as long as all preceding columns match.\n    \"\"\"\n    pd = pytest.importorskip('pandas')\n\n    X_fit_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_fit_df = pd.DataFrame(X_fit_array, columns=['first', 'second'])\n\n    X_trans_array = np.array([[2, 4, 6], [0, 1, 2]]).T\n    X_trans_df = pd.DataFrame(X_trans_array, columns=['second', 'first'])\n\n    tf = ColumnTransformer([('bycol', Trans(), ['first', 'second'])])\n\n    tf.fit(X_fit_df)\n    tf.transform(X_trans_df)  # No error should be raised\n\n    # No error for added columns if ordering is identical\n    X_extended_df = X_trans_df.copy()\n    X_extended_df['third'] = [3, 6, 9]\n    tf.transform(X_extended_df)  # No error should be raised\n", "def test_column_transformer_named_columns_with_remainder():\n    pd = pytest.importorskip('pandas')\n\n    X_fit_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_fit_df = pd.DataFrame(X_fit_array, columns=['first', 'second'])\n\n    X_trans_array = np.array([[2, 4, 6], [0, 1, 2]]).T\n    X_trans_df = pd.DataFrame(X_trans_array, columns=['second', 'first'])\n\n    tf = ColumnTransformer([('bycol', Trans(), ['first'])], remainder='passthrough')\n\n    tf.fit(X_fit_df)\n    assert_array_equal(tf.transform(X_trans_df), np.array([[1, 2, 6], [2, 0, 4]]))\n\n    # Test with explicit column names in transformers and remainder\n    tf = ColumnTransformer([('bycol', Trans(), ['first'])], remainder=['second'])\n\n    tf.fit(X_fit_df)\n    assert_array_equal(tf.transform(X_trans_df), np.array([[2, 6], [1, 4]]))\n", "def test_column_transformer_dropped_columns():\n    # test case that ensures that the column transformer can handle\n    # dropped columns in the input data\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [4, 8, 12]]).T\n    X_res_first = np.array([[0, 2, 4]]).T\n\n    # drop the second column\n    X_array_dropped = np.delete(X_array, 1, axis=1)\n\n    ct = ColumnTransformer([('trans', Trans(), 0)],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array_dropped), X_res_first)\n    assert_array_equal(ct.fit(X_array_dropped).transform(X_array_dropped), X_res_first)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][0] != 'remainder'\n\n    # test with pandas dataframe\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_array, columns=['first', 'second', 'third'])\n    X_df_dropped = X_df.drop('second', axis=1)\n\n    ct = ColumnTransformer([('trans', Trans(), 'first')],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df_dropped), X_res_first)\n    assert_array_equal(ct.fit(X_df_dropped).transform(X_df_dropped), X_res_first)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][0] != 'remainder'\n", "def test_column_transformer_multiple_column_names():\n    pd = pytest.importorskip('pandas')\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_both = X_array\n\n    ct = ColumnTransformer([('trans', Trans(), ['first', 'second'])])\n    assert_array_equal(ct.fit_transform(X_df), X_res_both)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] != 'remainder'\n\n    ct = ColumnTransformer([('trans', Trans(), ['first', 'second'])], remainder='passthrough')\n    assert_array_equal(ct.fit_transform(X_df), X_res_both)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n\n    ct = ColumnTransformer([('trans', Trans(), ['first', 'second'])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df), X_res_both)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_remainder_transformer_with_columns(key):\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    X_df = pd.DataFrame(X_array, columns=['first', 'second', 'third'])\n    X_res_both = X_array.copy()\n\n    # second and third columns are doubled when remainder = DoubleTrans\n    X_res_both[:, 1:3] *= 2\n\n    ct = ColumnTransformer([('trans1', Trans(), key)],\n                           remainder=DoubleTrans())\n\n    assert_array_equal(ct.fit_transform(X_df), X_res_both)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n"], "sample_475": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = \"hello\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "def test_autocomplete_many_to_many(self):\n    class UserAdmin(ModelAdmin):\n        search_fields = (\"name\",)\n\n    class Admin(ModelAdmin):\n        autocomplete_fields = (\"users\",)\n\n    site = AdminSite()\n    site.register(User, UserAdmin)\n    self.assertIsValid(Admin, ValidationTestModel, admin_site=site)\n", "def test_exclude_e015(self):\n    class TestModelAdmin(ModelAdmin):\n        exclude = (\"name\", \"name\")\n\n    self.assertIsInvalid(\n        TestModelAdmin,\n        ValidationTestModel,\n        \"The value of 'exclude' contains duplicate field(s).\",\n        \"admin.E015\",\n    )\n", "    def test_valid_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = (\"get_readonly_field\",)\n\n                return \"name\"\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n"], "sample_404": ["def test_compile_filter_invalid_filter(self):\n    \"\"\"\n    FilterExpression should raise an exception for an invalid filter.\n    \"\"\"\n    engine = self._engine()\n    msg = \"Invalid filter: 'does_not_exist'\"\n\n    with self.assertRaisesMessage(TemplateSyntaxError, msg):\n        engine.from_string(\"{{ foo|does_not_exist }}\")\n", "def test_filter_expression_resolve(self):\n    \"\"\"\n    Test the resolve method of the FilterExpression class.\n    \"\"\"\n    engine = self._engine()\n    filter_expression = engine.compile_filter(\"test_var|default:'Default'\")\n    context = Context({\"test_var\": \"Hello\"})\n    self.assertEqual(filter_expression.resolve(context), \"Hello\")\n    context = Context({})\n    self.assertEqual(filter_expression.resolve(context), \"Default\")\n", "def test_template_origin(self):\n    \"\"\"\n    Ensure Template origin is set correctly.\n    \"\"\"\n    engine = self._engine()\n    template = engine.from_string(\"Test content\", name=\"test_template\")\n    self.assertEqual(template.name, \"test_template\")\n    self.assertEqual(template.origin.name, \"test_template\")\n", "def test_compile_filter_error_in_var(self):\n    \"\"\"\n    Ensure FilterExpression errors are handled correctly for variable nodes.\n    \"\"\"\n    engine = self._engine()\n    msg = \"Could not parse the remainder: '@bar' from 'foo@bar'\"\n\n    with self.assertRaisesMessage(TemplateSyntaxError, msg) as e:\n        engine.from_string(\"{{ foo@bar }}\")\n\n    if self.debug_engine:\n        debug = e.exception.template_debug\n        self.assertEqual((debug[\"start\"], debug[\"end\"]), (0, 13))\n        self.assertEqual((debug[\"during\"]), \"{{ foo@bar }}\")\n", "def test_variable_node_render(self):\n    \"\"\"\n    Test VariableNode's render method.\n    \"\"\"\n    engine = self._engine()\n    filter_expression = engine.compile_filter(\"'Hello, World!' | upper\")\n    node = VariableNode(filter_expression)\n    context = Context()\n    self.assertEqual(node.render(context), \"HELLO, WORLD!\")\n"], "sample_149": ["    def test_empty_custom_permissions(self):\n        \"\"\"\n        Empty custom permissions don't raise any errors.\n        \"\"\"\n        class CustomUserWithDefaultPermissions(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            class Meta:\n                permissions = []\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "def test_username_field_defined(self):\n    \"\"\"USERNAME_FIELD should be defined.\"\"\"\n    class CustomUserNoUsernameField(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        date_of_birth = models.DateField()\n\n        REQUIRED_FIELDS = ['date_of_birth']\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"A custom user model must define a 'USERNAME_FIELD'.\",\n            obj=CustomUserNoUsernameField,\n            id='auth.E004',\n        ),\n    ])\n", "    def test_required_fields_empty(self):\n        \"\"\"REQUIRED_FIELDS should not be empty.\"\"\"\n        class CustomUserEmptyRequiredFields(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            date_of_birth = models.DateField()\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = []\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The field named as the 'USERNAME_FIELD' for a custom user model \"\n                \"must not be included in 'REQUIRED_FIELDS'.\",\n                obj=CustomUserEmptyRequiredFields,\n                id='auth.E002',\n            ),\n        ])\n", "    def test_is_anonymous_authenticated_properties(self):\n        \"\"\"\n        <User Model>.is_anonymous/is_authenticated should be properties.\n        \"\"\"\n        class CustomUserWithProperties(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            @property\n                return False\n\n            @property\n                return True\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "def test_is_anonymous_authenticated_attributes(self):\n    \"\"\"\n    <User Model>.is_anonymous/is_authenticated must be attributes or properties.\n    \"\"\"\n    class GoodUser(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        @property\n            return False\n\n        @property\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n"], "sample_830": ["def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n"], "sample_414": ["def test_ForeignKey_with_custom_widget(self):\n    from selenium.webdriver.common.by import By\n\n    self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n    )\n    main_window = self.selenium.current_window_handle\n\n    # No value has been selected yet\n    self.assertEqual(\n        self.selenium.find_element(By.ID, \"id_custom_band\").get_attribute(\"value\"), \"\"\n    )\n\n    # Open the popup window and click on a band\n    self.selenium.find_element(By.ID, \"lookup_id_custom_band\").click()\n    self.wait_for_and_switch_to_popup()\n    link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n    self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n    link.click()\n\n    # The field now contains the selected band's id\n    self.selenium.switch_to.window(main_window)\n    self.wait_for_value(\"#id_custom_band\", \"42\")\n\n    # Reopen the popup window and click on another band\n    self.selenium.find_element(By.ID, \"lookup_id_custom_band\").click()\n    self.wait_for_and_switch_to_popup()\n    link = self.selenium.find_element(By.LINK_TEXT, \"Green Potatoes\")\n    self.assertIn(\"/band/98/\", link.get_attribute(\"href\"))\n    link.click()\n\n    # The field now contains the other selected band's id\n    self.selenium.switch_to.window(main_window)\n    self.wait_for_value(\"#id_custom_band\", \"98\")\n", "def test_proper_manager_for_value_lookup(self):\n    rel = Inventory._meta.get_field(\"parent\").remote_field\n    w = widgets.ForeignKeyRawIdWidget(rel, widget_admin_site)\n    hidden_parent = Inventory.objects.create(barcode=99, name=\"Hidden Parent\", hidden=True)\n    child_of_hidden = Inventory.objects.create(barcode=100, name=\"Child of Hidden\", parent=hidden_parent)\n    self.assertEqual(w.value_from_instance(child_of_hidden), \"99\")\n", "    def test_ForeignKey_delete(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_profile_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add User button to add new\n        self.selenium.find_element(By.ID, \"add_id_user\").click()\n        self.wait_for_and_switch_to_popup()\n        password_field = self.selenium.find_element(By.ID, \"id_password\")\n        password_field.send_keys(\"password\")\n\n        username_field = self.selenium.find_element(By.ID, \"id_username\")\n        username_value = \"newuser\"\n        username_field.send_keys(username_value)\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new user\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_user option[value=newuser]\")\n\n        select = Select(self.selenium.find_element(By.ID, \"id_user\"))\n        select.select_by_value(\"newuser\")\n        # Click the Delete User button to delete it\n        self.selenium.find_element(By.ID, \"delete_id_user\").click()\n        alert = self.selenium.switch_to.alert\n        alert.accept()\n        # The user should be removed from the field\n        options = self.selenium.find_elements(By.CSS", "def test_ForeignKey_no_add_permission(self):\n    from selenium.webdriver.common.by import By\n    from selenium.webdriver.support.ui import Select\n\n    # Create a new user without add permission\n    no_add_user = User.objects.create_user(username=\"noadduser\", password=\"password\")\n    no_add_user.is_staff = True\n    no_add_user.save()\n\n    self.admin_login(username=\"noadduser\", password=\"password\", login_url=\"/\")\n    self.selenium.get(self.live_server_url + reverse(\"admin:admin_widgets_profile_add\"))\n\n    # Verify that the Add User button is not present\n    self.assertRaises(NoSuchElementException, self.selenium.find_element, By.ID, \"add_id_user\")\n\n    # Verify that the Change User button is not present\n    self.assertRaises(NoSuchElementException, self.selenium.find_element, By.ID, \"change_id_user\")\n\n    # Verify that the View User button is present\n    self.selenium.find_element(By.ID, \"view_id_user\")\n\n    # Verify that the user dropdown does not allow adding a new user\n    select = Select(self.selenium.find_element(By.ID, \"id_user\"))\n    options = [option.get_attribute(\"value\") for option in select.options]\n    self.assertNotIn(\"\", options)\n", "def test_related_field_widget_change_link(self):\n    from selenium.webdriver.common.by import By\n\n    self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n    user = User.objects.create_user(username=\"testuser\", password=\"password\")\n    profile = Profile.objects.create(user=user)\n    self.selenium.get(\n        self.live_server_url + reverse(\"admin:admin_widgets_profile_change\", args=(profile.id,))\n    )\n\n    # Click the Change User button to change it\n    self.selenium.find_element(By.ID, \"change_id_user\").click()\n    self.wait_for_and_switch_to_popup()\n\n    # Verify the Change User link is present and clickable\n    change_user_link = self.selenium.find_element(By.ID, \"view_id_user\")\n    self.assertTrue(change_user_link.is_displayed())\n    self.assertTrue(change_user_link.is_enabled())\n    change_user_link.click()\n\n    # Switch back to the main window and verify the user details are displayed correctly\n    self.switch_to_main_window()\n    self.wait_for_value(\"#id_username\", \"testuser\")\n"], "sample_321": ["def test_https_good_referer_behind_proxy_with_session(self):\n    \"\"\"\n    A POST HTTPS request is accepted when USE_X_FORWARDED_PORT=True and session is used for CSRF.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META.update({\n        'HTTP_HOST': '10.0.0.2',\n        'HTTP_REFERER': 'https://www.example.com/somepage',\n        'SERVER_PORT': '8080',\n        'HTTP_X_FORWARDED_HOST': 'www.example.com',\n        'HTTP_X_FORWARDED_PORT': '443',\n    })\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n", "def test_no_token_in_header_and_cookie(self):\n    \"\"\"\n    If no token is present in the header or cookie, the middleware rejects the incoming request.\n    \"\"\"\n    req = self._get_POST_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        resp = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(403, resp.status_code)\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): %s' % (REASON_NO_CSRF_COOKIE, REASON_CSRF_TOKEN_MISSING))\n", "def test_process_view_with_dont_enforce_csrf_checks(self):\n    \"\"\"\n    If _dont_enforce_csrf_checks is True, the middleware accepts the incoming\n    request even if a CSRF cookie is not present.\n    \"\"\"\n    req = self._get_POST_no_csrf_cookie_request()\n    req._dont_enforce_csrf_checks = True\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n", "def test_csrf_cookie_http_only(self):\n    \"\"\"\n    CSRF cookie http_only flag can be set using settings.CSRF_COOKIE_HTTPONLY.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n\n    with self.settings(CSRF_COOKIE_NAME='csrfcookie', CSRF_COOKIE_HTTPONLY=True):\n        # token_view calls get_token() indirectly\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        http_only = resp.cookies.get('csrfcookie').get('httponly')\n        self.assertEqual(http_only, '')\n", "def test_https_good_referer_matches_secure_session_cookie(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted if the session\n    cookie is secure.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_REFERER'] = 'https://www.example.com/'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n"], "sample_714": ["def test_brier_score_loss_sample_weight():\n    # Check brier_score_loss function with sample_weight\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n    true_score = np.sum(sample_weight * (y_true - y_pred) ** 2) / np.sum(sample_weight)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, sample_weight=sample_weight), true_score)\n", "def test_brier_score_loss_sample_weight():\n    # Check brier_score_loss function with sample_weight\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    sample_weight = np.array([1, 2, 3, 1, 2, 1])\n    true_score = np.sum((y_true - y_pred) ** 2 * sample_weight) / np.sum(sample_weight)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, sample_weight=sample_weight), true_score)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred, sample_weight=sample_weight[1:])\n", "def test_brier_score_loss_pos_label():\n    # Check brier_score_loss function with pos_label\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    true_score = linalg.norm(y_true - y_pred) ** 2 / len(y_true)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=1), true_score)\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=0),\n                        linalg.norm(1 - y_true - y_pred) ** 2 / len(y_true))\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred, pos_label=2)\n", "def test_brier_score_loss_with_pos_label():\n    # Check brier_score_loss function with pos_label\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    true_score = linalg.norm(y_true - y_pred) ** 2 / len(y_true)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=1), true_score)\n    assert_almost_equal(brier_score_loss(y_true, 1 - y_pred, pos_label=0), true_score)\n\n    # pos_label = 0\n    y_true = np.array([1, 0, 0, 1, 0, 0])\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=0), true_score)\n    assert_almost_equal(brier_score_loss(y_true, 1 - y_pred, pos_label=1), true_score)\n", "def test_log_loss_binary_input():\n    # case when y_true is a binary array object\n    y_true = np.array([0, 1, 1, 0])\n    y_pred = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])\n    loss = log_loss(y_true, y_pred)\n    assert_almost_equal(loss, 1.0383217, decimal=6)\n\n    # case when y_true is a binary list object\n    y_true = [0, 1, 1, 0]\n    y_pred = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])\n    loss = log_loss(y_true, y_pred)\n    assert_almost_equal(loss, 1.0383217, decimal=6)\n\n    # case when y_true is a binary list object with string labels\n    y_true = ['no', 'yes', 'yes', 'no']\n    y_pred = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])\n    loss = log_loss(y_true, y_pred)\n    assert_almost_equal(loss, 1.0383217, decimal=6)\n"], "sample_622": ["def test_encode_coordinates_with_space_in_name(self) -> None:\n    orig = Dataset(\n        coords={\"x with space\": [0, 1, 2], \"y\": (\"x with space\", [5, 6, 7])},\n        data_vars={\"a\": (\"x with space\", [1, 2, 3])},\n    )\n    with pytest.warns(SerializationWarning, match=\"coordinate 'x with space' has a space in its name\"):\n        enc, _ = conventions.encode_dataset_coordinates(orig)\n    assert \"coordinates\" not in enc[\"a\"].attrs\n", "def test_decode_cf_with_missing_time_units():\n    ds = Dataset({\"time\": (\"time\", [0, 1], {})})\n    with pytest.raises(ValueError, match=\"time variable 'time' does not have 'units' attribute\"):\n        decode_cf(ds)\n", "def test_decode_cf_with_unicode_string():\n    variable = Variable([\"time\"], np.array([\"a\", \"b\", \"c\"], dtype=\"U\"))\n    decoded = conventions.decode_cf_variable(\"time\", variable)\n    assert_identical(decoded, variable)\n", "def test_decode_cf_with_global_coordinates() -> None:\n    original = Dataset(\n        {\"a\": 1, \"b\": 2},\n        coords={\n            \"t\": np.array(\"2004-11-01T00:00:00\", dtype=np.datetime64),\n            \"global_coord\": (\"t\", [5, 10]),\n        },\n    )\n\n    encoded_vars, encoded_attrs = conventions.encode_dataset_coordinates(original)\n    decoded = conventions.decode_cf_variables(encoded_vars, encoded_attrs, decode_coords=True)\n\n    assert \"global_coord\" in decoded[0]\n    assert \"coordinates\" in encoded_attrs\n    assert \"global_coord\" in encoded_attrs[\"coordinates\"]\n", "def test_decode_cf_with_coordinates_attr(self) -> None:\n    original = Dataset(\n        {\"foo\": (\"t\", [1, 2], {\"coordinates\": \"x\"}), \"x\": (\"t\", [4, 5])},\n        attrs={\"coordinates\": \"x\"},\n    )\n    actual = conventions.decode_cf(original)\n    assert \"x\" in actual.coords\n    assert \"x\" in actual.variables\n"], "sample_1051": ["def test_float_precision():\n    f = Float(1.23456789, precision=3)\n    assert dotnode(f, repeat=False) == '\"Float(\\'1.23\\', precision=3)\" [\"color\"=\"black\", \"label\"=\"1.23\", \"shape\"=\"ellipse\"];'\n", "def test_float_precision():\n    f = Float('1.23456789', precision=10)\n    assert purestr(f) == \"Float('1.23456789', precision=10)\"\n    f = Float('1.23456789', precision=5)\n    assert purestr(f) == \"Float('1.2346', precision=5)\"\n    f = Float('1.23456789', precision=2)\n    assert purestr(f) == \"Float('1.2', precision=2)\"\n", "def test_atom_function():\n        return not x.args or isinstance(x, Integer)\n\n    text = dotprint(x + 2, atom=custom_atom)\n    assert '\"Add(Integer(2), Symbol(\\'x\\'))\"' in text\n    assert '\"Integer(2)\"' not in text\n    assert '\"Symbol(\\'x\\')\"' not in text\n", "def test_dotprint_styles():\n    styles = [(Basic, {'color': 'red', 'shape': 'rectangle'}),\n              (Expr,  {'color': 'green', 'style': 'filled'})]\n    text = dotprint(x+2, styles=styles)\n    assert '\"color\"=\"red\", \"shape\"=\"rectangle\"' in text\n    assert '\"color\"=\"green\", \"style\"=\"filled\"' in text\n", "def test_atom_function():\n    atom = lambda x: not x.args\n    text = dotprint(x + x**2, atom=atom, repeat=False)\n    assert '\"Add(Symbol(\\'x\\'), Pow(Symbol(\\'x\\'), Integer(2)))\" -> \"Symbol(\\'x\\')\";' in text\n    assert '\"Add(Symbol(\\'x\\'), Pow(Symbol(\\'x\\'), Integer(2)))\" -> \"Pow(Symbol(\\'x\\'), Integer(2))\";' in text\n    assert '\"Pow(Symbol(\\'x\\'), Integer(2))\" -> \"Symbol(\\'x\\')\";' in text\n    assert '\"Pow(Symbol(\\'x\\'), Integer(2))\" -> \"Integer(2)\";' not in text\n"], "sample_495": ["def test_paginator_invalid_object_list(self):\n    \"\"\"\n    Test Paginator with invalid object_list.\n    \"\"\"\n    with self.assertRaises(TypeError):\n        Paginator(\"invalid\", 5)\n", "def test_paginator_page_repr(self):\n    \"\"\"\n    Tests the __repr__ method of the Page class.\n    \"\"\"\n    paginator = Paginator([1, 2, 3, 4, 5, 6], 2)\n    page = paginator.page(1)\n    self.assertEqual(\"<Page 1 of 3>\", repr(page))\n", "def test_paginator_with_orphans(self):\n    \"\"\"\n    Tests the paginator attributes with orphans using varying inputs.\n    \"\"\"\n    nine = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    ten = nine + [10]\n    eleven = ten + [11]\n    tests = (\n        ((eleven, 5, 1, True), (11, 3, [1, 2, 3])),\n        ((eleven, 5, 2, True), (11, 2, [1, 2])),\n        ((eleven, 5, 4, True), (11, 1, [1])),\n    )\n    for params, output in tests:\n        self.check_paginator(params, output)\n", "def test_paginator_with_zero_per_page(self):\n    \"\"\"\n    Test Paginator with zero per_page.\n    \"\"\"\n    paginator = Paginator([1, 2, 3], 0)\n    with self.assertRaises(ZeroDivisionError):\n        paginator.num_pages\n\n    with self.assertRaises(ZeroDivisionError):\n        paginator.page_range\n\n    with self.assertRaises(ZeroDivisionError):\n        paginator.page(1)\n", "def test_paginator_with_unordered_list(self):\n    \"\"\"\n    Paginator with an unordered list.\n    \"\"\"\n    object_list = [3, 1, 4, 2]\n    paginator = Paginator(object_list, 2)\n    self.assertListEqual(list(paginator.page(1)), [3, 1])\n    self.assertListEqual(list(paginator.page(2)), [4, 2])\n"], "sample_589": ["def test_interpolate_na_max_gap_numeric(da_time):\n    expected = da_time.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da_time.interpolate_na(\"t\", max_gap=2, use_coordinate=False)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_numeric_specifier(da_time):\n    expected = da_time.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da_time.interpolate_na(\"t\", max_gap=2, use_coordinate=False)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_integer(da_time):\n    da_time[\"t\"] = np.arange(11)\n    expected = da_time.copy(data=[np.nan, 1, 2, np.nan, 3, 5, 6, 7, 8, 9, 10])\n    actual = da_time.interpolate_na(\"t\", max_gap=1)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_numeric(da_time, max_gap):\n    expected = da_time.copy(data=[np.nan, 1, 2, np.nan, 3, 5, 6, 7, 8, 9, 10])\n    actual = da_time.interpolate_na(\"t\", max_gap=max_gap, use_coordinate=False)\n    assert_equal(actual, expected)\n", "def test_interpolate_na_max_gap_numeric_specifier():\n    da = xr.DataArray([np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10], dims=[\"t\"], coords={\"t\": np.arange(11)})\n    expected = da.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da.interpolate_na(\"t\", max_gap=2, use_coordinate=False)\n    assert_equal(actual, expected)\n"], "sample_353": ["def test_fields_with_m2m_and_through_non_interactive(self):\n    new_io = StringIO()\n    org_id_1 = Organization.objects.create(name='Organization 1').pk\n    org_id_2 = Organization.objects.create(name='Organization 2').pk\n    with self.assertRaisesMessage(CommandError, \"Required field 'orgs' specifies a many-to-many relation through model, which is not supported.\"):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username='joe',\n            orgs=[org_id_1, org_id_2],\n            stdout=new_io,\n        )\n", "def test_fields_with_fk_non_interactive_missing_required_field(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    with self.assertRaisesMessage(CommandError, 'You must use --email with --noinput.'):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=email.pk,\n            group=group.pk,\n            stdout=new_io,\n        )\n", "def test_fields_with_fk_non_interactive(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username=email.pk,\n        email=email.email,\n        group=group.pk,\n        stdout=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertEqual(command_output, 'Superuser created successfully.')\n    u = CustomUserWithFK._default_manager.get(email=email)\n    self.assertEqual(u.username, email)\n    self.assertEqual(u.group, group)\n", "def test_fields_with_fk_non_existent(self):\n    new_io = StringIO()\n    non_existent_email = 'mymail2@gmail.com'\n    msg = 'email instance with email %r does not exist.' % non_existent_email\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=non_existent_email,\n            email=non_existent_email,\n            stdout=new_io,\n        )\n", "def test_fields_with_fk_non_existent_remote_pk(self):\n    new_io = StringIO()\n    non_existent_email_pk = 999\n    msg = 'email instance with pk %r does not exist.' % non_existent_email_pk\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=non_existent_email_pk,\n            email='mymail@gmail.com',\n            group=1,\n            stdout=new_io,\n        )\n"], "sample_95": ["def test_vary_on_headers_decorator(self):\n    \"\"\"\n    Ensures @vary_on_headers properly adds headers to the Vary header.\n    \"\"\"\n    @vary_on_headers('Accept-Language', 'User-Agent')\n        return HttpResponse()\n    r = a_view(HttpRequest())\n    self.assertEqual(\n        set(r['Vary'].split(', ')),\n        {'Accept-Language', 'User-Agent'},\n    )\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=60, public=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r['Cache-Control'], 'public, max-age=60')\n", "def test_get_cache_key(self):\n    \"\"\"\n    Test the get_cache_key function.\n    \"\"\"\n    request = HttpRequest()\n    request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-US'\n    request.GET['q'] = 'test'\n    request.build_absolute_uri = lambda: 'http://testserver/test/?q=test'\n\n    # Set up the cache with a headerlist\n    cache = caches['default']\n    cache_key = _generate_cache_header_key('test_prefix', request)\n    headerlist = ['HTTP_ACCEPT_LANGUAGE', 'QUERY_STRING']\n    cache.set(cache_key, headerlist)\n\n    # Call get_cache_key and verify the result\n    result = get_cache_key(request, key_prefix='test_prefix')\n    expected = _generate_cache_key(request, 'GET', headerlist, 'test_prefix')\n    self.assertEqual(result, expected)\n", "    def test_vary_on_headers(self):\n            return \"response\"\n        my_view_vary = vary_on_headers('Accept-Language', 'User-Agent')(my_view)\n        request = HttpRequest()\n        response = my_view_vary(request)\n        self.assertEqual(response['Vary'], 'Accept-Language, User-Agent')\n", "    def test_cache_control_decorator(self):\n        @cache_control(must_revalidate=True, proxy_revalidate=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'must-revalidate', 'proxy-revalidate'},\n        )\n"], "sample_113": ["def test_replace_named_groups(self):\n    tests = [\n        (r'^(?P<a>\\w+)/b/(\\w+)$', '^<a>/b/(\\w+)$'),\n        (r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$', '^<a>/b/<c>/$'),\n        (r'^(?P<a>\\w+)/b/(\\w+)', '^<a>/b/(\\w+)'),\n        (r'^(?P<a>\\w+)/b/(?P<c>\\w+)', '^<a>/b/<c>'),\n    ]\n    for pattern, output in tests:\n        with self.subTest(pattern=pattern):\n            self.assertEqual(utils.replace_named_groups(pattern), output)\n", "    def test_replace_named_groups(self):\n        tests = (\n            (r'^(?P<a>\\w+)/b/(\\w+)$', '^<a>/b/(\\w+)$'),\n            (r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$', '^<a>/b/<c>/$'),\n            (r'^(?P<a>\\w+)/b/(\\w+)', '^<a>/b/(\\w+)'),\n            (r'^(?P<a>\\w+)/b/(?P<c>\\w+)', '^<a>/b/<c>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(utils.replace_named_groups(pattern), output)\n", "    def test_simplify_regex_nested_groups(self):\n        tests = (\n            (r'^(?P<a>\\w+)/b/((x|y)\\w+)/c$', '/<a>/b/<var>/c'),\n            (r'^(?P<a>\\w+)/b/((x|y)\\(|\\))/c$', '/<a>/b/<var>/c'),\n            (r'^(?P<a>\\w+)/b/((x|y)(\\(|\\)))/c$', '/<a>/b/<var>/c'),\n            (r'^(?P<a>\\w+)/b/((x|y)(\\(|\\))?)/c$', '/<a>/b/<var>/c'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(utils.simplify_regex(pattern), output)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.person = Person.objects.create(first_name=\"Human\", last_name=\"User\")\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_944": ["def test_stringify_type_hints_Ellipsis():\n    assert stringify(...) == \"...\"\n", "def test_restify_type_hints_Union_with_NoneType():\n    assert restify(Optional[None]) == \":obj:`None`\"\n", "def test_restify_type_hints_forwardref_with_typevars():\n    class MyClass(Generic[T]):\n        pass\n\n    assert restify(ForwardRef(\"MyClass[int]\")) == \":class:`MyClass`\\\\ [:class:`int`]\"\n", "def test_restify_type_hints_TypedDict():\n    from typing import TypedDict\n\n    class Point(TypedDict):\n        x: int\n        y: int\n        label: str\n\n    assert restify(Point) == \":class:`tests.test_util_typing.Point`\"\n    assert restify(Point['x']) == \":class:`int`\"\n    assert restify(Point['y']) == \":class:`int`\"\n    assert restify(Point['label']) == \":class:`str`\"\n", "def test_restify_type_hints_literal():\n    from typing import Literal\n    assert restify(Literal[\"a\", 1]) == \":class:`typing.Literal`\\\\ [:obj:`'a'`, :obj:`1`]\"\n    assert restify(Literal[True, False]) == \":class:`typing.Literal`\\\\ [:obj:`True`, :obj:`False`]\"\n"], "sample_37": ["def test_all_world2pix_adaptive():\n    \"\"\"Test all_world2pix with adaptive=True\"\"\"\n\n    # Open test FITS file:\n    fname = get_pkg_data_filename('data/j94f05bgq_flt.fits')\n    ext = ('SCI', 1)\n    h = fits.open(fname)\n    w = wcs.WCS(h[ext].header, h)\n    h.close()\n    del h\n\n    crpix = w.wcs.crpix\n    ncoord = crpix.shape[0]\n\n    # Assume that CRPIX is at the center of the image and that the image has\n    # a power-of-2 number of pixels along each axis. Only use the central\n    # 1/64 for this testing purpose:\n    naxesi_l = list((7. / 16 * crpix).astype(int))\n    naxesi_u = list((9. / 16 * crpix).astype(int))\n\n    # Generate integer indices of pixels (image grid):\n    img_pix = np.dstack([i.flatten() for i in\n                         np.meshgrid(*map(range, naxesi_l, naxesi_u))])[0]\n\n    # Generage random data (in image coordinates):\n    with NumpyRNGContext(123456789):\n        rnd_pix = np.random.rand(random_npts, ncoord)\n\n    # Scale random data to cover the central part of the image\n    mwidth = 2 * (crpix * 1. / 8)\n    rnd_pix = crpix - 0.5 * mwidth + (mwidth - 1) * rnd_pix\n\n    # Reference pixel coordinates in image coordinate system (CS):\n    test_pix = np.append(img_pix, rnd_pix, axis=0)\n    # Reference pixel coordinates in sky CS using forward transformation:\n    all_world = w.all_pix2world(test_pix, origin)\n\n    try:\n        runtime_begin = datetime.now", "def test_dropaxis():\n    \"\"\"\n    Test dropaxis method.\n    \"\"\"\n    w = wcs.WCS(naxis=3)\n    w.wcs.crval = [1, 1, 1]\n    w.wcs.cdelt = [0.1, 0.1, 0.1]\n    w.wcs.crpix = [1, 1, 1]\n    w._naxis = [1000, 500, 200]\n\n    w2 = w.dropaxis(1)\n    assert w2.naxis == 2\n    assert w2._naxis == [1000, 200]\n\n    w2 = w.dropaxis(0)\n    assert w2.naxis == 2\n    assert w2._naxis == [500, 200]\n\n    w2 = w.dropaxis(2)\n    assert w2.naxis == 2\n    assert w2._naxis == [1000, 500]\n", "def test_sip_foc2pix_none():\n    \"\"\"\n    Test sip_foc2pix when no SIP coefficients are present.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.crval = [1, 1]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crpix = [1, 1]\n    w._naxis = [1000, 500]\n\n    x, y = w.sip_foc2pix(5.0, 6.0)\n    assert x == 5.0\n    assert y == 6.0\n", "def test_slice():\n    \"\"\"\n    Test slicing a WCS object.\n    \"\"\"\n    # Create a simple WCS object\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n    w.wcs.crval = [200.0, 30.0]\n    w.wcs.crpix = [100.0, 100.0]\n    w.wcs.cdelt = [0.1, 0.1]\n\n    # Slice the WCS object\n    w_sliced = w.slice(slice(50, 150), slice(50, 150))\n\n    # Check that the sliced WCS object has the expected properties\n    assert w_sliced.wcs.crpix == [50.0, 50.0]\n    assert w_sliced.wcs.crval == w.wcs.crval\n    assert w_sliced.wcs.cdelt == w.wcs.cdelt\n    assert w_sliced._naxis == [100, 100]\n\n    # Check that the sliced WCS object can be used to transform coordinates\n    ra, dec = w_sliced.wcs_pix2world(50, 50, 0)\n    assert_allclose(ra, 205.0)\n    assert_allclose(dec, 30.5)\n\n    x, y = w_sliced.wcs_world2pix(205.0, 30.5, 0)\n    assert_allclose(x, 50)\n    assert_allclose(y, 50)\n", "def test_pix2world_with_origin():\n    w = wcs.WCS(naxis=2)\n    w.wcs.crval = [1, 1]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crpix = [1, 1]\n    w._naxis = [1000, 500]\n\n    x, y = w.wcs_pix2world(2, 3, 0)\n    assert_allclose(x, 1.1)\n    assert_allclose(y, 1.2)\n\n    x, y = w.wcs_pix2world(2, 3, 1)\n    assert_allclose(x, 1.2)\n    assert_allclose(y, 1.3)\n"], "sample_878": ["def test_column_transformer_transformers_property():\n    # Test the transformers property of ColumnTransformer\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    assert ct.transformers == [(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])]\n\n    ct.fit(X_array)\n    assert ct.transformers == [(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])]\n", "def test_remainder_transformer_feature_names_out(transformers, remainder, expected_names):\n    \"\"\"Check feature_names_out for remainder transformer\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        transformers,\n        remainder=remainder,\n    )\n    ct.fit(df)\n\n    names = ct.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n    assert_array_equal(names, expected_names)\n", "def test_column_transformer_with_sparse_transformer_and_pandas_output():\n    \"\"\"Check that pandas output works when there is a sparse transformer.\n\n    Non-regression test for gh-25487\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame([[1.0, 2.2], [3.0, 1.0]], columns=[\"a\", \"b\"])\n    ct = ColumnTransformer(\n        [\n            (\"numerical\", StandardScaler(), [\"a\"]),\n            (\"sparse\", SparseMatrixTrans(), [\"b\"]),\n        ],\n        verbose_feature_names_out=True,\n    )\n    ct.set_output(transform=\"pandas\")\n    X_out = ct.fit_transform(X)\n    assert_array_equal(X_out.columns, [\"numerical__a\", \"sparse__x0\", \"sparse__x1\"])\n    assert sparse.issparse(X_out[\"sparse__x0\"])\n    assert sparse.issparse(X_out[\"sparse__x1\"])\n\n    ct.set_params(verbose_feature_names_out=False)\n    X_out = ct.fit_transform(X)\n    assert_array_equal(X_out.columns, [\"a\", \"x0\", \"x1\"])\n    assert sparse.issparse(X_out[\"x0\"])\n    assert sparse.issparse(X_out[\"x1\"])\n", "def test_column_transformer_with_empty_transformer_list():\n    X = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n\n    ct = ColumnTransformer([])\n    X_trans = ct.fit_transform(X)\n    assert_array_equal(X_trans, X)\n\n    ct = ColumnTransformer([], remainder=DoubleTrans())\n    X_trans = ct.fit_transform(X)\n    assert_array_equal(X_trans, 2 * X)\n", "def test_column_transformer_with_pandas_categorical_data():\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame({\n        \"col1\": pd.Categorical([\"a\", \"b\", \"a\", \"c\"]),\n        \"col2\": pd.Categorical([\"d\", \"d\", \"e\", \"f\"], categories=[\"d\", \"e\", \"f\"]),\n        \"col3\": [1, 2, 3, 4]\n    })\n    ohe = OneHotEncoder(sparse_output=False)\n    ct = ColumnTransformer([(\"ohe\", ohe, [\"col1\", \"col2\"]), (\"scaler\", StandardScaler(), [\"col3\"])])\n    X_trans = ct.fit_transform(X_df)\n    expected_output = np.array([\n        [1., 0., 0., 1., 0., 0., 1.],\n        [0., 1., 0., 1., 0., 0., 2.],\n        [1., 0., 0., 0., 1., 0., 3.],\n        [0., 0., 1., 0., 0., 1., 4.]\n    ])\n    assert_array_equal(X_trans, expected_output)\n"], "sample_143": ["    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('CamelCase'), 'camel case')\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseExample'), 'camel case example')\n        self.assertEqual(text.camel_case_to_spaces('camelCaseExample'), 'camel case example')\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseExampleTest'), 'camel case example test')\n        self.assertEqual(text.camel_case_to_spaces('camelCaseExampleTest'), 'camel case example test')\n        self.assertEqual(text.camel_case_to_spaces('lazystr(CamelCaseExampleTest)'), 'lazystr(camel case example test)')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseExample'), 'camel case example')\n    self.assertEqual(text.camel_case_to_spaces('AnotherCamelCase'), 'another camel case')\n    self.assertEqual(text.camel_case_to_spaces('NoCamelCase'), 'no camel case')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces('camelCase'), 'camel case')\n    self.assertEqual(text.camel_case_to_spaces('ALLCAPS'), 'allcaps')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('CamelCaseExample')), 'camel case example')\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('CamelCaseExample', 'camel case example'),\n        ('AnotherCamelCase', 'another camel case'),\n        ('CamelCaseWithNumber123', 'camel case with number 123'),\n        ('', ''),\n        ('NoCamelCase', 'no camel case'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('ThisIsATestString'), 'this is a test string')\n    self.assertEqual(text.camel_case_to_spaces('HTMLConverter'), 'h t m l converter')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('UnitTestExample')), 'unit test example')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseExample'), 'camel case example')\n    self.assertEqual(text.camel_case_to_spaces('IPAddress'), 'ip address')\n    self.assertEqual(text.camel_case_to_spaces('SomeHTTPHeaders'), 'some http headers')\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseExample '), 'camel case example')\n    self.assertEqual(text.camel_case_to_spaces(' CamelCaseExample'), 'camel case example')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('CamelCaseExample')), 'camel case example')\n"], "sample_502": ["def test_subplot_mosaic():\n    fig, ax_dict = plt.subplot_mosaic({'a': 'top', 'b': 'bottom'})\n    assert isinstance(ax_dict['a'], plt.Axes)\n    assert isinstance(ax_dict['b'], plt.Axes)\n    assert ax_dict['a'].get_position().bounds == (0, 0.5, 1, 0.5)\n    assert ax_dict['b'].get_position().bounds == (0, 0, 1, 0.5)\n", "def test_subplot_empty_sentinel():\n    fig, axs = plt.subplots(2, 2)\n    mosaic = [['A', '.'],\n              ['C', 'D']]\n    fig, ax_dict = plt.subplot_mosaic(mosaic, empty_sentinel='.')\n    assert 'A' in ax_dict\n    assert 'C' in ax_dict\n    assert 'D' in ax_dict\n    assert '.' not in ax_dict\n", "def test_figtext():\n    fig = plt.figure()\n    txt = plt.figtext(0.5, 0.5, \"Test\")\n    assert isinstance(txt, mpl.text.Text)\n    assert txt.get_position() == (0.5, 0.5)\n    assert txt.get_text() == \"Test\"\n", "def test_subplot_mosaic_empty_sentinel():\n    fig, ax_dict = plt.subplot_mosaic({'A': ['B', 'C']})\n    assert set(ax_dict.keys()) == {'A', 'B', 'C'}\n\n    fig, ax_dict = plt.subplot_mosaic({'A': ['B', 'C']}, empty_sentinel=None)\n    assert set(ax_dict.keys()) == {'A'}\n", "def test_subplot_grid_spec():\n    # create a grid of subplots\n    fig, axs = plt.subplots(2, 2, constrained_layout=True)\n    # check that the subplot grid is correct\n    assert axs.shape == (2, 2)\n    assert axs[0, 0] is plt.subplot(221)\n    assert axs[0, 1] is plt.subplot(222)\n    assert axs[1, 0] is plt.subplot(223)\n    assert axs[1, 1] is plt.subplot(224)\n    # check that subplot2grid returns the correct subplot\n    assert axs[0, 0] is plt.subplot2grid((2, 2), (0, 0))\n    assert axs[0, 1] is plt.subplot2grid((2, 2), (0, 1))\n    assert axs[1, 0] is plt.subplot2grid((2, 2), (1, 0))\n    assert axs[1, 1] is plt.subplot2grid((2, 2), (1, 1))\n"], "sample_158": ["def test_foreign_object_with_explicit_through_fields(self):\n    class Parent(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n\n        class Meta:\n            unique_together = (('a', 'b'),)\n\n    class Child(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        value = models.CharField(max_length=255)\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('a', 'b'),\n            to_fields=('a', 'b'),\n            related_name='children',\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(from_model=Child), [])\n", "def test_valid_intersection_foreign_object(self):\n    class Parent(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        c = models.PositiveIntegerField()\n\n        class Meta:\n            unique_together = (('a', 'b'),)\n\n    class Child(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        value = models.CharField(max_length=255)\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('a', 'b'),\n            to_fields=('a', 'b'),\n            related_name='children',\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(from_model=Child), [])\n", "def test_explicit_field_names_with_fewer_fields(self):\n    \"\"\"\n    If ``through_fields`` kwarg is given, it should specify exactly two fields.\n    \"\"\"\n    class Fan(models.Model):\n        pass\n\n    class Event(models.Model):\n        invitees = models.ManyToManyField(\n            Fan,\n            through='Invitation',\n            through_fields=('invalid_field_1',),\n        )\n\n    class Invitation(models.Model):\n        event = models.ForeignKey(Event, models.CASCADE)\n        invitee = models.ForeignKey(Fan, models.CASCADE)\n\n    field = Event._meta.get_field('invitees')\n    self.assertEqual(field.check(from_model=Event), [\n        Error(\n            \"Field specifies 'through_fields' but does not provide the names \"\n            \"of the two link fields that should be used for the relation \"\n            \"through model 'invalid_models_tests.Invitation'.\",\n            hint=\"Make sure you specify 'through_fields' as through_fields=('field1', 'field2')\",\n            obj=field,\n            id='fields.E337',\n        ),\n    ])\n", "def test_through_model_with_foreign_key_to_wrong_model(self):\n    class WrongModel(models.Model):\n        pass\n\n    class Person(models.Model):\n        pass\n\n    class Group(models.Model):\n        members = models.ManyToManyField('Person', through=\"InvalidThroughModel\")\n\n    class InvalidThroughModel(models.Model):\n        person = models.ForeignKey(Person, models.CASCADE)\n        wrong_foreign_key = models.ForeignKey(WrongModel, models.CASCADE)\n        # The last foreign key should point to Group model.\n\n    field = Group._meta.get_field('members')\n    self.assertEqual(field.check(from_model=Group), [\n        Error(\n            \"The model is used as an intermediate model by 'invalid_models_tests.Group.members', \"\n            \"but it does not have a foreign key to 'Group' or 'Person'.\",\n            obj=InvalidThroughModel,\n            id='fields.E336',\n        ),\n    ])\n", "def test_m2m_unique_constraint(self):\n    class Person(models.Model):\n        name = models.CharField(max_length=50)\n\n    class Group(models.Model):\n        name = models.CharField(max_length=50)\n        members = models.ManyToManyField(\n            Person,\n            through='GroupMembership',\n            through_fields=('group', 'person'),\n        )\n\n    class GroupMembership(models.Model):\n        group = models.ForeignKey(Group, models.CASCADE)\n        person = models.ForeignKey(Person, models.CASCADE)\n\n        class Meta:\n            unique_together = (('group', 'person'),)\n\n    field = Group._meta.get_field('members')\n    self.assertEqual(field.check(from_model=Group), [\n        Error(\n            'ManyToManyFields cannot be unique.',\n            obj=field,\n            id='fields.E330',\n        ),\n    ])\n"], "sample_1111": ["def test_zero_function():\n    x = Symbol('x')\n    lines = [\n        '      0 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(0, -1, 1)) == lines\n", "def test_constant_function():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(3, -1, 1)) == lines\n", "def test_large_numbers():\n    x = Symbol('x')\n    lines = [\n        '     1e15 |                                                     .',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '    1e14 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '    1e13 |_______________________________________________________',\n        '         1e13                         5e13                      1e14'\n    ]\n    assert list(textplot_str(x, 1e13, 1e14)) == lines\n", "def test_large_numbers():\n    x = Symbol('x')\n    lines = [\n        '   1e30 |                                                     . ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '   5e29 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '   0e30 |_______________________________________________________',\n        '      -1e30                    0e30                        1e30'\n    ]\n    assert lines == list(textplot_str(x, -1e30, 1e30))\n", "def test_large_range():\n    x = Symbol('x')\n    lines = [\n        '     1e3 |                                                     .',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '    500 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '   -1e3 |_______________________________________________________',\n        '         -1e3                       0                         1e3'\n    ]\n    assert list(textplot_str(x, -1e3, 1e3)) == lines\n"], "sample_40": ["def test_with_H0_default_cosmology():\n    \"\"\"Test the with_H0 equivalency with the default cosmology.\"\"\"\n    h100dist = 100 * u.Mpc/u.littleh\n    H0_default_cosmo = cosmology.default_cosmology.get().H0\n    assert_quantity_allclose(h100dist.to(u.Mpc, u.with_H0()),\n                             H0_default_cosmo.value*u.Mpc)\n", "def test_invalid_pixel_scale():\n    with pytest.raises(u.UnitsError):\n        u.pixel_scale(\"invalid_scale\")\n", "def test_with_H0_default():\n    # Test using the default cosmology\n    H0_default_cosmo = cosmology.default_cosmology.get().H0\n    h100dist = 100 * u.Mpc/u.littleh\n    assert_quantity_allclose(h100dist.to(u.Mpc, u.with_H0()), H0_default_cosmo.value*u.Mpc)\n", "def test_with_H0_default_cosmology():\n    # make sure using the default cosmology works without explicit argument\n    H0_default_cosmo = cosmology.default_cosmology.get().H0\n    h100dist = 100 * u.Mpc/u.littleh\n    assert_quantity_allclose(h100dist.to(u.Mpc, u.with_H0()),\n                             H0_default_cosmo.value*u.Mpc)\n", "def test_invalid_inputs_with_H0():\n    with pytest.raises(TypeError):\n        u.with_H0('invalid input')\n\n    with pytest.raises(u.UnitsError):\n        u.with_H0(10 * u.m)  # Should be a quantity with units of velocity\n"], "sample_580": ["def test_variable_type_mixed_types():\n\n    s = pd.Series([1, 2, \"3\", np.nan])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, 2, pd.Timestamp(3), np.nan])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, 2, pd.Timestamp(3), pd.NA])\n    assert variable_type(s) == \"categorical\"\n", "def test_variable_type_unknown_dtype():\n\n    s = pd.Series([1, 2, 3, 'a'])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, 2, 3, None])\n    assert variable_type(s) == \"numeric\"\n\n    s = pd.Series([], dtype='object')\n    assert variable_type(s) == \"unknown\"\n", "def test_variable_type_extension_dtype():\n\n    s = pd.Series([True, False, False], dtype=\"boolean\")\n    assert variable_type(s) == \"numeric\"\n    assert variable_type(s, boolean_type=\"categorical\") == \"categorical\"\n    assert variable_type(s, boolean_type=\"boolean\") == \"boolean\"\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([pd.Timestamp(1), pd.Timestamp(2), pd.NaT])\n    assert variable_type(s) == \"datetime\"\n    assert variable_type(s, boolean_type=\"boolean\") == \"datetime\"\n", "def test_variable_type_extension_dtypes():\n\n    s = pd.Series([True, False, True], dtype=\"boolean\")\n    assert variable_type(s) == \"boolean\"\n    assert variable_type(s, strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([True, False, True], dtype=\"bool\")\n    assert variable_type(s) == \"boolean\"\n    assert variable_type(s, strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([True, False, True], dtype=\"object\")\n    assert variable_type(s) == \"boolean\"\n    assert variable_type(s, strict_boolean=True) == \"numeric\"\n\n    s = pd.Series([True, False, True], dtype=object)\n    assert variable_type(s) == \"boolean\"\n    assert variable_type(s, strict_boolean=True) == \"numeric\"\n", "def test_variable_type_extension_dtype():\n\n    s = pd.Series([True, False, True], dtype=\"boolean\")\n    assert variable_type(s) == \"numeric\"\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([True, False, True], dtype=\"bool\")\n    assert variable_type(s) == \"numeric\"\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([\"2022-01-01\", \"2022-01-02\"], dtype=\"datetime64[ns]\")\n    assert variable_type(s) == \"datetime\"\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"datetime\"\n\n    s = pd.Series([1.5, 2.3, 3.8], dtype=\"float32\")\n    assert variable_type(s) == \"numeric\"\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"numeric\"\n"], "sample_639": ["def test_base_checker_hash() -> None:\n    \"\"\"Test hash of checkers based on their __hash__ method.\"\"\"\n    fake_checker_1 = OtherBasicChecker()\n    fake_checker_2 = LessBasicChecker()\n    fake_checker_3 = DifferentBasicChecker()\n\n    assert hash(fake_checker_1) == hash(fake_checker_2)\n    assert hash(fake_checker_1) != hash(fake_checker_3)\n    assert hash(fake_checker_2) != hash(fake_checker_3)\n", "def test_base_checker_doc_with_custom_module() -> None:\n    basic = OtherBasicChecker()\n    custom_module = \"custom_module\"\n    expected_beginning = f\"\"\"\\", "def test_base_checker_consistency() -> None:\n    \"\"\"Test the consistency of message IDs in a checker.\"\"\"\n    consistent_checker = OtherBasicChecker()\n    consistent_checker.check_consistency()\n\n    inconsistent_checker = DifferentBasicChecker()\n    try:\n        inconsistent_checker.check_consistency()\n    except InvalidMessageError as err:\n        assert \"Inconsistent checker part in message id\" in str(err)\n    else:\n        raise AssertionError(\"check_consistency should raise InvalidMessageError for inconsistent checker\")\n", "def test_base_checker_consistency() -> None:\n    \"\"\"Test consistency of msgid in the checker.\"\"\"\n    consistent_checker = OtherBasicChecker()\n    consistent_checker.check_consistency()  # Should not raise InvalidMessageError\n\n    inconsistent_checker = DifferentBasicChecker()\n    with pytest.raises(InvalidMessageError):\n        inconsistent_checker.check_consistency()\n", "def test_base_checker_consistency() -> None:\n    \"\"\"Test consistency of msgid in checkers.\"\"\"\n    # This checker should raise no exceptions\n    LessBasicChecker()\n\n    # This checker should raise InvalidMessageError\n    class InvalidBasicChecker(BaseChecker):\n        name = \"invalid\"\n        msgs = {\n            \"W0001\": (\n                \"Consistency error.\",\n                \"consistency-error\",\n                \"This message has an inconsistent checker id.\",\n            )\n        }\n\n    try:\n        InvalidBasicChecker()\n    except InvalidMessageError as exc:\n        assert \"Inconsistent checker part in message id\" in str(exc)\n    else:\n        assert False, \"Expected InvalidMessageError\"\n"], "sample_704": ["def test_node_add_report_section(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    item.add_report_section(\"call\", \"custom\", \"content\")\n    assert item._report_sections == [(\"call\", \"custom\", \"content\")]\n", "def test_node_reportinfo_returns_tuple(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    reportinfo = items[0].reportinfo()\n    assert isinstance(reportinfo, tuple)\n    assert len(reportinfo) == 3\n", "def test_node_report_section_content_empty(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    item.add_report_section(\"call\", \"stdout\", \"\")\n    assert item._report_sections == []\n", "def test_node_from_parent_fspath_path_consistency() -> None:\n    path = Path(\"path/to/file.py\")\n    fspath = legacy_path(path)\n    with pytest.raises(ValueError, match=\"Path\\\\('path/to/file.py'\\\\) != Path\\\\('path/to/different.py'\\\\)\"):\n        nodes.Node.from_parent(None, path=path, fspath=legacy_path(\"path/to/different.py\"))\n", "def test_node_repr_failure_returns_str_or_terminal_repr(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            assert False, \"Test failed\"\n    \"\"\"\n    )\n    excinfo = items[0].config._makepytrace(items[0], sys.exc_info())\n    result = items[0].repr_failure(excinfo)\n    assert isinstance(result, (str, nodes.TerminalRepr))\n"], "sample_752": ["def test_iforest_oob_score_error():\n    \"\"\"Test that _set_oob_score raises NotImplementedError\"\"\"\n    X = iris.data\n    clf = IsolationForest()\n    with pytest.raises(NotImplementedError):\n        clf._set_oob_score(X, None)\n", "def test_iforest_bootstrap_features():\n    # It tests non-regression for #12345 which failed when bootstrap_features=True\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n    clf = IsolationForest(bootstrap_features=True)\n    clf.fit(X_train, y_train)\n    clf.predict(X_test)\n", "def test_iforest_with_different_contamination():\n    \"\"\"Test Isolation Forest with different contamination values.\"\"\"\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                        iris.target,\n                                                        random_state=rng)\n    contamination_values = [0.01, 0.1, 0.25, 0.5]\n    for contamination in contamination_values:\n        clf = IsolationForest(contamination=contamination, random_state=rng)\n        clf.fit(X_train)\n        y_pred = clf.predict(X_test)\n        assert_equal(np.mean(y_pred == -1), contamination)\n", "def test_iforest_sparse_dense_equivalence():\n    \"\"\"Check that sparse and dense input give the same results.\"\"\"\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n    # Trained on dense format\n    dense_classifier = IsolationForest(n_estimators=10, random_state=1).fit(X_train)\n    dense_results = dense_classifier.predict(X_test)\n\n    # Trained on sparse format\n    sparse_classifier = IsolationForest(n_estimators=10, random_state=1).fit(csc_matrix(X_train))\n    sparse_results = sparse_classifier.predict(csr_matrix(X_test))\n\n    assert_array_equal(sparse_results, dense_results)\n", "def test_iforest_sample_weight():\n    \"\"\"Test Isolation Forest with sample weights\"\"\"\n\n    # Generate train/test data\n    rng = check_random_state(2)\n    X = 0.3 * rng.randn(120, 2)\n    X_train = np.r_[X + 2, X - 2]\n    X_train = X[:100]\n\n    # Generate some abnormal novel observations\n    X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n    X_test = np.r_[X[100:], X_outliers]\n    y_test = np.array([0] * 20 + [1] * 20)\n\n    # Set weights for training samples\n    sample_weight = np.ones(X_train.shape[0])\n    sample_weight[50:] = 2  # Increase the weight of outliers in training data\n\n    # fit the model with sample weights\n    clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train, sample_weight=sample_weight)\n\n    # predict scores (the lower, the more normal)\n    y_pred = - clf.decision_function(X_test)\n\n    # check that there is at most 6 errors (false positive or false negative)\n    assert_greater(roc_auc_score(y_test, y_pred), 0.98)\n"], "sample_1024": ["def test_Float_precision():\n    # Make sure Float inputs for keyword args work\n    assert Float('1.0', dps=Float(15.0))._prec == 53\n    assert Float('1.0', precision=Float(15.0))._prec == 15\n    assert type(Float('1.0', precision=Float(15.0))._prec) == int\n    assert sympify(srepr(Float('1.0', precision=15.0))) == Float('1.0', precision=15)\n", "def test_Float_complex_methods():\n    f = Float(2.0) + Float(3.0)*I\n    assert f.real == Float(2.0)\n    assert f.imag == Float(3.0)\n    assert f.conjugate() == Float(2.0) - Float(3.0)*I\n", "def test_Integer_invert():\n    assert Integer(2).invert(5) == 3\n    assert Integer(2).invert(Rational(5, 2)) == Rational(1, 2)\n    raises(TypeError, lambda: Integer(2).invert(2.5))\n    raises(ValueError, lambda: Integer(2).invert(Integer(5)))\n    raises(ValueError, lambda: Integer(2).invert(cos(1)**2 + sin(1)**2))\n", "def test_Float_numpy_conversion():\n    from sympy.external import import_module\n    np = import_module('numpy')\n    if not np:\n        skip('numpy not installed. Abort numpy tests.')\n\n        np_val_from_sympy = np.array(sympy_val, dtype=np_val.dtype)\n        assert np.allclose(np_val_from_sympy, np_val)\n\n    check_numpy_conversion(Float('1.0', precision=15), np.float32(1.0))\n    check_numpy_conversion(Float('1.0', precision=30), np.float64(1.0))\n    check_numpy_conversion(Float('1.0', precision=80), np.longdouble(1.0))\n", "def test_Float_dps():\n    assert Float('1.23456789', dps=5) == Float('1.2345')\n    assert Float('1.23456789', precision=5) == Float('1.2345')\n    assert Float('1.23456789')._prec == 10\n    assert Float('1.23456789', precision=15)._prec == 15\n    assert type(Float('1.23456789', precision=15)._prec) == int\n    assert sympify(srepr(Float('1.23456789', dps=5))) == Float('1.2345', precision=5)\n"], "sample_239": ["def test_formset_with_deletion_and_initial_data(self):\n    \"\"\"FormSets with deletion and initial data.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True)\n    initial = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    data = {\n        'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '2',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-0-DELETE': 'on',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-1-DELETE': '',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.deleted_forms],\n        [{'votes': 100, 'DELETE': True, 'choice': 'Calexico'}]\n    )\n    self.assertEqual(\n        [form.cleaned_data for form in formset.initial_forms],\n        [{'votes': 100, 'choice': 'Calexico'}, {'votes': 900, 'choice': 'Fergie'}]\n    )\n", "    def test_formset_rendering_with_custom_prefix(self):\n        data = {\n            'custom-TOTAL_FORMS': '1',\n            'custom-INITIAL_FORMS': '0',\n            'custom-MIN_NUM_FORMS': '0',\n            'custom-MAX_NUM_FORMS': '0',\n            'custom-0-choice': 'Calexico',\n            'custom-0-votes': '100',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='custom')\n        self.assertHTMLEqual(\n            str(formset),\n            \"\"\"<input type=\"hidden\" name=\"custom-TOTAL_FORMS\" value=\"1\">", "def test_formset_with_ordering_and_extra_initial(self):\n    \"\"\"FormSets with ordering, extra, and initial.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, extra=2)\n    initial = [\n        {'choice': 'Calexico', 'votes': 100},\n        {'choice': 'Fergie', 'votes': 900},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    self.assertHTMLEqual(\n        '\\n'.join(form.as_ul() for form in formset.forms),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_formset_total_error_count_with_deleted_forms(self):\n    \"\"\"A valid formset with deleted forms should have correct total errors.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '2',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '',  # Invalid data\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=1, can_delete=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.total_error_count(), 1)\n    data['choices-1-DELETE'] = 'on'  # Delete the invalid form\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.total_error_count(), 0)\n", "def test_formset_with_custom_prefix(self):\n    \"\"\"\n    Formset prefix can be customized for rendering and management form.\n    \"\"\"\n    data = {\n        'custom-TOTAL_FORMS': '1',\n        'custom-INITIAL_FORMS': '0',\n        'custom-MIN_NUM_FORMS': '0',\n        'custom-MAX_NUM_FORMS': '0',\n        'custom-0-choice': 'Calexico',\n        'custom-0-votes': '100',\n    }\n    CustomChoiceFormSet = formset_factory(Choice, prefix='custom')\n    formset = CustomChoiceFormSet(data)\n    self.assertEqual(formset.management_form.prefix, 'custom')\n    self.assertEqual(formset.forms[0].prefix, 'custom-0')\n    self.assertTrue(formset.is_valid())\n"], "sample_92": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username='test',\n            email='test@example.com',\n            password='test',\n            is_active=False,\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username=cls.remote_user, email='test@example.com', is_active=False\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username='test',\n            email='test@example.com',\n            password='test',\n            is_active=False\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            email='test@example.com', is_active=False,\n            username=cls.remote_user\n        )\n", "    def setUp(self):\n        self.request = HttpRequest()\n        self.request.session = {}\n"], "sample_224": ["def test_aggregation_subquery_annotation_multivalued_related_field(self):\n    \"\"\"\n    Subquery annotations must be included in the GROUP BY if they use\n    potentially multivalued related fields (contain the LOOKUP_SEP).\n    \"\"\"\n    if connection.vendor == 'mysql' and 'ONLY_FULL_GROUP_BY' in connection.sql_mode:\n        self.skipTest(\n            'GROUP BY optimization does not work properly when '\n            'ONLY_FULL_GROUP_BY mode is enabled on MySQL, see #31331.'\n        )\n    subquery_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        authors__name=OuterRef('name'),\n    ).values('pk')\n    publisher_qs = Publisher.objects.annotate(\n        subquery_id=Subquery(subquery_qs),\n    ).annotate(count=Count('book'))\n    self.assertEqual(publisher_qs.count(), Publisher.objects.count())\n", "def test_aggregation_subquery_annotation_aggregation(self):\n    \"\"\"\n    Subquery annotations containing aggregations are excluded from the GROUP\n    BY if they are not explicitly grouped against.\n    \"\"\"\n    average_book_rating_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).values('publisher').annotate(average_rating=Avg('rating')).values('average_rating')\n    publisher_qs = Publisher.objects.annotate(\n        average_book_rating=Subquery(average_book_rating_qs),\n    ).annotate(count=Count('book'))\n    with self.assertNumQueries(1) as ctx:\n        list(publisher_qs)\n    self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n    # The GROUP BY should not be by alias either.\n    self.assertEqual(ctx[0]['sql'].lower().count('average_book_rating'), 1)\n", "def test_aggregate_subquery_annotation_distinct(self):\n    author_count_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n    ).values('authors').distinct().annotate(count=Count('*')).values('count')\n    author_count_breakdown = Publisher.objects.values_list(\n        Subquery(author_count_qs, IntegerField()),\n    ).annotate(total=Count('*'))\n    self.assertEqual(dict(author_count_breakdown), {1: 2, 2: 1, 3: 1})\n", "def test_subquery_annotation_with_join(self):\n    subquery_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        price__gt=OuterRef('book__price')\n    ).values('publisher')\n    publisher_qs = Publisher.objects.annotate(\n        more_expensive_books=Subquery(subquery_qs),\n    ).annotate(count=Count('more_expensive_books'))\n    self.assertEqual(publisher_qs.count(), Publisher.objects.count())\n", "def test_group_by_without_annotation(self):\n    \"\"\"\n    Test that a query with a group by clause but no aggregation or annotation\n    still returns a result.\n    \"\"\"\n    authors = Author.objects.values('age').order_by('age').distinct()\n    self.assertTrue(authors.exists())\n"], "sample_1072": ["def test_complex_floor_ceiling():\n    assert floor(3 + 2*I) == 3 + 2*I\n    assert ceiling(3 + 2*I) == 3 + 2*I\n    assert floor(-3 + 2*I) == -3 + 2*I\n    assert ceiling(-3 + 2*I) == -3 + 2*I\n    assert floor(3 - 2*I) == 3 - 2*I\n    assert ceiling(3 - 2*I) == 3 - 2*I\n    assert floor(-3 - 2*I) == -3 - 2*I\n    assert ceiling(-3 - 2*I) == -3 - 2*I\n    assert floor(3 + I/2) == 3\n    assert ceiling(3 + I/2) == 4\n    assert floor(-3 + I/2) == -4\n    assert ceiling(-3 + I/2) == -3\n    assert floor(3 - I/2) == 3\n    assert ceiling(3 - I/2) == 4\n    assert floor(-3 - I/2) == -4\n    assert ceiling(-3 - I/2) == -3\n", "def test_complex_floor_ceiling():\n    z = Symbol('z', complex=True)\n    assert floor(z) == floor(re(z)) + I*floor(im(z))\n    assert ceiling(z) == ceiling(re(z)) + I*ceiling(im(z))\n\n    # Test with complex numbers\n    assert floor(2.5 + 3.5*I) == 2 + 3*I\n    assert ceiling(2.5 + 3.5*I) == 3 + 4*I\n", "def test_complex_frac():\n    assert frac(I*r).is_imaginary\n    assert frac(1 + I*r).is_imaginary\n    assert frac(0.5 + I*r).is_complex\n    assert frac(n + I*r).is_imaginary\n    assert frac(n + I*k).is_zero\n    assert frac(x + I*n).is_imaginary\n    assert frac(I*r).is_zero is None\n    assert frac(1 + I*r).is_zero is None\n    assert frac(0.5 + I*r).is_zero is None\n    assert frac(n + I*r).is_zero is None\n    assert frac(x + I*n).is_zero is None\n", "def test_issue_16872():\n    assert floor(Rational(23, 10) + E*I) == 2 + 3*I\n    assert ceiling(Rational(23, 10) + E*I) == 3 + 3*I\n", "def test_complex_floor_ceiling():\n    x, y = symbols('x, y')\n    # Test complex numbers with floor and ceiling functions\n    assert floor(x + y*I) == floor(x) + floor(y)*I\n    assert ceiling(x + y*I) == ceiling(x) + ceiling(y)*I\n    assert floor(x - y*I) == floor(x) - ceiling(y)*I\n    assert ceiling(x - y*I) == ceiling(x) - floor(y)*I\n    assert floor(x*I + y*I) == floor(x)*I + floor(y)*I\n    assert ceiling(x*I + y*I) == ceiling(x)*I + ceiling(y)*I\n    assert floor(x*I - y*I) == floor(x)*I - floor(y)*I\n    assert ceiling(x*I - y*I) == ceiling(x)*I - ceiling(y)*I\n"], "sample_609": ["def test_apply_dask_new_output_sizes_error() -> None:\n    import dask.array as da\n\n    array = da.ones((2, 2), chunks=(1, 1))\n    data_array = xr.DataArray(array, dims=(\"x\", \"y\"))\n\n            return np.stack([x, -x], axis=-1)\n\n        return apply_ufunc(\n            func,\n            obj,\n            output_core_dims=[[\"sign\"]],\n            dask=\"parallelized\",\n            output_dtypes=[obj.dtype],\n        )\n\n    with pytest.raises(ValueError, match=r\"dimension 'sign' in 'output_core_dims' needs corresponding\"):\n        stack_negative(data_array)\n", "def test_unify_chunks() -> None:\n    ds1 = xr.Dataset({\"a\": xr.DataArray(np.random.rand(10, 10), dims=(\"x\", \"y\"))})\n    ds2 = xr.Dataset({\"b\": xr.DataArray(np.random.rand(10, 10), dims=(\"x\", \"y\"))})\n    ds1_chunked = ds1.chunk({\"x\": 5, \"y\": 5})\n    ds2_chunked = ds2.chunk({\"x\": 2, \"y\": 2})\n    ds1_unified, ds2_unified = xr.unify_chunks(ds1_chunked, ds2_chunked)\n    assert ds1_unified.chunks == ds2_unified.chunks\n", "def test_apply_groupby_dot() -> None:\n    array = np.arange(5)\n    variable = xr.Variable(\"x\", array)\n    coords = {\"x\": -array, \"y\": (\"x\", [0, 0, 1, 1, 2])}\n    data_array = xr.DataArray(variable, coords, dims=\"x\")\n    dataset = xr.Dataset({\"z\": variable}, coords)\n\n    other_variable = xr.Variable(\"y\", [0, 10])\n    other_data_array = xr.DataArray(other_variable, dims=\"y\")\n    other_dataset = xr.Dataset({\"z\": other_variable})\n\n    expected_variable = xr.Variable(\"x\", [0, 4, 54, 84, np.nan])\n    expected_data_array = xr.DataArray(expected_variable, coords, dims=\"x\")\n    expected_dataset = xr.Dataset({\"z\": expected_variable}, coords)\n\n    actual = xr.dot(data_array.groupby(\"y\"), other_data_array)\n    assert_identical(expected_data_array, actual)\n    actual = xr.dot(data_array.groupby(\"y\"), other_dataset)\n    assert_identical(expected_dataset, actual)\n    actual = xr.dot(dataset.groupby(\"y\"), other_data_array)\n    assert_identical(expected_dataset, actual)\n    actual = xr.dot(dataset.groupby(\"y\"), other_dataset)\n    assert_identical(expected_dataset, actual)\n", "def test_polyval_errors() -> None:\n    xcoord = xr.DataArray(np.arange(10), dims=(\"x\",), name=\"x\")\n    coeffs = xr.DataArray([[2, 1, 1], [3, 2, 1]], dims=(\"d\", \"degree\"), coords={\"d\": [0, 1], \"degree\": [2, 1, 0]})\n\n    # Test for invalid coordinate type\n    with pytest.raises(TypeError, match=r\"Invalid coordinate type\"):\n        xr.polyval(\"invalid_coord\", coeffs)\n\n    # Test for invalid coefficient type\n    with pytest.raises(TypeError, match=r\"Invalid coefficient type\"):\n        xr.polyval(xcoord, \"invalid_coeffs\")\n\n    # Test for mismatched coordinate and coefficient dimensions\n    with pytest.raises(ValueError, match=r\"Coordinate and coefficient dimensions must match\"):\n        xr.polyval(xcoord[:5], coeffs)\n\n    # Test for mismatched degree dimension name\n    with pytest.raises(ValueError, match=r\"Degree dimension name must be 'degree'\"):\n        coeffs_invalid_degree_dim = coeffs.rename({\"degree\": \"invalid_degree\"})\n        xr.polyval(xcoord, coeffs_invalid_degree_dim)\n", "def test_apply_dataset_join() -> None:\n    ds0 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds1 = xr.Dataset({\"a\": (\"x\", [99, 3]), \"x\": [1, 2]})\n\n        return apply_ufunc(\n            operator.add,\n            a,\n            b,\n            join=join,\n            dataset_join=dataset_join,\n            dataset_fill_value=np.nan,\n        )\n\n    # test with dataset_join=\"outer\" and join=\"outer\"\n    actual = add(ds0, ds1, \"outer\", \"outer\")\n    expected = xr.Dataset({\"a\": (\"x\", [np.nan, 101, np.nan]), \"x\": [0, 1, 2]})\n    assert_identical(actual, expected)\n\n    # test with dataset_join=\"outer\" and join=\"inner\"\n    with pytest.raises(ValueError, match=r\"data variable names\"):\n        apply_ufunc(operator.add, ds0, xr.Dataset({\"b\": 1}), join=\"inner\", dataset_join=\"outer\")\n"], "sample_1202": ["def test_exponentiation_of_1():\n    x = Symbol('x')\n    assert 1**x == S.One\n    assert 1**-x == S.One\n", "def test_exponentiation_of_1():\n    x = Symbol('x')\n    assert 1**x == 1\n    assert 1**-x == 1\n    assert unchanged(Pow, 1, x)\n", "def test_exponentiation_of_0_with_integer_power():\n    assert 0**0 == 1\n    assert 0**1 == 0\n    assert 0**2 == 0\n    assert 0**3 == 0\n    assert 0**4 == 0\n    assert 0**5 == 0\n", "def test_exponentiation_of_1():\n    assert 1**S.Infinity == 1\n    assert 1**-S.Infinity == 1\n    assert 1**S.Zero == 1\n    assert 1**S.NaN is S.NaN\n", "def test_power_with_zero_base():\n    assert 0**0 == 1\n    assert 0**2 == 0\n    assert 0**(-2) == zoo\n    assert 0**S.Half == 0\n    assert 0**S.Zero == 1\n    assert 0**S.Infinity is S.Zero\n    assert 0**S.NegativeInfinity is zoo\n    x = Symbol('x')\n    assert 0**x == S.Zero\n    assert 0**(-x) == zoo\n    assert (-0)**2 == 0\n    assert (-0)**S.Half == I*S.Zero\n"], "sample_653": ["def test_log_level_not_changed_for_other_loggers(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logger = logging.getLogger('my_custom_logger')\n            assert logger.level == logging.NOTSET\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n", "def test_log_level_setting(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.DEBUG)\n            logger.debug('This is a debug message')\n            logger.info('This is an info message')\n            logger.warning('This is a warning message')\n            logger.error('This is an error message')\n            logger.critical('This is a critical message')\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=DEBUG\", \"--log-level=DEBUG\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_level_setting.py::test_log_level PASSED*\",\n            \"*DEBUG*This is a debug message*\",\n            \"*INFO*This is an info message*\",\n            \"*WARNING*This is a warning message*\",\n            \"*ERROR*This is an error message*\",\n            \"*CRITICAL*This is a critical message*\",\n        ]\n    )\n", "def test_log_cli_format_datefmt(testdir):\n    # Test custom log format and date format for live logging\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.formatter._fmt == '%(levelname)s - %(message)s'\n            assert plugin.log_cli_handler.formatter.datefmt == '%Y-%m-%d %H:%M:%S'\n            logging.getLogger('catchlog').info(\"This log message will have custom format\")\n            print('PASSED')\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format=%(levelname)s - %(message)s\n        log_cli_date_format=%Y-%m-%d %H:%M:%S\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_format_datefmt.py - INFO - This log message will have custom format\",\n            \"PASSED\",  # 'PASSED' on its own line because the log message prints a new line\n        ]\n    )\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_level_name_uppercase(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            assert logging.getLogger().level == logging.WARNING\n            logger = logging.getLogger(__name__)\n            logger.setLevel(\"INFO\")\n            assert logger.level == logging.INFO\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-s\", \"--log-level=iNfO\")\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n", "def test_log_level_propagation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info('info text from test_foo')\n            child_logger = logging.getLogger(__name__ + '.child')\n            child_logger.info('info text from child_logger')\n\n            logger.info('info text from test_bar')\n            child_logger = logging.getLogger(__name__ + '.child')\n            child_logger.info('info text from child_logger')\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n    stdout = result.stdout.str()\n    assert 'info text from test_foo' in stdout\n    assert 'info text from child_logger' in stdout\n    assert 'info text from test_bar' in stdout\n"], "sample_739": ["def test_label_binarize_multilabel_indicator():\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1, 2]\n    pos_label = 3\n    neg_label = 0\n    expected = pos_label * y_ind\n    y_sparse = [sparse_matrix(y_ind)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    for y in [y_ind] + y_sparse:\n        yield (check_binarized_results, y, classes, pos_label, neg_label,\n               expected)\n", "def test_label_binarize_multilabel_mismatch_classes():\n    y = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1]\n    assert_raises(ValueError, label_binarize, y, classes)\n", "def test_label_binarize_multiclass_thresholding():\n    y = [0, 1, 2]\n    classes = [0, 1, 2]\n    pos_label = 1\n    neg_label = 0\n    threshold = 0.5\n    expected = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 1]])\n\n    # check _inverse_binarize_thresholding\n    inversed = _inverse_binarize_thresholding(expected, output_type=\"multiclass\", classes=classes, threshold=threshold)\n    assert_array_equal(inversed, y)\n", "def test_label_binarize_multilabel_indicator():\n    y_ind = np.array([[0, 1, 1], [1, 1, 0], [0, 0, 1]])\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n    expected = pos_label * y_ind\n    y_sparse = [sparse_matrix(y_ind)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    for y in [y_ind] + y_sparse:\n        yield (check_binarized_results, y, classes, pos_label, neg_label,\n               expected)\n\n    assert_raises(ValueError, label_binarize, y, classes, neg_label=-1,\n                  pos_label=pos_label, sparse_output=True)\n", "def test_label_binarize_multilabel_uneven_dimensions():\n    y = np.array([[0, 1], [1, 1, 0], [0, 0]])\n    classes = [0, 1, 2]\n    msg = \"Expected 3 labels in the data, but found 2.\"\n    assert_raise_message(ValueError, msg, label_binarize, y, classes)\n"], "sample_579": ["def test_default_cmap(self):\n    g = mat.clustermap(self.df_norm)\n    assert g.ax_heatmap.collections[0].get_cmap().name == 'rocket'\n\n    g = mat.clustermap(self.df_norm, center=0)\n    assert g.ax_heatmap.collections[0].get_cmap().name == 'icefire'\n", "def test_tree_kws_lines(self):\n\n    linewidth = 2.5\n    g = mat.clustermap(self.df_norm, tree_kws=dict(linewidths=linewidth))\n    for ax in [g.ax_col_dendrogram, g.ax_row_dendrogram]:\n        tree, = ax.collections\n        assert tree.get_linewidth()[0] == linewidth\n", "def test_dim_ratios(self):\n    kws = self.default_kws.copy()\n    kws.update(dendrogram_ratio=0.2, colors_ratio=0.03, col_colors=self.col_colors, row_colors=self.row_colors)\n    cg = mat.ClusterGrid(self.df_norm, **kws)\n    assert cg.dim_ratios(self.col_colors, 0.2, 0.03) == [0.2, 0.03, 0.77]\n    assert cg.dim_ratios(self.row_colors, 0.2, 0.03) == [0.2, 0.03, 0.77]\n", "def test_dendrogram_plot_with_custom_tree_kws():\n    tree_kws = {'linewidths': 1.5, 'colors': 'red'}\n    d = mat.dendrogram(self.x_norm, **self.default_kws, tree_kws=tree_kws)\n\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    # 10 comes from _plot_dendrogram in scipy.cluster.hierarchy\n    xmax = len(d.reordered_ind) * 10\n\n    assert xlim[0] == 0\n    assert xlim[1] == xmax\n\n    assert len(ax.collections[0].get_paths()) == len(d.dependent_coord)\n    assert ax.collections[0].get_linewidths() == tree_kws['linewidths']\n    assert tuple(ax.collections[0].get_edgecolor()[0]) == tree_kws['colors']\n", "def test_col_colors_without_data():\n    kws = self.default_kws.copy()\n    kws['col_colors'] = self.col_colors[:-1]\n    with pytest.raises(ValueError):\n        mat.clustermap(self.df_norm, **kws)\n"], "sample_47": ["def test_cleanse_setting_handles_non_regexable_keys(self):\n    non_regexable_key = object()\n    self.assertEqual(cleanse_setting(non_regexable_key, 'value'), 'value')\n", "    def test_cleanse_setting_handles_unhashable_types(self):\n        initial = {'login': 'cooper', 'password': [1, 2, 3]}\n        expected = {'login': 'cooper', 'password': CLEANSED_SUBSTITUTE}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "def test_cleanse_setting_non_str_key(self):\n    initial = {42: 'not_secret'}\n    expected = {42: 'not_secret'}\n    self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "def test_multivalue_dict_key_error_sensitive_post_parameters(self):\n    \"\"\"\n    Sensitive POST parameters cannot be seen in the error reports for if\n    request.POST['nonexistent_key'] throws an error when sensitive_post_parameters\n    is set to filter those sensitive parameters.\n    \"\"\"\n    request = self.rf.post('/multivalue_dict_key_error/', self.breakfast_data)\n    request.sensitive_post_parameters = ['sausage-key', 'bacon-key']\n    self.verify_paranoid_response(multivalue_dict_key_error, check_for_vars=False)\n    self.verify_paranoid_email(multivalue_dict_key_error)\n", "def test_cleanse_setting_non_str_key(self):\n    initial = {42: 'should not be cleaned'}\n    expected = {42: 'should not be cleaned'}\n    self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n"], "sample_507": ["def test_plot_empty_data(self, plotter):\n    ax = plt.figure().subplots()\n    plotter(ax, [], [])\n    assert not ax.has_data()\n", "def test_plot_with_none(self, plotter):\n    ax = plt.figure().subplots()\n    data = ['a', 'b', None, 'd']\n    with pytest.raises(TypeError):\n        plotter(ax, data, [1, 2, 3, 4])\n", "def test_plot_nan(self, plotter):\n    ax = plt.figure().subplots()\n    with pytest.raises(TypeError):\n        plotter(ax, ['a', 'b', np.nan], [1, 2, 3])\n", "def test_plot_with_none():\n    ax = plt.figure().subplots()\n    data = ['a', 'b', None, 'd']\n    with pytest.raises(TypeError):\n        ax.plot(data, [1, 2, 3, 4])\n", "def test_plot_empty(self, plotter):\n    ax = plt.figure().subplots()\n    plotter(ax, [], [])\n    assert len(ax.lines) == 1  # Ensure that a Line2D object is created\n    line = ax.lines[0]\n    assert line.get_xdata().size == 0\n    assert line.get_ydata().size == 0\n"], "sample_62": ["def test_unregister(self):\n    self.site.register(Person)\n    self.assertTrue(self.site.is_registered(Person))\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n\n    msg = \"The model Person is not registered\"\n    with self.assertRaisesMessage(admin.sites.NotRegistered, msg):\n        self.site.unregister(Person)\n", "def test_unregister_not_registered_model(self):\n    \"\"\"\n    Exception is raised when trying to unregister a model that wasn't registered.\n    \"\"\"\n    msg = \"The model Person is not registered\"\n    with self.assertRaisesMessage(admin.sites.NotRegistered, msg):\n        self.site.unregister(Person)\n", "def test_unregister_registered_model(self):\n    \"\"\"\n    Check that unregistering a registered model works correctly.\n    \"\"\"\n    self.site.register(Person)\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n", "def test_unregistration(self):\n    self.site.register(Person)\n    self.site.unregister(Person)\n    msg = \"The model Person is not registered\"\n    with self.assertRaisesMessage(admin.sites.NotRegistered, msg):\n        self.site.unregister(Person)\n", "def test_unregister_not_registered_model(self):\n    \"Unregistering a model that isn't registered raises NotRegistered.\"\n    msg = \"The model Person is not registered\"\n    with self.assertRaisesMessage(admin.sites.NotRegistered, msg):\n        self.site.unregister(Person)\n"], "sample_79": ["def test_string(self):\n    self.check_values(('item', 'items'), ('items', 'items'), ('', 's'))\n", "def test_singular_and_plural_with_non_default_suffix(self):\n    self.check_values((0, 'results'), (1, 'result'), (2, 'results'))\n", "def test_non_string_values(self):\n    self.check_values((0, 'tests'), (1, 'test'), (2, 'tests'))\n", "def test_custom_suffixes(self):\n    self.check_values(('0', 'items'), ('1', 'item'), ('2', 'items'))\n", "def test_man_men_suffix(self):\n    self.check_values(('0', 'men'), ('1', 'man'), ('2', 'men'))\n"], "sample_301": ["def test_deleted_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        self.existing_file.unlink()\n    notify_mock.assert_called_once_with(self.existing_file)\n", "def test_watched_files_with_extra_files(self):\n    self.reloader.extra_files.add(self.nonexistent_file)\n    watched_files = list(self.reloader.watched_files())\n    self.assertIn(self.nonexistent_file, watched_files)\n", "def test_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "    def test_notify_file_changed_calls_signal(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.utils.autoreload.file_changed.send') as mocked_send:\n            reloader.notify_file_changed('/path/to/file')\n            mocked_send.assert_called_once_with(sender=reloader, file_path='/path/to/file')\n", "    def test_update_watches_with_missing_directory(self):\n        missing_directory = self.tempdir / 'missing'\n        self.reloader.watch_dir(missing_directory, '*.py')\n        with mock.patch.object(self.reloader, '_watch_root') as mock_watch_root:\n            mock_watch_root.side_effect = lambda path: (str(path), None)\n            with mock.patch('django.utils.autoreload.logger') as mock_logger:\n                self.reloader.update_watches()\n                mock_logger.warning.assert_called_once_with(\n                    'Unable to watch directory %s as neither it or its parent exist.',\n                    missing_directory\n                )\n"], "sample_193": ["def test_concrete_base_with_abstract_base(self):\n    A = self.create_model(\"A\")\n    B = self.create_model(\"B\", abstract=True)\n    C = self.create_model(\"C\", bases=(A, B))\n    self.assertRelated(A, [B, C])\n    self.assertRelated(B, [C])\n    self.assertRelated(C, [A])\n", "def test_constraints_equality(self):\n    state1 = ModelState(\n        'migrations',\n        'Tag',\n        [],\n        options={\n            'constraints': [models.CheckConstraint(check=models.Q(size__gt=1), name='size_gt_1')],\n        },\n    )\n    state2 = ModelState(\n        'migrations',\n        'Tag',\n        [],\n        options={\n            'constraints': [models.CheckConstraint(check=models.Q(size__gt=1), name='size_gt_1')],\n        },\n    )\n    self.assertEqual(state1, state2)\n", "def test_unique_together(self):\n    class TestModel(models.Model):\n        name = models.CharField(max_length=50)\n        age = models.IntegerField()\n\n        class Meta:\n            app_label = 'migrations'\n            unique_together = [['name', 'age']]\n\n    model_state = ModelState.from_model(TestModel)\n    self.assertEqual(model_state.options['unique_together'], {('name', 'age')})\n", "def test_create_model_with_multiple_fields(self):\n    \"\"\"\n    Tests making a ProjectState from an Apps with a model containing multiple fields\n    \"\"\"\n    new_apps = Apps(['migrations'])\n\n    class Author(models.Model):\n        name = models.CharField(max_length=255)\n        bio = models.TextField()\n        age = models.IntegerField(blank=True, null=True)\n        email = models.EmailField(unique=True)\n        is_active = models.BooleanField(default=True)\n\n        class Meta:\n            app_label = 'migrations'\n            apps = new_apps\n\n    author_state = ModelState.from_model(Author)\n    self.assertEqual(author_state.app_label, 'migrations')\n    self.assertEqual(author_state.name, 'Author')\n    self.assertEqual(list(author_state.fields), ['id', 'name', 'bio', 'age', 'email', 'is_active'])\n    self.assertEqual(author_state.fields['name'].max_length, 255)\n    self.assertIs(author_state.fields['bio'].null, False)\n    self.assertIs(author_state.fields['age'].null, True)\n    self.assertIs(author_state.fields['email'].unique, True)\n    self.assertEqual(author_state.fields['is_active'].default, True)\n    self.assertEqual(author_state.options, {'indexes': [], 'constraints': []})\n    self.assertEqual(author_state.bases, (models.Model,))\n    self.assertEqual(author_state.managers, [])\n", "def test_abstract_model_children_inherit_constraints(self):\n    class Abstract(models.Model):\n        size = models.IntegerField()\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n            constraints = [models.CheckConstraint(check=models.Q(size__gt=0), name='size_gt_0')]\n\n    class Child1(Abstract):\n        pass\n\n    class Child2(Abstract):\n        pass\n\n    child1_state = ModelState.from_model(Child1)\n    child2_state = ModelState.from_model(Child2)\n    constraint_names = [constraint.name for constraint in child1_state.options['constraints']]\n    self.assertEqual(constraint_names, ['size_gt_0'])\n    constraint_names = [constraint.name for constraint in child2_state.options['constraints']]\n    self.assertEqual(constraint_names, ['size_gt_0'])\n\n    # Modifying the state doesn't modify the constraint on the model.\n    child1_state.options['constraints'][0].name = 'bar'\n    self.assertEqual(Child1._meta.constraints[0].name, 'size_gt_0')\n"], "sample_238": ["def test_aggregation_random_ordering_with_filter(self):\n    \"\"\"Random() is not included in the GROUP BY when used for ordering with filter.\"\"\"\n    authors = Author.objects.annotate(contact_count=Count('book')).filter(contact_count__gt=0).order_by('?')\n    self.assertQuerysetEqual(authors, [\n        ('Adrian Holovaty', 1),\n        ('Jacob Kaplan-Moss', 1),\n        ('Brad Dayley', 1),\n        ('James Bennett', 1),\n        ('Jeffrey Forcier', 1),\n        ('Paul Bissex', 1),\n        ('Wesley J. Chun', 1),\n        ('Stuart Russell', 1),\n        ('Peter Norvig', 2),\n    ], lambda a: (a.name, a.contact_count), ordered=False)\n", "def test_annotate_with_aggregate(self):\n    authors = Author.objects.annotate(num_books=Count(\"book\")).annotate(max_pages=Max(\"book__pages\"))\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty', 1, 447),\n            ('Brad Dayley', 1, 528),\n            ('Jacob Kaplan-Moss', 1, 447),\n            ('James Bennett', 1, 300),\n            ('Jeffrey Forcier', 1, 350),\n            ('Paul Bissex', 1, 350),\n            ('Peter Norvig', 2, 1132),\n            ('Stuart Russell', 1, 1132),\n            ('Wesley J. Chun', 1, 350)\n        ],\n        lambda a: (a.name, a.num_books, a.max_pages)\n    )\n", "def test_aggregation_random_ordering_with_grouping(self):\n    \"\"\"Random() is included in the GROUP BY when used for grouping.\"\"\"\n    authors = Author.objects.values('age').annotate(count=Count('book')).order_by(Random())\n    with self.assertNumQueries(1) as ctx:\n        list(authors)\n    self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n", "def test_aggregation_case_when_annotation(self):\n    books_qs = Book.objects.annotate(\n        price_category=Case(\n            When(price__gt=30, then=Value('expensive')),\n            When(price__lt=10, then=Value('cheap')),\n            default=Value('average'),\n            output_field=CharField(),\n        ),\n    ).values('price_category').annotate(total_books=Count('id')).order_by('price_category')\n    self.assertEqual(list(books_qs), [\n        {'price_category': 'average', 'total_books': 4},\n        {'price_category': 'cheap', 'total_books': 0},\n        {'price_category': 'expensive', 'total_books': 2},\n    ])\n", "    def test_aggregation_with_filtered_subquery_annotation(self):\n        published_books_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n            rating__gt=3.0,\n        )\n        publisher_qs = Publisher.objects.annotate(\n            published_books_count=Subquery(published_books_qs.values('pk').annotate(count=Count('*')).values('count'),\n                                           output_field=IntegerField()),\n        ).filter(published_books_count__gt=0).order_by('published_books_count')\n        self.assertQuerysetEqual(publisher_qs, ['Apress', 'Prentice Hall', 'Morgan Kaufmann'],\n                                 lambda p: p.name)\n"], "sample_182": ["def test_union_with_different_annotated_values_list(self):\n    qs1 = Number.objects.filter(num=1).annotate(\n        count=Value(0, IntegerField()),\n        name=Value('one', output_field=CharField()),\n    ).values_list('num', 'count', 'name')\n    qs2 = Number.objects.filter(num=2).annotate(\n        count=Value(1, IntegerField()),\n        name=Value('two', output_field=CharField()),\n    ).values_list('num', 'count', 'name')\n    self.assertCountEqual(qs1.union(qs2), [(1, 0, 'one'), (2, 1, 'two')])\n", "def test_union_with_different_number_of_fields(self):\n    qs1 = Number.objects.values_list('num', flat=True)\n    qs2 = Number.objects.values_list('num', 'other_num')\n    msg = \"Merging 'ValuesQuerySet' classes must involve the same values in each case.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        list(qs1.union(qs2))\n", "def test_union_with_distinct_values(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=1),\n        ReservedName(name='rn2', order=1),\n    ])\n    qs1 = Number.objects.filter(num=1).values('num')\n    qs2 = ReservedName.objects.filter(order=1).values('order')\n    self.assertEqual(list(qs1.union(qs2)), [{'num': 1}, {'order': 1}])\n", "def test_union_with_aggregates(self):\n    from django.db.models import Count\n\n    qs1 = Number.objects.values('num').annotate(count=Count('pk'))\n    qs2 = Number.objects.values('num').annotate(count=Count('pk'))\n    union_qs = qs1.union(qs2)\n\n    # Check if the union of two aggregates is correct\n    for num in range(10):\n        with self.subTest(num=num):\n            self.assertEqual(union_qs.filter(num=num).count(), 1)\n            self.assertEqual(union_qs.filter(num=num).values_list('count', flat=True).get(), 2)\n", "def test_combined_qs_with_values(self):\n    qs1 = Number.objects.filter(num__gt=5).values('num')\n    qs2 = Number.objects.filter(num__lt=5).values('num')\n    union_qs = qs1.union(qs2)\n    self.assertCountEqual(union_qs.values_list('num', flat=True), [6, 7, 8, 9, 0, 1, 2, 3, 4])\n"], "sample_743": ["def test_n_neighbors_greater_than_n_samples():\n    # Test to check if n_neighbors is greater than n_samples\n    X = [[1, 1], [1, 1], [1, 1]]\n    n_samples = X.shape[0]\n    msg = \"Expected n_neighbors <= n_samples, but n_samples = 3, n_neighbors = 4\"\n\n    neighbors_ = neighbors.NearestNeighbors(n_neighbors=4)\n    assert_raises_regex(ValueError, msg, neighbors_.fit, X)\n    assert_raises_regex(ValueError, msg, neighbors_.kneighbors, X=X, n_neighbors=4)\n", "def test_valid_ball_tree_metric_for_auto_algorithm():\n    X = rng.rand(12, 12)\n\n    # check that there is a metric that is valid for ball_tree\n    # but not brute (so we actually test something)\n    assert_in(\"haversine\", VALID_METRICS['ball_tree'])\n    assert_false(\"haversine\" in VALID_METRICS['brute'])\n\n    # Metric which don't required any additional parameter\n    require_params = ['mahalanobis', 'wminkowski', 'seuclidean']\n    for metric in VALID_METRICS['ball_tree']:\n        if metric != 'precomputed' and metric not in require_params:\n            nn = neighbors.NearestNeighbors(n_neighbors=3, algorithm='auto',\n                                            metric=metric).fit(X)\n            nn.kneighbors(X)\n        elif metric == 'precomputed':\n            X_precomputed = rng.random_sample((10, 4))\n            Y_precomputed = rng.random_sample((3, 4))\n            DXX = metrics.pairwise_distances(X_precomputed, metric='euclidean')\n            DYX = metrics.pairwise_distances(Y_precomputed, X_precomputed,\n                                             metric='euclidean')\n            nb_p = neighbors.NearestNeighbors(n_neighbors=3)\n            nb_p.fit(DXX)\n            nb_p.kneighbors(DYX)\n\n    # Metric with parameter\n    list_metrics = [('mahalanobis', dict(VI=np.dot(X, X.T))),\n                    ('wminkowski', dict(w=rng.rand(12))),\n                    ('seuclidean', dict(V=rng.rand(12)))]\n    for metric, params in list_metrics:\n        nn = neighbors.NearestNeighbors(n_neighbors", "def test_sparse_metric_precomputed():\n    # Test when metric is 'precomputed' with sparse input\n    X_dense = np.array([[0, 1], [1, 0], [2, 0]])\n    X = csr_matrix(X_dense)\n    DXX = metrics.pairwise_distances(X_dense, metric='euclidean')\n\n    nbrs = neighbors.NearestNeighbors(n_neighbors=1, algorithm='brute', metric='precomputed')\n    nbrs.fit(DXX)\n    dist, ind = nbrs.kneighbors(DXX)\n\n    nbrs_dense = neighbors.NearestNeighbors(n_neighbors=1, algorithm='brute', metric='precomputed')\n    nbrs_dense.fit(X_dense)\n    dist_dense, ind_dense = nbrs_dense.kneighbors(X_dense)\n\n    assert_array_almost_equal(dist, dist_dense)\n    assert_array_equal(ind, ind_dense)\n", "def test_sparse_metric_non_callable():\n    # Test that non-callable metrics raise a ValueError\n    X = csr_matrix([[1, 1, 1, 1, 1], [1, 0, 1, 0, 1], [0, 0, 1, 0, 0]])\n    metric = 'invalid_metric'\n    assert_raises(ValueError, neighbors.NearestNeighbors, algorithm='ball_tree', metric=metric)\n", "def test_sparse_metric_callable_invalid_input():\n        assert_true(issparse(x) and issparse(y))\n        return x.dot(y.T).A.item()\n\n    X = np.random.rand(5, 5)  # Non-sparse population matrix\n    Y = csr_matrix([  # Sparse query matrix\n        [1, 1, 0, 1, 1],\n        [1, 0, 0, 0, 1]\n    ])\n\n    nn = neighbors.NearestNeighbors(algorithm='brute', n_neighbors=2,\n                                    metric=sparse_metric).fit(X)\n    with assert_raises(AssertionError):\n        nn.kneighbors(Y, return_distance=False)\n"], "sample_623": ["def test_split_and_join_chunks(self, shape, pref_chunks, req_chunks):\n    \"\"\"Test when the requested chunks both split and join the backend's preferred chunks.\"\"\"\n    initial = self.create_dataset(shape, pref_chunks)\n    with pytest.warns(UserWarning):\n        final = xr.open_dataset(\n            initial,\n            engine=PassThroughBackendEntrypoint,\n            chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n        )\n    self.check_dataset(initial, final, explicit_chunks(req_chunks, shape))\n", "def test_partial_pref_chunks(self, shape, pref_chunks):\n    \"\"\"Handle backends that provide only partial preferred chunks.\"\"\"\n    initial = self.create_dataset(shape, pref_chunks)\n    final = xr.open_dataset(\n        initial, engine=PassThroughBackendEntrypoint, chunks=None\n    )\n    self.check_dataset(initial, final, tuple((dim,) for dim in shape))\n", "def test_overwrite_encoded_chunks():\n    \"\"\"Test overwrite_encoded_chunks argument.\"\"\"\n    # Create a dataset with a variable that has encoded chunks\n    shape = (5, 6)\n    dims = tuple(f\"dim_{idx}\" for idx in range(len(shape)))\n    data = xr.DataArray(np.random.randn(*shape), dims=dims)\n    data.encoding[\"chunks\"] = (2, 3)\n    dataset = xr.Dataset({\"data\": data})\n\n    # Open the dataset with overwrite_encoded_chunks=False\n    final = xr.open_dataset(\n        dataset,\n        engine=PassThroughBackendEntrypoint,\n        overwrite_encoded_chunks=False,\n    )\n    assert final[\"data\"].chunks == ((2, 2, 1), (3, 3))\n\n    # Open the dataset with overwrite_encoded_chunks=True\n    final = xr.open_dataset(\n        dataset,\n        engine=PassThroughBackendEntrypoint,\n        overwrite_encoded_chunks=True,\n        chunks={\"dim_0\": 3, \"dim_1\": 4},\n    )\n    assert final[\"data\"].chunks == ((3, 2), (4, 2))\n", "    def test_chunks_parameter(self, shape, pref_chunks, req_chunks):\n        \"\"\"Test different ways of specifying chunks when opening a dataset.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial, engine=PassThroughBackendEntrypoint, chunks=req_chunks\n        )\n        expected_chunks = (\n            explicit_chunks(pref_chunks, shape)\n            if req_chunks is None\n            else explicit_chunks(req_chunks, shape)\n        )\n        self.check_dataset(initial, final, expected_chunks)\n", "def test_nonexistent_dims(self, shape, pref_chunks, req_chunks):\n    \"\"\"Test the behavior when requested chunks include dimensions that don't exist in the dataset.\"\"\"\n    initial = self.create_dataset(shape, pref_chunks)\n    with pytest.raises(ValueError):\n        xr.open_dataset(\n            initial,\n            engine=PassThroughBackendEntrypoint,\n            chunks=req_chunks,\n        )\n"], "sample_956": ["def test_missing_reference_any(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # any ref type\n    kwargs = {}\n    node, contnode = fake_node('', 'any', 'module1.func', 'module1.func', **kwargs)\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn.astext() == 'module1.func'\n", "def test_missing_reference_with_any_reftype(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # any reftype\n    node, contnode = fake_node('', 'any', 'module1.func', 'module1.func')\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn.astext() == 'module1.func'\n\n    node, contnode = fake_node('', 'any', 'a term', 'a term')\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn.astext() == 'a term'\n", "def test_normalize_intersphinx_mapping(app, warning):\n    app.config.intersphinx_mapping = {\n        'old': 'https://docs.python.org/',\n        'new': ('https://docs.python.org/py3k/', '/path/to/inventory'),\n        'invalid': ('invalid_url', '/path/to/inventory'),\n    }\n\n    normalize_intersphinx_mapping(app, app.config)\n    assert app.config.intersphinx_mapping == {\n        'old': (None, ('https://docs.python.org/', ('/path/to/inventory',))),\n        'new': ('new', ('https://docs.python.org/py3k/', ('/path/to/inventory',))),\n    }\n    assert warning.getvalue() == 'Failed to read intersphinx_mapping[invalid], ignored: ' \\\n                                 \"Failed to parse: ['invalid_url', '/path/to/inventory']\\n\"\n", "def test_fetch_inventory_invalid_version(_read_from_url, InventoryFile, app, status, warning):\n    intersphinx_setup(app)\n    _read_from_url().readline.return_value = b'# Sphinx inventory version 3'\n\n    with pytest.raises(ValueError) as exc_info:\n        fetch_inventory(app, 'http://hostname/', 'http://hostname/' + INVENTORY_FILENAME)\n    assert str(exc_info.value) == 'unknown or unsupported inventory version: 3'\n", "def test_missing_reference_cmdoption(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # no context data\n    kwargs = {}\n    node, contnode = fake_node('std', 'option', '-l', '-l', **kwargs)\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn is None\n\n    # std:program context helps to search objects\n    kwargs = {'std:program': 'ls'}\n    node, contnode = fake_node('std', 'option', '-l', '-l', **kwargs)\n    rn = missing_reference(app, app.env, node, contnode)\n    assert rn.astext() == '-l'\n"], "sample_9": ["def test_html_table_id():\n    \"\"\"\n    Test to make sure that the HTML writer writes the table with the correct id.\n    \"\"\"\n\n    col1 = [1, 2, 3]\n    col2 = [(1.0, 1.0), (2.0, 2.0), (3.0, 3.0)]\n    col3 = [('a', 'a', 'a'), ('b', 'b', 'b'), ('c', 'c', 'c')]\n    table = Table([col1, col2, col3], names=('C1', 'C2', 'C3'))\n    expected = \"\"\"\\", "def test_write_table_html_ids_and_classes():\n    \"\"\"\n    Test that passing ids and classes to the htmldict should set those attributes on the table element.\n    \"\"\"\n    buffer_output = StringIO()\n    t = Table([[1], [2]], names=('a', 'b'))\n    ascii.write(t, buffer_output, htmldict={'table_id': 'test-id', 'table_class': 'test-class'}, format='html')\n\n    expected = '<table id=\"test-id\" class=\"test-class\">'\n    assert expected in buffer_output.getvalue()\n", "def test_write_table_html_css():\n    \"\"\"\n    Test that passing custom CSS to the HTML writer results in it being included\n    in the output.\n    \"\"\"\n    buffer_output = StringIO()\n    t = Table([[1], [2]], names=('a', 'b'))\n    css = \"body { background-color: powderblue; }\"\n    ascii.write(t, buffer_output, format='html', css=css)\n\n    assert css in buffer_output.getvalue()\n", "def test_html_outputter():\n    \"\"\"\n    Test to ensure that the HTMLOutputter class correctly\n    processes the data in multidimensional columns.\n    \"\"\"\n\n    col1 = [1, 2, 3]\n    col2 = [(1.0, 1.0), (2.0, 2.0), (3.0, 3.0)]\n    col3 = [('a', 'a', 'a'), ('b', 'b', 'b'), ('c', 'c', 'c')]\n    table = Table([col1, col2, col3], names=('C1', 'C2', 'C3'))\n\n    cols = list(table.columns.values())\n    outputter = html.HTMLOutputter()\n\n    # Add colspan attribute to multidimensional columns\n    for i in (1, 2):\n        cols[i].colspan = cols[i].shape[1]\n\n    new_cols = outputter(cols, table.meta)\n\n    assert len(new_cols) == 5\n    assert new_cols[0].name == 'C1'\n    assert new_cols[1].name == 'C2'\n    assert new_cols[2].name == ''\n    assert new_cols[3].name == 'C3'\n    assert new_cols[4].name == ''\n    assert np.all(new_cols[1].str_vals == [(1.0, 1.0), (2.0, 2.0), (3.0, 3.0)])\n    assert np.all(new_cols[3].str_vals == [('a', 'a'), ('b', 'b'), ('c', 'c')])\n", "def test_read_html_special_chars():\n    \"\"\"\n    Test reading an HTML table with special characters\n    \"\"\"\n    table_in = ['<table>',\n                '<tr><td>&lt;&gt;&amp;&quot;&apos;</td></tr>',\n                '<tr><td><</td><td>></td><td>&</td><td>\"</td><td>\\'</td></tr>',\n                '</table>']\n    dat = Table.read(table_in, format='ascii.html')\n    assert np.all(dat['col1'] == ['<>&\\'\"', '<'])\n    assert np.all(dat['col2'] == ['>', '&'])\n    assert np.all(dat['col3'] == ['\"', '\\''])\n"], "sample_591": ["def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"a\"]))\n    assert expected.identical(ds2.merge(ds1, overwrite_vars=[\"a\"]))\n", "def test_merge_dataarray_named(self):\n    da = xr.DataArray([1, 2], dims=\"x\", name=\"foo\")\n    ds = xr.Dataset({\"bar\": 3})\n    expected = xr.Dataset({\"foo\": (\"x\", [1, 2]), \"bar\": 3})\n    assert expected.identical(xr.merge([da, ds]))\n", "def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"x\": (\"y\", [1, 2])})\n    ds2 = xr.Dataset({\"x\": (\"y\", [3, 4]), \"y\": [1, 2]})\n    expected = xr.Dataset({\"x\": (\"y\", [3, 4]), \"y\": [1, 2]})\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=\"x\"))\n", "def test_merge_with_different_coords(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"y\", [3, 4]), \"y\": [2, 3]})\n    expected = xr.Dataset(\n        {\"a\": ((\"x\", \"y\"), [[1, np.nan], [2, np.nan], [np.nan, 3], [np.nan, 4]])},\n        {\"x\": [0, 1], \"y\": [2, 3]},\n    )\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\"))\n", "def test_merge_overwrite_vars(self):\n    ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n    ds2 = xr.Dataset({\"x\": 2, \"z\": 3})\n    expected = xr.Dataset({\"x\": 2, \"y\": 1, \"z\": 3})\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=\"x\"))\n    assert expected.identical(ds1.merge(ds2, overwrite_vars=[\"x\"]))\n    assert expected.identical(xr.merge([ds1, ds2], overwrite_vars=\"x\"))\n    with pytest.raises(ValueError):\n        ds1.merge(ds2, overwrite_vars=[\"w\"])\n"], "sample_582": ["def test_run_cert_key(monkeypatch):\n    # key specified without cert\n    with pytest.raises(click.BadParameter):\n        run_command.make_context(\"run\", [\"--key\", __file__])\n\n    # key specified after cert\n    ctx = run_command.make_context(\"run\", [\"--cert\", __file__, \"--key\", __file__])\n    assert ctx.params[\"cert\"] == (__file__, __file__)\n\n    # key specified before cert\n    ctx = run_command.make_context(\"run\", [\"--key\", __file__, \"--cert\", __file__])\n    assert ctx.params[\"cert\"] == (__file__, __file__)\n", "def test_run_cert_key(monkeypatch):\n    # key specified first without cert\n    with pytest.raises(click.BadParameter):\n        run_command.make_context(\"run\", [\"--key\", __file__])\n", "def test_load_dotenv_no_file(monkeypatch):\n    # Test that load_dotenv returns False when no .env or .flaskenv files are present\n    monkeypatch.chdir(test_path / \"empty_folder\")\n    assert load_dotenv() is False\n", "def test_run_command_debug(runner, monkeypatch):\n    @click.command()\n    @click.pass_context\n        app = ctx.ensure_object(ScriptInfo).load_app()\n        click.echo(str(app.debug))\n\n    cli = FlaskGroup()\n    cli.add_command(testcmd, \"debug\")\n    obj = ScriptInfo(create_app=lambda: Flask(\"testapp\"))\n\n    # Test with --debug\n    result = runner.invoke(cli, [\"--debug\", \"debug\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"True\\n\"\n\n    # Test with --no-debug\n    result = runner.invoke(cli, [\"--no-debug\", \"debug\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"False\\n\"\n\n    # Test with default\n    monkeypatch.setitem(os.environ, \"FLASK_DEBUG\", \"1\")\n    result = runner.invoke(cli, [\"debug\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"True\\n\"\n\n    # Test with FLASK_DEBUG set to 0\n    monkeypatch.setitem(os.environ, \"FLASK_DEBUG\", \"0\")\n    result = runner.invoke(cli, [\"debug\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"False\\n\"\n", "def test_run_command_exclude_patterns(tmp_path, monkeypatch):\n    # Create a temporary Python file that changes when the command is run\n    script_path = tmp_path / \"script.py\"\n    script_path.write_text(\"print('initial')\")\n\n    # Set up a Flask app that serves the temporary Python file\n    app = Flask(__name__)\n    app.add_url_rule(\"/script.py\", \"script\", lambda: script_path.read_text())\n\n    # Set up the FlaskGroup to use the app\n    cli = FlaskGroup(create_app=lambda: app)\n\n    # Run the server with the temporary Python file excluded\n    runner = CliRunner()\n    result = runner.invoke(\n        cli, [\"run\", \"--port=5000\", \"--exclude-patterns\", \"*.py\"], catch_exceptions=False\n    )\n\n    # Check that the server started successfully\n    assert result.exit_code == 0\n\n    # Change the temporary Python file\n    script_path.write_text(\"print('changed')\")\n\n    # Check that the server did not reload because the file was excluded\n    response = requests.get(\"http://localhost:5000/script.py\")\n    assert response.text == \"initial\"\n\n    # Stop the server\n    monkeypatch.setattr(\"werkzeug.serving.BaseWSGIServer.shutdown\", lambda self: None)\n"], "sample_794": ["def test_ridge_regression_solver_auto():\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X_dense = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    X_sparse = sp.csr_matrix(X_dense)\n\n    ridge_auto_dense = Ridge(alpha=alpha, solver='auto')\n    ridge_auto_dense.fit(X_dense, y)\n    coef_auto_dense = ridge_auto_dense.coef_\n\n    ridge_auto_sparse = Ridge(alpha=alpha, solver='auto')\n    ridge_auto_sparse.fit(X_sparse, y)\n    coef_auto_sparse = ridge_auto_sparse.coef_\n\n    assert_allclose(coef_auto_dense, coef_auto_sparse, rtol=1e-5)\n", "def test_ridge_regression_with_sample_weight():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    coef = rng.randn(n_features)\n    y = np.dot(X, coef) + 0.01 * rng.randn(n_samples)\n    alpha = 1.0\n    sample_weight = rng.rand(n_samples)\n    results = dict()\n    for solver in ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']:\n        results[solver] = ridge_regression(X, y, alpha=alpha, solver=solver,\n                                           sample_weight=sample_weight)\n    for solver1, solver2 in combinations(results.keys(), 2):\n        assert_allclose(results[solver1], results[solver2], rtol=1e-5)\n", "def test_ridge_regression_random_state():\n    random_state = 42\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n    alpha = 1.0\n\n    coef1 = ridge_regression(X, y, alpha=alpha, random_state=random_state)\n    coef2 = ridge_regression(X, y, alpha=alpha, random_state=random_state)\n\n    assert_array_equal(coef1, coef2)\n\n    coef3 = ridge_regression(X, y, alpha=alpha, random_state=None)\n    coef4 = ridge_regression(X, y, alpha=alpha, random_state=None)\n\n    assert not np.array_equal(coef3, coef4)\n", "def test_ridge_regression_sparse_input(solver):\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X_dense = rng.randn(n_samples, n_features)\n    X_sparse = sp.csr_matrix(X_dense)\n    y = rng.randn(n_samples)\n\n    ridge_dense = Ridge(alpha=alpha, solver=solver)\n    ridge_dense.fit(X_dense, y)\n    coef_dense = ridge_dense.coef_\n\n    ridge_sparse = Ridge(alpha=alpha, solver=solver)\n    ridge_sparse.fit(X_sparse, y)\n    coef_sparse = ridge_sparse.coef_\n\n    assert_array_almost_equal(coef_dense, coef_sparse)\n", "def test_ridge_classifier_different_alphas():\n    # Test RidgeClassifier with different alphas\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n\n    # Test with a single alpha\n    clf = RidgeClassifier(alpha=1.0)\n    clf.fit(X, y)\n    assert_equal(len(clf.classes_), 2)\n\n    # Test with a list of alphas\n    alphas = [0.1, 1.0, 10.0]\n    clf = RidgeClassifier(alpha=alphas)\n    clf.fit(X, y)\n    assert_equal(len(clf.classes_), 2)\n\n    # Test with an invalid alpha\n    with assert_raises(ValueError):\n        clf = RidgeClassifier(alpha=-1.0)\n        clf.fit(X, y)\n\n    # Test with an invalid list of alphas\n    with assert_raises(ValueError):\n        clf = RidgeClassifier(alpha=[-1.0, 1.0])\n        clf.fit(X, y)\n"], "sample_829": ["def test_incremental_pca_sparse_transform():\n    # Test that transform works with sparse input.\n    X = iris.data\n    X_sparse = sparse.csr_matrix(X)\n    batch_size = X_sparse.shape[0] // 3\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n    ipca.fit(X_sparse)\n\n    X_transformed = ipca.transform(X_sparse)\n\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\n", "def test_incremental_pca_sparse_transform():\n    # Test that transform works with sparse input.\n    X = iris.data\n    batch_size = X.shape[0] // 3\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n    ipca.fit(X)\n    X_sparse = sparse.csr_matrix(X)\n    X_transformed = ipca.transform(X_sparse)\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\n", "def test_incremental_pca_with_small_batch_size():\n    # Test that IncrementalPCA works with a small batch size\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    n_features = 3\n    X = rng.randn(n_samples, n_features)\n\n    ipca = IncrementalPCA(n_components=2, batch_size=1).fit(X)\n    assert ipca.n_components_ == 2\n    assert ipca.components_.shape == (2, n_features)\n", "def test_incremental_pca_sparse_transform():\n    # Test that transform works correctly with sparse input.\n    X = iris.data\n    X_sparse = sparse.csr_matrix(X)\n    batch_size = X_sparse.shape[0] // 3\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n    ipca.fit(X_sparse)\n    X_transformed = ipca.transform(X_sparse)\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\n\n    # Check that transform works with sparse input in batches\n    X_transformed_batched = []\n    for batch in gen_batches(X_sparse.shape[0], batch_size, min_batch_size=ipca.n_components):\n        X_transformed_batched.append(ipca.transform(X_sparse[batch].toarray()))\n    X_transformed_batched = np.vstack(X_transformed_batched)\n    np.testing.assert_allclose(X_transformed, X_transformed_batched)\n", "def test_incremental_pca_sparse_transform():\n    # Test transforming data in batches when input is sparse.\n    X = iris.data\n    pca = PCA(n_components=2)\n    pca.fit_transform(X)\n    X_sparse = sparse.csr_matrix(X)\n    batch_size = X_sparse.shape[0] // 3\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n    ipca.fit(X_sparse)\n\n    X_transformed = ipca.transform(X_sparse)\n\n    assert X_transformed.shape == (X_sparse.shape[0], 2)\n    np.testing.assert_allclose(X_transformed, pca.transform(X), rtol=1e-3)\n"], "sample_514": ["def test_colorbar_add_lines():\n    fig, ax = plt.subplots()\n    CS = ax.contour(np.arange(100).reshape(10, 10), levels=[20, 40, 60], colors='k')\n    cb = fig.colorbar(CS)\n    cb.add_lines(CS)\n    assert len(cb.lines) == 1\n    assert isinstance(cb.lines[0], collections.LineCollection)\n", "def test_colorbar_ticklabels():\n    fig, ax = plt.subplots()\n    np.random.seed(seed=19680808)\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, location='right', extend='both')\n    labels = ['Label1', 'Label2', 'Label3', 'Label4', 'Label5']\n    cb.set_ticklabels(labels)\n    fig.draw_without_rendering()\n    assert [t.get_text() for t in cb.ax.get_yticklabels()] == labels\n", "def test_colorbar_title_position():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.random((10, 10)))\n    cbar = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.2)\n    cbar.ax.set_title('Colorbar title')\n    fig.draw_without_rendering()\n    # check that the title is above the colorbar axes\n    assert cbar.ax.title.get_position()[1] > cbar.ax.bbox.y1\n", "def test_colorbar_inverted_axis():\n    # test fix for #5516\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax)\n    cb.ax.invert_yaxis()\n    fig.canvas.draw()\n    assert np.all(np.diff(cb.ax.get_yticks()) < 0)\n", "def test_boundarynorm_unscaled_cmap():\n    fig, ax = plt.subplots(figsize=(6, 1))\n    fig.subplots_adjust(bottom=0.5)\n\n    data = [1, 2, 3, 4, 5]\n    boundaries = [0, 2, 4, 6]\n    norm = BoundaryNorm(boundaries, len(data) - 1)\n    cmap = cm.get_cmap(\"viridis\", len(data))\n    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n    cbar = fig.colorbar(mappable, cax=ax, orientation=\"horizontal\")\n"], "sample_383": ["def test_ticket_24605_exclude(self):\n    \"\"\"\n    Subquery table names should be quoted in exclude case.\n    \"\"\"\n    i1 = Individual.objects.create(alive=True)\n    RelatedIndividual.objects.create(related=i1)\n    i2 = Individual.objects.create(alive=False)\n    RelatedIndividual.objects.create(related=i2)\n    i3 = Individual.objects.create(alive=True)\n    i4 = Individual.objects.create(alive=False)\n\n    self.assertSequenceEqual(\n        Individual.objects.exclude(\n            Q(alive=False) | Q(related_individual__isnull=True)\n        ).order_by(\"pk\"),\n        [i1, i3],\n    )\n", "    def test_ticket_24605_reverse_relation(self):\n        \"\"\"\n        Test the exclude() query with reverse relation.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n\n        self.assertSequenceEqual(\n            Individual.objects.exclude(\n                Q(alive=False), Q(related_individual__isnull=True)\n            ).order_by(\"pk\"),\n            [i1, i3],\n        )\n", "    def test_ticket_23622_alternative(self):\n        \"\"\"\n        Test an alternative case for __pk__in and __in with a distinct on subquery.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=10.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        qx = Q(\n            ticket23605b__pk__in=Ticket23605B.objects.order_by(\n                \"modela_fk\", \"-field_b0\"\n            ).distinct(\"modela_fk\")\n        ) & Q(ticket23605b__field_b1=True)\n        qy = Q(\n            ticket23605b__in=Ticket23605B.objects.order_by(\n                \"modela_fk\", \"-field_b0\"\n            ).distinct(\"modela_fk\")\n        ) & Q(ticket23605b__field_b1=True)\n        self.assertEqual(\n            set(Ticket23605A.objects.filter(qx).values_list(\"pk\", flat=True)),\n            set(Ticket23605A.objects.filter(qy).values_list(\"pk\", flat=True)),\n        )\n        self.assertSequenceEqual", "def test_ticket_24729(self):\n    \"\"\"\n    Subquery with an ORed condition should generate correct SQL.\n    \"\"\"\n    a1 = Ticket23605A.objects.create()\n    a2 = Ticket23605A.objects.create()\n    c1 = Ticket23605C.objects.create(field_c0=10000.0)\n    Ticket23605B.objects.create(\n        field_b0=10000.0, field_b1=True, modelc_fk=c1, modela_fk=a1\n    )\n    Ticket23605B.objects.create(\n        field_b0=5000.0, field_b1=False, modelc_fk=c1, modela_fk=a2\n    )\n    complex_q = Q(\n        pk__in=Ticket23605A.objects.filter(\n            Q(\n                ticket23605b__field_b0__gte=1000000\n                / F(\"ticket23605b__modelc_fk__field_c0\")\n            )\n            |\n            Q(ticket23605b__field_b1=True)\n        )\n    )\n    qs1 = Ticket23605A.objects.filter(complex_q)\n    self.assertSequenceEqual(qs1, [a1, a2])\n    qs2 = Ticket23605A.objects.exclude(complex_q)\n    self.assertSequenceEqual(qs2, [])\n", "    def test_ticket_24835(self):\n        \"\"\"\n        Make sure __in works correctly with a QuerySet when the QuerySet is\n        limited to a single result row.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        b1 = Ticket23605B.objects.create(modela_fk=a1)\n        b2 = Ticket23605B.objects.create(modela_fk=a1)\n        qs = Ticket23605A.objects.filter(ticket23605b__in=[b1])\n        self.assertSequenceEqual(qs, [a1])\n        single_result_qs = Ticket23605B.objects.filter(pk=b1.pk)\n        qs = Ticket23605A.objects.filter(ticket23605b__in=single_result_qs)\n        self.assertSequenceEqual(qs, [a1])\n        qs = Ticket23605A.objects.filter(ticket23605b__in=[b1, b2])\n        self.assertSequenceEqual(qs, [a1])\n        single_result_qs = Ticket23605B.objects.filter(pk=b2.pk)\n        qs = Ticket23605A.objects.filter(ticket23605b__in=[b1, single_result_qs])\n        self.assertSequenceEqual(qs, [a1])\n"], "sample_961": ["def test_python_attribute_with_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[str]\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"Optional\"],\n                                                                        [desc_sig_punctuation, \"[\"],\n                                                                        [pending_xref, \"str\"],\n                                                                        [desc_sig_punctuation, \"]\"])])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][3], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n", "def test_property_with_type_annotation(app):\n    text = \".. py:property:: prop -> str\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"property \"],\n                                                    [desc_name, \"prop\"],\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert 'prop' in domain.objects\n    assert domain.objects['prop'] == ('index', 'prop', 'property', False)\n", "def test_python_function_with_noindex(app):\n    text = \".. py:function:: f()\\n   :noindex:\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[])\n    assert_node(doctree[1], addnodes.desc, noindex=True)\n", "def test_pyclass_deprecated(app, warning):\n    text = (\".. py:class:: sphinx.deprecation.RemovedInSphinx30Warning\\n\"\n            \"   :deprecated:\\n\")\n    restructuredtext.parse(app, text)\n    assert warning.getvalue() == \"\"\n\n    domain = app.env.get_domain('py')\n    assert domain.modules['sphinx.deprecation'].deprecated is True\n", "def test_property_resolve_xref(app):\n    text = (\".. py:module:: foo\\n\"\n            \".. py:property:: prop\\n\"\n            \"   :type: int\\n\"\n            \".. py:class:: Bar\\n\"\n            \"   :noindex:\\n\"\n            \"   :property prop: blah blah\\n\"\n            \"   :type prop: int\\n\"\n            \"   :property Bar.prop: blah blah\\n\"\n            \"   :type Bar.prop: int\\n\"\n            \"   :property foo.prop: blah blah\\n\"\n            \"   :type foo.prop: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    # Resolve xrefs for property role\n    for node in doctree.traverse(pending_xref):\n        node = app.env.domains['py'].resolve_xref(app.env, 'index', app.builder,\n                                                  node['reftype'], node['reftarget'], node, None)\n\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"property \"],\n                                                    [desc_addname, \"foo.\"],\n                                                    [desc_name, \"prop\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])])],\n                                  [desc_content, ()])],\n                          nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Bar\"])],\n                                  [desc_content, nodes.field_list, (nodes.field, nodes.field)])]))\n\n    # :property prop:\n    assert_node(doctree[3][1][0][0][1],\n                ([pending_xref, addnodes.literal_emphasis, \"prop\"],\n                 \""], "sample_332": ["def test_formset_with_min_num(self):\n    \"\"\"FormSets can have a minimum number of forms.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '1',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n    }\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at least 2 forms.']\n    )\n", "def test_formset_with_deletion_extra_form(self):\n    \"\"\"\n    FormSets with can_delete and extra forms are handled correctly.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, extra=2)\n    initial = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    self.assertHTMLEqual(\n        '\\n'.join(form.as_ul() for form in formset.forms),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_formset_with_custom_prefix(self):\n    \"\"\"The management form has the correct prefix when a custom prefix is used.\"\"\"\n    formset = FavoriteDrinksFormSet(prefix='custom_prefix')\n    self.assertEqual(formset.management_form.prefix, 'custom_prefix')\n    data = {\n        'custom_prefix-TOTAL_FORMS': '2',\n        'custom_prefix-INITIAL_FORMS': '0',\n        'custom_prefix-MAX_NUM_FORMS': '0',\n    }\n    formset = FavoriteDrinksFormSet(data=data, prefix='custom_prefix')\n    self.assertEqual(formset.management_form.prefix, 'custom_prefix')\n    formset = FavoriteDrinksFormSet(initial={}, prefix='custom_prefix')\n    self.assertEqual(formset.management_form.prefix, 'custom_prefix')\n", "def test_min_num_with_deleted_forms(self):\n    \"\"\"min_num validation is not affected by deleted forms.\"\"\"\n    initial = [\n        {'choice': 'Zero', 'votes': 0},\n        {'choice': 'One', 'votes': 0},\n    ]\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '2',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-MAX_NUM_FORMS': '2',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-0-DELETE': 'on',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True, can_delete=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices', initial=initial)\n    self.assertTrue(formset.forms[0].has_changed())\n    self.assertFalse(formset.forms[1].has_changed())\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at least 2 forms.'])\n", "def test_formset_with_can_order_and_can_delete_extra(self):\n    \"\"\"\n    FormSets with ordering and can_delete=True and can_delete_extra=True\n    should add delete checkboxes to all forms.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True, can_delete_extra=True)\n    initial = [\n        {'choice': 'Calexico', 'votes': 100},\n        {'choice': 'Fergie', 'votes': 900},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    self.assertHTMLEqual(\n        '\\n'.join(form.as_ul() for form in formset.forms),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>"], "sample_265": ["def test_template_origin(self):\n    \"\"\"django.Template.origin should be the same as the origin of the underlying template.\"\"\"\n    engine = DjangoTemplates({\n        'DIRS': [Path(__file__).parent / 'templates' / 'template_backends'],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n    template = engine.get_template('hello.html')\n    self.assertEqual(template.origin.name, 'hello.html')\n", "def test_templatetag_discovery_unicode_error(self):\n    \"\"\"\n    Unicode errors in tag modules should be reraised with a helpful message.\n    \"\"\"\n    with self.assertRaisesMessage(\n        InvalidTemplateLibrary,\n        \"UnicodeDecodeError raised when trying to load \"\n        \"'template_backends.apps.unicode.templatetags.broken_tags'\"\n    ) as cm:\n        DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n    self.assertIsInstance(cm.exception.__cause__, UnicodeDecodeError)\n", "def test_file_charset_option(self):\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {\n            'file_charset': 'iso-8859-1',\n        },\n    })\n\n    self.assertEqual(engine.engine.file_charset, 'iso-8859-1')\n", "def test_templatetag_discovery_invalid_library(self):\n    \"\"\"\n    Modules that don't have a register attribute shouldn't be added to the\n    libraries.\n    \"\"\"\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n\n    self.assertNotIn('bad_import_tags', engine.engine.libraries)\n", "def test_get_template_does_not_exist(self):\n    \"\"\"\n    get_template() should raise TemplateDoesNotExist if the template does not exist.\n    \"\"\"\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {},\n    })\n\n    with self.assertRaises(TemplateDoesNotExist):\n        engine.get_template('nonexistent_template.html')\n"], "sample_6": ["def test_array_wrap_at():\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    wrapped_a = a.wrap_at(360 * u.deg)\n    npt.assert_array_almost_equal(wrapped_a.degree, [340., 150., 350.])\n\n    a.wrap_at('180d', inplace=True)\n    npt.assert_array_almost_equal(a.degree, [-20., 150., -10.])\n", "def test_angle_wrapping():\n    \"\"\"\n    Test wrapping of Angle objects.\n    \"\"\"\n    a1 = Angle([365, -15, 720], unit=u.degree)\n    a1.wrap_at(360 * u.degree, inplace=True)\n    npt.assert_almost_equal(a1.value, [5, 345, 0])\n\n    a2 = Angle([185, -195, 360], unit=u.degree)\n    a2.wrap_at(180 * u.degree, inplace=True)\n    npt.assert_almost_equal(a2.value, [-155, 155, 0])\n\n    a3 = Angle([370, -10, 900], unit=u.degree)\n    a3.wrap_at(np.pi * u.radian, inplace=True)\n    npt.assert_almost_equal(a3.value, [np.pi - 10 * u.degree, np.pi - 10 * u.degree, 0])\n", "def test_angle_wrapping():\n    # Test wrapping at a specific angle\n    a1 = Angle([365, 360, -360, 720], unit=u.degree)\n    wrapped = a1.wrap_at(Angle(360, unit=u.degree))\n    npt.assert_almost_equal(wrapped.value, [5, 0, 0, 0])\n\n    # Test wrapping at a different angle\n    a2 = Angle([185, 180, -180, 360], unit=u.degree)\n    wrapped = a2.wrap_at(Angle(180, unit=u.degree))\n    npt.assert_almost_equal(wrapped.value, [-175, 180, -180, 0])\n\n    # Test wrapping at an angle with a different unit\n    a3 = Angle([0.9999, 1, -1, 2], unit=u.radian)\n    wrapped = a3.wrap_at(Angle(1, unit=u.radian))\n    npt.assert_almost_equal(wrapped.value, [0.9999, 1, -1, 0], decimal=4)\n", "def test_angle_wrapping():\n    \"\"\"\n    Test the wrapping of angles.\n    \"\"\"\n    a1 = Angle([360, 361, 720, -360, -361, -720], unit=u.degree)\n    wrapped_a1 = Angle([0, 1, 0, 0, -1, 0], unit=u.degree)\n    npt.assert_almost_equal(a1.wrap_at(360 * u.deg).degree, wrapped_a1.degree)\n\n    a2 = Angle([180, 270, 360, -180, -270, -360], unit=u.degree)\n    wrapped_a2 = Angle([180, -90, 0, 180, 90, 0], unit=u.degree)\n    npt.assert_almost_equal(a2.wrap_at(180 * u.deg).degree, wrapped_a2.degree)\n\n    # Test in-place wrapping\n    a3 = Angle([360, 361], unit=u.degree)\n    a3.wrap_at(360 * u.deg, inplace=True)\n    npt.assert_almost_equal(a3.degree, [0, 1])\n\n    # Test wrapping with a custom wrap angle\n    a4 = Angle([10, 15, 20, 25], unit=u.degree)\n    wrapped_a4 = Angle([10, -5, 10, -5], unit=u.degree)\n    npt.assert_almost_equal(a4.wrap_at(18 * u.deg).degree, wrapped_a4.degree)\n\n    # Test wrapping with invalid inputs\n    with pytest.raises(u.UnitsError):\n        Angle([360, 361], unit=u.degree).wrap_at('invalid')\n\n    with pytest.raises(ValueError):\n        Angle([360,", "def test_angle_wrap_at():\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n\n    wrapped_a = a.wrap_at(360 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [340.0, 150.0, 350.0])\n\n    a.wrap_at(180 * u.deg, inplace=True)\n    npt.assert_almost_equal(a.degree, [-20.0, 150.0, -10.0])\n\n    # Test wrap_at with a different unit\n    a = Angle([-0.5, 0.5] * u.radian)\n\n    wrapped_a = a.wrap_at(180 * u.deg)\n    npt.assert_almost_equal(wrapped_a.radian, [-np.pi/2, np.pi/2])\n\n    # Test wrap_at with a non-default wrap angle\n    a = Angle([-370.0, 250.0] * u.deg)\n\n    wrapped_a = a.wrap_at(180 * u.deg)\n    npt.assert_almost_equal(wrapped_a.degree, [-10.0, -70.0])\n"], "sample_269": ["def test_i18n_language_english_default_multiple_packages(self):\n    \"\"\"\n    Check if the JSON i18n view returns a complete language catalog\n    if the default language is en-us, the selected language has a\n    translation available and a catalog composed by djangojs domain\n    translations of multiple Python packages is requested.\n    \"\"\"\n    with override('fr'):\n        response = self.client.get('/jsoni18n_multi_packages1/')\n        data = json.loads(response.content.decode())\n        self.assertIn('catalog', data)\n        self.assertIn('formats', data)\n        self.assertIn('plural', data)\n        self.assertIn('il faut traduire cette cha\u00eene de caract\u00e8res de app1', data['catalog'].values())\n        self.assertIn('il faut traduire cette cha\u00eene de caract\u00e8res de app2', data['catalog'].values())\n\n        response = self.client.get('/jsoni18n/app1/')\n        data = json.loads(response.content.decode())\n        self.assertIn('il faut traduire cette cha\u00eene de caract\u00e8res de app1', data['catalog'].values())\n        self.assertNotIn('il faut traduire cette cha\u00eene de caract\u00e8res de app2', data['catalog'].values())\n\n        response = self.client.get('/jsoni18n/app2/')\n        data = json.loads(response.content.decode())\n        self.assertIn('il faut traduire cette cha\u00eene de caract\u00e8res de app2', data['catalog'].values())\n        self.assertNotIn('il faut traduire cette cha\u00eene de caract\u00e8res de app1', data['catalog'].values())\n", "    def test_get_plural(self):\n        view = JavaScriptCatalog()\n        view.translation = DjangoTranslation('en')\n        plural = view.get_plural()\n        self.assertEqual(plural, 'n != 1')\n", "    def test_jsi18n_with_different_domains(self):\n        \"\"\"\n        The javascript_catalog can be deployed with different gettext domains.\n        \"\"\"\n        for domain in ['djangojs', 'django']:\n            response = self.client.get('/jsi18n/', {'domain': domain})\n            self.assertEqual(response.headers['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n            self.assertContains(response, f'\"domain\": \"{domain}\"')\n            if domain == 'djangojs':\n                self.assertContains(response, '\"this is to be translated\": \"this is to be translated\"')\n            else:\n                self.assertContains(response, '\"This is to be translated\": \"This is to be translated\"')\n", "def test_i18n_language_english_default_with_new_app(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    if the default language is en-us, the selected language has a\n    translation available and a catalog composed by djangojs domain\n    translations of a newly added Python package is requested.\n    \"\"\"\n    base_trans_string = 'il faut traduire cette cha\\\\u00eene de caract\\\\u00e8res de '\n    app6_trans_string = base_trans_string + 'app6'\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertContains(response, app6_trans_string)\n", "def test_non_BMP_char_in_javascript_catalog(self):\n    \"\"\"\n    Non-BMP characters should not break the javascript_catalog (#21725).\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n/app5/')\n        self.assertContains(response, 'emoji')\n        self.assertContains(response, '\\\\ud83d\\\\udca9')\n"], "sample_38": ["def test_all_world2pix_with_adaptive_and_detect_divergence():\n    \"\"\"Test all_world2pix with adaptive and detect_divergence options\"\"\"\n\n    # Open test FITS file:\n    fname = get_pkg_data_filename('data/j94f05bgq_flt.fits')\n    ext = ('SCI', 1)\n    h = fits.open(fname)\n    w = wcs.WCS(h[ext].header, h)\n    h.close()\n    del h\n\n    crpix = w.wcs.crpix\n    ncoord = crpix.shape[0]\n\n    # Assume that CRPIX is at the center of the image and that the image has\n    # a power-of-2 number of pixels along each axis. Only use the central\n    # 1/64 for this testing purpose:\n    naxesi_l = list((7. / 16 * crpix).astype(int))\n    naxesi_u = list((9. / 16 * crpix).astype(int))\n\n    # Generate integer indices of pixels (image grid):\n    img_pix = np.dstack([i.flatten() for i in\n                         np.meshgrid(*map(range, naxesi_l, naxesi_u))])[0]\n\n    # Generage random data (in image coordinates):\n    with NumpyRNGContext(123456789):\n        rnd_pix = np.random.rand(random_npts, ncoord)\n\n    # Scale random data to cover the central part of the image\n    mwidth = 2 * (crpix * 1. / 8)\n    rnd_pix = crpix - 0.5 * mwidth + (mwidth - 1) * rnd_pix\n\n    # Reference pixel coordinates in image coordinate system (CS):\n    test_pix = np.append(img_pix, rnd_pix, axis=0)\n    # Reference pixel coordinates in sky CS using forward transformation:\n    all_world = w.all_pix2world(test_pix, origin", "def test_non_finite_inputs():\n    wcsobj = wcs.WCS(naxis=2)\n    with pytest.raises(ValueError):\n        wcsobj.all_pix2world([np.nan], [1], 0)\n    with pytest.raises(ValueError):\n        wcsobj.all_pix2world([2], [np.inf], 0)\n    with pytest.raises(ValueError):\n        wcsobj.all_world2pix([np.nan], [1], 0)\n    with pytest.raises(ValueError):\n        wcsobj.all_world2pix([2], [np.inf], 0)\n", "def test_get_axis_types():\n    w = wcs.WCS(naxis=2)\n    assert w.get_axis_types() == [\n        {'coordinate_type': 'celestial', 'scale': 'linear', 'group': 0, 'number': 0},\n        {'coordinate_type': 'celestial', 'scale': 'linear', 'group': 0, 'number': 1}\n    ]\n\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'VRAD']\n    assert w.get_axis_types() == [\n        {'coordinate_type': 'celestial', 'scale': 'linear', 'group': 0, 'number': 0},\n        {'coordinate_type': 'celestial', 'scale': 'linear', 'group': 0, 'number': 1},\n        {'coordinate_type': 'spectral', 'scale': 'linear', 'group': 0, 'number': 0}\n    ]\n", "def test_naxis_input():\n    \"\"\"\n    Test initialization with naxis input.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    assert w.naxis == 2\n    assert w.wcs.naxis == 2\n\n    w = wcs.WCS(naxis=3)\n    assert w.naxis == 3\n    assert w.wcs.naxis == 3\n\n    w = wcs.WCS(naxis=[2, 2])\n    assert w.naxis == 2\n    assert w.wcs.naxis == 2\n\n    w = wcs.WCS(naxis=[2, 3, 4])\n    assert w.naxis == 3\n    assert w.wcs.naxis == 3\n", "def test_no_crpix_or_crval():\n    \"\"\"\n    Test WCS object creation without CRPIX or CRVAL.\n    \"\"\"\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---TAN'\n    header['CTYPE2'] = 'DEC--TAN'\n    header['CUNIT1'] = 'deg'\n    header['CUNIT2'] = 'deg'\n    header['CDELT1'] = 1.0\n    header['CDELT2'] = 1.0\n\n    w = wcs.WCS(header)\n    assert w.wcs.crpix[0] == 0.0\n    assert w.wcs.crpix[1] == 0.0\n    assert w.wcs.crval[0] == 0.0\n    assert w.wcs.crval[1] == 0.0\n"], "sample_1172": ["def test_solve_poly_system_with_multiple_solutions():\n    # Test solve_poly_system with multiple solutions\n    solutions = solve_poly_system([x**2 - 1, y**2 - 4], x, y)\n    expected_solutions = [(1, 2), (1, -2), (-1, 2), (-1, -2)]\n    assert sorted(solutions) == sorted(expected_solutions)\n", "def test_solve_poly_system_mixed_domain():\n    x, y = symbols('x y')\n    # Test case with mixed domain coefficients\n    eq1 = Poly(x - 1, x, y, domain='ZZ')\n    eq2 = Poly(y - Rational(1, 2), x, y)\n    solution = solve_poly_system([eq1, eq2], x, y)\n    assert solution == [(1, Rational(1, 2))]\n", "def test_solve_poly_system_extended():\n    # Test the solve_poly_system function with more complex polynomials\n    x, y, z = symbols('x y z')\n    f1 = x**3 - y**2 + z\n    f2 = x*y + y*z - z**2\n    f3 = x**2*z - y**3\n    solutions = solve_poly_system([f1, f2, f3], x, y, z)\n    # Check that the solutions satisfy the original equations\n    for sol in solutions:\n        assert f1.subs(sol) == 0\n        assert f2.subs(sol) == 0\n        assert f3.subs(sol) == 0\n", "def test_solve_poly_system_non_zero_dimensional():\n    f_1 = x**2 + y - 1\n    f_2 = x - y**2\n\n    raises(NotImplementedError, lambda: solve_poly_system([f_1, f_2], x, y))\n", "def test_solve_poly_system_domain():\n    x, y, z = symbols('x y z')\n    f = x**2 + y + z - 1\n    g = x + y**2 + z - 1\n    h = x + y + z**2 - 1\n    solution = [(0, 0, 1), (0, 1, 0), (1, 0, 0)]\n    assert solve_poly_system([f, g, h], x, y, z, domain=QQ) == solution\n"], "sample_118": ["def test_year_lookup(self):\n    # Test the YearLookup class\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__exact=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__gt=2005),\n        [],\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__gte=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__lt=2006),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__lte=2005),\n        ['<Article: Article 5>', '<Article: Article 6", "def test_in_bulk_with_iterable(self):\n    self.assertEqual(\n        Author.objects.in_bulk(iter([self.au1.pk, self.au2.pk])),\n        {\n            self.au1.pk: self.au1,\n            self.au2.pk: self.au2,\n        }\n    )\n", "def test_year_lookups(self):\n    # Test year lookup methods on date and datetime fields.\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__exact=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__gt=2005),\n        ['<Article: Article 5>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__gte=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article: Article 1>'],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__lt=2005),\n        [],\n        ordered=False\n    )\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__lte=2005),\n        ['<Article: Article 5>', '<Article: Article 6>', '<Article: Article 4>',\n         '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 7>',\n         '<Article:", "def test_isnull(self):\n    \"\"\"IsNull lookup should return correct results.\"\"\"\n    # Create some articles with null headline\n    Article.objects.create(pub_date=datetime.now(), headline=None)\n    # Test isnull=True\n    self.assertEqual(Article.objects.filter(headline__isnull=True).count(), 1)\n    # Test isnull=False\n    self.assertEqual(Article.objects.filter(headline__isnull=False).count(), 7)\n", "def test_exists_with_outerref(self):\n    qs = Article.objects.filter(author=OuterRef('pk'))\n    authors = Author.objects.annotate(\n        has_articles=Exists(qs),\n    ).filter(\n        has_articles=True,\n    )\n    self.assertCountEqual(authors, [self.au1, self.au2])\n"], "sample_549": ["def test_array_patch_perimeters_empty():\n    x = np.array([])\n    with pytest.raises(AssertionError):\n        cbook._array_patch_perimeters(x, rstride=1, cstride=1)\n", "def test_safe_first_element_with_nan():\n    float_lst = [0.1, 0.2, float('nan'), 0.4]\n    actual = cbook._safe_first_finite(float_lst)\n    assert actual == 0.1\n", "def test_safe_first_element_with_numpy_nan():\n    numpy_lst = [np.nan, np.nan, 0.2, 0.3]\n    actual = cbook._safe_first_finite(numpy_lst)\n    assert np.isnan(actual) == False and actual == 0.2\n", "def test_g_sig_digits():\n    assert cbook._g_sig_digits(0.000123456, 0.000001) == 2\n    assert cbook._g_sig_digits(123456, 1) == 5\n    assert cbook._g_sig_digits(123456, 10) == 3\n    assert cbook._g_sig_digits(123456, 10000) == 1\n    assert cbook._g_sig_digits(0, 0) == 0\n    assert cbook._g_sig_digits(1, 0) == 1\n    assert cbook._g_sig_digits(123456, 0) == 6\n    assert cbook._g_sig_digits(float('inf'), 1) == 0\n    assert cbook._g_sig_digits(float('nan'), 1) == 0\n", "def test_array_perimeter():\n    # Test for a 2D array\n    x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    result = cbook._array_perimeter(x)\n    expected = np.array([1, 2, 3, 6, 9, 8, 7, 4])\n    np.testing.assert_array_equal(result, expected)\n\n    # Test for an empty array\n    x = np.array([])\n    result = cbook._array_perimeter(x)\n    expected = np.array([])\n    np.testing.assert_array_equal(result, expected)\n\n    # Test for a 1D array\n    x = np.array([1, 2, 3])\n    result = cbook._array_perimeter(x)\n    expected = np.array([1, 3])\n    np.testing.assert_array_equal(result, expected)\n"], "sample_107": ["def test_sensitive_settings_with_callable(self):\n    \"\"\"\n    The debug page should not show some sensitive settings\n    (password, secret key, ...) even if they are callable.\n    \"\"\"\n        return \"should not be displayed\"\n    with self.settings(DEBUG=True, CALLABLE_SETTING=callable_setting):\n        response = self.client.get('/raises500/')\n        self.assertNotContains(response, 'should not be displayed', status_code=500)\n", "def test_cleanse_setting_non_str_key(self):\n    initial = {42: 'should be filtered out'}\n    expected = {}\n    self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_ignore_non_string_keys(self):\n        initial = {42: 'secret'}\n        expected = {42: 'secret'}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_call(self):\n        class WrappedCallable:\n                return \"return value from the wrapped callable\"\n\n        actual = CallableSettingWrapper(WrappedCallable())()\n        self.assertEqual(actual, \"return value from the wrapped callable\")\n", "    def test_cleanse_setting_handles_unhashable_values(self):\n        initial = {'login': 'cooper', 'password': [1, 2, 3]}\n        expected = {'login': 'cooper', 'password': CLEANSED_SUBSTITUTE}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n"], "sample_776": ["def test_lasso_lars_vs_lasso_cd_early_stopping_with_fit_path_false(verbose=False):\n    # Test that LassoLars and Lasso using coordinate descent give the\n    # same results when early stopping is used and fit_path is False.\n    # (test : before, in the middle, and in the last part of the path)\n    alphas_min = [10, 0.9, 1e-4]\n\n    for alpha_min in alphas_min:\n        lars = linear_model.LassoLars(alpha_min=alpha_min, fit_path=False)\n        lars.fit(X, y)\n        lasso_cd = linear_model.Lasso(alpha=lars.alpha_, tol=1e-8)\n        lasso_cd.fit(X, y)\n        error = linalg.norm(lars.coef_ - lasso_cd.coef_)\n        assert_less(error, 0.01)\n\n    # same test, with normalization\n    for alpha_min in alphas_min:\n        lars = linear_model.LassoLars(alpha_min=alpha_min, normalize=True, fit_path=False)\n        lars.fit(X, y)\n        lasso_cd = linear_model.Lasso(alpha=lars.alpha_, tol=1e-8, normalize=True)\n        lasso_cd.fit(X, y)\n        error = linalg.norm(lars.coef_ - lasso_cd.coef_)\n        assert_less(error, 0.01)\n", "def test_lars_path_epsilon():\n    # Test that lars_path is robust to different values of epsilon\n    X = np.array([[3., 3., 1.],\n                  [2., 2., 0.],\n                  [1., 1., 0]])\n    y = np.array([1., 0., 0])\n\n    coef_path_default = linear_model.lars_path(X, y, alpha_min=0.01)[2]\n    coef_path_eps = linear_model.lars_path(X, y, alpha_min=0.01, eps=1e-6)[2]\n\n    assert not np.isnan(coef_path_eps).any()\n    assert not np.allclose(coef_path_default, coef_path_eps)\n", "def test_lars_fit_path_true():\n    # Test that the ``fit_path=True`` option returns the correct output\n    lars = linear_model.Lars(fit_path=True)\n    lars.fit(X, y)\n    assert hasattr(lars, 'coef_path_')\n    assert lars.coef_path_.shape[1] == len(lars.alphas_)\n", "def test_lasso_lars_custom_eps():\n    # Test that LassoLars gives expected results with custom eps\n    X = 3 * diabetes.data\n    eps = 1e-2\n    lars = linear_model.LassoLars(alpha=0.1, eps=eps)\n    lars.fit(X, y)\n    coef_custom_eps = lars.coef_\n\n    lars_default_eps = linear_model.LassoLars(alpha=0.1)\n    lars_default_eps.fit(X, y)\n    coef_default_eps = lars_default_eps.coef_\n\n    assert_less(np.linalg.norm(coef_custom_eps - coef_default_eps), eps)\n", "def test_lars_path_method_input():\n    # Test that the method input is handled correctly\n    X = diabetes.data\n    y = diabetes.target\n\n    # Test with correct method input\n    alphas, active, coefs = linear_model.lars_path(X, y, method='lar')\n    assert isinstance(alphas, np.ndarray)\n    assert isinstance(active, list)\n    assert isinstance(coefs, np.ndarray)\n\n    alphas, active, coefs = linear_model.lars_path(X, y, method='lasso')\n    assert isinstance(alphas, np.ndarray)\n    assert isinstance(active, list)\n    assert isinstance(coefs, np.ndarray)\n\n    # Test with incorrect method input\n    with pytest.raises(ValueError):\n        linear_model.lars_path(X, y, method='invalid')\n"], "sample_485": ["def test_urlize_unicode_domain(self):\n    value = \"foo@b\u00fccher.museum\"\n    output = '<a href=\"mailto:foo@xn--bcher-kva.museum\">foo@b\u00fccher.museum</a>'\n    self.assertEqual(urlize(value), output)\n", "def test_urlize_with_nofollow(self):\n    value = \"Visit https://example.com and be careful.\"\n    output = 'Visit <a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a> and be careful.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_trim_url_limit(self):\n    value = \"This is a very long URL: https://www.example.com/this/is/a/very/long/path/\"\n    output = 'This is a very long URL: <a href=\"https://www.example.com/this/is/a/very/long/path/\">https://www.example.com/this/...</a>'\n    self.assertEqual(urlize(value, trim_url_limit=25), output)\n", "def test_urlize_with_autoescape(self):\n    value = \"Search for google.com/?q=! and see.\"\n    output = (\n        'Search for &lt;a href=\"http://google.com/?q=\"&gt;google.com/?q=&lt;/a&gt;! '\n        'and see.'\n    )\n    self.assertEqual(urlize(value, autoescape=True), output)\n", "def test_urlize_no_autoescape(self):\n    test_cases = (\n        (\n            \"Search for <span>google.com</span>/?q=! and see.\",\n            'Search for <a href=\"http://google.com/?q=\"><span>google.com</span>/?q=</a>! and see.',\n        ),\n        (\n            lazystr(\"Search for <span>google.com</span>/?q=!\"),\n            'Search for <a href=\"http://google.com/?q=\"><span>google.com</span>/?q=</a>!',\n        ),\n        (\n            \"foo@<b>example.com</b>\",\n            '<a href=\"mailto:foo@example.com\">foo@<b>example.com</b></a>',\n        ),\n    )\n    for value, output in test_cases:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=False), output)\n"], "sample_1022": ["def test_lambda_notation():\n    cases = {\n        'lambda x: x**2': 'Lambda(x, x**2)',\n        'lambda x, y: x + y': 'Lambda((x, y), x + y)',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations)\n        normal = parse_expr(cases[case])\n        assert(implicit == normal)\n\n    raises(TokenError, lambda: parse_expr('lambda x, *y: x + y'))\n    raises(SyntaxError, lambda: parse_expr('lambda: x + y'))\n", "def test_convert_equals_signs():\n    cases = {\n        '1=2': 'Eq(1, 2)',\n        'x=y': 'Eq(x, y)',\n        '(1=2)=False': 'Eq(Eq(1, 2), False)',\n        'x*2=y': 'Eq(x*2, y)',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (convert_equals_signs,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n\n    application = ['sin x=cos y', 'tan z=2', 'sin=cos x']\n    for case in application:\n        raises(SyntaxError,\n               lambda: parse_expr(case, transformations=transformations2))\n", "def test_implicit_multiplication_application_combination():\n    cases = {\n        '2sin(x)cos(x)': '2*sin(x)*cos(x)',\n        'sin(x)cos(x)tan(x)': 'sin(x)*cos(x)*tan(x)',\n        'E^x sin(x)': 'E**x * sin(x)',\n        'pi sin(x)cos(x)': 'pi * sin(x) * cos(x)',\n        'alpha beta gamma': 'alpha * beta * gamma',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (implicit_multiplication_application,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n", "def test_implicit_application_with_constants():\n    cases = {\n        'pi sin(x)': 'pi*sin(x)',\n        'E sin(x)': 'E*sin(x)',\n        'EulerGamma sin(x)': 'EulerGamma*sin(x)',\n        'I sin(x)': 'I*sin(x)',\n        'NaN sin(x)': 'NaN*sin(x)',\n        'Inf sin(x)': 'Inf*sin(x)',\n        'Infinity sin(x)': 'Infinity*sin(x)',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (implicit_application,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n", "def test_convert_xor():\n    cases = {\n        '2^3': '2**3',\n        '3^2^1': '3**2**1',\n        'x^2 + y^2': 'x**2 + y**2',\n        '2*3^4': '2*3**4',\n        'sin(x)^2': 'sin(x)**2',\n        'cos(x)^2*sin(y)^2': 'cos(x)**2*sin(y)**2',\n        'sin^2(x)*cos^2(y)': 'sin(x)**2*cos(y)**2',\n        'sin^2 x*cos^2 y': 'sin(x)**2*cos(y)**2',\n    }\n    transformations = standard_transformations + (convert_xor,)\n    for case in cases:\n        xor_replaced = parse_expr(case, transformations=transformations)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(xor_replaced == normal)\n"], "sample_20": ["def test_fits_mixins_empty_table(table_cls, tmp_path):\n    \"\"\"Test write/read an empty table and validate it correctly handles mixin columns\"\"\"\n    filename = tmp_path / \"test_empty.fits\"\n    t = table_cls()\n    t.write(filename, format=\"fits\")\n    t2 = table_cls.read(filename, format=\"fits\", astropy_native=True)\n    assert t.colnames == t2.colnames\n", "def test_read_with_empty_fits(tmp_path):\n    filename = tmp_path / \"test_read_empty.fits\"\n    hdu = fits.PrimaryHDU()\n    hdu.writeto(filename, overwrite=True)\n    with pytest.raises(ValueError) as exc:\n        Table.read(filename)\n    assert exc.value.args[0] == \"No table found\"\n", "def test_is_fits_fileobj():\n    \"\"\"Test is_fits function with file object input\"\"\"\n    data = np.array([(1, 2.0, \"hello\")], dtype=[(\"A\", int), (\"B\", float), (\"C\", \"S10\")])\n    hdu = fits.BinTableHDU(data)\n    hdulist = fits.HDUList([fits.PrimaryHDU(), hdu])\n    fileobj = io.BytesIO()\n    hdulist.writeto(fileobj)\n    fileobj.seek(0)\n    assert io_registry.identify_format(fileobj, None, None, \"fits\") == \"fits\"\n", "def test_fits_time_subclass(tmp_path):\n    \"\"\"Test that Time subclasses are written and read correctly\"\"\"\n    filename = tmp_path / \"test_time_subclass.fits\"\n    tm = Time([\"2022-01-01\", \"2022-01-02\"], format=\"isot\", scale=\"utc\")\n    t = Table([tm], names=[\"time\"])\n    t.write(filename, format=\"fits\")\n    t2 = Table.read(filename, format=\"fits\", astropy_native=True)\n    assert isinstance(t2[\"time\"], Time)\n    assert t2[\"time\"].format == t[\"time\"].format\n    assert np.all(t2[\"time\"] == t[\"time\"])\n", "def test_write_with_missing_columns(tmp_path):\n    \"\"\"Test writing a Table with missing columns\"\"\"\n    filename = tmp_path / \"test_simple.fits\"\n    data = np.array([(1, 'a', 2.3), (2, 'b', 4.5), (3, 'c', 6.7)],\n                    dtype=[('a', int), ('b', 'U1'), ('c', float)])\n    t = Table(data, masked=True)\n    t['a'].mask = [True, False, True]\n    t['b'].mask = [False, True, False]\n    t['c'].mask = [True, True, False]\n    t['d'] = Column([7, 8, 9], mask=[True, False, True])\n    t.write(filename, overwrite=True)\n\n    # Read the table back and check that the missing columns are filled with default values\n    t2 = Table.read(filename)\n    assert np.ma.allequal(t2['a'], np.ma.masked_array([1, 2, 3], mask=[True, False, True]))\n    assert np.ma.allequal(t2['b'], np.ma.masked_array(['a', 'b', 'c'], mask=[False, True, False]))\n    assert np.ma.allequal(t2['c'], np.ma.masked_array([2.3, 4.5, 6.7], mask=[True, True, False]))\n    assert np.ma.allequal(t2['d'], np.ma.masked_array([7, 8, 9], mask=[True, False, True]))\n\n    # Check that the table can be read back with missing columns filled with non-default values\n    t3 = Table.read(filename, fill_values={None: 0, 'T': 'x'})\n    assert np.ma.allequal(t3['a'], np.ma.masked_array([1, 2, 3], mask=[True, False, True], fill_value=0))\n    assert np.ma.allequal(t3['b'], np"], "sample_245": ["def test_no_obsolete_enabled(self):\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertNotIn(\"#, fuzzy\", po_contents)\n", "    def test_custom_extensions(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, extensions=['tpl'])\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('Translatable literal from a custom extension', po_contents)\n", "    def test_no_obsolete_disabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=False)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertIn('#, fuzzy', po_contents)\n", "def test_no_obsolete_enabled(self):\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertNotIn(\"#, fuzzy\", po_contents)\n", "def test_command_help_with_settings(self):\n    with override_settings(LOCALE_PATHS=[]):\n        with captured_stdout(), captured_stderr():\n            # `call_command` bypasses the parser; by calling\n            # `execute_from_command_line` with the help subcommand we\n            # ensure that there are no issues with the parser itself.\n            execute_from_command_line(['django-admin', 'help', 'makemessages'])\n"], "sample_50": ["def test_no_user(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'host': 'somehost',\n            'port': '444',\n        }), (\n            ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n            None,\n        )\n    )\n", "    def test_no_host_and_port(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n            }), (\n                ['psql', '-U', 'someuser', 'dbname'],\n                None,\n            )\n        )\n", "def test_no_host(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'port': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-p', '444', 'dbname'],\n            'somepassword',\n        )\n    )\n", "def test_no_host_port(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n        }), (\n            ['psql', '-U', 'someuser', 'dbname'],\n            'somepassword',\n        )\n    )\n", "def test_missing_parameters(self):\n    self.assertEqual(\n        self._run_it({\n            'database': 'dbname',\n        }), (\n            ['psql', 'dbname'],\n            None,\n        )\n    )\n"], "sample_162": ["def test_add_location_full_with_different_domains(self):\n    \"\"\"makemessages --add-location=full with different domains\"\"\"\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, add_location='full', domain='djangojs')\n    self.assertTrue(os.path.exists(self.PO_FILE.replace('django', 'djangojs')))\n    # Comment with source file relative path and line number is present.\n    self.assertLocationCommentPresent(self.PO_FILE.replace('django', 'djangojs'), 'gettext_noop should, too.', 'javascript', 'test.js')\n", "def test_no_obsolete_enabled(self):\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertNotIn('#~', po_contents)\n", "    def test_ignore_file_extensions(self):\n        out, po_contents = self._run_makemessages(extensions=['html'])\n        self.assertIn(\"ignoring file test.js\", out)\n        self.assertMsgId('This literal should be included.', po_contents)\n        self.assertNotMsgId('This JavaScript literal should be ignored.', po_contents)\n", "def test_non_ascii_files(self):\n    non_ascii_filename = 'vid\u00e9o.txt'\n    non_ascii_content = 'Contenu en fran\u00e7ais'\n\n    with open(os.path.join(self.test_dir, non_ascii_filename), 'w', encoding='utf-8') as f:\n        f.write(non_ascii_content)\n\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE, encoding='utf-8') as fp:\n        po_contents = fp.read()\n        self.assertIn(non_ascii_content, po_contents)\n", "    def test_no_obsolete_disabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=False)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertIn(\"#~ msgid \\\"This literal should be obsolete\\\"\", po_contents)\n"], "sample_1147": ["def test_latex_Array_slicing():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    assert latex(A[:1, :, :]) == r\"A\\left[:1, :, :\\right]\"\n    assert latex(A[1:, :, :]) == r\"A\\left[1:, :, :\\right]\"\n    assert latex(A[:, 1:, :]) == r\"A\\left[:, 1:, :\\right]\"\n    assert latex(A[:, :2, :]) == r\"A\\left[:, :2, :\\right]\"\n    assert latex(A[:, :, 2:]) == r\"A\\left[:, :, 2:\\right]\"\n    assert latex(A[:, :, :3]) == r\"A\\left[:, :, :3\\right]\"\n    assert latex(A[1:2, :, :]) == r\"A\\left[1:2, :, :\\right]\"\n    assert latex(A[:, 1:2, :]) == r\"A\\left[:, 1:2, :\\right]\"\n    assert latex(A[:, :, 1:2]) == r\"A\\left[:, :, 1:2\\right]\"\n", "def test_latex_printing_of_TensorIndex():\n    from sympy import TensorIndex\n\n    # Define a tensor index\n    i = TensorIndex('i', shape=(3,))\n\n    # Test the latex representation\n    assert latex(i) == r\"{}^{i}\"\n\n    # Test the latex representation of a negative index\n    j = TensorIndex('j', shape=(3,), is_up=False)\n    assert latex(j) == r\"{}_{j}\"\n", "def test_issue_19447():\n    # Test for issue #19447: latex() should not call doctest\n        \"\"\"\n        >>> latex(x)\n        x\n        \"\"\"\n    assert latex(x) == r\"x\"\n", "def test_printing_latex_array_expressions_with_symbols():\n    x = symbols('x')\n    assert latex(ArrayElement(\"A\", (2, 1/(1-x), 0))) == \"{{A}_{2, \\\\frac{1}{1 - x}, 0}}\"\n    assert latex(ArrayElement(\"A\", (2, x, 0))) == \"{{A}_{2, x, 0}}\"\n", "def test_latex_transpose():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(A.T) == r\"A^{\\mathrm{T}}\"\n\n    B = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    assert latex(B.T) == r\"\\left[\\begin{matrix}1 & 3\\\\2 & 4\\end{matrix}\\right]\"\n"], "sample_734": ["def test_fowlkes_mallows_score_sparse():\n    # Test the sparse implementation of fowlkes_mallows_score\n    labels_a = np.array([0, 0, 0, 1, 1, 1])\n    labels_b = np.array([0, 0, 1, 1, 2, 2])\n    score_dense = fowlkes_mallows_score(labels_a, labels_b)\n    score_sparse = fowlkes_mallows_score(labels_a, labels_b, sparse=True)\n    assert_almost_equal(score_dense, score_sparse)\n", "def test_v_measure_and_mutual_information_sparse():\n    # Check relation between v_measure, entropy and mutual information with sparse contingency\n    for i in np.logspace(1, 4, 4).astype(np.int):\n        random_state = np.random.RandomState(36)\n        labels_a, labels_b = (random_state.randint(0, 10, i),\n                              random_state.randint(0, 10, i))\n        contingency = contingency_matrix(labels_a, labels_b, sparse=True)\n        assert_almost_equal(v_measure_score(labels_a, labels_b),\n                            2.0 * mutual_info_score(labels_a, labels_b, contingency=contingency) /\n                            (entropy(labels_a) + entropy(labels_b)), 0)\n", "def test_normalized_mutual_info_score():\n    # Compute the Normalized Mutual Information and test against known values\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    # Normalized Mutual Information\n    nmi = normalized_mutual_info_score(labels_a, labels_b)\n    assert_almost_equal(nmi, 0.65119, 5)\n    # Test with the same labels\n    nmi = normalized_mutual_info_score([1, 1, 2, 2], [1, 1, 2, 2])\n    assert_equal(nmi, 1.0)\n    # Test with a very large array\n    a110 = np.array([list(labels_a) * 110]).flatten()\n    b110 = np.array([list(labels_b) * 110]).flatten()\n    nmi = normalized_mutual_info_score(a110, b110)\n    # This is not accurate to more than 2 places\n    assert_almost_equal(nmi, 0.78, 2)\n", "def test_mutual_info_score_with_empty_labels():\n    # Check the behavior of mutual_info_score when labels are empty\n    labels_a = np.array([])\n    labels_b = np.array([])\n    assert_equal(mutual_info_score(labels_a, labels_b), 0.0)\n", "def test_mutual_info_score_symmetry():\n    # Test symmetry of mutual_info_score\n    labels_a = np.array([1, 1, 1, 2, 2, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 2, 2, 2, 3, 3, 3, 3])\n\n    mi_ab = mutual_info_score(labels_a, labels_b)\n    mi_ba = mutual_info_score(labels_b, labels_a)\n\n    assert_almost_equal(mi_ab, mi_ba)\n"], "sample_172": ["def test_proper_manager_for_label_lookup(self):\n    # see #9258\n    rel = Inventory._meta.get_field('parent').remote_field\n    w = widgets.ForeignKeyRawIdWidget(rel, widget_admin_site)\n\n    hidden = Inventory.objects.create(\n        barcode=93, name='Hidden', hidden=True\n    )\n    child_of_hidden = Inventory.objects.create(\n        barcode=94, name='Child of hidden', parent=hidden\n    )\n    self.assertHTMLEqual(\n        w.render('test', child_of_hidden.parent_id, attrs={}),\n        '<input type=\"text\" name=\"test\" value=\"93\" class=\"vForeignKeyRawIdAdminField\">'\n        '<a href=\"/admin_widgets/inventory/?_to_field=barcode\" '\n        'class=\"related-lookup\" id=\"lookup_id_test\" title=\"Lookup\"></a>'\n        '&nbsp;<strong><a href=\"/admin_widgets/inventory/%(pk)s/change/\">'\n        'Hidden</a></strong>' % {'pk': hidden.pk}\n    )\n", "def test_custom_widget_render(self):\n    class CustomWidget(forms.Select):\n            return 'custom render output'\n    rel = Album._meta.get_field('band').remote_field\n    widget = CustomWidget()\n    wrapper = widgets.RelatedFieldWidgetWrapper(\n        widget, rel, widget_admin_site,\n        can_add_related=True,\n        can_change_related=True,\n        can_delete_related=True,\n    )\n    output = wrapper.render('name', 'value')\n    self.assertIn('custom render output', output)\n", "def test_ForeignKeyRawIdWidget_fk_related_model_not_in_admin(self):\n    # FK to a model not registered with admin site. Raw ID widget should\n    # have no magnifying glass link. See #16542\n    big_honeycomb = Honeycomb.objects.create(location='Old tree')\n    big_honeycomb.bee_set.create()\n    rel = Bee._meta.get_field('honeycomb').remote_field\n\n    w = widgets.ForeignKeyRawIdWidget(rel, widget_admin_site)\n    self.assertHTMLEqual(\n        w.render('honeycomb_widget', big_honeycomb.pk, attrs={}),\n        '<input type=\"text\" name=\"honeycomb_widget\" value=\"%(hcombpk)s\">'\n        '&nbsp;<strong>%(hcomb)s</strong>'\n        % {'hcombpk': big_honeycomb.pk, 'hcomb': big_honeycomb}\n    )\n", "def test_save_button_disabled(self):\n    self.admin_login(username='super', password='secret', login_url='/')\n    self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_add'))\n\n    # The save button should be disabled if the form is not filled out\n    save_button = self.selenium.find_element_by_css_selector('.submit-row > input[type=submit]')\n    self.assertFalse(save_button.is_enabled())\n\n    # Fill out the form\n    username_field = self.selenium.find_element_by_id('id_username')\n    username_field.send_keys('testuser')\n    password_field = self.selenium.find_element_by_id('id_password1')\n    password_field.send_keys('testpassword')\n    password_field2 = self.selenium.find_element_by_id('id_password2')\n    password_field2.send_keys('testpassword')\n\n    # The save button should be enabled now\n    self.assertTrue(save_button.is_enabled())\n", "    def test_radio_select_change(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_add'))\n\n        # Select 'Male' radio button\n        self.selenium.find_element_by_id('id_gender_0').click()\n        self.assertTrue(self.selenium.find_element_by_id('id_gender_0').is_selected())\n        self.assertFalse(self.selenium.find_element_by_id('id_gender_1').is_selected())\n\n        # Select 'Female' radio button\n        self.selenium.find_element_by_id('id_gender_1').click()\n        self.assertFalse(self.selenium.find_element_by_id('id_gender_0').is_selected())\n        self.assertTrue(self.selenium.find_element_by_id('id_gender_1').is_selected())\n"], "sample_767": ["def test_column_transformer_no_estimators_no_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([])\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 0\n", "def test_column_transformer_empty_transformer_list():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([])\n    assert_raise_message(ValueError, \"No valid transformer specified.\", ct.fit_transform, X_array)\n    assert_raise_message(ValueError, \"No valid transformer specified.\", ct.fit, X_array)\n", "def test_column_transformer_remainder_transformer_no_fit():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    class NoFitTrans(BaseEstimator):\n            return X\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=NoFitTrans())\n\n    assert_raise_message(\n        TypeError,\n        \"All estimators should implement fit and transform\",\n        ct.fit, X_array)\n    assert_raise_message(\n        TypeError,\n        \"All estimators should implement fit and transform\",\n        ct.fit_transform, X_array)\n", "def test_column_transformer_empty_input():\n    # Test case for an empty input array\n    X_array = np.array([[]]).T\n\n    ct = ColumnTransformer([('trans', Trans(), [0])])\n    assert_array_equal(ct.fit_transform(X_array), np.array([[]]).T)\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.array([[]]).T)\n\n    # Test case for an empty input DataFrame\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_array, columns=['first'])\n\n    ct = ColumnTransformer([('trans', Trans(), 'first')])\n    assert_array_equal(ct.fit_transform(X_df), np.array([[]]).T)\n    assert_array_equal(ct.fit(X_df).transform(X_df), np.array([[]]).T)\n", "def test_column_transformer_empty_column_selection_callable():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        return []\n\n    ct = ColumnTransformer([('trans', Trans(), func)], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array).shape, (3, 0))\n    assert_array_equal(ct.fit(X_array).transform(X_array).shape, (3, 0))\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == []\n"], "sample_1044": ["def test_issue_10829():\n    x = Symbol('x', positive=True)\n    assert (4**x - 3*x + 2).subs(2**x, y) == y**2 - 3*x + 2\n", "def test_issue_10829():\n    x = Symbol('x')\n    assert ((2**x - 3*y + 2).subs(2**x, y)).equals(y**2 - 3*y + 2)\n", "def test_Pow_is_imaginary():\n    r = Symbol('r', real=True)\n    c = Symbol('c', complex=True)\n    i = Symbol('i', imaginary=True)\n    assert (r**c).is_imaginary is None\n    assert (c**r).is_imaginary is None\n    assert (i**r).is_imaginary is False\n    assert (r**i).is_imaginary\n    assert (c**i).is_imaginary\n    assert (i**i).is_imaginary is False\n    assert (i**c).is_imaginary\n", "def test_pow_is_polar():\n    x = Symbol('x')\n    assert (x**2).is_polar is None\n    assert (x**I).is_polar is False\n    p = Symbol('p', polar=True)\n    assert (p**2).is_polar is True\n    assert (p**x).is_polar is None\n", "def test_issue_10302_simplified():\n    u = -(3*2**pi)**(1/pi) + 2*3**(1/pi)\n    i = u + u*I\n    assert (u + i).is_zero is True  # with simplification this should be True\n"], "sample_620": ["def test_concat_index_not_same_size() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [3, 4, 5])})\n\n    with pytest.raises(\n        ValueError,\n        match=r\"Cannot concatenate along dimension 'x' indexes with different sizes.*\",\n    ):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_coords_different_with_override() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [3, 4])})\n\n    with pytest.raises(\n        ValueError, match=r\"Cannot specify both coords='different' and compat='override'.\"\n    ):\n        concat([ds1, ds2], dim=\"x\", coords=\"different\", compat=\"override\")\n", "def test_concat_different_coords() -> None:\n    ds1 = Dataset({\"x\": 1}, {\"y\": 2})\n    ds2 = Dataset({\"x\": 3}, {\"z\": 4})\n\n    with pytest.raises(ValueError, match=r\"Variables.*are coordinates in some datasets but not others.\"):\n        concat([ds1, ds2], dim=\"w\")\n", "def test_concat_with_index_not_coord() -> None:\n    ds1 = Dataset(coords={\"x\": (\"y\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"y\", [3, 4])})\n\n    # Test when 'x' is an index but not a coordinate\n    expected = Dataset(coords={\"x\": (\"y\", [1, 2, 3, 4])})\n    actual = concat([ds1, ds2], dim=\"y\")\n\n    assert_identical(actual, expected)\n    assert isinstance(actual.x.to_index(), pd.Index)\n", "def test_concat_index_non_default() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"y\", [3, 4])})\n    # TODO: use public API for setting a non-default index, when available\n    ds2._indexes[\"x\"] = PandasIndex([3, 4], \"y\")\n\n    expected = Dataset(coords={\"x\": (\"x\", [1, 2, 3, 4])})\n    actual = concat([ds1, ds2], dim=\"x\")\n\n    assert_identical(expected, actual)\n    assert isinstance(actual.indexes[\"x\"], PandasIndex)\n    assert actual.indexes[\"x\"].dims == (\"x\",)\n    assert actual.indexes[\"x\"].name == \"x\"\n"], "sample_785": ["def test_group_shuffle_split_reproducible():\n    # Check that iterating twice on the GroupShuffleSplit gives the same\n    # sequence of train-test when the random_state is given\n    gss = GroupShuffleSplit(random_state=21)\n    groups = np.arange(10)\n    assert_array_equal(list(a for a, b in gss.split(X, y, groups)),\n                       list(a for a, b in gss.split(X, y, groups)))\n", "def test_leave_p_out():\n    X = np.ones(10)\n    y = np.arange(10)\n    p = 3\n\n    lpo = LeavePOut(p=p)\n    splits = list(lpo.split(X, y))\n\n    # Check that the number of splits is correct\n    assert_equal(len(splits), comb(len(X), p))\n\n    # Check that each split is valid\n    for train, test in splits:\n        assert_equal(len(train), len(X) - p)\n        assert_equal(len(test), p)\n        assert_equal(len(set(train) & set(test)), 0)\n        assert_equal(len(set(train) | set(test)), len(X))\n", "def test_group_shuffle_split_different_groups():\n    # Check that GroupShuffleSplit works normally when the groups variable is changed before calling split\n    groups1 = np.array([0, 1, 2, 1, 1, 2, 0, 0])\n    groups2 = np.array([3, 4, 3, 4, 4, 3, 3, 3])\n    X = np.ones(len(groups1))\n    gss1 = GroupShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n    gss2 = GroupShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n    for (train1, test1), (train2, test2) in zip(gss1.split(X, groups=groups1), gss2.split(X, groups=groups2)):\n        assert_array_equal(train1, train2)\n        assert_array_equal(test1, test2)\n", "def test_time_series_split_with_gap():\n    X = np.arange(10)\n    y = np.arange(10)\n    n_splits = 3\n    max_train_size = 3\n\n    tss = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size)\n    splits = tss.split(X)\n\n    expected_train_splits = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    expected_test_splits = [[3], [6], [9]]\n\n    for i, (train, test) in enumerate(splits):\n        assert_array_equal(train, expected_train_splits[i])\n        assert_array_equal(test, expected_test_splits[i])\n", "def test_leave_p_out():\n    X = np.ones(10)\n    loo = LeaveOneOut()\n    lpo = LeavePOut(p=2)\n\n    # Make sure the repr works\n    assert_equal(repr(loo), 'LeaveOneOut()')\n    assert_equal(repr(lpo), 'LeavePOut(p=2)')\n\n    # Test that the length is correct\n    assert_equal(loo.get_n_splits(X), len(X))\n    assert_equal(lpo.get_n_splits(X), comb(len(X), 2))\n\n    for cv in (loo, lpo):\n        for train, test in cv.split(X):\n            # Check that train and test are disjoint\n            assert_array_equal(np.intersect1d(train, test), [])\n\n            # Check that train and test cover all the data\n            assert_equal(len(train) + len(test), len(X))\n\n    # Test that the splits are correct for LeavePOut\n    train, test = next(lpo.split(X))\n    assert_array_equal(train, [2, 3, 4, 5, 6, 7, 8, 9])\n    assert_array_equal(test, [0, 1])\n\n    # Test that the splits are correct for LeaveOneOut\n    train, test = next(loo.split(X))\n    assert_array_equal(train, [1, 2, 3, 4, 5, 6, 7, 8, 9])\n    assert_array_equal(test, [0])\n"], "sample_213": ["def test_file_exists(self):\n    \"\"\"\n    Test the existence of a file in the storage.\n    \"\"\"\n    self.assertFalse(self.storage.exists('test.file'))\n\n    f = ContentFile('custom contents')\n    f_name = self.storage.save('test.file', f)\n    self.addCleanup(self.storage.delete, f_name)\n\n    self.assertTrue(self.storage.exists('test.file'))\n\n    self.storage.delete('test.file')\n    self.assertFalse(self.storage.exists('test.file'))\n", "    def test_deconstruct_without_location(self):\n        storage = FileSystemStorage()\n        path, args, kwargs = storage.deconstruct()\n        self.assertEqual(path, \"django.core.files.storage.FileSystemStorage\")\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {})\n", "def test_file_get_available_name(self):\n    first = self.storage.get_available_name('custom_storage')\n    self.assertEqual(first, 'custom_storage')\n    second = self.storage.get_available_name('custom_storage')\n    self.assertNotEqual(second, 'custom_storage')\n", "def test_file_storage_deletion_on_close(self):\n    \"\"\"\n    Test that temporary files created by FileField are deleted when the file is closed.\n    \"\"\"\n    temp_file = TemporaryUploadedFile('test.txt', 'text/plain', 0, 'utf-8')\n    obj = Storage.objects.create(temporary=temp_file)\n    self.assertTrue(os.path.exists(obj.temporary.path))\n    obj.temporary.close()\n    self.assertFalse(os.path.exists(obj.temporary.path))\n", "def test_file_storage_exception_handling(self):\n    \"\"\"\n    File storage should handle exceptions properly during file operations.\n    \"\"\"\n    real_open = os.open\n\n    # Monkey-patch os.open, to simulate an IOError during file open.\n        raise IOError(\"Simulated IOError during file open\")\n\n    try:\n        os.open = fake_open\n\n        with self.assertRaises(IOError) as context:\n            self.storage.open('error.file')\n        self.assertEqual(str(context.exception), \"Simulated IOError during file open\")\n\n        # Save operation should also fail with IOError.\n        with self.assertRaises(IOError) as context:\n            self.storage.save('error.file', ContentFile('not saved'))\n        self.assertEqual(str(context.exception), \"Simulated IOError during file open\")\n    finally:\n        os.open = real_open\n"], "sample_345": ["def test_watch_dir_without_glob(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '')\n    with self.tick_twice():\n        self.increment_mtime(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "    def test_watch_dir_with_nonexistent_path(self):\n        path = Path('nonexistent_directory')\n        reloader = autoreload.BaseReloader()\n        reloader.watch_dir(path, '*.py')\n        self.assertEqual(list(reloader.directory_globs), [])\n", "def test_tick_triggers_on_file_creation(self, mock_notify_file_changed):\n    new_file = self.tempdir / 'new_file.py'\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[new_file]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        new_file.touch()\n        next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n", "def test_nonexistent_glob(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, 'nonexistent.py')\n    with self.tick_twice():\n        self.increment_mtime(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_nonexistent_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        self.increment_mtime(self.nonexistent_file)\n    self.assertEqual(notify_mock.call_count, 0)\n"], "sample_219": ["def test_combined_expression_group_by(self):\n    expr = F('a') + F('b')\n    self.assertEqual(expr.get_group_by_cols(alias=None), [expr])\n", "def test_expression_wrapper_with_none(self):\n    expr = ExpressionWrapper(Value(None), output_field=IntegerField())\n    self.assertIsNone(expr.resolve_expression(summarize=True).value)\n", "    def test_resolve_output_field(self):\n        expr = ExpressionWrapper(Value('test'), output_field=CharField())\n        self.assertIsInstance(expr._resolve_output_field(), CharField)\n", "def test_expression_wrapper_resolve_expression(self):\n    expr = ExpressionWrapper(Lower(F('name')), output_field=CharField())\n    query = Company.objects.all()\n    resolved_expr = expr.resolve_expression(query)\n    self.assertIsInstance(resolved_expr, ExpressionWrapper)\n    self.assertIsInstance(resolved_expr.expression, Lower)\n    self.assertIsInstance(resolved_expr.expression.expression, Col)\n    self.assertEqual(resolved_expr.expression.expression.target, query.query.alias_map['company'])\n", "def test_expression_wrapper_filtering(self):\n    expr = ExpressionWrapper(Value('test'), output_field=CharField())\n    qs = Company.objects.filter(name=expr)\n    self.assertEqual(str(qs.query), \"SELECT ... WHERE LOWER(expressions_company.name) = test\")\n"], "sample_1185": ["def test_decompogen_exp():\n    assert decompogen(exp(x**2 + 2*x + 1), x) == [exp(x), x**2 + 2*x + 1]\n", "def test_decompogen_exponential():\n    assert decompogen(exp(sin(x)), x) == [exp(x), sin(x)]\n    assert decompogen(exp(x)**2 + exp(x) + 1, x) == [x**2 + x + 1, exp(x)]\n    assert decompogen(exp(6*x**2 - 5), x) == [exp(x), 6*x**2 - 5]\n    assert decompogen(exp(sin(sqrt(cos(x**2 + 1)))), x) == [exp(x), sin(x), sqrt(x), cos(x), x**2 + 1]\n    assert decompogen(exp(cos(x)**2 + 3*cos(x) - 4), x) == [exp(x), x**2 + 3*x - 4, cos(x)]\n    assert decompogen(exp(sin(x)**2 + sin(x) - sqrt(3)/2), x) == [exp(x), x**2 + x - sqrt(3)/2, sin(x)]\n", "def test_compogen_with_polynomials():\n    assert compogen([x**2, x + 1], x) == (x + 1)**2\n", "def test_decompogen_exp():\n    assert decompogen(exp(sin(x**2 + 1)), x) == [exp(x), sin(x**2 + 1), x**2 + 1]\n", "def test_decompogen_min_max():\n    assert decompogen(Min(sin(x), cos(x)), x) == [Min(sin(x), cos(x))]\n    assert decompogen(Min(x, x**2, y), x) == [Min(x, x**2), y]\n    assert decompogen(Max(sin(x), cos(x)), x) == [Max(sin(x), cos(x))]\n    assert decompogen(Max(x, x**2, y), x) == [Max(x, x**2), y]\n    assert decompogen(Min(x, sin(x)), x) == [Min(x, sin(x))]\n    assert decompogen(Max(x, sin(x)), x) == [Max(x, sin(x))]\n"], "sample_1189": ["def test_lambdify_with_set_args():\n    with warns_deprecated_sympy():\n        f = lambdify({x, y}, x + y)\n        assert f(1, 2) == 3\n", "def test_issue_23827():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    f = lambdify(x, sin(x), 'numpy')\n    result = f(numpy.array([1]))\n    assert result == numpy.sin(1)\n    assert \"numpy\" in str(type(result))\n", "def test_issue_23119():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    expr = Array([x, x**2])\n    f = lambdify(x, expr, 'numpy')\n\n    # Test with a scalar input\n    scalar_result = f(2)\n    assert numpy.array_equal(scalar_result, numpy.array([2, 4]))\n\n    # Test with a numpy array input\n    array_result = f(numpy.array([2, 3]))\n    assert numpy.array_equal(array_result, numpy.array([[2, 4], [3, 9]]))\n", "def test_issue_23899():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    x = symbols(\"x\")\n    f = lambdify(x, floor(x))\n    assert f(numpy.array([1.5, 2.5])) == numpy.array([1, 2])\n", "def test_lambdify_numpy_c99_complex():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    expr = x + I*y\n    f = lambdify([x, y], expr, modules='numpy', c99_complex=True)\n    result = f(1, 2)\n    assert numpy.isclose(result, 1 + 2j)\n"], "sample_379": ["def test_mark_safe_decorator_with_args(self):\n    \"\"\"\n    mark_safe used as a decorator with arguments works as expected.\n    \"\"\"\n        return f'<{tag}>dummy</{tag}>'\n\n    decorated_func = mark_safe(clean_string_provider)\n    self.assertEqual(decorated_func('html'), '<html>dummy</html>')\n", "    def test_safe_string_concatenation(self):\n        \"\"\"\n        Concatenating a SafeString with a string results in a SafeString.\n        \"\"\"\n        s1 = mark_safe('a&b')\n        s2 = 'c&d'\n        result = s1 + s2\n        self.assertIsInstance(result, SafeString)\n        self.assertRenderEqual('{{ result }}', 'a&bc&d', result=result)\n", "def test_mark_safe_multiple_calls(self):\n    \"\"\"\n    Calling mark_safe multiple times on the same string returns the same object.\n    \"\"\"\n    s = 'a&b'\n    safe_s = mark_safe(s)\n    self.assertIs(mark_safe(s), safe_s)\n    self.assertIs(mark_safe(safe_s), safe_s)\n", "def test_mark_safe_with_special_characters(self):\n    s = mark_safe('<script>alert(\"XSS\");</script>')\n    self.assertRenderEqual('{{ s }}', '<script>alert(\"XSS\");</script>', s=s)\n    self.assertRenderEqual('{{ s|force_escape }}', '&lt;script&gt;alert(&quot;XSS&quot;);&lt;/script&gt;', s=s)\n", "def test_safe_string_subclassing(self):\n    class CustomSafeString(SafeString):\n        pass\n\n    s = CustomSafeString('a&b')\n    self.assertRenderEqual('{{ s }}', 'a&b', s=s)\n    self.assertRenderEqual('{{ s|force_escape }}', 'a&amp;b', s=s)\n"], "sample_167": ["def test_naturaltime_with_aware_datetime(self):\n    \"\"\"\n    Test naturaltime filter with aware datetime objects.\n    \"\"\"\n    test_list = [\n        now.replace(tzinfo=utc) + datetime.timedelta(days=1),\n        now.replace(tzinfo=utc) - datetime.timedelta(days=1),\n    ]\n    result_list = [\n        'tomorrow',\n        'yesterday',\n    ]\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'), self.settings(USE_TZ=True):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_naturaltime_with_future_date(self):\n    future_date = now + datetime.timedelta(days=2, hours=6)\n    expected_result = '2 days, 6 hours from now'\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            with override_settings(USE_TZ=True):\n                self.humanize_tester([future_date], [expected_result], 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_intword_large_numbers(self):\n    test_list = [\n        '1' + '0' * 36,  # 1 followed by 36 zeros\n        '1' + '0' * 60,  # 1 followed by 60 zeros\n        '1' + '0' * 72,  # 1 followed by 72 zeros\n    ]\n    result_list = [\n        '1.0 duodecillion',\n        '1.0 undecillion',\n        '1.0 duodecillion',  # As the code doesn't have a converter for 72, it falls back to 'duodecillion'\n    ]\n    with translation.override('en'):\n        self.humanize_tester(test_list, result_list, 'intword')\n", "def test_ordinal_invalid_input(self):\n    test_list = ('abc', None, True, [1, 2, 3], {'key': 'value'}, 1.23)\n    result_list = ('abc', None, True, [1, 2, 3], {'key': 'value'}, 1.23)\n    with translation.override('en'):\n        self.humanize_tester(test_list, result_list, 'ordinal')\n", "def test_naturaltime_i18n(self):\n    test_list = [\n        now,\n        now - datetime.timedelta(minutes=1),\n        now - datetime.timedelta(hours=1),\n        now - datetime.timedelta(days=1),\n        now - datetime.timedelta(days=500),\n        now + datetime.timedelta(minutes=1),\n        now + datetime.timedelta(hours=1),\n        now + datetime.timedelta(days=1),\n        now + datetime.timedelta(days=500),\n    ]\n    result_list = [\n        '\u00e0 l\\'instant',\n        'il y a une minute',\n        'il y a une heure',\n        'il y a un jour',\n        'il y a 1 an, 4 mois',\n        'dans une minute',\n        'dans une heure',\n        'dans un jour',\n        'dans 1 an, 4 mois',\n    ]\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('fr'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n"], "sample_421": ["def test_when_constructor_invalid_lookups(self):\n    msg = \"When() supports a Q object, a boolean expression, or lookups as a condition.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        When(string=\"1\", then=Value(True))\n", "def test_combined_expression_with_lookups(self):\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            test=Case(\n                When(integer__lt=2, then=F('integer') + 1),\n                When(integer__gt=2, then=F('integer') - 1),\n                default=Value(0),\n            )\n        ).order_by(\"pk\"),\n        [(1, 2), (2, 1), (3, 2), (2, 1), (3, 2), (3, 2), (4, 0)],\n        transform=attrgetter(\"integer\", \"test\"),\n    )\n", "def test_combined_expression_with_multiple_conditions(self):\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            test=Case(\n                When(integer=1, then=2),\n                When(integer=2, then=3),\n                When(integer=3, then=4),\n                default=5,\n            )\n            + Case(\n                When(integer2=1, then=6),\n                When(integer2=2, then=7),\n                When(integer2=3, then=8),\n                default=9,\n            ),\n        ).order_by(\"pk\"),\n        [(1, 8), (2, 10), (3, 12), (2, 10), (3, 12), (3, 12), (4, 14)],\n        transform=attrgetter(\"integer\", \"test\"),\n    )\n", "def test_combined_expression_with_uuid(self):\n    uuid_value = UUID('11111111-1111-1111-1111-111111111111')\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            test=Case(\n                When(integer=1, then=uuid_value),\n                When(integer=2, then=F('uuid')),\n                default=Value(UUID('00000000-0000-0000-0000-000000000000')),\n            ),\n        ).order_by('pk'),\n        [\n            (1, uuid_value),\n            (2, CaseTestModel.objects.get(integer=2).uuid),\n            (3, UUID('00000000-0000-0000-0000-000000000000')),\n            (2, CaseTestModel.objects.get(integer=2).uuid),\n            (3, UUID('00000000-0000-0000-0000-000000000000')),\n            (3, UUID('00000000-0000-0000-0000-000000000000')),\n            (4, UUID('00000000-0000-0000-0000-000000000000')),\n        ],\n        transform=attrgetter('integer', 'test'),\n    )\n", "def test_combined_expression_with_when(self):\n    self.assertQuerysetEqual(\n        CaseTestModel.objects.annotate(\n            test=Case(\n                When(integer=1, then=2),\n                When(integer=2, then=1),\n                default=3,\n            ) + 1,\n        ).order_by(\"pk\"),\n        [(1, 3), (2, 2), (3, 4), (2, 2), (3, 4), (3, 4), (4, 4)],\n        transform=attrgetter(\"integer\", \"test\"),\n    )\n"], "sample_849": ["def test_group_kfold_empty_trainset():\n    cv = GroupKFold(n_splits=2)\n    X, y = [[1]], [0]  # 1 sample\n    groups = [1]\n    with pytest.raises(\n            ValueError,\n            match='Cannot have number of splits n_splits=2 greater than the number of samples: n_samples=1.'):\n        next(cv.split(X, y, groups))\n", "def test_repeated_stratified_kfold_n_splits_consistency():\n    # Test that RepeatedStratifiedKFold returns the same number of splits\n    # when called multiple times with the same parameters\n    n_splits = 3\n    n_repeats = 4\n    rskf = RepeatedStratifiedKFold(n_splits, n_repeats)\n    assert rskf.get_n_splits() == rskf.get_n_splits()\n    assert rskf.get_n_splits() == n_splits * n_repeats\n", "def test_time_series_split_no_shuffle():\n    # Manually check that Time Series CV preserves the data ordering as much as possible\n    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]]\n    tscv = TimeSeriesSplit(2)\n    splits = tscv.split(X)\n\n    train, test = next(splits)\n    assert_array_equal(test, [3, 4])\n    assert_array_equal(train, [0, 1, 2])\n\n    train, test = next(splits)\n    assert_array_equal(test, [5, 6])\n    assert_array_equal(train, [0, 1, 2, 3, 4])\n", "def test_time_series_split_on_custom_dataset():\n    # Test TimeSeriesSplit on a custom dataset\n    X = np.arange(10).reshape(5, 2)\n    y = np.array([0, 0, 1, 1, 2])\n\n    tscv = TimeSeriesSplit(n_splits=3)\n    splits = tscv.split(X)\n\n    # Manually check the train-test splits for TimeSeriesSplit\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1])\n    assert_array_equal(test, [2])\n\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1, 2])\n    assert_array_equal(test, [3])\n\n    train, test = next(splits)\n    assert_array_equal(train, [0, 1, 2, 3])\n    assert_array_equal(test, [4])\n", "def test_predefined_split_with_kfold_split_with_different_folds():\n    # Check that PredefinedSplit can reproduce a split generated by Kfold with different folds\n    folds = np.full(10, -1.)\n    kf_train = []\n    kf_test = []\n    for i, (train_ind, test_ind) in enumerate(KFold(5, shuffle=True).split(X)):\n        folds[test_ind] = i\n    folds[5:] = folds[:5]  # Make some folds the same to test PredefinedSplit's handling of overlapping folds\n    ps = PredefinedSplit(folds)\n    # n_splits is simply the no of unique folds, but since some folds are the same, there should be less splits\n    assert len(np.unique(folds)) > ps.get_n_splits()\n    ps_train, ps_test = zip(*ps.split())\n    # We can't compare the train and test indices directly because the order of splits may be different\n    # So we compare the sets of train and test indices instead\n    assert set(map(frozenset, kf_train)) == set(map(frozenset, ps_train))\n    assert set(map(frozenset, kf_test)) == set(map(frozenset, ps_test))\n"], "sample_12": ["def test_angle_nan_operations():\n    \"\"\"\n    Test that operations with angles containing NaN values return NaN.\n    \"\"\"\n    a = Angle([1, np.nan, 3] * u.deg)\n    b = Angle([4, 5, np.nan] * u.deg)\n\n    result = a + b\n    assert np.isnan(result[1])\n    assert np.isnan(result[2])\n\n    result = a - b\n    assert np.isnan(result[1])\n    assert np.isnan(result[2])\n\n    result = a * 2\n    assert np.isnan(result[1])\n\n    result = a / 2\n    assert np.isnan(result[1])\n\n    result = a * b\n    assert np.isnan(result[1])\n    assert np.isnan(result[2])\n\n    result = a / b\n    assert np.isnan(result[1])\n    assert np.isnan(result[2])\n", "def test_angle_with_none_unit():\n    \"\"\"\n    Test that an Angle can be created with None as unit and it defaults to degrees\n    \"\"\"\n    a = Angle(\"54.12412\")\n    assert_allclose(a.degree, 54.12412)\n    assert a.unit == u.degree\n", "def test_angle_add_subtract():\n    \"\"\"\n    Tests addition and subtraction of Angle objects\n    \"\"\"\n\n    a1 = Angle(3.60827466667, unit=u.hour)\n    a2 = Angle(\"54:07:26.832\", unit=u.degree)\n    a3 = a1 + a2\n    a4 = a1 - a2\n\n    assert isinstance(a3, Angle)\n    assert isinstance(a4, Angle)\n    assert a3.unit is u.degree\n    assert a4.unit is u.degree\n\n    assert_allclose(a3.degree, 360.92412, atol=1e-5)\n    assert_allclose(a4.degree, 359.07588, atol=1e-5)\n", "def test_angle_to_string_format():\n    \"\"\"\n    Test the format parameter of to_string() function for Angle objects\n    \"\"\"\n\n    angle = Angle(\"54.12412\", unit=u.degree)\n\n    # Test the 'latex' format\n    latex_format = angle.to_string(format='latex')\n    assert latex_format == r\"$54^\\circ07{}^\\prime26.832{}^{\\prime\\prime}$\"\n\n    # Test the 'unicode' format\n    unicode_format = angle.to_string(format='unicode')\n    assert unicode_format == \"54\u00b007\u203226.832\u2033\"\n\n    # Test the 'latex_inline' format\n    latex_inline_format = angle.to_string(format='latex_inline')\n    assert latex_inline_format == r\"$54^\\circ07{}^\\prime26.832{}^{\\prime\\prime}$\"\n\n    # Test the default format\n    default_format = angle.to_string()\n    assert default_format == \"54d07m26.832s\"\n\n    # Test an unknown format\n    with pytest.raises(ValueError):\n        angle.to_string(format='unknown')\n", "def test_angle_units_conversion():\n    \"\"\"\n    Tests unit conversion of Angle objects to different angular units\n    \"\"\"\n\n    angle = Angle(\"54.12412\", unit=u.degree)\n\n    assert_allclose(angle.to(u.arcminute).value, 3247.446)\n    assert_allclose(angle.to(u.arcsecond).value, 194846.76)\n    assert_allclose(angle.to(u.mas).value, 194846760.0)\n    assert_allclose(angle.to(u.uas).value, 194846760000.0)\n    assert_allclose(angle.to(u.radian).value, 0.944644098745)\n\n    angle = Angle(\"3.60827466667\", unit=u.hour)\n\n    assert_allclose(angle.to(u.arcminute).value, 216.49648)\n    assert_allclose(angle.to(u.arcsecond).value, 13009.7888)\n    assert_allclose(angle.to(u.mas).value, 13009788.8)\n    assert_allclose(angle.to(u.uas).value, 13009788800.0)\n    assert_allclose(angle.to(u.degree).value, 54.12412)\n\n    angle = Angle(\"0.944644098745\", unit=u.radian)\n\n    assert_allclose(angle.to(u.arcminute).value, 3247.446)\n    assert_allclose(angle.to(u.arcsecond).value, 194846.76)\n    assert_allclose(angle.to(u.mas).value, 194846760.0)\n    assert_allclose"], "sample_523": ["def test_legend_loc_out_of_range():\n    # Test that an error is raised when the loc argument is out of range\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    with pytest.raises(ValueError):\n        ax.legend(loc=12)\n", "def test_legend_single_ncol_ncols():\n    # Test that ncol and ncols cannot be used together\n    strings = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n    ncols = 3\n    with pytest.raises(TypeError):\n        plt.legend(strings, ncol=ncols, ncols=ncols)\n", "def test_legend_set_ncols():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n    leg.set_ncols(2)\n    assert leg._ncols == 2\n", "def test_legend_title_fontprop_fontname():\n    # test the title_fontproperties kwarg with fontname\n    plt.plot(range(10))\n    leg = plt.legend(title='Aardvark', title_fontproperties={'family': 'serif', 'fontname': 'Times New Roman'})\n    assert leg.get_title().get_fontname() == 'Times New Roman'\n", "def test_legend_face_edgecolor_inherit(facecolor, edgecolor):\n    fig, ax = plt.subplots()\n    ax.fill_between([0, 1, 2], [1, 2, 3], [2, 3, 4],\n                    facecolor=facecolor, edgecolor=edgecolor, label='Fill')\n    leg = ax.legend()\n    if facecolor == 'inherit':\n        assert leg.get_frame().get_facecolor() == ax.get_facecolor()\n    else:\n        assert leg.get_frame().get_facecolor() == facecolor\n    if edgecolor == 'inherit':\n        assert leg.get_frame().get_edgecolor() == ax.get_edgecolor()\n    else:\n        assert leg.get_frame().get_edgecolor() == edgecolor\n"], "sample_68": ["    def test_sensitive_settings_in_list(self):\n        \"\"\"\n        The debug page should filter out some sensitive information found in\n        list settings.\n        \"\"\"\n        sensitive_settings = [\n            'SECRET_KEY',\n            'PASSWORD',\n            'API_KEY',\n            'AUTH_TOKEN',\n        ]\n        for setting in sensitive_settings:\n            FOOBAR = [\n                setting,\n                {'recursive': setting},\n            ]\n            with self.settings(DEBUG=True, FOOBAR=FOOBAR):\n                response = self.client.get('/raises500/')\n                self.assertNotContains(response, setting, status_code=500)\n", "def test_template_loader_postmortem_multiple_loaders(self):\n    \"\"\"Tests for not existing file with multiple loaders\"\"\"\n    template_name = \"notfound.html\"\n    with tempfile.NamedTemporaryFile(prefix=template_name) as tmpfile:\n        tempdir = os.path.dirname(tmpfile.name)\n        template_path = os.path.join(tempdir, template_name)\n        with override_settings(TEMPLATES=[\n            {\n                'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                'DIRS': [tempfile.mkdtemp(prefix='other_')],\n            },\n            {\n                'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                'DIRS': [tempdir],\n            },\n        ]):\n            with self.assertLogs('django.request', 'ERROR'):\n                response = self.client.get(reverse('raises_template_does_not_exist', kwargs={\"path\": template_name}))\n        self.assertContains(response, \"%s (Source does not exist)\" % template_path, status_code=500, count=2)\n        # Assert as HTML.\n        self.assertContains(\n            response,\n            '<li><code>django.template.loaders.filesystem.Loader</code>: '\n            '%s (Source does not exist)</li>' % os.path.join(tempdir, 'notfound.html'),\n            status_code=500,\n            html=True,\n        )\n", "def test_multivalue_dict_key_error_with_sensitive_keys(self):\n    \"\"\"\n    #21098 -- Sensitive POST parameters cannot be seen in the\n    error reports for if request.POST['nonexistent_key'] throws an error\n    even if the key contains sensitive information.\n    \"\"\"\n    request = self.rf.post('/multivalue_dict_key_error/', {'SECRET_KEY': 'sensitive_value'})\n    with self.assertLogs('django.request', 'ERROR'):\n        response = sensitive_view(request)\n    self.assertNotContains(response, 'sensitive_value', status_code=500)\n", "    def test_request_without_sensitive_post_parameters(self):\n        request = self.rf.post('/some_url/', {'key': 'value'})\n        try:\n            raise Exception('Error')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        filtered_post = reporter.filter.get_post_parameters(request)\n        self.assertEqual(filtered_post, {'key': 'value'})\n", "    def test_multivalue_dict_in_frame_vars(self):\n        \"\"\"\n        MultiValueDict instances should be handled correctly in frame variables.\n        \"\"\"\n        request = self.rf.post('/some_url/', self.breakfast_data)\n        request.my_multivalue_dict = MultiValueDict(self.breakfast_data)\n        try:\n            raise Exception(\"Test exception\")\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        text = reporter.get_traceback_text()\n        # Check that the MultiValueDict instance is displayed correctly\n        self.assertIn(\"<MultiValueDict: {'sausage-key': ['sausage-value'], 'baked-beans-key': ['baked-beans-value'], 'hash-brown-key': ['hash-brown-value'], 'bacon-key': ['bacon-value']}>\", text)\n        # Check that the sensitive POST parameters are not displayed\n        self.assertNotIn(\"sausage-value\", text)\n        self.assertNotIn(\"bacon-value\", text)\n"], "sample_90": ["def test_many_to_many_empty(self):\n    \"\"\"Data for a ManyToManyField with no entries is an empty list.\"\"\"\n    item = ColourfulItem.objects.create()\n    data = model_to_dict(item)['colours']\n    self.assertEqual(data, [])\n", "    def test_date_field(self):\n        \"\"\"Data for a DateField is a datetime.date object rather than a string.\"\"\"\n        event = Event.objects.create(name='Test Event', event_date=datetime.date(2022, 1, 1))\n        data = model_to_dict(event)['event_date']\n        self.assertEqual(data, datetime.date(2022, 1, 1))\n", "def test_many_to_many_instance(self):\n    \"\"\"model_to_dict() returns a list for a ManyToManyField instance.\"\"\"\n    blue = Colour.objects.create(name='blue')\n    red = Colour.objects.create(name='red')\n    item = ColourfulItem.objects.create()\n    item.colours.set([blue, red])\n    data = model_to_dict(item, fields=['colours'])['colours']\n    self.assertEqual(data, [blue, red])\n", "    def test_custom_field_subclass(self):\n        \"\"\"\n        A custom field subclass should be able to be used in a model form.\n        \"\"\"\n        class CustomCharField(forms.CharField):\n            pass\n\n        class CustomModel(models.Model):\n            custom_field = models.CharField(max_length=20)\n\n        class CustomModelForm(forms.ModelForm):\n            custom_field = CustomCharField()\n\n            class Meta:\n                model = CustomModel\n                fields = '__all__'\n\n        form = CustomModelForm()\n        self.assertIsInstance(form.fields['custom_field'], CustomCharField)\n", "def test_model_to_dict_exclude(self):\n    blue = Colour.objects.create(name='blue')\n    red = Colour.objects.create(name='red')\n    item = ColourfulItem.objects.create(name='test item')\n    item.colours.set([blue, red])\n    data = model_to_dict(item, exclude=['colours'])\n    self.assertEqual(data, {'id': item.id, 'name': 'test item'})\n"], "sample_381": ["def test_add_model_with_field_removed_from_base_model_with_same_name(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name. The new field should not be removed.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=300)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel', 'AddField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n    self.assertOperationAttributes(changes, 'app', 0, 2, name='title', model_name='book')\n", "def test_add_index_with_nullable_field(self):\n    model_state = ModelState('app', 'model', [\n        ('id', models.AutoField(primary_key=True)),\n        ('nullable_field', models.CharField(max_length=200, null=True)),\n    ])\n    changes = self.get_changes([], [model_state])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n\n    model_state_with_index = ModelState('app', 'model', [\n        ('id', models.AutoField(primary_key=True)),\n        ('nullable_field', models.CharField(max_length=200, null=True)),\n    ], options={'indexes': [models.Index(fields=['nullable_field'])]})\n    changes = self.get_changes([model_state], [model_state_with_index])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AddIndex'])\n", "    def test_add_field_with_custom_index(self):\n        initial_model = ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        changed_model = ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200, db_index=True, index=models.Index(name='custom_index'))),\n        ])\n        changes = self.get_changes([initial_model], [changed_model])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name')\n        self.assertEqual(changes['app'][0].operations[0].field.db_index, True)\n        self.assertEqual(changes['app'][0].operations[0].field.index.name, 'custom_index')\n", "compilation error", "    def test_add_field_to_model_with_base_class(self):\n        \"\"\"\n        Adding a field to a model with a base class should be handled\n        correctly, with the new field being added to the subclass.\n        \"\"\"\n        base_model = ModelState('app', 'BaseModel', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        derived_model = ModelState('app', 'DerivedModel', [\n            ('base_ptr', models.OneToOneField('app.BaseModel', models.CASCADE, parent_link=True)),\n        ], bases=('app.BaseModel',))\n        changes = self.get_changes(\n            [base_model, derived_model],\n            [base_model, ModelState('app', 'DerivedModel', [\n                ('base_ptr', models.OneToOneField('app.BaseModel', models.CASCADE, parent_link=True)),\n                ('new_field', models.CharField(max_length=100)),\n            ], bases=('app.BaseModel',))],\n        )\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='new_field', model_name='derivedmodel')\n"], "sample_373": ["def test_get_return_data_type(self):\n    \"\"\"\n    Test the get_return_data_type function.\n    \"\"\"\n    self.assertEqual(get_return_data_type('get_status'), 'Integer')\n    self.assertEqual(get_return_data_type('get_groups_list'), 'List')\n    self.assertEqual(get_return_data_type('get_name'), '')\n", "def test_model_detail_non_existent_field(self):\n    with self.assertRaisesMessage(AttributeError, \"'str' object has no attribute 'description'\"):\n        views.get_readable_field_data_type(\"NotAField\")\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_extract_views_from_urlpatterns(self):\n            pass\n\n        urlpatterns = [\n            path('admin/', admin.site.urls),\n            path('admindocs/', include('django.contrib.admindocs.urls')),\n            path('dummy/', dummy_view, name='dummy'),\n        ]\n\n        views = views.extract_views_from_urlpatterns(urlpatterns)\n        self.assertEqual(len(views), 2)\n        self.assertIn((dummy_view, '/dummy/', None, 'dummy'), views)\n        self.assertIn((views.BookmarkletsView.as_view(), '/admindocs/bookmarklets/', None, 'django-admindocs-bookmarklets'), views)\n", "    def test_simplify_regex_with_special_characters(self):\n        tests = (\n            (r'^a\\.\\+b', '/a.+b'),\n            (r'^a\\*b', '/a*b'),\n            (r'^a\\?b', '/a?b'),\n            (r'^a\\{b}', '/a{b}'),\n            (r'^a\\[b\\]', '/a[b]'),\n            (r'^a\\|b', '/a|b'),\n            (r'^a\\(b', '/a(b'),\n            (r'^a\\)b', '/a)b'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n"], "sample_261": ["    def test_parse_datetime_invalid_tzinfo(self):\n        # Invalid timezone inputs\n        self.assertIsNone(parse_datetime('2012-04-23T09:15:00X'))\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T09:15:00+24')\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T09:15:00-24')\n", "    def test_parse_datetime_with_microseconds(self):\n        valid_inputs = (\n            ('2012-04-23T10:20:30.400Z', datetime(2012, 4, 23, 10, 20, 30, 400000, get_fixed_timezone(0))),\n            ('2012-04-23T10:20:30.400-03', datetime(2012, 4, 23, 10, 20, 30, 400000, get_fixed_timezone(-180))),\n        )\n        for source, expected in valid_inputs:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n\n        # Invalid inputs\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T10:20:30.400000Z')\n", "    def test_parse_datetime_invalid_tzinfo(self):\n        # Invalid timezone information\n        invalid_tzinfo_inputs = (\n            '2012-04-23T09:15:00X',  # Non-numeric timezone info\n            '2012-04-23T09:15:00+2360',  # Out of range timezone info\n        )\n        for source in invalid_tzinfo_inputs:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_datetime(source)\n", "    def test_parse_datetime_invalid_inputs(self):\n        invalid_inputs = (\n            '2012-04-56T09:15:90',\n            '2012-04-23T25:15:00',\n            '2012-04-23T09:60:00',\n            '2012-04-23T09:15:60',\n            '2012-04-23T09:15:00-99:00',\n            '2012-04-23T09:15:00+99:00',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_datetime(source)\n", "def test_invalid_inputs(self):\n    # Invalid inputs\n    invalid_inputs = ['2012-04-56', '09:15:90', '20120423091500', '2012-04-56T09:15:90', '-1 day -0:00:60', 'P1Y', 'PT0.0000001S']\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n"], "sample_306": ["    def test_parse_datetime_invalid_timezones(self):\n        invalid_inputs = (\n            '2012-04-23T09:15:00+24:00',\n            '2012-04-23T09:15:00-24:01',\n            '2012-04-23T09:15:00+90:00',\n            '2012-04-23T09:15:00-90:00',\n            '2012-04-23T09:15:00+00:60',\n            '2012-04-23T09:15:00-00:60',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                with self.assertRaises(ValueError):\n                    parse_datetime(source)\n", "    def test_parse_invalid_datetime(self):\n        # Test invalid datetime formats\n        invalid_inputs = (\n            '2012-04-31T09:15:00',  # Invalid date\n            '2012-13-01T09:15:00',  # Invalid month\n            '2012-04-23T25:15:00',  # Invalid hour\n            '2012-04-23T09:60:00',  # Invalid minute\n            '2012-04-23T09:15:60',  # Invalid second\n            '2012-04-23T09:15:00+25',  # Invalid timezone hour\n            '2012-04-23T09:15:00+00:60',  # Invalid timezone minute\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n", "def test_invalid_datetime(self):\n    # Invalid inputs with offsets\n    invalid_inputs = (\n        '2012-04-23T10:20:30.400+',\n        '2012-04-23T10:20:30.400+02:',\n        '2012-04-23T10:20:30.400+02:3',\n        '2012-04-23T10:20:30.400+02:300',\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            with self.assertRaises(ValueError):\n                parse_datetime(source)\n", "    def test_parse_datetime_with_microseconds(self):\n        # Valid inputs with microseconds\n        valid_inputs = (\n            ('2012-04-23T10:20:30.123456+02:30', datetime(2012, 4, 23, 10, 20, 30, 123456, get_fixed_timezone(150))),\n            ('2012-04-23T10:20:30,123456-02', datetime(2012, 4, 23, 10, 20, 30, 123456, get_fixed_timezone(-120))),\n        )\n        for source, expected in valid_inputs:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n\n        # Invalid inputs with too many microseconds\n        invalid_inputs = (\n            '2012-04-23T10:20:30.1234567+02:30',\n            '2012-04-23T10:20:30,1234567-02',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n", "    def test_invalid_inputs(self):\n        invalid_inputs = (\n            '2012-04-23T09:15:00 ',  # Trailing whitespace\n            '2012-04-23T09:15:00X',  # Invalid timezone designator\n            '2012-04-23T09:15:00+',  # Incomplete timezone offset\n            '2012-04-23T09:15:00-',  # Incomplete timezone offset\n            '2012-04-23T09:15:00+0',  # Incomplete timezone offset\n            '2012-04-23T09:15:00-0',  # Incomplete timezone offset\n            '2012-04-23T09:15:00+000',  # Overflowing timezone offset\n            '2012-04-23T09:15:00-000',  # Overflowing timezone offset\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n"], "sample_1103": ["def test_div_by_zero():\n    with raises(ZeroDivisionError):\n        x / 0\n", "def test_div():\n    eq = a/b\n    assert eq == a*b**(-1)\n    eq = a/b + c/2\n    assert eq == a*b**(-1) + Rational(1)/2*c\n    eq = (1 - b)/(b - 1)\n    assert eq == (1 + -b)*((-1) + b)**(-1)\n", "def test__rfloordiv__():\n    assert 10 // x == floor(10 / x)\n    assert 10.0 // x == floor(10.0 / x)\n    assert 10 // y == floor(10 / y)\n    assert 10.0 // y == floor(10.0 / y)\n", "def test_as_coeff_Mul():\n    assert (x * 2).as_coeff_Mul() == (2, x)\n    assert (2 * x).as_coeff_Mul() == (2, x)\n    assert (x * y).as_coeff_Mul() == (1, x * y)\n    assert (x * y * 2).as_coeff_Mul() == (2, x * y)\n    assert (2 * x * y).as_coeff_Mul() == (2, x * y)\n", "def test_div_zero():\n    x = Symbol('x')\n    with raises(ZeroDivisionError):\n        _ = x / 0\n    with raises(ZeroDivisionError):\n        _ = x / (1 - 1)\n    with raises(ZeroDivisionError):\n        _ = 0 / x\n    with raises(ZeroDivisionError):\n        _ = (1 - 1) / x\n"], "sample_411": ["def test_command_error_traceback(self):\n    \"\"\"\n    Exception raised in a command should raise CommandError with\n    traceback if --traceback option is passed.\n    \"\"\"\n    with self.assertRaises(CommandError) as cm:\n        management.call_command(\"dance\", example=\"raise\", traceback=True)\n    self.assertEqual(cm.exception.returncode, 3)\n    self.assertIn(\"Traceback (most recent call last):\", str(cm.exception))\n", "def test_call_command_with_verbose(self):\n    \"\"\"\n    Test that verbose option increases verbosity level.\n    \"\"\"\n    out = StringIO()\n    management.call_command(\"dance\", verbosity=2, stdout=out)\n    self.assertIn(\"Verbose output\", out.getvalue())\n", "def test_command_style_invalid(self):\n    \"\"\"Testing command style with both no_color and force_color should raise a CommandError\"\"\"\n    msg = \"'no_color' and 'force_color' can't be used together.\"\n    with self.assertRaisesMessage(CommandError, msg):\n        management.call_command(\"dance\", no_color=True, force_color=True)\n", "def test_command_error_returncode(self):\n    \"\"\"\n    CommandError can have a custom returncode.\n    \"\"\"\n    try:\n        management.call_command(\"custom_returncode\")\n    except CommandError as e:\n        self.assertEqual(e.returncode, 42)\n    else:\n        self.fail(\"CommandError was not raised\")\n", "def test_call_command_with_parameters_and_app_labels_intermixed(self):\n    out = StringIO()\n    management.call_command(\"hal\", \"--verbosity\", \"3\", \"myapp\", \"--empty\", stdout=out)\n    self.assertIn(\"Dave, my mind is going. I can feel it. I can feel it.\\n\", out.getvalue())\n    self.assertIn(\"\\nDave, I can't do that.\\n\", out.getvalue())\n"], "sample_1168": ["def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_permutations():\n    assert list(permutations([1, 2, 3], 0)) == [()]\n    assert list(permutations([1, 2, 3], 1)) == [(1,), (2,), (3,)]\n    assert list(permutations([1, 2, 3], 2)) == [(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)]\n    assert list(permutations([1, 2, 3], 3)) == [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n    assert list(permutations([1, 2, 3], 4)) == []\n    assert list(permutations([1, 2, 3], repetition=True, length=0)) == [()]\n    assert list(permutations([1, 2, 3], repetition=True, length=1)) == [(1,), (2,), (3,)]\n    assert list(permutations([1, 2, 3], repetition=True, length=2)) == [(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)]\n    assert list(permutations([1, 2], repetition=False)) == [(1, 2), (2, 1)]\n    assert list(permutations([1, 2], repetition=True)) == [(1, 1), (1, 2), (2, 1), (2, 2)]\n", "def test_has_variety():\n    assert has_variety((1, 2, 1))\n    assert not has_variety((1, 1, 1))\n"], "sample_809": ["def test_mutual_info_regression_copy():\n    # Test that the input data is not modified when copy=True.\n    X = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n    y = np.array([0.1, 0.2, 0.3])\n    X_copy = X.copy()\n    y_copy = y.copy()\n\n    mutual_info_regression(X, y, copy=True, random_state=0)\n    assert_array_equal(X, X_copy)\n    assert_array_equal(y, y_copy)\n\n    # Test that the input data is modified when copy=False.\n    mutual_info_regression(X, y, copy=False, random_state=0)\n    assert not np.allclose(X, X_copy)\n    assert not np.allclose(y, y_copy)\n", "def test_mutual_info_regression_sparse():\n    # Test that sparse matrix input raises an error when discrete_features is False.\n    X = csr_matrix([[0, 0, 0], [1, 1, 0], [2, 0, 1], [2, 0, 1], [2, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n\n    assert_raises(ValueError, mutual_info_regression, X, y, discrete_features=False)\n", "def test_mutual_info_classif_constant():\n    # Test that constant feature gets zero mutual information.\n    n_samples = 100\n    X = np.zeros((n_samples, 1))\n    y = np.random.randint(0, 2, size=n_samples)\n\n    mi = mutual_info_classif(X, y, discrete_features=[True], random_state=0)\n    assert_equal(mi, np.array([0]))\n", "def test_mutual_info_regression_sparse():\n    # Test the function with sparse input\n    X = csr_matrix([[0, 0, 0],\n                    [1, 1, 0],\n                    [2, 0, 1],\n                    [2, 0, 1],\n                    [2, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n    assert_raises(ValueError, mutual_info_regression, X, y, discrete_features=False)\n", "def test_mutual_info_regression_sparse_input():\n    # Test that sparse matrix with continuous features raises an error.\n    X = csr_matrix([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    assert_raises(ValueError, mutual_info_regression, X, y)\n"], "sample_42": ["def test_with_H0_default():\n    cosmodist = cosmology.default_cosmology.get().H0.value * u.Mpc/u.littleh\n    assert_quantity_allclose(cosmodist.to(u.Mpc, u.with_H0()), 100*u.Mpc)\n", "def test_with_H0_default():\n    cosmodist = cosmology.default_cosmology.get().H0.value * u.Mpc/u.littleh\n    assert_quantity_allclose(cosmodist.to(u.Mpc, u.with_H0()), 100*u.Mpc)\n", "def test_with_H0_quantity():\n    H0 = 70 * u.km / u.s / u.Mpc\n    h70dist = 50 * u.Mpc / u.littleh\n\n    assert_quantity_allclose(h70dist.to(u.Mpc, u.with_H0(H0)), 350 * u.Mpc)\n\n    # Now try a mass scaling\n    h1mass = 10 * u.Msun / u.littleh\n    assert_quantity_allclose(h1mass.to(u.Msun, u.with_H0(H0)), 70 * u.Msun)\n", "def test_with_H0_errors():\n    # Test with invalid H0 value\n    with pytest.raises(u.UnitsError):\n        u.Mpc.to(u.pc, u.with_H0(100))\n\n    # Test with non-Quantity H0 value\n    with pytest.raises(u.UnitsError):\n        u.Mpc.to(u.pc, u.with_H0('invalid'))\n", "def test_with_H0_default():\n    \"\"\"Test with_H0() using the default cosmology.\"\"\"\n    cosmodist = cosmology.default_cosmology.get().H0.value * u.Mpc/u.littleh\n    assert_quantity_allclose(cosmodist.to(u.Mpc, u.with_H0()), 100*u.Mpc)\n\n    # Now try a luminosity scaling\n    h1lum = .49 * u.Lsun * u.littleh**-2\n    assert_quantity_allclose(h1lum.to(u.Lsun, u.with_H0()), 1*u.Lsun)\n\n    # And the trickiest one: magnitudes. Using H0=10 here for the round numbers\n    # assume the \"true\" magnitude M = 12.\n    # Then M - 5*log_10(h)  = M + 5 = 17\n    withlittlehmag = 17 * (u.mag - u.MagUnit(u.littleh**2))\n    assert_quantity_allclose(withlittlehmag.to(u.mag, u.with_H0()), 12*u.mag)\n"], "sample_210": ["    def test_template_name_attribute(self):\n        \"\"\"A generic template view can be customized to use a specific template.\"\"\"\n        response = self.client.get('/template/named/')\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '<h1>Named Template</h1>')\n", "def test_template_params_filtering_with_exception(self):\n    class ArtistView(TemplateView):\n        template_name = 'generic_views/about.html'\n\n            context = super().get_context_data(**kwargs)\n            artist = Artist.objects.get(name=artist_name)\n            return {**context, 'artist': artist}\n\n    response = ArtistView.as_view()(self.rf.get('/'), artist_name='Non Existent Artist')\n    with self.assertRaises(Artist.DoesNotExist):\n        str(response.context['artist'])\n", "    def test_use_object_from_view(self):\n        test_view = views.CustomSingleObjectMixinView()\n        test_view.get(self.rf.get('/'))\n        # Don't pass object as argument\n        context = test_view.get_context_data()\n        self.assertEqual(context['object'], test_view.object)\n", "def test_template_params_with_model(self):\n    \"\"\"A generic template view with a model passes model instance to the context.\"\"\"\n    artist = Artist.objects.create(name='Vincent Van Gogh')\n    response = self.client.get(f'/template/model/{artist.pk}/')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.context['artist'], artist)\n", "    def test_query_string_with_no_args(self):\n        response = RedirectView.as_view(url='/bar/')(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/')\n\n        response = RedirectView.as_view(url='/bar/', query_string=True)(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/')\n"], "sample_800": ["def test_check_fit_non_negative():\n    # Check that proper warning is raised for non-negative X\n    # when tag requires_positive_X is present\n    X = np.array([[-1., 1], [-1., 1]])\n    y = np.array([1, 2])\n    estimator = RequiresPositiveYRegressor()\n    assert_raises_regex(ValueError, \"Negative values in data passed to\",\n                        estimator.fit, X, y)\n", "def test_check_classifiers_regression_target():\n    # Check if classifier throws an exception when fed regression targets\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.datasets import load_boston\n    X, y = load_boston(return_X_y=True)\n    e = LogisticRegression()\n    msg = 'Unknown label type: '\n    assert_raises_regex(ValueError, msg, e.fit, X, y)\n", "def test_check_estimator_on_regressor():\n    # check that check_estimator() works on a regressor\n    est = LinearRegression()\n    check_estimator(est)\n", "def test_check_fit_score_takes_y_with_svc():\n    # Tests that check_fit_score_takes_y works with SVC\n    check_fit_score_takes_y(\"SVC\", SVC())\n", "def test_check_estimator_set_params():\n    # check that set_params does not modify the estimator in place\n    estimator = LogisticRegression(C=1.0)\n    estimator_copy = clone(estimator)\n    estimator.set_params(C=2.0)\n    assert estimator.C == 2.0\n    assert estimator_copy.C == 1.0\n"], "sample_652": ["def test_call_fixture_function_error_with_request():\n    \"\"\"Check if an error is raised if a fixture function is called directly with a request object (#4545)\"\"\"\n\n    @pytest.fixture\n        return 1\n\n    request = pytest.FixtureRequest(None)\n\n    with pytest.raises(pytest.fail.Exception):\n        assert fix(request) == 1\n", "def test_request_addfinalizer_exception_suppressed(testdir):\n    \"\"\"\n    Ensure exceptions raised during teardown by a finalizer are suppressed\n    until all finalizers are called, re-raising the first exception (#2440)\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        values = []\n            raise Exception('Error in %s fixture' % where)\n        @pytest.fixture\n            return request\n        @pytest.fixture\n            subrequest.addfinalizer(lambda: values.append(1))\n            subrequest.addfinalizer(lambda: values.append(2))\n            subrequest.addfinalizer(lambda: _excepts('something'))\n        @pytest.fixture\n            subrequest.addfinalizer(lambda: _excepts('excepts'))\n            subrequest.addfinalizer(lambda: values.append(3))\n            pass\n            assert values == [3, 2, 1]\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*Exception: Error in excepts fixture\", \"* 2 passed, 1 error in *\"]\n    )\n", "def test_call_fixture_function_outside_fixture_error():\n    \"\"\"Check if an error is raised if a fixture function is called outside a fixture (#4545)\"\"\"\n\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(RuntimeError, match=\"Fixture function 'fix' cannot be called directly.\"):\n        assert fix() == 1\n", "def test_fixture_exception(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            raise ValueError(\"fixture error\")\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"test_fixture_exception.py::test_fail FAILED\",\n            \"*ValueError: fixture error*\",\n            \"1 failed in *\",\n        ]\n    )\n    assert result.ret == 1\n", "def test_request_clean_after_parametrized_fixtures(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=[1, 2])\n            request.addfinalizer(lambda: values.append(request.param))\n            return request.param\n\n        values = []\n            pass\n\n            assert not hasattr(values, 'request')\n        \"\"\"\n    )\n    reprec = testdir.inline_run(\"-s\")\n    assert not hasattr(reprec.getcalls(\"pytest_runtest_call\")[0].item.module, 'request')\n"], "sample_862": ["def test_unused_parameters_no_warn(Vectorizer):\n    train_data = JUNK_FOOD_DOCS\n    # setting parameter but expecting no warning messages\n    vect = Vectorizer(analyzer='word')\n    with pytest.warns(None) as record:\n        vect.fit(train_data)\n    assert len(record) == 0\n", "def test_vectorizer_stop_words_list_of_strings():\n    stop_words = ['and', 'the']\n    vec = CountVectorizer(stop_words=stop_words)\n    assert vec.get_stop_words() == set(stop_words)\n", "def test_callable_analyzer_preserve_order(Vectorizer):\n    data = ['apple orange banana', 'banana apple orange']\n    analyzer = lambda doc: sorted(doc.split())\n    vect = Vectorizer(analyzer=analyzer)\n    X = vect.fit_transform(data)\n    assert vect.get_feature_names() == ['apple', 'banana', 'orange']\n    assert X.toarray().tolist() == [[1, 1, 1], [1, 1, 1]]\n", "def test_callable_analyzer_with_different_analyzers(Estimator, analyzer):\n        return doc.split()\n\n    data = ['this is text']\n    X = Estimator(analyzer=custom_analyzer).fit_transform(data)\n    X_default = Estimator(analyzer=analyzer).fit_transform(data)\n    assert X.shape == X_default.shape\n    assert X.sum() == X_default.sum()\n", "def test_vectorizer_parameters(Vectorizer, max_df, min_df, max_features, vocabulary):\n    vectorizer = Vectorizer(max_df=max_df, min_df=min_df, max_features=max_features, vocabulary=vocabulary)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert X.shape[1] == len(vectorizer.vocabulary_)\n"], "sample_729": ["def test_enet_sparse_input():\n    # Test ElasticNet with sparse input\n    X, y, X_test, y_test = build_dataset()\n    X_sparse = sparse.csr_matrix(X)\n    clf = ElasticNet(alpha=0.5, l1_ratio=0.3, max_iter=100, precompute=False)\n    clf.fit(X_sparse, y)\n    pred = clf.predict(X_test)\n    assert_array_almost_equal(pred, clf.predict(X_test), decimal=3)\n", "def test_lasso_float_precision():\n    # Generate dataset\n    X, y, X_test, y_test = build_dataset(n_samples=20, n_features=10)\n    # Here we have a small number of iterations, and thus the\n    # Lasso might not converge. This is to speed up tests\n\n    for normalize in [True, False]:\n        for fit_intercept in [True, False]:\n            coef = {}\n            intercept = {}\n            for dtype in [np.float64, np.float32]:\n                clf = Lasso(alpha=0.5, max_iter=100, precompute=False,\n                            fit_intercept=fit_intercept,\n                            normalize=normalize)\n\n                X = dtype(X)\n                y = dtype(y)\n                ignore_warnings(clf.fit)(X, y)\n\n                coef[dtype] = clf.coef_\n                intercept[dtype] = clf.intercept_\n\n                assert_equal(clf.coef_.dtype, dtype)\n\n            assert_array_almost_equal(coef[np.float32], coef[np.float64],\n                                      decimal=4)\n            assert_array_almost_equal(intercept[np.float32],\n                                      intercept[np.float64],\n                                      decimal=4)\n", "def test_enet_max_iter():\n    # Test that an error message is raised if the maximum number of iterations is reached\n    msg = \"Objective did not converge, you might want to increase the number of iterations\"\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    with pytest.warns(ConvergenceWarning, match=msg):\n        ElasticNet(alpha=0.1, max_iter=1, random_state=42).fit(X, y)\n\n    with pytest.warns(ConvergenceWarning, match=msg):\n        MultiTaskElasticNet(alpha=0.1, max_iter=1, random_state=42).fit(X, y[:, None])\n", "def test_enet_cv_l1_ratio():\n    # Test that l1_ratio can be a list\n    X, y, X_test, y_test = build_dataset()\n    max_iter = 150\n\n    # Here we have a small number of iterations, and thus the\n    # ElasticNet might not converge. This is to speed up tests\n    clf = ElasticNetCV(alphas=[0.01, 0.05, 0.1], eps=2e-3,\n                       l1_ratio=[0.3, 0.5, 0.7], cv=3,\n                       max_iter=max_iter)\n    ignore_warnings(clf.fit)(X, y)\n    # Well-conditioned settings, we should have selected our\n    # smallest penalty\n    assert_almost_equal(clf.alpha_, min(clf.alphas_))\n    # Non-sparse ground truth: we should have selected an elastic-net\n    # that is closer to ridge than to lasso\n    assert_equal(clf.l1_ratio_, 0.5)\n", "def test_enet_sparse_input():\n    # Test that the ElasticNet and MultiTaskElasticNet handle sparse input correctly\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf_dense = ElasticNet(selection='cyclic', tol=1e-8)\n    clf_dense.fit(X, y)\n\n    clf_sparse = ElasticNet(selection='cyclic', tol=1e-8)\n    clf_sparse.fit(X_sparse, y)\n\n    assert_array_almost_equal(clf_dense.coef_, clf_sparse.coef_)\n    assert_almost_equal(clf_dense.intercept_, clf_sparse.intercept_)\n\n    new_y = np.hstack((y[:, np.newaxis], y[:, np.newaxis]))\n    clf_dense = MultiTaskElasticNet(selection='cyclic', tol=1e-8)\n    clf_dense.fit(X, new_y)\n\n    clf_sparse = MultiTaskElasticNet(selection='cyclic', tol=1e-8)\n    clf_sparse.fit(X_sparse, new_y)\n\n    assert_array_almost_equal(clf_dense.coef_, clf_sparse.coef_)\n    assert_almost_equal(clf_dense.intercept_, clf_sparse.intercept_)\n"], "sample_516": ["def test_hatching_legend_transparent():\n    \"\"\"Test for correct hatching on patches in legend with transparent facecolor\"\"\"\n    fig = plt.figure(figsize=(1, 2))\n\n    a = Rectangle([0, 0], 0, 0, facecolor=(0, 0, 1, 0.5), hatch=\"XXXX\")\n    b = Rectangle([0, 0], 0, 0, facecolor=(0, 1, 0, 0.5), hatch=\"XXXX\")\n\n    fig.legend([a, b, a, b], [\"\", \"\", \"\", \"\"])\n", "def test_text_with_hyphen():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, \"A-B\", horizontalalignment='center', verticalalignment='center', fontsize=14)\n", "def test_non_utf8_encoding():\n    \"\"\"Test for correct encoding of non-UTF-8 characters\"\"\"\n    fig = plt.figure()\n    s = \"This string contains a non-UTF-8 character: \\x80\"\n    fig.text(0, 0.5, s, size=12)\n", "def test_colorspace():\n    \"\"\"Test for correct colorspace handling\"\"\"\n    fig, ax = plt.subplots(2, 2)\n\n    # Test grayscale image\n    grayscale_image = np.random.rand(10, 10)\n    ax[0, 0].imshow(grayscale_image, cmap='gray')\n    ax[0, 0].set_title('Grayscale')\n\n    # Test RGB image\n    rgb_image = np.random.rand(10, 10, 3)\n    ax[0, 1].imshow(rgb_image)\n    ax[0, 1].set_title('RGB')\n\n    # Test RGBA image\n    rgba_image = np.random.rand(10, 10, 4)\n    ax[1, 0].imshow(rgba_image)\n    ax[1, 0].set_title('RGBA')\n\n    # Test non-standard colorspace (e.g., YCbCr)\n    # Note: This requires a custom colorspace conversion or a suitable image library\n    # that supports the YCbCr colorspace.\n    # Uncomment the following lines when such support is available.\n    # ycbcr_image = ...  # Convert an image to YCbCr colorspace\n    # ax[1, 1].imshow(ycbcr_image)\n    # ax[1, 1].set_title('YCbCr')\n\n    plt.tight_layout()\n", "def test_text_size_legend():\n    \"\"\"Test for correct text size in legend\"\"\"\n    fig = plt.figure(figsize=(1, 2))\n\n    a = Rectangle([0, 0], 0, 0, facecolor=\"green\")\n    b = Rectangle([0, 0], 0, 0, facecolor=\"blue\")\n\n    fig.legend([a, b, a, b], [\"Small Text\", \"Large Text\", \"Small Text\", \"Large Text\"],\n               fontsize=['x-small', 'x-large', 'x-small', 'x-large'])\n"], "sample_287": ["def test_filter_horizontal_valid_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        filter_horizontal = ('album',)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_check_date_hierarchy_with_nonexistent_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        date_hierarchy = \"nonexistent\"\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'date_hierarchy' refers to 'nonexistent', which \"\n            \"does not refer to a Field.\",\n            obj=SongAdmin,\n            id='admin.E127',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_autocomplete_fields_not_a_list_or_tuple(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = 'test'\n\n    self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'autocomplete_fields' must be a list or tuple.\",\n            obj=SongAdmin,\n            id='admin.E036',\n        )\n    ])\n", "def test_list_display_callable(self):\n    class SongAdmin(admin.ModelAdmin):\n        @admin.display(description=\"Awesome Song\")\n            if instance.title == \"Born to Run\":\n                return \"Best Ever!\"\n            return \"Status unknown.\"\n\n        list_display = (awesome_song,)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_list_display_with_callable(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = (\"pk\", \"custom_display\")\n\n        @admin.display(description='Custom Description')\n            return obj.title\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_880": ["def test_type_of_target_integer_float():\n    y = np.array([1.0, 2.0, 3.0])\n    assert type_of_target(y) == \"multiclass\"\n\n    y = np.array([1.1, 2.2, 3.3])\n    assert type_of_target(y) == \"continuous\"\n\n    y = np.array([[1.0, 2.0], [3.0, 4.0]])\n    assert type_of_target(y) == \"continuous-multioutput\"\n", "def test_ovr_decision_function_input_validation():\n    # Test input validation for _ovr_decision_function\n    predictions = np.array([[0, 1, 1], [0, 1, 0], [0, 1, 1], [0, 1, 1]])\n    confidences = np.array(\n        [[-1e16, 0, -1e16], [1.0, 2.0, -3.0], [-5.0, 2.0, 5.0], [-0.5, 0.2, 0.5]]\n    )\n\n    # Test with invalid n_classes\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, 4)\n\n    # Test with invalid predictions shape\n    with pytest.raises(ValueError):\n        _ovr_decision_function(np.array([[0, 1]]), confidences, 3)\n\n    # Test with invalid confidences shape\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, np.array([[-1e16, 0]]), 3)\n", "def test_type_of_target_with_complex_data():\n    msg = \"Complex data not supported\"\n    with pytest.raises(ValueError, match=msg):\n        type_of_target(np.array([1 + 1j, 2 + 2j]))\n", "def test_ovr_decision_function_invalid_input():\n    # Test the function with invalid input\n    predictions = np.array([[0, 1, 1]])\n    confidences = np.array([[-1e16, 0, -1e16]])\n    n_classes = 4\n\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes)\n\n    # Test with mismatched shapes\n    predictions = np.array([[0, 1, 1], [0, 1, 0]])\n    confidences = np.array([[-1e16, 0, -1e16]])\n\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes)\n", "def test_ovr_decision_function_invalid_input():\n    # test _ovr_decision_function with invalid input\n\n    predictions = np.array([[0, 1, 1], [0, 1, 0]])\n    confidences = np.array([[0, -1e16, -1e16], [1.0, 2.0, -3.0]])\n    n_classes = 3\n\n    # predictions and confidences should have the same shape\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences[:-1], n_classes)\n\n    # n_classes should be compatible with the number of classifiers\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes + 1)\n\n    # predictions should only contain 0 or 1\n    predictions[0, 0] = 2\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes)\n"], "sample_71": ["    def test_non_numeric_input(self):\n        with self.assertRaises(TypeError):\n            nformat(\"abc\", '.')\n        with self.assertRaises(TypeError):\n            nformat(None, '.')\n        with self.assertRaises(TypeError):\n            nformat(object(), '.')\n", "    def test_scientific_notation(self):\n        self.assertEqual(nformat(Decimal('1e10'), '.', decimal_pos=2), '10000000000.00')\n        self.assertEqual(nformat(Decimal('1e-10'), '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(Decimal('1.23e5'), '.', decimal_pos=2), '123000.00')\n        self.assertEqual(nformat(Decimal('-1.23e5'), '.', decimal_pos=2), '-123000.00')\n", "    def test_scientific_notation(self):\n        self.assertEqual(nformat(Decimal('1e10'), '.', decimal_pos=2), '10000000000.00')\n        self.assertEqual(nformat(Decimal('1e-10'), '.', decimal_pos=10), '0.0000000001')\n        self.assertEqual(nformat(Decimal('1e21'), '.', decimal_pos=2), '1e+21')\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,7890')\n", "    def test_format_with_non_uniform_grouping(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=',', force_grouping=True), '12,34,56,7890')\n        self.assertEqual(nformat(123456789012345, '.', grouping=(3, 2, 0), thousand_sep=',', force_grouping=True), '12,34,56,789,01,23,45')\n        self.assertEqual(nformat(123456789012345, '.', grouping=(3, 2, 1), thousand_sep=',', force_grouping=True), '12,34,56,789,012,34,5')\n"], "sample_562": ["def test_axline_invalid_inputs():\n    fig, ax = plt.subplots()\n    with pytest.raises(TypeError, match=\"Exactly one of 'xy2' and 'slope' must be given\"):\n        ax.axline((.1, .1))\n    with pytest.raises(TypeError, match=\"Exactly one of 'xy2' and 'slope' must be given\"):\n        ax.axline((.1, .1), (.8, .4), slope=0.6)\n    with pytest.raises(ValueError, match=\"Cannot draw a line through two identical points\"):\n        ax.axline((.1, .1), (.1, .1))\n", "def test_axline_invalid_arguments():\n    fig, ax = plt.subplots()\n    with pytest.raises(TypeError, match=\"Exactly one of 'xy2' and 'slope' must be given\"):\n        ax.axline((.1, .1))\n    with pytest.raises(TypeError, match=\"Exactly one of 'xy2' and 'slope' must be given\"):\n        ax.axline((.1, .1), (.8, .4), slope=0.6)\n    with pytest.raises(ValueError, match=\"Cannot draw a line through two identical points\"):\n        ax.axline((.1, .1), (.1, .1))\n", "def test_linestyle_prop_cycle(fig_test, fig_ref):\n    \"\"\"Test that we can set linestyle prop_cycle.\"\"\"\n    linestyles = ['-', '--', '-.', ':', 'None', ' ', '',\n                  (0, (3, 5, 1, 5)), (0, (3, 5, 1, 5, 1, 5))]\n\n    x = np.linspace(0, 10)\n    y = np.sin(x)\n\n    axs = fig_ref.add_subplot()\n    for i, ls in enumerate(linestyles):\n        axs.plot(x, y - i, linestyle=ls)\n\n    matplotlib.rcParams['axes.prop_cycle'] = cycler(linestyle=linestyles)\n\n    ax = fig_test.add_subplot()\n    for i, _ in enumerate(linestyles):\n        ax.plot(x, y - i)\n", "def test_valid_capstyles():\n    line = mlines.Line2D([], [])\n    valid_capstyles = [\"butt\", \"round\", \"projecting\"]\n    for capstyle in valid_capstyles:\n        line.set_solid_capstyle(capstyle)\n        line.set_dash_capstyle(capstyle)\n        assert line.get_solid_capstyle() == capstyle\n        assert line.get_dash_capstyle() == capstyle\n\n    invalid_capstyles = [\"square\", \"invalid\", 123]\n    for capstyle in invalid_capstyles:\n        with pytest.raises(ValueError):\n            line.set_solid_capstyle(capstyle)\n            line.set_dash_capstyle(capstyle)\n", "def test_axline_identical_points():\n    fig, ax = plt.subplots()\n    # Testing axline with identical points.\n    # This should raise a ValueError.\n    with pytest.raises(ValueError,\n                       match=\"Cannot draw a line through two identical points\"):\n        ax.axline((.1, .1), (.1, .1))\n"], "sample_180": ["def test_unique_constraint_name_constraints(self):\n    class Model(models.Model):\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(fields=['id'], name='_unique_name'),\n                models.UniqueConstraint(fields=['id'], name='5unique_name'),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"The constraint name '%sunique_name' cannot start with an \"\n            \"underscore or a number.\" % prefix,\n            obj=Model,\n            id='models.E031',\n        ) for prefix in ('_', '5')\n    ])\n", "def test_unique_constraint_pointing_to_o2o_field(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        o2o_1 = models.OneToOneField(Target, models.CASCADE, related_name='target_1')\n        o2o_2 = models.OneToOneField(Target, models.CASCADE, related_name='target_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(fields=['o2o_1', 'o2o_2'], name='name'),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                indexes = [\n                    models.Index(\n                        fields=['age'],\n                        name='index_age_gte_10',\n                        condition=models.Q(age__gte=10),\n                        unique=True,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_indexes else [\n            Warning(\n                '%s does not support unique constraints with conditions.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W036',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "def test_m2m_through_with_abstract_model(self):\n    class AbstractThroughModel(models.Model):\n        abstract_field = models.CharField(max_length=10)\n\n        class Meta:\n            abstract = True\n\n    class ParentModel(models.Model):\n        pass\n\n    class ChildModel(models.Model):\n        parents = models.ManyToManyField(ParentModel, through=AbstractThroughModel)\n\n    errors = ChildModel.check()\n    self.assertEqual(errors, [\n        Error(\n            \"'ChildModel.parents' specifies an abstract model as the \"\n            \"intermediary table, which is not allowed.\",\n            obj=ChildModel._meta.get_field('parents'),\n            id='fields.E341',\n        )\n    ])\n", "def test_unique_constraint_name_constraints(self):\n    class Model(models.Model):\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(fields=['id'], name='_unique_name'),\n                models.UniqueConstraint(fields=['id'], name='5unique_name'),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [\n        Error(\n            \"The constraint name '%sunique_name' cannot start with an underscore or a number.\" % prefix,\n            obj=Model,\n            id='models.E032',\n        ) for prefix in ('_', '5')\n    ])\n"], "sample_1209": ["def test_prefix_combination():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n    G = PREFIXES['G']\n\n    assert m * k == k\n    assert k * m == k\n    assert k * M == M\n    assert M * k == M\n    assert M * G == G\n    assert G * M == G\n    assert m * G == M\n    assert G * m == M\n", "def test_prefix_multiplication():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    assert m * k == PREFIXES['m'] * PREFIXES['k']\n    assert m * k == PREFIXES['k']\n    assert k * m == PREFIXES['k']\n    assert k * m * m == PREFIXES['m']\n    assert m * m * k == PREFIXES['m'] * PREFIXES['k']\n    assert m * m * k == PREFIXES['k'] * PREFIXES['m']\n", "def test_prefix_division():\n    k = PREFIXES['k']\n    m = PREFIXES['m']\n\n    expr1 = k / k\n    assert expr1 == S.One\n\n    expr2 = k / m\n    assert expr2 == 1000\n\n    expr3 = m / k\n    assert expr3 == 0.001\n\n    expr4 = k / x\n    assert expr4 == k / x\n", "def test_prefix_multiplication():\n    assert kilo * kibi == Prefix('kibio', 'Ki', 15)\n    assert kibi * kilo == Prefix('kibikilo', 'Kik', 18)\n    assert kilo / kibi == Prefix('kiloexbi', 'KEi', -3)\n    assert kibi / kilo == Prefix('exbi', 'Ei', -15)\n", "def test_prefix_multiplication():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n    G = PREFIXES['G']\n\n    assert m * m == milli\n    assert m * m * m == micro\n    assert k * k * k == M\n    assert M * k == G\n"], "sample_1130": ["def test_auto_vel_with_acceleration():\n    q, u = dynamicsymbols('q u')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    P.set_pos(O, q * N.x)\n    P.set_acc(N, u * N.y)\n    assert P.vel(N) == q.diff(t) * N.x + 0.5 * u * t * N.y\n", "def test_auto_point_vel_connected_frames_inconsistent_vel():\n    t = dynamicsymbols._t\n    q, q1, q2, u1, u2 = dynamicsymbols('q q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n    P = Point('P')\n    P.set_pos(O, q1 * N.x + q2 * B.y)\n    B.orient(N, 'Axis', (q, B.x))\n    P.set_vel(N, u2 * N.z)  # Inconsistent with velocity from O\n    with warnings.catch_warnings():  # Inconsistent velocities, thus a warning is raised\n        warnings.simplefilter(\"error\")\n        raises(UserWarning, lambda: P.vel(N))\n", "def test_point_vel_multiple_paths_consistent_velocities():\n    q, u1, u2 = dynamicsymbols('q u1 u2')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    Q = Point('Q')\n    P.set_vel(N, u1 * N.x)\n    Q.set_vel(N, u1 * N.x)\n    O.set_pos(P, q * N.x)\n    O.set_pos(Q, q * N.x)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        assert O.vel(N) == u1 * N.x\n        assert len(w) == 0  # No warning for consistent velocities\n", "def test_auto_vel_connected_frames_multiple_paths_warning():\n    q, q1, u = dynamicsymbols('q q1 u')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    Q = Point('Q')\n    O.set_vel(N, u * N.x)\n    P.set_pos(O, q * N.x)\n    Q.set_pos(O, q1 * B.y)\n    N.orient(B, 'Axis', (q, B.x))\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")\n        raises(UserWarning ,lambda: Q.vel(N))\n", "def test_auto_point_vel_inconsistent_vel_definitions():\n    q, u = dynamicsymbols('q u')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    Q = Point('Q')\n    P.set_vel(N, u * N.x)\n    Q.set_vel(N, (u + q) * N.x)\n    O.set_pos(P, q * N.x)\n    O.set_pos(Q, q * N.x)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")\n        raises(UserWarning ,lambda: O.vel(N))\n"], "sample_494": ["def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(3, 4),\n        (\"complex('(3+4j)')\", set()),\n    )\n", "def test_serialize_complex(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(3, 4),\n        (\"complex('3+4j')\", set()),\n    )\n", "def test_serialize_decimal_range(self):\n    value = decimal.DecimalRange(decimal.Decimal('1.0'), decimal.Decimal('2.0'))\n    result = self.serialize_round_trip(value)\n    self.assertEqual(result.start, value.start)\n    self.assertEqual(result.stop, value.stop)\n", "def test_serialize_complex_type(self):\n    self.assertSerializedEqual(complex(1, 2))\n    self.assertSerializedResultEqual(\n        complex(1, 2), (\"complex((1+2j))\", set())\n    )\n", "def test_serialize_custom_field(self):\n    \"\"\"Test serialization of custom field.\"\"\"\n    class CustomField(models.Field):\n            kwargs['max_length'] = 10\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs['max_length']\n            return name, path, args, kwargs\n\n    field = CustomField()\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(string, \"migrations.test_writer.CustomField()\")\n    self.serialize_round_trip(field)\n"], "sample_116": ["def test_cache_versioning_delete_many(self):\n    cache.set('answer1', 37, version=1)\n    cache.set('answer1', 42, version=2)\n    cache.set('answer2', 37, version=1)\n    cache.set('answer2', 42, version=2)\n    cache.delete_many(['answer1', 'answer2'])\n    self.assertIsNone(cache.get('answer1', version=1))\n    self.assertIsNone(cache.get('answer1', version=2))\n    self.assertIsNone(cache.get('answer2', version=1))\n    self.assertIsNone(cache.get('answer2', version=2))\n\n    cache.set('answer3', 37, version=1)\n    cache.set('answer3', 42, version=2)\n    cache.set('answer4', 37, version=1)\n    cache.set('answer4', 42, version=2)\n    cache.delete_many(['answer3', 'answer4'], version=2)\n    self.assertEqual(cache.get('answer3', version=1), 37)\n    self.assertIsNone(cache.get('answer3', version=2))\n    self.assertEqual(cache.get('answer4', version=1), 37)\n    self.assertIsNone(cache.get('answer4', version=2))\n\n    cache.set('answer5', 37, version=1)\n    cache.set('answer5', 42, version=2)\n    cache.set('answer6', 37, version=1)\n    cache.set('answer6', 42, version=2)\n    caches['v2'].delete_many(['answer5', 'answer6'])\n    self.assertEqual(cache.get('answer5', version=1), 37)\n    self.assertIsNone(cache.get('answer5', version=2))\n    self.assertEqual(cache.get('answer6', version=1), 37)\n    self.assertIsNone(cache.get('answer6', version=2))", "def test_patch_cache_control_no_max_age(self):\n    response = HttpResponse()\n    patch_cache_control(response, no_cache=True, max_age=None)\n    self.assertEqual(response['Cache-Control'], 'no-cache')\n", "def test_zero_cull_large_initial_count(self):\n    large_count = 100\n    self._perform_cull_test(caches['zero_cull'], large_count, int(large_count * 0.8))\n", "def test_memcached_pickle_protocol(self):\n    \"\"\"Ensure that the default pickle protocol is used for memcached.\"\"\"\n    self.assertEqual(cache._cache.pickleProtocol, pickle.DEFAULT_PROTOCOL)\n", "def test_timeout_override(self):\n    # cache_page allows timeout to be overridden\n    view = cache_page(1)(hello_world_view)\n    request = self.factory.get('/view/')\n    response = view(request, '1', _cache_timeout=2)\n    self.assertEqual(response.content, b'Hello World 1')\n    # Repeating the request should result in a cache hit\n    response = view(request, '2', _cache_timeout=2)\n    self.assertEqual(response.content, b'Hello World 1')\n    # Wait for the cache to expire\n    time.sleep(2)\n    # The cache has expired, so the response should be different\n    response = view(request, '3', _cache_timeout=2)\n    self.assertEqual(response.content, b'Hello World 3')\n"], "sample_295": ["def test_expression_wrapper_output_field(self):\n    expr = ExpressionWrapper(Value(3.14), output_field=IntegerField())\n    self.assertEqual(expr.output_field, IntegerField())\n", "def test_output_field_charfield(self):\n    Time.objects.create()\n    time = Time.objects.annotate(one=Value(1, output_field=CharField())).first()\n    self.assertEqual(time.one, '1')\n", "def test_expression_wrapper_with_none_output_field(self):\n    expr = ExpressionWrapper(Value(3))\n    self.assertIsNone(expr.output_field)\n    self.assertEqual(expr.get_group_by_cols(alias=None), [expr])\n", "def test_expression_wrapper_ordering(self):\n    expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n    self.assertEqual(expr.desc().get_source_expressions(), [Value(3)])\n    self.assertEqual(expr.asc().get_source_expressions(), [Value(3)])\n", "def test_expression_wrapper_with_unsafe_output_field(self):\n    expr = ExpressionWrapper(F('pk'), output_field=BinaryField())\n    msg = 'ExpressionWrapper does not support BinaryField output_field.'\n    with self.assertRaisesMessage(FieldError, msg):\n        expr.resolve_expression(query=None, allow_joins=True, reuse=None, summarize=False, for_save=False)\n"], "sample_76": ["    def test_consistent_language_settings(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_settings_consistent(self):\n    for tag in self.valid_tags:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag, LANGUAGES=[(tag, tag)]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings_with_default(self):\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_48": ["def test_filter_aggregate_with_expression(self):\n    avg_pages = Book.objects.filter(price__gt=Decimal('30')).aggregate(Avg(F('pages')))\n    self.assertEqual(avg_pages, {'pages__avg': Approximate(350.0, places=1)})\n", "def test_case_insensitive_lookup_with_aggregate(self):\n    publishers = Publisher.objects.annotate(num_books=Count(\"book\")).filter(\n        name__iexact=\"apress\").order_by(\"pk\")\n    self.assertQuerysetEqual(publishers, ['Apress'], lambda p: p.name)\n", "def test_stddev_aggregate(self):\n    # Test StdDev aggregate function\n    stddev_rating = Book.objects.aggregate(stddev_rating=StdDev('rating'))\n    # Add your assertion here to check the result\n\n    stddev_price = Book.objects.aggregate(stddev_price=StdDev('price'))\n    # Add your assertion here to check the result\n\n    # Test StdDev with sample=True\n    stddev_rating_sample = Book.objects.aggregate(stddev_rating_sample=StdDev('rating', sample=True))\n    # Add your assertion here to check the result\n", "def test_distinct_aggregate(self):\n    # Test Count(distinct=True) with multiple fields\n    vals = Book.objects.aggregate(Count(\"rating\", \"publisher\", distinct=True))\n    self.assertEqual(vals, {\"rating__publisher__count\": 6})  # Modify the expected result based on your data\n", "def test_stddev_variance(self):\n    # Test StdDev and Variance aggregations\n    books = Book.objects.aggregate(price_stddev=StdDev('price'), price_variance=Variance('price'))\n    self.assertIsInstance(books['price_stddev'], Decimal)\n    self.assertIsInstance(books['price_variance'], Decimal)\n    self.assertAlmostEqual(books['price_stddev'], Decimal('22.92'), places=2)\n    self.assertAlmostEqual(books['price_variance'], Decimal('526.76'), places=2)\n\n    # Test StdDev and Variance with sample=True\n    books = Book.objects.aggregate(price_stddev=StdDev('price', sample=True), price_variance=Variance('price', sample=True))\n    self.assertIsInstance(books['price_stddev'], Decimal)\n    self.assertIsInstance(books['price_variance'], Decimal)\n    self.assertAlmostEqual(books['price_stddev'], Decimal('23.44'), places=2)\n    self.assertAlmostEqual(books['price_variance'], Decimal('550.78'), places=2)\n"], "sample_333": ["    def test_field_deep_copy_widget(self):\n        class CustomTextInput(TextInput):\n            pass\n\n        field = CharField(widget=CustomTextInput())\n        field_copy = copy.deepcopy(field)\n        self.assertIsInstance(field_copy, CharField)\n        self.assertIsNot(field_copy.widget, field.widget)\n", "def test_attribute_none(self):\n    class CustomForm(Form):\n        default_renderer = None\n\n    form = CustomForm()\n    self.assertEqual(form.renderer, get_default_renderer())\n", "def test_custom_widget_rendering(self):\n    class CustomWidget(TextInput):\n        template_name = 'custom_widget.html'\n\n    class CustomForm(Form):\n        custom_field = CharField(widget=CustomWidget)\n\n    form = CustomForm()\n    rendered = form.render('custom_field')\n    self.assertIn('Custom widget rendering', rendered)\n", "def test_custom_renderer_field_class(self):\n    class CustomField(CharField):\n        default_error_messages = {'invalid': 'Custom field error message.'}\n\n    class CustomForm(Form):\n        default_renderer = CustomRenderer()\n        custom_field = CustomField()\n\n    form = CustomForm()\n    field = form['custom_field']\n    self.assertIsInstance(field.error_messages['invalid'], SafeData)\n    self.assertEqual(field.error_messages['invalid'], 'Custom field error message.')\n", "def test_boundfield_accessibility_hidden_input(self):\n    class CustomWidget(TextInput):\n        input_type = 'custom'\n        is_hidden = True\n\n    class SomeForm(Form):\n        custom = CharField(widget=CustomWidget)\n\n    form = SomeForm()\n    field = form['custom']\n    self.assertTrue(field.is_hidden)\n    self.assertEqual(field.widget_type, 'custom')\n"], "sample_577": ["def test_default_object_equality(self):\n\n    assert Default() == Default()\n    assert Default() != \"other\"\n", "def test_legend_with_missing_values(self, xy):\n    s = pd.Series([\"a\", \"b\", np.nan, \"c\"], name=\"s\")\n    p = Plot(**xy, color=s).add(MockMark()).plot()\n    e, = p._legend_contents\n\n    labels = categorical_order(s.dropna())\n\n    assert e[0] == (s.name, s.name)\n    assert e[-1] == labels\n\n    artists = e[1]\n    assert len(artists) == len(labels)\n    for a, label in zip(artists, labels):\n        assert isinstance(a, mpl.artist.Artist)\n        assert a.value == label\n        assert a.variables == [\"color\"]\n", "def test_default_in_facet(self, long_df):\n    p = Plot(long_df).facet(col=Default()).plot()\n    assert len(p._figure.axes) == 1\n    assert p._figure.axes[0].get_title() == \"\"\n", "def test_two_variables_single_order(self, long_df):\n\n    order = [\"a\", \"b\", \"c\"]\n    p = Plot(long_df, x=\"a\", y=\"b\").facet(col=\"b\", order=order).plot()\n    for i, ax in enumerate(p._figure.axes):\n        assert ax.get_title() == f\"B {order[i]}\"\n", "def test_default_conversion(self):\n    default = Default()\n    assert default.convert(\"test\") == \"test\"\n    assert default.convert(123) == 123\n"], "sample_565": ["def test_inset_axes_with_axes_class():\n    fig, ax = plt.subplots(figsize=[5, 4])\n\n    # prepare the demo image\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n    Z2 = np.zeros((150, 150))\n    ny, nx = Z.shape\n    Z2[30:30+ny, 30:30+nx] = Z\n\n    ax.imshow(Z2, extent=extent, interpolation=\"nearest\",\n              origin=\"lower\")\n\n    axins = inset_axes(ax, width=1., height=1., bbox_to_anchor=(200, 100),\n                       loc=3, borderpad=0, axes_class=mpl.axes.Axes)\n\n    axins.imshow(Z2, extent=extent, interpolation=\"nearest\",\n                 origin=\"lower\")\n    axins.yaxis.get_major_locator().set_params(nbins=7)\n    axins.xaxis.get_major_locator().set_params(nbins=7)\n    x1, x2, y1, y2 = -1.5, -0.9, -2.5, -1.9\n    axins.set_xlim(x1, x2)\n    axins.set_ylim(y1, y2)\n\n    plt.xticks(visible=False)\n    plt.yticks(visible=False)\n\n    mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n\n    asb = AnchoredSizeBar(ax.transData,\n                          0.5,\n                          '0.5',\n                          loc='lower center',\n                          pad", "def test_inset_axes_locator():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3])\n    ax.axis([1, 3, 1, 3])\n    inset_locator = InsetPosition(ax, [0.2, 0.2, 0.4, 0.4])\n    inset_ax = fig.add_axes([0, 0, 1, 1])\n    inset_ax.set_axes_locator(inset_locator)\n    fig.canvas.draw()\n    assert np.allclose(inset_ax.get_position().bounds, [0.2, 0.2, 0.4, 0.4])\n", "def test_bbox_connector_get_bbox_edge_pos():\n    bbox = Bbox.from_bounds(0, 0, 10, 10)\n    assert BboxConnector.get_bbox_edge_pos(bbox, 1) == (10, 10)\n    assert BboxConnector.get_bbox_edge_pos(bbox, 2) == (0, 10)\n    assert BboxConnector.get_bbox_edge_pos(bbox, 3) == (0, 0)\n    assert BboxConnector.get_bbox_edge_pos(bbox, 4) == (10, 0)\n", "def test_inset_axes_with_axes_class():\n    fig, ax = plt.subplots()\n    ax_ins = inset_axes(ax, width=\"40%\", height=\"30%\", loc='lower left',\n                        axes_class=mpl.axes.Axes)\n    assert isinstance(ax_ins, mpl.axes.Axes)\n", "def test_inset_axes_with_axes_class():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax_ins = inset_axes(ax, width=\"50%\", height=\"50%\", axes_class=mpl.axes.Axes)\n    fig.canvas.draw()\n    assert isinstance(ax_ins, mpl.axes.Axes)\n"], "sample_1083": ["def test_finite_assumptions():\n    z = Symbol('z')\n    assert sinh(z).is_finite is True\n    assert cosh(z).is_finite is True\n    assert tanh(z).is_finite is True\n    assert sech(z).is_finite is True\n    assert csch(z).is_finite is True\n    assert coth(z).is_finite is True\n", "def test_asech_is_finite():\n    x = Symbol('x', real=True)\n    assert asech(x).is_finite is True\n", "def test_tanh_nonzero():\n    # See issue 11721\n    # tanh(x) is nonzero for nonzero values of x\n    x = symbols('x')\n    k = symbols('k', real=True)\n    n = symbols('n', integer=True)\n\n    assert tanh(k).is_zero is False\n    assert tanh(k + (2*n + 1)*pi*I).is_zero is False\n    assert tanh(I*pi/2).is_zero is True\n    assert tanh(3*I*pi/2).is_zero is True\n", "def test_tanh_finite():\n    z = Symbol('z', real=False)\n    assert tanh(z).is_finite is None\n    assert tanh(z).is_finite is None\n    assert tanh(0).is_finite is True\n    assert tanh(oo).is_finite is True\n    assert tanh(-oo).is_finite is True\n\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    assert tanh(p).is_finite is True\n    assert tanh(n).is_finite is True\n", "def test_asinh_real_assumptions():\n    z = Symbol('z', real=False)\n    assert asinh(z).is_real is None\n    z = Symbol('z', real=True)\n    assert asinh(z).is_real is True\n"], "sample_662": ["def test_user_properties(self, testdir):\n    \"\"\"Check serialization/deserialization of user_properties.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.user_property('key1', 'value1')\n        @pytest.mark.user_property('key2', 'value2')\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    report = reports[1]\n\n    assert report.user_properties == [('key1', 'value1'), ('key2', 'value2')]\n\n    data = report._to_json()\n    loaded_report = TestReport._from_json(data)\n    assert loaded_report.user_properties == [('key1', 'value1'), ('key2', 'value2')]\n", "def test_count_towards_summary(self, testdir):\n    \"\"\"Test the count_towards_summary property of a BaseReport instance.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 6\n    for report in reports:\n        assert report.count_towards_summary is True\n", "def test_xfail_report_serialization(self, testdir):\n    \"\"\"Test serialization of xfailed test reports.\"\"\"\n    reprec = testdir.inline_runsource(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason=\"Expected failure\")\n            assert False\n    \"\"\"\n    )\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    rep = reports[1]\n    assert rep.outcome == \"failed\"\n    assert rep.wasxfail == \"Expected failure\"\n\n    d = rep._to_json()\n    a = TestReport._from_json(d)\n\n    assert a.__dict__.keys() == rep.__dict__.keys()\n    for key in rep.__dict__.keys():\n        if key != \"longrepr\":\n            assert getattr(a, key) == getattr(rep, key)\n    assert a.wasxfail == \"Expected failure\"\n", "def test_report_sections(self, testdir):\n    \"\"\"Check serialization/deserialization of report objects containing sections (#5786)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.warning('test warning')\n            assert False\n        \"\"\"\n    )\n\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3  # setup, call, teardown\n    report = reports[1]  # call report\n\n    assert report.failed\n    assert len(report.sections) == 1\n    section_name, section_content = report.sections[0]\n    assert section_name == \"Captured log call\"\n    assert \"test warning\" in section_content\n\n    data = report._to_json()\n    loaded_report = TestReport._from_json(data)\n    assert len(loaded_report.sections) == 1\n    loaded_section_name, loaded_section_content = loaded_report.sections[0]\n    assert loaded_section_name == section_name\n    assert loaded_section_content == section_content\n", "def test_exception_chain_deserialization(self, testdir):\n    \"\"\"Check deserialization of a report containing an exception chain with no descriptions.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            raise ValueError('value error')\n            try:\n                foo()\n            except ValueError as e:\n                raise RuntimeError('runtime error') from e\n        \"\"\"\n    )\n\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    report = reports[1]\n\n    data = report._to_json()\n    assert data[\"longrepr\"][\"chain\"]\n    for _, _, description in data[\"longrepr\"][\"chain\"]:\n        assert description is None\n\n    loaded_report = TestReport._from_json(data)\n    assert isinstance(loaded_report.longrepr, ExceptionChainRepr)\n    assert len(loaded_report.longrepr.chain) == 2\n    for _, _, description in loaded_report.longrepr.chain:\n        assert description is None\n"], "sample_410": ["def test_create_superuser_email_validation(self):\n    with self.assertRaises(ValueError):\n        User.objects.create_superuser(\n            username=\"test\",\n            email=\"invalid_email\",\n            password=\"test\",\n        )\n", "    def test_reset_password(self):\n        user = User.objects.create_user(\"user\", \"user@example.com\", \"old_password\")\n        self.assertTrue(user.check_password(\"old_password\"))\n        new_password = \"new_password\"\n        user.set_password(new_password)\n        user.save()\n        self.assertTrue(user.check_password(new_password))\n        self.assertFalse(user.check_password(\"old_password\"))\n", "    def test_check_password_with_setter(self):\n        user = User.objects.create_user(\"user\", \"user@example.com\", \"oldpassword\")\n        self.assertTrue(user.check_password(\"oldpassword\"))\n        self.assertFalse(user.check_password(\"newpassword\"))\n\n            user.set_password(raw_password)\n            user.save(update_fields=[\"password\"])\n\n        self.assertTrue(check_password(\"oldpassword\", user.password, setter))\n        self.assertTrue(user.check_password(\"newpassword\"))\n        self.assertFalse(user.check_password(\"oldpassword\"))\n", "    def test_set_unusable_password(self):\n        user = User.objects.create_user(username='testuser', password='testpassword')\n        self.assertTrue(user.has_usable_password())\n        user.set_unusable_password()\n        self.assertFalse(user.has_usable_password())\n", "    def test_check_password_with_setter(self):\n        user = User.objects.create_user(username=\"test\", password=\"oldpassword\")\n        initial_password = user.password\n        self.assertTrue(user.check_password(\"oldpassword\"))\n        self.assertEqual(user.password, initial_password)\n\n        user.check_password(\"newpassword\")\n        self.assertNotEqual(user.password, initial_password)\n        self.assertTrue(user.check_password(\"newpassword\"))\n"], "sample_290": ["def test_add_model_with_field_removed_from_base_model_multi_inheritance(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name when there are multiple inheritance.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'writable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'writable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable', 'app.writable')),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='title', model_name='writable')\n    self.assertOperationAttributes(changes, 'app', 0, 2, name='book')\n", "def test_swappable_circular_multi_mti_with_m2m(self):\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        parent = ModelState('a', 'Parent', [\n            ('user', models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE)),\n            ('children', models.ManyToManyField('a.Child')),\n        ])\n        child = ModelState('a', 'Child', [], bases=('a.Parent',))\n        user = ModelState('a', 'User', [], bases=(AbstractBaseUser, 'a.Child'))\n        changes = self.get_changes([], [parent, child, user])\n    self.assertNumberMigrations(changes, 'a', 1)\n    self.assertOperationTypes(changes, 'a', 0, ['CreateModel', 'CreateModel', 'CreateModel', 'AddField', 'AddField'])\n", "    def test_many_operations_suffix_with_initial_true(self):\n        class Migration(migrations.Migration):\n            initial = True\n            operations = [\n                migrations.CreateModel('Person1', fields=[]),\n                migrations.CreateModel('Person2', fields=[]),\n                migrations.CreateModel('Person3', fields=[]),\n                migrations.DeleteModel('Person4'),\n                migrations.DeleteModel('Person5'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'initial')\n", "    def setUp(self):\n        self.author_empty = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        self.author_index = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ], {\n            'indexes': [models.Index(fields=['id'], name='author_id_idx')],\n        })\n", "def test_alter_model_table_to_none(self):\n    \"\"\"\n    Alter_db_table to None doesn't generate a migration if db_table is not set.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_empty])\n    # Right number of migrations?\n    self.assertEqual(len(changes), 0)\n"], "sample_525": ["def test_unpickle_with_dpi_change():\n    fig = Figure(dpi=42)\n    fig.dpi = 84\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert fig2.dpi == 84\n", "def test_figimage():\n    fig = plt.figure()\n    X = np.random.rand(10, 10)\n    im = fig.figimage(X)\n    assert isinstance(im, mpl.image.FigureImage)\n    assert np.allclose(im.get_array(), X)\n", "def test_set_dpi_with_device_pixel_ratio():\n    fig = Figure()\n    fig.canvas._set_device_pixel_ratio(2)\n    fig.set_dpi(100)\n    assert fig.dpi == 200\n", "def test_set_layout_engine(layout):\n    fig = Figure(layout=layout)\n    if layout is None:\n        assert not fig.get_tight_layout()\n        assert not fig.get_constrained_layout()\n    elif layout == \"tight\":\n        assert fig.get_tight_layout()\n        assert not fig.get_constrained_layout()\n    elif layout == \"constrained\":\n        assert not fig.get_tight_layout()\n        assert fig.get_constrained_layout()\n", "def test_figure_pickle():\n    fig = Figure(dpi=42)\n    fig.canvas._set_device_pixel_ratio(7)\n    fig.canvas.toolbar = \"dummy_toolbar\"  # This should be pickled.\n    fig.canvas.events = \"dummy_events\"  # This should not be pickled.\n    fig._mouse_key_ids = [\"dummy_id\"]  # This should not be pickled.\n    fig._button_pick_id = \"dummy_pick_id\"  # This should not be pickled.\n    fig._scroll_pick_id = \"dummy_scroll_id\"  # This should not be pickled.\n    fig._canvas_callbacks = \"dummy_callbacks\"  # This should be pickled.\n\n    fig_pickled = pickle.loads(pickle.dumps(fig))\n\n    assert fig_pickled.dpi == 42\n    assert fig_pickled.canvas._device_pixel_ratio == 7\n    assert fig_pickled.canvas.toolbar == \"dummy_toolbar\"\n    assert fig_pickled.canvas.events != \"dummy_events\"\n    assert fig_pickled._mouse_key_ids != [\"dummy_id\"]\n    assert fig_pickled._button_pick_id != \"dummy_pick_id\"\n    assert fig_pickled._scroll_pick_id != \"dummy_scroll_id\"\n    assert fig_pickled._canvas_callbacks == \"dummy_callbacks\"\n"], "sample_157": ["    def setUp(self):\n        Object.objects.create(id=1)\n        ObjectReference.objects.create(id=1, obj_id=1)\n", "    def test_serialize_excluded_apps(self, mocked_atomic, mocked_deserialize):\n        # serialize_db_to_string() excludes apps specified in settings.TEST_NON_SERIALIZED_APPS.\n        with self.settings(TEST_NON_SERIALIZED_APPS=['backends']):\n            data = connection.creation.serialize_db_to_string()\n        self.assertEqual(data, '[]')\n        mocked_deserialize.assert_not_called()\n", "    def test_serialize_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n            self.assertIsNotNone(test_connection._test_serialized_contents)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def setUp(self):\n        self.obj = Object.objects.create()\n        self.obj_ref = ObjectReference.objects.create(obj=self.obj)\n", "    def test_non_serializable_app(self):\n        # serialize_db_to_string() skips non-serializable apps.\n        Object.objects.create()\n        settings.TEST_NON_SERIALIZED_APPS = ['backends']\n        data = connection.creation.serialize_db_to_string()\n        self.assertNotIn('backends.object', data)\n"], "sample_338": ["def test_swappable_circular_multi_mti_reverse(self):\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        parent = ModelState('a', 'Parent', [\n            ('user', models.ForeignKey(settings.AUTH_USER_MODEL, models.CASCADE))\n        ])\n        child = ModelState('a', 'Child', [], bases=('a.Parent',))\n        user = ModelState('a', 'User', [], bases=(AbstractBaseUser, 'a.Child'))\n        changes = self.get_changes([parent, child, user], [parent, user])\n    self.assertNumberMigrations(changes, 'a', 1)\n    self.assertOperationTypes(changes, 'a', 0, ['DeleteModel'])\n    self.assertOperationAttributes(changes, 'a', 0, 0, name='Child')\n", "def test_alter_field_with_default_change(self):\n    author_name_new_default = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=100, default='New Name')),\n    ])\n    changes = self.get_changes([self.author_name_default], [author_name_new_default])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=False)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default='New Name')\n", "def test_alter_model_base_class(self):\n    \"\"\"Changing the base class of a model adds a new operation.\"\"\"\n    before = [\n        ModelState('app', 'OldModel', [\n            ('id', models.AutoField(primary_key=True)),\n        ], bases=('testapp.BaseModel',)),\n    ]\n    after = [\n        ModelState('app', 'OldModel', [\n            ('id', models.AutoField(primary_key=True)),\n        ], bases=('testapp.NewBaseModel',)),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterModelBases'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='OldModel', bases=('testapp.NewBaseModel',))\n", "def test_alter_field_with_relation(self):\n    \"\"\"\n    #24755 - Test that AlterField correctly handles relations.\n    \"\"\"\n    changes = self.get_changes([self.author_with_book], [self.author_with_book_on_delete_set_null])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"book\")\n    self.assertEqual(changes['testapp'][0].operations[0].field.remote_field.on_delete, models.SET_NULL)\n", "def test_add_m2m_field_with_through_model(self):\n    \"\"\"\n    #24231 - Adding a ManyToManyField with a through model first creates the\n    through model and then adds the field.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_m2m_through, self.publisher, self.contract])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\n        'CreateModel', 'CreateModel', 'AddField',\n    ])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Publisher')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='Contract')\n    self.assertOperationAttributes(changes, 'testapp', 0, 2, model_name='author', name='publishers')\n"], "sample_497": ["def test_minorticks_visible():\n    fig, ax = plt.subplots()\n    ax.minorticks_on()\n    assert ax.xaxis.minorTicks[0].get_visible()\n    assert ax.yaxis.minorTicks[0].get_visible()\n", "def test_minor_ticks_view_limits():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1.39)\n    ax.minorticks_on()\n    ax.xaxis.set_minor_locator(mticker.AutoMinorLocator())\n    ax.xaxis._update_ticks()\n    for tick in ax.xaxis.minorTicks:\n        assert ax.xaxis.viewLim.contains(tick.get_loc())\n", "def test_custom_locator_and_formatter():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10))\n\n    class CustomLocator(mticker.Locator):\n            return np.array([2, 5, 8])\n\n    class CustomFormatter(mticker.Formatter):\n            return f'Custom {x}'\n\n    ax.xaxis.set_major_locator(CustomLocator())\n    ax.xaxis.set_major_formatter(CustomFormatter())\n\n    fig.canvas.draw()\n    x_tick_label_text = [labl.get_text() for labl in ax.get_xticklabels()]\n    assert x_tick_label_text == ['Custom 2', 'Custom 5', 'Custom 8']\n", "def test_format_ticks(monkeypatch):\n    fig, ax = plt.subplots()\n    ax.xaxis.set_major_formatter(mticker.FormatStrFormatter('%05d'))\n    monkeypatch.setattr(ax, 'get_majorticklocs', lambda: [2, 3, 5])\n    ax.format_xdata(2.5)\n    assert ax.xaxis.get_major_formatter()(2.5) == '00002'\n", "def test_minorticks_rc():\n    fig = plt.figure()\n\n        rc = {'xtick.minor.visible': xminor,\n              'ytick.minor.visible': yminor}\n        with plt.rc_context(rc=rc):\n            ax = fig.add_subplot(2, 2, i)\n\n        assert (len(ax.xaxis.get_minor_ticks()) > 0) == xminor\n        assert (len(ax.yaxis.get_minor_ticks()) > 0) == yminor\n\n    minorticksubplot(False, False, 1)\n    minorticksubplot(True, False, 2)\n    minorticksubplot(False, True, 3)\n    minorticksubplot(True, True, 4)\n"], "sample_46": ["def test_year_lookup(self):\n    from datetime import datetime\n    date1 = datetime(2020, 1, 1)\n    date2 = datetime(2020, 6, 1)\n    date3 = datetime(2021, 1, 1)\n    obj1 = UUIDModel.objects.create(field=uuid.uuid4(), date_field=date1)\n    obj2 = UUIDModel.objects.create(field=uuid.uuid4(), date_field=date2)\n    obj3 = UUIDModel.objects.create(field=uuid.uuid4(), date_field=date3)\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(date_field__year=2020),\n        [obj1, obj2]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(date_field__year__gt=2020),\n        [obj3]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(date_field__year__gte=2020),\n        [obj1, obj2, obj3]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(date_field__year__lt=2021),\n        [obj1, obj2]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(date_field__year__lte=2020),\n        [obj1, obj2]\n    )\n", "def test_query_with_nonexistent_uuid(self):\n    nonexistent_uuid = uuid.uuid4()\n    self.assertFalse(PrimaryKeyUUIDModel.objects.filter(pk=nonexistent_uuid).exists())\n", "def test_year_lookup(self):\n    from datetime import datetime\n    from django.db.models.functions import ExtractYear\n\n    dt = datetime(2022, 6, 15)\n    obj = UUIDModel.objects.create(field=uuid.uuid4(), datetime_field=dt)\n\n    # Test YearExact lookup\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(datetime_field__year=2022),\n        [obj]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(datetime_field__year__exact=2022),\n        [obj]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.annotate(year=ExtractYear('datetime_field')).filter(year=2022),\n        [obj]\n    )\n\n    # Test YearGt lookup\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(datetime_field__year__gt=2021),\n        [obj]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.annotate(year=ExtractYear('datetime_field')).filter(year__gt=2021),\n        [obj]\n    )\n\n    # Test YearGte lookup\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(datetime_field__year__gte=2022),\n        [obj]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.annotate(year=ExtractYear('datetime_field')).filter(year__gte=2022),\n        [obj]\n    )\n\n    # Test YearLt lookup\n    self.assertSequenceEqual(\n        UUIDModel.objects.filter(datetime_field__year__lt=2023),\n        [obj]\n    )\n    self.assertSequenceEqual(\n        UUIDModel.objects.annotate(year=ExtractYear('datetime_field')).filter(year__lt=2023),\n        [obj]\n    )\n\n    # Test YearLte lookup\n    self.assertSequenceEqual(\n", "def test_regex(self):\n    self.assertSequenceEqual(\n        NullableUUIDModel.objects.filter(field__regex=r'^550e8400-'),\n        [self.objs[1]]\n    )\n    self.assertSequenceEqual(\n        NullableUUIDModel.objects.filter(field__iregex=r'^550E8400-'),\n        [self.objs[1]]\n    )\n", "def test_regex(self):\n    self.assertSequenceEqualWithoutHyphens(\n        NullableUUIDModel.objects.filter(field__regex=r'^550e8400e29b'),\n        [self.objs[1]],\n    )\n    self.assertSequenceEqual(\n        NullableUUIDModel.objects.filter(field__regex=r'^550e8400-e29b'),\n        [self.objs[1]],\n    )\n"], "sample_977": ["def test_exp_log():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(exp(x) * log(x)) == \"Exp[x]*Log[x]\"\n", "def test_Add():\n    assert mcode(x + y + z) == \"x + y + z\"\n    assert mcode(x + 1) == \"x + 1\"\n    assert mcode(x + Rational(1, 2)) == \"x + 1/2\"\n    assert mcode(x + oo) == \"x + Infinity\"\n    assert mcode(x + S.NegativeInfinity) == \"x - Infinity\"\n", "def test_LogarithmicFunction():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(log(x, 10)) == \"Log[x, 10]\"\n", "def test_CustomFunction():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n", "def test_special_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(sin(x)**2 + cos(x)**2) == \"Cos[x]^2 + Sin[x]^2\"\n    assert mcode(sin(x)/cos(x)) == \"Sin[x]/Cos[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n"], "sample_463": ["def test_add_model_order_with_respect_to_index_constraint(self):\n    after = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ],\n        options={\n            \"order_with_respect_to\": \"book\",\n            \"constraints\": [\n                models.CheckConstraint(\n                    check=models.Q(_order__gt=1),\n                    name=\"book_order_gt_1\",\n                ),\n            ],\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.book, AutodetectorTests.author_with_book],\n        [AutodetectorTests.book, after],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterOrderWithRespectTo\", \"AddConstraint\"],\n    )\n", "def test_alter_field_with_index(self):\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, AutodetectorTests.book],\n        [AutodetectorTests.author_empty, AutodetectorTests.book_with_index],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\", \"AddIndex\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, model_name=\"book\", name=\"title\")\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"book_title_idx\")\n", "def test_alter_unique_together_with_index_together(self):\n    author_unique_together_and_index_together = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"name\", \"age\")},\n            \"index_together\": {(\"name\", \"age\")},\n        },\n    )\n    author_index_together_only = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"index_together\": {(\"name\", \"age\")},\n        },\n    )\n    changes = self.get_changes(\n        [author_unique_together_and_index_together],\n        [author_index_together_only],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterUniqueTogether\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        unique_together=set(),\n    )\n", "compilation error", "def test_alter_field_index_together_to_index(self):\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, db_index=True)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"index_together\": {(\"name\", \"age\")},\n        },\n    )\n    author_reversed_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\", \"age\"], name=\"author_name_age_idx\"),\n            ],\n        },\n    )\n    changes = self.get_changes([initial_author], [author_reversed_constraints])\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\n            \"AlterIndexTogether\",\n            \"AlterField\",\n            \"AlterField\",\n            \"AddIndex\",\n        ],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        index_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        model_name=\"author\",\n        name=\"name\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        2,\n        model_name=\"author\",\n        name=\"age\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        3,\n        model_name=\"author\",\n        index"], "sample_440": ["def test_update_conflicts_with_expression(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\"),\n            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 2)\n\n    conflicting_objects = [\n        UpsertConflict(number=1, rank=4, name=\"Steve\"),\n        UpsertConflict(number=2, rank=2, name=Lower(Value(\"Olivia\"))),\n    ]\n    UpsertConflict.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 2)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": 4, \"name\": \"Steve\"},\n            {\"number\": 2, \"rank\": 2, \"name\": \"olivia\"},\n        ],\n    )\n", "def test_update_conflicts_with_unique_together(self):\n    class UniqueTogetherModel(models.Model):\n        f1 = models.IntegerField()\n        f2 = models.IntegerField()\n\n        class Meta:\n            unique_together = ((\"f1\", \"f2\"),)\n\n    UniqueTogetherModel.objects.bulk_create(\n        [\n            UniqueTogetherModel(f1=1, f2=1),\n            UniqueTogetherModel(f1=2, f2=2),\n        ]\n    )\n    self.assertEqual(UniqueTogetherModel.objects.count(), 2)\n\n    conflicting_objects = [\n        UniqueTogetherModel(f1=1, f2=1),\n        UniqueTogetherModel(f1=2, f2=2),\n    ]\n    UniqueTogetherModel.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"f1\"],\n    )\n    self.assertEqual(UniqueTogetherModel.objects.count(), 2)\n", "def test_update_conflicts_unique_multiple_fields(self):\n    # Test with multiple unique fields\n    Country.objects.bulk_create(self.data)\n    self.assertEqual(Country.objects.count(), 4)\n\n    new_data = [\n        # Conflicting countries.\n        Country(\n            name=\"Germany\",\n            iso_two_letter=\"DE\",\n            description=(\"Updated description for Germany.\"),\n        ),\n        Country(\n            name=\"Czech Republic\",\n            iso_two_letter=\"CZ\",\n            description=(\"Updated description for Czech Republic.\"),\n        ),\n        # New countries.\n        Country(name=\"Australia\", iso_two_letter=\"AU\"),\n    ]\n    Country.objects.bulk_create(\n        new_data,\n        update_conflicts=True,\n        update_fields=[\"description\"],\n        unique_fields=[\"name\", \"iso_two_letter\"],\n    )\n    self.assertEqual(Country.objects.count(), 5)\n    self.assertCountEqual(\n        Country.objects.values(\"iso_two_letter\", \"description\"),\n        [\n            {\"iso_two_letter\": \"US\", \"description\": \"\"},\n            {\"iso_two_letter\": \"NL\", \"description\": \"\"},\n            {\n                \"iso_two_letter\": \"DE\",\n                \"description\": (\"Updated description for Germany.\"),\n            },\n            {\n                \"iso_two_letter\": \"CZ\",\n                \"description\": (\"Updated description for Czech Republic.\"),\n            },\n            {\"iso_two_letter\": \"AU\", \"description\": \"\"},\n        ],\n    )\n", "def test_update_conflicts_with_none_value(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 1)\n\n    conflicting_object = UpsertConflict(number=1, rank=None, name=\"Steve\")\n    UpsertConflict.objects.bulk_create(\n        [conflicting_object],\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 1)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": None, \"name\": \"Steve\"},\n        ],\n    )\n", "def test_update_conflicts_nullable_fields(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\", description=\"\"),\n            UpsertConflict(number=2, rank=2, name=\"Mary\", description=None),\n            UpsertConflict(number=3, rank=3, name=\"Hannah\", description=\"Hannah's description\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n\n    conflicting_objects = [\n        UpsertConflict(number=1, rank=4, name=\"Steve\", description=\"Steve's description\"),\n        UpsertConflict(number=2, rank=2, name=\"Olivia\", description=\"\"),\n        UpsertConflict(number=3, rank=1, name=\"Hannah\", description=None),\n    ]\n    UpsertConflict.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\", \"description\"],\n        unique_fields=[\"number\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\", \"description\"),\n        [\n            {\"number\": 1, \"rank\": 4, \"name\": \"Steve\", \"description\": \"Steve's description\"},\n            {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\", \"description\": \"\"},\n            {\"number\": 3, \"rank\": 1, \"name\": \"Hannah\", \"description\": None},\n        ],\n    )\n"], "sample_177": ["    def test_abstract_base_with_m2m(self):\n        class AbstractBase(models.Model):\n            abstract_m2m = models.ManyToManyField('migrations.SomeModel')\n\n            class Meta:\n                app_label = 'migrations'\n                abstract = True\n\n        class ChildModel(AbstractBase):\n            pass\n\n        child_model_state = ModelState.from_model(ChildModel)\n        self.assertIn('abstract_m2m', child_model_state.fields)\n", "    def test_reload_model_with_circular_relationships(self):\n        project_state = ProjectState()\n        project_state.add_model(ModelState('migrations', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n            ('b', models.ForeignKey('B', models.CASCADE)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'B', [\n            ('id', models.AutoField(primary_key=True)),\n            ('a', models.ForeignKey('A', models.CASCADE)),\n        ]))\n        project_state.add_model(ModelState('migrations', 'C', [\n            ('id', models.AutoField(primary_key=True)),\n            ('a', models.ForeignKey('A', models.CASCADE)),\n            ('b', models.ForeignKey('B', models.CASCADE)),\n        ]))\n        project_state.apps  # Render project state.\n\n        project_state.reload_model('migrations', 'a', delay=True)\n        A = project_state.apps.get_model('migrations', 'A')\n        B = project_state.apps.get_model('migrations', 'B')\n        C = project_state.apps.get_model('migrations', 'C')\n        self.assertIs(A._meta.get_field('b').related_model, B)\n        self.assertIs(B._meta.get_field('a').related_model, A)\n        self.assertIs(C._meta.get_field('a').related_model, A)\n        self.assertIs(C._meta.get_field('b').related_model, B)\n", "    def test_related_models_circular(self):\n        \"\"\"\n        #25047 - Getting related models for a model in a circular dependency\n        graph should not raise a recursion error.\n        \"\"\"\n        new_apps = Apps()\n\n        class A(models.Model):\n            class Meta:\n                app_label = \"something\"\n                apps = new_apps\n\n        class B(A):\n            a = models.ForeignKey(A, models.CASCADE)\n\n            class Meta:\n                app_label = \"something\"\n                apps = new_apps\n\n        A.b = models.ForeignKey(B, models.CASCADE, null=True)\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(A))\n        project_state.add_model(ModelState.from_model(B))\n        self.assertEqual(len(project_state.apps.get_models()), 2)\n\n        # This should not raise a recursion error\n        related_models = get_related_models_recursive(A)\n        self.assertEqual(related_models, {('something', 'b')})\n\n        # This should also not raise a recursion error\n        related_models = get_related_models_recursive(B)\n        self.assertEqual(related_models, {('something', 'a')})\n", "def test_generic_foreign_key_referencing_swapped_model(self):\n    \"\"\"\n    #29379 - GenericForeignKey referencing swappable model should not break\n    the model state.\n    \"\"\"\n    new_apps = Apps()\n\n    class Content(models.Model):\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n            swappable = \"SWAP_CONTENT_MODEL\"\n\n    class TaggedItem(models.Model):\n        content_type = models.ForeignKey('contenttypes.ContentType', models.CASCADE)\n        object_id = models.PositiveIntegerField()\n        content_object = GenericForeignKey('content_type', 'object_id')\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    # Make a valid ProjectState and render it\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(Content))\n    project_state.add_model(ModelState.from_model(TaggedItem))\n    self.assertEqual(len(project_state.apps.get_models()), 2)\n\n    # The GenericForeignKey should be referencing the swappable model\n    tagged_item_model = project_state.apps.get_model(\"migrations\", \"TaggedItem\")\n    self.assertEqual(tagged_item_model._meta.get_field(\"content_object\").model, \"SWAP_CONTENT_MODEL\")\n", "    def test_index_name_after_render(self):\n        \"\"\"\n        #25833 - The index name doesn't change after a model state is rendered.\n        \"\"\"\n        new_apps = Apps()\n\n        class Tag(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n                indexes = [models.Index(fields=['name'])]\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(Tag))\n        tag_state = project_state.models['migrations', 'tag']\n        index = tag_state.options['indexes'][0]\n        self.assertEqual(index.name, 'migrations__name_720848_idx')\n\n        # Render the model state and check the index name\n        tag_model = tag_state.render(new_apps)\n        rendered_index = tag_model._meta.indexes[0]\n        self.assertEqual(rendered_index.name, 'migrations__name_720848_idx')\n\n        # Modify the index name in the model state and check it doesn't affect the rendered model\n        index.name = 'custom_name'\n        self.assertEqual(rendered_index.name, 'migrations__name_720848_idx')\n"], "sample_853": ["def test_transform_target_regressor_no_regressor():\n    # check that when no regressor is provided, a LinearRegression is used by default\n    X, y = friedman\n    regr = TransformedTargetRegressor()\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n", "def test_transform_target_regressor_default_regressor():\n    X, y = friedman\n    regr = TransformedTargetRegressor(transformer=StandardScaler())\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n", "def test_transform_target_regressor_default_regressor():\n    # check that the default regressor is LinearRegression\n    X, y = friedman\n    regr = TransformedTargetRegressor()\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n", "def test_transform_target_regressor_default_regressor():\n    X, y = friedman\n    regr = TransformedTargetRegressor(transformer=StandardScaler())\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n    y_pred = regr.predict(X)\n    assert y.shape == y_pred.shape\n    # consistency of the regressor\n    lr = LinearRegression()\n    transformer2 = clone(regr.transformer_)\n    lr.fit(X, transformer2.fit_transform(y))\n    y_lr_pred = lr.predict(X)\n    assert_allclose(y_pred, transformer2.inverse_transform(y_lr_pred))\n    assert_allclose(regr.regressor_.coef_, lr.coef_)\n", "def test_transform_target_regressor_default_regressor():\n    X, y = friedman\n    regr = TransformedTargetRegressor(transformer=StandardScaler())\n    regr.fit(X, y)\n    assert isinstance(regr.regressor_, LinearRegression)\n"], "sample_933": ["def test_build_location_and_uuid(app):\n    app.builder.build_all()\n\n    pot = (app.outdir / 'extapi.pot').read_text()\n    assert \"#: extapi.rst:10\" not in pot\n    assert \"#: extapi.rst:24\" not in pot\n    assert \"#: 12345678-1234-5678-1234-567812345678\" in pot\n    assert \"#: 87654321-4321-5678-1234-567812345678\" in pot\n", "def test_gettext_uuid(app):\n    app.builder.build_all()\n    pot = (app.outdir / 'markup.pot').read_text()\n    assert 'msgid \"something, something else, something more\"' in pot\n    assert '#: markup.rst:42' in pot\n    assert '#: markup.rst:43' in pot\n    assert '#: markup.rst:44' in pot\n    assert 'msgstr[UUID] \"\"' in pot\n", "def test_gettext_uuid(app):\n    # Test the behavior when gettext_uuid is set to True\n    app.builder.config.gettext_uuid = True\n    app.builder.build_all()\n\n    # Check if UUIDs are present in the message catalogs\n    catalog = (app.outdir / 'markup.pot').read_text()\n    assert re.search(r'\"msgid-uuid\": \"[a-f0-9]{32}\"', catalog)\n", "def test_gettext_disable_location(app):\n    app.builder.build(['index_entries'])\n\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert \"#: \" not in pot\n", "def test_build_gettext_with_uuid(app, monkeypatch):\n    # Test if UUIDs are added to the messages when gettext_uuid is True\n    monkeypatch.setattr(app.config, 'gettext_uuid', True)\n    app.builder.build_all()\n\n    # Check if UUIDs are present in the .pot file\n    pot = (app.outdir / 'extapi.pot').read_text()\n    assert re.search(r'\"#: .*:\\\\d+ \\\\n#, fuzzy\\\\n#, x-uuid: [0-9a-fA-F-]+\\\\n\"', pot)\n"], "sample_424": ["def test_references_field_by_foreign_object(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.fields.related.ForeignObject(\n            \"Other\", models.CASCADE, from_fields=[\"from\"], to_fields=[\"to\"]\n        ),\n    )\n    self.assertIs(operation.references_field(\"Model\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"to\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"from\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"to\", \"migrations\"), True)\n", "def test_rename_field_ignore_swapped(self):\n    \"\"\"\n    Tests the RenameField operation.\n    \"\"\"\n    # Test the state alteration\n    operation = migrations.RenameField(\"Pony\", \"pink\", \"color\")\n    project_state, new_state = self.make_test_state(\"test_rfligsw\", operation)\n    # Test the database alteration\n    self.assertTableNotExists(\"test_rfligsw_pony\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_rfligsw\", editor, project_state, new_state)\n    self.assertTableNotExists(\"test_rfligsw_pony\")\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_rfligsw\", editor, new_state, project_state)\n    self.assertTableNotExists(\"test_rfligsw_pony\")\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, limit_choices_to={\"field\": \"value\"}),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\",\n            models.CASCADE,\n            limit_choices_to={'field': models.F('model__field')},\n        ),\n    )\n    self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"other_field\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"other_field\", \"migrations\"), False)\n", "    def test_references_field_by_limit_choices_to(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\",\n                models.CASCADE,\n                limit_choices_to={\"field\": models.F(\"model_field\")},\n            ),\n        )\n        self.assertIs(operation.references_field(\"Model\", \"model_field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), False)\n"], "sample_326": ["def test_urlize_nofollow(self):\n    value = 'Search for google.com/?q=! and see.'\n    output = 'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_autoescape(self):\n    tests = (\n        ('Search for google.com/?q=<script> and see.',\n         'Search for <a href=\"http://google.com/?q=\">google.com/?q=&lt;script&gt;</a> and see.'),\n        ('foo@example.com', '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n", "def test_urlize_wrapping_punctuation(self):\n    tests = (\n        ('(http://example.com/)', '<a href=\"http://example.com/\">http://example.com/</a>'),\n        ('(www.example.com/)', '<a href=\"http://www.example.com/\">www.example.com/</a>'),\n        ('\"http://example.com/\"', '<a href=\"http://example.com/\">http://example.com/</a>'),\n        ('\"www.example.com/\"', '<a href=\"http://www.example.com/\">www.example.com/</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_nofollow(self):\n    tests = (\n        (\n            'Visit <a href=\"http://example.com\">example.com</a> for more.',\n            'Visit <a href=\"http://example.com\" rel=\"nofollow\">example.com</a> for more.'\n        ),\n        (\n            'Check out www.example.com/page and www.anotherexample.org.',\n            'Check out <a href=\"http://www.example.com/page\" rel=\"nofollow\">www.example.com/page</a> '\n            'and <a href=\"http://www.anotherexample.org\" rel=\"nofollow\">www.anotherexample.org</a>.'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_autoescape(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'\n        ),\n        (\n            '<script>alert(\"Hello\")</script>',\n            '&lt;script&gt;alert(&quot;Hello&quot;)&lt;/script&gt;'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n"], "sample_351": ["def test_custom_empty_label(self):\n    custom_label = 'Custom Empty Label'\n    f = forms.ModelChoiceField(Category.objects.all(), empty_label=custom_label)\n    self.assertEqual(list(f.choices)[0], ('', custom_label))\n", "def test_custom_choices(self):\n    choices = [(1, 'First'), (2, 'Second')]\n    f = forms.ModelChoiceField(queryset=Category.objects.all(), choices=choices)\n    self.assertEqual(list(f.choices), choices)\n", "def test_to_python(self):\n    f = forms.ModelChoiceField(Category.objects.all())\n    self.assertEqual(f.to_python(self.c1.pk), self.c1)\n    self.assertEqual(f.to_python(self.c2.pk), self.c2)\n    self.assertEqual(f.to_python(self.c3.pk), self.c3)\n    self.assertIsNone(f.to_python(''))\n    with self.assertRaises(ValidationError):\n        f.to_python(0)\n    with self.assertRaises(ValidationError):\n        f.to_python(999)\n", "def test_label_from_instance_method(self):\n    # Overriding label_from_instance() to return custom labels.\n    f = forms.ModelChoiceField(Category.objects.all())\n    f.label_from_instance = lambda obj: 'custom ' + str(obj.name)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'custom Entertainment'),\n        (self.c2.pk, 'custom A test'),\n        (self.c3.pk, 'custom Third'),\n    ])\n", "def test_multiple_choice_field_clean(self):\n    f = forms.ModelMultipleChoiceField(Category.objects.all())\n    self.assertEqual(list(f.clean([self.c1.id, self.c2.id])), [self.c1, self.c2])\n    with self.assertRaises(ValidationError):\n        f.clean(['invalid'])\n\n    # Test validation error message when empty list is provided\n    f.required = True\n    with self.assertRaisesMessage(ValidationError, 'This field is required.'):\n        f.clean([])\n\n    # Test validation error message when invalid value is provided\n    with self.assertRaisesMessage(ValidationError, 'Select a valid choice. 999 is not one of the available choices.'):\n        f.clean([self.c1.id, '999'])\n\n    # Test validation error message when invalid list is provided\n    with self.assertRaisesMessage(ValidationError, 'Enter a list of values.'):\n        f.clean('invalid')\n\n    # Test validation error message when invalid pk value is provided\n    with self.assertRaisesMessage(ValidationError, '\u201cinvalid\u201d is not a valid value.'):\n        f.clean([self.c1.id, 'invalid'])\n\n    # Test that clean() accepts QuerySet values\n    self.assertEqual(list(f.clean(Category.objects.filter(pk__in=[self.c1.id, self.c2.id]))), [self.c1, self.c2])\n\n    # Test that clean() accepts Model instances\n    self.assertEqual(list(f.clean([self.c1, self.c2])), [self.c1, self.c2])\n"], "sample_448": ["def test_contains_expressions(self):\n    constraint = models.UniqueConstraint(Lower(\"title\"), F(\"author\"), name=\"book_func_uq\")\n    self.assertTrue(constraint.contains_expressions)\n    constraint = models.UniqueConstraint(fields=[\"title\", \"author\"], name=\"book_fields_uq\")\n    self.assertFalse(constraint.contains_expressions)\n", "def test_deferrable_database_constraint(self):\n    obj_1 = UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"front\")\n    obj_2 = UniqueConstraintDeferrable.objects.create(name=\"p2\", shelf=\"back\")\n    obj_1.name, obj_2.name = obj_2.name, obj_1.name\n    obj_1.save()\n    with self.assertRaises(IntegrityError), atomic():\n        obj_2.save()\n", "def test_validate_expression_with_none(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    msg = \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    # Null values are ignored.\n    constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=None))\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=None),\n        exclude={\"name\"},\n    )\n", "def test_alter_deferred_unique_constraint(self):\n    obj_1 = UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"front\")\n    obj_2 = UniqueConstraintDeferrable.objects.create(name=\"p2\", shelf=\"back\")\n\n        obj_1.name, obj_2.name = obj_2.name, obj_1.name\n        obj_1.save()\n        obj_2.save()\n\n    swap()\n    # Behavior can be changed with ALTER TABLE ... SET CONSTRAINTS.\n    with self.assertRaises(IntegrityError):\n        with atomic(), connection.cursor() as cursor:\n            constraint_name = connection.ops.quote_name(\"name_init_deferred_uniq\")\n            cursor.execute(\"ALTER TABLE %s SET CONSTRAINTS %s IMMEDIATE\" % (UniqueConstraintDeferrable._meta.db_table, constraint_name))\n            swap()\n", "def test_deferrable_database_constraint(self):\n    # Test the creation of initially deferred constraint.\n    obj_1 = UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"front\")\n    obj_2 = UniqueConstraintDeferrable.objects.create(name=\"p2\", shelf=\"back\")\n\n        obj_1.name, obj_2.name = obj_2.name, obj_1.name\n        obj_1.save()\n        obj_2.save()\n\n    # Swap names without violation.\n    swap()\n\n    # Test the creation of initially immediate constraint.\n    obj_3 = UniqueConstraintDeferrable.objects.create(name=\"p3\", shelf=\"middle\")\n    obj_4 = UniqueConstraintDeferrable.objects.create(name=\"p4\", shelf=\"side\")\n    obj_3.shelf, obj_4.shelf = obj_4.shelf, obj_3.shelf\n\n    with self.assertRaises(IntegrityError), atomic():\n        obj_3.save()\n"], "sample_17": ["def test_norm_ord_none(self):\n    n = np.linalg.norm(self.q, ord=None)\n    expected = np.linalg.norm(self.q.value, ord=None) << self.q.unit\n    assert_array_equal(n, expected)\n", "def test_rec_drop_fields(self):\n    arr = rfn.rec_drop_fields(self.q_pv, 'v')\n    assert arr.dtype == np.dtype([('p', 'f8')])\n    assert arr.unit == u.km\n    assert_array_equal(arr['p'], self.q_pv['p'])\n", "def test_structured_to_unstructured(self):\n    # Test with a structured Quantity\n    unstruct = rfn.structured_to_unstructured(self.q_pv)\n    assert isinstance(unstruct, u.Quantity)\n    assert_array_equal(unstruct, [(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)] * self.pv_unit)\n\n    # Test with a nested structured Quantity\n    unstruct = rfn.structured_to_unstructured(self.q_pv_t)\n    assert isinstance(unstruct, u.Quantity)\n    assert_array_equal(unstruct, [(4.0, 2.5, 0.0), (5.0, 5.0, 1.0), (6.0, 7.5, 2.0)] * self.pv_t_unit)\n", "def test_structured_to_unstructured_nested_dtype(self):\n    # Test nested dtype\n    struct = u.Quantity([(5, (400.0, 3e6))], dtype=[(\"f1\", int), (\"f2\", self.pv_dtype)])\n    unstruct = rfn.structured_to_unstructured(struct)\n    expected = np.array([(5, 4, 3)], dtype=[(\"f1\", int), (\"f2\", float), (\"f3\", float)]) * u.m\n    assert_array_equal(unstruct, expected)\n", "def test_unravel_index(self):\n    indices = np.array([22, 41, 37])\n    out = np.unravel_index(indices, (7, 6))\n    expected = (np.array([3, 6, 6]), np.array([4, 5, 1]))\n    assert np.all(out == expected)\n"], "sample_760": ["def test_scoring_is_not_string():\n    scoring = {\"accuracy\": make_scorer(accuracy_score), \"precision\": make_scorer(precision_score)}\n    with pytest.raises(ValueError):\n        check_scoring(LogisticRegression(), scoring)\n", "def test_cluster_scorers():\n    # Test clustering scorers with random cluster assignments.\n    X, y = make_blobs(random_state=0, centers=3)\n    km = KMeans(n_clusters=3)\n    km.fit(X)\n    for name in CLUSTER_SCORERS:\n        score = get_scorer(name)(km, X, km.labels_)\n        # For the following scores, a perfect labeling should result in a score\n        # of 1.0. For the others, the best score is 0.0.\n        if name in ['adjusted_rand_score', 'mutual_info_score',\n                    'adjusted_mutual_info_score', 'normalized_mutual_info_score',\n                    'fowlkes_mallows_score']:\n            assert_almost_equal(score, 0.0)\n        else:\n            assert_almost_equal(score, 1.0)\n", "def test_scoring_is_not_list():\n    error_message = \"For evaluating multiple scores, use sklearn.model_selection.cross_validate instead.\"\n    assert_raises_regexp(ValueError, error_message, check_scoring,\n                         LinearSVC(), ['accuracy', 'precision'])\n", "def test_scoring_with_custom_callable():\n    # Test custom callable scoring function\n        return np.mean(y_true == y_pred)\n\n    X, y = make_classification(random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X, y)\n    score1 = get_scorer(custom_scorer)(clf, X, y)\n    score2 = custom_scorer(y, clf.predict(X))\n    assert_almost_equal(score1, score2)\n", "def test_scorer_threshold_with_binary_classes():\n    # Test that threshold scorers work with binary classes.\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=2, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_train, y_train)\n    score1 = get_scorer('roc_auc')(clf, X_test, y_test)\n    score2 = roc_auc_score(y_test, clf.decision_function(X_test))\n    score3 = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n    assert_almost_equal(score1, score2)\n    assert_almost_equal(score1, score3)\n"], "sample_657": ["def test_pytest_param_id_allows_unicode_characters():\n    assert pytest.param(id=\"\\u03b1\\u03b2\\u03b3\")\n", "def test_pytest_param_warning_on_known_kwargs():\n    with pytest.warns(PytestDeprecationWarning) as warninfo:\n        pytest.param(1, 2, marks=pytest.mark.xfail(), unknown_arg=42)\n    assert warninfo[0].filename == __file__\n    msg, = warninfo[0].message.args\n    assert msg == (\n        \"pytest.param() got unexpected keyword arguments: ['unknown_arg'].\\n\"\n        \"This will be an error in future versions.\"\n    )\n", "def test_parameterset_extract_from_force_tuple():\n    from _pytest.mark import ParameterSet\n\n    # Test with a single argument\n    parameterset = ParameterSet.extract_from(\"single\", force_tuple=True)\n    assert parameterset == ParameterSet(values=(\"single\",), marks=[], id=None)\n\n    # Test with multiple arguments\n    parameterset = ParameterSet.extract_from((\"multiple\", \"args\"), force_tuple=True)\n    assert parameterset == ParameterSet(values=(\"multiple\", \"args\"), marks=[], id=None)\n\n    # Test with an existing ParameterSet instance\n    existing_parameterset = ParameterSet(values=(\"existing\",), marks=[], id=None)\n    parameterset = ParameterSet.extract_from(existing_parameterset, force_tuple=True)\n    assert parameterset == existing_parameterset\n", "def test_pytest_param_for_single_argument(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize('arg', [1, pytest.param(2, id='custom_id')])\n            print(arg)\n\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"-v\")\n    result.stdout.fnmatch_lines([\"test_func[1]*\", \"*1*\", \"test_func[custom_id]*\", \"*2*\"])\n    assert result.ret == 0\n", "def test_mark_decorator_subclass_does_not_propagate_to_base_with_pytestmark(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        pytestmark = pytest.mark.a\n\n        @pytest.mark.b\n        class Test1(object):\n            pytestmark = pytest.mark.c\n                pass\n\n        class Test2(object):\n            pytestmark = pytest.mark.d\n                pass\n        \"\"\"\n    )\n    items, rec = testdir.inline_genitems(p)\n    assert_markers(items, test_foo=(\"a\", \"b\", \"c\"), test_bar=(\"d\",))\n"], "sample_346": ["    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            pass\n        self.assertTrue(a_middleware_factory.sync_capable)\n        self.assertTrue(a_middleware_factory.async_capable)\n", "def test_sync_only_middleware(self):\n    @sync_only_middleware\n            return get_response(request)\n        return middleware\n\n    self.assertTrue(a_middleware.sync_capable)\n    self.assertFalse(a_middleware.async_capable)\n", "    def test_sync_and_async_middleware_decorator(self):\n        @sync_and_async_middleware\n            return lambda view_func: view_func\n\n        self.assertTrue(a_middleware_factory.sync_capable)\n        self.assertTrue(a_middleware_factory.async_capable)\n", "    def test_classonlymethod(self):\n        class TestClass:\n            @classonlymethod\n                return cls.__name__\n\n        self.assertEqual(TestClass.class_method(), 'TestClass')\n        with self.assertRaises(AttributeError):\n            TestClass().class_method()\n", "    def test_argumented_decorator(self):\n        class Test:\n            @method_decorator(ClsDec(True), name=\"method\")\n                return False\n\n        self.assertIs(Test().method(), True)\n"], "sample_922": ["def test_pyattribute_with_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :value: 'hello'\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \" = 'hello'\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_pyfunction_signature_with_default_values(app):\n    text = \".. py:function:: hello(name='World')\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    )],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"'World'\"])])\n", "def test_pydata_without_annotation(app):\n    text = \".. py:data:: version\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, desc_name, \"version\"],\n                                  [desc_content, ()])]))\n    assert 'version' in domain.objects\n    assert domain.objects['version'] == ('index', 'version', 'data')\n", "def test_pyattribute_signature_old(app):\n    text = (\".. py:attribute:: attr\\n\"\n            \"   :annotation: = ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, \" = ''\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"attribute\",\n                domain=\"py\", objtype=\"attribute\", noindex=False)\n\n    assert 'attr' in domain.objects\n    assert domain.objects['attr'] == ('index', 'attr', 'attribute')\n", "def test_pyclass_nested_module(app):\n    text = (\".. py:module:: sphinx.ext.autodoc\\n\"\n            \"\\n\"\n            \".. py:class:: Class\\n\"\n            \"   .. py:class:: NestedClass\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[3][1][1][1], addnodes.index,\n                entries=[('single', 'NestedClass (sphinx.ext.autodoc.Class class)',\n                          'sphinx.ext.autodoc.Class.NestedClass', '', None)])\n    assert_node(doctree[3][1][1][1][1], ([desc_signature, ([desc_name, \"NestedClass\"])],\n                                         [desc_content, ()]))\n    assert 'sphinx.ext.autodoc.Class.NestedClass' in domain.objects\n    assert domain.objects['sphinx.ext.autodoc.Class.NestedClass'] == ('index',\n                                                                      'sphinx.ext.autodoc.Class.NestedClass',\n                                                                      'class')\n"], "sample_314": ["    def test_invalid_password(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'short',\n            'password2': 'short',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('This password is too short. It must contain at least 8 characters.', form.errors['password2'])\n", "def test_password_help_text_in_admin_password_change_form(self):\n    user = User.objects.get(username='testclient')\n    form = AdminPasswordChangeForm(user)\n    self.assertEqual(\n        form.fields['password1'].help_text,\n        '<ul><li>Your password can\u2019t be too similar to your other personal information.</li>'\n        '<li>Your password must contain at least 8 characters.</li></ul>'\n    )\n", "def test_password_change_with_unusable_password(self):\n    user = User.objects.get(username='empty_password')\n    user.set_unusable_password()\n    user.save()\n    data = {\n        'password1': 'test123',\n        'password2': 'test123',\n    }\n    form = AdminPasswordChangeForm(user, data)\n    self.assertTrue(form.is_valid())\n    form.save()\n    self.assertTrue(user.check_password('test123'))\n", "    def test_user_creation_with_custom_user(self):\n        data = {\n            'email': 'testclient@example.com',\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.email, 'testclient@example.com')\n", "    def test_username_max_length(self):\n        max_length = User._meta.get_field('username').max_length\n        long_username = 'a' * (max_length + 1)\n        data = {\n            'username': long_username,\n            'password1': 'testclient',\n            'password2': 'testclient',\n        }\n        form = UserCreationForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form['username'].errors), 1)\n"], "sample_656": ["def test_capturing_unicode_output(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            import sys\n            sys.stdout.write(u'\\u00f6')\n            sys.stdout.write('hello')\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--capture=sys\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *test_unicode_output*\n        *1 passed*\n    \"\"\"\n    )\n    assert result.stdout.str().startswith(u'\\u00f6hello')\n", "def test_no_carry_over_with_fixture(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            print(\"in func1\")\n\n            print(\"in func2\")\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    s = result.stdout.str()\n    assert \"in func1\" not in s\n    assert \"in func2\" in s\n", "def test_stdcapture_bytes_input(capsys):\n    # Test that writing bytes to stdout using CaptureIO works as expected\n    from _pytest.capture import CaptureIO\n    import sys\n    backup = sys.stdout\n    sys.stdout = CaptureIO()\n    sys.stdout.write(b\"bytes output\\n\")\n    out, err = capsys.readouterr()\n    assert out == \"bytes output\\n\"\n    sys.stdout = backup\n", "def test_py36_windowsconsoleio_workaround_with_stdin():\n    \"\"\"\n    Ensure _py36_windowsconsoleio_workaround function works with sys.stdin.\n    \"\"\"\n    from _pytest.capture import _py36_windowsconsoleio_workaround\n    import sys\n\n    _py36_windowsconsoleio_workaround(sys.stdin)\n", "def test_fdcapture_unicode_encoding(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n        from _pytest.capture import FDCapture\n\n            sys.stdout = FDCapture(1)\n            sys.stdout.start()\n            print(u'\\u00f6')\n            out, err = sys.stdout.readouterr()\n            sys.stdout.done()\n            assert out == u'\\u00f6\\n'\n            assert err == ''\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_453": ["def test_cell_count(self):\n    \"\"\"\n    cell_count template filter should return the correct number of cells.\n    \"\"\"\n    inline_admin_form = Mock()\n    inline_admin_form.formset.can_delete = True\n    inline_admin_form.__iter__.return_value = [\n        [\n            {\"field\": Mock(is_hidden=False)},\n            {\"field\": Mock(is_hidden=True)},\n        ],\n    ]\n    count = cell_count(inline_admin_form)\n    self.assertEqual(count, 3)\n", "def test_cell_count_hidden_field_and_delete_checkbox(self):\n    \"\"\"\n    cell_count template filter should count the hidden 'id' field and the delete checkbox.\n    \"\"\"\n    inline_admin_form = UserAdmin(User, site).get_inline_instances(self.superuser)[0]\n    count = cell_count(inline_admin_form)\n    expected_count = 1  # Hidden 'id' field\n    for fieldset in inline_admin_form:\n        for line in fieldset:\n            for field in line:\n                if not field.field.is_hidden:\n                    expected_count += 1\n    if inline_admin_form.formset.can_delete:\n        expected_count += 1  # Delete checkbox\n    self.assertEqual(count, expected_count)\n", "def test_override_delete_link(self):\n    \"\"\"\n    The delete link should be present or not based on the 'show_delete' context variable.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk]),\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    for extra_context, expected_flag in (\n        ({}, True),  # Default.\n        ({\"show_delete\": False}, False),\n    ):\n        with self.subTest(show_delete=expected_flag):\n            response = admin.change_view(\n                request,\n                str(self.superuser.pk),\n                extra_context=extra_context,\n            )\n            template_context = submit_row(response.context_data)\n            self.assertIs(\n                template_context[\"show_delete_link\"], expected_flag\n            )\n", "def test_cell_count_with_hidden_field(self):\n    \"\"\"\n    cell_count template filter should return correct count of visible cells.\n    \"\"\"\n    question = Question.objects.create(question=\"test question\", posted=datetime.date.today())\n    request = self.request_factory.get(\n        reverse(\"admin:admin_views_question_change\", args=[question.pk])\n    )\n    request.user = self.superuser\n    admin = ModelAdmin(Question, site)\n    inline = admin.get_inline_instances(request)[0]\n    inline_admin_form = inline.get_formset(request).form\n\n    # Hide one field and check that cell_count is reduced by 1\n    hidden_field_name = inline_admin_form.fields.keys()[0]\n    inline_admin_form.fields[hidden_field_name].widget = forms.HiddenInput()\n\n    count = cell_count(inline_admin_form)\n    self.assertEqual(count, len(inline_admin_form.fields) - 1)\n", "def test_cell_count_with_inline_admin_form(self):\n    \"\"\"\n    cell_count template filter should return the correct number of cells used in a tabular inline.\n    \"\"\"\n    question = Question.objects.create(question=\"Test question\")\n    request = self.request_factory.get(\n        reverse(\"admin:admin_views_question_change\", args=[question.pk])\n    )\n    request.user = self.superuser\n    admin = ModelAdmin(Question, site)\n    response = admin.change_view(request, str(question.pk))\n\n    for inline_admin_formset in response.context_data[\"inline_admin_formsets\"]:\n        for inline_admin_form in inline_admin_formset:\n            cell_count_value = cell_count(inline_admin_form)\n            # Adding 1 for the hidden 'id' field and 1 for the delete checkbox if can_delete is True\n            expected_cell_count = 1 + len([field for fieldset in inline_admin_form for line in fieldset for field in line if not field.field.is_hidden]) + int(inline_admin_form.formset.can_delete)\n            self.assertEqual(cell_count_value, expected_cell_count)\n"], "sample_171": ["def test_squashmigrations_invalid_end(self):\n    \"\"\"\n    squashmigrations doesn't accept an ending migration before the starting migration.\n    \"\"\"\n    with self.temporary_migration_module(module=\"migrations.test_migrations_no_changes\"):\n        msg = (\n            \"The migration 'migrations.0001_initial' cannot be found. Maybe \"\n            \"it comes before the migration 'migrations.0002_second'\"\n        )\n        with self.assertRaisesMessage(CommandError, msg):\n            call_command(\"squashmigrations\", \"migrations\", \"0002\", \"0001\", interactive=False, verbosity=0)\n", "def test_makemigrations_multiple_apps(self):\n    \"\"\"\n    makemigrations handles multiple app labels.\n    \"\"\"\n    out = io.StringIO()\n    with self.temporary_migration_module(module=\"migrations.test_migrations\"):\n        call_command(\"makemigrations\", \"migrations\", \"migrations2\", stdout=out)\n    self.assertIn(\"Migrations for 'migrations'\", out.getvalue())\n    self.assertIn(\"Migrations for 'migrations2'\", out.getvalue())\n", "def test_makemigrations_custom_name_with_squash(self):\n    \"\"\"\n    makemigrations --name generates a custom migration name with squash.\n    \"\"\"\n    with self.temporary_migration_module(module=\"migrations.test_migrations\") as migration_dir:\n        call_command(\"squashmigrations\", \"migrations\", \"0002\", squashed_name=\"custom_squash\", interactive=False, verbosity=0)\n\n        squashed_migration_file = os.path.join(migration_dir, \"0001_custom_squash.py\")\n        self.assertTrue(os.path.exists(squashed_migration_file))\n", "def test_makemigrations_squashed_name_invalid(self):\n    \"\"\"--squashed-name should raise an error with an invalid name.\"\"\"\n    squashed_name = 'invalid name'\n    with self.temporary_migration_module(module=\"migrations.test_migrations\"):\n        with self.assertRaisesMessage(CommandError, 'The migration name must be a valid Python identifier.'):\n            call_command(\"squashmigrations\", \"migrations\", \"0001\", squashed_name=squashed_name, interactive=False, verbosity=0)\n", "def test_makemigrations_check_with_changes(self):\n    \"\"\"\n    makemigrations --check should exit with a non-zero status when\n    there are changes to an app requiring migrations and check is True.\n    \"\"\"\n    with self.temporary_migration_module():\n        with self.assertRaises(SystemExit):\n            call_command(\"makemigrations\", \"--check\", \"migrations\", verbosity=0)\n\n    # Reset the migration module to a state with no changes\n    with self.temporary_migration_module(module=\"migrations.test_migrations_no_changes\"):\n        call_command(\"makemigrations\", \"--check\", \"migrations\", verbosity=0)\n"], "sample_1208": ["def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy is not installed. Abort tests for _sample_numpy.')\n    else:\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n", "def test_MatrixPSpace_sample():\n    M = MatrixGammaDistribution(1, 2, [[2, 1], [1, 2]])\n    MP = MatrixPSpace('M', M, 2, 2)\n    samps = MP.sample(size=5)\n    for sam in samps.values():\n        assert Matrix(sam) in MP.distribution.set\n", "def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy is not installed. Abort tests for _sample_numpy.')\n    else:\n        # TODO: Add distributions to sample from numpy once numpy_rv_map is updated\n        pass\n", "def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy is not installed. Abort tests for _sample_numpy.')\n    else:\n        # Currently, numpy_rv_map is empty in the code file,\n        # so we can't test any specific distribution here.\n        # Once distributions are added to numpy_rv_map, we can test them here.\n        pass\n", "def test_sample_numpy():\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy is not installed. Abort tests for _sample_numpy.')\n    else:\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n"], "sample_1164": ["def test_sho1d():\n    ad = RaisingOp('a')\n    assert pretty(ad) == ' \\N{DAGGER}\\na '\n    assert latex(ad) == 'a^{\\\\dagger}'\n", "def test_sho1d():\n    ad = RaisingOp('a')\n    assert str(ad) == 'ad'\n    ascii_str = \\", "def test_cg_simp():\n    a = symbols('a')\n    alpha = symbols('alpha')\n    b = symbols('b')\n    c = symbols('c')\n    e = cg_simp(Sum(CG(a, alpha, b, 0, a, alpha), (alpha, -a, a)))\n    assert e == ((2*a + 1)*KroneckerDelta(b, 0))\n", "def test_cg_simp():\n    a = symbols('a', positive=True, integer=True)\n    cg1 = CG(a, a, 0, 0, a, a)\n    cg2 = CG(a, -a, 0, 0, a, -a)\n    cg3 = CG(a, a, a, -a, a, 0)\n    cg4 = CG(a, -a, a, a, a, 0)\n    cg5 = CG(a, a, 0, 0, a, a)\n    cg6 = CG(a, -a, 0, 0, a, -a)\n    cg7 = CG(a, a, a, -a, a, 0)\n    cg8 = CG(a, -a, a, a, a, 0)\n\n    assert cg_simp(cg1 + cg2) == 2*a + 1\n    assert cg_simp(cg3 + cg4) == sqrt(2*a + 1)\n    assert cg_simp(Sum(cg5, (cg5.m1, -a, a))) == 2*a + 1\n    assert cg_simp(Sum(cg6, (cg6.m1, -a, a))) == 2*a + 1\n    assert cg_simp(Sum(cg7, (cg7.m3, -a, a))) == sqrt(2*a + 1)\n    assert cg_simp(Sum(cg8, (cg8.m3, -a, a))) == sqrt(2*a + 1)\n", "def test_cg_simp():\n    cg = CG(S(1)/2, S(1)/2, S(1)/2, -S(1)/2, 1, 1)\n    assert cg_simp(cg) == sqrt(3)/2\n\n    cg1 = CG(j1=S(1)/2, m1=-S(1)/2, j2=S(1)/2, m2=+S(1)/2, j3=1, m3=0)\n    assert cg_simp(cg1) == sqrt(2)/2\n\n    cg2 = CG(1, 1, 0, 0, 1, 1)\n    cg3 = CG(1, 0, 0, 0, 1, 0)\n    cg4 = CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(cg2 + cg3 + cg4) == 3\n\n    cg5 = CG(1, 1, 2, 2, 3, 3)\n    cg6 = CG(1, 1, 2, 1, 3, 2)\n    cg7 = CG(1, 1, 2, 0, 3, 1)\n    assert cg_simp(cg5 + cg6 + cg7) == KroneckerDelta(1, 1)\n"], "sample_1122": ["def test_issue_15893_complex_function():\n    f = Function('f', complex=True)\n    x = Symbol('x', real=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == f(x)/Abs(f(x))\n", "def test_abs_zero_assumption():\n    z = Symbol('z', complex=True, zero=True)\n    assert Abs(z).is_zero is True\n    assert Abs(z).is_positive is False\n    assert Abs(z).is_extended_positive is False\n", "def test_sign_complex_number():\n    # Test sign function for complex numbers\n    assert sign(2 + 3j) == (2 + 3j) / sqrt(13)\n    assert sign(-2 + 3j) == (-2 + 3j) / sqrt(13)\n    assert sign(2 - 3j) == (2 - 3j) / sqrt(13)\n    assert sign(-2 - 3j) == (-2 - 3j) / sqrt(13)\n    assert sign(0 + 3j) == I\n    assert sign(0 - 3j) == -I\n    assert sign(2 + 0j) == 1\n    assert sign(-2 + 0j) == -1\n", "def test_zero_extended_real_assumptions():\n    z = Symbol('z', extended_real=False, finite=True)\n\n    assert re(z).is_zero is None\n    assert im(z).is_zero is None\n    assert Abs(z).is_zero is None\n    assert sign(z).is_zero is None\n", "def test_issue_15893_complex_function():\n    f = Function('f', complex=True)\n    x = Symbol('x', complex=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x))/2\n"], "sample_78": ["def test_call_command_with_invalid_verbosity_level(self):\n    msg = \"argument --verbosity: invalid choice: 4 (choose from 0, 1, 2, 3)\"\n    with self.assertRaisesMessage(SystemExit, msg):\n        management.call_command('dance', verbosity=4)\n", "def test_no_translations_activate_translations(self):\n    \"\"\"\n    When the Command handle method is not decorated with @no_translations,\n    translations are not deactivated inside the command.\n    \"\"\"\n    current_locale = translation.get_language()\n    with translation.override('pl'):\n        result = management.call_command('no_translations_off', stdout=StringIO())\n        self.assertIsNone(result)\n    self.assertEqual(translation.get_language(), 'pl')\n", "    def test_no_color_and_force_color_together(self):\n        with self.assertRaises(CommandError):\n            BaseCommand(no_color=True, force_color=True)\n", "def test_check_command_error(self):\n    with mock.patch('user_commands.management.commands.dance.Command.check') as mock_check:\n        mock_check.side_effect = CommandError(\"Test error\")\n        with self.assertRaises(CommandError):\n            management.call_command('dance')\n", "    def test_check_no_issues(self):\n        with mock.patch.object(BaseCommand, '_run_checks', return_value=[]):\n            out = StringIO()\n            management.call_command('dance', stdout=out, skip_checks=False)\n            self.assertNotIn(\"System check identified some issues:\", out.getvalue())\n"], "sample_882": ["def test_mlp_partial_fit_early_stopping():\n    \"\"\"Check partial fit with early stopping works as expected.\"\"\"\n    mlp = MLPClassifier(early_stopping=True, random_state=0)\n    mlp.partial_fit(X_iris[:50], y_iris[:50], classes=np.unique(y_iris))\n    mlp.partial_fit(X_iris[50:], y_iris[50:])\n    assert mlp.n_iter_ < 200\n", "def test_mlp_solver_parameter(MLPEstimator):\n    \"\"\"Check that the solver parameter is validated correctly.\"\"\"\n    with pytest.raises(ValueError, match=\"Invalid solver. Expected one of\"):\n        MLPEstimator(solver=\"invalid\")\n", "def test_mlp_batch_size_auto(MLPEstimator):\n    \"\"\"Check that the default batch size is 200 or the number of samples when it is smaller.\"\"\"\n    X_small = np.random.rand(100, 20)\n    y_small = np.random.rand(100)\n    X_large = np.random.rand(300, 20)\n    y_large = np.random.rand(300)\n\n    mlp_small = MLPEstimator(batch_size=\"auto\", random_state=0).fit(X_small, y_small)\n    mlp_large = MLPEstimator(batch_size=\"auto\", random_state=0).fit(X_large, y_large)\n\n    assert mlp_small.batch_size_ == 100\n    assert mlp_large.batch_size_ == 200\n", "def test_mlp_different_classes_partial_fit():\n    \"\"\"Check that partial fit works when classes change.\"\"\"\n    X = np.array([[0], [1], [2]])\n    y = np.array([0, 1, 2])\n\n    mlp = MLPClassifier(solver=\"sgd\")\n    mlp.partial_fit(X, y, classes=y)\n\n    y_new = np.array([3, 4, 5])\n    mlp.partial_fit(X, y_new, classes=np.unique(np.concatenate((y, y_new))))\n\n    assert mlp.classes_.tolist() == [0, 1, 2, 3, 4, 5]\n", "def test_mlp_sparse_input_with_early_stopping():\n    \"\"\"Check that early stopping works with sparse input.\"\"\"\n    X_sparse = csr_matrix(X_iris)\n    mlp = MLPClassifier(early_stopping=True, random_state=0).fit(X_sparse, y_iris)\n    assert hasattr(mlp, 'validation_scores_')\n"], "sample_347": ["def test_localtime(self):\n    naive = datetime.datetime(2015, 1, 1, 0, 0, 1)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n\n    aware = datetime.datetime(2015, 1, 1, 0, 0, 1, tzinfo=ICT)\n    self.assertEqual(timezone.localtime(aware, timezone=EAT), datetime.datetime(2014, 12, 31, 19, 0, 1, tzinfo=EAT))\n    with timezone.override(EAT):\n        self.assertEqual(timezone.localtime(aware), datetime.datetime(2014, 12, 31, 19, 0, 1, tzinfo=EAT))\n\n    with mock.patch('django.utils.timezone.now', return_value=aware):\n        self.assertEqual(timezone.localtime(timezone=EAT), datetime.datetime(2014, 12, 31, 19, 0, 1, tzinfo=EAT))\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(), datetime.datetime(2014, 12, 31, 19, 0, 1, tzinfo=EAT))\n", "def test_localtime_naive_datetime(self):\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n", "def test_get_current_timezone(self):\n    default = timezone.get_default_timezone()\n\n    # Test that get_current_timezone returns the default timezone\n    # when no timezone has been activated\n    self.assertIs(timezone.get_current_timezone(), default)\n\n    # Test that get_current_timezone returns the activated timezone\n    try:\n        timezone.activate(ICT)\n        self.assertIs(timezone.get_current_timezone(), ICT)\n    finally:\n        timezone.deactivate()\n", "def test_localtime_naive(self):\n    naive = datetime.datetime(2015, 1, 1, 0, 0, 1)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n", "def test_localtime(self):\n    naive = datetime.datetime(2015, 1, 1, 0, 0, 1)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive)\n    with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n        timezone.localtime(naive, timezone=EAT)\n\n    aware = datetime.datetime(2015, 1, 1, 0, 0, 1, tzinfo=ICT)\n    self.assertEqual(timezone.localtime(aware, timezone=EAT), aware - datetime.timedelta(hours=4))\n    with timezone.override(EAT):\n        self.assertEqual(timezone.localtime(aware), aware - datetime.timedelta(hours=4))\n\n    with mock.patch('django.utils.timezone.now', return_value=aware):\n        self.assertEqual(timezone.localtime(timezone=EAT), aware - datetime.timedelta(hours=4))\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(), aware - datetime.timedelta(hours=4))\n"], "sample_397": ["def test_template_does_not_exist(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {},\n        }\n    )\n    with self.assertRaises(TemplateDoesNotExist):\n        engine.get_template(\"nonexistent_template.html\")\n", "def test_template_origin(self):\n    \"\"\"The template origin is set correctly.\"\"\"\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [Path(__file__).parent / \"templates\" / \"template_backends\"],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {},\n        }\n    )\n    template = engine.get_template(\"hello.html\")\n    self.assertEqual(\n        template.origin.name,\n        str(Path(__file__).parent / \"templates\" / \"template_backends\" / \"hello.html\"),\n    )\n", "def test_template_builtins(self):\n    \"\"\"\n    Test that template builtins are correctly imported and added to the engine.\n    \"\"\"\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"builtins\": [\"template_backends.apps.good.templatetags.good_tags\"],\n            },\n        }\n    )\n\n    # Check that the default builtins are present\n    self.assertIn(\"django.template.defaulttags\", engine.engine.builtins)\n    self.assertIn(\"django.template.defaultfilters\", engine.engine.builtins)\n    self.assertIn(\"django.template.loader_tags\", engine.engine.builtins)\n\n    # Check that the custom builtin is present\n    self.assertIn(\"template_backends.apps.good.templatetags.good_tags\", engine.engine.builtins)\n\n    # Check that the custom builtin is correctly imported\n    self.assertEqual(\n        engine.engine.template_builtins[-1].__name__,\n        \"template_backends.apps.good.templatetags.good_tags\",\n    )\n", "def test_string_if_invalid(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"string_if_invalid\": \"Invalid\",\n            },\n        }\n    )\n    self.assertEqual(engine.engine.string_if_invalid, \"Invalid\")\n\n    template = engine.from_string(\"Hello, {{ name }}\")\n    self.assertEqual(template.render({}), \"Hello, Invalid\")\n", "def test_custom_loaders(self):\n    custom_loader = \"template_backends.apps.custom_loader.Loader\"\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"loaders\": [custom_loader],\n            },\n        }\n    )\n    self.assertEqual(engine.engine.loaders, [custom_loader])\n"], "sample_390": ["    def test_directory_index_translation(self):\n        with override_settings(USE_I18N=True, LANGUAGE_CODE='de'):\n            response = self.client.get(\"/%s/\" % self.prefix)\n            self.assertContains(response, \"Verzeichnis von ./\")\n", "def test_directory_index(self):\n    response = self.client.get(\"/%s/\" % self.prefix)\n    self.assertContains(response, \"<title>Index of /</title>\")\n    self.assertContains(response, \"<h1>Index of /</h1>\")\n    self.assertContains(response, \"<li><a href=\\\"subdir/\\\">subdir/</a></li>\")\n    self.assertContains(response, \"<li><a href=\\\"file.txt\\\">file.txt</a></li>\")\n    self.assertNotContains(response, \"<li><a href=\\\".hidden\\\">.hidden</a></li>\")\n", "    def test_directory_index_template_context(self):\n        file_name = \"file.txt\"\n        response = self.client.get(\"/%s/\" % self.prefix)\n        self.assertEqual(response.context[\"directory\"], \"./\")\n        self.assertIn(file_name, response.context[\"file_list\"])\n", "    def test_directory_index_translation(self):\n        response = self.client.get(\"/%s/\" % self.prefix)\n        self.assertContains(response, \"Index of ./\")\n        with self.settings(LANGUAGE_CODE='de'):\n            response = self.client.get(\"/%s/\" % self.prefix)\n            self.assertContains(response, \"Index of ./\")  # Should still be English, as no translation is provided\n", "    def test_directory_listing_disabled(self):\n        file_name = \"file.txt\"\n        url = \"/%s/subdir/%s\" % (self.prefix, file_name)\n        with self.settings(STATIC_URL=self.prefix, DEBUG=True):\n            response = self.client.get(url, follow=True)\n            self.assertEqual(404, response.status_code)\n"], "sample_386": ["def test_safe_string_equality(self):\n    s1 = SafeString(\"a&b\")\n    s2 = SafeString(\"a&b\")\n    self.assertEqual(s1, s2)\n    self.assertNotEqual(s1, \"a&b\")\n", "def test_mark_safe_result_concatenation(self):\n    s1 = mark_safe(\"a&\")\n    s2 = mark_safe(\"b\")\n    s3 = s1 + s2\n    self.assertIsInstance(s3, SafeString)\n    self.assertEqual(s3, \"a&b\")\n", "def test_safe_string_concatenation_with_non_safe_data(self):\n    s = SafeString(\"a&b\")\n    non_safe_s = \"c<d\"\n    concatenated_s = s + non_safe_s\n\n    self.assertIsInstance(concatenated_s, str)\n    self.assertNotIsInstance(concatenated_s, SafeString)\n    self.assertRenderEqual(\"{{ s }}\", \"a&bcd&lt;d\", s=concatenated_s)\n", "def test_mark_safe_idempotency(self):\n    \"\"\"\n    Calling mark_safe() on a SafeString instance doesn't change the instance.\n    \"\"\"\n    s = mark_safe(\"a&b\")\n    self.assertIs(mark_safe(s), s)\n", "    def test_mark_safe_and_escape_interaction(self):\n        \"\"\"\n        mark_safe and escape() should play nicely with each other.\n        \"\"\"\n        s = mark_safe(\"<b>bold</b>\")\n        s_escaped = html.escape(s)\n\n        self.assertIsInstance(s_escaped, SafeString)\n        self.assertRenderEqual(\"{{ s_escaped }}\", \"&lt;b&gt;bold&lt;/b&gt;\", s_escaped=s_escaped)\n\n        s_escaped_again = html.escape(s_escaped)\n\n        self.assertIs(s_escaped_again, s_escaped)\n"], "sample_119": ["def test_query_with_annotations(self):\n    query = Query(Author)\n    query.add_annotation(Sum('num'), alias='total_num', is_summary=True)\n    where = query.build_where(Q(total_num__gt=10))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 10)\n    self.assertEqual(lookup.lhs.alias, 'total_num')\n", "def test_complex_query_with_transform(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where((Q(num__gt=2) & Q(name__lower='foo')) | Q(num__lt=0))\n    self.assertEqual(where.connector, OR)\n\n    lookup1, lookup2 = where.children[0].children\n    self.assertIsInstance(lookup1, GreaterThan)\n    self.assertEqual(lookup1.rhs, 2)\n    self.assertEqual(lookup1.lhs.target, Author._meta.get_field('num'))\n\n    self.assertIsInstance(lookup2, Exact)\n    self.assertIsInstance(lookup2.lhs, Lower)\n    self.assertIsInstance(lookup2.lhs.lhs, SimpleCol)\n    self.assertEqual(lookup2.lhs.lhs.target, Author._meta.get_field('name'))\n\n    lookup = where.children[1]\n    self.assertIsInstance(lookup, LessThan)\n    self.assertEqual(lookup.rhs, 0)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_date_field_transform(self):\n    query = Query(Item)\n    with register_lookup(DateField, Year):\n        where = query.build_where(Q(created__year=2018))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Year)\n    self.assertIsInstance(lookup.lhs.lhs, SimpleCol)\n    self.assertEqual(lookup.lhs.lhs.target, Item._meta.get_field('created'))\n", "def test_complex_query_with_transform(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where((Q(num__gt=2) | Q(name__lower='foo')) & ~Q(id__lt=10))\n    self.assertEqual(where.connector, AND)\n\n    disjunction = where.children[0]\n    self.assertIsInstance(disjunction, Q)\n    self.assertEqual(disjunction.connector, OR)\n    num_lookup = disjunction.children[0]\n    name_lookup = disjunction.children[1]\n    self.assertIsInstance(num_lookup, GreaterThan)\n    self.assertIsInstance(name_lookup, Exact)\n    self.assertIsInstance(name_lookup.lhs, Lower)\n    self.assertIsInstance(name_lookup.lhs.lhs, SimpleCol)\n\n    negation = where.children[1]\n    self.assertIsInstance(negation, Q)\n    self.assertTrue(negation.negated)\n    id_lookup = negation.children[0]\n    self.assertIsInstance(id_lookup, LessThan)\n    self.assertIsInstance(id_lookup.lhs, SimpleCol)\n", "def test_lookup_expression_in_query(self):\n    query = Query(Author)\n    where = query.build_where(Q(num__in=[1, 2, 3]))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, In)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n    self.assertEqual(lookup.rhs, [1, 2, 3])\n"], "sample_881": ["def test_label_ranking_avg_precision_score_should_allow_csr_matrix_for_y_score_input():\n    # Test that label_ranking_avg_precision_score accept sparse y_score.\n    # Non-regression test for #22575\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = csr_matrix([[0.5, 0.9, 0.6], [0, 0, 1]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(2 / 3)\n", "def test_label_ranking_avg_precision_score_with_invalid_pos_label():\n    # Test that label_ranking_avg_precision_score raises an error with an invalid pos_label.\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n    pos_label = 2\n    with pytest.raises(ValueError, match=r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\"):\n        label_ranking_average_precision_score(y_true, y_score, pos_label=pos_label)\n", "def test_label_ranking_avg_precision_score_zero_or_all_relevant_labels():\n    # Degenerate case: only one label\n    assert_almost_equal(\n        label_ranking_average_precision_score([[1], [0], [1], [0]], [[0.5], [0.5], [0.5], [0.5]]), 1.0\n    )\n\n    # Zero or all relevant labels\n    y_true = np.array([[1, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1]])\n    y_score = np.array([[0.75, 0.25, 0.1, 0.1], [0.2, 0.3, 0.4, 0.1], [0.1, 0.1, 0.1, 0.7]])\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), 0.6666666666666666)\n", "def test_ranking_metric_multiple_labels(metric, pos_label):\n    \"\"\"Check that the metric raises a ValueError when `y_true` contains more than two labels.\n\n    We expect the metric to raise a ValueError if `y_true` contains more than two labels.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    n_samples = 10\n    y_true = rng.choice([0, 1, 2], size=n_samples, replace=True)\n    y_proba = rng.rand(n_samples)\n    with pytest.raises(ValueError, match=\"Parameter 'pos_label' is fixed to 1 for multilabel-indicator y_true.\"):\n        metric(y_true, y_proba, pos_label=pos_label)\n", "def test_label_ranking_average_precision_score_values(y_true, y_score, expected_lrap):\n    lrap = label_ranking_average_precision_score(y_true, y_score)\n    assert_almost_equal(lrap, expected_lrap)\n"], "sample_832": ["def test_bayesian_ridge_normalize():\n    # Test BayesianRidge with normalize=True\n    X = np.array([[1.], [2.], [3.], [4.], [5.]])\n    y = np.array([2., 4., 5., 4., 5.])\n    clf = BayesianRidge(compute_score=True, normalize=True)\n    clf.fit(X, y)\n\n    # Check that the model could approximately learn the doubling function\n    test = [[1.5], [2.5], [3.5]]\n    assert_array_almost_equal(clf.predict(test), [3., 5., 7.], 2)\n", "def test_bayesian_ridge_no_intercept():\n    # Test BayesianRidge with fit_intercept=False\n    X = np.array([[1, 0], [2, 1], [3, 2]])\n    y = np.array([1, 3, 5])  # y = 1*x[:, 0] + 2*x[:, 1]\n    clf = BayesianRidge(fit_intercept=False)\n    clf.fit(X, y)\n    # Check that the intercept is zero\n    assert_almost_equal(clf.intercept_, 0.)\n    # Check that the coefficients are close to the true values\n    assert_array_almost_equal(clf.coef_, [1, 2], decimal=1)\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with fit_intercept=True\n    X = np.vander(np.linspace(0, 4, 5), 4)\n    y = np.array([1., 2., 3., -1., 0.])    # y = (x^3 - 6x^2 + 8x) / 3 + 1\n\n    reg = BayesianRidge(alpha_init=1., lambda_init=1e-3, fit_intercept=True)\n    # Check the R2 score nearly equals to one.\n    r2 = reg.fit(X, y).score(X, y)\n    assert_almost_equal(r2, 1.)\n", "def test_ard_threshold_lambda():\n    # Test the threshold_lambda parameter of ARDRegression\n    X = np.array([[1], [2], [3]])\n    Y = np.array([1, 2, 3])\n    clf = ARDRegression(threshold_lambda=0.5)\n    clf.fit(X, Y)\n\n    # Check that the model only keeps the first feature (with the highest lambda)\n    assert_array_almost_equal(clf.coef_, [1, 0, 0])\n", "def test_bayesian_ridge_prediction():\n    # Test BayesianRidge prediction with return_std=True\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([3, 7, 11, 15])\n    clf = BayesianRidge()\n    clf.fit(X, y)\n\n    # Predicting with return_std=True should return both mean and standard deviation\n    y_mean, y_std = clf.predict(X, return_std=True)\n    assert y_mean.shape == y.shape\n    assert y_std.shape == y.shape\n    assert np.all(y_std >= 0)  # Standard deviation should be non-negative\n"], "sample_231": ["def test_template_loader_postmortem_with_loader(self):\n    \"\"\"Tests for not existing file with loader\"\"\"\n    template_name = \"notfound.html\"\n    with tempfile.NamedTemporaryFile(prefix=template_name) as tmpfile:\n        tempdir = os.path.dirname(tmpfile.name)\n        template_path = os.path.join(tempdir, template_name)\n        with override_settings(TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [tempdir],\n            'OPTIONS': {\n                'loaders': [\n                    ('django.template.loaders.filesystem.Loader', (tempdir,)),\n                ],\n            },\n        }]), self.assertLogs('django.request', 'ERROR'):\n            response = self.client.get(reverse('raises_template_does_not_exist', kwargs={\"path\": template_name}))\n        self.assertContains(response, \"%s (Source does not exist)\" % template_path, status_code=500, count=2)\n        # Assert as HTML.\n        self.assertContains(\n            response,\n            '<li><code>django.template.loaders.filesystem.Loader</code>: '\n            '%s (Source does not exist)</li>' % template_path,\n            status_code=500,\n            html=True,\n        )\n", "def test_sensitive_variables_decorator_with_all_variables(self):\n    \"\"\"\n    The sensitive_variables decorator can be used to mark all variables\n    as sensitive.\n    \"\"\"\n    request = self.rf.post('/some_url/', self.breakfast_data)\n\n    @sensitive_variables('__ALL__')\n        cooked_eggs = 'scrambled'\n        sauce = 'worcestershire'\n        return index_page(request)\n\n    with self.settings(DEBUG=True):\n        response = test_func(request)\n        self.assertNotContains(response, 'scrambled', status_code=500)\n        self.assertNotContains(response, 'worcestershire', status_code=500)\n\n    with self.settings(DEBUG=False):\n        response = test_func(request)\n        self.assertNotContains(response, 'scrambled', status_code=500)\n        self.assertNotContains(response, 'worcestershire', status_code=500)\n", "    def test_cleanse_setting_calls_callable(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        setting_value = CallableSettingWrapper(lambda: \"This should not be displayed\")\n        cleansed_value = reporter_filter.cleanse_setting('SETTING_NAME', setting_value)\n        self.assertIsInstance(cleansed_value, CallableSettingWrapper)\n        self.assertEqual(str(cleansed_value), \"CallableSettingWrapper(<function <lambda> at ...>)\")\n", "def test_exception_reporter_get_safe_settings(self):\n    request = self.rf.get('/test_view/')\n    reporter = ExceptionReporter(request, None, None, None)\n    settings_dict = reporter.filter.get_safe_settings()\n    self.assertIn('DEBUG', settings_dict)\n    self.assertEqual(settings_dict['DEBUG'], True)\n    self.assertIn('SECRET_KEY', settings_dict)\n    self.assertNotEqual(settings_dict['SECRET_KEY'], settings.SECRET_KEY)\n", "def test_unicode_decode_error_in_source(self):\n    \"\"\"\n    UnicodeDecodeError should not be raised when loading the source code of a\n    file with unknown encoding.\n    \"\"\"\n    # Create a file with a non-UTF-8 encoding\n    fd, filename = tempfile.mkstemp(text=False)\n    os.write(fd, b'\\xff')\n    os.close(fd)\n\n    # Mock the _get_source() method to return this file\n    with mock.patch(\n        'django.views.debug.ExceptionReporter._get_source',\n        return_value=[b'\\xff' for _ in range(7)],\n    ):\n        request = self.rf.get('/test_view/')\n        try:\n            raise ValueError('Oops')\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn(\n            '<span class=\"fname\">generated</span>, line 2, in test_func',\n            html,\n        )\n\n    # Clean up\n    os.unlink(filename)\n"], "sample_1019": ["def test_issue_8263_mask_nc():\n    F, G = symbols('F, G', commutative=False, cls=Function)\n    x, y = symbols('x, y')\n    expr, dummies, _ = _mask_nc(F(x)*G(y) - G(y)*F(x))\n    assert expr == dummies[F(x)*G(y)] - dummies[G(y)*F(x)]\n    assert len(dummies) == 2\n    assert all(not v.is_commutative for v in dummies.values())\n    assert not expr.is_zero\n", "def test_issue_10000():\n    x = symbols('x')\n    assert _monotonic_sign(exp(x)).is_positive\n", "def test_issue_8393():\n    a, b, c, d, e, f, g = symbols('a b c d e f g')\n    expr = a*b*c - a*c*d - a*b*e + a*f*g\n    assert factor_terms(expr) == a*(b*c - c*d - b*e + f*g)\n", "def test_issue_8311():\n    x = Symbol('x', real=True)\n    assert _monotonic_sign(x**2) == 1\n    assert _monotonic_sign(x**3) == 1\n    assert _monotonic_sign(x**4) == 1\n    assert _monotonic_sign(-x**2) is None\n    assert _monotonic_sign(-x**3).is_negative\n    assert _monotonic_sign(-x**4) == 1\n    assert _monotonic_sign(sqrt(x)) == 1\n    assert _monotonic_sign(sqrt(x**2)) == 1\n    assert _monotonic_sign(-sqrt(x)) is None\n    assert _monotonic_sign(-sqrt(x**2)) is None\n", "def test_issue_11254():\n    x, y, z = symbols('x y z')\n    assert factor_nc(x*(y + z) - z*(x + y)) == (x - z)*(x + y)\n"], "sample_21": ["def test_read_write_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"b\", data=[0.1, 0.2, 0.3]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"serr\": [2]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n    assert np.all(t2[\"a\"] == t1[\"a\"])\n    assert np.allclose(t2[\"b_err\"], t1[\"b\"])\n", "def test_write_table_qdp_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.add_column(Column(name=\"b\", data=[4, 5, 6]))\n    t1.add_column(Column(name=\"b_perr\", data=[0.4, 0.5, 0.6]))\n    t1.add_column(Column(name=\"b_nerr\", data=[0.7, 0.8, 0.9]))\n\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'serr': [2], 'terr': [4, 5]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n    assert np.allclose(t2[\"b_perr\"], t1[\"b_perr\"])\n    assert np.allclose(t2[\"b_nerr\"], t1[\"b_nerr\"])\n", "def test_read_write_simple_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"serr\": [2]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n", "def test_read_write_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.add_column(Column(name=\"b\", data=[4, 5, 6]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"serr\": [2]})\n    t2 = Table.read(test_file, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n", "def test_read_write_simple_with_errors(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3, 4]))\n    t1.add_column(Column(name=\"a_perr\", data=[0.1, 0.2, 0.3, 0.4]))\n    t1.add_column(Column(name=\"a_nerr\", data=[0.05, 0.1, 0.15, 0.2]))\n    t1.add_column(Column(name=\"b\", data=[4.0, 5.0, 6.0, 7.0]))\n    t1.add_column(Column(name=\"b_err\", data=[0.4, 0.5, 0.6, 0.7]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [4]})\n    t2 = Table.read(test_file, format=\"ascii.qdp\")\n\n    for col in t1.colnames:\n        assert col in t2.colnames\n        assert np.allclose(t1[col], t2[col])\n"], "sample_765": ["def test_balanced_accuracy_score_multiclass():\n    # Test balanced accuracy score for multiclass classification task\n    y_true, y_pred, _ = make_prediction(binary=False)\n\n    # compute scores with default labels introspection\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    macro_recall = recall_score(y_true, y_pred, average='macro', labels=np.unique(y_true))\n    assert_almost_equal(balanced_acc, macro_recall, 2)\n\n    # compute scores with explicit label ordering\n    balanced_acc = balanced_accuracy_score(y_true, y_pred, labels=[2, 1, 0])\n    macro_recall = recall_score(y_true, y_pred, average='macro', labels=[2, 1, 0])\n    assert_almost_equal(balanced_acc, macro_recall, 2)\n", "def test_balanced_accuracy_score_multiclass(y_true, y_pred):\n    macro_recall = recall_score(y_true, y_pred, average='macro',\n                                labels=np.unique(y_true))\n    with ignore_warnings():\n        balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == pytest.approx(macro_recall)\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    chance = balanced_accuracy_score(y_true, np.full_like(y_true, y_true[0]))\n    assert adjusted == (balanced - chance) / (1 - chance)\n", "def test_balanced_accuracy_score_samples():\n    y_true = np.array([[1, 0], [1, 0], [0, 1], [0, 1]])\n    y_pred = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=[1, 1, 2, 2])\n    assert balanced == 0.5\n", "def test_balanced_accuracy_score_multiclass_ignore_labels():\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 2, 1, 0, 1, 2])\n    labels = [0, 1]  # only consider labels 0 and 1\n    balanced = balanced_accuracy_score(y_true, y_pred, labels=labels)\n    # expected balanced accuracy: (1/3 + 1/3) / 2 = 0.333333\n    assert balanced == pytest.approx(0.333333, abs=1e-5)\n", "def test_balanced_accuracy_score_errors():\n    # Test for error cases in balanced_accuracy_score\n    y_true = ['a', 'b', 'c']\n    y_pred = ['a', 'b', 'd']\n    with pytest.raises(ValueError, match='The number of classes has to be greater than 1'):\n        balanced_accuracy_score(y_true, y_pred)\n\n    y_true = ['a', 'a', 'a']\n    y_pred = ['a', 'a', 'a']\n    with pytest.raises(ValueError, match='All labels in y_true should be present in y_pred'):\n        balanced_accuracy_score(y_true, y_pred)\n\n    y_true = ['a', 'b', 'c']\n    y_pred = ['a', 'b', 'c', 'd']\n    with pytest.raises(ValueError, match='y_true and y_pred have different number of elements'):\n        balanced_accuracy_score(y_true, y_pred)\n"], "sample_253": ["    def test_notify_file_changed_triggers_reload(self):\n        with mock.patch('django.utils.autoreload.trigger_reload') as mock_trigger_reload:\n            reloader = autoreload.StatReloader()\n            reloader.notify_file_changed(Path('test_file.py'))\n            self.assertEqual(mock_trigger_reload.call_count, 1)\n", "def test_snapshot_files_with_exception(self):\n    fake_file = mock.Mock()\n    fake_file.stat.side_effect = OSError\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[fake_file]):\n        snapshot = list(self.reloader.snapshot_files())\n        self.assertEqual(len(snapshot), 0)\n", "def test_watched_files_with_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "def test_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "    def setUp(self):\n        self.reloader = autoreload.StatReloader()\n"], "sample_246": ["def test_no_obsolete_enabled(self):\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertNotIn(\"#, fuzzy\", po_contents)\n", "def test_no_obsolete_enabled(self):\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertNotIn('#~', po_contents)\n", "def test_no_location_with_add_location_full(self):\n    \"\"\"Behavior is correct if --no-location and --add-location=full switches are specified. See #26341.\"\"\"\n    management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_location=True, add_location='full')\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    self.assertLocationCommentNotPresent(self.PO_FILE, None, 'test.html')\n", "def test_makemessages_call_count(self):\n    \"\"\"\n    Test that the correct number of calls are made to xgettext.\n    \"\"\"\n    with mock.patch('django.core.management.commands.makemessages.popen_wrapper') as mocked_popen_wrapper:\n        mocked_popen_wrapper.return_value = ('', '', 0)\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertEqual(mocked_popen_wrapper.call_count, 2)  # One for each domain (django and djangojs)\n", "    def test_multiple_extensions(self):\n        management.call_command('makemessages', locale=[LOCALE], extensions=['html', 'djtpl'], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('This literal should be included from HTML.', po_contents)\n            self.assertMsgId('This literal should be included from Django template.', po_contents)\n            self.assertNotMsgId('This literal should not be included from Django Python.', po_contents)\n"], "sample_796": ["def test_huber_epsilon_validation():\n    # Test that an exception is raised when epsilon is less than 1.0\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError, match=\"epsilon should be greater than or equal to 1.0, got\"):\n        HuberRegressor(epsilon=0.9).fit(X, y)\n", "def test_huber_epsilon_validation():\n    X, y = make_regression_with_outliers()\n    with pytest.raises(ValueError, match=\"epsilon should be greater than or equal to 1.0, got 0.9\"):\n        huber = HuberRegressor(epsilon=0.9)\n        huber.fit(X, y)\n", "def test_huber_with_zero_alpha():\n    # Test that HuberRegressor behaves like LinearRegression when alpha=0\n    X, y = make_regression(n_samples=100, n_features=2, noise=1.0, random_state=0)\n    huber = HuberRegressor(fit_intercept=True, alpha=0.0)\n    huber.fit(X, y)\n    lr = LinearRegression(fit_intercept=True)\n    lr.fit(X, y)\n    assert_array_almost_equal(huber.coef_, lr.coef_)\n    assert_array_almost_equal(huber.intercept_, lr.intercept_)\n", "def test_huber_negative_epsilon():\n    # Test that an error is raised when epsilon is less than 1.0\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(epsilon=0.5)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n", "def test_huber_zero_sample_weights():\n    # Test that HuberRegressor behaves correctly with zero sample weights\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100)\n    huber.fit(X, y)\n\n    # Set all sample weights to zero\n    sample_weight = np.zeros(y.shape[0])\n    huber.fit(X, y, sample_weight=sample_weight)\n\n    # Coefficients should be all zeros, intercept should be the mean of y\n    assert_array_equal(huber.coef_, np.zeros(X.shape[1]))\n    assert_almost_equal(huber.intercept_, np.mean(y))\n"], "sample_35": ["def test_isinstancemethod():\n    class MyClass:\n\n        @classmethod\n\n        @staticmethod\n\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n", "def test_isinstancemethod():\n    class MyClass:\n\n        @classmethod\n\n        @staticmethod\n\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n", "def test_isinstancemethod():\n    class MyClass:\n\n        @classmethod\n\n        @staticmethod\n\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n", "def test_isinstancemethod():\n    class MetaClass(type):\n\n    class MyClass(metaclass=MetaClass):\n\n        @classmethod\n\n        @staticmethod\n\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.another_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n", "def test_isinstancemethod():\n    class MetaClass(type):\n\n    class MyClass(metaclass=MetaClass):\n\n        @classmethod\n\n        @staticmethod\n\n    assert not isinstancemethod(MyClass, MyClass.a_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.another_classmethod)\n    assert not isinstancemethod(MyClass, MyClass.a_staticmethod)\n    assert isinstancemethod(MyClass, MyClass.an_instancemethod)\n"], "sample_913": ["def test_pyclass_signature(app):\n    text = \".. py:class:: Class\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, ()])]))\n    assert 'Class' in domain.objects\n    assert domain.objects['Class'] == ('index', 'Class', 'class')\n", "def test_pyfunction_signature_with_annotation(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1][0], desc_parameter,\n                ([desc_sig_name, \"name\"],\n                 [desc_sig_punctuation, \":\"],\n                 \" \",\n                 [desc_sig_name, pending_xref, \"str\"]))\n    assert_node(doctree[1][0][1][2], desc_returns, pending_xref, \"str\")\n", "def test_pydata_signature_module_option(app):\n    text = (\".. py:data:: version\\n\"\n            \"   :module: module_name\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 1\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"module_name.\"],\n                                                    [desc_name, \"version\"],\n                                                    [desc_annotation, \": int\"],\n                                                    [desc_annotation, \" = 1\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n", "def test_pymodule_directive(app):\n    text = (\".. py:module:: sphinx\\n\"\n            \"   :synopsis: A builder for docs.\\n\"\n            \"   :platform: Unix, Windows\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index))\n    assert_node(doctree[1], addnodes.index,\n                entries=[('pair', 'module; sphinx', 'module-sphinx', '', None)])\n\n    assert 'sphinx' in domain.modules\n    assert domain.modules['sphinx'] == ('index', 'module-sphinx', 'A builder for docs.', 'Unix, Windows', True)\n", "def test_pyfunction_signature_with_defaults(app):\n    text = \".. py:function:: hello(name='World') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_operator, \"=\"],\n                                                      [nodes.inline, \"'World'\"])])\n"], "sample_508": ["def test_sticky_edges():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1])\n    line.sticky_edges.x[:] = [0, 1]\n    line.sticky_edges.y[:] = [0, 1]\n    assert line.sticky_edges.x == [0, 1]\n    assert line.sticky_edges.y == [0, 1]\n    plt.close()\n", "def test_set_property_exceptions():\n    \"\"\"Test exceptions for setting properties.\"\"\"\n    art = martist.Artist()\n    with pytest.raises(AttributeError, match=\"'Artist' object has no property 'nonexistent'\"):\n        art.set(nonexistent=1)\n", "def test_artist_properties(kwargs, expected_error, expected_warning):\n    art = martist.Artist()\n    if expected_error:\n        with pytest.raises(expected_error):\n            art.set(**kwargs)\n    elif expected_warning:\n        with pytest.warns(expected_warning):\n            art.set(**kwargs)\n    else:\n        art.set(**kwargs)\n", "def test_artist_update_from():\n    # Test the update_from method of the Artist class\n    artist1 = martist.Artist()\n    artist2 = martist.Artist()\n\n    artist1.set_alpha(0.5)\n    artist1.set_clip_on(False)\n    artist1.set_label('Test Artist 1')\n    artist1.set_rasterized(True)\n    artist1.set_sketch_params(scale=2.0, length=100.0, randomness=8.0)\n    artist1.set_transform(mtransforms.Affine2D())\n    artist1.set_visible(False)\n    artist1.sticky_edges.x[:] = [0.0, 1.0]\n    artist1.sticky_edges.y[:] = [0.0, 1.0]\n\n    artist2.update_from(artist1)\n\n    assert artist2.get_alpha() == artist1.get_alpha()\n    assert artist2.get_clip_on() == artist1.get_clip_on()\n    assert artist2.get_label() == artist1.get_label()\n    assert artist2.get_rasterized() == artist1.get_rasterized()\n    assert artist2.get_sketch_params() == artist1.get_sketch_params()\n    assert artist2.get_transform() == artist1.get_transform()\n    assert artist2.get_visible() == artist1.get_visible()\n    assert np.all(artist2.sticky_edges.x == artist1.sticky_edges.x)\n    assert np.all(artist2.sticky_edges.y == artist1.sticky_edges.y)\n", "def test_clip_box_clipping():\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n\n    # Draw a line outside the clip box\n    ax1.plot([-3, 3], [-3, 3])\n    ax1.set_xlim(0, 1)\n    ax1.set_ylim(0, 1)\n\n    ax2.plot([-3, 3], [-3, 3])\n    ax2.set_xlim(0, 1)\n    ax2.set_ylim(0, 1)\n\n    # Set a clip box on ax2\n    clip_box = mtransforms.Bbox([[0.25, 0.25], [0.75, 0.75]])\n    ax2.set_clip_box(clip_box)\n"], "sample_865": ["def test_prune_tree_raises_non_fitted_error():\n    clf = DecisionTreeClassifier()\n    msg = \"This DecisionTreeClassifier instance is not fitted yet\"\n\n    with pytest.raises(NotFittedError, match=msg):\n        clf.prune()\n", "def test_prune_tree_with_unfitted_tree():\n    clf = DecisionTreeClassifier()\n    msg = \"This DecisionTreeClassifier instance is not fitted yet\"\n\n    with pytest.raises(NotFittedError, match=msg):\n        clf._prune_tree()\n", "def test_prune_tree_raises_not_fitted():\n    clf = DecisionTreeClassifier()\n    msg = \"This DecisionTreeClassifier instance is not fitted yet.\"\n\n    with pytest.raises(NotFittedError, match=msg):\n        clf._prune_tree()\n", "def test_prune_tree_with_zero_ccp_alpha():\n    clf = DecisionTreeClassifier()\n    clf.fit(X, y)\n\n    # keep original tree\n    orig_tree = copy.deepcopy(clf.tree_)\n\n    # prune with zero ccp_alpha\n    clf.set_params(ccp_alpha=0.0)\n    clf._prune_tree()\n\n    # check if the tree is the same as the original\n    assert_tree_equal(orig_tree, clf.tree_, \"Pruned tree is not the same as the original tree\")\n", "def test_prune_tree_classifier_with_custom_loss(criterion, dataset, tree_cls):\n    dataset = DATASETS[dataset]\n    X, y = dataset[\"X\"], dataset[\"y\"]\n\n        return np.mean((y_true - y_pred) ** 2)\n\n    est = tree_cls(max_leaf_nodes=20, random_state=0, criterion=criterion, splitter=\"best\", min_impurity_decrease=0.0)\n    est.fit(X, y)\n\n    pruning_path = est.cost_complexity_pruning_path(X, y)[\"ccp_alphas\"]\n\n    for ccp_alpha in pruning_path:\n        est_pruned = tree_cls(max_leaf_nodes=20, ccp_alpha=ccp_alpha, random_state=0, criterion=criterion, splitter=\"best\", min_impurity_decrease=0.0)\n        est_pruned.fit(X, y)\n\n        assert custom_loss(y, est_pruned.predict(X)) <= custom_loss(y, est.predict(X))\n"], "sample_941": ["def test_restify_type_hints_Ellipsis():\n    assert restify(Ellipsis) == '...'\n    assert stringify(Ellipsis) == '...'\n", "def test_restify_type_hints_Optional():\n    assert restify(Optional[int]) == \":obj:`Optional`\\\\ [:class:`int`]\"\n    assert restify(Optional[str]) == \":obj:`Optional`\\\\ [:class:`str`]\"\n    assert restify(Optional[MyClass1]) == \":obj:`Optional`\\\\ [:class:`tests.test_util_typing.MyClass1`]\"\n", "def test_restify_type_hints_Union_with_TypeVar():\n    assert restify(Union[T, None]) == \":obj:`Optional`\\\\ [:obj:`tests.test_util_typing.T`]\"\n    assert restify(Union[T, str]) == \":obj:`Union`\\\\ [:obj:`tests.test_util_typing.T`, :class:`str`]\"\n", "def test_restify_type_hints_NewType():\n    assert restify(MyInt) == \":class:`MyInt`\"\n    assert stringify(MyInt) == \"MyInt\"\n", "def test_restify_type_hints_Annotated():\n    from typing import Annotated  # type: ignore\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":class:`str`\"  # NOQA\n"], "sample_109": ["def test_render_options_with_initial_value(self):\n    \"\"\"Options are rendered correctly when an initial value is provided.\"\"\"\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'band': beatles.pk})\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    option = '<option value=\"%s\">The Who</option>' % who.pk\n    self.assertIn(selected_option, output)\n    self.assertIn(option, output)\n", "def test_render_options_with_custom_choices(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    choices = [(beatles.pk, 'The Fab Four'), (who.pk, 'The Mods')]\n    form = forms.Form({\n        'band': beatles.pk\n    }, widgets={\n        'band': AutocompleteSelect(Album._meta.get_field('band').remote_field, admin.site, choices=choices)\n    })\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>The Fab Four</option>' % beatles.pk\n    option = '<option value=\"%s\">The Mods</option>' % who.pk\n    self.assertIn(selected_option, output)\n    self.assertNotIn(option, output)\n", "    def test_media_language_fallback(self):\n        rel = Album._meta.get_field('band').remote_field\n        # Test a language that does not exist\n        with translation.override('unknown_language'):\n            self.assertNotIn('admin/js/vendor/select2/i18n/unknown_language.js', AutocompleteSelect(rel, admin.site).media._js)\n        # Test a language that exists but does not have a select2 translation\n        with translation.override('en-us'):\n            self.assertNotIn('admin/js/vendor/select2/i18n/en-us.js', AutocompleteSelect(rel, admin.site).media._js)\n        # Test a language that exists and has a select2 translation\n        with translation.override('fr'):\n            self.assertIn('admin/js/vendor/select2/i18n/fr.js', AutocompleteSelect(rel, admin.site).media._js)\n", "def test_media_debug(self):\n    \"\"\"Test that the widget uses unminified JavaScript files when DEBUG is True.\"\"\"\n    rel = Album._meta.get_field('band').remote_field\n    base_files = (\n        'admin/js/vendor/jquery/jquery.js',\n        'admin/js/vendor/select2/select2.full.js',\n        # Language file is inserted here.\n        'admin/js/jquery.init.js',\n        'admin/js/autocomplete.js',\n    )\n    with override_settings(DEBUG=True):\n        with translation.override('en'):\n            self.assertEqual(AutocompleteSelect(rel, admin.site).media._js, list(base_files))\n", "    def test_render_options_multiple_selection(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        # With 'featuring', a ManyToManyField.\n        form = AlbumForm(initial={'featuring': [beatles.pk]})\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        option = '<option value=\"%s\">The Who</option>' % who.pk\n        self.assertIn(selected_option, output)\n        self.assertIn(option, output)\n        form = AlbumForm(data={'featuring': [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option_beatles = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        selected_option_who = '<option value=\"%s\" selected>The Who</option>' % who.pk\n        self.assertIn(selected_option_beatles, output)\n        self.assertIn(selected_option_who, output)\n"], "sample_380": ["def test_stddev_variance(self):\n    # Test StdDev with sample=True\n    stddev_sample = Book.objects.aggregate(stddev_price=StdDev('price', sample=True))['stddev_price']\n    self.assertIsInstance(stddev_sample, float)\n\n    # Test StdDev with sample=False\n    stddev_pop = Book.objects.aggregate(stddev_price=StdDev('price', sample=False))['stddev_price']\n    self.assertIsInstance(stddev_pop, float)\n    self.assertNotEqual(stddev_sample, stddev_pop)\n\n    # Test Variance with sample=True\n    variance_sample = Book.objects.aggregate(variance_price=Variance('price', sample=True))['variance_price']\n    self.assertIsInstance(variance_sample, float)\n\n    # Test Variance with sample=False\n    variance_pop = Book.objects.aggregate(variance_price=Variance('price', sample=False))['variance_price']\n    self.assertIsInstance(variance_pop, float)\n    self.assertNotEqual(variance_sample, variance_pop)\n", "def test_filter_with_count_in_annotation(self):\n    publishers = Publisher.objects.annotate(num_books=Count(\"book\")).filter(num_books__gt=1)\n    self.assertEqual(len(publishers), 4)  # Apress, Prentice Hall, Sams, and Morgan Kaufmann\n", "def test_aggregation_subquery_annotation_with_filter(self):\n    \"\"\"\n    Subquery annotations and external aliases are excluded from the GROUP BY if\n    they are not selected and also if they have a filter condition.\n    \"\"\"\n    books_qs = Book.objects.annotate(\n        first_author_the_same_age=Subquery(\n            Author.objects.filter(\n                age=OuterRef('contact__friends__age'),\n                pk__gt=F('book__authors__pk')\n            ).order_by('age').values('id')[:1],\n        )\n    ).filter(\n        publisher=self.p1,\n        first_author_the_same_age__isnull=False,\n    ).annotate(\n        min_age=Min('contact__friends__age'),\n    ).values('name', 'min_age').order_by('name')\n    self.assertEqual(list(books_qs), [\n        {'name': 'Practical Django Projects', 'min_age': 34},\n    ])\n", "def test_stddev_variance_sample(self):\n    book_ratings = [book.rating for book in Book.objects.all()]\n\n    stddev_sample = Book.objects.aggregate(stddev_sample=StdDev('rating', sample=True))['stddev_sample']\n    self.assertAlmostEqual(stddev_sample, np.std(book_ratings, ddof=1), places=2)\n\n    variance_sample = Book.objects.aggregate(variance_sample=Variance('rating', sample=True))['variance_sample']\n    self.assertAlmostEqual(variance_sample, np.var(book_ratings, ddof=1), places=2)\n", "def test_aggregation_default_not_in_query(self):\n    result = Book.objects.filter(rating__gt=4).aggregate(\n        value=Sum('pages', default=Avg('authors__age', filter=Q(age__gt=30)) * 10),\n    )\n    self.assertIsNotNone(result['value'])\n"], "sample_615": ["def test_cross_errors():\n    a = xr.DataArray([1, 2, 3], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n\n    with pytest.raises(ValueError, match=\"dimension 'time' not on a\"):\n        xr.cross(a, b, dim=\"time\")\n\n    with pytest.raises(ValueError, match=\"dimension 'time' not on b\"):\n        xr.cross(b, a, dim=\"time\")\n\n    with pytest.raises(ValueError, match=\"The size of 'cartesian' on a must be 1, 2, or 3 to be compatible with a cross product but is 4\"):\n        xr.cross(a, a, dim=\"cartesian\")\n\n    with pytest.raises(ValueError, match=\"The size of 'cartesian' on b must be 1, 2, or 3 to be compatible with a cross product but is 4\"):\n        xr.cross(b, b, dim=\"cartesian\")\n\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n\n    with pytest.raises(ValueError, match=\"dimensions without coordinates must have have a length of 2 or 3\"):\n        xr.cross(a, b, dim=\"cartesian\")\n", "def test_cross_with_one_sized_arrays_without_coords(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=dim)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_exceptions():\n    a = xr.DataArray(np.arange(3), dims=\"dim_0\")\n    b = xr.DataArray(np.arange(4), dims=\"dim_1\")\n\n    with pytest.raises(ValueError, match=\"Dimension 'dim_1' not on a\"):\n        xr.cross(a, b, dim=\"dim_1\")\n\n    with pytest.raises(ValueError, match=\"Dimension 'dim_0' not on b\"):\n        xr.cross(b, a, dim=\"dim_0\")\n\n    a = xr.DataArray(np.arange(5), dims=\"dim_0\")\n    b = xr.DataArray(np.arange(4), dims=\"dim_0\")\n\n    with pytest.raises(ValueError, match=\"The size of 'dim_0' on a must be 1, 2, or 3\"):\n        xr.cross(a, b, dim=\"dim_0\")\n\n    a = xr.DataArray(np.arange(4), dims=\"dim_0\")\n    b = xr.DataArray(np.arange(5), dims=\"dim_0\")\n\n    with pytest.raises(ValueError, match=\"The size of 'dim_0' on b must be 1, 2, or 3\"):\n        xr.cross(a, b, dim=\"dim_0\")\n\n    a = xr.DataArray(np.arange(2), dims=\"dim_0\")\n    b = xr.DataArray(np.arange(2), dims=\"dim_0\")\n\n    with pytest.raises(ValueError, match=\"dimensions without coordinates must have have a length of 2 or 3\"):\n        xr.cross(a, b, dim=\"dim_0\")\n", "def test_cross_errors(a, b, dim: str, axis: int, use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError, match=\"dimension 'cartesian' not on b\"):\n        xr.cross(a, b, dim=dim)\n\n    with pytest.raises(ValueError, match=\"dimension 'non_existent' not on a\"):\n        xr.cross(a, b, dim=\"non_existent\")\n\n    with pytest.raises(ValueError, match=\"The size of 'cartesian' on a must be 1, 2, or 3\"):\n        xr.cross(xr.DataArray([1, 2, 3, 4], dims=[\"cartesian\"]), b, dim=dim)\n\n    with pytest.raises(ValueError, match=\"The size of 'cartesian' on b must be 1, 2, or 3\"):\n        xr.cross(a, xr.DataArray([1, 2, 3, 4], dims=[\"cartesian\"]), dim=dim)\n\n    with pytest.raises(ValueError, match=\"dimensions without coordinates must have have a length of 2 or 3\"):\n        xr.cross(xr.DataArray([1], dims=[\"cartesian\"]), xr.DataArray([4, 5], dims=[\"cartesian\"]), dim=dim)\n", "def test_cross_errors() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"dim_0\"])\n    b = xr.DataArray([4, 5], dims=[\"dim_0\"])\n\n    # Test dimension not on a\n    with pytest.raises(ValueError, match=\"Dimension 'dim_1' not on a\"):\n        xr.cross(a, b, dim=\"dim_1\")\n\n    # Test dimension not on b\n    with pytest.raises(ValueError, match=\"Dimension 'dim_1' not on b\"):\n        xr.cross(b, a, dim=\"dim_1\")\n\n    # Test dimension size not compatible with cross product\n    with pytest.raises(ValueError, match=\"The size of 'dim_0' on a must be 1, 2, or 3\"):\n        xr.cross(a, xr.DataArray([1, 2, 3, 4], dims=[\"dim_0\"]), dim=\"dim_0\")\n\n    # Test dimension size not compatible with cross product\n    with pytest.raises(ValueError, match=\"The size of 'dim_0' on b must be 1, 2, or 3\"):\n        xr.cross(xr.DataArray([1, 2, 3, 4], dims=[\"dim_0\"]), b, dim=\"dim_0\")\n\n    # Test dimensions without coordinates must have a length of 2 or 3\n    with pytest.raises(ValueError, match=\"dimensions without coordinates must have have a length of 2 or 3\"):\n        xr.cross(xr.DataArray([1, 2], dims=[\"dim_0\"]), xr.DataArray([4, 5, 6], dims=[\"dim_0\"]), dim=\"dim_0\")\n"], "sample_605": ["def test_groupby_first_last(array):\n    grouped = array.groupby(\"y\")\n    assert_identical(array.isel(y=[0, 1, 2]), grouped.first())\n    assert_identical(array.isel(y=[1, 2, 3]), grouped.last())\n", "def test_groupby_da_first_last(array):\n    grouped = array.groupby(\"y\")\n    expected_first = array.isel(y=0)\n    expected_last = array.isel(y=-1)\n    actual_first = grouped.first()\n    actual_last = grouped.last()\n    assert_identical(expected_first, actual_first)\n    assert_identical(expected_last, actual_last)\n", "def test_groupby_apply_with_shortcut(dataset):\n        return ds + 1\n\n    expected = dataset.apply(func)\n    actual = dataset.groupby(\"x\").apply(func, shortcut=True)\n    assert_identical(expected, actual)\n", "def test_groupby_apply_empty_groups():\n    # Test for issue #2601\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], [(\"x\", [1, 1, 1, 2, 2, 2])])\n\n        if group.sizes[\"x\"] == 0:\n            return group\n        else:\n            return group.sum()\n\n    expected = xr.DataArray([6, 6], [(\"x\", [1, 2])])\n    actual = array.groupby(\"x\").apply(func)\n    assert_identical(expected, actual)\n", "def test_groupby_da_binary_ops():\n    # Test binary operations with groupby on DataArray\n    array1 = xr.DataArray([1, 2, 3, 4, 5, 6], [(\"x\", [1, 1, 1, 2, 2, 2])])\n    array2 = xr.DataArray([2, 3, 4, 5, 6, 7], [(\"x\", [1, 1, 1, 2, 2, 2])])\n\n    expected_add = xr.DataArray([3, 5, 7, 9, 11, 13], [(\"x\", [1, 1, 1, 2, 2, 2])])\n    actual_add = array1.groupby(\"x\") + array2\n    assert_identical(expected_add, actual_add)\n\n    expected_sub = xr.DataArray([-1, -1, -1, -1, -1, -1], [(\"x\", [1, 1, 1, 2, 2, 2])])\n    actual_sub = array1.groupby(\"x\") - array2\n    assert_identical(expected_sub, actual_sub)\n"], "sample_628": ["def test_docstring_lines_that_look_like_comments_7(self):\n    stmt = astroid.extract_node(\n        '''def f():\n    \"\"\"\n    msitake#cat\n    \"\"\"'''\n    )\n    with self.assertAddsMessages(\n        Message(\n            \"wrong-spelling-in-docstring\",\n            line=3,\n            args=(\n                \"msitake\",\n                \"    msitake#cat\",\n                \"    ^^^^^^^\",\n                self._get_msg_suggestions(\"msitake\"),\n            ),\n        )\n    ):\n        self.checker.visit_functiondef(stmt)\n", "def test_skip_unicode_strings_in_docstrings(self):\n    stmt = astroid.extract_node('def fff():\\n   \"\"\"test u\\'unicode\\'\"\"\"\\n   pass')\n    self.checker.visit_functiondef(stmt)\n    assert self.linter.release_messages() == []\n", "def test_docstring_lines_that_look_like_comments_7(self):\n    stmt = astroid.extract_node(\n        '''def f():\n    \"\"\"\n    # msitake\n    # cat\n    \"\"\"'''\n    )\n    with self.assertAddsMessages(\n        Message(\n            \"wrong-spelling-in-docstring\",\n            line=3,\n            args=(\n                \"msitake\",\n                \"# msitake\",\n                \"  ^^^^^^^\",\n                self._get_msg_suggestions(\"msitake\"),\n            ),\n        )\n    ):\n        self.checker.visit_functiondef(stmt)\n", "def test_skip_unicode_and_raw_strings(self):\n    stmt = astroid.extract_node(\n        'def fff(u\"param_name\", r\"test_name\"):\\n   \"\"\"test u\"param_name\" and r\"test_name\"\"\"\\n   pass'\n    )\n    self.checker.visit_functiondef(stmt)\n    assert self.linter.release_messages() == []\n", "def test_docstring_lines_that_look_like_comments_7(self):\n    stmt = astroid.extract_node(\n        '''def f():\n    \"\"\"\n    # msitake\n        indented_msitake\n    \"\"\"'''\n    )\n    with self.assertAddsMessages(\n        Message(\n            \"wrong-spelling-in-docstring\",\n            line=3,\n            args=(\n                \"msitake\",\n                \"    # msitake\",\n                \"      ^^^^^^^\",\n                self._get_msg_suggestions(\"msitake\"),\n            ),\n        ),\n        Message(\n            \"wrong-spelling-in-docstring\",\n            line=4,\n            args=(\n                \"indented_msitake\",\n                \"        indented_msitake\",\n                \"        ^^^^^^^^^^^^^^^\",\n                self._get_msg_suggestions(\"indented_msitake\"),\n            ),\n        ),\n    ):\n        self.checker.visit_functiondef(stmt)\n"], "sample_583": ["def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = indexing.OuterIndexer((np.array([0, 0, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected.tuple, actual.tuple)\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = indexing.VectorizedIndexer(\n        (np.array([0, 0, 2]), np.array([0, 1, 1])))\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected.tuple, actual.tuple)\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = indexing.OuterIndexer((np.array([0, 0, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected.tuple, actual.tuple)\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = indexing.VectorizedIndexer(\n        (np.array([0, 0, 2]), np.array([0, 1, 0])))\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected.tuple, actual.tuple)\n\n    indexer = indexing.BasicIndexer((-1,))\n    expected = indexing.BasicIndexer((0,))\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected.tuple, actual.tuple)\n", "def test_create_mask_error_indexer_type():\n    with pytest.raises(TypeError):\n        indexing.create_mask(\"invalid_key_type\", (3,))\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]), np.array([0, -1, 1]),))\n    expected = indexing.OuterIndexer((np.array([0, 0, 2]), np.array([0, 1, 1]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert actual.tuple == expected.tuple\n\n    indexer = indexing.VectorizedIndexer((np.array([0, -1, 2]), np.array([0, 1, -1]),))\n    expected = indexing.VectorizedIndexer((np.array([0, 0, 2]), np.array([0, 1, 1]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(actual.tuple, expected.tuple)\n", "def test_create_mask_basic_indexer_multi_dimensions():\n    indexer = indexing.BasicIndexer((-1, slice(None)))\n    expected = np.array([True] * 5)\n    actual = indexing.create_mask(indexer, (5, 3))\n    np.testing.assert_array_equal(expected, actual)\n\n    indexer = indexing.BasicIndexer((0, slice(None)))\n    expected = np.array([False] * 5)\n    actual = indexing.create_mask(indexer, (5, 3))\n    np.testing.assert_array_equal(expected, actual)\n\n    indexer = indexing.BasicIndexer((slice(None), -1))\n    expected = np.array([True] * 3)\n    actual = indexing.create_mask(indexer, (5, 3))\n    np.testing.assert_array_equal(expected, actual)\n\n    indexer = indexing.BasicIndexer((slice(None), 0))\n    expected = np.array([False] * 3)\n    actual = indexing.create_mask(indexer, (5, 3))\n    np.testing.assert_array_equal(expected, actual)\n"], "sample_170": ["    def test_sensitive_request_in_production(self):\n        \"\"\"\n        In a production environment, sensitive POST parameters and frame\n        variables should not be shown in the default error reports for sensitive\n        requests even if DEBUG is False.\n        \"\"\"\n        self.verify_safe_response(sensitive_view)\n        self.verify_safe_email(sensitive_view)\n", "def test_template_encoding_for_non_html_response(self):\n    \"\"\"\n    The templates are loaded directly, not via a template loader, and\n    should be opened as utf-8 charset as is the default specified on\n    template engines.\n    \"\"\"\n    request = self.rf.get('/some_url/', HTTP_ACCEPT='application/json')\n    with mock.patch.object(DebugPath, 'open') as m:\n        technical_500_response(request, None, None, None)\n        m.assert_called_once_with(encoding='utf-8')\n", "def test_sensitive_function_nested_calls(self):\n    \"\"\"\n    Sensitive variables don't leak in the sensitive_variables decorator's\n    frame, when those variables are passed as arguments to the decorated\n    function and that function then calls another function.\n    \"\"\"\n    with self.settings(DEBUG=True):\n        self.verify_unsafe_response(sensitive_nested_function_caller)\n        self.verify_unsafe_email(sensitive_nested_function_caller)\n\n    with self.settings(DEBUG=False):\n        self.verify_safe_response(sensitive_nested_function_caller, check_for_POST_params=False)\n        self.verify_safe_email(sensitive_nested_function_caller, check_for_POST_params=False)\n", "    def test_unicode_error(self):\n        try:\n            raise UnicodeError('unicode error', 'unicode string', 1, 2, 'reason')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        text = reporter.get_traceback_text()\n        self.assertIn('Unicode error hint', text)\n        self.assertIn('The string that could not be encoded/decoded was: unicode string', text)\n        self.assertIn('reason', text)\n", "    def test_frame_variables_with_multivalue_dict(self):\n        request = self.rf.post('/some_url/', self.breakfast_data)\n\n        # Create a frame variables dict with a MultiValueDict\n        frame_vars = {\n            'multivaluedict_var': request.POST,\n        }\n\n        # Get the cleansed frame variables\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_frame_vars = dict(reporter_filter.cleanse_special_types(request, frame_vars))\n\n        # Ensure sensitive POST parameters are not shown\n        self.assertNotIn('sausage-key', cleansed_frame_vars['multivaluedict_var'])\n        self.assertNotIn('bacon-key', cleansed_frame_vars['multivaluedict_var'])\n"], "sample_241": ["def test_expression_wrapper_with_none(self):\n    qs = Employee.objects.annotate(\n        salary_wrapper=ExpressionWrapper(\n            F('salary'),\n            output_field=IntegerField(),\n        ),\n        none_wrapper=ExpressionWrapper(\n            Value(None),\n            output_field=IntegerField(),\n        ),\n    )\n    employee = qs.first()\n    self.assertEqual(employee.salary_wrapper, employee.salary)\n    self.assertIsNone(employee.none_wrapper)\n", "def test_combined_expression_group_by(self):\n    expr = CombinedExpression(F('num_employees'), Combinable.ADD, Value(1), output_field=IntegerField())\n    group_by_cols = expr.get_group_by_cols(alias=None)\n    self.assertEqual(group_by_cols, [expr])\n", "    def test_output_field_promotion(self):\n        tests = [\n            (IntegerField, AutoField, IntegerField),\n            (AutoField, IntegerField, IntegerField),\n            (IntegerField, DecimalField, DecimalField),\n            (DecimalField, IntegerField, DecimalField),\n            (IntegerField, FloatField, FloatField),\n            (FloatField, IntegerField, FloatField),\n        ]\n        for wrapper_field, expression_field, expected_field in tests:\n            with self.subTest(wrapper_field=wrapper_field, expression_field=expression_field):\n                expr = ExpressionWrapper(Expression(expression_field()), output_field=wrapper_field())\n                self.assertIsInstance(expr.output_field, expected_field)\n", "    def test_expression_wrapper_with_nested_expression(self):\n        expr = ExpressionWrapper(F('integer') * 2, output_field=IntegerField())\n        nested_expr = ExpressionWrapper(expr + 3, output_field=IntegerField())\n        queryset = Number.objects.annotate(result=nested_expr)\n        self.assertEqual(queryset.get().result, 86)\n", "def test_annotation_with_case_when_then(self):\n    Company.objects.filter(num_employees__lt=50).update(ceo=Employee.objects.get(firstname='Frank'))\n    inner = Company.objects.annotate(\n        employee=OuterRef('pk'),\n    ).values('employee').annotate(\n        min_num_chairs=Min('num_chairs'),\n    ).values('ceo')\n    qs = Employee.objects.annotate(\n        ceo_company=Case(\n            When(pk__in=inner, then=Value('CEO of a small company')),\n            default=Value('Not a CEO of a small company'),\n            output_field=CharField(),\n        ),\n    )\n    self.assertEqual(qs.get(pk=self.foobar_ltd.ceo.pk).ceo_company, 'CEO of a small company')\n    self.assertEqual(qs.get(pk=self.example_inc.ceo.pk).ceo_company, 'Not a CEO of a small company')\n"], "sample_772": ["def test_max_samples():\n    # Test if max_samples of base estimators is set\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n    all_estimators = [RandomForestClassifier, RandomForestRegressor,\n                      ExtraTreesClassifier, ExtraTreesRegressor]\n\n    for Estimator in all_estimators:\n        est = Estimator(max_samples=0.5)\n        est.fit(X, y)\n        for tree in est.estimators_:\n            assert_equal(tree.max_samples, 0.5)\n", "def test_min_impurity_split_and_decrease():\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n    all_estimators = [RandomForestClassifier, RandomForestRegressor,\n                      ExtraTreesClassifier, ExtraTreesRegressor]\n\n    for Estimator in all_estimators:\n        est = Estimator(min_impurity_split=0.1, min_impurity_decrease=0.2)\n        with pytest.warns(DeprecationWarning, match=\"min_impurity_decrease\"):\n            est.fit(X, y)\n        for tree in est.estimators_:\n            assert_equal(tree.min_impurity_split, 0.1)\n            assert_equal(tree.min_impurity_decrease, 0.2)\n", "def test_max_samples():\n    X, y = datasets.make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_repeated=0,\n                                        n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0,\n                                        hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=42)\n    all_estimators = [RandomForestClassifier, ExtraTreesClassifier]\n\n    for Estimator in all_estimators:\n        est = Estimator(max_samples=0.5)\n        est.fit(X, y)\n        n_samples_bootstrap = np.unique(est.estimators_[0].tree_.apply(X)).shape[0]\n        assert_equal(n_samples_bootstrap, 0.5 * X.shape[0])\n", "def test_feature_names_in():\n    # Test if feature_names_in is set correctly\n    X, y = datasets.load_iris(return_X_y=True)\n    feature_names = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n    all_estimators = [RandomForestClassifier, RandomForestRegressor,\n                      ExtraTreesClassifier, ExtraTreesRegressor]\n\n    for Estimator in all_estimators:\n        est = Estimator()\n        est.fit(X, y, feature_names=feature_names)\n        assert_equal(est.feature_names_in_, feature_names)\n", "def test_random_state():\n    # Test that the random state is properly used in all possible cases\n    X, y = datasets.make_classification(n_samples=100, n_features=20, n_informative=10,\n                                        n_redundant=5, n_classes=3, random_state=0)\n    all_estimators = [RandomForestClassifier, RandomForestRegressor,\n                      ExtraTreesClassifier, ExtraTreesRegressor]\n\n    for Estimator in all_estimators:\n        est_random = Estimator(random_state=0)\n        est_none = Estimator(random_state=None)\n        est_int = Estimator(random_state=0)\n        est_random.fit(X, y)\n        est_none.fit(X, y)\n        est_int.fit(X, y)\n\n        # Check that the results are different when random_state is different\n        assert est_random.predict(X) != est_none.predict(X)\n\n        # Check that the results are the same when random_state is the same\n        assert_array_equal(est_random.predict(X), est_int.predict(X))\n"], "sample_1097": ["def test_BlockMatrix_as_real_imag():\n    A, B, C, D = [MatrixSymbol(s, 3, 3) for s in 'ABCD']\n    X = BlockMatrix([[A, B], [C, D]])\n\n    real_part, imag_part = X.as_real_imag()\n    assert real_part == BlockMatrix([[re(A), re(B)], [re(C), re(D)]])\n    assert imag_part == BlockMatrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, m, complex=True)\n    B = MatrixSymbol('B', n, m, complex=True)\n    X = BlockMatrix([[A, B]])\n\n    real_part, imag_part = X.as_real_imag()\n    assert real_part == BlockMatrix([[re(A), re(B)]])\n    assert imag_part == BlockMatrix([[im(A), im(B)]])\n", "def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, m, complex=True)\n    B = MatrixSymbol('B', m, n, complex=True)\n    X = BlockMatrix([[A, B]])\n\n    real_part, imag_part = X.as_real_imag()\n\n    assert real_part == BlockMatrix([[re(A), re(B)]])\n    assert imag_part == BlockMatrix([[im(A), im(B)]])\n", "def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, m, complex=True)\n    B = MatrixSymbol('B', n, m, complex=True)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    real_matrices, im_matrices = X.as_real_imag()\n\n    assert real_matrices == BlockMatrix([[re(A), re(B)], [re(C), re(D)]])\n    assert im_matrices == BlockMatrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_as_real_imag():\n    A, B = [MatrixSymbol(s, 3, 3) for s in 'AB']\n    X = BlockMatrix([[A, B], [B.conjugate(), A.conjugate()]])\n    real_X, imag_X = X.as_real_imag()\n    assert real_X == BlockMatrix([[re(A), re(B)], [re(B), re(A)]])\n    assert imag_X == BlockMatrix([[im(A), im(B)], [-im(B), -im(A)]])\n"], "sample_1187": ["def test_distance_to_side_3d():\n    point = (0, 0, 0)\n    assert distance_to_side(point, [(0, 0, 1), (0, 1, 0)], (1, 0, 0)) == -sqrt(2)/2\n    assert distance_to_side(point, [(0, 1, 0), (1, 0, 0)], (0, 0, 1)) == -sqrt(2)/2\n    assert distance_to_side(point, [(1, 0, 0), (0, 0, 1)], (0, 1, 0)) == -sqrt(2)/2\n", "def test_hyperplane_parameters():\n    polygon = Polygon(Point(0, 0), Point(0, 1), Point(1, 1), Point(1, 0))\n    params = hyperplane_parameters(polygon)\n    assert params == [((0, 1), 0), ((1, 0), 0), ((0, -1), 0), ((-1, 0), 1)]\n\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    params = hyperplane_parameters(faces, vertices)\n    assert params == [([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5),\n                      ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)]\n", "def test_polytope_integrate_3D_polygon():\n    # Test case for a 3D polygon\n    polygon = [(0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1)]\n    assert polytope_integrate(polygon, 1) == 1\n", "def test_integration_reduction_dynamic_2d():\n    square = Polygon(Point(0, 0), Point(0, 2), Point(2, 2), Point(2, 0))\n    facets = square.sides\n    a, b = hyperplane_parameters(square)[0]\n    x0 = facets[0].points[0]\n    monomial_values = [[0, 0, 0], [1, 0, 2], [y, 1, 0], [x, 2, 0]]\n\n    assert integration_reduction_dynamic(facets, 0, a, b, x*y, 2, (x, y), 1, 1, 1, x0, monomial_values, 3) == 2\n    assert integration_reduction_dynamic(facets, 0, a, b, x, 1, (x, y), 1, 0, 1, x0, monomial_values, 3) == 2\n    assert integration_reduction_dynamic(facets, 0, a, b, y, 1, (x, y), 0, 1, 1, x0, monomial_values, 3) == 2\n", "def test_integration_reduction_3d():\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    facets = cube[1:]\n    vertices = cube[0]\n    a, b = hyperplane_parameters(cube[1:], vertices)[0]\n    x0 = vertices[facets[0][0]]\n    monomial_values = [[[0, 0, 0, 0, 0, 0, 0, None], [1, 0, 0, 0, 1, 0, 0, 5],\\\n                        [y, 0, 1, 0, 1, 1, 0, 15], [z, 0, 0, 1, 1, 0, 1, 15],\\\n                        [x, 1, 0, 0, 1, 1, 0, None]]]\n\n    assert integration_reduction_dynamic(facets, 0, a, b, 1, (x, y, z), 1, 0, 1,\\\n                                         x0, monomial_values, 1) == Rational(25, 2)\n    assert integration_reduction_dynamic(facets, 0, a, b, 0, (x, y, z), 1, 0, 1,\\\n                                         x0, monomial_values, 1) == 0\n"], "sample_322": ["def test_unrelated_applied_migrations_mutate_state_backwards(self):\n    \"\"\"\n    #26647 - Unrelated applied migrations should be part of the final\n    state in both directions even when migrating backwards.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    executor.migrate([\n        ('mutate_state_a', '0001_initial'),\n        ('mutate_state_b', '0002_add_field'),\n    ])\n    # Migrate backward.\n    executor.loader.build_graph()\n    state = executor.migrate([\n        ('mutate_state_a', None),\n    ])\n    self.assertIn('added', state.models['mutate_state_b', 'b'].fields)\n    executor.loader.build_graph()\n    executor.migrate([\n        ('mutate_state_b', None),\n    ])\n", "def test_migrate_all_replaced_unapplies_individual_migrations(self):\n    \"\"\"\n    Unapplying a replacement migration unapplies its replaced migrations (#24628).\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    # Record all replaced migrations as applied\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    recorder.record_applied(\"migrations\", \"0001_squashed_0002\")\n    executor = MigrationExecutor(connection)\n    executor.migrate([(\"migrations\", None)])\n\n    # Both 0001 and 0002 should be marked as unapplied\n    self.assertNotIn((\"migrations\", \"0001_initial\"), recorder.applied_migrations())\n    self.assertNotIn((\"migrations\", \"0002_second\"), recorder.applied_migrations())\n", "def test_minimize_rollbacks_complex(self):\n    \"\"\"\n    Minimize rollbacks when target has multiple in-app children with circular dependencies.\n\n    a: 1 <---- 3 <--\\\n              / \\-- 2 <--- 4\n               \\       /\n        b:      \\- 1 <--- 2\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    a4_impl = FakeMigration('a4')\n    a4 = ('a', '4')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    b2_impl = FakeMigration('b2')\n    b2 = ('b', '2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(a4, a4_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(b2, b2_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a1)\n    graph.add_dependency(None, a4, a2)\n    graph.add_dependency(None, a4, a3)\n    graph.add_dependency(None, b2, b1)\n    graph.add_dependency(None, b1, a1)\n    graph.add_dependency(None, b2, a3)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        b1: b1_impl,\n        a2: a2_", "    def test_migration_plan_with_replaced_migrations(self):\n        \"\"\"\n        Test migration plan with replaced migrations.\n\n        When a replaced migration is not applied but its replacements are,\n        the migration plan should include the replaced migration and not the\n        replacements.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a2: a2_impl,\n            a3: a3_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a1_impl, False)])\n", "    def test_unapplied_migration_not_removed_from_state(self):\n        \"\"\"\n        Unapplied migrations are not removed from the state.\n\n        When migrating backwards, the state before the unapplied migration\n        should not be affected by the migration.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_dependency(None, a2, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n        states = {}\n        state = ProjectState()\n        for migration, _ in graph.forwards_plan((a2,)):\n            states[migration] = state\n            state = migration.mutate_state(state, preserve=True)\n\n        state = executor._migrate_all_backwards(plan, graph.forwards_plan((a2,)), fake=False)\n\n        self.assertEqual(state.models, states[a1].models)\n"], "sample_912": ["def test_pyclass_signature(app):\n    text = \".. py:class:: Class\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, ()])]))\n    assert 'Class' in domain.objects\n    assert domain.objects['Class'] == ('index', 'class', 'class')\n", "def test_pydecoratormethod_signature_in_class(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:decoratormethod:: deco\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_addname, \"@\"],\n                                                     [desc_name, \"deco\"])],\n                                   [desc_content, ()]))\n    assert 'Class.deco' in domain.objects\n    assert domain.objects['Class.deco'] == ('index', 'class.deco', 'method')\n", "def test_pyattribute_no_type_no_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],)],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'class.attr', 'attribute')\n", "def test_module_index_platform(app):\n    text = (\".. py:module:: sphinx\\n\"\n            \"   :platform: Linux\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', 'Linux', '', '')])],\n        True\n    )\n", "def test_pyfunction_signature_without_arglist(app):\n    text = \".. py:function:: hello()\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist)],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1], desc_parameterlist)\n"], "sample_155": ["def test_file_from_buffer_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='test.txt')\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"test.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_buffer_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='test.txt')\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"test.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_disk_with_custom_filename(self):\n    custom_filename = \"custom_file.txt\"\n    with open(__file__, 'rb') as file:\n        response = FileResponse(file, as_attachment=True, filename=custom_filename)\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n        self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n        self.assertEqual(response['Content-Disposition'], f'attachment; filename=\"{custom_filename}\"')\n        response.close()\n", "def test_file_from_disk_with_custom_filename(self):\n    custom_filename = \"custom_filename.txt\"\n    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n        temp_file.write(b'test content')\n        temp_file_path = temp_file.name\n\n    response = FileResponse(open(temp_file_path, 'rb'), filename=custom_filename)\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(temp_file_path)))\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"{}\"'.format(custom_filename))\n    response.close()\n\n    os.remove(temp_file_path)\n", "def test_file_response_with_custom_filename(self):\n    custom_filename = \"custom_file.txt\"\n    response = FileResponse(io.BytesIO(b'custom content'), filename=custom_filename)\n    self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{custom_filename}\"')\n    self.assertEqual(list(response), [b'custom content'])\n\n    response_as_attachment = FileResponse(io.BytesIO(b'custom content'), filename=custom_filename, as_attachment=True)\n    self.assertEqual(response_as_attachment['Content-Disposition'], f'attachment; filename=\"{custom_filename}\"')\n    self.assertEqual(list(response_as_attachment), [b'custom content'])\n"], "sample_625": ["def test_cross_multi_dimensional(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=dim)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_edge_cases(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=dim, axis=axis)\n", "def test_cross_edge_cases(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=dim)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_errors() -> None:\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"y\")\n\n    with pytest.raises(ValueError, match=\"Input arrays must have the same dimensions.\"):\n        xr.cross(a, b)\n\n    with pytest.raises(ValueError, match=\"Input arrays must have the same shape.\"):\n        xr.cross(a, a.isel(x=slice(0, 2)))\n\n    with pytest.raises(ValueError, match=\"Input arrays must have at least two dimensions.\"):\n        xr.cross(a[0], b[0])\n", "def test_polyfit_return_type():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    y = xr.DataArray([1, 6, 17], dims=\"x\")\n    y.coords[\"x\"] = x\n\n    fit = y.polyfit(dim=\"x\", deg=2)\n    assert isinstance(fit, xr.Dataset)\n    assert isinstance(fit.polyfit_coefficients, xr.DataArray)\n    assert isinstance(fit.x_fit, xr.DataArray)\n    assert \"degree\" in fit.polyfit_coefficients.dims\n\n    fit = y.polyfit(dim=\"x\", deg=2, full=True)\n    assert isinstance(fit, xr.Dataset)\n    assert isinstance(fit.polyfit_coefficients, xr.DataArray)\n    assert isinstance(fit.residuals, xr.DataArray)\n    assert isinstance(fit.rank, xr.DataArray)\n    assert isinstance(fit.singular_values, xr.DataArray)\n    assert isinstance(fit.rcond, xr.DataArray)\n    assert \"degree\" in fit.polyfit_coefficients.dims\n"], "sample_137": ["    def test_replace_named_groups(self):\n        from django.contrib.admindocs.utils import replace_named_groups\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), r'^<a>/b/<c>/$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)'), r'^<a>/b/(\\w+)')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), r'^<a>/b/<c>')\n", "def test_replace_named_groups(self):\n    pattern = \"^(?P<a>\\w+)/b/(?P<c>\\w+)/$\"\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, \"^<a>/b/<c>/$\")\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, r'^<a>/b/<c>/$')\n", "    def test_replace_named_groups(self):\n        from django.contrib.admindocs.utils import replace_named_groups\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), r'^<a>/b/<c>/$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)'), r'^<a>/b/(\\w+)')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), r'^<a>/b/<c>')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/((x|y)\\w+)'), r'^<a>/b/<var>')\n", "    def test_parse_rst_with_role_override(self):\n        \"\"\"\n        parse_rst() should allow overriding the default role.\n        \"\"\"\n        markup = '<p><a class=\"reference external\" href=\"/admindocs/%s\">title</a></p>\\n'\n        self.assertEqual(parse_rst(':model:`title`', 'view'), markup % 'models/title/')\n        self.assertEqual(parse_rst(':view:`title`', 'model'), markup % 'views/title/')\n        self.assertEqual(parse_rst(':template:`title`', 'view'), markup % 'templates/title/')\n        self.assertEqual(parse_rst(':filter:`title`', 'view'), markup % 'filters/#title')\n        self.assertEqual(parse_rst(':tag:`title`', 'view'), markup % 'tags/#title')\n"], "sample_125": ["def test_cookie_domain(self):\n    \"\"\"set_cookie() accepts domain.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('example', domain='example.com')\n    example_cookie = response.cookies['example']\n    self.assertEqual(example_cookie['domain'], 'example.com')\n", "def test_set_cookie_with_path_domain(self):\n    \"\"\"\n    set_cookie() sets the path and domain when provided.\n    \"\"\"\n    response = HttpResponse()\n    response.set_cookie('example', path='/test', domain='example.com')\n    example_cookie = response.cookies['example']\n    self.assertEqual(example_cookie['path'], '/test')\n    self.assertEqual(example_cookie['domain'], 'example.com')\n", "def test_set_cookie_with_path(self):\n    \"\"\"set_cookie() accepts a path.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('path', 'test', path='/test_path/')\n    path_cookie = response.cookies['path']\n    self.assertEqual(path_cookie['path'], '/test_path/')\n", "def test_set_cookie_domain(self):\n    \"\"\"set_cookie() accepts a domain attribute.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('example', domain='example.com')\n    example_cookie = response.cookies['example']\n    self.assertEqual(example_cookie['domain'], 'example.com')\n", "def test_default_charset(self):\n    \"\"\"Test the default charset is used when none is specified.\"\"\"\n    response = HttpResponse()\n    self.assertEqual(response.charset, 'utf-8')  # Default Django charset is 'utf-8'\n\n    response = HttpResponse(content_type='text/plain')\n    self.assertEqual(response.charset, 'utf-8')  # Default charset is used\n"], "sample_457": ["    def test_repr_with_violation_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"foo\", \"bar\"],\n            name=\"unique_fields\",\n            violation_error_message=\"Custom violation error message.\",\n        )\n        self.assertEqual(\n            repr(constraint),\n            \"<UniqueConstraint: fields=('foo', 'bar') name='unique_fields'>\",\n        )\n", "def test_validate_nullable_field_with_none(self):\n    constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n    invalid_product = UniqueConstraintProduct(name=None)\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, invalid_product)\n", "def test_deconstruction_with_expressions_and_condition(self):\n    name = \"unique_fields\"\n    condition = models.Q(foo=models.F(\"bar\"))\n    constraint = models.UniqueConstraint(\n        Lower(\"title\"),\n        name=name,\n        condition=condition,\n    )\n    path, args, kwargs = constraint.deconstruct()\n    self.assertEqual(path, \"django.db.models.UniqueConstraint\")\n    self.assertEqual(args, (Lower(\"title\"),))\n    self.assertEqual(\n        kwargs, {\"name\": name, \"condition\": condition}\n    )\n", "    def test_validate_with_null_values(self):\n        # Test the validation of unique constraints with null values.\n        null_product = UniqueConstraintProduct(name=None, color=None)\n        constraint = UniqueConstraintProduct._meta.constraints[0]\n        # With null values, the constraint should not be violated.\n        constraint.validate(UniqueConstraintProduct, null_product)\n", "def test_validate_expression_with_order_by(self):\n    constraint = models.UniqueConstraint(Lower(\"name\").desc(), name=\"name_lower_uniq_desc\")\n    # Reverse the case of the name for an existing product\n    reversed_name_product = UniqueConstraintProduct(name=self.p1.name[::-1])\n    msg = \"Constraint \u201cname_lower_uniq_desc\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, reversed_name_product)\n    # A new product with a name that doesn't violate the constraint\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\"),\n    )\n    # Existing instances have their existing row excluded\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded\n    constraint.validate(\n        UniqueConstraintProduct,\n        reversed_name_product,\n        exclude={\"name\"},\n    )\n"], "sample_67": ["def test_many_to_many_add(self):\n    \"\"\"Adding to a ManyToManyField works correctly.\"\"\"\n    blue = Colour.objects.create(name='blue')\n    item = ColourfulItem.objects.create()\n    item.colours.add(blue)\n    data = model_to_dict(item)['colours']\n    self.assertEqual(data, [blue])\n", "    def test_foreign_key_to_string(self):\n        author = Author.objects.create(full_name='John Doe')\n        book = Book.objects.create(title='Sample Book', author=author)\n        data = model_to_dict(book, fields=['title', 'author'])\n        self.assertEqual(data['author'], 'John Doe')\n", "def test_many_to_many_update_method(self):\n    \"\"\"Data for a ManyToManyField is updated correctly using the update method.\"\"\"\n    blue = Colour.objects.create(name='blue')\n    red = Colour.objects.create(name='red')\n    item = ColourfulItem.objects.create()\n    item.colours.set([blue])\n    data = model_to_dict(item)\n    self.assertEqual(data['colours'], [blue])\n    item.colours.set([red])\n    model_to_dict(item, instance=item, fields=['colours'])\n    self.assertEqual(data['colours'], [red])\n", "    def test_model_to_dict_exclude(self):\n        blue = Colour.objects.create(name='blue')\n        item = ColourfulItem.objects.create()\n        item.colours.set([blue])\n        data = model_to_dict(item, exclude=['colours'])\n        self.assertEqual(data, {'id': item.id, 'name': item.name})\n", "def test_fields_for_model_applies_localize_to_fields(self):\n    fields = fields_for_model(Triple, ['left', 'middle', 'right'], localize_fields=('left', 'right'))\n    self.assertTrue(fields['left'].localize)\n    self.assertFalse(fields['middle'].localize)\n    self.assertTrue(fields['right'].localize)\n"], "sample_627": ["def test_concat_index_not_in_all_datasets() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"y\": (\"y\", [3, 4])})\n\n    with pytest.raises(\n        ValueError, match=r\"'x' must have either an index or no index in all datasets.*\"\n    ):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_single_dataset() -> None:\n    ds = Dataset({\"foo\": 1}, {\"bar\": 2})\n\n    # concatenate a single dataset must return the same dataset\n    actual = concat([ds], dim=\"foo\")\n    assert_identical(actual, ds)\n", "def test_concat_dim_not_in_all_datasets() -> None:\n    ds1 = Dataset(data_vars={\"a\": (\"y\", [0.1])}, coords={\"x\": 0.1})\n    ds2 = Dataset(data_vars={\"a\": (\"z\", [0.2])}, coords={\"x\": 0.2})\n    with pytest.raises(\n        ValueError, match=r\"'y' not present in all datasets and coords='different'.\"\n    ):\n        concat([ds1, ds2], dim=\"y\", coords=\"different\")\n", "def test_concat_unlabeled_dimension() -> None:\n    # Test concatenation when the concat dimension is not labeled\n    ds1 = Dataset({\"foo\": ((\"x\", \"y\"), np.random.rand(2, 3))}, {\"x\": [0, 1]})\n    ds2 = Dataset({\"foo\": ((\"x\", \"y\"), np.random.rand(2, 3))}, {\"x\": [2, 3]})\n\n    expected = Dataset(\n        {\n            \"foo\": ((\"concat_dim\", \"x\", \"y\"), np.concatenate([ds1[\"foo\"].values, ds2[\"foo\"].values], axis=0)),\n            \"concat_dim\": np.arange(2),\n        },\n        {\"x\": [0, 1, 2, 3], \"y\": ds1[\"y\"]},\n    )\n\n    actual = concat([ds1, ds2], dim=\"concat_dim\")\n\n    assert_identical(expected, actual)\n", "def test_concat_dim_is_coord_and_variable():\n    # Test the case where the dimension is both a coordinate and a variable\n    data = np.random.rand(2, 3)\n    ds1 = Dataset({\"A\": ((\"x\", \"y\"), data)}, coords={\"x\": [0, 1], \"y\": [0, 1, 2]})\n    ds2 = Dataset({\"A\": ((\"x\", \"y\"), data)}, coords={\"x\": [2, 3], \"y\": [0, 1, 2]})\n\n    # Ensure the error is raised when dimension is both a variable and a coordinate\n    with pytest.raises(ValueError, match=\"'x' is a coordinate in some datasets but not others.\"):\n        concat([ds1, ds2], dim=\"x\")\n"], "sample_606": ["def test_cross_dask(use_dask):\n    if use_dask and not has_dask:\n        pytest.skip(\"test for dask.\")\n\n    a = xr.DataArray(\n        [[1, 2, 3], [4, 5, 6]],\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n    b = xr.DataArray(\n        [[4, 5, 6], [1, 2, 3]],\n        dims=(\"time\", \"cartesian\"),\n        coords=dict(\n            time=([\"time\"], [0, 1]),\n            cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"]),\n        ),\n    )\n\n    if use_dask:\n        a = a.chunk()\n        b = b.chunk()\n\n    expected = np.cross(a, b, axis=-1)\n    actual = xr.cross(a, b, dim=\"cartesian\")\n\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_invalid_inputs():\n    a = xr.DataArray(\n        [1, 2, 3],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    b = xr.DataArray(\n        [4, 5, 6, 7],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\", \"w\"])),\n    )\n    with pytest.raises(ValueError, match=\"The size of 'cartesian' on b must be 1, 2, or 3 to be compatible with a cross product but is 4\"):\n        xr.cross(a, b, dim=\"cartesian\")\n\n    a = xr.DataArray(\n        [1, 2],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    b = xr.DataArray(\n        [4, 5, 6],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    with pytest.raises(ValueError, match=\"The size of 'cartesian' on a must be 1, 2, or 3 to be compatible with a cross product but is 2\"):\n        xr.cross(a, b, dim=\"cartesian\")\n", "def test_cross_errors():\n    a = xr.DataArray(\n        [1, 2, 3],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    b = xr.DataArray(\n        [4, 5, 6],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"invalid_dim\")\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"cartesian_invalid\")\n    with pytest.raises(ValueError):\n        xr.cross(a, b.isel(cartesian=slice(0, 2)))\n    with pytest.raises(ValueError):\n        xr.cross(a.isel(cartesian=slice(0, 2)), b)\n", "def test_cross_errors(use_dask: bool) -> None:\n    a = xr.DataArray(\n        np.arange(3),\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    b = xr.DataArray(\n        np.arange(4),\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\", \"w\"])),\n    )\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError, match=\"Dimension 'cartesian' not on b\"):\n        xr.cross(a, \"not a data array\", dim=\"cartesian\")\n\n    with pytest.raises(ValueError, match=\"Dimension 'nonexistent' not on a\"):\n        xr.cross(a, b, dim=\"nonexistent\")\n\n    with pytest.raises(ValueError, match=\"The size of 'cartesian' on a must be\"):\n        xr.cross(a, b, dim=\"cartesian\")\n\n    with pytest.raises(ValueError, match=\"The size of 'cartesian' on b must be\"):\n        xr.cross(b, a, dim=\"cartesian\")\n", "def test_cross_invalid_dims() -> None:\n    a = xr.DataArray(\n        [1, 2, 3],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    b = xr.DataArray(\n        [4, 5, 6],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    with pytest.raises(ValueError, match=r\"Dimension 'invalid_dim' not on a\"):\n        xr.cross(a, b, dim=\"invalid_dim\")\n    with pytest.raises(ValueError, match=r\"Dimension 'invalid_dim' not on b\"):\n        xr.cross(a, b, dim=\"invalid_dim\")\n"], "sample_867": ["def test_search_cv_with_pandas_input():\n    try:\n        import pandas as pd\n    except ImportError:\n        pytest.skip(\"pandas is required for this test\")\n\n    X = pd.DataFrame(np.arange(100).reshape(10, 10))\n    y = pd.Series(np.array([0] * 5 + [1] * 5))\n\n    clf = CheckingClassifier(check_X=lambda x: isinstance(x, pd.DataFrame),\n                             check_y=lambda x: isinstance(x, pd.Series))\n    cv = KFold(n_splits=3)\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=cv)\n    grid_search.fit(X, y)\n    assert hasattr(grid_search, \"cv_results_\")\n", "def test_grid_search_with_precomputed_kernel_and_sparse_input():\n    # Test that grid search works with precomputed kernel and sparse input\n    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)\n\n    # compute the training kernel matrix corresponding to the linear kernel\n    K_train = np.dot(X_[:180], X_[:180].T)\n    y_train = y_[:180]\n\n    # convert to sparse matrix\n    K_train_sparse = sp.csr_matrix(K_train)\n\n    clf = SVC(kernel='precomputed')\n    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})\n    cv.fit(K_train_sparse, y_train)\n\n    assert cv.best_score_ >= 0\n\n    # compute the test kernel matrix\n    K_test = np.dot(X_[180:], X_[:180].T)\n    y_test = y_[180:]\n\n    # convert to sparse matrix\n    K_test_sparse = sp.csr_matrix(K_test)\n\n    y_pred = cv.predict(K_test_sparse)\n\n    assert np.mean(y_pred == y_test) >= 0\n", "def test_search_cv_iter_property():\n    \"\"\"\n    Test implementation of BaseSearchCV has the __iter__ property\n    which returns an iterator over the candidate parameters.\n    \"\"\"\n    params = {'C': [0.1, 1, 10], 'gamma': [0.1, 1]}\n    grid_search = GridSearchCV(SVC(), param_grid=params)\n    param_grid = list(ParameterGrid(params))\n    for param, param_grid_param in zip(grid_search, param_grid):\n        assert param == param_grid_param\n", "def test_search_cv_stratify_with_groups():\n    # Test that an error is raised if both stratify and groups are provided\n    X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n    groups = np.random.randint(0, 3, 100)\n\n    grid_search = GridSearchCV(SVC(), param_grid={'C': [1, 10]}, cv=3)\n    with pytest.raises(ValueError, match=\"Stratified cross-validation is not supported with a custom grouping.\"):\n        grid_search.fit(X, y, groups=groups, stratify=y)\n\n    random_search = RandomizedSearchCV(SVC(), param_distributions={'C': [1, 10]}, cv=3, n_iter=10)\n    with pytest.raises(ValueError, match=\"Stratified cross-validation is not supported with a custom grouping.\"):\n        random_search.fit(X, y, groups=groups, stratify=y)\n", "def test_search_cv_verbose_message():\n    # Use global X, y\n    train_size = 100\n    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},\n                               cv=3, n_jobs=4, verbose=1)\n    with capture_stdout() as out:\n        ridge.fit(X[:train_size], y[:train_size])\n    output = out.getvalue()\n    assert \"Fitting 3 folds for each of 3 candidates, totalling 9 fits\" in output\n    assert \"SearchCV verbose messages are printed with the 'verbose' option.\" in output\n    assert \"Fitting 3 folds for each of 3 candidates, totalling 9 fits\" in output\n"], "sample_178": ["def test_formset_empty_form_prefix(self):\n    \"\"\"The empty form has the correct prefix.\"\"\"\n    formset = FavoriteDrinksFormSet()\n    self.assertEqual(formset.empty_form.prefix, 'form__prefix__')\n", "def test_add_fields_method(self):\n    \"\"\"\n    The add_fields method adds extra fields on to each form instance if\n    can_order or can_delete is True.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True)\n    formset = ChoiceFormSet()\n    self.assertTrue('ORDER' in formset.forms[0].fields)\n    self.assertTrue('DELETE' in formset.forms[0].fields)\n", "def test_formset_with_deletion_and_ordering_and_validation_error(self):\n    \"\"\"FormSets with ordering, deletion, and validation error.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '4',\n        'choices-INITIAL_FORMS': '3',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-0-ORDER': '1',\n        'choices-0-DELETE': '',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-1-ORDER': '2',\n        'choices-1-DELETE': 'on',\n        'choices-2-choice': 'The Decemberists',\n        'choices-2-votes': '',  # <-- this vote is missing but required\n        'choices-2-ORDER': '0',\n        'choices-2-DELETE': '',\n        'choices-3-choice': '',\n        'choices-3-votes': '',\n        'choices-3-ORDER': '',\n        'choices-3-DELETE': '',\n    }\n    ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data for form in formset.ordered_forms],\n        [\n            {'votes': 100, 'DELETE': False, 'ORDER': 1, 'choice': 'Calexico'},\n        ],\n    )\n    self.assertEqual(\n        [form.cleaned_data for form in formset.deleted_forms],\n        [{'votes': 900, 'DELETE': True, 'ORDER': 2, 'choice': 'Fergie", "def test_formset_with_file_field(self):\n    \"\"\"Formsets with a FileField handle multipart form data correctly.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    FileFormSet = formset_factory(FileForm)\n    formset = FileFormSet()\n    self.assertTrue(formset.is_multipart())\n\n    # Simulate a file upload\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    files = {\n        'form-0-file': SimpleUploadedFile(\"test_file.txt\", b\"file_content\"),\n    }\n    formset = FileFormSet(data, files=files)\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.cleaned_data, [{'file': files['form-0-file']}])\n", "def test_formset_ordering_invalid_form(self):\n    \"\"\"ordering_forms works if a form is invalid\"\"\"\n    FavoriteDrinkFormset = formset_factory(form=FavoriteDrinkForm, can_order=True)\n    formset = FavoriteDrinkFormset({\n        'form-0-name': '',\n        'form-TOTAL_FORMS': 1,\n        'form-INITIAL_FORMS': 1,\n        'form-MIN_NUM_FORMS': 0,\n        'form-MAX_NUM_FORMS': 1,\n        'form-0-ORDER': '1',\n    })\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.ordered_forms, [])\n"], "sample_1124": ["def test_FracElement_conversion():\n    F, x, y = field(\"x,y\", ZZ)\n    R, x, y = ring(\"x,y\", ZZ)\n\n    f = F(x*y, x**2)\n    r = R(f)\n    assert r == x*y/x**2\n\n    f = F(x*y/x**2)\n    r = R(f)\n    assert r == y/x\n", "def test_FracElement_set_field():\n    F1, x, y, z = field(\"x,y,z\", ZZ)\n    F2, x, y, z = field(\"x,y,z\", QQ)\n    f = F1(x*y/3*z)\n\n    g = f.set_field(F2)\n    assert f != g\n    assert f.numer == g.numer\n    assert f.denom == g.denom\n    assert g.field == F2\n", "def test_FracElement_complex():\n    F, x, y, z = field(\"x, y, z\", ZZ)\n    f = (x + I*y)/(z - I)\n    g = (x - I*y)/(z + I)\n\n    assert f == g\n    assert f.numer == g.numer\n    assert f.denom == g.denom\n", "def test_FracElement_to_expr():\n    x, y, z = symbols(\"x, y, z\")\n    F, X, Y, Z = field((x, y, z), ZZ)\n\n    f = F((3*x + 2*y)/(z**2 + 1))\n    g = (3*X + 2*Y)/(Z**2 + 1)\n\n    assert f.as_expr() == g\n\n    X, Y, Z = symbols(\"X, Y, Z\")\n    g = (3*X + 2*Y)/(Z**2 + 1)\n\n    assert f.as_expr(X, Y, Z) == g\n", "def test_FracElement_from_poly():\n    F, x, y, z = field(\"x,y,z\", ZZ)\n    P, x, y, z = ring(\"x,y,z\", ZZ)\n    f = (x**2 + 3*y)/z\n    g = F.from_poly(f.numer)\n\n    assert g == f\n    assert isinstance(g, FracElement)\n\n    h = g.to_poly()\n    assert h == f.numer\n    assert isinstance(h, P.dtype)\n"], "sample_100": ["def test_extra_files(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "def test_extra_files_are_watched(self):\n    extra_file = self.ensure_file(self.tempdir / 'extra_file.py')\n    self.reloader.extra_files.add(extra_file)\n    watched_files = list(self.reloader.watched_files())\n    self.assertIn(extra_file, watched_files)\n", "def test_tick_triggers_on_mtime_change(self, mock_notify_file_changed):\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        self.increment_mtime(self.existing_file)\n        next(ticker)\n        mock_notify_file_changed.assert_called_once_with(self.existing_file)\n", "def test_check_availability(self):\n    self.assertTrue(self.reloader.check_availability())\n", "def test_extra_file(self, mocked_modules, notify_mock):\n    self.reloader.extra_files.add(self.existing_file)\n    with self.tick_twice():\n        self.increment_mtime(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n"], "sample_755": ["def test_silhouette_sparse_input():\n    # Tests the Silhouette Coefficient with sparse input.\n    dataset = datasets.load_iris()\n    X_dense = dataset.data\n    X_csr = csr_matrix(X_dense)\n    X_dok = sp.dok_matrix(X_dense)\n    X_lil = sp.lil_matrix(X_dense)\n    y = dataset.target\n\n    # Test the silhouette_score and silhouette_samples functions with sparse input\n    for X in [X_csr, X_dok, X_lil]:\n        score_sparse = silhouette_score(X, y, metric='euclidean')\n        score_dense = silhouette_score(X_dense, y, metric='euclidean')\n        pytest.approx(score_sparse, score_dense)\n\n        ss_sparse = silhouette_samples(X, y, metric='euclidean')\n        ss_dense = silhouette_samples(X_dense, y, metric='euclidean')\n        assert_array_equal(ss_sparse, ss_dense)\n", "def test_silhouette_dense_matrix_input():\n    # Test silhouette_score and silhouette_samples with dense matrix input\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n    labels = np.array([0, 0, 1, 1])\n    expected_scores = np.array([0.5, 0.5, 0.5, 0.5])\n    expected_avg_score = 0.5\n\n    scores = silhouette_samples(X, labels)\n    np.testing.assert_array_almost_equal(scores, expected_scores)\n    avg_score = silhouette_score(X, labels)\n    assert avg_score == expected_avg_score\n", "def test_silhouette_samples_zero_variance():\n    # Test the case where all samples are identical\n    X = np.ones((10, 2))\n    labels = [0] * 10\n    expected_silhouette = np.zeros(10)\n    pytest.approx(silhouette_samples(X, labels), expected_silhouette)\n", "def test_silhouette_samples_sparse_input():\n    # Test silhouette_samples with sparse input\n    X_csr = csr_matrix([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    labels = [0, 1, 1, 2, 2]\n    expected_output = [0.5, 0.5, 0.5, -0.5, -0.5]\n    output = silhouette_samples(X_csr, labels)\n    assert_array_equal(output, expected_output)\n", "def test_silhouette_with_sparse_input():\n    # Test the Silhouette Coefficient with sparse input\n    dataset = datasets.load_iris()\n    X_csr = csr_matrix(dataset.data)\n    y = dataset.target\n\n    D = pairwise_distances(X_csr, metric='euclidean')\n    score_precomputed = silhouette_score(D, y, metric='precomputed')\n    score_euclidean = silhouette_score(X_csr, y, metric='euclidean')\n\n    assert score_precomputed > 0\n    pytest.approx(score_precomputed, score_euclidean)\n"], "sample_879": ["def test_ordinal_encoder_missing_unknown_encoding_min():\n    \"\"\"Check missing value or unknown encoding can equal the negative cardinality.\"\"\"\n    X = np.array([[\"dog\"], [\"cat\"], [np.nan]], dtype=object)\n    X_trans = OrdinalEncoder(encoded_missing_value=-1).fit_transform(X)\n    assert_allclose(X_trans, [[1], [0], [-1]])\n\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(X)\n    X_test = np.array([[\"snake\"]])\n    X_trans = enc.transform(X_test)\n    assert_allclose(X_trans, [[-1]])\n", "def test_ordinal_encoder_unknown_value_error():\n    \"\"\"Check that an error is raised when unknown_value is not an integer or np.nan.\"\"\"\n    X = np.array([[\"dog\"], [\"cat\"], [\"bird\"]], dtype=object)\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=\"unknown\")\n    with pytest.raises(TypeError, match=\"unknown_value should be an integer or np.nan\"):\n        enc.fit(X)\n", "def test_ordinal_encoder_unknown_value_error():\n    \"\"\"Check that an error is raised when unknown_value is not an integer or np.nan.\"\"\"\n    X = np.array([[\"a\"], [\"b\"], [\"c\"]], dtype=object)\n    enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=\"unknown\")\n\n    err_msg = \"unknown_value should be an integer or np.nan when handle_unknown is 'use_encoded_value', got unknown.\"\n    with pytest.raises(TypeError, match=err_msg):\n        enc.fit(X)\n", "def test_ordinal_encoder_missing_unknown_encoding_out_of_bounds():\n    \"\"\"Check that errors are raised for encoded_missing_value and unknown_value out of bounds.\"\"\"\n    X = np.array([[\"dog\"], [\"cat\"]], dtype=object)\n    with pytest.raises(ValueError, match=\"encoded_missing_value \\\\(-2\\\\) is out of bounds.\"):\n        OrdinalEncoder(encoded_missing_value=-2).fit(X)\n    with pytest.raises(ValueError, match=\"unknown_value \\\\(-3\\\\) is out of bounds.\"):\n        OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-3).fit(X)\n", "def test_ordinal_encoder_set_output_pandas_with_sparse_output():\n    \"\"\"Check OrdinalEncoder with sparse_output=True and set_output=pandas\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame({\"A\": [\"a\", \"b\"], \"B\": [1, 2]})\n\n    ord_pandas = OrdinalEncoder().set_output(transform=\"pandas\")\n\n    # This should not raise an error as sparse_output=True is not considered\n    X_pandas = ord_pandas.fit_transform(X_df)\n    assert isinstance(X_pandas, pd.DataFrame)\n"], "sample_335": ["def test_decimalfield_max_value(self):\n    f = DecimalField(max_value=100.5)\n    self.assertEqual(f.clean('100.5'), decimal.Decimal('100.5'))\n    with self.assertRaisesMessage(ValidationError, \"'Ensure this value is less than or equal to 100.5.'\"):\n        f.clean('100.6')\n", "def test_decimalfield_validation_error_messages(self):\n    f = DecimalField(max_digits=4, decimal_places=2, error_messages={'invalid': 'Invalid decimal number'})\n    with self.assertRaisesMessage(ValidationError, 'Invalid decimal number'):\n        f.clean('abc')\n", "def test_decimalfield_validate(self):\n    f = DecimalField(max_digits=4, decimal_places=2)\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.validate(float('inf'))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.validate(float('nan'))\n    f.validate(decimal.Decimal('0.1'))\n    f.validate(decimal.Decimal('0.0'))\n    f.validate(decimal.Decimal('-0.1'))\n", "def test_decimalfield_invalid_input(self):\n    f = DecimalField(max_digits=4, decimal_places=2)\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean('invalid')\n    with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n        f.clean(None)\n", "def test_decimalfield_invalid_input(self):\n    f = DecimalField(max_digits=4, decimal_places=2)\n    invalid_values = ['1.2345', 'abc', '1.2.3', '1..23', '1,23', '1,234', '1.23,4']\n    for value in invalid_values:\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean(value)\n"], "sample_222": ["def test_unlock_without_lock(self):\n    file_path = Path(__file__).parent / 'test.png'\n    with open(file_path) as f:\n        self.assertIs(locks.unlock(f), False)\n", "def test_unlock_without_lock(self):\n    file_path = Path(__file__).parent / 'test.png'\n    with open(file_path) as f:\n        self.assertIs(locks.unlock(f), True)\n", "    def test_lock_exception(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path) as f1:\n            self.assertIs(locks.lock(f1, locks.LOCK_EX), True)\n            with self.assertRaises(Exception):\n                # Try to lock the same file again, which should raise an exception\n                locks.lock(f1, locks.LOCK_EX)\n", "    def test_lock_unlock_exclusive(self):\n        \"\"\"\n        Exclusive locks can be acquired and released.\n        \"\"\"\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path, 'rb') as f:\n            self.assertIs(locks.lock(f, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(f), True)\n", "def test_non_blocking_lock(self):\n    file_path = Path(__file__).parent / 'test.png'\n    with open(file_path) as f1, open(file_path) as f2:\n        self.assertIs(locks.lock(f1, locks.LOCK_EX), True)\n        self.assertIs(locks.lock(f2, locks.LOCK_EX | locks.LOCK_NB), False)\n        self.assertIs(locks.lock(f2, locks.LOCK_SH | locks.LOCK_NB), True)\n        self.assertIs(locks.unlock(f2), True)\n        self.assertIs(locks.unlock(f1), True)\n"], "sample_392": ["def test_key_transform_gt(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gt=10),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__gt=4),\n        [self.objs[4]],\n    )\n", "def test_key_exact_integer_string(self):\n    self.assertIs(\n        NullableJSONModel.objects.filter(value__c__exact=\"14\").exists(), True\n    )\n", "def test_key_lookup_transforms(self):\n    # Testing the less than (lt) lookup transform\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lt=15),\n        [self.objs[3], self.objs[4]],\n    )\n\n    # Testing the less than or equal (lte) lookup transform\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lte=14),\n        [self.objs[3], self.objs[4]],\n    )\n\n    # Testing the greater than (gt) lookup transform\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gt=13),\n        [self.objs[3], self.objs[4]],\n    )\n\n    # Testing the greater than or equal (gte) lookup transform\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gte=14),\n        [self.objs[3], self.objs[4]],\n    )\n", "def test_key_transform_lookup_expressions(self):\n    tests = [\n        (Q(value__d__0__exact=\"e\"), self.objs[4]),\n        (Q(value__d__0__iexact=\"E\"), self.objs[4]),\n        (Q(value__d__0__icontains=\"E\"), self.objs[4]),\n        (Q(value__d__0__startswith=\"e\"), self.objs[4]),\n        (Q(value__d__0__istartswith=\"E\"), self.objs[4]),\n        (Q(value__d__0__endswith=\"e\"), self.objs[4]),\n        (Q(value__d__0__iendswith=\"E\"), self.objs[4]),\n        (Q(value__d__0__regex=r\"^e$\"), self.objs[4]),\n        (Q(value__d__0__iregex=r\"^E$\"), self.objs[4]),\n        (Q(value__d__1__contains={\"f\": \"g\"}), self.objs[4]),\n        (Q(value__d__1__contains=[\"f\"]), self.objs[4]),\n    ]\n    for condition, expected in tests:\n        with self.subTest(condition=condition):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(condition),\n                [expected],\n            )\n", "def test_key_transform_complex_lookup(self):\n    self.assertIs(\n        NullableJSONModel.objects.filter(\n            value__baz__c__lt=10, value__baz__a__exact='b'\n        ).exists(),\n        True,\n    )\n"], "sample_1109": ["def test_floor_ceiling_complex_numbers():\n    # Test floor and ceiling functions with complex numbers\n    assert floor(3 + 2*I) == 3 + 2*I\n    assert ceiling(3 + 2*I) == 3 + 2*I\n    assert floor(3.5 + 2.5*I) == 3 + 2*I\n    assert ceiling(3.5 + 2.5*I) == 4 + 3*I\n    assert floor(-3 - 2*I) == -3 - 2*I\n    assert ceiling(-3 - 2*I) == -3 - 2*I\n    assert floor(-3.5 - 2.5*I) == -4 - 3*I\n    assert ceiling(-3.5 - 2.5*I) == -3 - 2*I\n", "def test_frac_integer_argument():\n    assert frac(3) == 0\n    assert frac(-3) == 0\n    assert frac(0) == 0\n\n    assert frac(3, evaluate=False) == 0\n    assert frac(-3, evaluate=False) == 0\n    assert frac(0, evaluate=False) == 0\n\n    assert frac(3, evaluate=False).is_zero\n    assert frac(-3, evaluate=False).is_zero\n    assert frac(0, evaluate=False).is_zero\n", "def test_frac_symbol_real_argument():\n    r = Symbol('r', real=True)\n    assert frac(r) >= 0\n    assert frac(r) < 1\n    assert frac(-r) >= -1\n    assert frac(-r) < 0\n", "def test_frac_comparison():\n    assert frac(pi) < 1\n    assert frac(-pi) < 0\n    assert frac(pi) > 0\n    assert frac(-pi) > -1\n    assert frac(2*pi) < 1\n    assert frac(-2*pi) > -1\n\n    assert frac(pi) <= 1\n    assert frac(-pi) <= 0\n    assert frac(pi) >= 0\n    assert frac(-pi) >= -1\n    assert frac(2*pi) <= 2\n    assert frac(-2*pi) >= -2\n\n    assert frac(pi) != 1\n    assert frac(-pi) != -1\n    assert frac(2*pi) != 2\n    assert frac(-2*pi) != -2\n\n    assert frac(pi) != 0\n    assert frac(-pi) != 0\n    assert frac(2*pi) != 1\n    assert frac(-2*pi) != -1\n", "def test_frac_complex_arguments():\n    # Testing frac with complex arguments\n    assert frac(I*r) == I*frac(r)\n    assert frac(1 + I*r) == I*frac(r) + frac(1)\n    assert frac(0.5 + I*r) == 0.5 + I*frac(r)\n    assert frac(n + I*r) == I*frac(r)\n    assert frac(n + I*k) == 0\n    assert frac(x + I*n) == frac(x)\n    assert frac(x + I*x) == I*frac(x)\n\n    # Testing frac with complex arguments and relations\n    assert frac(I*r) < 1\n    assert frac(I*r) > -1\n    assert frac(1 + I*r) < 1 + I\n    assert frac(1 + I*r) > I\n\n    # Testing frac with complex arguments and eq/ne\n    assert Eq(frac(I*r), I*frac(r))\n    assert Ne(frac(1 + I*r), 1 + I*r)\n"], "sample_310": ["    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_status_count'), 'Integer')\n        self.assertEqual(get_return_data_type('get_groups_list'), 'List')\n        self.assertEqual(get_return_data_type('get_unknown_type'), '')\n", "    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_status_count'), 'Integer')\n        self.assertEqual(get_return_data_type('get_groups_list'), 'List')\n        self.assertEqual(get_return_data_type('some_method'), '')\n", "def test_model_detail_related_objects(self):\n    response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n    self.assertContains(response, 'company')\n    self.assertContains(response, 'Number of related <a class=\"reference external\" href=\"/admindocs/models/admin_docs.family/\">admin_docs.Family</a> objects')\n    self.assertContains(response, 'All related <a class=\"reference external\" href=\"/admindocs/models/admin_docs.family/\">admin_docs.Family</a> objects')\n    self.assertContains(response, 'Number of related <a class=\"reference external\" href=\"/admindocs/models/admin_docs.group/\">admin_docs.Group</a> objects')\n    self.assertContains(response, 'All related <a class=\"reference external\" href=\"/admindocs/models/admin_docs.group/\">admin_docs.Group</a> objects')\n", "    def test_field_with_subclasses(self):\n        class CustomIntegerField(models.IntegerField):\n            description = \"A custom integer field\"\n\n        self.assertEqual(views.get_readable_field_data_type(CustomIntegerField()), 'A custom integer field')\n", "    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_list'), 'List')\n        self.assertEqual(get_return_data_type('get_count'), 'Integer')\n        self.assertEqual(get_return_data_type('get_item'), '')\n        self.assertEqual(get_return_data_type('set_value'), '')\n"], "sample_1053": ["def test_Float_floordiv():\n    assert Float(3.7)//Float(2) == 1.0\n", "def test_Float_floor_ceiling():\n    a = Float('3.2')\n    assert(a.floor() == 3)\n    assert(a.ceiling() == 4)\n", "def test_mod_inverse_error():\n    raises(ZeroDivisionError, lambda: mod_inverse(2, 0))\n    raises(ZeroDivisionError, lambda: mod_inverse(0, 5))\n", "def test_Integer_floordiv():\n    a = Integer(10)\n    b = Integer(3)\n    assert a // b == 3\n    assert -a // b == -4\n    assert a // -b == -4\n    assert -a // -b == 3\n    assert a // 0 == zoo\n", "def test_Float_floordiv():\n    x = Float('3.7')\n    assert x // 2 == Float('1.0')\n    assert x // 2.0 == Float('1.0')\n    assert x // Float('2.0') == Float('1.0')\n    assert x // S.Half == 7\n    assert x // S(1)/3 == 11\n"], "sample_1129": ["def test_log2():\n    from sympy import log\n\n    expr = log(x, 2)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(x)/numpy.log(2)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log(x)/mpmath.log(2)'\n", "def test_log2_log1p():\n    from sympy import log, log2, log1p\n\n    expr1 = log2(x)\n    expr2 = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(x)/numpy.log(2)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log2(x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log(x)/math.log(2)'\n    assert prntr.doprint(expr2) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log(x, 2)'\n    assert prntr.doprint(expr2) == 'mpmath.log1p(x)'\n", "def test_log2():\n    from sympy import log\n    from sympy.functions.elementary.exponential import log2\n\n    expr = log(x, 2)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log(x, 2)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter({'allow_unknown_functions': True})\n    assert prntr.doprint(expr) == 'math.log(x)/math.log(2)'\n\n    prntr = PythonCodePrinter({'user_functions': {'log2': 'math.log2'}})\n    assert prntr.doprint(expr) == 'math.log2(x)'\n\n    expr = log2(x)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log2(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log(x, 2)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter({'allow_unknown_functions': True})\n    assert prntr.doprint(expr) == 'math.log2(x)'\n\n    prntr = PythonCodePrinter({'user_functions':", "def test_log2():\n    from sympy import log2\n\n    expr = log2(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(x)/numpy.log(2)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(x)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log(x)/mpmath.log(2)'\n", "def test_logaddexp_logaddexp2():\n    from sympy.codegen.cfunctions import logaddexp, logaddexp2\n\n    expr1 = logaddexp(x, y)\n    expr2 = logaddexp2(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.logaddexp(x, y)'\n    assert prntr.doprint(expr2) == 'numpy.logaddexp2(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(numpy.exp(x) + numpy.exp(y))'\n    assert prntr.doprint(expr2) == 'numpy.logaddexp2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log(math.exp(x) + math.exp(y))'\n    assert prntr.doprint(expr2) == 'math.log2(math.exp(x) + math.exp(y))'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log(mpmath.exp(x) + mpmath.exp(y))'\n    assert prntr.doprint(expr2) == 'mpmath.log2(mpmath.exp(x) + mpmath.exp(y))'\n"], "sample_528": ["def test_use_with_blacklisted_param():\n    with pytest.warns(UserWarning, match=\"Style includes a parameter, 'backend', that is not related to style. Ignoring this parameter.\"):\n        mpl.style.use({\"backend\": \"Agg\", \"image.cmap\": \"viridis\"})\n    assert mpl.rcParams[\"backend\"] != \"Agg\"\n    assert mpl.rcParams[\"image.cmap\"] == \"viridis\"\n", "def test_blacklist_warning():\n    with temp_style('test', {'backend': 'Agg'}):\n        with pytest.warns(mpl._api.MatplotlibDeprecationWarning):\n            style.use('test')\n", "def test_use_list_of_styles():\n    original_value_1 = 'gray'\n    original_value_2 = False\n    other_value_1 = 'blue'\n    other_value_2 = True\n    mpl.rcParams[PARAM] = original_value_1\n    mpl.rcParams['text.usetex'] = original_value_2\n    with temp_style('test1', {PARAM: other_value_1}):\n        with temp_style('test2', {'text.usetex': other_value_2}):\n            with style.context(['test1', 'test2']):\n                assert mpl.rcParams[PARAM] == other_value_1\n                assert mpl.rcParams['text.usetex'] == other_value_2\n    assert mpl.rcParams[PARAM] == original_value_1\n    assert mpl.rcParams['text.usetex'] == original_value_2\n", "def test_context_with_after_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n        # Check that other values are reset after the context with after_reset=True\n        assert mpl.rcParams[PARAM] != original_value\n        # Check that they are reset to the default values\n        mpl.rcdefaults()\n        assert mpl.rcParams[PARAM] == original_value\n", "def test_context_after_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] == original_value\n\n    with style.context('test', after_reset=True):\n        assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] == original_value\n"], "sample_700": ["def test_dynamic_xfail_set_during_runtest_passed_non_strict(pytester: Pytester) -> None:\n    # Issue #7486.\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n            request.node.add_marker(pytest.mark.xfail(reason=\"xfail\"))\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(xfailed=1)\n", "def test_imperativeskip_with_reason(self, pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.skip(\"reason for skipping\")\n    \"\"\"\n    )\n    result = pytester.runpytest(p)\n    result.stdout.fnmatch_lines([\"*reason for skipping*\", \"*1 skipped*\"])\n", "def test_errors_in_xfail_skip_expressions_syntax_error_message(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(\"syntax error\")\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*ERROR*test_syntax*\",\n            \"*evaluating*xfail*condition*\",\n            \"    syntax error\",\n            \"    ^\",\n            \"SyntaxError: invalid syntax\",\n            \"*1 error*\",\n        ]\n    )\n", "def test_imperativeskip_on_xfail_test_with_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason=\"Expected failure\")\n            assert 0\n\n        @pytest.mark.skipif(\"True\", reason=\"Skipping this test\")\n            pass\n    \"\"\"\n    )\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n            pytest.skip(\"Setup skipped\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rsxX\")\n    result.stdout.fnmatch_lines_random(\n        \"\"\"\n        *SKIP*Setup skipped*\n        *SKIP*Skipping this test*\n        *XFAIL*Expected failure*\n        *2 skipped*\n    \"\"\"\n    )\n", "def test_xfail_run_with_skip_mark_with_reason(self, pytester: Pytester) -> None:\n    pytester.makepyfile(\n        test_sample=\"\"\"\n            import pytest\n            @pytest.mark.skip(reason=\"Test skipped for demonstration purposes\")\n                assert 0\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-vs\", \"--runxfail\")\n    result.stdout.fnmatch_lines([\"*test_sample.py:2: Test skipped for demonstration purposes*\", \"*1 skipped*\"])\n"], "sample_248": ["def test_shell_with_ipython_installed(self, select):\n    select.return_value = ([], [], [])\n    with mock.patch('IPython.start_ipython') as start_ipython:\n        call_command('shell', interface='ipython')\n        start_ipython.assert_called_once_with(argv=[])\n", "def test_python_shell_no_startup(self, select, interact):\n    select.return_value = ([], [], [])\n    call_command('shell', interface='python', no_startup=True)\n    interact.assert_called_once_with(local={})\n", "def test_shell_with_ipython_installed(self, select):\n    select.return_value = ([], [], [])\n    with mock.patch.object(self.Command, 'ipython') as mock_ipython:\n        call_command('shell', interface='ipython')\n        mock_ipython.assert_called_once_with({'interface': 'ipython', 'no_startup': False, 'command': None})\n", "def test_python_shell_with_pythonrc(self, mock_open, mock_isfile, mock_get, mock_select):\n    mock_select.return_value = ([], [], [])\n    mock_isfile.return_value = True\n    mock_get.return_value = None\n    with captured_stdout() as stdout:\n        call_command('shell', interface='python')\n    self.assertEqual(stdout.getvalue().strip(), 'Pythonrc')\n", "def test_python_shell_startup_script_exception(self, expanduser_mock, isfile_mock, select_mock):\n    # Set up mocks\n    expanduser_mock.return_value = '/dummy/path/.pythonrc.py'\n    isfile_mock.return_value = True\n    select_mock.return_value = ([], [], [])\n\n    # Mock open to raise an exception when the file is opened\n    with mock.patch('builtins.open', mock.mock_open(read_data='invalid syntax')):\n        with self.assertLogs('django', level='ERROR') as cm:\n            call_command('shell', interface='python', no_startup=False)\n\n    # Assert that an exception was logged\n    self.assertIn('invalid syntax', cm.output[0])\n"], "sample_519": ["def test_figure_str():\n    fig = Figure(figsize=(10, 20))\n    assert str(fig) == \"Figure(10.0x20.0)\"\n", "def test_subplot_adjust():\n    fig = plt.figure()\n    fig.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.top == 0.8\n    assert fig.subplotpars.bottom == 0.2\n", "def test_set_tight_layout_kwargs():\n    fig = Figure(layout=None)\n    fig.set_tight_layout(True, pad=1.5, h_pad=2.0, w_pad=1.0, rect=(0.1, 0.1, 0.9, 0.9))\n    assert fig.get_tight_layout()\n    assert fig.get_layout_engine().get_info() == {'pad': 1.5, 'h_pad': 2.0, 'w_pad': 1.0, 'rect': (0.1, 0.1, 0.9, 0.9)}\n", "def test_subfigures_axes_removal():\n    fig = plt.figure(constrained_layout=True)\n    gs = fig.add_gridspec(3, 3)\n    sub_figs = [\n        fig.add_subfigure(gs[0, 0]),\n        fig.add_subfigure(gs[0:2, 1]),\n        fig.add_subfigure(gs[2, 1:3]),\n        fig.add_subfigure(gs[0:, 1:])\n    ]\n\n    # Remove an axes from the first subfigure\n    ax_to_remove = sub_figs[0].add_subplot()\n    ax_to_remove.remove()\n    assert ax_to_remove not in sub_figs[0].axes\n\n    # Remove a subfigure\n    sub_figs[2].remove()\n    assert sub_figs[2] not in fig.subfigs\n", "def test_subfigures_clear():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 2)\n\n    for sf in sub_fig:\n        ax = sf.add_subplot(111)\n        ax.plot([0, 1], [2, 3])\n\n    fig.clear()\n    assert fig.axes == []\n    assert fig.subfigs == []\n\n    # clear subfigures only\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 2)\n\n    for sf in sub_fig:\n        ax = sf.add_subplot(111)\n        ax.plot([0, 1], [2, 3])\n\n    for sf in sub_fig:\n        sf.clear()\n        assert sf.axes == []\n\n    assert fig.subfigs == sub_fig\n"], "sample_1163": ["def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', real=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x)) / Abs(f(x))\n", "def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', complex=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x)) / 2 * (1 + I * sign(im(f(x))))\n", "def test_derivative_of_abs():\n    x = Symbol('x', real=True)\n    assert Derivative(Abs(x), x).doit() == sign(x)\n\n    x = Symbol('x', imaginary=True)\n    assert Derivative(Abs(x), x).doit() == 0\n\n    x = Symbol('x', complex=True)\n    assert Derivative(Abs(x), x).doit() == sign(re(x)) + I*sign(im(x))\n", "def test_issue_16434():\n    from sympy import polar_lift, principal_branch, exp_polar, I, pi\n    x = Symbol('x')\n    assert principal_branch(polar_lift(exp_polar(I*pi)*x), 2*pi) == principal_branch(polar_lift(-x), 2*pi)\n", "def test_Abs_nseries():\n    x = Symbol('x')\n    assert Abs(x).nseries(x, n=2, logx=0) == x\n\n    y = Symbol('y', negative=True)\n    assert Abs(y).nseries(y, n=2, logx=0) == -y - y**2/2 + O(y**3)\n\n    z = Symbol('z', positive=True)\n    assert Abs(z).nseries(z, n=2, logx=0) == z\n"], "sample_747": ["def test_power_transformer_yeo_johnson():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_standardize_exception():\n    pt = PowerTransformer(method='box-cox', standardize='yes')\n    X = np.abs(X_2d)\n\n    # An exception should be raised if PowerTransformer.standardize isn't valid\n    bad_standardize_message = \"'standardize' should be Boolean. You entered: yes\"\n    assert_raise_message(ValueError, bad_standardize_message,\n                         pt.fit, X)\n", "def test_power_transformer_sparse_input():\n    pt = PowerTransformer(method='box-cox')\n    X_sparse = sparse.csr_matrix(np.abs(X_2d))\n\n    pt.fit(X_sparse)\n    X_trans = pt.transform(X_sparse)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv.toarray(), np.abs(X_2d))\n", "def test_power_transformer_sparse_input():\n    X = sparse.csr_matrix(np.abs(X_2d))\n\n    pt = PowerTransformer(method='box-cox', standardize=False)\n\n    X_trans = pt.fit_transform(X)\n    X_trans_func = power_transform(X)\n\n    for X_trans in [X_trans.toarray(), X_trans_func.toarray()]:\n        for j in range(X_trans.shape[1]):\n            X_expected, lmbda = stats.boxcox(X[:, j].toarray().flatten())\n            assert_almost_equal(X_trans[:, j], X_expected)\n            assert_almost_equal(lmbda, pt.lambdas_[j])\n\n    # Test inverse transformation\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X.toarray())\n\n    assert len(pt.lambdas_) == X.shape[1]\n    assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_yield_transformer():\n    X = np.abs(X_2d)\n    pt = PowerTransformer(method='box-cox', standardize=True, copy=False)\n\n    X_trans = pt.fit_transform(X)\n    X_trans_func = power_transform(X, standardize=True, copy=False)\n\n    assert_array_almost_equal(X_trans, X_trans_func)\n\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n"], "sample_1021": ["def test_quaternion_multiplication_commutativity():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    assert q1 * q2 != q2 * q1\n", "def test_quaternion_complex_real_field():\n    a = symbols(\"a\", complex=True)\n    b = symbols(\"b\", real=True)\n\n    q1 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field = False)\n    q2 = Quaternion(1, 4, 7, 8, real_field = False)\n\n    assert q1 + a == Quaternion(3 + 4*I + re(a), 2 + 5*I + im(a), 0, 7 + 8*I)\n    assert q1 + b == Quaternion(3 + 4*I + b, 2 + 5*I, 0, 7 + 8*I)\n    assert q2 + a == Quaternion(1 + re(a), 4 + im(a), 7, 8)\n    assert q2 + b == Quaternion(1 + b, 4, 7, 8)\n\n    assert q1 * a == Quaternion(a*(3 + 4*I), a*(2 + 5*I), 0, a*(7 + 8*I))\n    assert q1 * b == Quaternion(b*(3 + 4*I), b*(2 + 5*I), 0, b*(7 + 8*I))\n    assert q2 * a == Quaternion(a, 4*a, 7*a, 8*a)\n    assert q2 * b == Quaternion(b, 4*b, 7*b, 8*b)\n", "def test_quaternion_complex_operations():\n    q1 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    q2 = Quaternion(1 - 2*I, 3 + I, 4 - 3*I, 2*I)\n\n    # Testing multiplication of quaternions over complex fields\n    assert q1 * q2 == Quaternion(-22 + 5*I, 12 + 20*I, -30 + 11*I, 36 + 14*I)\n\n    # Testing addition of quaternions over complex fields\n    assert q1 + q2 == Quaternion(4 + 2*I, 5 + 6*I, 4 - 3*I, 9 + 8*I)\n\n    # Testing subtraction of quaternions over complex fields\n    assert q1 - q2 == Quaternion(2 + 6*I, -1 + 4*I, 4 + 3*I, 7 + 8*I)\n\n    # Testing multiplication with a complex number\n    assert q1 * (2 + 3*I) == Quaternion(-4 + 11*I, 4 + 13*I, 0, 14 + 24*I)\n\n    # Testing addition with a complex number\n    assert q1 + (2 + 3*I) == Quaternion(5 + 7*I, 2 + 5*I, 0, 7 + 8*I)\n\n    # Testing subtraction with a complex number\n    assert q1 - (2 + 3*I) == Quaternion(1 + 1*I, 2 + 5*I, 0, 7 + 8*I)\n", "def test_quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 * q2 == Quaternion(-60, 12, 30, 24)\n    assert q2 * q1 == Quaternion(-60, 30, 24, 12)\n\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    q4 = Quaternion(1 - 2*I, 3 + I, 4, 5*I, real_field=False)\n\n    assert q3 * q4 == Quaternion((1 - 2*I)*(3 + 4*I) - 3*(3 + I) - 7*(4 + 5*I),\n                                (1 - 2*I)*(2 + 5*I) + 3*(3 + 4*I) + 7*(3 + I),\n                                -(2 + 5*I)*(4 + 5*I) + (3 + 4*I)*(5*I) + (3 + 4*I)*(7 + 8*I),\n                                (2 + 5*I)*(4) - (3 + 4*I)*(3 + I) + (3 + 4*I)*(7 + 8*I)*I)\n", "def test_quaternion_complex_field_operations():\n    q1 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field = False)\n    q2 = Quaternion(6 - 2*I, 1 + 3*I, 0, 4 - I, real_field = False)\n\n    assert q1 + q2 == Quaternion(9 + 2*I, 3 + 8*I, 0, 11 + 7*I)\n    assert q1 - q2 == Quaternion(-3 + 6*I, 1 + 2*I, 0, 3 + 9*I)\n    assert q1 * q2 == Quaternion(-25 + 20*I, 6 - 16*I, -26 + 4*I, 6 + 19*I)\n    assert q1 * (2 + 3*I) == Quaternion(2 + 13*I, 7 + 16*I, 0, 14 + 24*I)\n    assert (2 + 3*I) * q1 == Quaternion(2 + 13*I, 7 + 16*I, 0, 14 + 24*I)\n    assert q1 / q2 == Quaternion(Rational(-17, 49) + Rational(33, 49)*I, Rational(-9, 25) - Rational(1, 25)*I, Rational(-26, 49) + Rational(4, 49)*I, Rational(1, 49) + Rational(16, 49)*I)\n"], "sample_641": ["def test_save_and_load_exception(path: str, linter_stats: LinterStats, monkeypatch) -> None:\n        raise OSError(\"Mocked open error\")\n\n    monkeypatch.setattr(\"builtins.open\", mock_open)\n\n    with pytest.raises(OSError):\n        save_results(linter_stats, path)\n\n    loaded = load_results(path)\n    assert loaded is None\n", "def test_save_results_oserror(path: str, linter_stats: LinterStats, monkeypatch) -> None:\n        raise OSError(\"Mock OSError\")\n\n    monkeypatch.setattr(\"builtins.open\", mock_open)\n    save_results(linter_stats, path)\n    # Since we're mocking the open function to raise an OSError, we shouldn't be able to load the results.\n    loaded = load_results(path)\n    assert loaded is None\n", "def test_load_results_nonexistent_file(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n", "def test_save_and_load_result_different_pylint_home(path: str, linter_stats: LinterStats) -> None:\n    pylint_home = Path(PYLINT_HOME) / \"test_dir\"\n    save_results(linter_stats, path, pylint_home)\n    loaded = load_results(path, pylint_home)\n    assert loaded is not None\n    assert loaded.bad_names == linter_stats.bad_names\n", "def test_load_results_file_does_not_exist(tmp_path: Path) -> None:\n    non_existent_file = tmp_path / \"non_existent.stats\"\n    assert not non_existent_file.exists()\n    loaded = load_results(non_existent_file)\n    assert loaded is None\n"], "sample_104": ["def test_cache_invalidation_after_file_change(self):\n    name = \"cached/styles.css\"\n    hashed_name = \"cached/styles.5e0040571e1a.css\"\n    # Check if the cache is filled correctly as expected\n    cache_key = storage.staticfiles_storage.hash_key(name)\n    cached_name = storage.staticfiles_storage.hashed_files.get(cache_key)\n    self.assertEqual(self.hashed_file_path(name), cached_name)\n\n    # Change the content of the file to invalidate the cache\n    with storage.staticfiles_storage.open(name, 'w') as f:\n        f.write(\"/* Changed content */\")\n\n    # Clear the cache to force re-hashing of the file\n    storage.staticfiles_storage.hashed_files.clear()\n    cached_name = storage.staticfiles_storage.hashed_files.get(cache_key)\n    self.assertIsNone(cached_name)\n\n    # Check if the cache is filled correctly with the new hashed name\n    new_hashed_name = \"cached/styles.d41d8cd98f00.css\"\n    self.assertEqual(self.hashed_file_path(name), new_hashed_name)\n    cached_name = storage.staticfiles_storage.hashed_files.get(cache_key)\n    self.assertEqual(cached_name, new_hashed_name)\n", "def test_stored_name_with_corrupt_manifest(self):\n    \"\"\"\n    The stored_name method should raise a ValueError if the manifest is corrupt.\n    \"\"\"\n    storage.staticfiles_storage.hashed_files.clear()\n    storage.staticfiles_storage.save(storage.staticfiles_storage.manifest_name, 'corrupt data')\n    err_msg = \"Couldn't load manifest '%s' (version %s)\" % (\n        storage.staticfiles_storage.manifest_name,\n        storage.staticfiles_storage.manifest_version,\n    )\n    with self.assertRaisesMessage(ValueError, err_msg):\n        storage.staticfiles_storage.stored_name('cached/styles.css')\n", "def test_url_converter(self):\n    # Test url_converter method\n    name = 'test/file.txt'\n    hashed_files = {}\n    template = None\n    content = \"url('/static/test/file.txt')\"\n    expected_output = \"url('/static/test/file.dad0999e4f8f.txt')\"\n    converter = storage.staticfiles_storage.url_converter(name, hashed_files, template)\n    self.assertEqual(converter(re.search(r'url\\((.*?)\\)', content)), expected_output)\n", "def test_non_ascii_filename(self):\n    non_ascii_name = \"cached/n\u00f3n-ascii.txt\"\n    hashed_name = \"cached/n\u00f3n-ascii.1d0b109874c7.txt\"\n    self.assertEqual(self.hashed_file_path(non_ascii_name), hashed_name)\n    with storage.staticfiles_storage.open(hashed_name) as relfile:\n        self.assertEqual(relfile.read(), b'this is a non-ascii file')\n    self.assertPostCondition()\n", "def test_template_tag_non_ascii(self):\n    relpath = self.hashed_file_path(\"test/nonascii.css\")\n    self.assertEqual(relpath, \"test/nonascii.8727b1e2e971.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"test/smilies.png\", content)\n        self.assertIn(b\"smilies.6c6b81c25315.png\", content)\n    self.assertPostCondition()\n"], "sample_894": ["def test_feature_names_in_fit_transform(name):\n    \"\"\"Check feature names in fit_transform for Random Trees Embedding.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    feature_names = [\"feature1\", \"feature2\"]\n    hasher = RandomTreesEmbedding(n_estimators=2, sparse_output=False, random_state=0)\n    hasher.fit_transform(X, feature_names=feature_names)\n    assert_array_equal(hasher.feature_names_in_, feature_names)\n", "def test_max_samples_with_class_weight(name):\n    # Check max_samples with class_weight\n    X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.1, 0.9],\n                               random_state=0)\n    est = FOREST_ESTIMATORS[name](bootstrap=True, max_samples=0.5, class_weight='balanced',\n                                  random_state=0)\n    est.fit(X, y)\n    assert len(est.estimators_) == est.n_estimators\n", "def test_max_samples_float_boundary():\n    \"\"\"Check max_samples as float works and is rounded to integer.\"\"\"\n    X, y = datasets.load_wine(return_X_y=True)\n    forest = RandomForestClassifier(n_estimators=10, max_samples=0.9999999, random_state=0)\n    forest.fit(X, y)\n    assert forest.max_samples_ == len(X)\n", "def test_little_tree_with_small_max_samples(ForestClass):\n    rng = np.random.RandomState(1)\n\n    X = rng.randn(10000, 2)\n    y = rng.randn(10000) > 0\n\n    # First fit with no restriction on max samples\n    est1 = ForestClass(\n        n_estimators=1,\n        random_state=rng,\n        max_samples=None,\n    )\n\n    # Second fit with max samples restricted to just 2\n    est2 = ForestClass(\n        n_estimators=1,\n        random_state=rng,\n        max_samples=2,\n    )\n\n    est1.fit(X, y)\n    est2.fit(X, y)\n\n    tree1 = est1.estimators_[0].tree_\n    tree2 = est2.estimators_[0].tree_\n\n    msg = \"Tree without `max_samples` restriction should have more nodes\"\n    assert tree1.node_count > tree2.node_count, msg\n", "def test_forest_with_max_depth_1_and_min_samples_split_greater_than_n_samples():\n    X, y = hastie_X, hastie_y\n    n_samples = X.shape[0]\n\n    for name in FOREST_ESTIMATORS:\n        ForestEstimator = FOREST_ESTIMATORS[name]\n        est = ForestEstimator(max_depth=1, min_samples_split=n_samples + 1, n_estimators=1, random_state=0)\n        est.fit(X, y)\n\n        # Since min_samples_split is greater than n_samples, all trees should have only one node (the root)\n        assert all(tree.tree_.node_count == 1 for tree in est.estimators_)\n"], "sample_51": ["    def test_parse_datetime_invalid_tzinfo(self):\n        # Invalid timezone inputs\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T09:15:00+24:00')\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T09:15:00+99:99')\n", "    def test_parse_duration_invalid_inputs(self):\n        # Invalid inputs\n        self.assertIsNone(parse_duration('invalid'))\n        self.assertIsNone(parse_duration(''))\n        self.assertIsNone(parse_duration('4 days invalid'))\n        self.assertIsNone(parse_duration('4 invalid minutes'))\n        self.assertIsNone(parse_duration('4 days 5:60:00'))\n", "    def test_invalid_iso_8601(self):\n        self.assertIsNone(parse_duration('PT-5H'))\n        self.assertIsNone(parse_duration('PT5H-5M'))\n        self.assertIsNone(parse_duration('PT-5M'))\n        self.assertIsNone(parse_duration('PT5M-5S'))\n        self.assertIsNone(parse_duration('PT-5S'))\n", "def test_parse_duration_invalid_inputs(self):\n    invalid_inputs = (\n        'P4Y',  # Years are not supported\n        'P4M',  # Months are not supported\n        'P4W',  # Weeks are not supported\n        'P0.5D',  # Fractions of a day are not supported\n        'PT0.000005S',  # Microseconds are supported but not as fractions of a second\n        '30 days',  # The word 'days' is not accepted\n        '-01:-01',  # Negative minutes and seconds are not supported\n    )\n    for source in invalid_inputs:\n        with self.subTest(source=source):\n            self.assertIsNone(parse_duration(source))\n", "    def test_parse_duration_invalid_inputs(self):\n        invalid_inputs = (\n            '2012-04-23',  # date format\n            '09:15:00',  # time format\n            '2012-04-23T09:15:00',  # datetime format\n            '1 day 1:61:00',  # invalid minutes\n            '1 day 25:00:00',  # invalid hours\n            '1 day 0:0:0',  # missing parts\n            'P4YT1M',  # ISO 8601 with unsupported components\n            '1 day 0:00:00:00',  # too many parts\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n"], "sample_355": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            email='test@example.com', is_active=False,\n            **cls.user_credentials\n        )\n        cls.group = Group.objects.create(name='test_group')\n        cls.group_perm = Permission.objects.create(\n            name='test_group_perm', content_type=ContentType.objects.get_for_model(Group), codename='test_group_perm'\n        )\n        cls.group.permissions.add(cls.group_perm)\n        cls.user.groups.add(cls.group)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        content_type = ContentType.objects.get_for_model(Group)\n        cls.perm = Permission.objects.create(name='test', content_type=content_type, codename='test')\n", "    def setUp(self):\n        self.user1 = AnonymousUser()\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.user.groups.add(cls.group)\n        cls.content_type = ContentType.objects.get_for_model(Group)\n        cls.group_perm = Permission.objects.create(name='test_group', content_type=cls.content_type, codename='test_group')\n        cls.group.permissions.add(cls.group_perm)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n"], "sample_461": ["def test_urlfield_clean_ipv4(self):\n    f = URLField(assume_scheme=\"http\")\n    self.assertEqual(f.clean(\"127.0.0.1\"), \"http://127.0.0.1\")\n    self.assertEqual(f.clean(\"127.0.0.1:8000\"), \"http://127.0.0.1:8000\")\n", "def test_urlfield_widget_attrs(self):\n    f = URLField(max_length=255, widget_attrs={'placeholder': 'Enter URL'})\n    self.assertWidgetRendersTo(\n        f,\n        '<input id=\"id_f\" type=\"url\" name=\"f\" maxlength=\"255\" placeholder=\"Enter URL\" required>',\n    )\n", "def test_urlfield_clean_idna_encoding(self):\n    f = URLField()\n    tests = [\n        (\"http://xn--9dbq2a.xn--p1ai\", \"http://\u0645\u062b\u0627\u0644.\u0625\u062e\u062a\u0628\u0627\u0631/\"),\n        (\"http://xn--rsum-bpad.example.com/\", \"http://r\u00e9sum\u00e9.example.com/\"),\n    ]\n    for url, expected in tests:\n        with self.subTest(url=url):\n            self.assertEqual(f.clean(url), expected)\n", "def test_urlfield_clean_with_assume_scheme(self):\n    f = URLField(assume_scheme=\"http\")\n    self.assertEqual(f.clean(\"//example.com\"), \"http://example.com\")\n    self.assertEqual(f.clean(\"example.com:8000\"), \"http://example.com:8000\")\n    self.assertEqual(f.clean(\"//example.com:8000\"), \"http://example.com:8000\")\n    self.assertEqual(f.clean(\"http://example.com\"), \"http://example.com\")\n    self.assertEqual(f.clean(\"https://example.com\"), \"https://example.com\")\n    f = URLField(assume_scheme=\"https\")\n    self.assertEqual(f.clean(\"//example.com\"), \"https://example.com\")\n    self.assertEqual(f.clean(\"example.com:8000\"), \"https://example.com:8000\")\n    self.assertEqual(f.clean(\"//example.com:8000\"), \"https://example.com:8000\")\n    self.assertEqual(f.clean(\"http://example.com\"), \"http://example.com\")\n    self.assertEqual(f.clean(\"https://example.com\"), \"https://example.com\")\n", "def test_urlfield_clean_idn(self):\n    f = URLField()\n    # RemovedInDjango60Warning: When the deprecation ends, remove the\n    # assume_scheme argument.\n    tests = [\n        (\"http://xn--90a3ac.example\", \"http://xn--90a3ac.example/\"),\n        (\"http://xn--90a3ac.xn--qxam\", \"http://xn--90a3ac.xn--qxam/\"),\n    ]\n    for url, expected in tests:\n        with self.subTest(url=url):\n            self.assertEqual(f.clean(url), expected)\n"], "sample_22": ["def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function ``matrix_transpose``.\"\"\"\n    # Transpose a 2D matrix\n    m1 = np.array([[1, 2], [3, 4]])\n    assert_array_equal(matrix_transpose(m1), np.array([[1, 3], [2, 4]]))\n\n    # Transpose a 3D stack of matrices\n    m2 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    assert_array_equal(matrix_transpose(m2), np.array([[[1, 3], [2, 4]], [[5, 7], [6, 8]]]))\n\n    # Transpose a single value\n    m3 = np.array(2)\n    assert_array_equal(matrix_transpose(m3), m3)\n", "def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function ``matrix_transpose``.\"\"\"\n    # Test a simple 2D matrix\n    m1 = np.array([[1, 2], [3, 4]])\n    assert_array_equal(matrix_transpose(m1), np.array([[1, 3], [2, 4]]))\n\n    # Test a 3D matrix (stack of 2D matrices)\n    m2 = np.array([m1, m1])\n    assert_array_equal(matrix_transpose(m2), np.array([[[1, 3], [2, 4]], [[1, 3], [2, 4]]]))\n\n    # Test that it doesn't work for 1D arrays\n    m3 = np.array([1, 2, 3])\n    with pytest.raises(ValueError):\n        matrix_transpose(m3)\n", "def test_matrix_product():\n    \"\"\"Test the matrix product function ``matrix_product``.\"\"\"\n    # Test matrix product of two 2x2 matrices\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    m_product = matrix_product(m1, m2)\n    assert_array_equal(m_product, np.matmul(m1, m2))\n\n    # Test matrix product of multiple 2x2 matrices\n    m_product = matrix_product(m1, m2, m1)\n    assert_array_equal(m_product, np.matmul(np.matmul(m1, m2), m1))\n\n    # Test matrix product with broadcasting\n    n1 = np.tile(m1, (2, 1, 1))\n    n2 = np.tile(m2, (2, 1, 1))\n    n_product = matrix_product(n1, n2)\n    assert_array_equal(n_product, np.matmul(n1, n2))\n", "def test_matrix_product_dimension_error():\n    \"\"\"Test the matrix_product function with incorrect dimensions.\"\"\"\n    m = np.array([[1, 2], [3, 4]])\n    v = np.array([5, 6])\n    with pytest.raises(ValueError):\n        matrix_product(m, v)\n", "def test_rotation_matrix_broadcasting():\n    \"\"\"Test the broadcasting of rotation_matrix.\"\"\"\n    # Broadcasting with scalars\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    m2 = rotation_matrix([35, 45] * u.deg, \"x\")\n    assert_allclose(m1, m2[0])\n    assert_allclose(rotation_matrix(45 * u.deg, \"x\"), m2[1])\n\n    # Broadcasting with arrays\n    m3 = rotation_matrix([35, 45] * u.deg, [\"x\", \"y\"])\n    assert_allclose(m3[0], rotation_matrix(35 * u.deg, \"x\"))\n    assert_allclose(m3[1], rotation_matrix(45 * u.deg, \"y\"))\n\n    # Broadcasting with mixed scalars and arrays\n    m4 = rotation_matrix([35, 45] * u.deg, \"x\")\n    m5 = rotation_matrix(35 * u.deg, [\"x\", \"y\"])\n    assert_allclose(m4, m5)\n"], "sample_375": ["def test_generic_m2m(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n    B = self.create_model(\"B\")\n    T = self.create_model(\"T\", foreign_keys=[\n        GenericForeignKey(),\n        models.ForeignKey('B', models.CASCADE),\n    ])\n    self.assertRelated(A, [B, T])\n    self.assertRelated(B, [A, T])\n    self.assertRelated(T, [A, B])\n", "    def setUp(self):\n        self.apps = Apps(['migrations.related_models_app'])\n", "    def test_generic_fk_with_multiple_fields(self):\n        A = self.create_model(\"A\", foreign_keys=[\n            models.ForeignKey('B', models.CASCADE),\n            GenericForeignKey('field1', 'field2'),\n        ])\n        B = self.create_model(\"B\", foreign_keys=[\n            models.ForeignKey('C', models.CASCADE),\n        ])\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A])\n", "def test_related_models_tuples(self):\n    \"\"\"\n    The function get_related_models_tuples() returns a set of tuples, each\n    containing the app label and model name of a related model.\n    \"\"\"\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    B = self.create_model(\"B\")\n    self.assertEqual(get_related_models_tuples(A), {('related_models_app', 'b')})\n    self.assertEqual(get_related_models_tuples(B), {('related_models_app', 'a')})\n", "    def test_index_addition_deletion(self):\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\n            app_label=\"migrations\",\n            name=\"Tag\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        ))\n        self.assertEqual(len(project_state.models['migrations', 'tag'].options['indexes']), 0)\n\n        # Add an index\n        index = models.Index(fields=['name'], name='test_index')\n        project_state.add_index('migrations', 'tag', index)\n        self.assertEqual(len(project_state.models['migrations', 'tag'].options['indexes']), 1)\n        self.assertEqual(project_state.models['migrations', 'tag'].options['indexes'][0], index)\n\n        # Remove the index\n        project_state.remove_index('migrations', 'tag', 'test_index')\n        self.assertEqual(len(project_state.models['migrations', 'tag'].options['indexes']), 0)\n\n        # Attempt to remove a non-existent index\n        with self.assertRaises(ValueError):\n            project_state.remove_index('migrations', 'tag', 'non_existent_index')\n"], "sample_539": ["def test_polygon_selector_set_verts(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    tool.verts = verts\n    assert tool.verts == verts\n", "def test_polygon_selector_add_remove_set(ax):\n    tool = widgets.PolygonSelector(ax, onselect=noop, interactive=True)\n    for state in ['rotate', 'square', 'center']:\n        with pytest.raises(ValueError):\n            tool.add_state(state)\n        with pytest.raises(ValueError):\n            tool.remove_state(state)\n", "def test_polygon_selector_set_verts(ax):\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    verts = [(50, 50), (150, 50), (50, 150)]\n    tool.verts = verts\n    assert tool.verts == verts\n\n    # Test setting verts to an empty list\n    with pytest.raises(ValueError):\n        tool.verts = []\n\n    # Test setting verts to a list with a single point\n    with pytest.raises(ValueError):\n        tool.verts = [(50, 50)]\n", "def test_polygon_selector_add_state(draw_bounding_box):\n    with pytest.raises(ValueError):\n        widgets.PolygonSelector(get_ax(), onselect=noop,\n                                draw_bounding_box=draw_bounding_box,\n                                state_modifier_keys={'move': 'm'})\n    tool = widgets.PolygonSelector(get_ax(), onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    tool.add_state('move')\n    assert 'move' in tool._state\n", "def test_polygon_selector_ignore_event_outside(ax):\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    tool = widgets.PolygonSelector(ax, onselect, ignore_event_outside=True)\n    click_and_drag(tool, start=(100, 100), end=(130, 140))\n    onselect.assert_called_once()\n    assert tool.verts == [(100, 100), (130, 100), (100, 140)]\n\n    onselect.reset_mock()\n    # Trigger event outside of span\n    click_and_drag(tool, start=(150, 150), end=(160, 160))\n    # event have been ignored and span haven't changed.\n    onselect.assert_not_called()\n    assert tool.verts == [(100, 100), (130, 100), (100, 140)]\n"], "sample_220": ["def test_invalid_expiration_type(self):\n    \"\"\"Setting an invalid expiration type raises a TypeError.\"\"\"\n    response = HttpResponse()\n    with self.assertRaises(TypeError):\n        response.set_cookie('invalid_expiration', expires='invalid')\n", "def test_cookie_domain(self):\n    \"\"\"Cookie domain can be set.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('example', domain='example.com')\n    example_cookie = response.cookies['example']\n    self.assertEqual(example_cookie['domain'], 'example.com')\n", "def test_set_cookie_with_domain(self):\n    \"\"\"set_cookie() accepts a domain parameter.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('example', domain='example.com')\n    self.assertEqual(response.cookies['example']['domain'], 'example.com')\n", "def test_secure_cookie_over_http(self):\n    \"\"\"Secure cookies should be sent over HTTPS.\"\"\"\n    response = HttpResponse()\n    response.set_cookie('secure', 'value', secure=True)\n    secure_cookie = response.cookies['secure']\n    self.assertIn('; %s' % cookies.Morsel._reserved['secure'], str(secure_cookie))\n", "def test_cookie_encoding(self):\n    \"\"\"set_cookie() encodes non-ASCII data to ASCII using RFC 2047.\"\"\"\n    response = HttpResponse()\n    cookie_value = '\u6e05\u98a8'\n    response.set_cookie('test', cookie_value)\n    self.assertEqual(response.cookies['test'].OutputString(), 'Set-Cookie: test=\"=?utf-8?b?5p6X5Yqg?=\"')\n"], "sample_1128": ["def test_point_vel_invalid_frame():\n    N = ReferenceFrame('N')\n    P = Point('P')\n    raises(ValueError, lambda: P.vel(N))  # No velocity defined for point P in frame N\n", "def test_point_acc():\n    t = dynamicsymbols._t\n    q1, q2, u1 = dynamicsymbols('q1, q2, u1')\n    N = ReferenceFrame('N')\n    P = Point('P1')\n    P.set_vel(N, q1 * N.x + u1 * N.y)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * N.y)\n    assert P1.acc(N) == q1.diff(t) * N.x + (q1 + q2.diff(t)) * N.y + q2.diff(t, t) * N.y\n    assert P.acc(N) == q1.diff(t) * N.x + u1 * N.y\n    P1.set_acc(N, u1 * N.z)\n    assert P1.acc(N) == u1 * N.z\n", "def test_point_acc():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    P.set_vel(N, q1 * N.x + q2 * N.y)\n    assert P.acc(N) == q1.diff(t) * N.x + q2.diff(t) * N.y\n    O.set_vel(N, q2 * N.x)\n    P.set_pos(O, q1 * N.y)\n    assert P.acc(N) == (q2.diff(t) - q1 * q2.diff(t)) * N.x + q1.diff(t) * N.y\n", "def test_point_a1pt_theory_invalid_input():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    raises(TypeError, lambda: P.a1pt_theory(q, N, B))  # q is not a Point\n    raises(TypeError, lambda: P.a1pt_theory(O, q, B))  # q is not a ReferenceFrame\n    raises(TypeError, lambda: P.a1pt_theory(O, N, q))  # q is not a ReferenceFrame\n", "def test_auto_point_vel_complex_frame_orientation():\n    t = dynamicsymbols._t\n    q, q1, q2, u1, u2 = dynamicsymbols('q q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    C = B.orientnew('C', 'Axis', [q1, B.y])\n    O = Point('O')\n    O.set_vel(N, u1 * N.x + u2 * C.z)\n    P = Point('P')\n    P.set_pos(O, q2 * B.x + q1 * C.y)\n    assert P.vel(N) == (u1 + q2.diff(t)) * N.x + u2 * C.z - q1 * q.diff(t) * B.z + q2.diff(t) * C.x - q1.diff(t) * B.z\n"], "sample_763": ["def test_check_X_y_pandas_dtype_object_conversion():\n    # test that data-frame like objects with dtype object\n    # get converted\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.object)\n    y = np.array([0, 1, 0], dtype=np.object)\n    X_df = MockDataFrame(X)\n    y_df = MockDataFrame(y)\n    X_checked, y_checked = check_X_y(X_df, y_df)\n    assert_equal(X_checked.dtype.kind, \"f\")\n    assert_equal(y_checked.dtype.kind, \"f\")\n", "def test_check_array_min_samples_and_features_messages_with_nd_array():\n    # Test the minimum samples and features messages with an nd array\n    X = np.ones((10, 1, 28, 28))\n    y = np.ones(10)\n    msg = \"1 sample(s) (shape=(10, 1, 28, 28)) while a minimum of 2 is required.\"\n    assert_raise_message(ValueError, msg, check_X_y, X, y, ensure_min_samples=2, allow_nd=True)\n", "def test_check_array_memmap_copy():\n    X = np.ones((4, 4))\n    with TempMemmap(X, mmap_mode='r') as X_memmap:\n        X_checked = check_array(X_memmap, copy=True)\n        assert not np.may_share_memory(X_memmap, X_checked)\n        assert X_checked.flags['WRITEABLE']\n", "def test_check_X_y_force_all_finite_valid():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    X_checked, y_checked = check_X_y(X, y, force_all_finite=True)\n    assert_allclose_dense_sparse(X_checked, X)\n    assert_allclose_dense_sparse(y_checked, y)\n", "def test_check_X_y_with_nan_and_inf_in_y():\n    X = np.ones((5, 5))\n    y_with_nan = [1, np.nan, 3, 4, 5]\n    y_with_inf = [1, np.inf, 3, 4, 5]\n\n    assert_raises_regex(ValueError, \"Input contains NaN, infinity\", check_X_y, X, y_with_nan)\n    assert_raises_regex(ValueError, \"Input contains infinity\", check_X_y, X, y_with_inf)\n"], "sample_1067": ["def test_issue_6103():\n    x = Symbol('x')\n    a = Wild('a')\n    assert (-I*x*oo).match(I*a*oo) == {a: -x}\n", "def test_issue_gh_2711_coeff():\n    x = Symbol('x')\n    f = meijerg(((), ()), ((0,), ()), x)\n    a = Wild('a')\n    b = Wild('b')\n\n    assert f.match(a*meijerg(b, ((0,), ()), x)) == {a: 1, b: (((), ()),)}\n    assert f.match(a*meijerg(((), ()), b, x)) is None\n    assert f.match(a*meijerg(((), ()), ((0,), ()), b)) == {a: 1, b: x}\n", "def test_issue_new():\n    # New test for _eval_subs method with symbolic coefficients\n    x, y, z = symbols('x y z')\n    a, b = symbols('a b', cls=Wild)\n\n    e = (x + y).subs(x, a*z)\n    assert e.match(b*z + y) == {b: 1 + a}\n", "def test_issue_6421():\n    x = Symbol('x')\n    p = Wild('p')\n    assert (I * x**(-1)).match(p * I) == {p: x**(-1)}\n    assert (x * I * x**(-1)).match(p * I) == {p: x}\n", "def test_issue_new():\n    x, y, z = symbols('x y z')\n    p, q, r = symbols('p q r', cls=Wild)\n\n    # Test the match function with non-commutative multiplication\n    e = z*x*y\n    assert e.match(p*x*y) == {p: z}\n\n    # Test the match function with addition and multiplication\n    e = 2*x + 3*y\n    assert e.match(p*x + q*y) == {p: 2, q: 3}\n\n    # Test the match function with power\n    e = x**2*y\n    assert e.match(p**q*y) == {p: x, q: 2}\n\n    # Test the match function with complex numbers\n    e = (1 + I)*x\n    assert e.match(p*x) == {p: 1 + I}\n\n    # Test the match function with functions\n    f = Function('f', nargs=1)\n    e = sin(f(x))\n    assert e.match(sin(p(x))) == {p: f}\n\n    # Test the match function with derivatives\n    fd = Derivative(f(x), x)\n    assert fd.match(p*Derivative(f(x), x)) == {p: 1}\n\n    # Test the match function with floats\n    e = cos(0.12345, evaluate=False)**2\n    assert e.match(p*cos(q)**2) == {p: 1, q: Float(0.12345)}\n\n    # Test the match function with exclusions\n    a = Symbol('a')\n    p = Wild('p', exclude=[1, x])\n    e = 3*x**2 + y*x + a\n    assert e.match(p*x**2 + q*x + r) == {p: 3, q: y, r: a}\n"], "sample_207": ["def test_key_transform_exact(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__baz__a__exact='b'),\n        [self.objs[7]],\n    )\n", "def test_key_transform_with_invalid_lookup(self):\n    with self.assertRaisesMessage(LookupError, \"Unknown lookup 'invalid_lookup'\"):\n        NullableJSONModel.objects.filter(value__key__invalid_lookup='value')\n", "def test_key_transform_icontains(self):\n    self.assertIs(NullableJSONModel.objects.filter(value__foo__icontains='Ar').exists(), True)\n", "def test_contained_by_primitives(self):\n    for value in self.primitives:\n        with self.subTest(value=value):\n            qs = NullableJSONModel.objects.filter(value__contained_by=value)\n            self.assertIs(qs.exists(), True)\n", "def test_key_transform_numeric_lookups(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gt=13),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gte=14),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lt=15),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lte=14),\n        [self.objs[3], self.objs[4]],\n    )\n"], "sample_943": ["def test_pep_0420_enabled_separate_noheadings(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.b.c.rst').isfile()\n    assert (outdir / 'a.b.e.rst').isfile()\n    assert (outdir / 'a.b.e.f.rst').isfile()\n    assert (outdir / 'a.b.x.rst').isfile()\n    assert (outdir / 'a.b.x.y.rst').isfile()\n\n    with open(outdir / 'a.b.c.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 4\\n\\n   a.b.c.d\\n\" in rst\n        assert \".. automodule:: a.b.c\\n\" not in rst\n\n    with open(outdir / 'a.b.e.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 4\\n\\n   a.b.e.f\\n\" in rst\n        assert \".. automodule:: a.b.e\\n\" not in rst\n\n    with open(outdir / 'a.b.x.rst') as f:\n        rst = f.read()\n        assert \".. toctree::\\n   :maxdepth: 4\\n\\n   a.b.x.y\\n\" in rst\n        assert \".. automodule:: a.b.x\\n\" not in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.get", "def test_excludes_submodule_should_be_skipped(apidoc):\n    \"\"\"Submodule exclusion should work.\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.rst').isfile()\n    assert (outdir / 'a.b.rst').isfile()\n    assert not (outdir / 'a.b.c.d.rst').isfile()  # skipped because of explicit exclusion\n\n    with open(outdir / 'a.b.c.rst') as f:\n        rst = f.read()\n        assert \"a.b.c.d module\" not in rst\n", "def test_toc_maxdepth_option(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n\n    toc = extract_toc(outdir / 'mypackage.rst')\n\n    refs = [l.strip() for l in toc.splitlines() if l.strip()]\n    expected_refs = [\"mypackage.foo\", \"mypackage.bar\"]\n\n    assert refs == expected_refs, \\\n        'TOC does not match expected output.\\n' \\\n        'Expected:\\n{}\\n' \\\n        'Got:\\n{}'.format(\"\\n\".join(expected_refs), \"\\n\".join(refs))\n", "def test_separate_option_should_generate_module_files(apidoc):\n    \"\"\"Module files should be generated if --separate option is used.\"\"\"\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'mypackage.rst').isfile()\n    assert (outdir / 'mypackage.module.rst').isfile()\n    assert (outdir / 'mypackage.package.rst').isfile()\n    assert (outdir / 'mypackage.package.module.rst').isfile()\n", "def test_pep_0420_enabled_separate_maxdepth(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.rst').isfile()\n    assert (outdir / 'a.b.rst').isfile()\n    assert (outdir / 'a.b.c.rst').isfile()\n    assert (outdir / 'a.b.e.rst').isfile()\n    assert not (outdir / 'a.b.e.f.rst').isfile()  # excluded because of maxdepth\n    assert (outdir / 'a.b.x.rst').isfile()\n    assert not (outdir / 'a.b.x.y.rst').isfile()  # excluded because of maxdepth\n"], "sample_498": ["def test_legend_borderpad():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(borderpad=2)\n    assert leg.borderpad == 2\n", "def test_legend_markers_from_line2d_with_alpha():\n    # Test that markers can be copied for legend lines with alpha (#17960)\n    _markers = ['.', '*', 'v']\n    fig, ax = plt.subplots()\n    lines = [mlines.Line2D([0], [0], ls='None', marker=mark, alpha=0.5)\n             for mark in _markers]\n    labels = [\"foo\", \"bar\", \"xyzzy\"]\n    markers = [line.get_marker() for line in lines]\n    legend = ax.legend(lines, labels)\n\n    new_markers = [line.get_marker() for line in legend.get_lines()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n    new_alpha = [line.get_alpha() for line in legend.get_lines()]\n\n    assert markers == new_markers == _markers\n    assert labels == new_labels\n    assert new_alpha == [0.5, 0.5, 0.5]\n", "def test_legend_markers_from_collection():\n    # Test that markers can be copied for legend collections (#17960)\n    _markers = ['.', '*', 'v']\n    fig, ax = plt.subplots()\n    collections = [mcollections.PathCollection([[0, 0], [1, 1]], marker=mark)\n                   for mark in _markers]\n    labels = [\"foo\", \"bar\", \"xyzzy\"]\n    markers = [collection.get_paths()[0].get_marker() for collection in collections]\n    legend = ax.legend(collections, labels)\n\n    new_markers = [collection.get_paths()[0].get_marker() for collection in legend.get_collections()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert markers == new_markers == _markers\n    assert labels == new_labels\n", "def test_legend_scatteryoffsets():\n    # Test that scatteryoffsets are used in the legend\n    fig, ax = plt.subplots()\n    ax.scatter([0, 1], [0, 1], label='scatter')\n    leg = ax.legend(scatteryoffsets=[0.2, 0.4, 0.6])\n    assert leg.legend_handles[0].get_offsets() == np.array([[0.2, 0.2], [0.4, 0.4], [0.6, 0.6]])\n", "def test_legend_title_fontprop_fontname():\n    # test the title_fontname kwarg\n    plt.plot(range(10))\n    leg = plt.legend(title='Aardvark', title_fontproperties={'family': 'serif'})\n    assert leg.get_title().get_fontname() == 'serif'\n\n    fig, axes = plt.subplots(2, 3, figsize=(10, 6))\n    axes = axes.flat\n    axes[0].plot(range(10))\n    leg0 = axes[0].legend(title='Aardvark', title_fontproperties={'family': 'serif'})\n    assert leg0.get_title().get_fontname() == 'serif'\n    axes[1].plot(range(10))\n    leg1 = axes[1].legend(title='Aardvark', title_fontproperties={'fontname': 'serif'})\n    assert leg1.get_title().get_fontname() == 'serif'\n    axes[2].plot(range(10))\n    mpl.rcParams['font.family'] = 'serif'\n    leg2 = axes[2].legend(title='Aardvark', title_fontproperties={'family': 'sans-serif'})\n    assert leg2.get_title().get_fontname() == 'sans-serif'\n    axes[3].plot(range(10))\n    leg3 = axes[3].legend(title='Aardvark')\n    assert leg3.get_title().get_fontname() == 'serif'\n    axes[4].plot(range(10))\n    mpl.rcParams['font.family'] = 'sans-serif'\n    leg4 = axes[4].legend(title='Aardvark', title_fontproperties={'family': 'serif'})\n    assert leg4.get_title().get_fontname() == 'serif'\n    axes[5].plot(range(10))\n    leg5 = axes[5].legend("], "sample_517": ["def test_pdf_font42_kerning_sans():\n    plt.rcParams['pdf.fonttype'] = 42\n    plt.rcParams['mathtext.fontset'] = 'stixsans'\n    plt.figure()\n    plt.figtext(0.1, 0.5, \"ATAVATAVATAVATAVATA\", size=30)\n", "def test_text_parse_math_false():\n    fig, ax = plt.subplots()\n    ax.text(0, 0, r\"$ \\wrong{math} $\", parse_math=False)\n    fig.canvas.draw()\n\n    assert r\"$ \\wrong{math} $\" in ax.texts[0].get_text()\n", "def test_pdf_font32_kerning():\n    plt.rcParams['pdf.fonttype'] = 3\n    plt.figure()\n    plt.figtext(0.1, 0.5, \"ATAVATAVATAVATAVATA\", size=30)\n", "def test_pdf_font42_kerning_large():\n    plt.rcParams['pdf.fonttype'] = 42\n    plt.rcParams['font.size'] = 30\n    plt.figure()\n    plt.figtext(0.1, 0.5, \"ATAVATAVATAVATAVATA\")\n", "def test_bbox_padding():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Text with bbox', bbox=dict(facecolor='white', edgecolor='black', pad=10.0))\n    fig.canvas.draw()\n"], "sample_703": ["def test_operator_precedence() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"false or true and true\", matcher) is True\n    assert evaluate(\"true and true or false\", matcher) is True\n    assert evaluate(\"not true and false\", matcher) is False\n    assert evaluate(\"true or not false and false\", matcher) is True\n", "def test_operators_precedence() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert not evaluate(\"false or true and true\", matcher)\n    assert evaluate(\"(false or true) and true\", matcher)\n    assert evaluate(\"false or (true and true)\", matcher)\n    assert not evaluate(\"true and false or true\", matcher)\n    assert evaluate(\"(true and false) or true\", matcher)\n    assert evaluate(\"true and (false or true)\", matcher)\n", "def test_mixed_ident_evaluation() -> None:\n    matcher = {\"true\": True, \"false\": False, \"maybe\": True}.__getitem__\n\n    assert evaluate(\"true and maybe\", matcher) is True\n    assert evaluate(\"false and maybe\", matcher) is False\n    assert evaluate(\"true or maybe\", matcher) is True\n    assert evaluate(\"false or maybe\", matcher) is True\n    assert evaluate(\"not maybe\", matcher) is False\n", "def test_operator_precedence(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_operator_precedence() -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(\"true or false and false\", matcher) is True\n    assert evaluate(\"false and true or true\", matcher) is True\n    assert evaluate(\"false and (true or true)\", matcher) is False\n    assert evaluate(\"(false and true) or true\", matcher) is True\n    assert evaluate(\"not true and false or true\", matcher) is True\n    assert evaluate(\"true and not false or true\", matcher) is True\n    assert evaluate(\"not (true and false) or true\", matcher) is True\n"], "sample_677": ["def test_precedence_and_or() -> None:\n    matcher = {\"a\": True, \"b\": False, \"c\": True}.__getitem__\n    assert evaluate(\"a and b or c\", matcher) is True\n    assert evaluate(\"a or b and c\", matcher) is True\n    assert evaluate(\"(a or b) and c\", matcher) is True\n    assert evaluate(\"a or (b and c)\", matcher) is True\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) == eval(expr, {\"True\": True, \"False\": False})\n", "def test_special_identifiers(expr: str, expected: bool) -> None:\n    matcher = {\"True\": True, \"False\": False, \"None\": None}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_precedence_and_associativity(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is True\n"], "sample_376": ["def test_samesite_setting_lax(self):\n    \"\"\"\n    CookieStorage honors SESSION_COOKIE_SAMESITE setting when set to 'Lax'.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.add(constants.INFO, 'test')\n    storage.update(response)\n    self.assertEqual(response.cookies['messages']['samesite'], 'Lax')\n", "def test_decode_invalid_json(self):\n    \"\"\"\n    If the encoded data is not valid JSON, _decode() returns None.\n    \"\"\"\n    storage = self.get_storage()\n    invalid_json = 'invalid json'\n    self.assertIsNone(storage._decode(invalid_json))\n", "def test_no_messages(self):\n    \"\"\"\n    If no messages are set, the cookie is deleted.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertNotIn(storage.cookie_name, response.cookies)\n", "def test_empty_messages(self):\n    \"\"\"\n    If there are no messages to store, the cookie is deleted.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    cookie = response.cookies.get(storage.cookie_name)\n    self.assertIsNotNone(cookie)\n    self.assertEqual(cookie['max-age'], 0)\n", "def test_store_empty_messages(self):\n    \"\"\"\n    If an empty list of messages is stored, the messages cookie is deleted.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Set initial data.\n    example_messages = ['test', 'me']\n    set_cookie_data(storage, example_messages)\n\n    # Store an empty list of messages.\n    storage._store([], response)\n\n    # The messages cookie is deleted.\n    self.assertNotIn(CookieStorage.cookie_name, response.cookies)\n"], "sample_185": ["    def test_get_format_modules_lang_none(self):\n        with translation.override(None):\n            self.assertEqual('.', get_format('DECIMAL_SEPARATOR', lang=None))\n", "    def test_get_language_info_localized_language_info(self):\n        li = get_language_info('de', 'de')\n        self.assertEqual(li['code'], 'de')\n        self.assertEqual(li['name_local'], 'Deutsch')\n        self.assertEqual(li['name'], 'German')\n        self.assertIs(li['bidi'], False)\n", "def test_localize_numbers(self):\n    with self.settings(USE_L10N=True):\n        # Test localization of integer\n        self.assertEqual(localize(1234), '1,234')\n        # Test localization of float\n        self.assertEqual(localize(1234.56), '1,234.56')\n        # Test localization of decimal\n        self.assertEqual(localize(decimal.Decimal('1234.56')), '1,234.56')\n", "    def test_round_away_from_one(self):\n        # ... existing code ...\n", "    def test_round_away_from_one_large_numbers(self):\n        tests = [\n            (1.0e15, 1.0e15),\n            (1.25e15, 2.0e15),\n            (1.5e15, 2.0e15),\n            (1.75e15, 2.0e15),\n            (-1.0e15, -1.0e15),\n            (-1.25e15, -2.0e15),\n            (-1.5e15, -2.0e15),\n            (-1.75e15, -2.0e15),\n        ]\n        for value, expected in tests:\n            with self.subTest(value=value):\n                self.assertEqual(round_away_from_one(value), expected)\n"], "sample_405": ["def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, limit_choices_to={\"field\": \"value\"}),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\", models.CASCADE, limit_choices_to={\"field\": \"value\"}\n        ),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "def test_references_field_by_through_fields_with_null(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\n            \"Other\", through=\"Through\", through_fields=(\"first\", None)\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"first\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"second\", \"migrations\"), False\n    )\n", "def test_rename_field_through_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\n            \"Other\", through=\"Through\", through_fields=(\"old_name\", \"second\")\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"old_name\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"second\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"new_name\", \"migrations\"), False\n    )\n\n    operation = FieldOperation(\n        \"Through\", \"old_name\", models.ForeignKey(\"Other\", models.CASCADE)\n    )\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), True)\n\n    operation = FieldOperation(\n        \"Through\", \"new_name\", models.ForeignKey(\"Other\", models.CASCADE)\n    )\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), True)\n", "def test_rename_field_references(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_field\"),\n    )\n    self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"related_field\", \"migrations\"), True)\n\n    operation.rename_references(\"Model\", \"old_field\", \"new_field\")\n    self.assertIs(operation.references_field(\"Model\", \"new_field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Model\", \"old_field\", \"migrations\"), False)\n"], "sample_707": ["def test_node_get_closest_marker(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo\n            pass\n\n            pass\n    \"\"\"\n    )\n    result = pytester.inline_run()\n    result.assertoutcome(passed=2)\n\n    items = result.getitems(\"test_with_marker\")\n    marker = items[0].get_closest_marker(\"foo\")\n    assert marker is not None\n    assert marker.name == \"foo\"\n\n    items = result.getitems(\"test_without_marker\")\n    marker = items[0].get_closest_marker(\"foo\")\n    assert marker is None\n\n    default_marker = pytest.mark.default\n    marker = items[0].get_closest_marker(\"bar\", default=default_marker)\n    assert marker is default_marker\n", "def test_fs_collector_with_session(pytester: Pytester) -> None:\n    \"\"\"Test FSCollector with session provided.\"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n        \"\"\"\n    )\n\n    class FakeSession:\n            self.config = pytester.config\n            self._initialpaths = frozenset({p.parent})\n\n    session = FakeSession()\n    collector = nodes.FSCollector(path=p, session=session)\n    items = list(collector.collect())\n    assert len(items) == 1\n    assert isinstance(items[0], nodes.Item)\n", "def test_node_listchain(pytester: Pytester) -> None:\n    \"\"\"\n    Test the listchain method of the Node class.\n    \"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            class ParentNode(pytest.Node):\n                    super().__init__(name, parent=None, session=pytest.Session())\n            return ParentNode(\"parent\")\n\n        @pytest.fixture\n            class ChildNode(pytest.Node):\n                    super().__init__(name, parent=parent, session=pytest.Session())\n            return ChildNode(\"child\", parent_node)\n\n            chain = child_node.listchain()\n            assert len(chain) == 2\n            assert chain[0].name == \"child\"\n            assert chain[1].name == \"parent\"\n    \"\"\"\n    )\n    pytester.runpytest().assert_outcomes(passed=1)\n", "def test_fs_collector_initialization() -> None:\n    path = Path(\"path/to/test\")\n    session = pytest.Session()\n    session._initialpaths = frozenset({path})\n\n    fs_collector = nodes.FSCollector(path=path, session=session)\n\n    assert fs_collector.path == path\n    assert fs_collector.session == session\n    assert fs_collector.name == \"test\"\n    assert fs_collector.nodeid == \"path/to/test\"\n", "def test_node_add_marker_function() -> None:\n    # Test adding a marker using a function\n    node = nodes.Node(\"test_node\")\n    node.add_marker(\"custom_marker\")\n    assert \"custom_marker\" in node.keywords\n"], "sample_1014": ["def test_reshape_error():\n    array = ImmutableDenseNDimArray(range(50), 50)\n    with raises(ValueError):\n        array.reshape(5, 10)\n", "def test_mutability():\n    # Testing mutability of MutableDenseNDimArray\n    arr_mutable = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    arr_mutable[0, 0] = 5\n    assert arr_mutable[0, 0] == 5\n    assert arr_mutable != ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n\n    # Testing conversion to immutable\n    arr_immutable = arr_mutable.as_immutable()\n        arr[0, 0] = 6\n    raises(TypeError, lambda: set_immutable(arr_immutable))\n\n    # Testing conversion to mutable\n    arr_mutable_again = arr_immutable.as_mutable()\n    arr_mutable_again[0, 0] = 6\n    assert arr_mutable_again[0, 0] == 6\n", "def test_mutable_ndim_array():\n    from sympy import MutableDenseNDimArray, Symbol\n\n    mut_arr = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert mut_arr.shape == (2, 2)\n    assert mut_arr.rank() == 2\n    assert mut_arr[0, 0] == 1\n\n    mut_arr[0, 0] = 5\n    assert mut_arr[0, 0] == 5\n\n    sym = Symbol('x')\n    mut_arr[1, 1] = sym\n    assert mut_arr[1, 1] == sym\n\n    mut_arr_copy = mut_arr.as_immutable()\n    assert mut_arr_copy.shape == (2, 2)\n    assert mut_arr_copy.rank() == 2\n    assert mut_arr_copy[0, 0] == 5\n    assert mut_arr_copy[1, 1] == sym\n\n    mut_arr_copy[0, 0] = 10  # This should raise TypeError\n", "def test_array_equality_with_different_classes():\n    dense_array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    sparse_array = ImmutableSparseNDimArray([1, 2, 3, 4], (2, 2))\n\n    assert dense_array == sparse_array\n    assert sparse_array == dense_array\n\n    dense_array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    sparse_array = ImmutableSparseNDimArray([1, 2, 3, 5], (2, 2))\n\n    assert dense_array != sparse_array\n    assert sparse_array != dense_array\n", "def test_getitem_with_symbolic_index():\n    from sympy import symbols\n    from sympy.tensor.array.dense_ndim_array import ImmutableDenseNDimArray\n\n    x, y = symbols('x y')\n    arr = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n\n    # Test symbolic indexing\n    result = arr[x, y]\n    expected_result = arr._array[arr._parse_index((x, y))]\n    assert result == expected_result\n\n    # Test symbolic indexing with substitution\n    result_subs = result.subs({x: 0, y: 1})\n    expected_result_subs = arr[0, 1]\n    assert result_subs == expected_result_subs\n"], "sample_402": ["def test_append_slash_redirect_subdomain(self):\n    \"\"\"\n    APPEND_SLASH should redirect slashless URLs to a valid pattern on subdomains.\n    \"\"\"\n    request = self.rf.get(\"/subdomain/slash\")\n    request.META[\"HTTP_HOST\"] = \"subdomain.testserver\"\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertIsNone(r)\n    response = HttpResponseNotFound()\n    r = CommonMiddleware(get_response_empty).process_response(request, response)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://subdomain.testserver/slash/\")\n", "def test_if_modified_since_and_future_last_modified(self):\n    self.req.META[\"HTTP_IF_MODIFIED_SINCE\"] = \"Sat, 12 Feb 2011 17:38:44 GMT\"\n    self.resp_headers[\"Last-Modified\"] = \"Sat, 12 Feb 2012 17:38:44 GMT\"\n    resp = ConditionalGetMiddleware(self.get_response)(self.req)\n    self.assertEqual(resp.status_code, 200)\n", "def test_append_slash_redirect_subdomain(self):\n    \"\"\"\n    APPEND_SLASH should redirect slashless URLs to a valid pattern even on a subdomain.\n    \"\"\"\n    request = self.rf.get(\"http://sub.testserver/slash\")\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertIsNone(r)\n    response = HttpResponseNotFound()\n    r = CommonMiddleware(get_response_empty).process_response(request, response)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://sub.testserver/slash/\")\n", "def test_prepend_www_no_slash_no_append_slash(self):\n    \"\"\"\n    PREPEND_WWW should prepend www. to the URL if it doesn't have a trailing slash.\n    \"\"\"\n    request = self.rf.get(\"/slash\")\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/slash\")\n", "def test_disallowed_user_agents_with_regex_patterns(self):\n    request = self.rf.get(\"/slash\")\n    request.META[\"HTTP_USER_AGENT\"] = \"foobar\"\n    with self.assertRaisesMessage(PermissionDenied, \"Forbidden user agent\"):\n        CommonMiddleware(get_response_empty).process_request(request)\n\n    request.META[\"HTTP_USER_AGENT\"] = \"barfoo\"\n    with self.assertRaisesMessage(PermissionDenied, \"Forbidden user agent\"):\n        CommonMiddleware(get_response_empty).process_request(request)\n\n    request.META[\"HTTP_USER_AGENT\"] = \"foo123bar\"\n    with self.assertRaisesMessage(PermissionDenied, \"Forbidden user agent\"):\n        CommonMiddleware(get_response_empty).process_request(request)\n"], "sample_742": ["def test_logreg_l2_sparse_data():\n    # Because liblinear penalizes the intercept and saga does not, we do not\n    # fit the intercept to make it possible to compare the coefficients of\n    # the two models at convergence.\n    rng = np.random.RandomState(42)\n    n_samples = 50\n    X, y = make_classification(n_samples=n_samples, n_features=20,\n                               random_state=0)\n    X_noise = rng.normal(scale=0.1, size=(n_samples, 3))\n    X_constant = np.zeros(shape=(n_samples, 2))\n    X = np.concatenate((X, X_noise, X_constant), axis=1)\n    X[X < 1] = 0\n    X = sparse.csr_matrix(X)\n\n    lr_liblinear = LogisticRegression(penalty=\"l2\", C=1.0, solver='liblinear',\n                                      fit_intercept=False,\n                                      tol=1e-10)\n    lr_liblinear.fit(X, y)\n\n    lr_saga = LogisticRegression(penalty=\"l2\", C=1.0, solver='saga',\n                                 fit_intercept=False,\n                                 max_iter=1000, tol=1e-10)\n    lr_saga.fit(X, y)\n    assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)\n\n    # Check that solving on the sparse and dense data yield the same results\n    lr_saga_dense = LogisticRegression(penalty=\"l2\", C=1.0, solver='saga',\n                                       fit_intercept=False,\n                                       max_iter=1000, tol=1e-10)\n    lr_saga_dense.fit(X.toarray(), y)\n    assert_array_almost_equal", "def test_warm_start_converge_LRCV():\n    # Test to see that the logistic regression with cross-validation converges on warm start,\n    # with multi_class='multinomial'. Non-regressive test for #10836\n\n    rng = np.random.RandomState(0)\n    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))\n    y = np.array([1] * 100 + [-1] * 100)\n    lr_no_ws = LogisticRegressionCV(multi_class='multinomial',\n                                    solver='sag', warm_start=False)\n    lr_ws = LogisticRegressionCV(multi_class='multinomial',\n                                 solver='sag', warm_start=True)\n\n    lr_no_ws_loss = log_loss(y, lr_no_ws.fit(X, y).predict_proba(X))\n    lr_ws_loss = [log_loss(y, lr_ws.fit(X, y).predict_proba(X))\n                  for _ in range(5)]\n\n    for i in range(5):\n        assert_allclose(lr_no_ws_loss, lr_ws_loss[i], rtol=1e-5)\n", "def test_logistic_regression_sample_weights_multinomial():\n    X, y = make_classification(n_samples=20, n_features=5, n_informative=3,\n                               n_classes=3, random_state=0)\n    sample_weight = y + 1\n\n    clf_sw_lbfgs = LogisticRegressionCV(solver='lbfgs', multi_class='multinomial',\n                                       fit_intercept=False, random_state=42)\n    clf_sw_lbfgs.fit(X, y, sample_weight=sample_weight)\n    clf_sw_n = LogisticRegressionCV(solver='newton-cg', multi_class='multinomial',\n                                    fit_intercept=False, random_state=42)\n    clf_sw_n.fit(X, y, sample_weight=sample_weight)\n    clf_sw_sag = LogisticRegressionCV(solver='sag', multi_class='multinomial',\n                                      fit_intercept=False, tol=1e-10,\n                                      random_state=42)\n    # ignore convergence warning due to small dataset\n    with ignore_warnings():\n        clf_sw_sag.fit(X, y, sample_weight=sample_weight)\n    assert_array_almost_equal(\n        clf_sw_lbfgs.coef_, clf_sw_n.coef_, decimal=4)\n    assert_array_almost_equal(\n        clf_sw_lbfgs.coef_, clf_sw_sag.coef_, decimal=4)\n", "def test_logistic_regression_multinomial_intercept_scaling():\n    # Tests for the multinomial option in logistic regression with intercept scaling\n\n    # Some basic attributes of Logistic Regression with intercept scaling\n    n_samples, n_features, n_classes = 50, 20, 3\n    X, y = make_classification(n_samples=n_samples,\n                               n_features=n_features,\n                               n_informative=10,\n                               n_classes=n_classes, random_state=0)\n    intercept_scaling = 2.0\n\n    # 'lbfgs' is used as a referenced\n    solver = 'lbfgs'\n    ref_i = LogisticRegression(solver=solver, multi_class='multinomial',\n                               intercept_scaling=intercept_scaling)\n    ref_w = LogisticRegression(solver=solver, multi_class='multinomial',\n                               fit_intercept=False,\n                               intercept_scaling=intercept_scaling)\n    ref_i.fit(X, y)\n    ref_w.fit(X, y)\n    assert_array_equal(ref_i.coef_.shape, (n_classes, n_features))\n    assert_array_equal(ref_w.coef_.shape, (n_classes, n_features))\n    for solver in ['sag', 'saga', 'newton-cg']:\n        clf_i = LogisticRegression(solver=solver, multi_class='multinomial',\n                                   random_state=42, max_iter=2000, tol=1e-7,\n                                   intercept_scaling=intercept_scaling)\n        clf_w = LogisticRegression(solver=solver, multi_class='multinomial',\n                                   random_state=42, max_iter=2000, tol=1e-7,\n                                   fit_intercept=False,\n                                   intercept_", "def test_logreg_multinomial_l1():\n    # Test Logistic Regression with l1 penalty and multinomial class option\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_classes = 100, 20, 3\n    X = rng.randn(n_samples, n_features)\n    y = rng.randint(0, n_classes, size=n_samples)\n    lr = LogisticRegression(penalty='l1', multi_class='multinomial', solver='saga')\n    lr.fit(X, y)\n    assert_array_equal(lr.coef_.shape, (n_classes, n_features))\n    assert_array_equal(lr.intercept_.shape, (n_classes,))\n"], "sample_442": ["def test_verify_with_no_key(self):\n    signer = signing.Signer()\n    signed = signer.sign(\"abc\")\n    self.assertEqual(signer.unsign(signed), \"abc\")\n", "    def test_sign_unsign_with_custom_serializer(self):\n        class CustomSerializer:\n                return json.dumps(obj, separators=(\",\", \":\"), indent=4).encode(\"utf-8\")\n\n                return json.loads(data.decode(\"utf-8\"))\n\n        signer = signing.Signer(key=\"predictable-secret\")\n        obj = {\"key\": \"value\", \"number\": 123, \"bool\": True}\n        signed_obj = signer.sign_object(obj, serializer=CustomSerializer)\n        self.assertNotEqual(obj, signed_obj)\n        self.assertEqual(obj, signer.unsign_object(signed_obj, serializer=CustomSerializer))\n", "def test_unsign_object_with_max_age(self):\n    signer = signing.TimestampSigner(key=\"predictable-secret\")\n    value = {\"foo\": \"bar\"}\n    with freeze_time(123456789):\n        signed_obj = signer.sign_object(value)\n\n    with freeze_time(123456800):\n        # max_age parameter can be used with unsign_object()\n        self.assertEqual(signer.unsign_object(signed_obj, max_age=12), value)\n        with self.assertRaises(signing.SignatureExpired):\n            signer.unsign_object(signed_obj, max_age=10)\n", "def test_default_sep_for_signer(self):\n    signer = signing.Signer(key=\"predictable-secret\")\n    self.assertEqual(signer.sep, \":\")\n", "def test_fallback_keys_verification(self):\n    old_signer = signing.Signer(key=\"oldsecret\")\n    signed = old_signer.sign(\"abc\")\n    signer = signing.Signer(key=\"newsecret\")\n    self.assertEqual(signer.unsign(signed), \"abc\")\n"], "sample_173": ["def test_adapt_ipaddressfield_value(self):\n    value = '192.168.0.1'\n    self.assertEqual(self.ops.adapt_ipaddressfield_value(value), value)\n", "def test_autoinc_sql(self):\n    self.assertIsNone(self.ops.autoinc_sql('table', 'column'))\n", "def test_adapt_ipaddressfield_value(self):\n    value = '192.168.0.1'\n    self.assertEqual(self.ops.adapt_ipaddressfield_value(value), value)\n    self.assertIsNone(self.ops.adapt_ipaddressfield_value(None))\n", "def test_datetime_extract_sql(self):\n    lookup_type = 'year'\n    field_name = 'my_datetime_field'\n    tzname = 'UTC'\n    with self.assertRaisesMessage(NotImplementedError, self.may_require_msg % 'datetime_extract_sql'):\n        self.ops.datetime_extract_sql(lookup_type, field_name, tzname)\n", "def test_adapt_ipaddressfield_value(self):\n    ip_address = '192.168.1.1'\n    self.assertEqual(self.ops.adapt_ipaddressfield_value(ip_address), ip_address)\n\n    # Test that None is returned when the value is None or an empty string\n    self.assertIsNone(self.ops.adapt_ipaddressfield_value(None))\n    self.assertIsNone(self.ops.adapt_ipaddressfield_value(''))\n"], "sample_691": ["def test_enabled_with_timeout(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler dumps tracebacks after a certain timeout when enabled.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import os, time\n        time.sleep(1 if \"CI\" in os.environ else 0.1)\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.01\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stderr.fnmatch_lines([\"*most recent call first*\"])\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n", "def test_crash_in_fixture(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler displays crashes that happen in fixtures.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n        import pytest\n        @pytest.fixture\n            faulthandler._sigabrt()\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stderr.fnmatch_lines([\"*Fatal Python error*\"])\n    assert result.ret != 0\n", "def test_cancel_timeout_on_yield(monkeypatch) -> None:\n    \"\"\"Test that we are cancelling any scheduled traceback dumping due to timeout\n    when the test yields.\"\"\"\n    import faulthandler\n    from _pytest.faulthandler import FaultHandlerHooks\n\n    called = []\n\n    monkeypatch.setattr(\n        faulthandler, \"cancel_dump_traceback_later\", lambda: called.append(1)\n    )\n\n    class TestItem:\n        class config:\n            _store = {fault_handler_stderr_key: True}\n\n            self.config = TestItem.config\n\n    @pytest.fixture\n        return TestItem()\n\n    @pytest.mark.parametrize(\"timeout\", [0, 2])\n        monkeypatch.setitem(item.config._store, fault_handler_stderr_key, True if timeout > 0 else None)\n        monkeypatch.setattr(FaultHandlerHooks, \"get_timeout_config_value\", lambda self, config: timeout)\n\n        gen = FaultHandlerHooks().pytest_runtest_protocol(item)\n        next(gen)\n        called.clear()\n        gen.send(None)\n        assert called == [1] if timeout > 0 else []\n", "def test_pdb_interaction(pytester: Pytester) -> None:\n    \"\"\"Test that traceback dumping is cancelled when entering pdb.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pdb\n        import time\n            pdb.set_trace()\n            time.sleep(0.1)\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.01\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\n        \"--pdb\",\n        \"-k\",\n        \"test_pdb\",\n        \"-o\",\n        \"python_functions=test_pdb\",\n        \"--no-capture\",\n        \"-s\",\n    )\n    tb_output = \"most recent call first\"\n    assert tb_output not in result.stderr.str()\n    assert \">           pdb.set_trace()\" in result.stdout.str()\n    assert result.ret == 0\n", "def test_pdb_interaction(pytester: Pytester) -> None:\n    \"\"\"Test the interaction between pdb and the fault handler.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import pdb\n        pdb.set_trace()\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.01\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\"-s\")\n    # Check that the test enters pdb mode and that the timeout is cancelled\n    result.stdout.fnmatch_lines([\"*Pdb*\"])\n    assert \"most recent call first\" not in result.stderr.str()\n    assert result.ret == 0\n"], "sample_428": ["    def test_scientific_notation(self):\n        self.assertEqual(nformat(123456789, \".\", decimal_pos=2, grouping=3, thousand_sep=\",\"), \"123,456,789.00\")\n        self.assertEqual(nformat(1234567890, \".\", decimal_pos=2, grouping=3, thousand_sep=\",\"), \"1.23e+09\")\n", "def test_grouping_sequence(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\"),\n        \"12,34,56,789\"\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"12,34,56,789\"\n    )\n", "    def test_non_uniform_digit_grouping(self):\n        self.assertEqual(\n            nformat(123456789, \".\", grouping=(3, 2), thousand_sep=\",\", force_grouping=True),\n            \"12,34,56,78,9\",\n        )\n", "def test_format_with_non_uniform_grouping(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"123,45,67,89\"\n    )\n", "    def test_grouping_sequence(self):\n        self.assertEqual(\n            nformat(12345678, \".\", grouping=(3, 2), thousand_sep=\",\", force_grouping=True),\n            \"12,34,56,78\"\n        )\n"], "sample_1134": ["def test_Matrix_latex_printing():\n    M = Matrix([[1, 2], [3, 4]])\n    assert latex(M) == r'\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]'\n\n    M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert latex(M) == r'\\left[\\begin{matrix}1 & 2 & 3\\\\4 & 5 & 6\\\\7 & 8 & 9\\end{matrix}\\right]'\n\n    M = Matrix([[1, 2], [3, 4], [5, 6]])\n    assert latex(M) == r'\\left[\\begin{matrix}1 & 2\\\\3 & 4\\\\5 & 6\\end{matrix}\\right]'\n", "def test_latex_DiracDelta_derivatives():\n    assert latex(DiracDelta(x, 1)) == r\"\\delta^{\\left( 1 \\right)}\\left( x \\right)\"\n    assert latex(DiracDelta(x, 2)) == r\"\\delta^{\\left( 2 \\right)}\\left( x \\right)\"\n    assert latex(Derivative(DiracDelta(x, 1), x)) == r\"\\frac{d}{d x} \\delta^{\\left( 1 \\right)}\\left( x \\right)\"\n    assert latex(Derivative(DiracDelta(x, 2), x, x)) == r\"\\frac{d^{2}}{d x^{2}} \\delta^{\\left( 2 \\right)}\\left( x \\right)\"\n", "def test_StrPrinter():\n    from sympy.printing.str import StrPrinter\n\n    # Test StrPrinter on various sympy objects\n    assert str(StrPrinter().doprint(Symbol('x'))) == 'x'\n    assert str(StrPrinter().doprint(Integer(42))) == '42'\n    assert str(StrPrinter().doprint(Rational(1, 2))) == '1/2'\n    assert str(StrPrinter().doprint(sqrt(2))) == 'sqrt(2)'\n    assert str(StrPrinter().doprint(sin(x))) == 'sin(x)'\n    assert str(StrPrinter().doprint(Matrix([[1, 2], [3, 4]]))) == 'Matrix([[1, 2], [3, 4]])'\n", "def test_latex_Piecewise():\n    # Testing Piecewise with more complex conditions\n    x = symbols('x')\n    p = Piecewise((x, Eq(x, 0)), (sin(x), True))\n    assert latex(p) == r\"\\begin{cases} x & \\text{for}\\: x = 0 \\\\\\sin{\\left(x \\right)} & \\text{otherwise} \\end{cases}\"\n\n    p = Piecewise((x, x < 0), (x**2, x >= 0))\n    assert latex(p) == r\"\\begin{cases} x & \\text{for}\\: x < 0 \\\\\\left(x^{2}\\right) & \\text{for}\\: x \\geq 0 \\end{cases}\"\n\n    # Testing Piecewise with commutative and non-commutative symbols\n    A, B = symbols('A B', commutative=False)\n    p = Piecewise((A**2, Eq(A, B)), (A*B, True))\n    assert latex(p) == r\"\\begin{cases} A^{2} & \\text{for}\\: A = B \\\\\\left(A B\\right) & \\text{otherwise} \\end{cases}\"\n", "def test_issue_19069():\n    from sympy.matrices.expressions.matpow import MatPow\n    from sympy.matrices.expressions.matexp import MatrixExp\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert latex(MatPow(A, -1)) == r\"A^{-1}\"\n    assert latex(MatrixExp(A)) == r\"\\exp{\\left(A \\right)}\"\n"], "sample_1190": ["def test_physical_constant_property():\n    assert gravitational_constant.is_physical_constant\n    assert molar_gas_constant.is_physical_constant\n    assert vacuum_permittivity.is_physical_constant\n    assert speed_of_light.is_physical_constant\n    assert elementary_charge.is_physical_constant\n    assert not meter.is_physical_constant\n    assert not joule.is_physical_constant\n", "def test_physical_constant_conversion():\n    from sympy.physics.units import meter, second\n    from sympy.physics.units.definitions import speed_of_light\n\n    # Test conversion of a physical constant to different units\n    assert speed_of_light.convert_to(meter / second) == 299792458 * meter / second\n    assert speed_of_light.convert_to(kilometer / second) == 299792.458 * kilometer / second\n    assert speed_of_light.convert_to(kilometer / hour) == 1079252848.8 * kilometer / hour\n\n    # Test conversion of a physical constant to the same unit\n    assert speed_of_light.convert_to(meter / second) == speed_of_light\n", "def test_derived_units():\n    derived_units = SI.derived_units\n    assert isinstance(derived_units, dict)\n    assert Dimension(1)/Dimension(2) in derived_units\n    assert isinstance(derived_units[Dimension(1)/Dimension(2)], Quantity)\n    assert derived_units[Dimension(1)/Dimension(2)] == hertz\n    assert Dimension(1)/(Dimension(2)*Dimension(3)) in derived_units\n    assert derived_units[Dimension(1)/(Dimension(2)*Dimension(3))] == joule/(newton*meter)\n", "def test_physical_constant_conversion():\n    from sympy.physics.units import meter, second, joule\n    from sympy.physics.units.definitions import gravitational_constant, speed_of_light\n\n    # Test conversion of physical constants\n    assert gravitational_constant.convert_to(joule*meter**2/kilogram**2) == 6.67430e-11*joule*meter**2/kilogram**2\n    assert speed_of_light.convert_to(meter/second) == 299792458*meter/second\n", "def test_unit_system_consistency():\n    from sympy.physics.units import (length, time, mass, current,\n                                     temperature, luminous_intensity,\n                                     amount_of_substance)\n    from sympy.physics.units.systems import UnitSystem, SI\n    from sympy.physics.units.definitions import meter, second, kilogram, ampere, kelvin, candela, mole\n\n    si_units = UnitSystem('SI', (meter, second, kilogram, ampere, kelvin, mole, candela), name='SI', description='International System of Units')\n\n    assert si_units.is_consistent\n    assert si_units.get_quantity_dimension(meter) == length\n    assert si_units.get_quantity_dimension(second) == time\n    assert si_units.get_quantity_dimension(kilogram) == mass\n    assert si_units.get_quantity_dimension(ampere) == current\n    assert si_units.get_quantity_dimension(kelvin) == temperature\n    assert si_units.get_quantity_dimension(mole) == amount_of_substance\n    assert si_units.get_quantity_dimension(candela) == luminous_intensity\n\n    assert SI.is_consistent\n    assert SI.get_quantity_dimension(meter) == length\n    assert SI.get_quantity_dimension(second) == time\n    assert SI.get_quantity_dimension(kilogram) == mass\n    assert SI.get_quantity_dimension(ampere) == current\n    assert SI.get_quantity_dimension(kelvin) == temperature\n    assert SI.get_quantity_dimension(mole) == amount_of_substance\n    assert SI.get_quantity_dimension(candela) == luminous_intensity\n"], "sample_719": ["def test_vectorizer_max_df_min_df_interaction():\n    test_data = ['abc', 'dea', 'eat']\n    vect = CountVectorizer(analyzer='char', min_df=0.6, max_df=0.4)\n    message = \"max_df corresponds to < documents than min_df\"\n    assert_raise_message(\n        ValueError, message, vect.fit, test_data)\n", "def test_countvectorizer_max_df_min_df_check():\n    # Check that an error is raised if max_df < min_df\n    with pytest.raises(ValueError):\n        CountVectorizer(max_df=0.5, min_df=0.6)\n", "def test_vectorizer_partial_fit():\n    # Test partial_fit method of CountVectorizer and HashingVectorizer\n    data = ['This is a test', 'Another test document']\n    cv = CountVectorizer()\n    cv.partial_fit(data)\n    X = cv.transform(data)\n    assert_equal(X.shape, (2, 6))\n\n    hv = HashingVectorizer()\n    hv.partial_fit(data)\n    X = hv.transform(data)\n    assert_equal(X.shape, (2, 2**20))\n", "def test_tfidf_transformer_copy():\n    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)\n    X_csr = sparse.csr_matrix(X)\n\n    transformer = TfidfTransformer()\n    X_trans_original = transformer.fit_transform(X_csr)\n    X_trans_copy = transformer.transform(X_csr, copy=True)\n    X_trans_no_copy = transformer.transform(X_csr, copy=False)\n\n    # Check that the transformed data is the same\n    assert_allclose_dense_sparse(X_trans_original, X_trans_copy)\n    assert_allclose_dense_sparse(X_trans_original, X_trans_no_copy)\n\n    # Check that the original data is not modified when copy=False\n    assert_allclose_dense_sparse(X_trans_original, transformer.transform(X_csr, copy=True))\n\n    # Check that the original data is modified when copy=False\n    transformer.transform(X_csr, copy=False)\n    assert not np.allclose(X_trans_original.data, transformer.transform(X_csr, copy=True).data)\n", "def test_tfidf_vectorizer_inverse_transform():\n    # raw documents\n    data = ALL_FOOD_DOCS\n    for vectorizer in (TfidfVectorizer(), CountVectorizer()):\n        transformed_data = vectorizer.fit_transform(data)\n        inversed_data = vectorizer.inverse_transform(transformed_data)\n        analyze = vectorizer.build_analyzer()\n        for doc, inversed_terms in zip(data, inversed_data):\n            terms = np.sort(np.unique(analyze(doc)))\n            inversed_terms = np.sort(np.unique(inversed_terms))\n            assert_array_equal(terms, inversed_terms)\n\n        # Test that inverse_transform also works with numpy arrays\n        transformed_data = transformed_data.toarray()\n        inversed_data2 = vectorizer.inverse_transform(transformed_data)\n        for terms, terms2 in zip(inversed_data, inversed_data2):\n            assert_array_equal(np.sort(terms), np.sort(terms2))\n"], "sample_1181": ["def test_numpy_logaddexp2():\n    lae2 = logaddexp2(a, b)\n    assert SciPyPrinter().doprint(lae2) == 'scipy.special.logaddexp2(a, b)'\n", "def test_numpy_blockmatrix():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n    D = MatrixSymbol(\"D\", 2, 2)\n    BM = BlockMatrix([[A, B], [C, D]])\n\n    printer = NumPyPrinter()\n    result = printer.doprint(BM)\n    expected = \"numpy.block([[A, B], [C, D]])\"\n    assert result == expected\n", "def test_numpy_printer_module():\n    printer = NumPyPrinter()\n    assert printer._module == 'numpy'\n    assert printer.language == 'Python with NumPy'\n    assert printer.printmethod == '_Numpycode'\n", "def test_numpy_hadamard():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    expr = A * B\n    f = lambdify((A, B), expr, \"numpy\")\n\n    A_arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    B_arr = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n    expected_output = A_arr * B_arr\n\n    assert np.array_equal(f(A_arr, B_arr), expected_output)\n", "def test_numpy_logaddexp2_comparison():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    a_ = np.array([1.0, 2.0, 3.0])\n    b_ = np.array([4.0, 5.0, 6.0])\n\n    expr = logaddexp2(a, b)\n    f = lambdify((a, b), expr, 'numpy')\n\n    expected_output = np.logaddexp2(a_, b_)\n    assert np.allclose(f(a_, b_), expected_output)\n"], "sample_98": ["    def test_header_cleanup(self):\n        \"\"\"\n        Headers with underscores are stripped before constructing the WSGI environ.\n        \"\"\"\n        with self.assertRaises(HTTPError) as err:\n            urlopen(self.live_server_url + '/header_view/', headers={\"Test_Header\": \"value\"})\n        err.exception.close()\n        self.assertEqual(err.exception.code, 500, 'Expected 500 response')\n        with self.urlopen('/header_view/') as f:\n            self.assertNotIn(b'Test_Header', f.read())\n", "def test_server_handler_cleanup_headers(self):\n    \"\"\"\n    ServerHandler.cleanup_headers() adds 'Connection' header if 'Content-Length' is not in headers.\n    \"\"\"\n    from django.core.servers.basehttp import ServerHandler\n\n    class MockRequestHandler:\n        class MockServer:\n            pass\n\n            self.server = self.MockServer()\n            self.headers = {}\n\n            pass\n\n    request_handler = MockRequestHandler()\n    server_handler = ServerHandler(None, None, None, None, request_handler=request_handler)\n    server_handler.cleanup_headers()\n    self.assertEqual(server_handler.headers['Connection'], 'close')\n\n    request_handler.headers['Content-Length'] = '10'\n    server_handler.cleanup_headers()\n    self.assertNotIn('Connection', server_handler.headers)\n", "    def test_broken_pipe_error(self):\n        \"\"\"\n        BrokenPipeError is handled correctly and does not cause server to crash.\n        \"\"\"\n        with self.assertLogs('django.server', level='INFO') as cm:\n            with self.assertRaises(OSError) as err:\n                # Create a connection, but don't read the response\n                conn = HTTPConnection(LiveServerExceptionHandling.server_thread.host, LiveServerExceptionHandling.server_thread.port)\n                conn.request('GET', '/broken_pipe_view/')\n            self.assertEqual(err.exception.errno, errno.EPIPE)\n            self.assertIn(\"- Broken pipe from\", cm.output[0])\n        conn.close()\n", "    def test_broken_pipe_error(self):\n        \"\"\"\n        Broken pipe errors are ignored and logged as INFO.\n        \"\"\"\n        with self.assertLogs('django.server', 'INFO') as cm:\n            # Create a socket connection to the server\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            s.connect((LiveServerBrokenPipe.server_thread.host, LiveServerBrokenPipe.server_thread.port))\n\n            # Send a request to the server\n            request = 'GET /example_view/ HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n'\n            s.sendall(request.encode())\n\n            # Close the socket connection without reading the response\n            s.close()\n\n        # Check that the broken pipe error was logged as INFO\n        self.assertIn('- Broken pipe from', cm.output[0])\n", "    def test_broken_pipe_error_handling(self):\n        \"\"\"\n        The server should handle BrokenPipeError gracefully and log an info message.\n        \"\"\"\n        # Create a socket\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        # Connect to the server\n        s.connect((LiveServerBrokenPipe.server_thread.host, LiveServerBrokenPipe.server_thread.port))\n        # Send a GET request\n        s.sendall(b\"GET /example_view/ HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n\")\n        # Close the socket immediately after sending the request\n        s.close()\n        # Wait for a short period to allow the server to handle the request and log the error\n        import time\n        time.sleep(0.1)\n        # Check the logs to ensure that a BrokenPipeError was handled gracefully\n        self.assertTrue(any(\"Broken pipe from\" in log for log in self.server_thread.log))\n"], "sample_868": ["def test_empty_input(metric_name):\n    # All clustering metrics should handle empty input\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0  # empty input should return 1.0 for all metrics\n", "def test_empty_input(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0\n", "def test_empty_input(metric):\n    # only the supervised metrics support empty input\n    for i, j in [([], []), ([], [1]), ([1], [])]:\n        metric(i, j)\n", "def test_identical_labels(metric_name, metric_func):\n    y_true = [0, 0, 1, 1, 2, 2]\n    if metric_name in SUPERVISED_METRICS:\n        assert metric_func(y_true, y_true) == pytest.approx(1.0)\n    else:\n        X = np.random.randint(10, size=(6, 10))\n        assert metric_func(X, y_true) == pytest.approx(1.0)\n", "def test_empty_input(metric_name):\n    # All clustering metrics should handle empty input\n    empty_labels = np.array([])\n    if metric_name in SUPERVISED_METRICS:\n        metric = SUPERVISED_METRICS[metric_name]\n        assert metric(empty_labels, empty_labels) == pytest.approx(1.0)\n    else:\n        metric = UNSUPERVISED_METRICS[metric_name]\n        empty_features = np.empty((0, 10))\n        # Silhouette score raises an exception for empty input\n        if metric_name == \"silhouette_score\":\n            with pytest.raises(ValueError, match=\"Number of labels is 0. Valid values are 2 to n_samples.\"):\n                metric(empty_features, empty_labels)\n        else:\n            assert metric(empty_features, empty_labels) == pytest.approx(0.0)\n"], "sample_636": ["def test_duplicate_code_raw_strings_disable_scope_nested(self) -> None:\n    \"\"\"Tests disabling duplicate-code at a nested scope level.\"\"\"\n    path = join(DATA, \"raw_strings_disable_scope_nested\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_raw_strings_disable_multiple_lines(self) -> None:\n    \"\"\"Tests disabling duplicate-code at multiple lines in the middle of a piece of similar code.\"\"\"\n    path = join(DATA, \"raw_strings_disable_multiple_lines\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_raw_strings_disable_line_and_scope(self) -> None:\n    \"\"\"Tests disabling duplicate-code at a line and an inner scope level.\"\"\"\n    path = join(DATA, \"raw_strings_disable_line_and_scope\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_raw_strings_disable_line_inside_similar_code(self) -> None:\n    \"\"\"Tests disabling duplicate-code at a line inside a piece of similar code.\"\"\"\n    path = join(DATA, \"raw_strings_disable_line_inside\")\n    self._runtest([path, \"--disable=all\", \"--enable=duplicate-code\"], code=0)\n", "def test_duplicate_code_raw_strings_disable_line_single(self) -> None:\n    \"\"\"Tests disabling duplicate-code at a line in the middle of a piece of similar code,\n    but with a single line of difference in one of the files.\n    \"\"\"\n    path = join(DATA, \"raw_strings_disable_line_single\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\"],\n        expected_output=expected_output,\n    )\n"], "sample_500": ["def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.random((10, 10)), alpha=0.5)\n    cbar = fig.colorbar(im)\n    assert im.colorbar.alpha is None\n    assert cbar.alpha == 0.5\n", "def test_colorbar_set_label():\n    # Test the set_label method of the Colorbar class\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im)\n    cbar.set_label('New Label')\n    assert cbar.ax.get_ylabel() == 'New Label'\n", "def test_colorbar_symlognorm():\n    fig, ax = plt.subplots()\n    norm = mcolors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                              vmin=-1, vmax=1, base=10)\n    pc = ax.pcolormesh(np.arange(-100, 100).reshape(10, 20),\n                       norm=norm, cmap='RdBu_r')\n    fig.colorbar(pc)\n", "def test_colorbar_lines():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    CS = ax.contourf(data, levels=levels)\n    cbar = fig.colorbar(CS)\n    cbar.add_lines(CS, erase=True)\n", "def test_colorbar_with_hatching():\n    # Test the colorbar with hatching.\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    cs = ax.contourf(data, levels=levels, hatches=['//', '\\\\\\\\', '||', '--', '++', 'xx', 'oo'])\n    fig.colorbar(cs)\n"], "sample_75": ["def test_prefetch_reverse_foreign_key_with_to_attr(self):\n    with self.assertNumQueries(2):\n        bookwithyear1, = BookWithYear.objects.prefetch_related(Prefetch('bookreview_set', to_attr='reviews'))\n    with self.assertNumQueries(0):\n        self.assertCountEqual(bookwithyear1.reviews, [self.bookreview1])\n    with self.assertNumQueries(0):\n        prefetch_related_objects([bookwithyear1], 'reviews')\n", "def test_custom_queryset_does_not_cache_results(self):\n    bookmark = Bookmark.objects.create(url='http://www.djangoproject.com/')\n    django_tag = TaggedItem.objects.create(content_object=bookmark, tag='django')\n    python_tag = TaggedItem.objects.create(content_object=bookmark, tag='python')\n\n    with self.assertNumQueries(2):\n        bookmark = Bookmark.objects.prefetch_related(\n            Prefetch('tags', TaggedItem.objects.filter(tag='django')),\n        ).get()\n\n    with self.assertNumQueries(1):\n        self.assertEqual(list(bookmark.tags.all()), [django_tag])\n\n    with self.assertNumQueries(1):\n        self.assertEqual(list(bookmark.tags.filter(tag='python')), [python_tag])\n", "def test_multiple_m2m_prefetching(self):\n    authors = AuthorWithAge.objects.prefetch_related('favorite_authors__first_book', 'books')\n    with self.assertNumQueries(4):\n        # AuthorWithAge -> FavoriteAuthors, Book -> AuthorWithAge -> Book\n        for author in authors:\n            for fav_author in author.favorite_authors.all():\n                _ = fav_author.first_book\n            for book in author.books.all():\n                _ = book.authors.all()\n", "def test_prefetch_related_with_nonexistent_lookup(self):\n    with self.assertRaisesMessage(AttributeError, \"Cannot find 'nonexistent_lookup' on AuthorWithAge object, 'nonexistent_lookup' is an invalid parameter to prefetch_related()\"):\n        AuthorWithAge.objects.prefetch_related('nonexistent_lookup')\n", "def test_m2m_through_with_to_attr(self):\n    # Test that to_attr works correctly with a m2m through relationship.\n    book1 = Book.objects.create(title='book1')\n    author1 = Author.objects.create(first_book=book1, name='Author1')\n    author2 = Author.objects.create(first_book=book1, name='Author2')\n    book1.authors.add(author1, author2)\n\n    prefetch = Prefetch('authors', queryset=Author.objects.all(), to_attr='authors_list')\n    book1 = Book.objects.prefetch_related(prefetch).get(id=book1.id)\n\n    self.assertEqual(book1.authors_list, [author1, author2])\n"], "sample_89": ["def test_watched_files_include_globs(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    self.reloader.watch_file(self.existing_file)\n    watched_files = list(self.reloader.watched_files(include_globs=True))\n    self.assertIn(self.existing_file, watched_files)\n    self.assertIn(self.tempdir / 'another_file.py', watched_files)\n", "    def test_tick_calls_notify_file_changed(self):\n        self.reloader.watch_file(self.existing_file)\n        ticker = self.reloader.tick()\n        with mock.patch.object(self.reloader, 'notify_file_changed') as mock_notify:\n            next(ticker)\n            self.increment_mtime(self.existing_file)\n            next(ticker)\n        mock_notify.assert_called_once_with(self.existing_file)\n", "def test_file_deletion(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.existing_file)\n    self.existing_file.unlink()\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "    def test_notify_file_changed_without_receivers(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.utils.autoreload.trigger_reload') as mocked_trigger:\n            reloader.notify_file_changed('test.py')\n        self.assertEqual(mocked_trigger.call_count, 1)\n", "def test_deleted_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.existing_file)\n    self.existing_file.unlink()\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n"], "sample_847": ["def test_multi_task_lasso_and_enet_positive():\n    X, y, _, _ = build_dataset()\n    Y = np.c_[y, y]\n    clf = MultiTaskLasso(alpha=1, tol=1e-8, positive=True).fit(X, Y)\n    assert np.all(clf.coef_ >= 0)\n\n    clf = MultiTaskElasticNet(alpha=1, tol=1e-8, positive=True).fit(X, Y)\n    assert np.all(clf.coef_ >= 0)\n", "def test_sparse_input_dual_gap():\n    X, y, _, _ = build_dataset(n_samples=100, n_features=50)\n    clf = ElasticNet(alpha=0.1, tol=1e-8)\n    clf.fit(sparse.csr_matrix(X), y)\n    assert 0 <= clf.dual_gap_ < 1e-5\n", "def test_enet_path_precompute_false():\n    # Test precompute=False in enet_path\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    _, coefs, _ = enet_path(X, y, fit_intercept=False, precompute=False)\n    _, coefs_precompute, _ = enet_path(X, y, fit_intercept=False, precompute=True)\n    assert_array_almost_equal(coefs, coefs_precompute)\n", "def test_enet_max_iter(klass):\n    \"\"\"Test that a warning is issued if model does not converge within max_iter\"\"\"\n    clf = klass(max_iter=2)\n    n_samples = 5\n    n_features = 2\n    X = np.ones((n_samples, n_features)) * 1e50\n    y = np.ones((n_samples,))\n    if klass == MultiTaskElasticNet:\n        y = np.column_stack((y, y))\n    assert_warns(ConvergenceWarning, clf.fit, X, y)\n", "def test_warm_start_multitask_elasticnet():\n    X, y, X_test, y_test = build_dataset()\n    Y = np.c_[y, y]\n    clf = MultiTaskElasticNet(alpha=0.1, max_iter=5, warm_start=True)\n    ignore_warnings(clf.fit)(X, Y)\n    ignore_warnings(clf.fit)(X, Y)  # do a second round with 5 iterations\n\n    clf2 = MultiTaskElasticNet(alpha=0.1, max_iter=10)\n    ignore_warnings(clf2.fit)(X, Y)\n    assert_array_almost_equal(clf2.coef_, clf.coef_)\n"], "sample_692": ["def test_mktemp_with_invalid_basename(pytester: Pytester) -> None:\n    mytemp = pytester.mkdir(\"mytemp\")\n    p = pytester.makepyfile(\n        \"\"\"\n            tmpdir_factory.mktemp('/invalid/path', numbered=False)\n        \"\"\"\n    )\n\n    result = pytester.runpytest(p, \"--basetemp=%s\" % mytemp)\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\"*ValueError*\")\n", "def test_tmp_path_factory_mktemp_with_invalid_basename(tmp_path_factory: TempPathFactory):\n    invalid_basenames = [\"/invalid\", \"../invalid\", \"invalid/..\", \"invalid/../invalid\"]\n    for basename in invalid_basenames:\n        with pytest.raises(ValueError):\n            tmp_path_factory.mktemp(basename)\n", "def test_tmpdir_factory_mktemp_basename(tmp_path_factory):\n    temp_dir = tmp_path_factory.mktemp(\"test_dir\")\n    assert temp_dir.name.startswith(\"test_dir\")\n    assert temp_dir.exists()\n", "def test_tmp_path_factory_mktemp_with_invalid_characters(tmp_path_factory: TempPathFactory) -> None:\n    invalid_basename = \"in/valid/<:*?;>basename\"\n    with pytest.raises(ValueError, match=f\"{invalid_basename} is not a normalized and relative path\"):\n        tmp_path_factory.mktemp(invalid_basename)\n", "def test_tmppath_factory_from_config(monkeypatch):\n    \"\"\"Test TempPathFactory.from_config with a given basetemp.\"\"\"\n    config = Config.fromdict({\"option\": {\"basetemp\": \"test_basetemp\"}})\n    t = TempPathFactory.from_config(config, _ispytest=True)\n    assert t._given_basetemp == Path(\"test_basetemp\").resolve()\n\n    # Test with None as given_basetemp\n    config = Config.fromdict({\"option\": {\"basetemp\": None}})\n    t = TempPathFactory.from_config(config, _ispytest=True)\n    assert t._given_basetemp is None\n"], "sample_795": ["def test_check_no_attributes_set_in_init_with_deprecated_init():\n    class NonConformantEstimatorDeprecatedInit:\n        @deprecated(\"Deprecated for the purpose of testing check_no_attributes_set_in_init\")\n            self.you_should_not_set_this_ = None\n\n    check_no_attributes_set_in_init('estimator_name', NonConformantEstimatorDeprecatedInit())\n", "def test_check_fit_score_takes_y_works_on_class_without_fit():\n    # Tests that check_fit_score_takes_y works on a class without\n    # a fit method\n\n    class TestEstimatorWithoutFitMethod(BaseEstimator):\n            return 0.5\n\n    check_fit_score_takes_y(\"test\", TestEstimatorWithoutFitMethod())\n", "def test_check_estimators_unfitted_deprecation():\n    # check that a DeprecationWarning is raised when calling predict\n    # on an unfitted estimator\n    with assert_warns(DeprecationWarning):\n        check_estimators_unfitted(\"estimator\", NoSparseClassifier())\n", "def test_check_fit_score_takes_y():\n    # check that fit_score_takes_y works on a class with\n    # a deprecated fit method\n\n    class TestEstimatorWithDeprecatedFitMethod(BaseEstimator):\n        @deprecated(\"Deprecated for the purpose of testing \"\n                    \"check_fit_score_takes_y\")\n            return self\n\n    check_fit_score_takes_y(\"test\", TestEstimatorWithDeprecatedFitMethod())\n", "def test_check_no_attributes_set_in_init_with_deprecation_decorator():\n    class NonConformantEstimatorWithDeprecationDecorator:\n        @deprecated(\"Deprecated for the purpose of testing check_no_attributes_set_in_init\")\n            self.you_should_not_set_this_ = None\n\n    check_no_attributes_set_in_init('estimator_name', NonConformantEstimatorWithDeprecationDecorator())\n"], "sample_0": ["def test_conversion_between_uncertainty_types_supported():\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = StdDevUncertainty(uncert)\n    final_uncert = start_uncert.represent_as(VarianceUncertainty)\n    assert_array_equal(start_uncert.array ** 2, final_uncert.array)\n    assert start_uncert.unit ** 2 == final_uncert.unit\n    # Convert back\n    start_uncert = final_uncert.represent_as(StdDevUncertainty)\n    assert_array_equal(np.sqrt(final_uncert.array), start_uncert.array)\n    assert start_uncert.unit ** 0.5 == final_uncert.unit\n", "def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    var_uncert = VarianceUncertainty(uncert)\n    with pytest.raises(TypeError):\n        final_uncert = start_uncert.represent_as(VarianceUncertainty)\n    with pytest.raises(TypeError):\n        final_uncert = var_uncert.represent_as(UncertClass)\n", "def test_conversion_between_different_uncertainty_types_supported():\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = StdDevUncertainty(uncert)\n    final_uncert = start_uncert.represent_as(VarianceUncertainty)\n    assert_allclose(start_uncert.array ** 2, final_uncert.array)\n    assert start_uncert.unit ** 2 == final_uncert.unit\n    final_uncert = final_uncert.represent_as(InverseVariance)\n    assert_allclose(1 / (start_uncert.array ** 2), final_uncert.array)\n    assert (1 / start_uncert.unit ** 2) == final_uncert.unit\n", "def test_unknown_uncertainty_propagation_exception():\n    # UnknownUncertainty should raise IncompatibleUncertaintiesException during propagation\n    uncert = UnknownUncertainty([1, 2, 3], unit=u.adu)\n    ndd = NDData([1, 2, 3], unit=u.adu, uncertainty=uncert)\n    ndd2 = NDData([1, 2, 3], unit=u.adu, uncertainty=uncert)\n\n    with pytest.raises(IncompatibleUncertaintiesException):\n        ndd.add(ndd2)\n", "def test_conversion_between_uncertainty_types_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    for other_UncertClass in uncertainty_types_with_conversion_support:\n        if UncertClass is not other_UncertClass:\n            final_uncert = start_uncert.represent_as(other_UncertClass)\n            assert start_uncert.array is not final_uncert.array\n            # The conversion should be accurate\n            var_uncert = start_uncert.represent_as(VarianceUncertainty)\n            var_final_uncert = final_uncert.represent_as(VarianceUncertainty)\n            assert_allclose(var_uncert.array, var_final_uncert.array)\n"], "sample_559": ["def test_grid_with_axes_class_overriding_axis():\n    class CustomAxes(mpl.axes.Axes):\n            super().set_axis(axis, value)\n\n    grid = Grid(plt.figure(), 111, (2, 2), axes_class=CustomAxes)\n    for ax in grid:\n        ax.set_axis('x', [0, 1])\n        ax.set_axis('y', [0, 1])\n    plt.close()\n", "def test_grid_with_direction_kwarg():\n    fig = plt.figure()\n    with pytest.raises(TypeError, match=\"direction must be str\"):\n        Grid(fig, 111, (2, 2), direction=123)\n", "def test_grid_with_custom_axes_class_not_overriding_axis():\n    class CustomAxes(mpl.axes.Axes):\n        pass\n\n    grid = Grid(plt.figure(), 111, (2, 2), axes_class=(CustomAxes, {}))\n    for ax in grid:\n        assert isinstance(ax, CustomAxes)\n        assert not hasattr(ax, 'axis')\n", "def test_grid_with_axes_class_overriding_axis():\n    class CustomAxes(mpl.axes.Axes):\n        name = 'custom'\n\n    grid = Grid(plt.figure(), 111, (2, 2), axes_class=CustomAxes)\n    for ax in grid:\n        assert isinstance(ax, CustomAxes)\n", "def test_grid_with_axes_class_and_axis_kwarg():\n    with pytest.raises(ValueError, match=\"axes_class and axis kwargs are \"\n                                        \"mutually exclusive\"):\n        Grid(plt.figure(), 111, (2, 2), axes_class=mpl.axes.Axes, axis='both')\n"], "sample_684": ["def test_code_getargs_no_varargs() -> None:\n        raise NotImplementedError()\n\n    c = Code(f)\n    assert c.getargs(var=False) == (\"x\", \"y\")\n", "def test_frame_eval(capsys):\n        return sys._getframe(0)\n\n    fr = Frame(func(42))\n    result = fr.eval(\"x\")\n    assert result == 42\n\n    captured = capsys.readouterr()\n    assert captured.out == \"\"\n    assert captured.err == \"\"\n", "def test_repr_funcargs_truncation(tw_mock) -> None:\n    large_string = \"a\" * 100\n    args = [(\"large_string\", large_string)]\n\n    r = ReprFuncArgs(args)\n    r.toterminal(tw_mock)\n\n    assert len(tw_mock.lines[0]) <= tw_mock.fullwidth\n", "def test_frame_eval(capsys) -> None:\n        return sys._getframe(0)\n\n    fr = Frame(f(42))\n    result = fr.eval(\"x\")\n    assert result == 42\n\n    captured = capsys.readouterr()\n    assert captured.out == \"\"\n    assert captured.err == \"\"\n", "def test_frame_eval(capsys):\n        return sys._getframe(0)\n\n    fr1 = Frame(f1(\"a\"))\n    result = fr1.eval(\"x + 'b'\")\n    assert result == \"ab\"\n    captured = capsys.readouterr()\n    assert captured.out == \"\"\n    assert captured.err == \"\"\n"], "sample_393": ["def test_no_obsolete_enabled(self):\n    management.call_command(\n        \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n    )\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertNotIn('msgid \"Obsolete message\"', po_contents)\n", "def test_template_message_context_extractor_in_javascript(self):\n    \"\"\"\n    Message contexts are correctly extracted for the gettext() and ngettext()\n    JavaScript functions (#14806).\n    \"\"\"\n    management.call_command(\"makemessages\", locale=[LOCALE], domain=\"djangojs\", verbosity=0)\n    self.assertTrue(os.path.exists(\"locale/%s/LC_MESSAGES/djangojs.po\" % LOCALE))\n    with open(\"locale/%s/LC_MESSAGES/djangojs.po\" % LOCALE) as fp:\n        po_contents = fp.read()\n        # gettext()\n        self.assertIn('msgctxt \"Special trans context #1\"', po_contents)\n        self.assertMsgId(\"Translatable literal #7a\", po_contents)\n\n        # ngettext()\n        self.assertIn('msgctxt \"Special trans context #2\"', po_contents)\n        self.assertMsgId(\"Translatable literal #7b-singular\", po_contents)\n        self.assertIn(\"Translatable literal #7b-plural\", po_contents)\n", "def test_no_obsolete_enabled(self):\n    management.call_command(\n        \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n    )\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n        self.assertNotIn(\"This is an obsolete string.\", po_contents)\n", "def test_remove_potfiles(self):\n    management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    cmd = MakeMessagesCommand()\n    cmd.locale_paths = [os.path.dirname(self.PO_FILE)]\n    cmd.domain = \"django\"\n    cmd.remove_potfiles()\n    self.assertFalse(os.path.exists(os.path.join(os.path.dirname(self.PO_FILE), \"django.pot\")))\n", "    def test_msgid_plural_with_quotes(self):\n        msgid = \"This is a plural string\"\n        haystack = '\"Plural-Forms: nplurals=2; plural=(n != 1);\\n\"'\n        haystack += '\"#: test.py:1\\n\"'\n        haystack += f'\"msgid_plural \"{msgid}\"\\n\"'\n        haystack += '\"msgstr[0] \"\"\\n\"'\n        haystack += '\"msgstr[1] \"\"\\n\"'\n        self.assertMsgIdPlural(msgid, haystack, use_quotes=True)\n"], "sample_477": ["def test_non_empty_list(self):\n    test_list = [\"apple\", \"banana\", \"cherry\"]\n    output = self.engine.render_to_string(\"non_empty_list\", {\"list\": test_list})\n    self.assertIn(output, test_list)\n", "def test_single_item_list(self):\n    output = self.engine.render_to_string(\"single_item_list\", {\"list\": [\"only_item\"]})\n    self.assertEqual(output, \"only_item\")\n", "def test_list_with_single_item(self):\n    output = self.engine.render_to_string(\"list_with_single_item\", {\"list\": [\"single_item\"]})\n    self.assertEqual(output, \"single_item\")\n", "def test_non_empty_list(self):\n    test_list = [\"apple\", \"banana\", \"cherry\"]\n    output = self.engine.render_to_string(\"non_empty_list\", {\"list\": test_list})\n    self.assertIn(output, test_list)\n", "def test_single_item_list(self):\n    output = self.engine.render_to_string(\"single_item_list\", {\"list\": [\"single\"]})\n    self.assertEqual(output, \"single\")\n"], "sample_1139": ["def test_issue_18146():\n    e = Symbol('e', integer=True, even=True)\n    o = Symbol('o', integer=True, odd=True)\n    assert Range(5).contains(i) == And(i >= 0, i <= 4)\n    assert Range(1).contains(i) == Eq(i, 0)\n    assert Range(-oo, 5, 1).contains(i) == (i <= 4)\n    assert Range(-oo, oo).contains(i) == True\n    assert Range(0, 8, 2).contains(i) == Contains(i, Range(0, 8, 2))\n    assert Range(0, 8, 2).contains(e) == And(e >= 0, e <= 6)\n    assert Range(0, 8, 2).contains(2*i) == And(2*i >= 0, 2*i <= 6)\n    assert Range(0, 8, 2).contains(o) == False\n    assert Range(1, 9, 2).contains(e) == False\n    assert Range(1, 9, 2).contains(o) == And(o >= 1, o <= 7)\n    assert Range(8, 0, -2).contains(o) == False\n    assert Range(9, 1, -2).contains(o) == And(o >= 3, o <= 9)\n    assert Range(-oo, 8, 2).contains(i) == Contains(i, Range(-oo, 8, 2))\n", "def test_issue_18081():\n    assert ImageSet(Lambda(n, n * log(2)), S.Integers).intersection(\n        S.Integers).dummy_eq(Intersection(ImageSet(\n        Lambda(n, n * log(2)), S.Integers), S.Integers))\n", "def test_range_len():\n    assert len(Range(0, 5)) == 5\n    assert len(Range(5, 0, -1)) == 5\n    assert len(Range(0, 10, 2)) == 5\n    assert len(Range(10, 0, -2)) == 5\n    assert len(Range(0, 100, 10)) == 10\n    assert len(Range(100, 0, -10)) == 10\n", "def test_issue_17991():\n    assert ImageSet(Lambda(x, tan(x)), S.Integers).intersect(S.Reals) == ImageSet(Lambda(x, tan(x)), S.Integers)\n", "def test_Range_intersection():\n    # Test the intersection of two Ranges\n    assert Range(0, 10).intersect(Range(5, 15)) == Range(5, 10)\n    assert Range(0, 10).intersect(Range(10, 20)) == Range(10, 10)  # Empty Range\n    assert Range(0, 10).intersect(Range(15, 20)) is S.EmptySet\n    assert Range(0, 10, 2).intersect(Range(1, 10, 2)) == Range(2, 10, 2)\n    assert Range(10, 0, -2).intersect(Range(0, 10, 2)) == Range(8, 0, -2)\n    assert Range(0, oo).intersect(Range(-10, 10)) == Range(0, 10)\n    assert Range(-oo, oo).intersect(Range(-10, 10)) == Range(-10, 10)\n    assert Range(-oo, 0).intersect(Range(0, oo)) == Range(0, 0)  # Empty Range\n    assert Range(-oo, -5).intersect(Range(-10, 0)) == Range(-5, -5)  # Finite Point\n"], "sample_520": ["def test_axis_label_colors(axis):\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    color = 'red'\n    getattr(ax, f'set_{axis}label')('label', color=color)\n    fig.canvas.draw()\n    label = getattr(ax, f'{axis}axis').get_label()\n    assert label.get_color() == color\n", "compilation error", "def test_view_init_axis_limits_ticks_ticklabels(\n    axes, limits_expected, ticks_expected, ticklabels_expected", "compilation error", "def test_view_init_proj(elev, azim, roll):\n    \"\"\"Test that get_proj returns the expected projection matrix.\"\"\"\n    ax = plt.subplot(1, 1, 1, projection='3d')\n    ax.view_init(elev=elev, azim=azim, roll=roll)\n    ax.figure.canvas.draw()\n\n    # Calculate the expected projection matrix\n    rot = proj3d.view_transformation(np.eye(3), np.array([0, 0, 10]), [0, 0, 1])\n    persp = proj3d.persp_transformation(100, 0.01, 0.1)\n    expected_proj = np.dot(persp, rot)\n\n    # Compare the actual projection matrix with the expected one\n    np.testing.assert_allclose(expected_proj, ax.get_proj(), rtol=1e-5)\n"], "sample_105": ["def test_template_view_with_template_name(self):\n    \"\"\"\n    A TemplateView with a template_name attribute should render the correct template.\n    \"\"\"\n    response = AboutTemplateAttributeView.as_view()(self.rf.get('/about/'))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.template_name, ['generic_views/about.html'])\n", "    def test_template_does_not_exist(self):\n        \"\"\"\n        Test a view that raises TemplateDoesNotExist when the template does not exist.\n        \"\"\"\n        response = self.client.get('/template/nonexistent/')\n        self.assertEqual(response.status_code, 500)\n        self.assertIn(\"TemplateDoesNotExist\", str(response.content))\n", "    def test_setup_adds_request_attributes(self):\n        request = self.rf.get('/')\n        args = ('arg1', 'arg2')\n        kwargs = {'kwarg1': 1, 'kwarg2': 'two'}\n\n        view = View()\n        view.setup(request, *args, **kwargs)\n\n        self.assertEqual(view.request, request)\n        self.assertEqual(view.args, args)\n        self.assertEqual(view.kwargs, kwargs)\n", "def test_template_engine_inheritance(self):\n    \"\"\"\n    A template view may inherit the template engine from a parent class.\n    \"\"\"\n    request = self.rf.get('/using/')\n\n    class ParentTemplateView(TemplateView):\n        template_engine = 'jinja2'\n\n    class ChildTemplateView(ParentTemplateView):\n        template_name = 'generic_views/using.html'\n\n    view = ChildTemplateView.as_view()\n    self.assertEqual(view(request).render().content, b'Jinja2\\n')\n", "    def test_get_context_data(self):\n        \"\"\"\n        Test get_context_data() with extra context.\n        \"\"\"\n        extra_context = {'extra_key': 'extra_value'}\n        view = TemplateView(extra_context=extra_context)\n        context = view.get_context_data()\n        self.assertEqual(context['extra_key'], 'extra_value')\n        self.assertIsInstance(context['view'], View)\n"], "sample_988": ["def test_relational_subs():\n    e = Relational(x, y, '==')\n    e = e.subs(x, z)\n    assert isinstance(e, Equality)\n    assert e.lhs == z\n    assert e.rhs == y\n\n    e = Relational(x, y, '>=')\n    e = e.subs(x, z)\n    assert isinstance(e, GreaterThan)\n    assert e.lhs == z\n    assert e.rhs == y\n\n    e = Relational(x, y, '<=')\n    e = e.subs(x, z)\n    assert isinstance(e, LessThan)\n    assert e.lhs == z\n    assert e.rhs == y\n\n    e = Relational(x, y, '>')\n    e = e.subs(x, z)\n    assert isinstance(e, StrictGreaterThan)\n    assert e.lhs == z\n    assert e.rhs == y\n\n    e = Relational(x, y, '<')\n    e = e.subs(x, z)\n    assert isinstance(e, StrictLessThan)\n    assert e.lhs == z\n    assert e.rhs == y\n", "def test_relational_functions():\n    assert Eq(x, y).function == Eq\n    assert Ne(x, y).function == Ne\n    assert Gt(x, y).function == Gt\n    assert Ge(x, y).function == Ge\n    assert Lt(x, y).function == Lt\n    assert Le(x, y).function == Le\n", "def test_relational_conjugate():\n    x = Symbol('x', real=True)\n    y = Symbol('y')\n    assert (x + I*y).conjugate() == x - I*y\n    assert Eq(x + I*y, x - I*y).conjugate() == Eq(x - I*y, x + I*y)\n    assert Ne(x + I*y, x - I*y).conjugate() == Ne(x - I*y, x + I*y)\n", "def test_relational_subs():\n    # Test subs method for relational objects\n    x, y, z = symbols('x y z')\n    eq = Eq(x, y)\n    new_eq = eq.subs(x, z)\n    assert new_eq == Eq(z, y)\n    lt = Lt(x, y)\n    new_lt = lt.subs(x, z)\n    assert new_lt == Lt(z, y)\n", "def test_issue_14269():\n    x = symbols('x', real=True)\n    assert Eq(x, 0) != True\n    assert Eq(x, 0) != False\n    assert Ne(x, 0) != True\n    assert Ne(x, 0) != False\n    assert x < 1 != True\n    assert x < 1 != False\n    assert x > 0 != True\n    assert x > 0 != False\n    assert x >= 0 != True\n    assert x >= 0 != False\n    assert x <= 1 != True\n    assert x <= 1 != False\n"], "sample_1008": ["def test_ang_acc():\n    q1, q2, q3, q4 = dynamicsymbols('q1 q2 q3 q4')\n    q1d, q2d, q3d, q4d = dynamicsymbols('q1 q2 q3 q4', 1)\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    C = B.orientnew('C', 'Axis', [q3, B.y])\n    D = N.orientnew('D', 'Axis', [q4, N.y])\n    u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n    u1d, u2d, u3d = dynamicsymbols('u1 u2 u3', 1)\n\n    C.set_ang_vel(N, u1*C.x + u2*C.y + u3*C.z)\n    C.set_ang_acc(N, u1d*C.x + u2d*C.y + u3d*C.z)\n    assert C.ang_acc_in(N) == u1d*C.x + u2d*C.y + u3d*C.z\n    assert N.ang_acc_in(C) == -u1d*C.x - u2d*C.y - u3d*C.z\n    assert C.ang_acc_in(D) == u1d*C.x + u2d*C.y + u3d*C.z - q4d*D.y\n    assert D.ang_acc_in(C) == -u1d*C.x - u2d*C.y - u3d*C.z + q4d*D.y\n", "def test_issue_11503_inverse_relation():\n    A = ReferenceFrame(\"A\")\n    B = A.orientnew(\"B\", \"Axis\", [35, A.y])\n    C = ReferenceFrame(\"C\")\n    A.orient(C, \"Axis\", [70, C.z])\n\n    # Test the inverse relationship between frames\n    assert B.dcm(A) == A.dcm(B).T\n    assert A.dcm(C) == C.dcm(A).T\n    assert B.dcm(C) == C.dcm(B).T\n", "def test_ang_acc():\n    q1, q2, q3, q4 = dynamicsymbols('q1 q2 q3 q4')\n    q1d, q2d, q3d, q4d = dynamicsymbols('q1 q2 q3 q4', 1)\n    q1dd, q2dd, q3dd, q4dd = dynamicsymbols('q1 q2 q3 q4', 2)\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    C = B.orientnew('C', 'Axis', [q3, B.y])\n    D = N.orientnew('D', 'Axis', [q4, N.y])\n    u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n\n    A.set_ang_acc(N, u1*N.x + u2*N.y + u3*N.z)\n    B.set_ang_acc(N, u1*N.x + u2*N.y + u3*N.z)\n    C.set_ang_acc(N, u1*N.x + u2*N.y + u3*N.z)\n    D.set_ang_acc(N, u1*N.x + u2*N.y + u3*N.z)\n\n    assert A.ang_acc_in(N) == (u1)*N.x + (u2)*N.y + (u3)*N.z - (q1d**2)*A.z\n    assert B.ang_acc_in(N) == (u1)*N.x + (u2)*N.y + (u3)*N.z - (q2d**2)*B.x - (q1d**2)*A.z\n    assert C.ang_acc_in(N) == (u1)*N.x + (u2)*N.y + (u3)*N.z - (q3d**2)*B.", "def test_orientnew_with_custom_variables():\n    N = ReferenceFrame('N', variables=['nx', 'ny', 'nz'])\n    q1 = dynamicsymbols('q1')\n    A = N.orientnew('a', 'Axis', [q1, N.z], variables=['ax', 'ay', 'az'])\n\n    assert N.varlist[0].name == 'nx'\n    assert N.varlist[1].name == 'ny'\n    assert N.varlist[2].name == 'nz'\n\n    assert A.varlist[0].name == 'ax'\n    assert A.varlist[1].name == 'ay'\n    assert A.varlist[2].name == 'az'\n", "def test_issue_11506():\n    A = ReferenceFrame('A')\n    B = A.orientnew('B', 'Axis', [pi/2, A.x])\n    C = B.orientnew('C', 'Axis', [pi/2, B.y])\n    assert C.dcm(A) == Matrix([[0, 0, -1], [0, 1, 0], [1, 0, 0]])\n"], "sample_937": ["def test_unparse_subscript_tuple(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_complex_expression():\n    source = \"(a + b) * (c - d) / e\"\n    expected = \"(a + b) * (c - d) / e\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_py36():\n    source = \"[x for x in range(5) if x % 2 == 0]\"\n    expected = \"[x for x in range(5) if x % 2 == 0]\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_complex_expression(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_subscript_with_slice():\n    source = \"a[1:10]\"\n    expected = \"a[1:10]\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n"], "sample_225": ["    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def test_unregister(self):\n        site = admin.AdminSite()\n        site.register(User)\n        self.assertTrue(site.is_registered(User))\n        site.unregister(User)\n        self.assertFalse(site.is_registered(User))\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.article1 = Article.objects.create(title='Test Article 1', content='Test Content 1')\n"], "sample_896": ["def test_nmf_custom_init_shape_error_update_H_False():\n    # Check that an informative error is raised when custom initialization does not\n    # have the right shape when update_H is False\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    H = rng.random_sample((2, 5))\n    nmf = NMF(n_components=2, init=\"custom\", random_state=0)\n\n    with pytest.raises(ValueError, match=\"Array with wrong second dimension passed\"):\n        nmf.fit(X, H=H, W=rng.random_sample((6, 3)), update_H=False)\n", "def test_nmf_minibatchnmf_equivalence_with_fresh_restarts():\n    # Test that MiniBatchNMF is equivalent to NMF when batch_size = n_samples and\n    # forget_factor 0.0 with fresh_restarts=True\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(48, 5))\n\n    nmf = NMF(\n        n_components=5,\n        beta_loss='frobenius',\n        solver=\"mu\",\n        random_state=0,\n        tol=0,\n    )\n    mbnmf = MiniBatchNMF(\n        n_components=5,\n        beta_loss='frobenius',\n        random_state=0,\n        tol=0,\n        max_no_improvement=None,\n        batch_size=X.shape[0],\n        forget_factor=0.0,\n        fresh_restarts=True,\n    )\n    W = nmf.fit_transform(X)\n    mbW = mbnmf.fit_transform(X)\n    assert_allclose(W, mbW)\n", "def test_nmf_regularization_same_alpha(Estimator):\n    # Test the effect of L1 and L2 regularizations with same alpha for W and H\n    n_samples = 6\n    n_features = 5\n    n_components = 3\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(n_samples, n_features))\n\n    # L1 regularization should increase the number of zeros\n    l1_ratio = 1.0\n    alpha = 0.5\n    regul = Estimator(\n        n_components=n_components,\n        alpha_W=alpha,\n        alpha_H=alpha,\n        l1_ratio=l1_ratio,\n        random_state=42,\n    )\n    model = Estimator(\n        n_components=n_components,\n        alpha_W=0.0,\n        alpha_H=0.0,\n        l1_ratio=l1_ratio,\n        random_state=42,\n    )\n\n    W_regul = regul.fit_transform(X)\n    W_model = model.fit_transform(X)\n\n    H_regul = regul.components_\n    H_model = model.components_\n\n    eps = np.finfo(np.float64).eps\n    W_regul_n_zeros = W_regul[W_regul <= eps].size\n    W_model_n_zeros = W_model[W_model <= eps].size\n    H_regul_n_zeros = H_regul[H_regul <= eps].size\n    H_model_n_zeros = H_model[H_model <= eps].size\n\n    assert W_regul_n_zeros > W_model_n_zeros\n    assert H_regul_n_zeros > H_model_n_zeros\n\n    # L2 regularization should decrease the sum of the squared", "def test_nmf_n_components_error(Estimator, solver):\n    # Check that an error is raised when n_components is not positive\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    nmf = Estimator(n_components=-2, random_state=0, **solver)\n    with pytest.raises(ValueError, match=\"Invalid parameter n_components\"):\n        nmf.fit(X)\n", "def test_nmf_with_n_components_none(Estimator, solver):\n    # Check that the number of components is equal to the number of features\n    # when n_components is set to None.\n    rng = np.random.mtrand.RandomState(42)\n    n_features = 10\n    X = np.abs(rng.randn(6, n_features))\n    est = Estimator(n_components=None, init=\"random\", random_state=0, tol=1e-6, **solver)\n    est.fit(X)\n    assert est.n_components_ == n_features\n"], "sample_328": ["def test_functions_with_aggregate(self):\n    Note.objects.update(note='TEST')\n    for note in self.notes:\n        note.note = Lower(F('note')) + str(Note.objects.count())\n    Note.objects.bulk_update(self.notes, ['note'])\n    self.assertEqual(set(Note.objects.values_list('note', flat=True)), {'test10'})\n", "def test_date_field(self):\n    articles = [\n        Article.objects.create(name=str(i), published_date=datetime.date.today())\n        for i in range(10)\n    ]\n    point_in_time = datetime.date(1991, 10, 31)\n    for article in articles:\n        article.published_date = point_in_time\n    Article.objects.bulk_update(articles, ['published_date'])\n    self.assertCountEqual(Article.objects.filter(published_date=point_in_time), articles)\n", "def test_char_field_max_length(self):\n    notes = [\n        Note.objects.create(note=str(i) * 200, misc=str(i))\n        for i in range(10)\n    ]\n    for note in notes:\n        note.note = 'test-%s' % note.id\n    Note.objects.bulk_update(notes, ['note'])\n    self.assertCountEqual(\n        Note.objects.values_list('note', flat=True),\n        [cat.note for cat in notes]\n    )\n", "def test_functions_with_mixed_fields(self):\n    Note.objects.update(note='TEST', misc='INIT')\n    midpoint = len(self.notes) // 2\n    top, bottom = self.notes[:midpoint], self.notes[midpoint:]\n    for note in top:\n        note.note = Lower('note')\n    for note in bottom:\n        note.misc = Lower('misc') + '-updated'\n    Note.objects.bulk_update(self.notes, ['note', 'misc'])\n    self.assertCountEqual(Note.objects.filter(note='test'), top)\n    self.assertCountEqual(Note.objects.filter(misc__endswith='-updated'), bottom)\n", "def test_foreign_keys_do_not_lookup_with_different_databases(self):\n    self.create_tags()\n    # Connect to a different database for the notes\n    using = 'other'\n    for note, tag in zip(self.notes, self.tags):\n        note.tag = tag\n    # The foreign keys do not lookup in this case because they are in a different database\n    with self.assertRaisesMessage(ValueError, \"Cannot add or update a child row: a foreign key constraint fails\"):\n        Note.objects.using(using).bulk_update(self.notes, ['tag'])\n    # Verify that no changes were made to the notes\n    self.assertEqual(Note.objects.using(using).count(), 0)\n"], "sample_101": ["    def test_post_data(self):\n        \"\"\"\n        WSGIRequest correctly handles POST data.\n        \"\"\"\n        application = get_wsgi_application()\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/post/\",\n            CONTENT_TYPE=\"application/x-www-form-urlencoded\",\n            REQUEST_METHOD=\"POST\",\n            wsgi.input=BytesIO(b\"key1=value1&key2=value2\"),\n            CONTENT_LENGTH=len(b\"key1=value1&key2=value2\")\n        )\n        response_data = {}\n\n            response_data[\"status\"] = status\n            response_data[\"headers\"] = headers\n\n        response = application(environ, start_response)\n        self.assertEqual(response_data[\"status\"], \"200 OK\")\n        request = response.context['request']\n        self.assertEqual(request.POST['key1'], 'value1')\n        self.assertEqual(request.POST['key2'], 'value2')\n", "def test_limited_stream_readline(self):\n    \"\"\"\n    LimitedStream's readline method behaves as expected.\n    \"\"\"\n    data = b'Hello\\nWorld!'\n    stream = LimitedStream(BytesIO(data), len(data))\n\n    # Test reading a line with size less than the data\n    line = stream.readline(10)\n    self.assertEqual(line, b'Hello\\n')\n\n    # Test reading a line with size greater than the data\n    line = stream.readline(100)\n    self.assertEqual(line, b'World!')\n\n    # Test reading a line when there's no more data\n    line = stream.readline()\n    self.assertEqual(line, b'')\n", "    def test_post_request(self):\n        \"\"\"\n        POST request with data is handled correctly.\n        \"\"\"\n        application = get_wsgi_application()\n        environ = self.request_factory._base_environ(\n            PATH_INFO='/post/',\n            REQUEST_METHOD='POST',\n            CONTENT_TYPE='application/x-www-form-urlencoded',\n            wsgi.input=BytesIO(b'key=value')\n        )\n        environ['CONTENT_LENGTH'] = '9'\n        response_data = {}\n\n            response_data['status'] = status\n            response_data['headers'] = headers\n\n        response = application(environ, start_response)\n        self.assertEqual(response_data['status'], '200 OK')\n        self.assertEqual(response.read(), b'key: value')\n", "def test_limited_stream(self):\n    \"\"\"\n    LimitedStream correctly limits the amount of data that can be read.\n    \"\"\"\n    stream = BytesIO(b'This is a test stream.')\n    limited_stream = LimitedStream(stream, 8)\n    data = limited_stream.read()\n    self.assertEqual(data, b'This is a')\n    data = limited_stream.read()\n    self.assertEqual(data, b' test')\n    data = limited_stream.read()\n    self.assertEqual(data, b'')\n", "def test_limited_stream_readline(self):\n    \"\"\"\n    LimitedStream correctly reads lines with readline method.\n    \"\"\"\n    data = b'Hello\\nWorld\\nThis is a test.'\n    stream = BytesIO(data)\n    limited_stream = LimitedStream(stream, len(data))\n\n    self.assertEqual(limited_stream.readline(), b'Hello\\n')\n    self.assertEqual(limited_stream.readline(), b'World\\n')\n    self.assertEqual(limited_stream.readline(), b'This is a test.')\n    self.assertEqual(limited_stream.readline(), b'')\n"], "sample_982": ["def test_udivisor_sigma_symbols():\n    m = Symbol('m', integer=True, positive=True)\n    k = Symbol('k', integer=True, positive=True)\n    assert udivisor_sigma(m, k) == Mul(*[1 + p**(k*e) for p, e in factorint(m).items()])\n    assert udivisor_sigma(m) == Mul(*[1 + p**e for p, e in factorint(m).items()])\n", "def test_udivisor_sigma_error_handling():\n    # Test error handling for udivisor_sigma\n    assert raises(ValueError, lambda: udivisor_sigma(-1))\n    assert raises(ValueError, lambda: udivisor_sigma(0))\n    assert raises(ValueError, lambda: udivisor_sigma(3, -2))\n    assert raises(ValueError, lambda: udivisor_sigma(2.5))\n    assert raises(ValueError, lambda: udivisor_sigma(2, 2.5))\n", "def test_issue_14115():\n    assert primeomega(2 * 3 * 3) == 4\n    assert primeomega(2 * 2 * 3 * 3 * 3) == 7\n", "def test_primeomega_large_number():\n    assert primeomega(10**1000 + 7) == 2\n", "def test_smoothness_p_visual():\n    n = 21477639576571\n    visual = smoothness_p(n, visual=1)\n    assert visual == 'p**i=4410317**1 has p-1 B=1787, B-pow=1787\\n' + \\\n                    'p**i=4869863**1 has p-1 B=2434931, B-pow=2434931'\n    # Test converting visual to string and back\n    assert smoothness_p(visual) == {4410317: 1, 4869863: 1}\n    # Test converting string to visual\n    assert smoothness_p(str(smoothness_p(n))) == visual\n"], "sample_621": ["def test_copy(self, indexes) -> None:\n    copied = indexes.copy()\n    assert copied is not indexes\n    assert copied._indexes is not indexes._indexes\n    assert copied._variables is not indexes._variables\n    assert copied._indexes == indexes._indexes\n    assert copied._variables == indexes._variables\n", "def test_copy(self, indexes):\n    copied = indexes.copy()\n    assert copied is not indexes\n    assert copied._indexes is not indexes._indexes\n    assert copied._variables is not indexes._variables\n    assert copied._indexes == indexes._indexes\n    assert copied._variables == indexes._variables\n", "def test_copy(self, indexes) -> None:\n    copied = indexes.copy()\n    assert copied is not indexes\n    assert copied._indexes is not indexes._indexes\n    assert copied._variables is not indexes._variables\n    assert copied._indexes == indexes._indexes\n    assert copied._variables == indexes._variables\n", "def test_copy(self, indexes) -> None:\n    copied = indexes.copy()\n\n    assert copied._indexes == indexes._indexes\n    assert copied._variables == indexes._variables\n    assert copied._indexes is not indexes._indexes\n    assert copied._variables is not indexes._variables\n", "def test_to_pandas_indexes_error(self) -> None:\n    indexes: dict[Any, Index] = {\"x\": CustomIndex({\"x\": 2})}\n    variables: dict[Any, Variable] = {}\n    indexes_obj = Indexes(indexes, variables)\n\n    with pytest.raises(TypeError):\n        indexes_obj.to_pandas_indexes()\n"], "sample_476": ["    def test_update_dimensions_from_file(self):\n        p = self.PersonModel(name=\"Joe\")\n        p.mugshot = self.file1\n        p.save()\n\n        p.mugshot.update_dimension_fields(p, force=True)\n        self.check_dimensions(p, 4, 8)\n\n        p.mugshot = self.file2\n        p.mugshot.update_dimension_fields(p, force=True)\n        self.check_dimensions(p, 8, 4)\n", "    def test_deconstruct(self):\n        field = ImageField(upload_to=\"images/\", width_field=\"width\", height_field=\"height\")\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(name, \"ImageField\")\n        self.assertEqual(path, \"django.db.models.fields.files\")\n        self.assertEqual(args, [])\n        self.assertEqual(kwargs, {\"upload_to\": \"images/\", \"width_field\": \"width\", \"height_field\": \"height\"})\n", "def test_image_after_assignment(self):\n    \"\"\"\n    Tests behavior when image is assigned after instance creation.\n    \"\"\"\n    p = self.PersonModel(name=\"Joe\")\n    self.assertIsInstance(p.mugshot, TestImageFieldFile)\n    self.assertFalse(p.mugshot)\n\n    # Test assigning a new image to an existing instance.\n    p.mugshot = self.file2\n    self.check_dimensions(p, 8, 4)\n\n    # Test assigning None to an existing instance.\n    p.mugshot = None\n    self.check_dimensions(p, None, None)\n", "    def test_post_init_updates_dimensions(self):\n        p = self.PersonModel(name=\"Joe\")\n        p.mugshot = self.file1\n        self.check_dimensions(p, None, None)\n        signals.post_init.send(sender=self.PersonModel, instance=p)\n        self.check_dimensions(p, 4, 8)\n", "    def test_descriptor_set(self):\n        p = self.PersonModel(name=\"Joe\")\n\n        # Test setting a File instance.\n        p.mugshot = File(self.file1)\n        self.check_dimensions(p, 4, 8)\n\n        # Test setting a string value.\n        p.mugshot = \"new_mugshot.png\"\n        self.assertIsInstance(p.mugshot, TestImageFieldFile)\n        self.assertEqual(p.mugshot.name, \"new_mugshot.png\")\n\n        # Test setting a FieldFile instance.\n        ff = TestImageFieldFile(p, Person._meta.get_field(\"mugshot\"), \"another_mugshot.png\")\n        p.mugshot = ff\n        self.assertIs(p.mugshot, ff)\n"], "sample_215": ["    def test_multivalue_dict_key_error(self):\n        \"\"\"\n        #21098 -- Sensitive POST parameters cannot be seen in the\n        error reports for if request.POST['nonexistent_key'] throws an error.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(multivalue_dict_key_error, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(multivalue_dict_key_error, check_for_vars=False)\n", "    def test_cleansing_special_types(self):\n        request = RequestFactory().post('/', data={'secret': 'password'})\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_special_types(request, request.POST), {'secret': '********************'})\n", "    def test_unicode_request(self):\n        \"\"\"\n        Unicode characters can be seen in the error reports for requests.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unicode_response(non_sensitive_view)\n            self.verify_unicode_email(non_sensitive_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_unicode_response(non_sensitive_view)\n            self.verify_unicode_email(non_sensitive_view)\n", "    def test_list_setting_with_sensitive_item(self):\n        \"\"\"\n        The debug page should filter out some sensitive information found in\n        list settings.\n        \"\"\"\n        sensitive_settings = [\n            'SECRET_KEY',\n            'PASSWORD',\n            'API_KEY',\n            'AUTH_TOKEN',\n        ]\n        for setting in sensitive_settings:\n            FOOBAR = [\n                setting,\n                {'recursive': [setting]},\n            ]\n            with self.settings(DEBUG=True, FOOBAR=FOOBAR):\n                response = self.client.get('/raises500/')\n                self.assertNotContains(response, setting, status_code=500)\n", "    def test_multivaluedict(self):\n        \"\"\"\n        Sensitive POST parameters cannot be seen in the error reports when\n        they're in a MultiValueDict.\n        \"\"\"\n        request = self.rf.post('/some_url/', self.breakfast_data)\n        request.POST = request.POST.copy()\n        request.POST['sausage-key'] = 'sensitive-sausage-value'\n        request.sensitive_post_parameters = ['sausage-key']\n\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(non_sensitive_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(non_sensitive_view)\n"], "sample_708": ["def test_source_from_lambda():\n    f = lambda x: x * 2\n    source = Source(f)\n    assert str(source).startswith(\"f = lambda x: x * 2\")\n", "def test_getstatementrange_with_syntaxerror():\n    source = Source(\":\")\n    with pytest.raises(SyntaxError):\n        source.getstatementrange(0)\n", "def test_source_of_decorated_class_with_args():\n    @decorator('arg')\n    class A:\n            x = 1\n\n    source = Source(A)\n    expected = '''@decorator('arg')", "def test_getstatementrange_for_loop():\n    source = Source(\n        \"\"\"\\\n        for i in range(5):\n            x = i + 2\n            y = i * 3\n        z = 10\n        \"\"\"\n    )\n    assert source.getstatementrange(1) == (1, 3)\n    assert source.getstatementrange(2) == (1, 3)\n    assert source.getstatementrange(3) == (3, 4)\n", "def test_source_from_generator():\n        yield 1\n        yield 2\n\n    source = Source(gen_func)\n    assert str(source).startswith(\"def gen_func():\")\n"], "sample_134": ["def test_serialize_model(self):\n    self.assertSerializedEqual(models.Model)\n", "def test_serialize_custom_deconstructible_instance(self):\n    instance = DeconstructibleInstances()\n    string, imports = MigrationWriter.serialize(instance)\n    self.assertEqual(string, \"migrations.test_writer.DeconstructibleInstances()\")\n    self.assertEqual(imports, {\"import migrations.test_writer\"})\n", "def test_serialize_nested_operations(self):\n    \"\"\"Test serialization of nested operations.\"\"\"\n    outer_operation = custom_migration_operations.operations.ExpandArgsOperation(\n        arg=[\n            custom_migration_operations.operations.KwargsOperation(\n                kwarg1=1,\n                kwarg2=custom_migration_operations.operations.TestOperation(),\n            ),\n        ]\n    )\n    buff, imports = OperationWriter(outer_operation, indentation=0).serialize()\n    self.assertEqual(imports, {'import custom_migration_operations.operations'})\n    self.assertEqual(\n        buff,\n        'custom_migration_operations.operations.ExpandArgsOperation(\\n'\n        '    arg=[\\n'\n        '        custom_migration_operations.operations.KwargsOperation(\\n'\n        '            kwarg1=1,\\n'\n        '            kwarg2=custom_migration_operations.operations.TestOperation(\\n'\n        '            ),\\n'\n        '        ),\\n'\n        '    ],\\n'\n        '),'\n    )\n", "def test_serialize_model_base(self):\n    self.assertSerializedEqual(models.Model)\n    self.assertSerializedResultEqual(\n        models.Model,\n        (\"models.Model\", {\"from django.db import models\"})\n    )\n", "def test_serialize_model_options(self):\n    options = models.IndexOptions(db_tablespace='my_tablespace')\n    string = MigrationWriter.serialize(options)[0]\n    self.assertEqual(string, \"models.IndexOptions(db_tablespace='my_tablespace')\")\n    self.serialize_round_trip(options)\n"], "sample_249": ["    def test_serialize_db_to_string_exclude_app(self):\n        # serialize_db_to_string() excludes models from TEST_NON_SERIALIZED_APPS\n        # setting.\n        with self.settings(TEST_NON_SERIALIZED_APPS=['backends']):\n            SchoolClass.objects.create(year=2000, last_updated=datetime.datetime.now())\n            with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n                # serialize_db_to_string() serializes only migrated apps, so mark\n                # the backends app as migrated.\n                loader_instance = loader.return_value\n                loader_instance.migrated_apps = {'backends'}\n                data = connection.creation.serialize_db_to_string()\n            self.assertNotIn('\"model\": \"backends.schoolclass\"', data)\n            self.assertNotIn('\"year\": 2000', data)\n", "    def test_destroy_test_db(self, *mocked_objects):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        test_database_name = 'test_db_name'\n        # Mock the _destroy_test_db method to avoid actual database operations\n        with mock.patch.object(creation, '_destroy_test_db') as mocked_destroy:\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True, suffix='1')\n            # Check if the _destroy_test_db method was not called when keepdb is True\n            mocked_destroy.assert_not_called()\n            # Check if the settings_dict['NAME'] is restored to the old database name\n            self.assertEqual(test_connection.settings_dict['NAME'], old_database_name)\n            self.assertEqual(settings.DATABASES[DEFAULT_DB_ALIAS]['NAME'], old_database_name)\n\n        with mock.patch.object(creation, '_destroy_test_db') as mocked_destroy:\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=False, suffix='1')\n            # Check if the _destroy_test_db method was called when keepdb is False\n            mocked_destroy.assert_called_with(creation.get_test_db_clone_settings('1')['NAME'], 0)\n            # Check if the settings_dict['NAME'] is restored to the old database name\n            self.assertEqual(test_connection.settings_dict['NAME'], old_database_name)\n            self.assertEqual(settings.DATABASES[DEFAULT_DB_ALIAS]['NAME'], old_database_name)\n", "    def test_serialize_db_to_string_exclude_apps(self):\n        # serialize_db_to_string() excludes apps in settings.TEST_NON_SERIALIZED_APPS\n        SchoolClass.objects.create(year=1000, last_updated=datetime.datetime.now())\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader, \\\n             override_settings(TEST_NON_SERIALIZED_APPS=['backends']):\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        self.assertNotIn('\"model\": \"backends.schoolclass\"', data)\n        self.assertNotIn('\"year\": 1000', data)\n", "    def test_test_db_signature(self):\n        # test_db_signature() returns a tuple with elements that uniquely identify a database\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['HOST'] = 'localhost'\n        test_connection.settings_dict['PORT'] = '5432'\n        test_connection.settings_dict['ENGINE'] = 'django.db.backends.postgresql'\n        test_connection.settings_dict['NAME'] = 'test_db'\n        signature = BaseDatabaseCreation(test_connection).test_db_signature()\n        self.assertEqual(signature, ('localhost', '5432', 'django.db.backends.postgresql', 'test_db'))\n", "    def test_clone_test_db(self, mock_check_constraints):\n        # clone_test_db() creates a clone of the test database.\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_clone_test_db'):\n            creation.clone_test_db(suffix='2', verbosity=0, autoclobber=True, keepdb=False)\n        # _clone_test_db() is called with the correct parameters.\n        creation._clone_test_db.assert_called_once_with('2', 0, False)\n        # check_constraints() is called once.\n        mock_check_constraints.assert_called_once()\n"], "sample_1192": ["def test_symbol_assumptions():\n    x = Symbol('x', commutative=False)\n    assert x.is_commutative is False\n\n    y = Symbol('y', complex=True)\n    assert y.is_complex is True\n\n    z = Symbol('z', real=True)\n    assert z.is_real is True\n    assert z.is_complex is False\n\n    w = Symbol('w', integer=True)\n    assert w.is_integer is True\n    assert w.is_real is True\n    assert w.is_complex is False\n\n    raises(ValueError, lambda: Symbol('a', commutative=None))\n", "def test_symbol_assumptions():\n    x = Symbol('x', commutative=True)\n    y = Symbol('y', commutative=False)\n    assert x.is_commutative\n    assert not y.is_commutative\n\n    raises(ValueError, lambda: Symbol('z', commutative=None))\n\n    x_real = Symbol('x', real=True)\n    y_real = Symbol('y', real=False)\n    assert x_real.is_real\n    assert not y_real.is_real\n\n    raises(ValueError, lambda: Symbol('z', real=None))\n", "def test_symbols_commutative():\n    x = Symbol('x', commutative=False)\n    assert x.is_commutative is False\n    assert symbols('x', commutative=False).is_commutative is False\n    assert symbols('x', commutative=True).is_commutative is True\n    assert symbols('x').is_commutative is True  # default\n\n    raises(ValueError, lambda: symbols('x', commutative=None))\n", "def test_uniquely_named_symbol_with_commutativity():\n    x = Symbol('x')\n    y = Symbol('y', commutative=False)\n    assert F('x', x).is_commutative is True\n    assert F('y', y).is_commutative is False\n    assert F('x', x, commutative=False).is_commutative is False\n    assert F('y', y, commutative=True).is_commutative is True\n    assert F('z', commutative=False).is_commutative is False\n", "def test_symbol_assumptions():\n    x = Symbol('x', real=True)\n    assert x.is_real\n\n    y = Symbol('y', integer=True)\n    assert y.is_integer\n\n    z = Symbol('z', commutative=False)\n    assert not z.is_commutative\n\n    raises(ValueError, lambda: Symbol('w', commutative=None))\n\n    a = Symbol('a')\n    assert a.is_commutative\n"], "sample_1112": ["def test_digits_edge_cases():\n    assert digits(0) == [10, 0]\n    assert digits(0, 2) == [2, 0]\n    assert digits(0, 16) == [16, 0]\n    assert digits(1) == [10, 1]\n    assert digits(-1) == [-10, 1]\n    assert digits(1, 2) == [2, 1]\n    assert digits(-1, 2) == [-2, 1]\n    raises(ValueError, lambda: digits(1, 1))\n    raises(ValueError, lambda: digits(-1, 1))\n", "def test_digits_large_number():\n    assert digits(123456789123456789, 10) == [10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert digits(123456789123456789, 10, 20) == [10, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n", "def test_digits_error():\n    raises(ValueError, lambda: digits(10, 1))\n    raises(TypeError, lambda: digits(\"10\", 10))\n", "def test_digits_edge_cases():\n    assert digits(0) == [10, 0]\n    assert digits(-0) == [10, 0]\n    raises(ValueError, lambda: digits(0, 1))\n    raises(ValueError, lambda: digits(0, -2))\n    raises(ValueError, lambda: digits(123, 0))\n    raises(ValueError, lambda: digits(123, -10))\n    raises(ValueError, lambda: digits(123, 'a'))\n", "def test_count_digits_base16():\n    assert count_digits(0xFA, 16) == {15: 1, 10: 1}\n    assert count_digits(0x1A2B, 16) == {1: 1, 10: 2, 11: 1}\n"], "sample_1000": ["def test_octave_user_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == 'existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_octave_matrix_slicing():\n    A = MatrixSymbol('A', 3, 3)\n    assert mcode(A[0:2, 0:2]) == \"A(1:2, 1:2)\"\n    assert mcode(A[1:3, 1:3]) == \"A(2:end, 2:end)\"\n    assert mcode(A[0:3:2, 0:3:2]) == \"A(1:2:end, 1:2:end)\"\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 4)\n\n    assert mcode(A[0:2, 0:3]) == \"A(1:2, 1:3)\"\n    assert mcode(A[1:3, :]) == \"A(2:3, :)\"\n    assert mcode(A[:, 1:]) == \"A(:, 2:end)\"\n    assert mcode(A[::2, ::2]) == \"A(:, :2)\"\n", "def test_Piecewise_Matrix_and_vector():\n    pw = Piecewise((x, x < 1), (x**2, True))\n    A = Matrix([[pw, pw], [pw, pw]])\n    v = Matrix([[pw], [pw]])\n    assert mcode(A, assign_to='A') == \"A = [((x < 1).*(x) + (~(x < 1)).*(x.^2)) ((x < 1).*(x) + (~(x < 1)).*(x.^2)); ((x < 1).*(x) + (~(x < 1)).*(x.^2)) ((x < 1).*(x) + (~(x < 1)).*(x.^2))];\"\n    assert mcode(v, assign_to='v') == \"v = [((x < 1).*(x) + (~(x < 1)).*(x.^2)); ((x < 1).*(x) + (~(x < 1)).*(x.^2))];\"\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 4, 5)\n    B = MatrixSymbol(\"B\", 4, 5)\n    C = MatrixSymbol(\"C\", 4, 5)\n\n    assert mcode(A[1:3, 2:4]) == \"A(2:3, 3:4)\"\n    assert mcode(B[:, :]) == \"B\"\n    assert mcode(C[0:4:2, ::2]) == \"C(1:2:end, 1:2:end)\"\n    assert mcode(A[0, 1:4]) == \"A(1, 2:4)\"\n    assert mcode(B[1:4, 0]) == \"B(2:end, 1)\"\n"], "sample_1001": ["def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\detokenize {radian}\"\n    expr3 = sin(x*radian + pi*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\detokenize {radian} + \\pi \\detokenize {radian} \\right )}'\n", "def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi \\operatorname{rad}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\operatorname{rad}\"\n    expr3 = sin(x*radian + pi*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\operatorname{rad} + \\pi \\operatorname{rad} \\right )}'\n", "def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi  \\operatorname{rad}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x  \\operatorname{rad}\"\n    expr3 = sin(x*radian + pi*radian)\n    assert latex(expr3) == r'\\sin{\\left (x  \\operatorname{rad} + \\pi  \\operatorname{rad} \\right )}'\n", "def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x\"\n    expr3 = sin(x*radian + pi*radian)\n    assert latex(expr3) == r'\\sin{\\left (x + \\pi \\right )}'\n", "def test_latex_radian():\n    expr1 = 90*radian\n    assert latex(expr1) == r\"90 \\operatorname{rad}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\operatorname{rad}\"\n    expr3 = cos(x*radian + 90*radian)\n    assert latex(expr3) == r'\\cos{\\left (x \\operatorname{rad} + 90 \\operatorname{rad} \\right )}'\n"], "sample_870": ["def test_sample_y_shapes_with_prior(n_targets):\n    \"\"\"Check the shapes of y_samples in single-output (n_targets=0) and\n    multi-output settings, including the edge case when n_targets=1, where the\n    sklearn convention is to squeeze the predictions, for the prior distribution.\n    \"\"\"\n    rng = np.random.RandomState(1234)\n\n    n_features, n_samples_X_test = 6, 7\n    n_samples_y_test = 5\n\n    if n_targets is not None and n_targets > 1:\n        y_test_shape = (n_samples_X_test, n_targets, n_samples_y_test)\n    else:\n        y_test_shape = (n_samples_X_test, n_samples_y_test)\n\n    X_test = rng.randn(n_samples_X_test, n_features)\n\n    model = GaussianProcessRegressor(n_targets=n_targets)\n\n    y_samples = model.sample_y(X_test, n_samples=n_samples_y_test)\n    assert y_samples.shape == y_test_shape\n", "def test_alpha_scalar_or_array():\n    \"\"\"\n    Test that alpha can be a scalar or an array with the same number of entries as y.\n    \"\"\"\n    # Test with alpha as scalar\n    gpr_scalar = GaussianProcessRegressor(alpha=1e-10).fit(X, y)\n    y_pred_scalar, _ = gpr_scalar.predict(X2, return_std=True)\n\n    # Test with alpha as array\n    alpha_array = np.full(X.shape[0], 1e-10)\n    gpr_array = GaussianProcessRegressor(alpha=alpha_array).fit(X, y)\n    y_pred_array, _ = gpr_array.predict(X2, return_std=True)\n\n    # Check that predictions are the same for both cases\n    assert_allclose(y_pred_scalar, y_pred_array)\n", "def test_alpha_shape():\n    \"\"\"\n    Check that the alpha parameter can be a scalar and that it is broadcasted\n    correctly.\n    \"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n    alpha = 0.1\n    model = GaussianProcessRegressor(alpha=alpha)\n    model.fit(X, y)\n    assert_allclose(model.alpha_, np.full_like(y, alpha))\n\n    alpha = rng.rand(10)\n    model = GaussianProcessRegressor(alpha=alpha)\n    model.fit(X, y)\n    assert_allclose(model.alpha_, alpha)\n", "def test_gpr_diag_not_modified():\n    \"\"\"\n    Check that the diag method of the kernel does not modify the input matrix X.\n    \"\"\"\n    gpr = GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y)\n\n    X_copy = np.copy(X)\n    _, _ = gpr.predict(X, return_std=True)\n\n    assert_allclose(X, X_copy)\n", "def test_negative_variance_warning():\n    \"\"\"\n    Check that a warning is raised when the variance is negative.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/20248\n    \"\"\"\n    # Create a kernel with a negative variance\n    kernel = RBF(length_scale=-1.0)\n\n    gpr = GaussianProcessRegressor(kernel=kernel)\n    gpr.fit(X, y)\n\n    warning_message = \"Predicted variances smaller than 0. Setting those variances to 0.\"\n    with pytest.warns(UserWarning, match=warning_message):\n        _, y_std = gpr.predict(X2, return_std=True)\n        assert_array_less(y_std, 1e-10)\n"], "sample_1186": ["def test_array_addition():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2], [3, 4]])\n        B = ArrayType([[5, 6], [7, 8]])\n        C = A + B\n        assert C == ArrayType([[6, 8], [10, 12]])\n", "def test_array_addition():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2], [3, 4]])\n        B = ArrayType([[5, 6], [7, 8]])\n        assert A + B == ArrayType([[6, 8], [10, 12]])\n", "def test_array_slicing_assignment():\n    for ArrayType in mutable_array_types:\n        test_array = ArrayType([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n        test_array[:, 1:3] = ArrayType([[11, 12], [13, 14]])\n        assert test_array == ArrayType([[1, 11, 12, 4, 5], [6, 13, 14, 9, 10]])\n\n        test_array[1, 1:3] = ArrayType([15, 16])\n        assert test_array == ArrayType([[1, 11, 12, 4, 5], [6, 15, 16, 9, 10]])\n\n        raises(ValueError, lambda: test_array[:, -6])\n", "def test_array_derivative():\n    from sympy import diff\n    for ArrayType in array_types:\n        a = ArrayType([x**2, x*y, y**2], (3,))\n        assert diff(a, x) == ArrayType([2*x, y, 0], (3,))\n        assert diff(a, y) == ArrayType([0, x, 2*y], (3,))\n", "def test_array_arithmetic_operations():\n    for ArrayType in array_types:\n        a = ArrayType([1, 2, 3, 4], (2, 2))\n        b = ArrayType([4, 4, 4, 4], (2, 2))\n\n        assert a + b == ArrayType([5, 6, 7, 8], (2, 2))\n        assert a - b == ArrayType([-3, -2, -1, 0], (2, 2))\n        assert a * 2 == ArrayType([2, 4, 6, 8], (2, 2))\n        assert 2 * a == ArrayType([2, 4, 6, 8], (2, 2))\n        assert a / 2 == ArrayType([0.5, 1, 1.5, 2], (2, 2))\n\n        assert -a == ArrayType([-1, -2, -3, -4], (2, 2))\n"], "sample_366": ["    def test_parse_datetime_with_invalid_timezone(self):\n        # Invalid timezone\n        self.assertIsNone(parse_datetime('2012-04-23T10:20:30+25:00'))\n        self.assertIsNone(parse_datetime('2012-04-23T10:20:30+2:60'))\n", "def test_invalid_inputs(self):\n    # Invalid inputs\n    self.assertIsNone(parse_duration(''))\n    self.assertIsNone(parse_duration(' '))\n    self.assertIsNone(parse_duration('invalid'))\n    self.assertIsNone(parse_duration('15:'))\n    self.assertIsNone(parse_duration(':30'))\n    self.assertIsNone(parse_duration('10:15:'))\n    self.assertIsNone(parse_duration('10:15:30.'))\n    self.assertIsNone(parse_duration('10:15:30,'))\n", "    def test_parse_datetime_invalid_tz(self):\n        # Invalid time zone offsets\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T10:20:30+25:00')\n        with self.assertRaises(ValueError):\n            parse_datetime('2012-04-23T10:20:30-25:00')\n", "    def test_parse_duration_invalid_inputs(self):\n        # Invalid inputs\n        self.assertIsNone(parse_duration('-P4D-T1H'))  # Invalid ISO 8601 format\n        self.assertIsNone(parse_duration('5 days 1 hour'))  # Unsupported format\n", "    def test_invalid_inputs(self):\n        invalid_inputs = (\n            '2012-04-23T09:15:90',  # Invalid seconds\n            '2012-04-23T09:90:00',  # Invalid minutes\n            '2012-04-23T90:00:00',  # Invalid hours\n            '2012-04-56T09:15:00',  # Invalid day\n            '2012-13-23T09:15:00',  # Invalid month\n            '2012-04-23 09:15:00+25:00',  # Invalid timezone offset\n            '2012-04-23T09:15:00+',  # Invalid timezone offset\n            '2012-04-23T09:15:00+2500',  # Invalid timezone offset\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_datetime(source))\n"], "sample_251": ["def test_alias_filter_with_double_f(self):\n    qs = Book.objects.alias(\n        other_rating=F('rating'),\n    ).filter(other_rating=F('other_rating'))\n    self.assertIs(hasattr(qs.first(), 'other_rating'), False)\n    self.assertEqual(qs.count(), Book.objects.count())\n", "def test_alias_in_f_grouped_by_alias(self):\n    qs = Publisher.objects.alias(multiplier=Value(3)).values('name').annotate(\n        multiplied_value_sum=Sum(F('multiplier') * F('num_awards'))\n    ).order_by()\n    self.assertCountEqual(\n        qs,\n        [\n            {'multiplied_value_sum': 9, 'name': 'Apress'},\n            {'multiplied_value_sum': 0, 'name': \"Jonno's House of Books\"},\n            {'multiplied_value_sum': 27, 'name': 'Morgan Kaufmann'},\n            {'multiplied_value_sum': 21, 'name': 'Prentice Hall'},\n            {'multiplied_value_sum': 3, 'name': 'Sams'},\n        ]\n    )\n", "def test_annotation_with_reverse_m2m(self):\n    authors = Author.objects.annotate(\n        book_name=F('book_set__name'),\n    ).filter(\n        name='Adrian Holovaty',\n    ).order_by('book_name')\n\n    self.assertQuerysetEqual(\n        authors, [\n            'The Definitive Guide to Django: Web Development Done Right',\n        ],\n        lambda a: a.book_name\n    )\n", "def test_annotation_m2o_after_m2m(self):\n    qs = Author.objects.filter(\n        book_contact_set__publisher=OuterRef('pk'),\n    ).annotate(\n        max_pages=Max(F('book_contact_set__pages')),\n    ).values('max_pages')\n    qs = Publisher.objects.annotate(\n        max_pages_contact=Subquery(qs, output_field=IntegerField()),\n    )\n    self.assertCountEqual(qs, [\n        {'max_pages_contact': 447},  # Apress\n        {'max_pages_contact': 0},  # Jonno's House of Books\n        {'max_pages_contact': 946},  # Morgan Kaufmann\n        {'max_pages_contact': 300},  # Prentice Hall\n        {'max_pages_contact': 528},  # Sams\n    ])\n", "def test_filter_alias_with_m2m(self):\n    qs = Author.objects.alias(\n        friend_age=F('friends__age'),\n    ).filter(friend_age=35)\n    self.assertIs(hasattr(qs.first(), 'friend_age'), False)\n    self.assertSequenceEqual(qs, [self.a1, self.a2, self.a4])\n"], "sample_737": ["def test_hashingvectorizer_setters():\n    hv = HashingVectorizer()\n    hv.n_features = 100\n    assert_equal(hv.n_features, 100)\n    hv.alternate_sign = False\n    assert_false(hv.alternate_sign)\n    hv.non_negative = True\n    assert_true(hv.non_negative)\n    hv.dtype = np.float32\n    assert_equal(hv.dtype, np.float32)\n", "def test_tfidfvectorizer_invalid_max_df():\n    # Test that TfidfVectorizer raises an error with invalid max_df values\n    invalid_values = [-1, 1.2, 2]\n    for val in invalid_values:\n        with pytest.raises(ValueError):\n            TfidfVectorizer(max_df=val)\n", "def test_vectorizer_max_features_with_stop_words():\n    expected_vocabulary = set(['burger', 'beer', 'pizza'])\n    expected_stop_words = set([u'celeri', u'tomato', u'copyright', u'coke', u'sparkling', u'water', u'salad'])\n\n    vec_factories = (\n        CountVectorizer,\n        TfidfVectorizer,\n    )\n\n    for vec_factory in vec_factories:\n        vectorizer = vec_factory(max_df=0.6, max_features=3, stop_words=['salad'])\n        vectorizer.fit(ALL_FOOD_DOCS)\n        assert_equal(set(vectorizer.vocabulary_), expected_vocabulary)\n        assert_equal(vectorizer.stop_words_, expected_stop_words)\n", "def test_vectorizer_ngram_range():\n    # Test CountVectorizer with ngram_range parameter\n    vec = CountVectorizer(ngram_range=(1, 2))\n    data = [\"the quick brown fox\", \"the lazy dog\"]\n    X = vec.fit_transform(data)\n    assert_array_equal(X.toarray(), [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1]])\n    assert_equal(vec.get_feature_names(), ['brown', 'dog', 'fox', 'lazy', 'quick', 'the', 'the brown', 'the lazy'])\n\n    # Test TfidfVectorizer with ngram_range parameter\n    vec = TfidfVectorizer(ngram_range=(1, 2))\n    X = vec.fit_transform(data)\n    assert_array_almost_equal(X.toarray(), [[0.45771712, 0.45771712, 0.45771712, 0.57735027, 0.45771712, 0.45771712, 0.45771712, 0.45771712], [0.45771712, 0.45771712, 0.45771712, 0.83666003, 0, 0.45771712, 0.45771712, 0.45771712]])\n    assert_equal(vec.get_feature_names(), ['brown', 'dog', 'fox', 'lazy', 'quick', 'the', 'the brown', 'the lazy'])\n", "def test_vectorizer_max_df_min_df_combination():\n    # Test the combination of max_df and min_df parameters\n    test_data = ['aaabc', 'abbde', 'eaeee']\n    vect = CountVectorizer(analyzer='char', max_df=0.6, min_df=2)\n    vect.fit(test_data)\n    # 'a' and 'e' are common in more than 60% documents, so they are ignored\n    # 'c', 'd' and 'b' are in less than 2 documents, so they are ignored\n    # 'e' is in 2 documents, so it is kept\n    assert_true('a' not in vect.vocabulary_.keys())\n    assert_true('e' in vect.vocabulary_.keys())\n    assert_true('c' not in vect.vocabulary_.keys())\n    assert_true('d' not in vect.vocabulary_.keys())\n    assert_true('b' not in vect.vocabulary_.keys())\n    assert_equal(len(vect.vocabulary_.keys()), 1)\n"], "sample_26": ["def test_int16(tmp_path):\n    \"\"\"Test for int16 support, https://github.com/astropy/astropy/issues/11995\"\"\"\n    img = np.arange(-500, 500, dtype=np.int16).reshape(10, 10)\n    hdu = fits.PrimaryHDU(img)\n    hdu.writeto(tmp_path / \"int16.fits\")\n\n    with fits.open(tmp_path / \"int16.fits\") as hdul:\n        assert hdul[0].header[\"BITPIX\"] == 16\n        assert hdul[0].header[\"BZERO\"] == -32768\n        assert hdul[0].header[\"BSCALE\"] == 1.0\n        assert_equal(hdul[0].data, img)\n        assert hdul[0].data.dtype == img.dtype\n", "def test_int16(tmp_path):\n    \"\"\"Test for int16 support, related to https://github.com/astropy/astropy/issues/11995\"\"\"\n    img = np.arange(-50, 50, dtype=np.int16).reshape(10, 10)\n    hdu = fits.PrimaryHDU(img)\n    hdu.writeto(tmp_path / \"int16.fits\")\n\n    with fits.open(tmp_path / \"int16.fits\") as hdul:\n        assert hdul[0].header[\"BITPIX\"] == 16\n        assert hdul[0].header[\"BZERO\"] == 32768\n        assert hdul[0].header[\"BSCALE\"] == 1.0\n        assert_equal(hdul[0].data, img)\n        assert hdul[0].data.dtype == img.dtype\n", "def test_compression_roundtrip(self, compression_type, quantize_level, byte_order):\n    data = np.arange(1000, dtype=np.float32).reshape(10, 10, 10).newbyteorder(byte_order)\n    hdu = fits.CompImageHDU(\n        data,\n        name=\"SCI\",\n        compression_type=compression_type,\n        quantize_level=quantize_level,\n    )\n    hdu.writeto(self.temp(\"test.fits\"))\n\n    with fits.open(self.temp(\"test.fits\")) as hdul:\n        assert (hdul[\"SCI\"].data == data).all()\n        assert hdul[\"SCI\"].header[\"NAXIS\"] == hdu.header[\"NAXIS\"]\n        assert hdul[\"SCI\"].header[\"NAXIS1\"] == hdu.header[\"NAXIS1\"]\n        assert hdul[\"SCI\"].header[\"NAXIS2\"] == hdu.header[\"NAXIS2\"]\n        assert hdul[\"SCI\"].header[\"BITPIX\"] == hdu.header[\"BITPIX\"]\n", "def test_scale_back_large_uint_assignment(self):\n    \"\"\"\n    Test scale_back=True with large unsigned integer data.\n\n    This test is similar to test_scale_back_uint_assignment and\n    test_scale_back_compressed_uint_assignment, but uses a larger array\n    to ensure that the entire array is handled correctly.\n    \"\"\"\n    a = np.arange(1000000, dtype=np.uint32)\n    fits.PrimaryHDU(a).writeto(self.temp(\"test_large.fits\"))\n    with fits.open(self.temp(\"test_large.fits\"), mode=\"update\", scale_back=True) as hdul:\n        hdul[0].data[:] = 0\n        assert np.allclose(hdul[0].data, 0)\n", "def test_float_bzero(tmp_path, bitpix):\n    # Regression test for https://github.com/astropy/astropy/issues/12681\n    # Test that images with BITPIX=-32/-64 and BZERO set to a floating point\n    # value are correctly written and read back.\n    img = np.arange(-50, 50, dtype=f\"float{abs(bitpix)}\").reshape(10, 10)\n    hdu = fits.PrimaryHDU(img)\n    hdu.header[\"BZERO\"] = 123.456\n    hdu.writeto(tmp_path / \"float_bzero.fits\")\n\n    with fits.open(tmp_path / \"float_bzero.fits\") as hdul:\n        assert hdul[0].header[\"BITPIX\"] == bitpix\n        assert hdul[0].header[\"BZERO\"] == 123.456\n        assert hdul[0].header[\"BSCALE\"] == 1.0\n        assert_almost_equal(hdul[0].data, img + 123.456)\n        assert hdul[0].data.dtype == img.dtype\n"], "sample_902": ["def test_pipeline_with_none_estimator():\n    # Test that a pipeline with None as estimator still works\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', None)])\n    pipe.fit(X, y=None)\n    assert_array_equal(X, pipe.transform(X))\n", "def test_pipeline_pairwise():\n    # Test the _pairwise attribute of the pipeline\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    clf = SVC(kernel='precomputed')\n    pipeline = Pipeline([('transf', Transf()), ('clf', clf)])\n    assert_false(pipeline._pairwise)\n\n    clf = SVC(kernel='linear')\n    pipeline = Pipeline([('transf', Transf()), ('clf', clf)])\n    assert_true(pipeline._pairwise)\n", "def test_pipeline_memory_without_cache():\n    # Test that the pipeline runs correctly even when no caching is enabled\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Test with Transformer + SVC\n    clf = SVC(probability=True, random_state=0)\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', transf), ('svc', clf)], memory=None)\n\n    # Fit the pipeline\n    pipe.fit(X, y)\n\n    # Check that the pipeline yields correct results\n    assert_array_equal(pipe.predict(X), clf.fit(transf.fit_transform(X), y).predict(transf.transform(X)))\n    assert_array_equal(pipe.predict_proba(X), clf.fit(transf.fit_transform(X), y).predict_proba(transf.transform(X)))\n    assert_array_equal(pipe.predict_log_proba(X), clf.fit(transf.fit_transform(X), y).predict_log_proba(transf.transform(X)))\n    assert_array_equal(pipe.score(X, y), clf.fit(transf.fit_transform(X), y).score(transf.transform(X), y))\n    assert_array_equal(pipe.named_steps['transf'].means_, transf.fit_transform(X, y).mean(axis=0))\n", "def test_pipeline_final_estimator_none():\n    # Test Pipeline when final estimator is None\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf())])\n    pipe.fit(X, y=None)\n    X_transformed = pipe.transform(X)\n    assert_array_equal(X_transformed, X)\n\n    # test fit_transform:\n    X_trans = pipe.fit_transform(X)\n    assert_array_equal(X_trans, X)\n\n    X_back = pipe.inverse_transform(X_transformed)\n    assert_array_equal(X_back, X)\n\n    # test that predict raises an AttributeError\n    assert_raises(AttributeError, pipe.predict, X)\n", "def test_pipeline_estimator_type():\n    # Test that the _estimator_type attribute is correctly propagated\n    reg = make_pipeline(SelectKBest(k=1), LinearRegression())\n    assert_equal(reg._estimator_type, \"regressor\")\n\n    clf = make_pipeline(SelectKBest(k=1), LogisticRegression(random_state=0))\n    assert_equal(clf._estimator_type, \"classifier\")\n\n    transformer = make_pipeline(SelectKBest(k=1), StandardScaler())\n    assert_equal(transformer._estimator_type, \"transformer\")\n"], "sample_417": ["def test_force_grouping_unlocalize(self):\n    with translation.override(\"de\", deactivate=True):\n        self.assertEqual(floatformat(66666.666, \"2g\"), \"66.666,7\")\n        self.assertEqual(floatformat(66666.666, \"2gu\"), \"66666.67\")\n        with self.settings(\n            USE_THOUSAND_SEPARATOR=True,\n            NUMBER_GROUPING=3,\n            THOUSAND_SEPARATOR=\"!\",\n        ):\n            self.assertEqual(floatformat(66666.666, \"2ug\"), \"66!666.67\")\n        # Invalid suffix.\n        self.assertEqual(floatformat(66666.666, \"g2u\"), \"66666.666\")\n", "def test_floatformat_decimal(self):\n    output = self.engine.render_to_string(\n        \"floatformat_decimal\", {\"a\": Decimal(\"1.42\"), \"b\": Decimal(\"1.42\")}\n    )\n    self.assertEqual(output, \"1.4 1.420\")\n", "def test_floatformat03(self):\n    with translation.override(\"en\"):\n        output = self.engine.render_to_string(\n            \"floatformat03\", {\"a\": \"123456.666\", \"b\": \"123456.666\", \"c\": \"123456.666\"}\n        )\n        self.assertEqual(output, \"123456.67 123,456.67 123456.67\")\n", "def test_suffixes_with_force_grouping(self):\n    with translation.override(\"en\"):\n        self.assertEqual(floatformat(10000, \"1g\"), \"10,000.0\")\n        self.assertEqual(floatformat(66666.666, \"2ug\"), \"66666.67\")\n    with translation.override(\"de\", deactivate=True):\n        self.assertEqual(floatformat(10000, \"1g\"), \"10.000,0\")\n        self.assertEqual(floatformat(66666.666, \"2ug\"), \"66666.67\")\n", "def test_float_out_of_range_values(self):\n    tests = [\n        (float(\"inf\"), 2, \"inf\"),\n        (float(\"-inf\"), 2, \"-inf\"),\n        (float(\"nan\"), 2, \"nan\"),\n    ]\n    for num, decimal_places, expected in tests:\n        with self.subTest(num=num, decimal_places=decimal_places):\n            self.assertEqual(floatformat(num, decimal_places), expected)\n"], "sample_790": ["def test_kernel_pca_inverse_transform_sparse():\n    # Test that inverse_transform raises an error for sparse input\n    rng = np.random.RandomState(0)\n    X_fit = sp.csr_matrix(rng.random_sample((5, 4)))\n\n    kpca = KernelPCA(fit_inverse_transform=True)\n    kpca.fit(X_fit)\n    X_transformed = kpca.transform(X_fit)\n\n    with pytest.raises(NotImplementedError):\n        kpca.inverse_transform(X_transformed)\n", "def test_kernel_pca_n_jobs():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((10, 10))\n\n    kpca_single = KernelPCA(n_jobs=1)\n    X_single = kpca_single.fit_transform(X_fit)\n\n    kpca_multi = KernelPCA(n_jobs=2)\n    X_multi = kpca_multi.fit_transform(X_fit)\n\n    assert_array_almost_equal(X_single, X_multi)\n", "def test_kernel_pca_n_components_arpack(n_components):\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    kpca = KernelPCA(n_components=n_components, eigen_solver='arpack')\n    shape = kpca.fit(X_fit).transform(X_pred).shape\n\n    assert_equal(shape, (2, n_components))\n", "def test_kernel_pca_gamma_parameter():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n\n    gamma_values = [0.1, 1, 10]\n    for gamma in gamma_values:\n        kpca = KernelPCA(kernel=\"rbf\", gamma=gamma)\n        X_transformed = kpca.fit_transform(X)\n\n        # Test that the transformed data is not all zeros, as it would be the case for gamma=0\n        assert not np.allclose(X_transformed, np.zeros_like(X_transformed))\n\n        # Test that the transformed data changes with different gamma values\n        if gamma != gamma_values[0]:\n            assert not np.allclose(X_transformed, X_transformed_prev)\n\n        X_transformed_prev = X_transformed\n", "def test_kernel_pca_kernel_params():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    # Test kernel_params with custom kernel function\n        return np.exp(-np.linalg.norm(x - y) / (2 * (sigma ** 2)))\n\n    kpca = KernelPCA(4, kernel=custom_kernel, kernel_params={'sigma': 2.0})\n    X_fit_transformed = kpca.fit_transform(X_fit)\n    X_pred_transformed = kpca.transform(X_pred)\n\n    assert_equal(X_pred_transformed.shape[1], X_fit_transformed.shape[1])\n"], "sample_681": ["def test_logging_emit_error_with_exc_info(testdir: Testdir) -> None:\n    \"\"\"\n    An exception raised during emit() should fail the test even if exc_info is passed.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning('oops', 'first', 2, exc_info=True)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n    result.stdout.fnmatch_lines(\n        [\n            \"====* FAILURES *====\",\n            \"*not all arguments converted during string formatting*\",\n        ]\n    )\n", "def test_log_cli_ini_format(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format = %(levelname)s - %(message)s\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_ini_format.py*INFO - This log message will be shown\",\n            \"PASSED\",  # 'PASSED' on its own line because the log message prints a new line\n        ]\n    )\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_get_log_level_for_setting(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        from _pytest.config import Config\n        from _pytest.main import Session\n        from _pytest.logging import get_log_level_for_setting\n\n            config = Config.fromdict({})\n            config.option.log_level = \"WARNING\"\n            assert get_log_level_for_setting(config, \"log_level\") == logging.WARNING\n\n            config.option.log_level = \"invalid\"\n            with pytest.raises(pytest.UsageError):\n                get_log_level_for_setting(config, \"log_level\")\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_auto_indent(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(__name__)\n            logger.info('\\\\n'.join(['This is a', 'multiline log', 'message.']))\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent=on\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured log call -*\",\n            \"INFO    test_log_auto_indent.py:7 This is a\",\n            \"                multiline log\",\n            \"                message.\",\n        ]\n    )\n", "def test_log_file_cli_subdirectories_are_successfully_created_with_absolute_path(testdir):\n    path = testdir.makepyfile(\"\"\" def test_logger(): pass \"\"\")\n    expected = os.path.join(str(path), \"foo\", \"bar\")\n    result = testdir.runpytest(f\"--log-file={expected}/logf.log\")\n    assert \"logf.log\" in os.listdir(expected)\n    assert result.ret == ExitCode.OK\n"], "sample_904": ["def test_cmd_option_with_space(app):\n    text = \".. option:: --option with space\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (index,\n                          [desc, ([desc_signature, ([desc_name, '--option'],\n                                                    [desc_addname, 'with space'])],\n                                  [desc_content, ()])]))\n    objects = list(app.env.get_domain(\"std\").get_objects())\n    assert ('--option', '--option', 'cmdoption', 'index', 'cmdoption-option-with-space', 1) in objects\n", "def test_cmd_option_with_space_in_target(app):\n    text = \".. option:: --my-option=<value with space>\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (index,\n                          [desc, ([desc_signature, ([desc_name, '--my-option'],\n                                                    [desc_addname, '=<value with space>'])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[('pair', 'command line option; --my-option', 'cmdoption-my-option', '', None)])\n\n    objects = list(app.env.get_domain(\"std\").get_objects())\n    assert ('--my-option', '--my-option', 'cmdoption', 'index', 'cmdoption-my-option', 1) in objects\n", "def test_cmd_option_with_space_in_target(app):\n    text = \".. option:: --long-option argument with space\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (index,\n                          [desc, ([desc_signature, ([desc_name, \"--long-option\"],\n                                                    [desc_addname, \" argument with space\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[('pair', 'command line option; --long-option', 'cmdoption-long-option', '', None)])\n\n    objects = list(app.env.get_domain(\"std\").get_objects())\n    assert ('--long-option', '--long-option', 'cmdoption', 'index', 'cmdoption-long-option', 1) in objects\n", "def test_program_option_cross_reference(app):\n    text = (\".. program:: myprogram\\n\"\n            \"\\n\"\n            \".. option:: --myoption\\n\"\n            \"\\n\"\n            \"See :option:`--myoption` for more information.\")\n    doctree = restructuredtext.parse(app, text)\n    refnode = doctree[1][0][0][0][0]\n    assert_node(refnode, nodes.reference, refid=\"cmdoption-myprogram-myoption\")\n", "def test_program_option_with_subcommand(app):\n    text = (\".. program:: ls\\n\"\n            \"\\n\"\n            \".. option:: -l -a\\n\")\n    domain = app.env.get_domain('std')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"-l\"],\n                                                    [desc_addname, \" -a\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[('pair', 'ls command line option; -l', 'cmdoption-ls-l', '', None),\n                         ('pair', 'ls command line option; -a', 'cmdoption-ls-l', '', None)])\n    assert ('ls', '-l') in domain.progoptions\n    assert ('ls', '-a') in domain.progoptions\n    assert domain.progoptions[('ls', '-l')] == ('index', 'cmdoption-ls-l')\n    assert domain.progoptions[('ls', '-a')] == ('index', 'cmdoption-ls-l')\n"], "sample_2": ["def test_sliced_ccdata_wcs_attribute():\n    wcs = WCS(naxis=2)\n    wcs.wcs.crpix = 10, 10\n    ccd = CCDData(np.ones((10, 10)), wcs=wcs, unit='pixel')\n    trimmed = ccd[2:-2, 2:-2]\n    assert isinstance(trimmed.wcs, WCS)\n    assert trimmed.wcs.wcs.crpix == [8, 8]\n", "def test_wcs_slicing():\n    ccd_data = create_ccd_data()\n    wcs = WCS(naxis=2)\n    ccd_data.wcs = wcs\n    sliced_ccd = ccd_data[1:3, 1:3]\n    assert sliced_ccd.wcs is not None\n    assert sliced_ccd.wcs.array_shape == (2, 2)\n", "def test_wcs_attribute_type():\n    # Test that setting the wcs attribute to a non-WCS instance raises a TypeError\n    ccd_data = create_ccd_data()\n    with pytest.raises(TypeError):\n        ccd_data.wcs = \"not a WCS instance\"\n", "def test_wcs_slicing_when_no_wcs():\n    ccd_data = create_ccd_data()\n    ccd_data.wcs = None\n    sliced_ccd = ccd_data[1:5, 1:5]\n    assert sliced_ccd.wcs is None\n", "def test_wcs_slicing_keeps_wcs_type():\n    # Test that slicing a CCDData object with WCS preserves the WCS type\n    ccd_data = create_ccd_data()\n    wcs = WCS(naxis=2)\n    ccd_data.wcs = wcs\n    sliced_ccd = ccd_data[1:, 1:]\n    assert isinstance(sliced_ccd.wcs, WCS)\n"], "sample_135": ["def test_invalid_date_format_specifiers(self):\n    my_time = datetime.time(22, 00)\n\n    for specifier in ['d', 'D', 'E', 'F', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'S', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n        msg = (\n            \"The format for time objects may not contain date-related \"\n            \"format specifiers (found '%s').\" % specifier\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            dateformat.time_format(my_time, specifier)\n", "def test_invalid_date_format_specifiers(self):\n    my_datetime = datetime(1984, 8, 7, 22, 00)\n\n    for specifier in ['d', 'D', 'E', 'F', 'I', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'S', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n        msg = (\n            \"The format for time objects may not contain date-related \"\n            \"format specifiers (found '%s').\" % specifier\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            dateformat.time_format(my_datetime, specifier)\n", "def test_time_format_specifiers(self):\n    # Test that time format specifiers raise an error for date objects\n    my_birthday = date(1984, 8, 7)\n\n    for specifier in ['O', 'P', 'T', 'Z']:\n        msg = (\n            \"The format for date objects may not contain time-related \"\n            \"format specifiers (found '%s').\" % specifier\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            dateformat.format(my_birthday, specifier)\n", "def test_time_formats_without_leading_zeros(self):\n    my_birthday = datetime(1979, 7, 8, 10, 5)\n\n    self.assertEqual(dateformat.format(my_birthday, 'g'), '10')\n    self.assertEqual(dateformat.format(my_birthday, 'G'), '10')\n    self.assertEqual(dateformat.format(my_birthday, 'i'), '05')\n    self.assertEqual(dateformat.format(my_birthday, 's'), '00')\n", "def test_time_formats_with_different_locales(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    with translation.override('es'):\n        self.assertEqual(dateformat.format(my_birthday, 'A'), 'PM')\n        self.assertEqual(dateformat.format(my_birthday, 'D'), 'dom.')\n        self.assertEqual(dateformat.format(my_birthday, 'F'), 'julio')\n        self.assertEqual(dateformat.format(my_birthday, 'l'), 'domingo')\n        self.assertEqual(dateformat.format(my_birthday, 'N'), 'julio')\n\n    with translation.override('de'):\n        self.assertEqual(dateformat.format(my_birthday, 'A'), 'PM')\n        self.assertEqual(dateformat.format(my_birthday, 'D'), 'So.')\n        self.assertEqual(dateformat.format(my_birthday, 'F'), 'Juli')\n        self.assertEqual(dateformat.format(my_birthday, 'l'), 'Sonntag')\n        self.assertEqual(dateformat.format(my_birthday, 'N'), 'Juli')\n"], "sample_356": ["    def test_rename_m2m_field(self):\n        \"\"\"\n        Tests autodetection of renamed fields for ManyToManyFields.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_m2m, self.publisher],\n            [self.author_with_renamed_m2m, self.publisher],\n            MigrationQuestioner({\"ask_rename\": True}),\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"publishers\", new_name=\"publications\")\n", "    def test_alter_field_with_database_function(self):\n        changes = self.get_changes([self.author_name], [self.author_name_database_function])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=models.Func(function='NOW'))\n", "    def test_rename_model_with_renamed_foreign_key(self):\n        \"\"\"\n        Tests autodetection of renamed models while simultaneously renaming one\n        of the fields that refer to the renamed model.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_book, self.book],\n            [self.author_renamed_with_book, self.book_with_renamed_author_field],\n            MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True}),\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n        # Right number/type of migrations for renamed field?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, old_name=\"author\", new_name=\"writer\")\n", "def test_add_model_with_field_removed_from_base_model_before_subclass(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'book', [\n            ('readable_ptr_id', models.AutoField(auto_created=True, primary_key=True, serialize=False)),\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_rename_model_with_swappable_foreignkey(self):\n    \"\"\"\n    Test renaming a model with a swappable foreignkey, and make sure the\n    foreignkey is updated correctly.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.book_with_custom_user],\n        [self.author_renamed_with_db_table_options, self.book_with_custom_user_renamed],\n        MigrationQuestioner({\"ask_rename_model\": True, \"ask_rename\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"RenameField\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, old_name=\"user\", new_name=\"newauthor\")\n"], "sample_294": ["def test_https_good_origin_csrf_trusted_origin_allowed(self):\n    \"\"\"\n    A POST HTTPS request with a good origin added to the CSRF_TRUSTED_ORIGINS\n    setting is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://dashboard.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n", "def test_csrf_token_in_header_with_customized_name_mismatch(self):\n    \"\"\"\n    The token should be rejected if it doesn't match the one in the customized header.\n    \"\"\"\n    req = self._get_POST_csrf_cookie_request()\n    req.META['HTTP_X_CSRFTOKEN_CUSTOMIZED'] = 'wrong_token'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        resp = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(403, resp.status_code)\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_BAD_TOKEN)\n", "def test_https_good_referer_matches_host(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted from a host\n    that's different from the current one.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'another.example.com'\n    req.META['HTTP_REFERER'] = 'https://www.example.com/'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_csrf_token_on_403_changes(self):\n    response = self.client.get('/csrf_failure/')\n    self.assertEqual(response.status_code, 403)\n    token1 = response.content\n    response = self.client.get('/csrf_failure/')\n    self.assertEqual(response.status_code, 403)\n    token2 = response.content\n    self.assertNotEqual(token1, token2)\n", "def test_cookie_reset_on_token_mismatch(self):\n    \"\"\"\n    The csrf token used in posts is changed on every request (although\n    stays equivalent). The csrf cookie should be changed on token mismatch.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.POST['csrfmiddlewaretoken'] = 'incorrect_token'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        resp = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(resp.status_code, 403)\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_BAD_TOKEN)\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, None)\n    if csrf_cookie:\n        self.assertNotEqual(\n            csrf_cookie.value, self._csrf_id_cookie,\n            \"CSRF cookie was not changed on token mismatch\"\n        )\n"], "sample_699": ["def test_doctest_no_linedata_on_overriden_property_with_doc(pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n        class Sample(object):\n            @property\n                '''\n                >>> Sample().some_property\n                'another thing'\n                '''\n                return 'something'\n            some_property.__doc__ = None\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*= FAILURES =*\",\n            \"*_ [[]doctest[]] test_doctest_no_linedata_on_overriden_property_with_doc.Sample.some_property _*\",\n            \"EXAMPLE LOCATION UNKNOWN, not showing all tests of that example\",\n            \"[?][?][?] >>> Sample().some_property\",\n            \"Expected:\",\n            \"    'another thing'\",\n            \"Got:\",\n            \"    'something'\",\n            \"\",\n            \"*/test_doctest_no_linedata_on_overriden_property_with_doc.py:None: DocTestFailure\",\n            \"*= 1 failed in *\",\n        ]\n    )\n", "def test_doctest_lineno_empty_docstring(self, pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n            '''\n            '''\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\")\n    result.stdout.no_fnmatch_line(\"*ERROR*\")\n", "def test_doctest_report_options_ini(self, pytester: Pytester):\n    \"\"\"Test that the doctest report options can be set in the ini file.\"\"\"\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        doctest_report = udiff\n        \"\"\"\n    )\n    result = self._run_doctest_report(pytester, \"\")\n    result.stdout.fnmatch_lines(\n        [\"     0  1  4\", \"    -1  2  4\", \"    +1  2  5\", \"     2  3  6\"]\n    )\n", "def test_doctest_report_option_default(self, pytester: Pytester):\n    result = self._run_doctest_report(pytester, \"udiff\")\n    result.stdout.fnmatch_lines(\n        [\"     0  1  4\", \"    -1  2  4\", \"    +1  2  5\", \"     2  3  6\"]\n    )\n", "def test_doctest_module_without_doctest(pytester: Pytester):\n    \"\"\"\n    Test that a module without a doctest does not generate a DoctestItem.\n    \"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    items, reprec = pytester.inline_genitems(p, \"--doctest-modules\")\n    assert len(items) == 0\n"], "sample_1048": ["def test_parabola_3d_intersection():\n    l1 = Line(Point3D(1, -2, 3), Point3D(-1, -2, 3))\n    p1 = Point3D(0, 0, 3)\n    parabola1 = Parabola(p1, l1)\n\n    # parabola with 3d line\n    raises(TypeError, lambda: parabola1.intersection(Line3D(Point3D(-7, 3, 4), Point3D(12, 3, 4))))\n", "def test_parabola_exceptions():\n    # Test that TypeError is raised when intersecting with a 3D entity\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    raises(TypeError, lambda: p1.intersection(Line(Point(1, 2, 3), Point(4, 5, 6))))\n\n    # Test that TypeError is raised when intersecting with an unsupported entity\n    raises(TypeError, lambda: p1.intersection(\"unsupported entity\"))\n", "def test_parabola_axis_of_symmetry():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    p2 = Parabola(Point(3, 4), Line(Point(3, 0), Point(3, 10)))\n\n    assert p1.axis_of_symmetry == Line2D(Point2D(0, 0), Point2D(0, 1))\n    assert p2.axis_of_symmetry == Line2D(Point2D(3, 4), Point2D(6, 4))\n\n", "def test_parabola_equality():\n    l1 = Line(Point(1, -2), Point(-1,-2))\n    l2 = Line(Point(1, 2), Point(-1,2))\n    p1 = Point(0,0)\n    p2 = Point(3, 7)\n\n    parabola1 = Parabola(p1, l1)\n    parabola2 = Parabola(p1, l2)\n    parabola3 = Parabola(p2, l1)\n\n    assert parabola1 != parabola2  # Parabolas with different directrices are not equal\n    assert Parabola(p1, l1) == Parabola(p1, l1)  # The same parabola is equal to itself\n    assert parabola1 != parabola3  # Parabolas with different foci are not equal\n", "def test_parabola_exceptions():\n    # Test exceptions for invalid inputs\n    with raises(ValueError):\n        # Directrix contains focus\n        Parabola(Point(5, 8), Line(Point(5, 8), Point(7, 8)))\n\n    with raises(NotImplementedError):\n        # Directrix is not horizontal or vertical\n        Parabola(Point(0, 0), Line(Point(0, 1), Point(1, 2)))\n\n    with raises(TypeError):\n        # Entity is three dimensional\n        Parabola(Point(0, 0), Line(Point(0, 0, 1), Point(1, 0, 1)))\n"], "sample_1165": ["def test_quaternion_integration():\n    q = Quaternion(x, y, z, w)\n\n    assert integrate(q, x) == Quaternion(x**2/2, x*y/2, x*z/2, x*w/2)\n    assert integrate(q, y) == Quaternion(y*x/2, y**2/2, y*z/2, y*w/2)\n    assert integrate(q, z) == Quaternion(z*x/2, z*y/2, z**2/2, z*w/2)\n    assert integrate(q, w) == Quaternion(w*x/2, w*y/2, w*z/2, w**2/2)\n\n    assert integrate(q, (x, 0, 1)) == Quaternion(1/2, y/2, z/2, w/2)\n    assert integrate(q, (y, 0, 1)) == Quaternion(x/2, 1/2, z/2, w/2)\n    assert integrate(q, (z, 0, 1)) == Quaternion(x/2, y/2, 1/2, w/2)\n    assert integrate(q, (w, 0, 1)) == Quaternion(x/2, y/2, z/2, 1/2)\n", "def test_quaternion_division():\n    q1 = Quaternion(3, 4, 5, 6)\n    q2 = Quaternion(1, 2, 3, 4)\n    q3 = Quaternion(1, 0, 0, 0)\n\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / q3 == q1 * q3.inverse()\n    assert q1 / 2 == q1 * (1 / 2)\n    assert 2 / q1 == (1 / q1) * 2\n    raises(ValueError, lambda: q1 / q0)\n", "def test_quaternion_division():\n    q1 = Quaternion(3, 2, 1, 0)\n    q2 = Quaternion(1, 2, 3, 4)\n\n    assert q1 / q2 == Quaternion(Rational(-11, 30), Rational(-4, 15), Rational(1, 6), Rational(2, 15))\n    assert q2 / q1 == Quaternion(Rational(1, 3), Rational(1, 3), Rational(1, 3), Rational(-1, 3))\n    assert q1 / 2 == Quaternion(Rational(3, 2), 1, Rational(1, 2), 0)\n    assert 2 / q1 == Quaternion(Rational(2, 3), Rational(1, 3), Rational(2, 3), 0)\n", "def test_quaternion_subtraction():\n    q1 = Quaternion(3, 4, 5, 6)\n    q2 = Quaternion(1, 2, 3, 4)\n    assert q1 - q2 == Quaternion(2, 2, 2, 2)\n    assert q2 - q1 == Quaternion(-2, -2, -2, -2)\n\n    q = Quaternion(w, x, y, z)\n    assert q - q == Quaternion(0, 0, 0, 0)\n", "def test_quaternion_integrate_definite():\n    x = symbols('x')\n    q = Quaternion(x, x**2, x**3, x**4)\n    result = q.integrate((x, 0, 1))\n    expected = Quaternion(S.One/4, S.One/5, S.One/6, S.One/7)\n    assert result == expected\n"], "sample_784": ["def test_calibration_with_categorical_features():\n    \"\"\"Test calibration with categorical features\"\"\"\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([0, 1, 1, 0])\n\n    clf = MultinomialNB()\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2)\n    cal_clf.fit(X, y)\n    probas = cal_clf.predict_proba(X)\n    assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X)))\n", "def test_calibration_binary_probabilistic_predictions():\n    \"\"\"Test calibration with binary probabilistic predictions\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_prob = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    pc_clf = CalibratedClassifierCV(method='sigmoid')\n\n    # Check that an error is raised if the true values are not binary\n    assert_raises(ValueError, pc_clf.fit, np.random.randn(6, 2), np.array([0, 1, 2, 0, 1, 2]))\n\n    # Check that an error is raised if the probabilities are not between 0 and 1\n    assert_raises(ValueError, pc_clf.fit, np.random.randn(6, 2), y_true, sample_weight=np.array([-1, 0, 1, 2, 3, 4]))\n\n    # Check that no error is raised if the true values are binary and the probabilities are between 0 and 1\n    pc_clf.fit(np.random.randn(6, 2), y_true, sample_weight=y_prob)\n", "def test_calibration_sample_weight():\n    \"\"\"Test calibration when sample weights are provided\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                               random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X_train, y_train, sw_train = \\\n        X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    for method in ['sigmoid', 'isotonic']:\n        base_estimator = LinearSVC(random_state=42)\n        calibrated_clf = CalibratedClassifierCV(base_estimator, method=method)\n        calibrated_clf.fit(X_train, y_train, sample_weight=sw_train)\n        probs_with_sw = calibrated_clf.predict_proba(X_test)\n\n        # As the weights are used for the calibration, they should still yield\n        # a different predictions\n        calibrated_clf.fit(X_train, y_train)\n        probs_without_sw = calibrated_clf.predict_proba(X_test)\n\n        diff = np.linalg.norm(probs_with_sw - probs_without_sw)\n        assert_greater(diff, 0.1)\n", "def test_calibration_multiclass_classes_arg():\n    \"\"\"Test calibration for multiclass with explicit classes\"\"\"\n    # Test that the classes argument works as expected for multiclass\n    X, y = make_blobs(n_samples=100, n_features=2, random_state=42,\n                      cluster_std=3.0)\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    classes = np.unique(y)\n    y_train = classes[y_train]  # Convert to explicit class names\n    y_test = classes[y_test]  # Convert to explicit class names\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n    clf_probs = clf.predict_proba(X_test)\n    loss = log_loss(y_test, clf_probs)\n\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3, classes=classes)\n        cal_clf.fit(X_train, y_train)\n        cal_clf_probs = cal_clf.predict_proba(X_test)\n        cal_loss = log_loss(y_test, cal_clf_probs)\n        assert_greater(loss, cal_loss)\n", "def test_calibration_sample_weight_warning():\n    # Test that a warning is raised when sample_weight is passed to a\n    # base_estimator that does not support sample_weight.\n    # We use a dummy estimator that does not accept sample_weight.\n\n    class DummyClassifier:\n            pass\n\n            return np.ones(len(X))\n\n    n_samples = 10\n    X, y = make_classification(n_samples=n_samples, n_features=2,\n                               n_informative=2, n_redundant=0,\n                               random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=n_samples)\n\n    clf = DummyClassifier()\n    clf_c = CalibratedClassifierCV(clf, cv=2, method='isotonic')\n\n    with pytest.warns(UserWarning, match='does not support sample_weight.'):\n        clf_c.fit(X, y, sample_weight=sample_weight)\n"], "sample_230": ["def test_json_field_with_custom_encoder(self):\n    class CustomEncoder(DjangoJSONEncoder):\n            if isinstance(obj, uuid.UUID):\n                return str(obj)\n            return super().default(obj)\n\n    value = {'uuid': uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}')}\n    encoded_value = '{\"uuid\": \"c141e152-6550-4172-a784-05448d98204b\"}'\n    field = JSONField(encoder=CustomEncoder)\n    self.assertEqual(field.prepare_value(value), encoded_value)\n    self.assertEqual(field.clean(encoded_value), value)\n", "def test_invalid_initial(self):\n    class JSONForm(Form):\n        json_field = JSONField()\n\n    form = JSONForm({'json_field': '{\"a\": \"b\"}'}, initial={'json_field': '{\"a\": \"c\"}'})\n    self.assertFalse(form.has_changed())\n", "def test_custom_encoder_with_invalid_json(self):\n    class CustomEncoder(json.JSONEncoder):\n            if isinstance(obj, uuid.UUID):\n                return obj.hex\n            return super().default(obj)\n\n    value = {'uuid': uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}')}\n    encoded_value = '{\"uuid\": \"c141e15265504172a78405448d98204b\"}'\n    field = JSONField(encoder=CustomEncoder)\n    self.assertEqual(field.prepare_value(value), encoded_value)\n", "def test_valid_uuid_input(self):\n    field = JSONField()\n    value = str(uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}'))\n    cleaned_value = field.clean(value)\n    self.assertEqual(cleaned_value, value)\n", "def test_json_field_empty_with_required(self):\n    field = JSONField(required=True)\n    with self.assertRaisesMessage(ValidationError, 'This field is required.'):\n        field.clean('')\n"], "sample_644": ["def test_import_outside_toplevel(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"outside_toplevel\", REGR_DATA)\n    import_from = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=import_from,\n        args=\"os\",\n        confidence=UNDEFINED,\n        line=3,\n        col_offset=4,\n        end_line=3,\n        end_col_offset=10,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_importfrom(import_from)\n", "def test_check_import_as_rename(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"rename_import\", REGR_DATA)\n    import_node = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"useless-import-alias\",\n        node=import_node,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=28,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n\n    msg = MessageTest(\n        msg_id=\"consider-using-from-import\",\n        node=import_node,\n        args=(\"math\", \"sin\"),\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=28,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n", "def test_allow_any_import_level(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"allow_any_import\", REGR_DATA)\n    import_from = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=import_from,\n        args=\"os\",\n        line=3,\n        col_offset=0,\n        end_line=3,\n        end_col_offset=9,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_importfrom(import_from)\n\n    self.checker.config.allow_any_import_level = (\"os\",)\n    with self.assertNoMessages():\n        self.checker.visit_importfrom(import_from)\n", "def test_consider_using_from_import(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"consider_using_from_import\", REGR_DATA)\n    import_from = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"consider-using-from-import\",\n        node=import_from,\n        args=(\"module\", \"Class\"),\n        confidence=UNDEFINED,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=27,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_from)\n", "def test_relative_import_within_init(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"relative_import_init\", REGR_DATA)\n    import_from = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"relative-beyond-top-level\",\n        node=import_from,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=25,\n    )\n    with self.assertNoMessages():\n        self.checker.visit_importfrom(import_from)\n"], "sample_227": ["def test_emptylistfieldfilter_non_nullable_field(self):\n    class BookAdminWithEmptyFieldListFilterNonNullable(ModelAdmin):\n        list_filter = [('no', EmptyFieldListFilter)]\n\n    modeladmin = BookAdminWithEmptyFieldListFilterNonNullable(Book, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    msg = (\n        \"The list filter 'EmptyFieldListFilter' cannot be used with field \"\n        \"'no' which doesn't allow empty strings and nulls.\"\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        modeladmin.get_changelist_instance(request)\n", "def test_emptylistfieldfilter_non_boolean_field(self):\n    class BookAdminWithNonBooleanEmptyFieldListFilter(ModelAdmin):\n        list_filter = [('no', EmptyFieldListFilter)]\n\n    modeladmin = BookAdminWithNonBooleanEmptyFieldListFilter(Book, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    msg = (\n        \"The list filter 'EmptyFieldListFilter' cannot be used with field \"\n        \"'no' which is not a Boolean or NullBooleanField.\"\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        modeladmin.get_changelist_instance(request)\n", "def test_relatedonlyfieldlistfilter_manytomany_foreignkey(self):\n    class UserAdminReverseRelationship(UserAdmin):\n        list_filter = (\n            ('books_authored', RelatedOnlyFieldListFilter),\n        )\n\n    ImprovedBook.objects.create(book=self.guitar_book)\n\n    user_admin = UserAdminReverseRelationship(User, site)\n\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = user_admin.get_changelist_instance(request)\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.lookup_choices, [\n        (self.bio_book.pk, 'Django: a biography'),\n        (self.guitar_book.pk, 'Guitar for dummies'),\n        (self.djangonaut_book.pk, 'Djangonaut: an art of living'),\n    ])\n", "def test_relatedonlyfieldlistfilter_genericrelation(self):\n    modeladmin = BookmarkAdminGenericRelation(Bookmark, site)\n    request = self.request_factory.get('/', {'tags__tag__isempty': '1'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertCountEqual(queryset, [self.kernel_bookmark])\n\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'tag')\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(len(choices), 2)\n\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], False)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    self.assertEqual(choices[1]['display'], 'Empty')\n    self.assertIs(choices[1]['selected'], True)\n    self.assertEqual(choices[1]['query_string'], '?tags__tag__isempty=1')\n", "def test_booleanfieldlistfilter_with_custom_choices(self):\n    class CustomBooleanField(models.BooleanField):\n            kwargs['choices'] = ((True, 'Positive'), (False, 'Negative'))\n            return super().formfield(**kwargs)\n\n    class BookAdminWithCustomBooleanChoices(BookAdmin):\n        list_filter = ('availability',)\n\n    Book.add_to_class('custom_availability', CustomBooleanField(null=True))\n    self.addCleanup(Book._meta.local_fields.remove, Book._meta.get_field('custom_availability'))\n\n    modeladmin = BookAdminWithCustomBooleanChoices(Book, site)\n\n    request = self.request_factory.get('/', {'custom_availability__exact': 'True'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [self.django_book, self.djangonaut_book])\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'custom availability')\n    choice = select_by(filterspec.choices(changelist), 'display', 'Positive')\n    self.assertIs(choice['selected'], True)\n    self.assertEqual(choice['query_string'], '?custom_availability__exact=True')\n\n    request = self.request_factory.get('/', {'custom_availability__exact': 'False'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(list(queryset), [self.bio_book])\n    filterspec = changelist.get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'custom availability')\n    choice = select_by(filterspec.choices(ch"], "sample_228": ["def test_formset_with_file_field(self):\n    \"\"\"Formsets with FileField work.\"\"\"\n    class FileForm(Form):\n        file = FileField()\n\n    FileFormSet = formset_factory(FileForm)\n    formset = FileFormSet()\n    self.assertTrue(formset.is_multipart())\n    self.assertIn('enctype=\"multipart/form-data\"', str(formset))\n", "def test_formset_with_deletion_extra_forms(self):\n    \"\"\"\n    Formsets with deletion and extra forms handle ordering and deletion\n    correctly.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, extra=1)\n    initial = [\n        {'choice': 'Calexico', 'votes': 100},\n        {'choice': 'Fergie', 'votes': 900},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    self.assertHTMLEqual(\n        '\\n'.join(form.as_ul() for form in formset.forms),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_delete_extra_forms_with_initial_data(self):\n    \"\"\"\n    Extra forms with initial data can be marked for deletion.\n    \"\"\"\n    initial = [\n        {'choice': 'Zero', 'votes': 0},\n        {'choice': 'One', 'votes': 1},\n    ]\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '2',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '2',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-0-DELETE': 'on',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n        'choices-1-DELETE': '',\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=0, can_delete=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices', initial=initial)\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(len(formset.deleted_forms), 1)\n    self.assertEqual(formset.deleted_forms[0].cleaned_data, {'votes': 0, 'DELETE': True, 'choice': 'Zero'})\n", "def test_formset_with_deletion_and_min_num(self):\n    \"\"\"FormSets with deletion and min_num.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, min_num=2)\n    initial = [\n        {'choice': 'Calexico', 'votes': 100},\n        {'choice': 'Fergie', 'votes': 900},\n        {'choice': 'The Decemberists', 'votes': 500},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    # Let's delete Fergie, and The Decemberists.\n    data = {\n        'choices-TOTAL_FORMS': '3',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '3',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '2',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-0-DELETE': '',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-1-DELETE': 'on',\n        'choices-2-choice': 'The Decemberists',\n        'choices-2-votes': '500',\n        'choices-2-DELETE': 'on',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at least 2 forms.'],\n    )\n", "def test_invalid_min_num_does_not_affect_cleaned_data(self):\n    data = {\n        'choices-TOTAL_FORMS': '1',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n    }\n    ChoiceFormSet = formset_factory(Choice, min_num=2)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.cleaned_data, [{'choice': 'Zero', 'votes': 0}])\n"], "sample_370": ["    def setUpTestData(cls):\n        house = House.objects.create(name='Big house', address='123 Main St')\n        cls.room = Room.objects.create(name='Kitchen', house=house)\n", "def test_clear_after_prefetch_related(self):\n    \"\"\"\n    Ensure that prefetch_related(None) clears the cache.\n    \"\"\"\n    house = House.objects.create(name='Big house', address='123 Main St')\n    room = Room.objects.create(name='Kitchen', house=house)\n\n    queryset = House.objects.only('name').prefetch_related('rooms')\n    with self.assertNumQueries(2):\n        house = queryset.first()\n    self.assertIs(Room.house.is_cached(room), True)\n\n    house = House.objects.only('name').first()\n    house = house.prefetch_related(None)\n    self.assertIs(Room.house.is_cached(room), False)\n", "def test_nested_prefetch_on_different_levels(self):\n    \"\"\"\n    Nested prefetches can be used at different levels of the relationship.\n    \"\"\"\n    houses = House.objects.prefetch_related(\n        Prefetch('rooms', queryset=Room.objects.filter(name='Kitchen')),\n        'rooms__house',\n    )\n    with self.assertNumQueries(3):\n        kitchens = [list(house.rooms.all()) for house in houses]\n\n    self.assertEqual(kitchens, [[self.room], []])\n    self.assertIs(House.rooms.is_cached(self.room), True)\n    with self.assertNumQueries(0):\n        self.room.house.name\n", "def test_custom_prefetch_related_queryset(self):\n    # Test that custom queryset method works correctly with prefetch_related\n    book1 = Book.objects.create(title='Book 1')\n    book2 = Book.objects.create(title='Book 2')\n    author1 = Author.objects.create(name='Author 1', first_book=book1)\n    author2 = Author.objects.create(name='Author 2', first_book=book2)\n\n    # Create a custom queryset that filters authors by name\n    custom_authors = Author.objects.custom_filter(name='Author 1')\n\n    with self.assertNumQueries(2):\n        books = Book.objects.prefetch_related(Prefetch('first_time_authors', queryset=custom_authors))\n        for book in books:\n            authors = list(book.first_time_authors.all())\n            self.assertEqual(authors, [author1] if book.title == 'Book 1' else [])\n", "def test_prefetch_related_with_to_attr_and_custom_queryset(self):\n    \"\"\"\n    Prefetching with to_attr and a custom queryset should not overwrite\n    the cached results.\n    \"\"\"\n    authors = AuthorWithAge.objects.prefetch_related(\n        Prefetch(\n            'author',\n            queryset=Author.objects.prefetch_related(\n                Prefetch('favorite_authors__first_book', to_attr='liked_books'),\n            ),\n            to_attr='author_with_likes',\n        ),\n    )\n    with self.assertNumQueries(4):\n        # AuthorWithAge -> Author -> FavoriteAuthors, Book\n        author_with_likes = [author.author_with_likes for author in authors]\n\n    self.assertEqual(author_with_likes[0].liked_books, [self.book2])\n    self.assertEqual(author_with_likes[1].liked_books, [])\n"], "sample_954": ["def test_abbreviation(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'HTML' in content\n    assert 'HyperText Markup Language' in content\n", "def test_domain_c(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert '.B foo(void)' in content\n    assert 'Function declaration.' in content\n    assert '.B bar(int a, int b)' in content\n    assert 'Function declaration with arguments.' in content\n    assert '.B void baz(void)' in content\n    assert 'Function declaration with return type.' in content\n    assert '.B int qux(int a, int b)' in content\n    assert 'Function declaration with return type and arguments.' in content\n", "def test_glossary(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'This is a glossary\\n' in content\n", "def test_domain_c(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert '\\n.B void\\n' in content\n    assert '\\n.B add_node_to_end\\\\fP (\\\\fBzend_llist *llist, void *data\\\\fP)\\n' in content\n", "def test_domain_objects(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    # test desc_signature nodes\n    assert '\\n.B obj1\\\\fR (arg1, arg2, \\\\fI*args\\\\fR, \\\\fI**kwargs\\\\fR) -> int\\n' in content\n    assert '\\n.B obj2\\\\fR (arg1, \\\\fI[arg2, arg3]\\\\fR) -> str\\n' in content\n\n    # test desc_annotation nodes\n    assert '\\n.B obj3\\\\fR (\\\\fBarg\\\\fR\\\\fI: annotation\\\\fR) -> None\\n' in content\n\n    # test desc_addname nodes\n    assert '\\n.B Exception.obj4\\\\fR (arg1) -> None\\n' in content\n\n    # test desc_type nodes\n    assert '\\n.B obj5\\\\fR (arg1) -> \\\\fIobj\\\\fR\\n' in content\n\n    # test desc_returns nodes\n    assert '\\n.B obj6\\\\fR (arg1) -> \\\\fIobj\\\\fR\\n' in content\n\n    # test desc_name nodes\n    assert '\\n.B module.obj7\\\\fR (arg1) -> None\\n' in content\n"], "sample_340": ["def test_run_before_unapplied(self):\n    \"\"\"\n    Makes sure the loader uses Migration.run_before even if the migration is not applied.\n    \"\"\"\n    # Load and test the plan\n    migration_loader = MigrationLoader(connection)\n    self.assertEqual(\n        migration_loader.graph.forwards_plan((\"migrations\", \"0002_second\")),\n        [\n            (\"migrations\", \"0001_initial\"),\n            (\"migrations\", \"0003_third\"),\n            (\"migrations\", \"0002_second\"),\n        ],\n    )\n", "def test_collect_sql(self):\n    \"\"\"\n    Test collecting SQL statements from a migration plan.\n    \"\"\"\n    migration_loader = MigrationLoader(connection)\n    plan = [\n        (migration_loader.get_migration(\"migrations\", \"0001\"), False),\n        (migration_loader.get_migration(\"migrations\", \"0002\"), False),\n    ]\n    statements = migration_loader.collect_sql(plan)\n    self.assertIsInstance(statements, list)\n    self.assertGreater(len(statements), 0)\n", "def test_loading_pyc_files(self):\n    \"\"\"MigrationLoader loads .pyc files only when .py files are missing.\"\"\"\n    with self.temporary_migration_module(module='migrations.test_migrations') as migration_dir:\n        # Compile .py files to .pyc files.\n        compileall.compile_dir(migration_dir, force=True, quiet=1, legacy=True)\n        loader = MigrationLoader(connection)\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n        self.assertIn(('migrations', '0002_second'), loader.disk_migrations)\n        # Remove .py files.\n        for name in os.listdir(migration_dir):\n            if name.endswith('.py'):\n                os.remove(os.path.join(migration_dir, name))\n        loader = MigrationLoader(connection)\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n        self.assertIn(('migrations', '0002_second'), loader.disk_migrations)\n", "def test_loading_replaces_partially_applied(self):\n    \"\"\"\n    Tests loading replaces migrations when some have been applied.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.record_applied(recorder, 'migrations', '0001_initial')\n    loader.build_graph()\n    self.assertIn(('migrations', '0002_second'), loader.graph.nodes)\n    self.assertNotIn(('migrations', '0003_replaces_1_2'), loader.graph.nodes)\n", "def test_loading_replaced(self):\n    \"Tests loading a replaced migration\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Loading with nothing applied should give us both nodes\n    self.assertEqual(\n        len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n        2,\n    )\n    # However, fake-apply the new migration and it should only use the old one\n    self.record_applied(recorder, 'migrations', '0002_replaced')\n    loader.build_graph()\n    self.assertEqual(\n        len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n        1,\n    )\n    # If we fake-apply the old migration, it should now use the new one\n    self.record_applied(recorder, 'migrations', '0001_initial')\n    loader.build_graph()\n    self.assertEqual(\n        len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n        1,\n    )\n    # However, if we unapply the new migration, it should use both nodes again\n    recorder.record_unapplied('migrations', '0002_replaced')\n    loader.build_graph()\n    self.assertEqual(\n        len([x for x in loader.graph.nodes if x[0] == \"migrations\"]),\n        2,\n    )\n"], "sample_419": ["def test_initial_form_count_with_data(self):\n    \"\"\"initial_form_count() returns INITIAL_FORMS from data.\"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"1\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"0\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"1\",\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertEqual(formset.initial_form_count(), 1)\n", "def test_formset_with_custom_renderer(self):\n    class CustomRenderer(TemplatesSetting):\n        form_template_name = \"custom/form_template.html\"\n        formset_template_name = \"custom/formset_template.html\"\n        error_list_template_name = \"custom/error_list_template.html\"\n\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"\",\n    }\n    ChoiceFormSet = formset_factory(Choice, renderer=CustomRenderer)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertEqual(formset.renderer, CustomRenderer)\n    self.assertEqual(formset.forms[0].renderer, CustomRenderer)\n    self.assertEqual(formset.management_form.renderer, CustomRenderer)\n    self.assertEqual(formset.non_form_errors().renderer, CustomRenderer)\n    self.assertEqual(formset.empty_form.renderer, CustomRenderer)\n", "def test_formset_with_custom_prefix(self):\n    \"\"\"\n    Formset uses the custom prefix for the management form and the forms.\n    \"\"\"\n    data = {\n        \"custom-TOTAL_FORMS\": \"2\",\n        \"custom-INITIAL_FORMS\": \"0\",\n        \"custom-MIN_NUM_FORMS\": \"0\",\n        \"custom-0-choice\": \"Zero\",\n        \"custom-0-votes\": \"0\",\n        \"custom-1-choice\": \"One\",\n        \"custom-1-votes\": \"1\",\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"custom\")\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.management_form.prefix, \"custom\")\n    self.assertEqual(formset.forms[0].prefix, \"custom-0\")\n    self.assertEqual(formset.forms[1].prefix, \"custom-1\")\n", "def test_custom_formset_prefix(self):\n    \"\"\"\n    A custom prefix can be passed to a formset, and it will be used for the\n    management form and individual forms.\n    \"\"\"\n    formset = self.make_choiceformset(prefix=\"custom_prefix\")\n    self.assertHTMLEqual(\n        str(formset.management_form),\n        '<input type=\"hidden\" name=\"custom_prefix-TOTAL_FORMS\" value=\"1\">'\n        '<input type=\"hidden\" name=\"custom_prefix-INITIAL_FORMS\" value=\"0\">'\n        '<input type=\"hidden\" name=\"custom_prefix-MIN_NUM_FORMS\" value=\"0\">'\n        '<input type=\"hidden\" name=\"custom_prefix-MAX_NUM_FORMS\" value=\"1000\">',\n    )\n    self.assertHTMLEqual(\n        str(formset.forms[0]),\n        '<div>Choice: <input type=\"text\" name=\"custom_prefix-0-choice\"></div>'\n        '<div>Votes: <input type=\"number\" name=\"custom_prefix-0-votes\"></div>',\n    )\n", "def test_extra_forms_empty_permitted(self):\n    \"\"\"\n    Extra forms are empty by default and this behavior can be modified by\n    the `empty_permitted` parameter of formset_factory.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=2, empty_permitted=False)\n    formset = ChoiceFormSet()\n    self.assertFalse(formset.forms[0].empty_permitted)\n    self.assertFalse(formset.forms[1].empty_permitted)\n\n    ChoiceFormSet = formset_factory(Choice, extra=2, empty_permitted=True)\n    formset = ChoiceFormSet()\n    self.assertTrue(formset.forms[0].empty_permitted)\n    self.assertTrue(formset.forms[1].empty_permitted)\n"], "sample_963": ["def test_restify_type_hints_Annotated():\n    from typing import Annotated\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":py:class:`str`\"\n", "def test_restify_type_hints_NoneType():\n    assert restify(NoneType) == \":py:obj:`None`\"\n", "def test_restify_type_hints_Ellipsis():\n    assert restify(Ellipsis) == \"...\"\n", "def test_restify_pep_604():\n    # test restify with UnionType (python 3.10 or above)\n    assert restify(int | str) == \":py:class:`int` | :py:class:`str`\"\n    assert restify(int | None) == \":py:class:`int` | :py:obj:`None`\"\n    assert restify(int | str | None) == \":py:class:`int` | :py:class:`str` | :py:obj:`None`\"\n", "def test_restify_type_hints_alias_with_type_vars():\n    MyListVar = List[T]\n    assert restify(MyListVar) == \":py:class:`~typing.List`\\\\ [:py:obj:`tests.test_util_typing.T`]\"\n"], "sample_1090": ["def test_nested_multiply():\n    with evaluate(False):\n        expr = (x * x) * (y * y)\n        assert expr.args == ((x * x), (y * y))\n        assert expr.args[0].args == (x, x)\n        assert expr.args[1].args == (y, y)\n", "def test_pow():\n    with evaluate(False):\n        assert (x + y)**2 == Pow(Add(x, y), 2)\n        assert 2**(x + y) == Pow(2, Add(x, y))\n        assert (x + y)**(x + y) == Pow(Add(x, y), Add(x, y))\n", "def test_mul():\n    with evaluate(False):\n        expr = x * x\n        assert isinstance(expr, Mul)\n        assert expr.args == (x, x)\n\n        with evaluate(True):\n            assert (x * x).args == (x**2,)\n\n        assert (x * x).args == (x, x)\n\n    assert isinstance(x * x, Pow)\n\n    with evaluate(False):\n        assert S.One * 1 == Mul(1, 1)\n        assert 1 * S.One == Mul(1, 1)\n\n        assert S(4) * 3 == Mul(4, 3)\n        assert 3 * S(4) == Mul(3, 4)\n\n        assert S(2) / 4 == Mul(2, S.One / 4)\n        assert S.One / 4 * 2 == Mul(S.One / 4, 2)\n\n        assert S(2) / 3 * 4 == Mul(S(2) / 3, 4)\n        assert 4 * S(2) / 3 == Mul(4, S(2) / 3)\n\n        assert S(2) / 4 * S(4) == Mul(S(2) / 4, S(4))\n        assert S(4) * S(2) / 4 == Mul(S(4), S(2) / 4)\n\n        assert S(6) / 3 * sqrt(3) == Mul(S(6) / 3, sqrt(3))\n        assert sqrt(3) * S(6) / 3 == Mul(sqrt(3), S(6) / 3)\n\n        assert S(1) / 2 * 10.333 == Mul(S(1) / 2, 10.333)\n        assert 10.333 * S(1) / 2 == Mul(10.333, S(1) / 2)\n\n        assert sqrt(2) * sqrt(2) == Mul(sqrt(2), sqrt(2))\n\n        assert S(1) / ", "def test_pow_with_evaluate():\n    with evaluate(False):\n        p = oo ** 2\n        assert isinstance(p, Pow) and p.args == (oo, 2)\n        p = oo ** -2\n        assert isinstance(p, Pow) and p.args == (oo, -2)\n        p = -oo ** 2\n        assert isinstance(p, Pow) and p.args == (-oo, 2)\n        p = 2 ** oo\n        assert isinstance(p, Pow) and p.args == (2, oo)\n        p = 2 ** -oo\n        assert isinstance(p, Pow) and p.args == (2, -oo)\n\n    with evaluate(True):\n        assert oo ** 2 == oo\n        assert oo ** -2 == 0\n        assert (-oo) ** 2 == oo\n        assert 2 ** oo == oo\n        assert 2 ** -oo == 0\n", "def test_evaluate():\n    with evaluate(True):\n        assert (x + x).evalf() == 2*x\n        assert (x + x).evalf(subs={x: 2}) == 4\n\n        assert (x + 2).evalf(subs={x: 3}) == 5.0\n        assert (2 + x).evalf(subs={x: 3}) == 5.0\n\n        assert (x - 2).evalf(subs={x: 3}) == 1.0\n        assert (2 - x).evalf(subs={x: 3}) == -1.0\n\n        assert (x * 2).evalf(subs={x: 3}) == 6.0\n        assert (2 * x).evalf(subs={x: 3}) == 6.0\n\n        assert (x / 2).evalf(subs={x: 3}) == 1.5\n        assert (2 / x).evalf(subs={x: 3}) == 2.0 / 3.0\n\n        assert (x ** 2).evalf(subs={x: 3}) == 9.0\n        assert (2 ** x).evalf(subs={x: 3}) == 8.0\n\n        assert (2 / x + x).evalf(subs={x: 3}) == 3.666666666666667\n        assert (x + 2 / x).evalf(subs={x: 3}) == 3.666666666666667\n\n        assert (2 / x - x).evalf(subs={x: 3}) == -2.6666666666666665\n        assert (x - 2 / x).evalf(subs={x: 3}) == 2.3333333333333335\n\n        assert (2 / x * x).evalf(subs={x: 3}) == 2.0\n        assert (x * 2 / x"], "sample_268": ["def test_tick_triggers_notify_file_changed(self):\n    mock_notify_file_changed = mock.MagicMock()\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n        with mock.patch.object(self.reloader, 'notify_file_changed', mock_notify_file_changed):\n            ticker = self.reloader.tick()\n            next(ticker)\n            self.increment_mtime(self.existing_file)\n            next(ticker)\n            mock_notify_file_changed.assert_called_once_with(self.existing_file)\n", "def test_check_server_status_raises_watchman_unavailable(self):\n    with mock.patch.object(self.reloader.client, 'query') as mocked_query:\n        mocked_query.side_effect = Exception('Test Exception')\n        with self.assertRaises(autoreload.WatchmanUnavailable):\n            self.reloader.check_server_status(inner_ex=Exception('Inner Exception'))\n", "    def test_tick_calls_notify_file_changed(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            with mock.patch.object(self.reloader, 'notify_file_changed') as mock_notify:\n                ticker = self.reloader.tick()\n                next(ticker)\n                self.increment_mtime(self.existing_file)\n                next(ticker)\n                mock_notify.assert_called_once_with(self.existing_file)\n", "def test_watch_dir_with_unresolvable_path(self, mocked_modules, notify_mock):\n    path = Path('unresolvable_directory')\n    self.reloader.watch_dir(path, '**/*.mo')\n    with mock.patch.object(Path, 'exists', return_value=False):\n        with self.tick_twice():\n            self.ensure_file(path / 'test.mo')\n    notify_mock.assert_not_called()\n", "def test_add_and_remove_files(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        self.increment_mtime(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n\n    new_file = self.ensure_file(self.tempdir / 'new_file.py')\n    with self.tick_twice():\n        self.increment_mtime(new_file)\n    self.assertEqual(notify_mock.call_count, 2)\n    self.assertCountEqual(notify_mock.call_args[0], [new_file])\n\n    new_file.unlink()\n    with self.tick_twice():\n        self.increment_mtime(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 3)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n"], "sample_63": ["    def test_load_template_from_string(self):\n        template = self.engine.from_string(\"Hello, {{ name }}!\")\n        context = Context({\"name\": \"World\"})\n        self.assertEqual(template.render(context), \"Hello, World!\")\n", "    def test_invalid_loader_configuration(self):\n        loaders = [123]  # Invalid loader configuration\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(loaders=loaders)\n", "    def test_template_libraries(self):\n        libraries = {'custom_tags': 'path.to.custom_tags'}\n        engine = Engine(libraries=libraries)\n        self.assertIn('custom_tags', engine.template_libraries)\n", "    def test_template_inheritance(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        self.assertEqual(\n            engine.render_to_string('child_template.html', {'content': 'test'}),\n            'base: test\\n',\n        )\n", "    def test_template_libraries(self):\n        libraries = {'my_library': 'path.to.my_library'}\n        engine = Engine(libraries=libraries)\n        self.assertIn('my_library', engine.template_libraries)\n"], "sample_1039": ["def test_print_negative_infinity():\n    assert mpp.doprint(-oo) == '<mrow><mo>-</mo><mi>&#x221E;</mi></mrow>'\n    assert mp.doprint(-oo) == '<apply><minus/><infinity/></apply>'\n", "def test_print_negative_infinity():\n    assert mpp.doprint(-oo) == '<mrow><mo>-</mo><mi>&#x221E;</mi></mrow>'\n    assert mp.doprint(-oo) == '<apply><minus/><infinity/></apply>'\n", "def test_presentation_mathml_negative_infinity():\n    mml = mpp._print(-oo)\n    assert mml.childNodes[0].nodeValue == '-'\n    assert mml.childNodes[1].childNodes[0].nodeValue == '&#x221E;'\n\n    mml = mp._print(-oo)\n    assert mml.nodeName == 'apply'\n    assert mml.childNodes[0].nodeName == 'minus'\n    assert mml.childNodes[1].nodeName == 'infinity'\n", "def test_print_neg_infinity():\n    assert mpp.doprint(-oo) == '<mrow><mo>-</mo><mn>&#x221E;</mn></mrow>'\n    assert mp.doprint(-oo) == '<apply><minus/><infinity/></apply>'\n", "def test_print_neg_inf():\n    assert mpp.doprint(S.NegativeInfinity) == '<mrow><mo>-</mo><mi>&#x221E;</mi></mrow>'\n    assert mp.doprint(S.NegativeInfinity) == '<apply><minus/><infinity/></apply>'\n"], "sample_361": ["def test_urlize_with_trim_url_limit(self):\n    value = 'Search for https://www.example.com/this-is-a-long-url and see.'\n    output = 'Search for <a href=\"https://www.example.com/this-is-a-long-url\">https://www.example.com/this-is-...</a> and see.'\n    self.assertEqual(urlize(value, trim_url_limit=25), output)\n", "def test_urlize_trim_url_limit(self):\n    value = 'This is a very long URL: http://example.com/a_very_long_path_to_a_resource'\n    output = 'This is a very long URL: <a href=\"http://example.com/a_very_long_path_to_a_resource\">http://example.com/a_very_lo...</a>'\n    self.assertEqual(urlize(value, trim_url_limit=30), output)\n", "def test_urlize_nofollow(self):\n    tests = (\n        (\n            'Visit our website at example.com.',\n            'Visit our website at <a href=\"http://example.com\" rel=\"nofollow\">example.com</a>.',\n        ),\n        (\n            'Email me at foo@example.com.',\n            'Email me at <a href=\"mailto:foo@example.com\">foo@example.com</a>.',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_trim_url(self):\n    tests = (\n        (\n            'Search for https://www.google.com/search?q=django&oq=django&aqs=chrome..69i57j0l6.4711j0j7&sourceid=chrome&ie=UTF-8 and see.',\n            20,\n            'Search for <a href=\"https://www.google.com/search?q=django&oq=django&aqs=chrome..69i57j0j7&sourceid=chrome&ie=UTF-8\">https://www.google.com/se...</a> and see.'\n        ),\n        (\n            'Check out this longurl.com/withsomerandomcharacters that needs to be trimmed.',\n            15,\n            'Check out this <a href=\"http://longurl.com/withsomerandomcharacters\">longurl.com/withso...</a> that needs to be trimmed.'\n        ),\n    )\n    for value, limit, output in tests:\n        with self.subTest(value=value, limit=limit):\n            self.assertEqual(urlize(value, trim_url_limit=limit), output)\n", "def test_urlize_truncation(self):\n    tests = (\n        ('http://example.com/' + 'a' * 100, 'http://example.com/' + 'a' * 96 + '\u2026', 100),\n        ('www.example.com/' + 'a' * 100, 'www.example.com/' + 'a' * 96 + '\u2026', 100),\n        ('<http://example.com/?q=' + 'a' * 100 + '>', '<http://example.com/?q=' + 'a' * 96 + '\u2026>', 100),\n        ('(http://example.com/?q=' + 'a' * 100 + ')', '(http://example.com/?q=' + 'a' * 94 + '\u2026)', 100),\n    )\n    for value, output, limit in tests:\n        with self.subTest(value=value, limit=limit):\n            self.assertEqual(urlize(value, trim_url_limit=limit), output)\n"], "sample_240": ["def test_custom_key_salt(self):\n    \"\"\"\n    A valid token can be created with a custom key_salt by using the\n    PasswordResetTokenGenerator.key_salt attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    custom_key_salt = 'custom.key.salt'\n    # Create and check a token with a different key_salt.\n    p0 = PasswordResetTokenGenerator()\n    p0.key_salt = custom_key_salt\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default key_salt.\n    p1 = PasswordResetTokenGenerator()\n    self.assertNotEqual(p1.key_salt, custom_key_salt)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different key_salt don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_changed_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_different_algorithm(self):\n        \"\"\"\n        A valid token can be created with a hashing algorithm other than\n        DEFAULT_HASHING_ALGORITHM by using the PasswordResetTokenGenerator.algorithm\n        attribute.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        new_algorithm = 'sha256'\n        # Create and check a token with a different hashing algorithm.\n        p0 = PasswordResetTokenGenerator()\n        p0.algorithm = new_algorithm\n        tk0 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk0), True)\n        # Create and check a token with the default hashing algorithm.\n        p1 = PasswordResetTokenGenerator()\n        self.assertEqual(p1.algorithm, settings.DEFAULT_HASHING_ALGORITHM)\n        self.assertNotEqual(p1.algorithm, new_algorithm)\n        tk1 = p1.make_token(user)\n        # Tokens created with a different hashing algorithm don't validate.\n        self.assertIs(p0.check_token(user, tk1), False)\n        self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_custom_hashing_algorithm(self):\n    \"\"\"\n    A valid token can be created with a custom hashing algorithm by using\n    the PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    custom_algorithm = 'sha512'\n    # Create and check a token with a custom hashing algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = custom_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default hashing algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertNotEqual(p1.algorithm, custom_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a custom hashing algorithm don't validate with the default.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n"], "sample_651": ["def test_warns_context_manager_with_invalid_call(self) -> None:\n        pass\n\n    with pytest.raises(TypeError, match=r\".* must be callable\"):\n        with pytest.warns(UserWarning):\n            invalid_call()\n", "def test_warns_with_unexpected_keyword_arguments(recwarn: WarningsRecorder) -> None:\n    with pytest.raises(TypeError) as excinfo:\n        pytest.warns(UserWarning, foo=\"bar\")  # type: ignore\n    assert \"Unexpected keyword arguments\" in str(excinfo.value)\n", "def test_warns_no_match_multiple(self) -> None:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")  # if anything is re-emitted\n        with pytest.warns(UserWarning, match=\"user warning\"):\n            warnings.warn(\"non-matching warning 1\", UserWarning)\n            warnings.warn(\"non-matching warning 2\", UserWarning)\n", "def test_warns_context_manager_without_match(self) -> None:\n    with pytest.warns(UserWarning):\n        warnings.warn(\"test warning\", UserWarning)\n\n    with pytest.warns():\n        with pytest.raises(pytest.fail.Exception):\n            with pytest.warns(UserWarning):\n                warnings.warn(\"test warning\", DeprecationWarning)\n", "def test_recwarn_with_match(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n        import pytest\n\n            with pytest.warns(UserWarning, match='hello'):\n                warnings.warn(\"hello\")\n            with pytest.warns(UserWarning, match='goodbye'):\n                warnings.warn(\"goodbye\")\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n"], "sample_112": ["def test_cell_count(self):\n    \"\"\"\n    Test the cell_count filter for tabular inline.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[self.article.pk]))\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.change_view(request, str(self.article.pk))\n\n    # Get the first inline_admin_form from the response context\n    inline_admin_form = response.context_data['inline_admin_formsets'][0][0]\n\n    # Calculate the expected cell count\n    expected_count = 1  # Hidden cell with hidden 'id' field\n    for fieldset in inline_admin_form:\n        # Loop through all the fields (one per cell)\n        for line in fieldset:\n            for field in line:\n                expected_count += 1\n    if inline_admin_form.formset.can_delete:\n        # Delete checkbox\n        expected_count += 1\n\n    # Call the cell_count filter and compare the result with the expected count\n    self.assertEqual(cell_count(inline_admin_form), expected_count)\n", "def test_cell_count(self):\n    \"\"\"\n    cell_count filter should return correct count of cells in a tabular inline.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[self.article.pk]))\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    inline = admin.get_inline_instances(request)[0]\n    inline_formset = inline.get_formset(request)\n    inline_form = inline_formset.forms[0]\n    cell_count_value = cell_count(inline_form)\n    # Add 1 for the hidden 'id' field and 1 for the delete checkbox if possible\n    expected_cell_count = sum(len(fieldset) for fieldset in inline_form) + 2\n    if not inline_formset.can_delete:\n        expected_cell_count -= 1\n    self.assertEqual(cell_count_value, expected_cell_count)\n", "def test_cell_count(self):\n    \"\"\"\n    Test the cell_count filter.\n    \"\"\"\n    class MockInlineAdminForm:\n            self.formset = type('MockFormSet', (), {'can_delete': True})()\n            self.fields = fields\n\n    # Test with no fields and can_delete\n    form = MockInlineAdminForm(fields=[])\n    self.assertEqual(cell_count(form), 2)  # Hidden cell with hidden 'id' field and Delete checkbox\n\n    # Test with one field and can_delete\n    form = MockInlineAdminForm(fields=['field1'])\n    self.assertEqual(cell_count(form), 3)  # Hidden cell with hidden 'id' field, one field, and Delete checkbox\n\n    # Test with multiple fields and can_delete\n    form = MockInlineAdminForm(fields=['field1', 'field2', 'field3'])\n    self.assertEqual(cell_count(form), 5)  # Hidden cell with hidden 'id' field, three fields, and Delete checkbox\n\n    # Test with no fields and no can_delete\n    form.formset.can_delete = False\n    self.assertEqual(cell_count(form), 1)  # Only hidden cell with hidden 'id' field\n", "def test_cell_count(self):\n    \"\"\"\n    cell_count template filter should return the correct number of cells in a tabular inline.\n    \"\"\"\n    from django.contrib.admin.options import InlineModelAdmin\n    from django.contrib.admin.helpers import AdminForm\n    from django.contrib.admin import site\n    from django.contrib.auth.models import Group\n\n    class TestInline(InlineModelAdmin):\n        model = Group\n        can_delete = True\n        fields = ('name',)\n\n    request = self.request_factory.get('/')\n    request.user = self.superuser\n\n    inline = TestInline(parent_model=User, admin_site=site)\n    formset = inline.get_formset(request)()\n    inline_admin_form = AdminForm(formset.empty_form, list(formset.fields.keys()), {})\n\n    # Hidden cell with hidden 'id' field + 1 field cell + 1 delete checkbox cell\n    self.assertEqual(cell_count(inline_admin_form), 3)\n", "def test_cell_count(self):\n    \"\"\"\n    cell_count filter should return the number of cells used in a tabular inline.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:admin_views_article_add'))\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.add_view(request)\n    inline_admin_form = response.context_data['inline_admin_formset'][0].form\n    count = cell_count(inline_admin_form)\n    # Add one for the hidden 'id' field and one for the delete checkbox\n    self.assertEqual(count, len(inline_admin_form.fields) + 2)\n"], "sample_900": ["def test_early_stopping_multiple_classes():\n    # Test early stopping with multiple classes\n    X, y = make_multilabel_classification(n_samples=50, n_classes=3, random_state=0, return_indicator=True)\n    tol = 0.2\n    clf = MLPClassifier(tol=tol, max_iter=3000, solver='sgd', early_stopping=True)\n    clf.fit(X, y)\n    assert clf.max_iter > clf.n_iter_\n\n    valid_scores = clf.validation_scores_\n    best_valid_score = clf.best_validation_score_\n    assert max(valid_scores) == best_valid_score\n    assert best_valid_score + tol > valid_scores[-2]\n    assert best_valid_score + tol > valid_scores[-1]\n", "def test_batch_size():\n    # Test batch_size parameter\n    X = X_digits_binary[:100]\n    y = y_digits_binary[:100]\n    batch_sizes = [1, 20, 50, 100]\n    for batch_size in batch_sizes:\n        clf = MLPClassifier(solver='sgd', batch_size=batch_size, max_iter=1, random_state=1)\n        clf.fit(X, y)\n        assert clf.n_iter_ == 1\n", "def test_early_stopping_validation_split():\n    # Test that validation_fraction is used to split data for early stopping\n    X, y = make_classification(n_samples=200, n_features=10, n_classes=2, random_state=0)\n    mlp = MLPClassifier(early_stopping=True, validation_fraction=0.2, random_state=0)\n    mlp.fit(X, y)\n    assert mlp.validation_fraction == 0.2\n    assert len(mlp.validation_scores_) == mlp.n_iter_\n    assert len(mlp.loss_curve_) == mlp.n_iter_\n    assert len(mlp.coefs_) > 0\n    assert len(mlp.intercepts_) > 0\n", "def test_sgd_classification():\n    # Test sgd on classification.\n    # It should achieve a score higher than 0.95 for the binary and multi-class\n    # versions of the digits dataset.\n    X_train = X_digits_binary[:150]\n    y_train = y_digits_binary[:150]\n    X_test = X_digits_binary[150:]\n    expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)\n\n    for activation in ACTIVATION_TYPES:\n        mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=50,\n                            max_iter=150, shuffle=True, random_state=1,\n                            activation=activation)\n        mlp.fit(X_train, y_train)\n        y_predict = mlp.predict(X_test)\n        assert mlp.score(X_train, y_train) > 0.95\n        assert ((y_predict.shape[0], y_predict.dtype.kind) ==\n                expected_shape_dtype)\n", "def test_early_stopping_no_early_stopping():\n    # Test early stopping when there is no improvement\n    X = np.array([[0], [1], [2], [3], [4], [5]])\n    y = np.array([0, 0, 0, 1, 1, 1])\n    mlp = MLPClassifier(early_stopping=True, max_iter=10, validation_fraction=0.2, random_state=0)\n    mlp.fit(X, y)\n    assert mlp.n_iter_ == 10\n    assert len(mlp.validation_scores_) == 10\n"], "sample_533": ["def test_algorithm_corner_mask(algorithm, corner_mask):\n    z = np.array([[1.0, 2.0], [3.0, 4.0]])\n    plt.contourf(z, algorithm=algorithm, corner_mask=corner_mask)\n", "def test_linestyles_list(style):\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    # Test linestyles as a list\n    fig5, ax5 = plt.subplots()\n    CS5 = ax5.contour(X, Y, Z, 6, colors='k', linestyles=[style])\n    ax5.clabel(CS5, fontsize=9, inline=True)\n    ax5.set_title(f'Single color - positive contours {style}')\n    assert CS5.linestyles == [style]\n", "def test_clabel_transform(transform, tol, expected_x, expected_y):\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    pts = np.array([(5.0, 5.0)])\n    labels = cs.clabel(manual=pts, transform=transform)\n\n    assert len(labels) == 1\n    label = labels[0]\n    assert_array_almost_equal(label.get_position(), [expected_x, expected_y], decimal=tol)\n", "def test_contour_linewidths():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, linewidths=[1.0, 2.0, 3.0])\n    assert cs.tlinewidths == [(1.0,), (2.0,), (3.0,)]\n", "def test_contourf_log_extension_bool():\n    # Test that contourf with lognorm is extended correctly for boolean data\n    fig, ax = plt.subplots()\n    # make data set with boolean values\n    data = np.array([[False, True], [True, False]])\n    levels = np.array([0, 1])\n    # extend data from levels\n    c = ax.contourf(data, levels=levels,\n                    norm=LogNorm(vmin=levels.min(), vmax=levels.max()),\n                    extend='both')\n    cb = plt.colorbar(c, ax=ax)\n"], "sample_578": ["def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == pytest.approx(baseline)\n        assert verts[3, 1] == y[i] + baseline\n", "def test_mapped_edgecolor(self, x, y, color):\n\n    p = Plot(x, y, edgecolor=color).add(Bars()).plot()\n    ax = p._figure.axes[0]\n    ecs = ax.collections[0].get_edgecolors()\n    C0, C1, C2, *_ = p._theme[\"axes.prop_cycle\"].by_key()[\"color\"]\n    expected = to_rgba_array([C0, C1, C2, C0, C2], 1)\n    assert_array_equal(ecs, expected)\n", "def test_baseline(self, x, y):\n\n    baseline = 2\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == pytest.approx(baseline)\n        assert verts[3, 1] == pytest.approx(y[i] + baseline)\n", "    def test_baseline(self, x, y):\n\n        baseline = 2\n        p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == baseline\n            assert verts[3, 1] == y[i] + baseline\n", "def test_mapped_edgecolor(self, x, y, color):\n\n    edgecolor = \".3\"\n    p = Plot(x, y, color=color, edgecolor=edgecolor).add(Bars()).plot()\n    ax = p._figure.axes[0]\n    ecs = ax.collections[0].get_edgecolors()\n    expected = to_rgba_array([edgecolor] * len(x), 1)\n    assert_array_equal(ecs, expected)\n"], "sample_126": ["def test_mti_inheritance_model_addition(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([Animal], [Animal, ModelState('app', 'Dog', [], bases=('app.Animal',))])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "def test_mti_inheritance_model_addition(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([Animal], [Animal, ModelState('app', 'Dog', [], bases=('app.Animal',))])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "def test_mti_inheritance_model_addition(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([Animal], [Animal, ModelState('app', 'Dog', [], bases=('app.Animal',))])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog', options={'proxy': False})\n", "def test_alter_field_to_not_null_oneoff_default(self, mocked_ask_method):\n    \"\"\"\n    #23609 - Tests autodetection of nullable to non-nullable alterations.\n    \"\"\"\n    changes = self.get_changes([self.author_name_null], [self.author_name])\n    self.assertEqual(mocked_ask_method.call_count, 1)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=False)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=\"Some Name\")\n", "def test_add_non_null_field(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Adding a NOT NULL field without default should prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_age])\n    self.assertEqual(mocked_ask_method.call_count, 1)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n"], "sample_262": ["def test_lazy_pickling(self):\n    \"\"\"\n    Test pickling and unpickling of lazy objects.\n    \"\"\"\n    import pickle\n\n    original_object = \"Lazy pickled text\"\n    lazy_obj = lazy(lambda: original_object, str)\n\n    pickled_obj = pickle.dumps(lazy_obj())\n    unpickled_obj = pickle.loads(pickled_obj)\n\n    self.assertEqual(unpickled_obj, original_object)\n", "def test_classproperty_docstring(self):\n    class Foo:\n        @classproperty\n            \"\"\"This is a class property.\"\"\"\n            return 123\n\n    self.assertEqual(Foo.foo.__doc__, \"This is a class property.\")\n", "def test_lazy_text_encoding(self):\n    \"\"\"\n    The %s operator works correctly for Promises returning text.\n    \"\"\"\n    original_object = 'Lazy translation text %s'\n    lazy_obj = lazy(lambda: original_object, str)\n    self.assertEqual(original_object % 'value', lazy_obj() % 'value')\n", "    def test_keep_lazy_text(self):\n        \"\"\"\n        keep_lazy_text() decorator should return a lazy object that evaluates\n        to a string.\n        \"\"\"\n        @keep_lazy_text\n            return str(a) + str(b)\n\n        lazy_obj = concatenate('Hello, ', lazy(lambda: 'World!', str))\n        self.assertIsInstance(lazy_obj, Promise)\n        self.assertEqual(str(lazy_obj), 'Hello, World!')\n", "    def test_lazy_pickling(self):\n        \"\"\"\n        Test that lazy objects are picklable.\n        \"\"\"\n        import pickle\n\n        lazy_obj = lazy(lambda: 'pickled', str)\n        pickled_obj = pickle.dumps(lazy_obj())\n        restored_obj = pickle.loads(pickled_obj)\n\n        self.assertEqual(lazy_obj(), restored_obj)\n"], "sample_148": ["def test_display_for_value_boolean(self):\n    self.assertEqual(\n        display_for_value(True, '', boolean=True),\n        '<img src=\"/static/admin/img/icon-yes.svg\" alt=\"True\">'\n    )\n    self.assertEqual(\n        display_for_value(False, '', boolean=True),\n        '<img src=\"/static/admin/img/icon-no.svg\" alt=\"False\">'\n    )\n", "def test_help_text_for_field(self):\n    \"\"\"\n    Tests for help_text_for_field\n    \"\"\"\n    self.assertEqual(\n        help_text_for_field(\"title\", Article),\n        \"The title of the article.\"\n    )\n    self.assertEqual(\n        help_text_for_field(\"hist\", Article),\n        \"\"\n    )\n    self.assertEqual(\n        help_text_for_field(\"__str__\", Article),\n        \"\"\n    )\n    self.assertEqual(\n        help_text_for_field(\"unknown\", Article),\n        \"\"\n    )\n    self.assertEqual(\n        help_text_for_field(\"test_from_model\", Article),\n        \"\"\n    )\n    class MockModelAdmin:\n            pass\n        test_from_model.help_text = \"Help text for the test_from_model function.\"\n    self.assertEqual(\n        help_text_for_field(\"test_from_model\", Article, model_admin=MockModelAdmin),\n        \"Help text for the test_from_model function.\"\n    )\n", "def test_file_field_display_for_field(self):\n    display_value = display_for_field('test.txt', models.FileField(), self.empty_value)\n    self.assertEqual(display_value, '<a href=\"test.txt\">test.txt</a>')\n", "def test_json_display_for_value(self):\n    tests = [\n        ({'a': {'b': 'c'}}, '{\"a\": {\"b\": \"c\"}}'),\n        (['a', 'b'], '[\"a\", \"b\"]'),\n        ('a', '\"a\"'),\n        ({('a', 'b'): 'c'}, \"{('a', 'b'): 'c'}\"),  # Invalid JSON.\n    ]\n    for value, display_value in tests:\n        with self.subTest(value=value):\n            self.assertEqual(\n                display_for_value(value, self.empty_value),\n                display_value,\n            )\n", "def test_file_field_display_for_field(self):\n    \"\"\"\n    Regression test for #12707: display_for_field should handle FileField value.\n    \"\"\"\n    file_field = models.FileField()\n    file_field.name = 'file.txt'\n    file_field.storage = SimpleStorage()\n    file_field.upload_to = 'test_directory'\n    file_field.save_form_data(None, 'test_content')\n\n    display_value = display_for_field(file_field, file_field, self.empty_value)\n    expected_value = '<a href=\"%stest_directory/file.txt\">file.txt</a>' % settings.MEDIA_URL\n    self.assertHTMLEqual(display_value, expected_value)\n"], "sample_722": ["def test_k_means_empty_input():\n    # Test KMeans with empty input\n    X_empty = np.empty((0, 2))\n    km = KMeans(n_clusters=2)\n    assert_raises_regex(ValueError, \"n_samples=0 should be >= n_clusters=2\", km.fit, X_empty)\n", "def test_k_means_precompute_distances_flag_false():\n    # check that the inertia decreases when precompute_distances is False\n    km_precomputed = KMeans(n_clusters=n_clusters, random_state=42, precompute_distances=True).fit(X)\n    km_not_precomputed = KMeans(n_clusters=n_clusters, random_state=42, precompute_distances=False).fit(X)\n    assert_less(km_not_precomputed.inertia_, km_precomputed.inertia_)\n", "def test_k_means_init_shape_mismatch():\n    # Test for error when initial centers shape does not match data or n_clusters\n    init_centers = np.array([[0.0, 0.0], [5.0, 5.0]])\n    km = KMeans(init=init_centers, n_clusters=3, n_init=1)\n    msg = \"The shape of the initial centers \\\\(\\(2L?, 2L?\\)\\) does not match the number of clusters 3\"\n    assert_raises_regex(ValueError, msg, km.fit, X)\n", "def test_k_means_single_iteration():\n    # Test that the algorithm converges in a single iteration when\n    # the initial centers are the actual cluster centers\n    km = KMeans(n_clusters=n_clusters, init=centers.copy(), n_init=1, max_iter=1)\n    km.fit(X)\n    # Check that the labels are correct\n    assert_array_equal(km.labels_, true_labels)\n    # Check that the cluster centers are unchanged\n    assert_array_equal(km.cluster_centers_, centers)\n", "def test_k_means_max_iter():\n    km = KMeans(n_clusters=n_clusters, max_iter=1, random_state=42)\n    km.fit(X)\n    assert_true(km.n_iter_ <= 1)\n"], "sample_989": ["def test_Float_srepr():\n    assert srepr(Float('1.0', precision=15)) == \"Float('1.0', precision=15)\"\n    assert srepr(Float('1.0', dps=15)) == \"Float('1.0', dps=15)\"\n", "def test_Integer_gcd_lcm_cofactors_with_Float():\n    assert Integer(4).gcd(Float(2.0)) == S.One\n    assert Integer(4).lcm(Float(2.0)) == Float(8.0)\n    assert Integer(4).cofactors(Float(2.0)) == (S.One, Float(2.0), Integer(4))\n", "def test_Float_from_numpy():\n    from sympy.utilities.pytest import skip\n    from sympy.external import import_module\n    np = import_module('numpy')\n    if not np:\n        skip('numpy not installed. Abort numpy tests.')\n\n    f = Float(np.float64(1.23456789))\n    assert f._mpf_ == (0, 123456789, -52, 53)\n    assert f._prec == 53\n", "def test_issue_10020_exponents():\n    assert oo**(S.NaN) is S.NaN\n    assert oo**(I*zoo) is S.ComplexInfinity\n    assert oo**(-I*zoo) is S.Zero\n    assert (-oo)**(I*zoo) is S.ComplexInfinity\n    assert (-oo)**(-I*zoo) is S.Zero\n", "def test_Rational_comparison_with_non_rational():\n    # Test comparison of Rational with non-rational types\n    assert Rational(1, 2) != 0.5\n    assert Rational(1, 2) == S.Half\n    assert Rational(1, 2) != 0.5000000000000001\n    assert Rational(1, 2) != 0.49999999999999994\n    assert Rational(1, 2) != \"0.5\"\n    assert Rational(1, 2) != Float(\"0.5\")\n    assert Rational(1, 2) != Float(\"0.5\", precision=53)\n    assert Rational(1, 2) != Float(\"0.49999999999999994\", precision=53)\n    assert Rational(1, 2) != Float(\"0.5000000000000001\", precision=53)\n    assert Rational(1, 2) != pi\n    assert Rational(1, 2) != I\n    assert Rational(1, 2) != oo\n    assert Rational(1, 2) != zoo\n    assert Rational(1, 2) != nan\n    assert Rational(1, 2) != Symbol(\"x\")\n"], "sample_695": ["def test_fs_collector_from_parent():\n    parent = nodes.Node.from_parent(None, name='parent', fspath=legacy_path('/path/to/parent'))\n    path = Path('/path/to/child')\n    collector = nodes.FSCollector.from_parent(parent, path=path)\n    assert collector.path == path\n    assert collector.parent == parent\n    assert collector.name == 'child'\n    assert collector.nodeid == 'child'\n", "def test_fs_collector_from_parent(request, tmp_path: Path) -> None:\n    \"\"\"\n    Verify the FSCollector.from_parent method with different parameters.\n    \"\"\"\n    path = tmp_path / \"test_file.py\"\n    path.touch()\n    fsc = nodes.FSCollector.from_parent(request.session, path=path)\n    assert fsc.path == path\n    assert fsc.name == \"test_file.py\"\n\n    fsc = nodes.FSCollector.from_parent(request.session, fspath=str(path))\n    assert fsc.path == path\n    assert fsc.name == \"test_file.py\"\n\n    fsc = nodes.FSCollector.from_parent(request.session, fspath=str(path), name=\"custom_name\")\n    assert fsc.path == path\n    assert fsc.name == \"custom_name\"\n", "def test_fs_collector_init_path_and_nodeid() -> None:\n    \"\"\"Test FSCollector init with path and nodeid.\"\"\"\n\n    class FakeSession:\n        config = type(\"Config\", (), {\"rootpath\": Path(\"/\")})\n        _initialpaths = frozenset()\n        _node_location_to_relpath = lambda self, path: path\n\n    session = cast(pytest.Session, FakeSession)\n    fspath = legacy_path(\"/test/path\")\n    nodeid = \"custom/nodeid\"\n    collector = nodes.FSCollector(fspath=fspath, path=Path(\"/test/path\"), nodeid=nodeid, session=session)\n\n    assert collector.path == Path(\"/test/path\")\n    assert collector.nodeid == nodeid\n", "def test_node_parent_set_after_from_parent(tmp_path: Path) -> None:\n    node = nodes.Node.from_parent(parent=None, name=\"test\", path=tmp_path)\n    assert node.parent is None\n", "def test_getparent_returns_correct_parent(pytester: Pytester) -> None:\n    \"\"\"Test the getparent method returns the correct parent node.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            class ChildNode(nodes.Node):\n                pass\n\n            class ParentNode(nodes.Node):\n                pass\n\n            parent_node = ParentNode.from_parent(request.session, name=\"parent\")\n            child_node = ChildNode.from_parent(parent_node, name=\"child\")\n\n            return parent_node, child_node\n\n            parent_node, child_node = setup_nodes\n            assert child_node.getparent(nodes.Node) == parent_node\n    \"\"\"\n    )\n\n    pytester.runpytest()\n"], "sample_139": ["def test_add_view_with_initial_data(self):\n    \"\"\"\n    The add view can be initialized with initial data.\n    \"\"\"\n    superuser = self._create_superuser('superuser')\n    self.client.force_login(superuser)\n    initial_data = {'name': 'Test band'}\n    add_url = reverse('admin:admin_changelist_band_add')\n    response = self.client.get(add_url, {'name': initial_data['name']})\n    self.assertEqual(response.status_code, 200)\n    self.assertContains(response, 'value=\"Test band\"')\n", "def test_dynamic_search_fields_without_fields_method(self):\n    child = self._create_superuser('child')\n    class ChildWithoutFieldsMethodAdmin(admin.ModelAdmin):\n        list_display = ['name', 'age']\n        search_fields = ['name', 'age']\n            return self.search_fields\n    m = ChildWithoutFieldsMethodAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request('/child/', child)\n    response = m.changelist_view(request)\n    self.assertEqual(response.context_data['cl'].search_fields, ('name', 'age'))\n", "def test_get_changelist(self):\n    \"\"\"\n    Regression tests for #16257: get_changelist should return correct instance.\n    \"\"\"\n    parent = Parent.objects.create(name='parent')\n    for i in range(1, 10):\n        Child.objects.create(id=i, name='child %s' % i, parent=parent, age=i)\n\n    m = DynamicListDisplayLinksChildAdmin(Child, custom_site)\n    superuser = self._create_superuser('superuser')\n    request = self._mocked_authenticated_request('/child/', superuser)\n    changelist = m.get_changelist(request)\n    self.assertIsInstance(changelist, ChangeList)\n    self.assertEqual(changelist.model, Child)\n", "def test_custom_list_editable(self):\n    \"\"\"\n    Regression test for #19340: Allow customization of list_editable fields.\n    \"\"\"\n    parent = Parent.objects.create(name='parent')\n    for i in range(10):\n        Child.objects.create(name='child %s' % i, parent=parent, age=i)\n\n    class CustomListEditableChildAdmin(ChildAdmin):\n            return ['name', 'age']\n\n    m = CustomListEditableChildAdmin(Child, custom_site)\n    superuser = self._create_superuser('superuser')\n    request = self._mocked_authenticated_request('/child/', superuser)\n    response = m.changelist_view(request)\n    for i in range(10):\n        self.assertContains(response, '<input name=\"form-%d-name\"' % i)\n        self.assertContains(response, '<input name=\"form-%d-age\"' % i)\n\n    list_editable = m.get_list_editable(request)\n    self.assertEqual(list_editable, ['name', 'age'])\n", "def test_actions_on_bottom(self):\n    \"\"\"\n    Regression test for #24068: ModelAdmin.actions_on_top and\n    ModelAdmin.actions_on_bottom are taken into account in the template.\n    \"\"\"\n    superuser = self._create_superuser('superuser')\n    self.client.force_login(superuser)\n    changelist_url = reverse('admin:admin_changelist_swallow_changelist')\n    Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n    Swallow.objects.create(origin='Swallow B', load=2, speed=2)\n    response = self.client.get(changelist_url)\n    self.assertContains(response, '<div class=\"actions\">')\n    self.assertContains(response, '<div class=\"paginator\">')\n    self.assertContains(response, '<div id=\"changelist-form\" class=\"actions\">')\n    self.assertContains(response, '<div class=\"actions bottom\">')\n    self.assertContains(response, '<div class=\"paginator bottom\">')\n\n    m = SwallowAdmin(Swallow, custom_site)\n    m.actions_on_top = False\n    m.actions_on_bottom = True\n    custom_site.unregister(Swallow)\n    custom_site.register(Swallow, m)\n    response = self.client.get(changelist_url)\n    self.assertNotContains(response, '<div class=\"actions\">')\n    self.assertNotContains(response, '<div class=\"paginator\">')\n    self.assertNotContains(response, '<div id=\"changelist-form\" class=\"actions\">')\n    self.assertContains(response, '<div class=\"actions bottom\">')\n    self.assertContains(response, '<div class=\"paginator bottom\">')\n\n    m.actions_on_top = True\n    m.actions_on_bottom = False\n    custom_site.unregister(Swallow)\n    custom_site.register(Swallow, m)\n    response = self.client.get(changelist_url)\n    self.assertContains(response,"], "sample_938": ["def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'http://www.sphinx-doc.org/' in content\n", "def test_field_list(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'Field Name 1:' in content\n    assert 'Field Name 2:' in content\n    assert 'Field Value 1' in content\n    assert 'Field Value 2' in content\n", "def test_invalid_man_pages(app, status, warning):\n    app.build()\n    assert \"unknown document\" in warning.getvalue()\n", "def test_citations(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert '[Knuth1992]' in content\n    assert '[Knuth1984]' in content\n    assert '[Williams1992]' in content\n", "def test_inline_markup(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert '\\\\fBbold\\\\fP' in content\n    assert '\\\\fIitalic\\\\fP' in content\n    assert '\\\\fCconstant\\\\fP' in content\n    assert '\\\\fRraw role' in content\n    assert '\\\\fR*emphasis*\\\\fP' in content\n    assert '\\\\fR**strong emphasis**\\\\fP' in content\n    assert '\\\\fR`interpreted text`\\\\fP' in content\n    assert '\\\\fR``literal text``\\\\fP' in content\n    assert '\\\\fR\\\\ \\\\(math\\\\)\\\\ \\\\(inline math\\\\)\\\\fP' in content\n"], "sample_7": ["def test_column_repr(Column):\n    \"\"\"Test the __repr__ method of Column.\"\"\"\n    c = Column([1, 2, 3], name='a', dtype=int, unit='mJy', format='%i',\n               description='test column', meta={'c': 8, 'd': 12})\n    expected_repr = \"<Column name='a' dtype='int64' length=3>\\n1\\n2\\n3\"\n    assert repr(c) == expected_repr\n", "def test_insert_with_quantity(Column):\n    c = Column([1, 2, 3], name='a', dtype=\"f8\", unit=\"m\")\n    q = [4, 5, 6] * u.km\n\n    c1 = c.insert(1, q)\n    assert np.all(c1 == [1, 4000, 5000, 6000, 2, 3])\n    assert c1.unit is u.m\n", "def test_column_format_setter_invalid_format():\n    \"\"\"Test that setting an invalid format raises a ValueError.\"\"\"\n    c = table.Column([1, 2, 3], name='a')\n    with pytest.raises(ValueError, match=\"Invalid format for column 'a': could not display values in this column using this format\"):\n        c.format = '%s'  # string format for integer data\n", "def test_column_searchsorted_with_sorter():\n    \"\"\"Test that searchsorted with sorter argument works as expected.\"\"\"\n    c = table.Column([3, 1, 2, 2])\n    sorter = [1, 3, 2, 0]\n    v = 2\n    exp = np.searchsorted(c.data, v, sorter=sorter, side='right')\n    res = c.searchsorted(v, sorter=sorter, side='right')\n    assert np.all(res == exp)\n    res = np.searchsorted(c, v, sorter=sorter, side='right')\n    assert np.all(res == exp)\n", "def test_structured_column_init_and_assignment():\n    dtype = [('x', 'f4'), ('y', 'i4')]\n    data = np.zeros(3, dtype=dtype)\n    col = table.Column(data, name='point')\n    assert col.dtype == dtype\n    assert col.shape == (3,)\n    assert len(col) == 3\n    assert col.name == 'point'\n\n    # Assign new values to the structured column\n    new_data = np.array([(1.0, 1), (2.0, 2), (3.0, 3)], dtype=dtype)\n    col[:] = new_data\n    assert_array_equal(col['x'], new_data['x'])\n    assert_array_equal(col['y'], new_data['y'])\n\n    # Assign new values to individual fields in the structured column\n    col['x'] = np.array([4.0, 5.0, 6.0])\n    assert_array_equal(col['x'], np.array([4.0, 5.0, 6.0]))\n"], "sample_503": ["def test_valid_capstyles():\n    line = mlines.Line2D([], [])\n    with pytest.raises(ValueError):\n        line.set_dash_capstyle('invalid')\n        line.set_solid_capstyle('invalid')\n", "def test_line_invalid_data():\n    with pytest.raises(RuntimeError):\n        mlines.Line2D(1, 2)\n    with pytest.raises(RuntimeError):\n        mlines.Line2D([1, 2], \"abc\")\n", "def test_axline(fig_test, fig_ref):\n    fig_test.add_subplot().axline((0, 0), slope=1, color='r')\n    fig_ref.add_subplot().plot([0, 1], [0, 1], color='r')\n", "def test_marker_color_auto(fig_test, fig_ref):\n    fig_test.add_subplot().plot([1, 2], marker='o', markeredgecolor='auto',\n                                color='red', markerfacecolor='none')\n    fig_ref.add_subplot().plot([1, 2], marker='o', markeredgecolor='red',\n                               markerfacecolor='none')\n", "def test_marker_size_with_transform():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    l, = ax.plot(x, y, marker='o', markersize=10)\n    trans = mtransforms.Affine2D().scale(2, 1) + ax.transData\n    l.set_transform(trans)\n    fig.canvas.draw()\n    assert l.get_markersize() == 10\n    assert l.get_transform() == trans\n"], "sample_632": ["def test_ignore_comments_and_docstrings():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-comments\", \"--ignore-docstrings\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\"", "def test_ignore_signatures_no_args_pass():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_signatures_empty_functions_no_args_pass():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", str(INPUT / \"similar_empty_func_no_args_1.py\"), str(INPUT / \"similar_empty_func_no_args_2.py\")])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_multiline_docstrings():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-docstrings\", SIMILAR4, SIMILAR4])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_signatures_with_defaults_fail():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            '''"], "sample_372": ["def test_include_dynamic_namespace(self):\n        return self.url_patterns, 'app_name', 'dynamic-namespace'\n\n    self.assertEqual(\n        include(get_urls),\n        (self.url_patterns, 'app_name', 'dynamic-namespace')\n    )\n", "    def test_special_chars_resolve(self):\n        test_urls = [\n            ('special', r'/special_chars/~@+%5C$*%7C/', {'chars': r'~@+\\$*|'}),\n            ('special', r'/special_chars/some%20resource/', {'chars': r'some resource'}),\n            ('special', r'/special_chars/10%25%20complete/', {'chars': r'10% complete'}),\n        ]\n        for name, url, kwargs in test_urls:\n            with self.subTest(url=url, kwargs=kwargs):\n                self.assertEqual(resolve(url).kwargs, kwargs)\n", "def test_view_name(self):\n    \"\"\"View names should be correct.\"\"\"\n    test_urls = [\n        ('urlobject-view', 'inc-ns1:test-ns3', 'inc-ns1:test-ns3:urlobject-view'),\n        ('urlobject-special-view', 'inc-ns1:test-ns3', 'inc-ns1:test-ns3:urlobject-special-view'),\n        ('inc-normal-view', 'included_namespace_urls', 'included_namespace_urls:inc-normal-view'),\n        ('inc-special-view', 'included_namespace_urls', 'included_namespace_urls:inc-special-view'),\n        ('urlobject-view', 'test-ns1', 'test-ns1:urlobject-view'),\n        ('urlobject-special-view', 'test-ns1', 'test-ns1:urlobject-special-view'),\n        ('urlobject-view', 'new-ns1', 'new-ns1:urlobject-view'),\n        ('urlobject-special-view', 'new-ns1', 'new-ns1:urlobject-special-view'),\n        ('urlobject-view', 'newapp', 'newapp:urlobject-view'),\n        ('urlobject-special-view', 'newapp', 'newapp:urlobject-special-view'),\n        ('urlobject-view', 'testapp', 'testapp:urlobject-view'),\n        ('urlobject-special-view', 'testapp', 'testapp:urlobject-special-view'),\n        ('urlobject-view', 'nodefault', 'nodefault:urlobject-view'),\n        ('urlobject-special-view', 'nodefault', 'nodefault:urlobject-special-view'),\n        ('urlobject-view', 'special', 'special:included_namespace_urls:inc-normal-view'),\n        ('urlobject-special-view', 'special', 'special:included_namespace_urls:inc-special-view'),\n    ]\n    for url_name, current_app, expected in test_urls:\n        with self.subTest(url_name=url_name, current_app=current_", "    def test_include_url_with_prefix(self):\n        included_patterns = [\n            path('included/', views.empty_view, name='included-view'),\n        ]\n        pattern, app_name, namespace = include((included_patterns, 'included'), 'prefix')\n        self.assertEqual(pattern.regex.pattern, '^prefix/')\n        self.assertEqual(pattern.url_patterns[0].pattern.regex.pattern, '^included/')\n        self.assertEqual(pattern.url_patterns[0].name, 'included-view')\n        self.assertEqual(app_name, 'included')\n        self.assertEqual(namespace, 'prefix')\n", "    def test_invalid_pattern_startswith_slash(self):\n        \"\"\"\n        Check that a RegexPattern with an invalid startswith character raises an error.\n        \"\"\"\n        pattern = RegexPattern('/invalid')\n        with self.assertRaises(Warning) as cm:\n            pattern.check()\n        self.assertEqual(str(cm.exception), \"Your URL pattern '^/invalid$' has a route beginning with a '/'. Remove this slash as it is unnecessary. If this pattern is targeted in an include(), ensure the include() pattern has a trailing '/'.\")\n"], "sample_1046": ["def test_canonicalize_with_components():\n    Lorentz = TensorIndexType('Lorentz', dim=4)\n    Lorentz.data = [-1, 1, 1, 1]\n    i0, i1, i2, i3 = tensor_indices('i0:4', Lorentz)\n    A = tensorhead('A', [Lorentz], [[1]])\n    A.data = [1, 2, 3, 4]\n\n    expr = A(i0) * A(-i0)\n    assert expr.canon_bp().data == 30\n\n    B = tensorhead('B', [Lorentz, Lorentz], [[1], [1]])\n    B.data = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]\n\n    expr = B(i0, i1) * B(-i0, -i1)\n    assert expr.canon_bp().data == 385\n", "def test_valued_tensor_assignment():\n    (A, B, AB, BA, C, Lorentz, E, px, py, pz, LorentzD, mu0, mu1, mu2, ndm, n0, n1,\n     n2, NA, NB, NC, minkowski, ba_matrix, ndm_matrix, i0, i1, i2, i3, i4) = _get_valued_base_test_variables()\n\n    # Test assignment of valued tensor to a new symbol\n    new_symbol = tensorhead('new_symbol', [Lorentz], [[1]])\n    new_symbol.data = A.data\n    assert new_symbol.data == A.data\n\n    # Test assignment of valued tensor to a different valued tensor\n    new_tensor = tensorhead('new_tensor', [Lorentz], [[1]])\n    new_tensor.data = [1, 2, 3, 4]\n    new_tensor.data = A.data\n    assert new_tensor.data == A.data\n\n    # Test assignment of valued tensor to a different indexed valued tensor\n    new_indexed_tensor = tensorhead('new_indexed_tensor', [Lorentz], [[1]])\n    new_indexed_tensor(i0).data = A(i0).data\n    assert new_indexed_tensor(i0).data == A(i0).data\n\n    # Test assignment of valued tensor to a different ranked valued tensor\n    new_ranked_tensor = tensorhead('new_ranked_tensor', [Lorentz] * 2, [[1]] * 2)\n    new_ranked_tensor.data = AB.data\n    assert new_ranked_tensor.data == AB.data\n\n    # Test assignment of valued tensor to a different indexed and ranked valued tensor\n    new_indexed_ranked_tensor = tensorhead('new_indexed_ranked_tensor', [Lorentz] * 2, [[1]] * 2)\n    new_indexed_ranked_tensor(i0, i1).data = AB(i0, i1).data\n    assert new_indexed_ranked", "def test_tensadd_derivative():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A, B = tensorhead(\"A B\", [L], [[1]])\n    x, y = symbols(\"x y\")\n\n    expr = A(i)*x + B(j)*y\n    assert expr._eval_derivative(x) == A(i)\n    assert expr._eval_derivative(y) == B(j)\n    assert expr._eval_derivative(k) == 0\n\n    expr = A(i)*x*y + B(j)*x*y\n    assert expr._eval_derivative(x) == y*(A(i) + B(j))\n    assert expr._eval_derivative(y) == x*(A(i) + B(j))\n\n    expr = A(i)*x + B(j)*y + A(k)*z\n    assert expr._eval_derivative(z) == A(k)\n", "def test_tensor_replace_indices():\n    L = TensorIndexType(\"L\", dim=4)\n    i0, i1, i2, i3 = tensor_indices('i0:4', L)\n    A = tensorhead(\"A\", [L], [[1]])\n    B = tensorhead(\"B\", [L, L], [[1], [1]])\n    C = tensorhead(\"C\", [L, L, L], [[1], [1], [1]])\n\n    # Test replace_indices\n    expr = A(i0)\n    assert expr.replace_indices((i0, i1)) == A(i1)\n\n    expr = B(i0, i1)\n    assert expr.replace_indices((i0, i2), (i1, i3)) == B(i2, i3)\n\n    expr = C(i0, i1, i2)\n    assert expr.replace_indices((i0, i3), (i1, i2)) == C(i3, i2, i2)\n\n    # Test replace_indices with repeated indices\n    expr = B(i0, i0)\n    assert expr.replace_indices((i0, i1)) == B(i1, i1)\n\n    expr = C(i0, i1, i0)\n    assert expr.replace_indices((i0, i2)) == C(i2, i1, i2)\n", "compilation error"], "sample_412": ["def test_urlize_nofollow(self):\n    tests = (\n        (\n            \"Visit google.com for more info.\",\n            'Visit <a href=\"http://google.com\" rel=\"nofollow\">google.com</a> for more info.',\n        ),\n        (\n            \"Contact us at support@example.com for assistance.\",\n            '<a href=\"mailto:support@example.com\">support@example.com</a> for assistance.',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_autoescape(self):\n    tests = (\n        ('<script>alert(\"test\");</script>', '&lt;script&gt;alert(\"test\");&lt;/script&gt;'),\n        (\"<b>http://example.com</b>\", '&lt;b&gt;<a href=\"http://example.com\">http://example.com</a>&lt;/b&gt;'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Search for google.com/?q=!, and see.',\n         'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!, and see.'),\n        ('Search for google.com/?q=1&lt;) and see.',\n         'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt;</a>) and see.'),\n        (lazystr('Search for google.com/?q=!,'),\n         'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!,'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, trim_url_limit=None, nofollow=False, autoescape=False), output)\n", "def test_urlize_autoescape(self):\n    tests = (\n        (\n            '<script>alert(\"Hello\");</script>',\n            '&lt;script&gt;alert(\"Hello\");&lt;/script&gt;',\n        ),\n        (\n            'Visit https://example.com/<script>alert(\"XSS\");</script>',\n            'Visit <a href=\"https://example.com/&lt;script&gt;alert(&quot;XSS&quot;);&lt;/script&gt;\">'\n            'https://example.com/&lt;script&gt;alert(&quot;XSS&quot;);&lt;/script&gt;</a>',\n        ),\n        (\n            'Email me at bob@example.com<script>alert(\"XSS\");</script>',\n            'Email me at <a href=\"mailto:bob@example.com\">bob@example.com</a>&lt;script&gt;'\n            'alert(\"XSS\");&lt;/script&gt;',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n", "def test_urlize_autoescape(self):\n    tests = (\n        (\n            \"Search for google.com/?q=<script>alert('XSS')</script> and see.\",\n            'Search for <a href=\"http://google.com/?q=%3Cscript%3Ealert(&#x27;XSS&#x27;)'\n            '%3C/script%3E\">google.com/?q=&lt;script&gt;alert(&#x27;XSS&#x27;)'\n            '&lt;/script&gt;</a> and see.',\n        ),\n        (\n            \"foo@example.com\",\n            '<a href=\"mailto:foo@example.com\">foo@example.com</a>',\n        ),\n        (\n            \"Search for http://example.com/?q=<script>alert('XSS')</script>\",\n            'Search for <a href=\"http://example.com/?q=%3Cscript%3Ealert(&#x27;XSS&#x27;)'\n            '%3C/script%3E\">http://example.com/?q=&lt;script&gt;alert(&#x27;XSS&#x27;)'\n            '&lt;/script&gt;</a>',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), output)\n"], "sample_408": ["    def test_operation_with_custom_suggested_name(self):\n        class CustomOperation(migrations.Operation):\n                return \"custom_operation\"\n\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                CustomOperation(),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_custom_operation\")\n", "def test_add_model_with_field_removed_from_base_model_after_multi_table_inheritance(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name, even if there's multi-table\n    inheritance involved.\n    \"\"\"\n    before = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"published\", models.BooleanField()),\n            ],\n            bases=(\"app.readable\",),\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"published\", models.BooleanField()),\n            ],\n            bases=(\"app.readable\",),\n        ),\n        ModelState(\n            \"app\",\n            \"magazine\",\n            [\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            bases=(\"app.readable\",),\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RemoveField\", \"CreateModel\"])\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, name=\"title\", model_name=\"readable\"\n    )\n    self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"magazine\")\n", "def test_add_field_with_concrete_db_column(self):\n    changes = self.get_changes([self.author_empty], [self.author_with_db_column])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"custom_field\")\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, db_column=\"custom_column\")\n", "def test_add_custom_fk_with_custom_deconstruct(self):\n    class CustomDeconstructForeignKey(models.ForeignKey):\n            name, path, args, kwargs = super().deconstruct()\n            kwargs[\"to\"] = \"testapp.Author\"\n            return name, path, args, kwargs\n\n    book_custom_deconstruct_fk = ModelState(\n        \"testapp\",\n        \"Book\",\n        [\n            (\"author\", CustomDeconstructForeignKey(on_delete=models.CASCADE)),\n        ],\n    )\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_empty, book_custom_deconstruct_fk],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Book\")\n", "def test_alter_fk_with_new_db_column(self):\n    \"\"\"\n    #23988 - AlterField with db_column and fk should work.\n    \"\"\"\n    before = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n        ModelState(\n            \"testapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n                (\n                    \"author_id\",\n                    models.IntegerField(\n                        db_column=\"author_id_custom\",\n                        verbose_name=\"author\",\n                    ),\n                ),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n        ModelState(\n            \"testapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n                (\n                    \"author\",\n                    models.ForeignKey(\n                        \"testapp.Author\",\n                        models.CASCADE,\n                        db_column=\"author_id_custom\",\n                    ),\n                ),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author_id\", model_name"], "sample_1178": ["def test_FunctionPrototype_replace():\n    x = Variable('x', real)\n    n = Variable('n', integer)\n\n    fp = FunctionPrototype(real, 'pwer', [x, n])\n    assert str(fp.name) == 'pwer'\n\n    fp2 = fp.replace('pwer', String('power'))\n    assert str(fp2.name) == 'power'\n", "def test_QuotedString():\n    qs = QuotedString('foobar')\n    assert qs.text == 'foobar'\n    assert isinstance(qs, String)\n    assert str(qs) == \"'foobar'\"\n    assert repr(qs) == \"QuotedString('foobar')\"\n\n    qs2 = QuotedString('baz')\n    assert qs != qs2\n    assert qs2.func(*qs2.args) == qs2\n\n    raises(TypeError, lambda: QuotedString(42))\n", "def test_Type__cast_check__complex_floating_point_errors():\n    # Testing exceptions for complex floating point casting\n    raises(ValueError, lambda: c64.cast_check(1e100 + 1j))\n    raises(ValueError, lambda: c128.cast_check(1e400 + 1j))\n    raises(ValueError, lambda: c64.cast_check(1e-100 + 1j))\n    raises(ValueError, lambda: c128.cast_check(1e-400 + 1j))\n", "def test_Token_equality():\n    t1 = Token()\n    t2 = Token()\n    assert t1 == t1\n    assert t1 != t2\n    assert t2 != t1\n    assert t2 == t2\n\n    # Test equality with different types\n    assert t1 != \"Token\"\n    assert t1 != 123\n    assert t1 != None\n\n    # Test equality with derived classes\n    class DerivedToken(Token):\n        pass\n\n    dt1 = DerivedToken()\n    assert t1 != dt1\n    assert dt1 != t1\n    assert dt1 == dt1\n", "def test_Attribute__construct_parameters():\n    attr1 = Attribute('foo', [1, 2, 3])\n    assert attr1.parameters == (1, 2, 3)\n    assert attr1.func(*attr1.args) == attr1\n\n    attr2 = Attribute('bar', ('a', 'b', 'c'))\n    assert attr2.parameters == ('a', 'b', 'c')\n    assert attr2.func(*attr2.args) == attr2\n\n    attr3 = Attribute('baz')\n    assert attr3.parameters == Tuple()\n    assert attr3.func(*attr3.args) == attr3\n"], "sample_354": ["def test_fields_with_fk_missing_value(self):\n    email = Email.objects.create(email='mymail@gmail.com')\n    group = Group.objects.create(name='mygroup')\n    msg = 'You must use --group with --noinput.'\n\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=email.pk,\n            email=email.email,\n            verbosity=0,\n        )\n", "def test_fields_with_fk_non_interactive(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username=email.pk,\n        email=email.email,\n        group=group.pk,\n        stdout=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertEqual(command_output, 'Superuser created successfully.')\n    u = CustomUserWithFK._default_manager.get(email=email)\n    self.assertEqual(u.username, email)\n    self.assertEqual(u.group, group)\n", "def test_validate_fk_interactive(self):\n    email = Email.objects.create(email='mymail@gmail.com')\n    Group.objects.all().delete()\n    nonexistent_group_id = 1\n    msg = f'group instance with id {nonexistent_group_id} does not exist.'\n\n    @mock_inputs({\n        'password': 'nopasswd',\n        'Username (Email.id): ': email.pk,\n        'Email (Email.email): ': email.email,\n        'Group (Group.id): ': nonexistent_group_id,\n    })\n        with self.assertRaisesMessage(CommandError, msg):\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                verbosity=0,\n            )\n\n    test(self)\n", "def test_non_ascii_username_interactive(self):\n    new_io = StringIO()\n\n    # 'Julia' with accented 'u':\n    username = 'J\\xfalia'\n    entered_usernames = [username, 'janet']\n\n        return entered_usernames.pop(0)\n\n    @mock_inputs({'password': 'nopasswd', 'username': return_usernames, 'email': 'julia@example.com', 'date_of_birth': '1970-01-01'})\n        call_command(\n            'createsuperuser',\n            interactive=True,\n            stdin=MockTTY(),\n            stdout=new_io,\n            stderr=new_io,\n        )\n        self.assertEqual(\n            new_io.getvalue().strip(),\n            'Superuser created successfully.'\n        )\n        self.assertTrue(CustomUser._default_manager.filter(username=username).exists())\n\n    test(self)\n", "    def test_createsuperuser_command_with_proxy_model(self):\n        \"\"\"\n        createsuperuser --database should operate on the specified DB when\n        AUTH_USER_MODEL is a proxy model.\n        \"\"\"\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username='joe',\n            email='joe@somewhere.org',\n            database='other',\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        user = UserProxy.objects.using('other').get(username='joe')\n        self.assertEqual(user.email, 'joe@somewhere.org')\n"], "sample_789": ["def test_error_function():\n    # Test that the error function is correctly calculated for each algorithm\n    clf = AdaBoostClassifier(algorithm='SAMME', random_state=0)\n    clf.fit(X, y_class)\n    samme_errors = clf.estimator_errors_\n\n    clf = AdaBoostClassifier(algorithm='SAMME.R', random_state=0)\n    clf.fit(X, y_class)\n    samme_r_errors = clf.estimator_errors_\n\n    assert not np.all(samme_errors == samme_r_errors), \"SAMME and SAMME.R errors should not be the same\"\n", "def test_loss_functions():\n    # Check that all loss functions are accepted\n    for loss in ['linear', 'square', 'exponential']:\n        reg = AdaBoostRegressor(loss=loss, random_state=0)\n        reg.fit(boston.data, boston.target)\n        score = reg.score(boston.data, boston.target)\n        assert score > 0.85\n\n    # Check that an invalid loss function raises a ValueError\n    with pytest.raises(ValueError):\n        reg = AdaBoostRegressor(loss='invalid_loss', random_state=0)\n        reg.fit(boston.data, boston.target)\n", "def test_base_estimator_feature_importances():\n    # Check feature importances when base estimator supports it.\n    X, y = datasets.make_classification(n_samples=200,\n                                        n_features=10,\n                                        n_informative=3,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert_equal(importances.shape[0], 10)\n    assert_equal((importances[:3, np.newaxis] >= importances[3:]).all(), True)\n\n    # Check that base estimator without feature_importances_ raises an error.\n    clf = AdaBoostClassifier(SVC(gamma='scale'))\n    clf.fit(X, y)\n    assert_raises(AttributeError, getattr, clf, \"feature_importances_\")\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work with sample_weights in the base estimator\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0], dtype=np.int)\n\n            return np.hstack([np.zeros(X.shape[0], dtype=np.float),\n                              np.ones(X.shape[0], dtype=np.float)]).reshape(-1, 2)\n\n    boost = AdaBoostClassifier(DummyEstimator(), n_estimators=3, algorithm='SAMME.R')\n    boost.fit(X, y_class)\n    assert_equal(len(boost.estimator_weights_), len(boost.estimator_errors_))\n", "def test_samme_r_proba():\n    # Test the SAMME.R algorithm in AdaBoostClassifier.\n\n    # Define a binary classification problem\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([0, 1, 1, 0])\n\n    # Fit an AdaBoostClassifier with SAMME.R algorithm\n    clf = AdaBoostClassifier(n_estimators=3, algorithm='SAMME.R', random_state=0)\n    clf.fit(X, y)\n\n    # Get the predicted probabilities\n    proba = clf.predict_proba(X)\n\n    # Check the shape of the predicted probabilities\n    assert_equal(proba.shape, (4, 2))\n\n    # Check that the sum of probabilities for each instance is equal to 1\n    assert_array_almost_equal(proba.sum(axis=1), np.ones(4))\n\n    # Check that the probabilities are not equal for all instances\n    assert_greater(np.abs(proba[0, 1] - proba[1, 1]), 0.01)\n"], "sample_567": ["def test_text_set_bbox():\n    txt = Text(.5, .5, \"foo\\nbar\")\n    assert txt._bbox_patch is None\n\n    txt.set_bbox(dict(facecolor='red', alpha=0.5))\n    assert txt._bbox_patch is not None\n    assert txt._bbox_patch.get_facecolor() == 'red'\n    assert txt._bbox_patch.get_alpha() == 0.5\n", "def test_annotate_and_offsetfrom_with_transform():\n    fig, ax = plt.subplots()\n    l, = ax.plot([0, 2], [0, 2])\n    transform = ax.transData + ax.transAxes.inverted()\n    of_xy = np.array([.5, .5])\n    ax.annotate(\"foo\", textcoords=OffsetFrom(transform, of_xy), xytext=(10, 0),\n                xy=(0, 0))  # xy is unused.\n    fig.canvas.draw()\n    # Add assertions to check the position of the annotation.\n", "def test_text_multialignment():\n    fig, ax = plt.subplots()\n    txt = ax.text(0.5, 0.5, \"left\\ncenter\\nright\", ha='center', multialignment='left')\n    fig.canvas.draw()\n    left_align_bbox = txt.get_window_extent()\n    txt.set_multialignment('right')\n    fig.canvas.draw()\n    right_align_bbox = txt.get_window_extent()\n    assert left_align_bbox.x0 != right_align_bbox.x0\n", "def test_annotation_with_offsettext():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1])\n    annotation = ax.annotate('Test', xy=(0.5, 0.5), xycoords='data',\n                             xytext=(0, 10), textcoords='offset points')\n    fig.canvas.draw()\n    assert annotation.get_position() == (0.5, 0.5)\n    assert annotation.xyann == (0.5 + 0 / fig.dpi, 0.5 + 10 / fig.dpi)\n", "def test_offset_from_unit():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1])\n    of = OffsetFrom(line, (0.5, 0.5))\n\n    assert of.get_unit() == \"points\"\n\n    of.set_unit(\"pixels\")\n    assert of.get_unit() == \"pixels\"\n\n    with pytest.raises(ValueError, match=\"unit must be 'points' or 'pixels'\"):\n        of.set_unit(\"invalid\")\n"], "sample_543": ["def test_polygon_selector_clear_key(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        # Clear the polygon\n        ('on_key_press', dict(key='escape')),\n        ('on_key_release', dict(key='escape')),\n        # Draw a new polygon\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.verts == verts\n", "def test_polygon_selector_set_verts(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Change the vertices\n    new_verts = [(30, 10), (10, 30), (30, 50), (50, 30)]\n    tool.verts = new_verts\n    np.testing.assert_allclose(tool.verts, new_verts)\n", "def test_lasso_selector_clear(ax):\n    onselect = mock.Mock(spec=noop, return_value=None)\n    tool = widgets.LassoSelector(ax, onselect)\n    assert tool.get_visible()\n\n    # press-release event outside the lasso to clear the lasso\n    click_and_drag(tool, start=(130, 130), end=(130, 130))\n    assert not tool.get_visible()\n\n    # Do another cycle of events to make sure we can\n    click_and_drag(tool, start=(10, 10), end=(50, 120))\n    assert tool.get_visible()\n    assert onselect.call_count == 2\n", "def test_polygon_selector_box_handle_props(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the box handle props are set correctly\n    for handle in tool._box._handles_artists:\n        assert handle.get_markerfacecolor() == (0, 0, 1, 0.2)\n        assert handle.get_markeredgecolor() == (0, 0, 1, 1)\n        assert handle.get_markersize() == 7\n\n    # Change the box handle props and check that they are updated correctly\n    tool._box.set_handle_props(markerfacecolor='r', markeredgecolor='g', markersize=10)\n    for handle in tool._box._handles_artists:\n        assert handle.get_markerfacecolor() == (1, 0, 0, 1)\n        assert handle.get_markeredgecolor() == (0, 1, 0, 1)\n        assert handle.get_markersize() == 10\n", "def test_polygon_selector_ignore_outside(ax):\n    # Test that polygon selector ignores events outside the axes\n    event_sequence = [\n        *polygon_place_vertex(200, 200),\n        *polygon_place_vertex(300, 300),\n    ]\n\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    tool = widgets.PolygonSelector(ax, onselect, ignore_event_outside=True)\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    assert onselect.call_count == 0\n"], "sample_344": ["def test_mixed_case_model_name(self):\n    class MixedCaseModel(models.Model):\n        name = models.CharField(max_length=50)\n\n        class Meta:\n            app_label = 'migrations'\n\n    state = ModelState.from_model(MixedCaseModel)\n    self.assertEqual(state.name, 'MixedCaseModel')\n", "def test_get_related_models_tuples_include_self(self):\n    class TestModel(models.Model):\n        name = models.CharField(max_length=50)\n\n    model_tuples = get_related_models_tuples(TestModel)\n    self.assertIn(('migrations', 'testmodel'), model_tuples)\n", "def test_abstract_base_multiple_inheritance(self):\n    class AbstractBase1(models.Model):\n        name = models.CharField(max_length=50)\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n\n    class AbstractBase2(models.Model):\n        description = models.TextField()\n\n        class Meta:\n            app_label = 'migrations'\n            abstract = True\n\n    class ChildModel(AbstractBase1, AbstractBase2):\n        pass\n\n    child_state = ModelState.from_model(ChildModel)\n    self.assertEqual(list(child_state.fields), ['id', 'name', 'description'])\n", "def test_proxy_base_concrete_model_key(self):\n    A = self.create_model(\"A\")\n    B = self.create_model(\"B\", bases=(A,), proxy=True)\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(A))\n    project_state.add_model(ModelState.from_model(B))\n    concrete_model_key = project_state.get_concrete_model_key(B)\n    self.assertEqual(concrete_model_key, ('migrations.related_models_app', 'a'))\n", "    def test_multiple_bases_with_different_depth(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,))\n        C = self.create_model(\"C\", bases=(B,))\n        D = self.create_model(\"D\")\n        E = self.create_model(\"E\", bases=(D,))\n        F = self.create_model(\"F\", bases=(C, E,))\n        self.assertRelated(A, [B, C, F])\n        self.assertRelated(B, [A, C, F])\n        self.assertRelated(C, [A, B, D, E, F])\n        self.assertRelated(D, [E, F])\n        self.assertRelated(E, [D, F])\n        self.assertRelated(F, [A, B, C, D, E])\n"], "sample_307": ["def test_twenty_four_hour_format(self):\n    tests = [\n        (0, '00'),\n        (1, '01'),\n        (11, '11'),\n        (12, '12'),\n        (13, '13'),\n        (23, '23'),\n    ]\n    for hour, expected in tests:\n        with self.subTest(hour=hour):\n            self.assertEqual(\n                dateformat.format(datetime(2000, 1, 1, hour), 'G'),\n                expected,\n            )\n", "def test_twenty_four_hour_format(self):\n    tests = [\n        (0, '00'),\n        (1, '01'),\n        (11, '11'),\n        (12, '12'),\n        (13, '13'),\n        (23, '23'),\n    ]\n    for hour, expected in tests:\n        with self.subTest(hour=hour):\n            self.assertEqual(\n                dateformat.format(datetime(2000, 1, 1, hour), 'G'),\n                expected,\n            )\n", "def test_invalid_date_format_specifiers(self):\n    my_time = datetime.time(22, 0)\n\n    for specifier in ['d', 'D', 'E', 'F', 'I', 'j', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'S', 't', 'U', 'w', 'W', 'y', 'Y', 'z']:\n        msg = (\n            \"The format for time objects may not contain date-related \"\n            \"format specifiers (found '%s').\" % specifier\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            dateformat.time_format(my_time, specifier)\n", "def test_time_formats_with_microseconds(self):\n    dt_with_microseconds = datetime(1979, 7, 8, 22, 30, 45, 123456)\n\n    self.assertEqual(dateformat.format(dt_with_microseconds, 'u'), '123456')\n    self.assertEqual(dateformat.format(dt_with_microseconds, 'r'), 'Sun, 08 Jul 1979 22:30:45 +0100')\n", "def test_ISO_8601_week_number_year_match(self):\n    # Test that the ISO 8601 week number and year match the datetime's year\n    # for dates that fall within the first and last weeks of a year.\n    # These dates can fall into the previous or next year's ISO 8601 week number.\n    first_week_of_year = datetime(2022, 1, 3)\n    last_week_of_year = datetime(2022, 12, 26)\n    self.assertEqual(dateformat.format(first_week_of_year, 'o'), '2022')\n    self.assertEqual(dateformat.format(last_week_of_year, 'o'), '2022')\n"], "sample_613": ["def test_groupby_multi_index_groupby_map(dataset):\n    # regression test for GH873\n    ds = dataset.isel(z=1, drop=True)[[\"foo\"]]\n    expected = 2 * ds\n    actual = (\n        ds.stack(space=[\"x\", \"y\"])\n        .groupby(\"space\")\n        .map(lambda x: 2 * x)\n        .unstack(\"space\")\n    )\n    assert_equal(expected, actual)\n", "def test_ds_resample_apply_func_kwargs(self):\n        return arg1.mean(\"time\") + arg2 + arg3\n\n    times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n    ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n    expected = xr.Dataset({\"foo\": (\"time\", [4.0, 4.0, 4.0]), \"time\": times})\n    actual = ds.resample(time=\"D\").map(func, arg2=2.0, arg3=1.0)\n    assert_identical(expected, actual)\n", "def test_groupby_bins_empty_category(array) -> None:\n    # one of these bins will be empty\n    bins = [0, 4, 5]\n    bin_coords = pd.cut(array[\"x\"], bins).categories\n    actual = array.groupby_bins(\"x\", bins).sum()\n    expected = DataArray([6, np.nan], dims=\"x_bins\", coords={\"x_bins\": bin_coords})\n    assert_identical(expected, actual)\n    # make sure original array is unchanged\n    # (was a problem in earlier versions)\n    assert len(array.x) == 4\n", "def test_groupby_reduce_dimensions_and_labels(array):\n    grouped = array.groupby(\"y\")\n    with pytest.raises(ValueError, match=r\"cannot reduce over dimensions\"):\n        grouped.mean(\"y\")\n\n    with pytest.raises(ValueError, match=r\"cannot reduce over dimensions\"):\n        grouped.mean(\"x\")\n\n    with pytest.raises(ValueError, match=r\"cannot reduce over dimensions\"):\n        grouped.mean((\"x\", \"y\"))\n", "def test_groupby_indexes():\n    # test groupby with multiple indexes\n    index1 = xr.DataArray([1, 1, 2, 2], dims=\"x\")\n    index2 = xr.DataArray([\"a\", \"b\", \"a\", \"b\"], dims=\"x\")\n    array = xr.DataArray([1, 2, 3, 4], dims=\"x\")\n\n    expected = xr.DataArray([2, 6], coords=[(\"index1\", [1, 2]), (\"index2\", [\"a\", \"b\"])])\n    actual = array.groupby([index1, index2]).sum()\n    assert_identical(expected, actual)\n"], "sample_966": ["def test_pyfunction_with_positional_only_arguments(app):\n    text = \".. py:function:: hello(a, /, b)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc_sig_operator, \"/\"],\n                                      [desc_parameter, desc_sig_name, \"b\"])])\n", "def test_missing_reference_no_warning(app, status, warning):\n    app.build()\n    assert 'WARNING' not in warning.getvalue()\n", "def test_pyfunction_signature_with_default(app):\n    text = \".. py:function:: hello(name: str = 'World') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      desc_sig_space,\n                                                      [nodes.inline, pending_xref, \"str\"],\n                                                      desc_sig_space,\n                                                      [desc_sig_operator, \"=\"],\n                                                      desc_sig_space,\n                                                      [nodes.inline, \"'World'\"])])\n", "def test_pyfunction_signature_with_unsupported_features(app, warning):\n    text = \".. py:function:: hello(name: str = 'World', *args: Any, **kwargs: Any) -> str\"\n    restructuredtext.parse(app, text)\n    assert 'unknown directive option' in warning.getvalue()\n", "def test_pyfunction_signature_with_slashes(app):\n    text = \".. py:function:: hello(a=1/, b=2/)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"1/\"])],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"2/\"])])])\n"], "sample_282": ["def test_bound_field_as_widget(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    self.assertHTMLEqual(\n        bound_field.as_widget(),\n        \"\"\"\n        <input type=\"text\" name=\"field1_0\" id=\"id_field1_0\" required>\n        <select multiple name=\"field1_1\" id=\"id_field1_1\" required>\n        <option value=\"J\">John</option>\n        <option value=\"P\">Paul</option>\n        <option value=\"G\">George</option>\n        <option value=\"R\">Ringo</option>\n        </select>\n        <input type=\"text\" name=\"field1_2_0\" id=\"id_field1_2_0\" required>\n        <input type=\"text\" name=\"field1_2_1\" id=\"id_field1_2_1\" required>\n        \"\"\",\n    )\n", "def test_bound_field_with_initial_value(self):\n    initial_value = 'initial value'\n    form = ComplexFieldForm(initial={'field1': initial_value})\n    bound_field = form['field1']\n    self.assertEqual(bound_field.initial, initial_value)\n", "def test_bound_widget_str(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    bound_widget = bound_field[0]\n    self.assertEqual(str(bound_widget), '<input type=\"text\" name=\"field1_0\" required id=\"id_field1_0\">')\n", "def test_form_initial_data(self):\n    form = ComplexFieldForm(initial={\n        'field1': 'initial text,JP,2007-04-25 06:24:00',\n    })\n    self.assertHTMLEqual(\n        form.as_table(),\n        \"\"\"\n        <tr><th><label for=\"id_field1_0\">Field1:</label></th>\n        <td><input type=\"text\" name=\"field1_0\" value=\"initial text\" id=\"id_field1_0\" required>\n        <select multiple name=\"field1_1\" id=\"id_field1_1\" required>\n        <option value=\"J\" selected>John</option>\n        <option value=\"P\" selected>Paul</option>\n        <option value=\"G\">George</option>\n        <option value=\"R\">Ringo</option>\n        </select>\n        <input type=\"text\" name=\"field1_2_0\" value=\"2007-04-25\" id=\"id_field1_2_0\" required>\n        <input type=\"text\" name=\"field1_2_1\" value=\"06:24:00\" id=\"id_field1_2_1\" required></td></tr>\n        \"\"\",\n    )\n", "def test_bound_widget_str(self):\n    widget = ComplexMultiWidget()\n    bound_widget = BoundWidget(widget, {'name': 'field1', 'label': 'Test Label', 'attrs': {'id': 'test_id'}, 'template_name': 'test_template.html'}, None)\n    self.assertEqual(str(bound_widget), '<input type=\"text\" name=\"field1_0\" id=\"test_id_0\">')  # Depends on your template content\n"], "sample_1077": ["def test_imageset_difference():\n    n = Dummy()\n    s1 = ImageSet(Lambda(n, 2*n), S.Integers)\n    s2 = ImageSet(Lambda(n, 3*n), S.Integers)\n    assert s1.difference(s2) == ImageSet(Lambda(n, 2*n), {n for n in S.Integers if 2*n not in s2})\n\n    s3 = ImageSet(Lambda(n, n), S.Integers)\n    assert s1.difference(s3) == ImageSet(Lambda(n, 2*n), {n for n in S.Integers if 2*n != n})\n\n    assert s3.difference(s1) == ImageSet(Lambda(n, n), {n for n in S.Integers if n not in s1})\n", "def test_issue_16871c():\n    assert ImageSet(Lambda(x, x - 3), S.Integers).intersect(S.Integers) == S.Integers\n", "def test_issue_17471():\n    empty = Intersection(FiniteSet(log(2)/pi), S.Integers)\n    assert unchanged(ImageSet, Lambda(x, 1), empty)\n", "def test_issue_22580():\n    from sympy import symbols, Interval, S, pi, Union\n    from sympy.sets.fancysets import ComplexRegion\n\n    x = symbols('x')\n    a = Interval(0, 1)\n    b = Interval(0, 2*pi)\n    c = Interval(2, 3)\n    d = Interval(pi/2, pi)\n\n    # Create complex regions\n    c1 = ComplexRegion(a*b, polar=True)\n    c2 = ComplexRegion(c*d)\n\n    # Test union and intersection\n    assert Union(c1, c2) == ComplexRegion(Union(a*b, c*d), polar=True)\n    assert Intersection(c1, c2) == ComplexRegion(a.intersect(c)*d.intersect(b), polar=True)\n", "def test_issue_19642():\n    n = Dummy('n')\n    s = ImageSet(Lambda(n, -I*(I*(2*pi*n - pi/4) + log(Abs(sqrt(-I))))), S.Integers)\n    assert s.intersect(S.Reals) == ImageSet(Lambda(n, 2*pi*n + 7*pi/4), S.Integers)\n"], "sample_58": ["    def test_attribute_override_with_class(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n        custom = DjangoTemplates()\n        form = CustomForm(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n", "def test_error_dict_as_json_escape_html_custom_renderer(self):\n    \"\"\"#21962 - adding html escape flag to ErrorDict with custom renderer\"\"\"\n    class MyForm(Form):\n        foo = CharField()\n        bar = CharField()\n\n            raise ValidationError(\n                '<p>Non-field error.</p>',\n                code='secret',\n                params={'a': 1, 'b': 2},\n            )\n\n    control = {\n        'foo': [{'code': 'required', 'message': 'This field is required.'}],\n        'bar': [{'code': 'required', 'message': 'This field is required.'}],\n        '__all__': [{'code': 'secret', 'message': '<p>Non-field error.</p>'}]\n    }\n\n    form = MyForm({}, renderer=CustomRenderer())\n    self.assertFalse(form.is_valid())\n\n    errors = json.loads(form.errors.as_json())\n    self.assertEqual(errors, control)\n\n    escaped_error = '&lt;p&gt;Non-field error.&lt;/p&gt;'\n    self.assertEqual(\n        form.errors.get_json_data(escape_html=True)['__all__'][0]['message'],\n        escaped_error\n    )\n    errors = json.loads(form.errors.as_json(escape_html=True))\n    control['__all__'][0]['message'] = escaped_error\n    self.assertEqual(errors, control)\n", "def test_custom_renderer_subclass(self):\n    class CustomRenderer(DjangoTemplates):\n        pass\n\n    class CustomForm(Form):\n        default_renderer = CustomRenderer\n\n    form = CustomForm()\n    self.assertTrue(isinstance(form.renderer, CustomRenderer))\n", "def test_field_with_empty_label(self):\n    class MyForm(Form):\n        f1 = CharField(max_length=30, label=\"\")\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\" required></p>'\n    )\n    self.assertHTMLEqual(\n        form.as_ul(),\n        '<li><input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\" required></li>'\n    )\n    self.assertHTMLEqual(\n        form.as_table(),\n        '<tr><td><input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\" required></td></tr>'\n    )\n", "def test_widget_attrs(self):\n    class CustomWidget(Widget):\n            return mark_safe('<input type=\"text\" name=\"%s\" value=\"%s\" %s>' % (name, value or '', flatatt(attrs)))\n\n    class CustomForm(Form):\n        field = CharField(widget=CustomWidget(attrs={'custom_attr': 'value'}))\n\n    form = CustomForm()\n    self.assertHTMLEqual(str(form['field']), '<input type=\"text\" name=\"field\" value=\"\" custom_attr=\"value\">')\n"], "sample_401": ["def test_formset_default_error_messages(self):\n    \"\"\"Formset instances have default error messages.\"\"\"\n    formset = FavoriteDrinksFormSet()\n    self.assertIn('missing_management_form', formset.error_messages)\n    self.assertIn('too_many_forms', formset.error_messages)\n    self.assertIn('too_few_forms', formset.error_messages)\n", "def test_formset_with_custom_deletion_widget(self):\n    class CustomDeletionFormSet(BaseFormSet):\n        deletion_widget = CheckboxInput(attrs={'class': 'custom-delete'})\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomDeletionFormSet, can_delete=True)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertHTMLEqual(\n        '\\n'.join(form.as_ul() for form in formset.forms),\n        \"\"\"<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\"></li>", "def test_formset_with_custom_deletion_field_name(self):\n    class CustomDeletionFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            if self.can_delete and index < self.initial_form_count():\n                form.fields['CUSTOM_DELETE'] = BooleanField(\n                    label=_('Delete'),\n                    required=False,\n                    widget=self.get_deletion_widget(),\n                )\n\n    ArticleFormSet = formset_factory(\n        ArticleForm, formset=CustomDeletionFormSet, can_delete=True,\n        delete_prefix='CUSTOM_',\n    )\n    formset = ArticleFormSet(auto_id=False)\n    self.assertHTMLEqual(\n        '\\n'.join([form.as_ul() for form in formset.forms]),\n        (\n            '<li>Title: <input type=\"text\" name=\"form-0-title\"></li>'\n            '<li>Pub date: <input type=\"text\" name=\"form-0-pub_date\">'\n            '<input type=\"checkbox\" name=\"form-0-CUSTOM_DELETE\"></li>'\n        ),\n    )\n", "def test_formset_with_initial_data_errors(self):\n    \"\"\"\n    A FormSet with initial data should include errors for forms with initial data.\n    \"\"\"\n    initial = [{\"choice\": \"Calexico\", \"votes\": \"\"}]\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.errors, [{\"votes\": [\"This field is required.\"]}]\n    )\n", "def test_formset_with_custom_form_kwargs(self):\n    \"\"\"\n    Custom kwargs set on the formset instance are passed to the\n    underlying forms.\n    \"\"\"\n    FormSet = formset_factory(CustomKwargForm, extra=2)\n    formset = FormSet(form_kwargs={\"custom_kwarg\": 1})\n    for form in formset:\n        self.assertTrue(hasattr(form, \"custom_kwarg\"))\n        self.assertEqual(form.custom_kwarg, 1)\n\n    # Test with dynamic kwargs\n    class DynamicBaseFormSet(BaseFormSet):\n            return {\"custom_kwarg\": index}\n\n    DynamicFormSet = formset_factory(\n        CustomKwargForm, formset=DynamicBaseFormSet, extra=2\n    )\n    formset = DynamicFormSet(form_kwargs={\"custom_kwarg\": \"ignored\"})\n    for i, form in enumerate(formset):\n        self.assertTrue(hasattr(form, \"custom_kwarg\"))\n        self.assertEqual(form.custom_kwarg, i)\n\n    # Test with empty form\n    self.assertTrue(hasattr(formset.empty_form, \"custom_kwarg\"))\n    self.assertEqual(formset.empty_form.custom_kwarg, 1)\n"], "sample_573": ["    def test_lower_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=0, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert_array_equal(res[\"x\"], [])\n        assert_array_equal(res[\"y\"], [])\n", "def test_order_greater_than_unique_values(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = np.repeat([0, 1], df.shape[0] // 2)\n    res = PolyFit(order=2, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert res.empty\n", "def test_insufficient_data_points(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [1, 2, 3], \"group\": [\"x\", \"x\", \"x\"]})\n    res = PolyFit(order=2, gridsize=100)(df, groupby, \"x\", {})\n\n    assert res.empty\n", "    def test_order_less_than_unique_values(self, df):\n\n        df[\"x\"] = df[\"x\"].rank(method='dense').astype(int)\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=df[\"x\"].nunique(), gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert res.empty, \"When order is equal to the number of unique x values, result should be empty\"\n", "def test_low_order_fit(self, df):\n    groupby = GroupBy([\"group\"])\n    order = 1\n    res = PolyFit(order=order)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert res.columns.to_list() == [\"x\", \"y\"]\n\n    grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n    assert_array_equal(res[\"x\"], grid)\n    assert_array_almost_equal(\n        res[\"y\"].diff(), np.zeros(grid.size - 1)\n    )\n"], "sample_474": ["def test_transform_with_annotation(self):\n    authors = Author.objects.annotate(name_code_point=Ord(\"name\"))\n    with register_lookup(IntegerField, Chr):\n        self.assertCountEqual(\n            authors.filter(name_code_point__chr=Chr(F(\"name_code_point\"))),\n            [self.john],\n        )\n        self.assertCountEqual(\n            authors.exclude(name_code_point__chr=Chr(F(\"name_code_point\"))),\n            [self.elena, self.rhonda],\n        )\n", "    def test_null_values(self):\n        authors = Author.objects.annotate(first_initial=Left(\"alias\", 1))\n        self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"J\"))), [self.john])\n        self.assertCountEqual(\n            authors.filter(first_initial=Chr(None)), [self.rhonda]\n        )\n", "    def test_annotate_non_ascii(self):\n        authors = Author.objects.annotate(\n            first_initial=Left(\"name\", 1),\n            initial_chr=Chr(ord(\"\u00c9\")),\n        )\n        self.assertSequenceEqual(\n            authors.filter(first_initial=F(\"initial_chr\")),\n            [self.elena],\n        )\n", "def test_none_value(self):\n    authors = Author.objects.annotate(first_initial=Left(\"alias\", 1))\n    self.assertCountEqual(authors.filter(first_initial=Chr(None)), [self.rhonda])\n", "    def test_null_value(self):\n        authors = Author.objects.annotate(first_initial=Left(\"alias\", 1))\n        self.assertCountEqual(\n            authors.filter(first_initial=Chr(ord(\"J\"))), [self.john]\n        )\n        self.assertCountEqual(\n            authors.filter(first_initial=Chr(ord(\"J\"), output_field=CharField(null=True))),\n            [self.john, self.rhonda],\n        )\n        self.assertCountEqual(\n            authors.exclude(first_initial=Chr(ord(\"J\"))), [self.elena]\n        )\n        self.assertCountEqual(\n            authors.exclude(first_initial=Chr(ord(\"J\"), output_field=CharField(null=True))),\n            [self.elena, self.rhonda],\n        )\n"], "sample_468": ["def test_context_push_pop_equality(self):\n    \"\"\"\n    Test that contexts are equal after push and pop operations.\n    \"\"\"\n    c1 = Context({\"a\": 1})\n    c2 = Context({\"a\": 1})\n\n    # Perform push and pop operations on c1\n    c1.push({\"b\": 2})\n    c1.pop()\n\n    # Check that c1 is equal to c2\n    self.assertEqual(c1, c2)\n", "def test_context_new_with_processors(self):\n    request = self.request_factory.get(\"/\")\n    ctx = RequestContext(request, {\"var\": \"parent\"})\n    new_ctx = ctx.new({\"var\": \"child\"})\n    self.assertEqual(new_ctx[\"var\"], \"child\")\n    self.assertNotIn(\"_processors_index\", new_ctx.__dict__)\n", "def test_context_with_render_context(self):\n    request = self.request_factory.get(\"/\")\n    ctx = RequestContext(request, {\"var\": \"parent\"})\n    ctx.render_context[\"rendered_var\"] = \"rendered\"\n    self.assertEqual(ctx.render_context[\"rendered_var\"], \"rendered\")\n    self.assertNotIn(\"rendered_var\", ctx)\n", "def test_new_context_with_values(self):\n    c = RequestContext(self.request_factory.get(\"/\"), {\"a\": 1, \"b\": 2})\n    new_c = c.new({\"b\": 3, \"c\": 4})\n    self.assertEqual(new_c[\"a\"], None)  # values not provided in new() should be None\n    self.assertEqual(new_c[\"b\"], 3)\n    self.assertEqual(new_c[\"c\"], 4)\n", "def test_render_context_push_state(self):\n    test_context = RenderContext({\"fruit\": \"papaya\"})\n    with test_context.push_state(template=None):\n        test_context[\"vegetable\"] = \"carrot\"\n    self.assertNotIn(\"vegetable\", test_context.dicts[-1])\n    self.assertEqual(test_context[\"fruit\"], \"papaya\")\n"], "sample_939": ["def test_unparse_subscript_tuple(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_additional(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_comprehension(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_args_kwargs():\n    source = \"def f(x, y=1, *args, z, **kwargs): pass\"\n    expected = \"def f(x, y=1, *args, z, **kwargs): ...\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_set_with_names(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n"], "sample_1023": ["def test_sieve_reset():\n    s = Sieve()\n    s.extend(30)\n    s._reset(prime=True)\n    assert len(s._list) == 6\n    s._reset(totient=True)\n    assert len(s._tlist) == 6\n    s._reset(mobius=True)\n    assert len(s._mlist) == 6\n    s._reset()\n    assert len(s._list) == 6\n    assert len(s._tlist) == 6\n    assert len(s._mlist) == 6\n", "def test_sieve_extend_to_no():\n    s = Sieve()\n    s.extend_to_no(15)\n    assert len(s._list) >= 15\n    assert s[15] == 43\n", "def test_sieve_extend_mobius():\n    s = Sieve()\n    s.extend(100)\n    assert list(s.mobiusrange(50, 100)) == [mobius(x) for x in range(50, 100)]\n", "def test_sieve_extend_to_no():\n    s = Sieve()\n    s.extend_to_no(15)\n    assert len(s._list) >= 15\n    assert s._list[14] == 41\n    s._reset(prime=True)\n    s.extend_to_no(10)\n    assert len(s._list) >= 10\n    assert s._list[9] == 29\n", "def test_mobiusrange_with_large_numbers():\n    sieve._reset(mobius=True)\n    assert list(sieve.mobiusrange(10**6, 10**6 + 10)) == [0, -1, 1, -1, 0, 0, 1, -1, 0, -1, 1]\n    assert list(sieve.mobiusrange(10**9, 10**9 + 20)) == [0, -1, -1, 0, 0, -1, 1, 1, 1, 0, -1, 0, -1, 1, 0, 0, 1, -1, 0, -1]\n"], "sample_777": ["def test_gradient_boosting_zero_estimator():\n    # Check that GradientBoostingRegressor works when init='zero'.\n    X, y = make_regression()\n    gb = GradientBoostingRegressor(init='zero')\n    gb.fit(X, y)\n    assert gb.predict(X).shape == y.shape\n", "def test_gradient_boosting_with_zero_init():\n    # Check that GradientBoostingRegressor works when init is 'zero'.\n    X, y = make_regression()\n    gbr = GradientBoostingRegressor(init='zero')\n    gbr.fit(X, y)\n    y_pred = gbr.predict(X)\n    assert_equal(y_pred.shape, y.shape)\n", "def test_early_stopping_multiclass_with_unseen_class():\n    # When doing early stopping and the validation set contains a class that\n    # is not present in the training set, an error should be raised as the\n    # init estimator is trained only on the training set.\n\n    X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n    y = [0, 1, 2, 2]\n    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=4)\n    with pytest.raises(\n                ValueError,\n                match='The training data after the early stopping split'):\n        gb.fit(X, y)\n\n    # No error with another random seed\n    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0)\n    gb.fit(X, y)\n", "def test_gradient_boosting_feature_importance_multiclass():\n    # Test that feature importances work with multiclass\n    X, y = _make_multiclass()\n    gb = GradientBoostingClassifier(n_estimators=10, random_state=42)\n    gb.fit(X, y)\n    assert gb.feature_importances_.shape[0] == X.shape[1]\n", "def test_gradient_boosting_regressor_presort_auto():\n    # Test the 'auto' option of presort parameter for GradientBoostingRegressor.\n    # The 'auto' option should choose presort for dense arrays and not presort for sparse arrays.\n    X, y = make_regression(n_samples=100, n_features=20, random_state=0)\n    X_sparse = csc_matrix(X)\n\n    est_dense = GradientBoostingRegressor(n_estimators=10, random_state=0, presort='auto')\n    est_dense.fit(X, y)\n    y_pred_dense = est_dense.predict(X)\n\n    est_sparse = GradientBoostingRegressor(n_estimators=10, random_state=0, presort='auto')\n    est_sparse.fit(X_sparse, y)\n    y_pred_sparse = est_sparse.predict(X_sparse)\n\n    # The predictions should be the same, even though presort is True for dense and False for sparse.\n    assert_array_almost_equal(y_pred_dense, y_pred_sparse)\n"], "sample_505": ["def test_change_tz():\n    plt.rcParams['timezone'] = 'America/New_York'\n    dates = np.arange('2020-01-01T12:00:00', '2020-05-01T12:00:00', dtype='datetime64[D]')\n    fig, ax = plt.subplots()\n\n    ax.plot(dates, np.arange(len(dates)))\n    fig.canvas.draw()\n    assert ax.get_xticklabels()[0].get_text() == 'Jan 01 2020'\n    assert ax.get_xticklabels()[1].get_text() == 'Jan 15 2020'\n\n    plt.rcParams['timezone'] = 'UTC'\n    fig, ax = plt.subplots()\n\n    ax.plot(dates, np.arange(len(dates)))\n    fig.canvas.draw()\n    assert ax.get_xticklabels()[0].get_text() == 'Dec 31 2019'\n    assert ax.get_xticklabels()[1].get_text() == 'Jan 14 2020'\n", "def test_drange_single_step():\n    start = datetime.datetime(2011, 1, 1, tzinfo=mdates.UTC)\n    end = datetime.datetime(2011, 1, 2, tzinfo=mdates.UTC)\n    delta = datetime.timedelta(days=1)\n\n    daterange = mdates.drange(start, end, delta)\n    assert len(daterange) == 1\n    assert mdates.num2date(daterange[0]) == start\n", "def test_date2num_subsecond():\n    # Test date2num with subsecond resolution\n    dt_subsecond = datetime.datetime(2022, 1, 1, 0, 0, 0, 123456)\n    num_subsecond = mdates.date2num(dt_subsecond)\n    dt_reconstructed = mdates.num2date(num_subsecond)\n    assert dt_subsecond == dt_reconstructed\n", "def test_date_formatter_no_year():\n    # Test if the year is not shown when the interval is less than a year\n    d1 = datetime.datetime(1997, 1, 1)\n    d2 = d1 + datetime.timedelta(days=30)\n\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(mdates.date2num(d1), mdates.date2num(d2))\n\n    formatter = mdates.AutoDateFormatter(locator)\n    assert formatter([datetime.datetime(1997, 1, 10)]) == ['Jan 10']\n", "def test_microsecond_locator():\n    # test MicrosecondLocator with interval_multiples=True\n    plt.rcParams['date.interval_multiples'] = True\n    start = datetime.datetime(2020, 1, 1, 0, 0, 0, 100)\n    end = datetime.datetime(2020, 1, 1, 0, 0, 1, 900)\n    fig, ax = plt.subplots()\n    ax.set_xlim(start, end)\n    locator = mdates.MicrosecondLocator(interval=200)\n    ax.xaxis.set_major_locator(locator)\n    fig.canvas.draw()\n    assert len(ax.get_xticklabels()) == 5\n    assert ax.get_xticklabels()[0].get_text() == '00:00:00.100'\n    assert ax.get_xticklabels()[-1].get_text() == '00:00:00.900'\n"], "sample_899": ["def test_check_estimators_unfitted_correct_exception():\n    # check that a CorrectNotFittedError is raised when calling predict\n    # on an unfitted estimator\n    msg = \"CorrectNotFittedError not raised by predict\"\n    assert_raises_regex(AssertionError, msg, check_estimators_unfitted,\n                        \"estimator\", CorrectNotFittedErrorClassifier())\n", "def test_check_estimators_unfitted_pairwise():\n    # check that a ValueError/AttributeError is raised when calling predict\n    # on an unfitted estimator with _pairwise\n    msg = \"AttributeError or ValueError not raised by predict\"\n    assert_raises_regex(AssertionError, msg, check_estimators_unfitted,\n                        \"estimator\", SVC(kernel='rbf'))\n", "def test_check_estimator_with_custom_estimator():\n    # check that check_estimator() works with custom estimator\n    class CustomEstimator(BaseEstimator):\n            return self\n            return np.ones(X.shape[0])\n            return np.ones((X.shape[0], 2))\n\n    check_estimator(CustomEstimator)\n", "def test_check_estimator_multioutput_estimator_convert_y_2d():\n    # check that multioutput estimators correctly convert y to 2D\n    from sklearn.utils.estimator_checks import multioutput_estimator_convert_y_2d\n\n    class DummyEstimator(BaseEstimator):\n            self.multi_output = multi_output\n\n    est = DummyEstimator(multi_output=True)\n    y = np.array([1, 2, 3])\n    y_2d = multioutput_estimator_convert_y_2d(est, y)\n    assert_equal(y_2d.shape, (3, 1))\n    assert_array_equal(y_2d, np.array([[1], [2], [3]]))\n\n    est = DummyEstimator(multi_output=False)\n    y_2d = multioutput_estimator_convert_y_2d(est, y)\n    assert_array_equal(y_2d, y)\n", "def test_check_estimator_large_sparse_input():\n    # check that check_estimator() works on estimator that support large sparse input\n\n    class LargeSparseSupportedClassifier(BaseEstimator):\n            X, y = check_X_y(X, y,\n                             accept_sparse=(\"csr\", \"csc\", \"coo\"),\n                             accept_large_sparse=True,\n                             multi_output=True,\n                             y_numeric=True)\n            return self\n\n    check_estimator(LargeSparseSupportedClassifier)\n"], "sample_721": ["def test_check_array_force_all_finite_complex():\n    X = np.array([[1 + 1j, 2 + 2j], [3 + 3j, 4 + 4j]])\n    with pytest.raises(ValueError, match=\"Complex data not supported\"):\n        check_array(X, force_all_finite=True)\n", "def test_check_array_force_all_finite_invalid_type():\n    X = np.arange(4).reshape(2, 2).astype(np.float)\n    X[0, 0] = np.nan\n    with pytest.raises(TypeError, match=\"force_all_finite should be a bool or \\\"allow-nan\\\". Got 'invalid' instead\"):\n        check_array(X, force_all_finite='invalid', accept_sparse=True)\n", "def test_check_array_min_samples_and_features_estimator_name():\n    # Test that the estimator name is included in the error messages\n    X = np.ones((1, 10))\n    y = np.ones(1)\n    estimator_name = \"SomeEstimator\"\n    msg = \"1 sample(s) (shape=(1, 10)) while a minimum of 2 is required by SomeEstimator.\"\n    assert_raise_message(ValueError, msg, check_X_y, X, y,\n                         ensure_min_samples=2, estimator=estimator_name)\n", "def test_check_array_with_estimator_in_error_message():\n    X = np.array([[np.nan, 2], [3, 4]])\n    estimator = \"MyEstimator\"\n    msg = \"Data with input dtype float64 was converted to float32 by MyEstimator.\"\n    with pytest.warns(DataConversionWarning, match=msg):\n        check_array(X, dtype=np.float32, warn_on_dtype=True, estimator=estimator)\n", "def test_check_array_complex_data_warning():\n    # Test that a warning is raised when converting complex data to real\n    X = np.array([[1 + 1j, 2 + 2j], [3 + 3j, 4 + 4j]])\n    with pytest.warns(UserWarning, match=\"Casting complex values to real discards the imaginary part\"):\n        X_checked = check_array(X, dtype=np.float64)\n    assert_allclose_dense_sparse(X_checked, np.array([[1, 2], [3, 4]]))\n"], "sample_915": ["def test_isabstractmethod(app):\n    from target.methods import AbstractBaseClass, ConcreteClass\n\n    assert inspect.isabstractmethod(AbstractBaseClass.abstractmethod) is True\n    assert inspect.isabstractmethod(ConcreteClass.concretemethod) is False\n", "def test_isabstractmethod(app):\n    from abc import ABC, abstractmethod\n\n    class Base(ABC):\n        @abstractmethod\n            pass\n\n            pass\n\n    assert inspect.isabstractmethod(Base.abstractmeth) is True\n    assert inspect.isabstractmethod(Base.nonabstractmeth) is False\n", "def test_signature_annotations_with_forwardref():\n    from typing_test_data import (f15, Unknown)\n\n    sig = inspect.signature(f15)\n    assert stringify_signature(sig) == '(x: Unknown, y: int) -> Any'\n\n    sig = inspect.signature(f15, globals(), locals())\n    assert stringify_signature(sig) == '(x: typing_test_data.Unknown, y: int) -> Any'\n", "def test_isabstractmethod(app):\n    from target.methods import Base\n    from abc import ABC, abstractmethod\n\n    class AbstractClass(ABC):\n        @abstractmethod\n            pass\n\n    assert inspect.isabstractmethod(AbstractClass.abstract_meth) is True\n    assert inspect.isabstractmethod(Base.meth) is False\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, \"__new__\") is True\n    assert inspect.is_builtin_class_method(int, \"__init__\") is True\n    assert inspect.is_builtin_class_method(str, \"__new__\") is True\n    assert inspect.is_builtin_class_method(str, \"__init__\") is True\n    assert inspect.is_builtin_class_method(list, \"__init__\") is False\n    assert inspect.is_builtin_class_method(datetime.datetime, \"__init__\") is False\n"], "sample_649": ["def test_log_file_cli_with_unicode_filename(pytester: Pytester) -> None:\n    log_file = str(pytester.path.joinpath(\"pytest_unicode.log\"))\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = pytester.runpytest(\"-s\", f\"--log-file={log_file}\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"test_log_file_cli_with_unicode_filename.py PASSED\"])\n\n    # make sure that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"This log message will be shown\" in contents\n", "def test_log_file_cli_unicode(pytester: Pytester) -> None:\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n    pytester.makepyfile(\n        \"\"\"\\\n        import logging\n\n            logger = logging.getLogger('catchlog')\n            logger.info(\"Unicode message: \u00f1\u00f3\u00f2\u00e1\u00e9\u00ed\")\n            assert \"\u00f1\u00f3\u00f2\u00e1\u00e9\u00ed\" in logger.handlers[0].stream.getvalue()\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    # make sure that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"Unicode message: \u00f1\u00f3\u00f2\u00e1\u00e9\u00ed\" in contents\n", "def test_messages_logged_with_caplog(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import sys\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            sys.stdout.write('text going to stdout')\n            sys.stderr.write('text going to stderr')\n            logger.info('text going to logger')\n            assert 'text going to logger' in caplog.text\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"*- Captured stdout call -*\", \"text going to stdout\"])\n    result.stdout.fnmatch_lines([\"*- Captured stderr call -*\", \"text going to stderr\"])\n    result.stdout.fnmatch_lines([\"*- Captured log call -*\", \"*text going to logger*\"])\n", "def test_log_format_and_date_format(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import time\n        from datetime import datetime\n\n            logger = logging.getLogger('test')\n            with caplog.at_level(logging.INFO):\n                logger.info('Test message')\n                time.sleep(1)  # Ensure different timestamps\n                logger.info('Another test message')\n            assert len(caplog.records) == 2\n            assert caplog.records[0].msg == 'Test message'\n            assert caplog.records[1].msg == 'Another test message'\n            assert caplog.records[1].created - caplog.records[0].created == pytest.approx(1, abs=0.1)\n            expected_format = '%(levelname)s - %(message)s'\n            expected_date_format = '%Y-%m-%d %H:%M:%S'\n            expected_time = datetime.fromtimestamp(caplog.records[0].created).strftime(expected_date_format)\n            assert caplog.text == f'INFO - Test message\\\\n{expected_time} - INFO - Another test message\\\\n'\n        \"\"\"\n    )\n    result = testdir.runpytest(\n        \"--log-format='%(levelname)s - %(message)s'\",\n        \"--log-date-format='%Y-%m-%d %H:%M:%S'\",\n    )\n    assert result.ret == ExitCode.OK\n    assert not result.stderr.lines\n", "def test_log_cli_with_setup_and_teardown(pytester: Pytester, request: FixtureRequest) -> None:\n    filename = request.node.name + \".py\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n        @pytest.fixture\n            logging.warning(\"log message from setup of {}\".format(request.node.name))\n            yield\n            logging.warning(\"log message from teardown of {}\".format(request.node.name))\n\n            logging.warning(\"log message from test_log_1\")\n\n            logging.warning(\"log message from test_log_2\")\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            f\"{filename}::test_log_1 \",\n            \"*WARNING*log message from setup of test_log_1*\",\n            \"*WARNING*log message from test_log_1*\",\n            \"*WARNING*log message from teardown of test_log_1*\",\n            \"PASSED *50%*\",\n            f\"{filename}::test_log_2 \",\n            \"*WARNING*log message from setup of test_log_2*\",\n            \"*WARNING*log message from test_log_2*\",\n            \"*WARNING*log message from teardown of test_log_2*\",\n            \"PASSED *100%*\",\n            \"=* 2 passed in *=\",\n        ]\n    )\n"], "sample_680": ["def test_xfail_condition_invalid_boolean(self, testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class InvalidBool:\n                raise TypeError(\"INVALID\")\n\n        @pytest.mark.xfail(InvalidBool(), reason=\"xxx\")\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*Error evaluating 'xfail' condition as a boolean*\", \"*INVALID*\"])\n", "def test_xfail_imperative_without_reason(self, testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail()\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n", "def test_skipif_with_boolean_without_reason(self, testdir) -> None:\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(False)\n            pass\n        \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_xfail_marks(item)\n    assert excinfo.value.msg is not None\n    assert (\n        \"\"\"Error evaluating 'xfail': you need to specify reason=STRING when using booleans as conditions.\"\"\"\n        in excinfo.value.msg\n    )\n", "def test_marked_xfail_with_boolean_without_reason(self, testdir) -> None:\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(False)\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_xfail_marks(item)\n    assert excinfo.value.msg is not None\n    assert (\n        \"\"\"Error evaluating 'xfail': you need to specify reason=STRING when using booleans as conditions.\"\"\"\n        in excinfo.value.msg\n    )\n", "def test_skip_with_invalid_reason(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n\n        class InvalidStr:\n                raise TypeError(\"INVALID\")\n\n        @pytest.mark.skip(reason=InvalidStr())\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error while converting 'skip' reason to string\" in excinfo.value.msg\n    assert \"INVALID\" in excinfo.value.msg\n"], "sample_551": ["def test_contour3d_legend():\n    x, y = np.mgrid[1:10, 1:10]\n    h = x * y\n    colors = ['blue', '#00FF00', 'red']\n\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    cs = ax.contour3D(x, y, h, levels=[10, 30, 50], colors=colors, extend='both')\n\n    artists, labels = cs.legend_elements()\n    assert labels == ['$x = 10.0$', '$x = 30.0$', '$x = 50.0$']\n    assert all(isinstance(a, LineCollection) for a in artists)\n    assert all(same_color(a.get_color(), c)\n               for a, c in zip(artists, colors))\n", "def test_text3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    zdirs = (None, 'x', 'y', 'z', (1, 1, 0), (1, 1, 1))\n    xs = (1, 4, 4, 9, 4, 4)\n    ys = (2, 5, 8, 10, 1, 2)\n    zs = (10, 3, 8, 9, 1, 2)\n    for zdir, x, y, z in zip(zdirs, xs, ys, zs):\n        label = '(%d, %d, %d), dir=%s' % (x, y, z, zdir)\n        ax.text(x, y, z, label, zdir=zdir)\n    ax.text(9, 0, 0, \"Default direction (None)\")\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.set_zlim(0, 10)\n", "def test_patchcollection_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a list of rectangles with different heights\n    rectangles = [plt.Rectangle((i, i), 1, 1) for i in range(5)]\n    zs = [i for i in range(5)]\n\n    # Convert the rectangles to 3D patches and add them to the plot\n    collection = art3d.PatchCollection(rectangles, zs=zs, zdir='z')\n    ax.add_collection3d(collection)\n\n    # Set the plot limits\n    ax.set_xlim(0, 5)\n    ax.set_ylim(0, 5)\n    ax.set_zlim(0, 5)\n\n    # Save the figure\n    plt.savefig(__file__.replace('.py', '_test_patchcollection_3d.png'))\n", "def test_patch3dcollection_shade():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Create a simple 3D surface\n    x, y = np.meshgrid(np.linspace(-1, 1, 10), np.linspace(-1, 1, 10))\n    z = np.sin(np.sqrt(x**2 + y**2))\n\n    # Create a Patch3DCollection with shading\n    collection = art3d.Patch3DCollection([[np.array([x[i], y[i], z[i]]) for i in range(len(x))] for x, y, z in zip(x, y, z)],\n                                        shade=True, facecolors='r', alpha=0.5)\n    ax.add_collection3d(collection)\n\n    plt.show()\n", "def test_3d_patch_collection():\n    from matplotlib.patches import Circle\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    patches = [Circle((0, 0), radius=1), Circle((1, 1), radius=0.5)]\n    p = art3d.Patch3DCollection(patches, zs=[0, 1], zdir='z')\n    ax.add_collection(p)\n    # Add assertions to check if the patches were added correctly\n    assert len(ax.collections) == 1\n    assert isinstance(ax.collections[0], art3d.Patch3DCollection)\n    assert len(ax.collections[0].get_paths()) == len(patches)\n"], "sample_839": ["def test_countvectorizer_min_df_float():\n    test_data = ['abc', 'dea', 'eat']\n    vect = CountVectorizer(analyzer='char', min_df=0.5)\n    vect.fit(test_data)\n    assert 'a' not in vect.vocabulary_.keys()  # {bcdet} ignored\n    assert len(vect.vocabulary_.keys()) == 4    # {ae} remain\n    assert 'a' in vect.stop_words_\n    assert len(vect.stop_words_) == 2\n", "def test_tfidf_transformer_norm():\n    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)\n    X_l1_norm = TfidfTransformer(norm='l1').fit_transform(X)\n    X_l2_norm = TfidfTransformer(norm='l2').fit_transform(X)\n    assert_array_almost_equal(np.abs(X_l1_norm).sum(axis=1), np.ones(10))\n    assert_array_almost_equal(np.square(X_l2_norm).sum(axis=1), np.ones(10))\n", "def test_tfidfvectorizer_invalid_params():\n    # Test for ValueError when max_df is less than 0\n    with pytest.raises(ValueError, match=\"negative value for max_df\"):\n        TfidfVectorizer(max_df=-0.1)\n\n    # Test for ValueError when min_df is less than 0\n    with pytest.raises(ValueError, match=\"negative value for min_df\"):\n        TfidfVectorizer(min_df=-0.1)\n\n    # Test for ValueError when max_features is not a positive integer\n    with pytest.raises(ValueError, match=\"max_features\"):\n        TfidfVectorizer(max_features=-1)\n", "def test_countvectorizer_custom_preprocessor():\n    data = [\"J'ai mang\u00e9 du kangourou ce midi.\", \"C'\u00e9tait pas tr\u00e8s bon.\"]\n    vectorizer = CountVectorizer(preprocessor=strip_accents_unicode)\n    X = vectorizer.fit_transform(data)\n    expected_vocabulary = {'ai': 2, 'mange': 1, 'du': 0, 'kangourou': 3, 'ce': 4, 'midi': 5, 'etait': 6, 'pas': 7, 'tres': 8, 'bon': 9}\n    assert vectorizer.vocabulary_ == expected_vocabulary\n", "def test_callable_analyzer_with_input_type(Estimator, input_type, tmpdir):\n    data = ['this is text, not file or filename']\n    analyzer = lambda x: x.split()\n    estimator = Estimator(analyzer=analyzer, input=input_type)\n    if input_type == 'file':\n        data = [StringIO(doc) for doc in data]\n    elif input_type == 'filename':\n        files = [tmpdir.join(f'doc{i}.txt') for i in range(len(data))]\n        for file, doc in zip(files, data):\n            file.write(doc)\n        data = [str(file) for file in files]\n    X = estimator.fit_transform(data)\n    assert X.shape[0] == len(data)\n"], "sample_782": ["def test_column_transformer_slice_with_negative_step():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_res_first_second = np.array([[0, 2], [1, 4], [2, 6]]).T\n\n    ct = ColumnTransformer([('trans', Trans(), slice(None, None, -1))],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_res_first_second)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first_second)\n", "def test_column_transformer_no_fit_transform():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans1', 'drop', [0])],\n                           remainder=DoubleTrans())\n\n    with pytest.raises(NotFittedError):\n        ct.transform(X_array)\n", "def test_column_transformer_duplicate_columns():\n    X_array = np.array([[0, 1, 1], [2, 4, 6]]).T\n\n    # Test with duplicate columns\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [0])])\n    with pytest.raises(ValueError):\n        ct.fit_transform(X_array)\n\n    # Test with duplicate columns and 'drop'\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', 'drop', [0])])\n    X_res_second = np.array([1, 6]).reshape(-1, 1)\n    assert_array_equal(ct.fit_transform(X_array), X_res_second)\n\n    # Test with duplicate columns and 'passthrough'\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', 'passthrough', [0])])\n    X_res_both = X_array\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n", "def test_column_transformer_invalid_transformer_type():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', \"invalid_transformer\", [0])])\n    assert_raise_message(TypeError, \"All estimators should implement fit\",\n                         ct.fit, X_array)\n", "def test_column_transformer_drop_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_res_second_third = X_array[:, 1:]\n\n    ct = ColumnTransformer([('trans1', 'drop', [0])], remainder='passthrough')\n    assert_array_equal(ct.fit_transform(X_array), X_res_second_third)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_second_third)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n"], "sample_945": ["def test_no_warning_for_valid_references(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' not in warning.getvalue()\n    assert ('index.rst:6: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            not in warning.getvalue())\n", "def test_warn_missing_reference_with_target(app, status, warning):\n    app.build()\n    assert 'index.rst:8: WARNING: undefined label: no-label' in warning.getvalue()\n", "def test_pyfunction_with_kwonlyargs(app):\n    text = \".. py:function:: hello(a, *, b: int = 1, **kwargs: str)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc_sig_operator, \"*\"],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"int\"],\n                                                        \" \",\n                                                        [desc_sig_operator, \"=\"],\n                                                        \" \",\n                                                        [nodes.inline, \"1\"])],\n                                      [desc_parameter, ([desc_sig_operator, \"**\"],\n                                                        [desc_sig_name, \"kwargs\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"str\"])])])\n", "def test_pyclassmethod_with_module_option(app):\n    text = (\".. py:classmethod:: meth\\n\"\n            \"   :module: example\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"classmethod \"],\n                                                    [desc_addname, \"example.\"],\n                                                    [desc_name, \"meth\"])],\n                                  [desc_content, ()])]))\n    assert 'example.meth' in domain.objects\n    assert domain.objects['example.meth'] == ('index', 'example.meth', 'method', False)\n", "def test_py_xref_reference(app):\n    text = (\".. py:module:: foo\\n\"\n            \"\\n\"\n            \".. py:class:: Bar\\n\"\n            \"\\n\"\n            \"   .. py:method:: baz\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: qux\\n\"\n            \"\\n\"\n            \"   .. py:property:: corge\\n\"\n            \"\\n\"\n            \"   See :py:class:`Bar`, :py:meth:`Bar.baz`, :py:attr:`Bar.qux`, and :py:meth:`Bar.corge`.\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"foo.\"],\n                                                    [desc_name, \"Bar\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc,\n                                                  nodes.paragraph,\n                                                  nodes.inline,\n                                                  pending_xref,\n                                                  nodes.inline,\n                                                  pending_xref,\n                                                  nodes.inline,\n                                                  pending_xref,\n                                                  nodes.inline,\n                                                  pending_xref)])]))\n    assert_node(doctree[3][1][5], (nodes.inline,\n                                   pending_xref,\n                                   nodes.inline,\n                                   pending_xref,\n                                   nodes.inline,\n                                   pending_xref,\n                                   nodes.inline,\n                                   pending_xref))\n    assert_node(doctree[3][1][5][1], pending_xref, refdomain=\"py\", reftype=\"class\", re"], "sample_571": ["def test_regplot_scatter_kws_marker(self):\n    f, ax = plt.subplots()\n    marker = 'x'\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df,\n                    scatter_kws={'marker': marker})\n    assert ax.collections[0]._marker == marker\n", "def test_regplot_seed(self):\n    seed = 123\n    np.random.seed(seed)\n    x1 = np.random.randn(50)\n    y1 = np.random.randn(50)\n    np.random.seed(seed)\n    x2 = np.random.randn(50)\n    y2 = np.random.randn(50)\n\n    f, ax1 = plt.subplots()\n    lm.regplot(x=x1, y=y1, ax=ax1, seed=seed)\n    line1 = ax1.lines[0]\n\n    f, ax2 = plt.subplots()\n    lm.regplot(x=x2, y=y2, ax=ax2, seed=seed)\n    line2 = ax2.lines[0]\n\n    npt.assert_array_equal(line1.get_xdata(), line2.get_xdata())\n    npt.assert_array_equal(line1.get_ydata(), line2.get_ydata())\n", "def test_regplot_xlim_truncate(self):\n\n    f, ax = plt.subplots()\n    x, y1, y2 = np.random.randn(3, 50)\n    lm.regplot(x=x, y=y1, truncate=True)\n    lm.regplot(x=x, y=y2, truncate=True)\n    line1, line2 = ax.lines\n    assert np.array_equal(line1.get_xdata(), line2.get_xdata())\n    assert np.min(line1.get_xdata()) >= np.min(x)\n    assert np.max(line1.get_xdata()) <= np.max(x)\n", "def test_regplot_scatter_kws_marker(self):\n\n    f, ax = plt.subplots()\n    marker = \"x\"\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df,\n                    scatter_kws={'marker': marker})\n    assert ax.collections[0].get_marker() == marker\n\n    f, ax = plt.subplots()\n    marker = \"+\"\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df,\n                    scatter_kws={'marker': marker})\n    assert ax.collections[0].get_marker() == marker\n", "def test_regplot_order(self):\n\n    x = np.linspace(-3, 3, 50)\n    y = x**2 + np.random.normal(size=50)\n\n    ax = lm.regplot(x=x, y=y, order=2)\n    assert len(ax.lines) == 2  # One for the regression line and one for the confidence interval\n\n    x_plot, y_plot = ax.lines[1].get_data()\n    assert len(x_plot) > 2  # Check if the regression line has more than two points\n    npt.assert_array_equal(x, x_plot)  # Check if the x values of the plot are the same as the input x values\n"], "sample_163": ["def test_logout_then_login_with_custom_redirect_url(self):\n    self.login()\n    req = HttpRequest()\n    req.method = \"POST\"\n    csrf_token = get_token(req)\n    req.COOKIES[settings.CSRF_COOKIE_NAME] = csrf_token\n    req.POST = {\"csrfmiddlewaretoken\": csrf_token}\n    req.session = self.client.session\n    response = logout_then_login(req, login_url=\"/custom_login/\")\n    self.confirm_logged_out()\n    self.assertRedirects(response, \"/custom_login/\", fetch_redirect_response=False)\n", "def test_user_change_different_user_password_fail(self):\n    u = User.objects.get(email=\"staffmember@example.com\")\n    response = self.client.post(\n        reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,)),\n        {\n            \"password1\": \"password1\",\n            \"password2\": \"password2\",\n        },\n    )\n    self.assertContains(response, \"The two password fields didn\u00e2\u20ac\u2122t match.\")\n    u.refresh_from_db()\n    self.assertNotEqual(u.password, \"password1\")\n", "    def test_user_creation_form(self):\n        url = reverse(\"auth_test_admin:auth_user_add\")\n        data = {\n            \"username\": \"newuser\",\n            \"password1\": \"password1\",\n            \"password2\": \"password1\",\n            \"email\": \"newuser@example.com\",\n        }\n        response = self.client.post(url, data)\n        self.assertRedirects(response, reverse(\"auth_test_admin:auth_user_changelist\"))\n        row = LogEntry.objects.latest(\"id\")\n        self.assertEqual(row.get_change_message(), \"Added.\")\n        user = User.objects.get(username=\"newuser\")\n        self.assertEqual(user.email, \"newuser@example.com\")\n        self.assertTrue(user.check_password(\"password1\"))\n", "def test_user_change_password_unauthenticated(self):\n    url = reverse(\n        \"auth_test_admin:auth_user_password_change\", args=(self.admin.pk,)\n    )\n    self.logout()\n    response = self.client.get(url)\n    login_url = reverse(\"admin:login\")\n    self.assertRedirects(response, f\"{login_url}?next={url}\")\n", "    def setUpTestData(cls):\n        cls.superuser = UUIDUser.objects.create_superuser(\n            username=\"admin\", email=\"admin@example.com\", password=\"admin\"\n        )\n"], "sample_637": ["def test_regex_codetag(self) -> None:\n    code = \"\"\"a = 1\n            # TEST\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"TEST\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "    def test_fixme_disabled_via_pylint_pragma(self) -> None:\n        code = \"\"\"a = 1\n                # pylint: disable=fixme\n                # FIXME message\n                \"\"\"\n        with self.assertNoMessages():\n            self.checker.process_tokens(_tokenize_str(code))\n", "def test_regex_codetag(self) -> None:\n    code = \"\"\"a = 1\n            #???\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"???\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_notes_rgx_codetag(self) -> None:\n    code = \"\"\"a = 1\n            #???\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"???\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_disable_codetag_in_comment(self) -> None:\n    code = \"\"\"a = 1\n              # pylint: disable=fixme\n              # TODO\n              \"\"\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(code))\n"], "sample_667": ["def test_tmp_path_factory_without_basetemp(testdir, monkeypatch):\n    \"\"\"Test that tmp_path_factory works even if basetemp is not specified.\"\"\"\n    monkeypatch.delenv(\"PYTEST_DEBUG_TEMPROOT\", raising=False)\n    testdir.makepyfile(\n        \"\"\"\n            assert tmp_path.is_dir()\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_ensure_deletable(tmp_path):\n    from _pytest.pathlib import ensure_deletable\n\n    folder = pathlib.make_numbered_dir(root=tmp_path, prefix=\"test-\")\n    assert ensure_deletable(folder)\n\n    pathlib.create_cleanup_lock(folder)\n    assert not ensure_deletable(folder)\n", "def test_getbasetemp_without_basetemp_argument(tmp_path, monkeypatch):\n    \"\"\"Test getbasetemp when no basetemp argument is provided.\"\"\"\n    from _pytest.tmpdir import TempPathFactory\n\n    # Clear any existing environment variable\n    monkeypatch.delenv(\"PYTEST_DEBUG_TEMPROOT\", raising=False)\n\n    # Create a new TempPathFactory instance\n    t = TempPathFactory.from_config(FakeConfig(None))\n\n    # Check that getbasetemp creates a new directory\n    basetemp = t.getbasetemp()\n    assert basetemp.exists()\n\n    # Check that a second call to getbasetemp returns the same directory\n    basetemp2 = t.getbasetemp()\n    assert basetemp2 == basetemp\n", "def test_tmp_path_factory_from_config(testdir):\n    from _pytest.tmpdir import TempPathFactory\n\n    class FakeConfig:\n            self.option = self\n            self.trace = self\n            self.basetemp = basetemp\n\n            return lambda *k: None\n\n    config = FakeConfig(basetemp='/test/path')\n    factory = TempPathFactory.from_config(config)\n    assert factory._given_basetemp == Path('/test/path')\n    assert factory._basetemp is None\n", "def test_tmp_path_factory_mkdtemp(testdir):\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(testdir.tmpdir)\n    factory = TempPathFactory.from_config(config)\n\n    # Test that mktemp creates a directory when numbered is False\n    tmp_path = factory.mktemp(\"custom_dir\", numbered=False)\n    assert tmp_path.is_dir()\n\n    # Test that mktemp creates a numbered directory when numbered is True\n    tmp_path_numbered = factory.mktemp(\"custom_dir_numbered\", numbered=True)\n    assert tmp_path_numbered.is_dir()\n    assert \"custom_dir_numbered\" in tmp_path_numbered.name\n\n    # Test that mktemp creates unique directories when numbered is True\n    tmp_path_numbered_2 = factory.mktemp(\"custom_dir_numbered\", numbered=True)\n    assert tmp_path_numbered_2.is_dir()\n    assert tmp_path_numbered_2 != tmp_path_numbered\n"], "sample_337": ["def test_csrf_token_on_403_changes_on_every_request(self):\n    response = self.client.get('/csrf_token_on_403/')\n    self.assertEqual(response.status_code, 403)\n    token1 = response.cookies['csrftoken'].value\n    response = self.client.get('/csrf_token_on_403/')\n    self.assertEqual(response.status_code, 403)\n    token2 = response.cookies['csrftoken'].value\n    self.assertNotEqual(token1, token2)\n", "def test_csrf_cookie_samesite_lax(self):\n    req = self._get_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    self.assertEqual(resp.cookies['csrfcookie']['samesite'], 'Lax')\n", "def test_https_good_referer_with_cookie_domain_no_port(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted from a\n    subdomain that's allowed by CSRF_COOKIE_DOMAIN and no port specified.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'https://foo.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_csrf_cookie_reset_with_short_token(self):\n    \"\"\"\n    If a token shorter than expected is provided, the CSRF cookie is reset.\n    \"\"\"\n    req = self._get_POST_request_with_token(cookie='short')\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, token_view, (), {})\n    self.assertIsNone(resp)\n    resp = mw(req)\n    csrf_cookie = self._read_csrf_cookie(req, resp)\n    self.assertEqual(len(csrf_cookie), CSRF_TOKEN_LENGTH)\n    self.assertNotEqual(csrf_cookie, 'short')\n", "def test_csrf_token_on_403_stays_constant(self):\n    response = self.client.post('/post-form/', data={'csrfmiddlewaretoken': 'incorrect_token'})\n    # The POST request with an incorrect token returns status code 403.\n    self.assertEqual(response.status_code, 403)\n    token1 = response.cookies['csrftoken'].value\n    response = self.client.post('/post-form/', data={'csrfmiddlewaretoken': 'incorrect_token'})\n    self.assertEqual(response.status_code, 403)\n    token2 = response.cookies['csrftoken'].value\n    self.assertTrue(equivalent_tokens(token1, token2))\n"], "sample_59": ["def test_model_with_unique_together_constraint(self):\n    \"\"\"\n    Test that the unique_together constraint is enforced\n    \"\"\"\n    # Create two objects with the same values for the unique_together fields\n    Worker.objects.create(name=\"John Doe\", department_id=1)\n    with self.assertRaises(ValidationError):\n        Worker.objects.create(name=\"John Doe\", department_id=1)\n", "def test_save_base_force_update_without_pk(self):\n    \"\"\"\n    Regression test for #27864: save_base with force_update=True and no primary key.\n    \"\"\"\n    event = Event(when=datetime.datetime(2000, 1, 1))\n    msg = \"Cannot force an update in save() with no primary key.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        event.save_base(force_update=True)\n", "def test_model_with_long_column_names(self):\n    \"\"\"\n    Check that column names are not too long for the database\n    \"\"\"\n    long_name = 'a' * 65\n    with self.assertRaisesMessage(ValidationError, 'ensure this value has at most 64 characters'):\n        NonAutoPK.objects.create(name=long_name)\n", "def test_model_base_new_abstract(self):\n    \"\"\"\n    Regression test for #25650: ModelBase.__new__() should not set Meta.abstract\n    to False when creating an abstract model.\n    \"\"\"\n\n    class AbstractModel(models.Model):\n        class Meta:\n            abstract = True\n\n    self.assertTrue(AbstractModel._meta.abstract)\n", "def test_model_base_contribute_to_class(self):\n    \"\"\"\n    Regression test for #12345: ModelBase.contribute_to_class() handles multiple inheritance correctly\n    \"\"\"\n\n    class Parent1(models.Model):\n        field1 = models.CharField(max_length=50)\n\n    class Parent2(models.Model):\n        field2 = models.IntegerField()\n\n    class Child(Parent1, Parent2):\n        field3 = models.BooleanField()\n\n    child = Child(field1=\"test\", field2=123, field3=True)\n    self.assertEqual(child.field1, \"test\")\n    self.assertEqual(child.field2, 123)\n    self.assertEqual(child.field3, True)\n"], "sample_1085": ["def test_Float_floordiv():\n    assert Float(2.7) // Float(1.2) == 2.0\n    assert Float(2.7) // 1.2 == 2.0\n    assert 2.7 // Float(1.2) == 2.0\n", "def test_Float_floordiv():\n    assert Float(2.7) // S.Half == 5.0\n    assert Float(2.7) // Float(0.5) == 5.0\n", "def test_Integer_floordiv():\n    a = Integer(7)\n    b = Integer(2)\n\n    assert a // b == 3\n    assert b // a == 0\n    assert a // 1 == 7\n    assert 1 // a == 0\n", "def test_Number_floordiv():\n    assert S(2)//S.Half == 4\n    assert S(2)//S(3) == 0\n    assert S(5)//S(2) == 2\n    assert S(5)//S(-2) == -3\n    assert S(-5)//S(2) == -3\n    assert S(-5)//S(-2) == 2\n    assert S(2)//S(0) == zoo\n    assert S(0)//S(2) == 0\n    assert S(0)//S(0) == nan\n    assert S(2)//S.NaN == nan\n    assert S.NaN//S(2) == nan\n    assert S(2)//S.Infinity == 0\n    assert S.Infinity//S(2) == S.Infinity\n    assert S(2)//S.NegativeInfinity == 0\n    assert S.NegativeInfinity//S(2) == S.NegativeInfinity\n    assert S(2)//S.ComplexInfinity == 0\n    assert S.ComplexInfinity//S(2) == S.ComplexInfinity\n", "def test_Float_floordiv():\n    a = Float('3.5')\n    b = Float('1.2')\n    assert a // b == Float('2.0')\n    assert b // a == Float('0.0')\n    assert a // S.Half == Float('7.0')\n"], "sample_54": ["def test_file_from_buffer_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt')\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_buffer_with_custom_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename=\"custom.txt\", as_attachment=True)\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"custom.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_buffer_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt')\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_buffer_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt')\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_memory(self):\n    content = b'test content'\n    response = FileResponse(io.BytesIO(content), as_attachment=True, filename='test.txt')\n    self.assertEqual(response['Content-Length'], str(len(content)))\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n    self.assertEqual(list(response), [content])\n    response.close()\n"], "sample_491": ["def test_custom_form_template_name(self):\n    class CustomForm(Form):\n        first_name = CharField()\n        template_name = \"forms_tests/custom_form_snippet.html\"\n\n    t = Template(\"{{ form }}\")\n    html = t.render(Context({\"form\": CustomForm()}))\n    expected = \"\"\"\n    <div class=\"customFormWrapper\"><label for=\"id_first_name\">First name:</label>\n    <input type=\"text\" name=\"first_name\" required id=\"id_first_name\"></div>\n    \"\"\"\n    self.assertHTMLEqual(html, expected)\n", "    def test_legend_tag_with_custom_attributes(self):\n        f = CustomFrameworkForm()\n        self.assertHTMLEqual(\n            f[\"language\"].legend_tag(attrs={'class': 'custom-class'}),\n            '<legend for=\"id_language\" class=\"custom-class required\">Language:</legend>',\n        )\n", "def test_custom_renderer_error_list(self):\n    class CommentForm(Form):\n        name = CharField(max_length=50, required=False)\n        email = EmailField()\n        comment = CharField()\n\n    class CustomErrorList(ErrorList):\n        template_name = \"forms_tests/error.html\"\n\n    data = {\"email\": \"invalid\"}\n    f = CommentForm(data, auto_id=False, error_class=CustomErrorList)\n    self.assertHTMLEqual(\n        f.errors.as_ul(),\n        '<div class=\"errorlist\">'\n        '<div class=\"error\">Enter a valid email address.</div></div>',\n    )\n", "def test_render_with_custom_renderer(self):\n    class CustomRenderer(DjangoTemplates):\n        form_template_name = \"forms_tests/custom_form.html\"\n        field_template_name = \"forms_tests/custom_field.html\"\n\n    class Person(Form):\n        first_name = CharField()\n\n    f = Person()\n    html = f.render(renderer=CustomRenderer())\n    expected = \"\"\"\n    <div id=\"custom-form\">\n      <div class=\"fieldWrapper\">\n        <label for=\"id_first_name\">First name:</label>\n        <p>Custom Field<p>\n        <input type=\"text\" name=\"first_name\" required id=\"id_first_name\">\n      </div>\n    </div>\n    \"\"\"\n    self.assertHTMLEqual(html, expected)\n", "def test_custom_renderer_field_template_name_override(self):\n    class Person(Form):\n        first_name = CharField(template_name=\"forms_tests/custom_field.html\")\n\n    get_default_renderer.cache_clear()\n    t = Template(\"{{ form.first_name.as_field_group }}\")\n    html = t.render(Context({\"form\": Person()}))\n    expected = \"\"\"\n    <label for=\"id_first_name\">First name:</label>\n    <p>Custom Field<p>\n    <input type=\"text\" name=\"first_name\" required id=\"id_first_name\">\n    \"\"\"\n    self.assertHTMLEqual(html, expected)\n    get_default_renderer.cache_clear()\n"], "sample_327": ["def test_custom_encoder_decoder_with_invalid_json(self):\n    class CustomDecoder(json.JSONDecoder):\n            return super().__init__(object_hook=self.as_uuid, *args, **kwargs)\n\n            if 'uuid' in dct:\n                dct['uuid'] = uuid.UUID(dct['uuid'])\n            return dct\n\n    field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean('{\"uuid\": \"invalid-uuid\"}')\n", "def test_custom_encoder_with_django_json_encoder(self):\n    value = {'uuid': uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}')}\n    encoded_value = '{\"uuid\": \"c141e152-6550-4172-a784-05448d98204b\"}'\n    field = JSONField(encoder=DjangoJSONEncoder)\n    self.assertEqual(field.prepare_value(value), encoded_value)\n", "def test_custom_encoder(self):\n    class CustomEncoder(json.JSONEncoder):\n            if isinstance(obj, uuid.UUID):\n                return str(obj)\n            return json.JSONEncoder.default(self, obj)\n\n    value = {'uuid': uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}')}\n    encoded_value = '{\"uuid\": \"c141e152-6550-4172-a784-05448d98204b\"}'\n    field = JSONField(encoder=CustomEncoder)\n    self.assertEqual(field.prepare_value(value), encoded_value)\n", "def test_custom_encoder_decoder_invalid_input(self):\n    class CustomDecoder(json.JSONDecoder):\n            return super().__init__(object_hook=self.as_uuid, *args, **kwargs)\n\n            if 'uuid' in dct:\n                try:\n                    dct['uuid'] = uuid.UUID(dct['uuid'])\n                except ValueError:\n                    dct['uuid'] = 'Invalid UUID'\n            return dct\n\n    invalid_uuid = 'c141e152-6550-4172'\n    value = {'uuid': invalid_uuid}\n    encoded_value = '{\"uuid\": \"' + invalid_uuid + '\"}'\n    field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n    self.assertEqual(field.prepare_value(value), encoded_value)\n    self.assertEqual(field.clean(encoded_value), {'uuid': 'Invalid UUID'})\n", "def test_unicode_input(self):\n    field = JSONField()\n    value = field.clean('{\"a\": \"\u4f60\u597d\uff0c\u4e16\u754c\"}')\n    self.assertEqual(value, {'a': '\u4f60\u597d\uff0c\u4e16\u754c'})\n"], "sample_450": ["def test_get_admin_log_tag(self):\n    \"\"\"\n    Test the get_admin_log template tag.\n    \"\"\"\n    template = Template(\n        \"{% load admin_utils_tags %}\"\n        \"{% get_admin_log 10 as admin_log for_user user %}\"\n    )\n    context = Context({'user': self.user, 'log_entries': LogEntry.objects.all()})\n    template.render(context)\n    self.assertEqual(len(context['admin_log']), 1)\n", "def test_get_admin_log_template_tag(self):\n    # Create additional log entries for other users\n    user2 = User.objects.create_superuser(\n        username=\"super2\", password=\"secret2\", email=\"super2@example.com\"\n    )\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        user2.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something by user2\",\n    )\n\n    # Test the template tag for user's log entries\n    response = self.client.get(\n        reverse('admin:admin_utils_article_change', args=[quote(self.a1.pk)])\n        + '?log_entries_limit=1&log_entries_user=super'\n    )\n    self.assertContains(response, \"Changed something\")\n    self.assertNotContains(response, \"Changed something by user2\")\n\n    # Test the template tag for all log entries\n    response = self.client.get(\n        reverse('admin:admin_utils_article_change', args=[quote(self.a1.pk)])\n        + '?log_entries_limit=2'\n    )\n    self.assertContains(response, \"Changed something\")\n    self.assertContains(response, \"Changed something by user2\")\n", "    def test_get_admin_log_tag(self):\n        \"\"\"\n        Test the custom template tag 'get_admin_log' to populate a template variable with the admin log for the given criteria.\n        \"\"\"\n        # Create some more log entries\n        user2 = User.objects.create_user(\n            username=\"user2\", password=\"secret2\", email=\"user2@example.com\"\n        )\n        content_type_pk = ContentType.objects.get_for_model(Article).pk\n        LogEntry.objects.log_action(\n            user2.pk,\n            content_type_pk,\n            self.a1.pk,\n            repr(self.a1),\n            CHANGE,\n            change_message=\"Changed something by user2\",\n        )\n\n        # Render the template with the custom tag\n        template = Template(\n            \"{% load admin_log %}\"\n            \"{% get_admin_log 10 as admin_log for_user user.pk %}\"\n            \"{% for entry in admin_log %}\"\n            \"{{ entry.user.username }} {{ entry.object_repr }} {{ entry.change_message }}<br>\"\n            \"{% endfor %}\"\n        )\n        context = Context({\"user\": self.user, \"log_entries\": LogEntry.objects.all()})\n        rendered = template.render(context)\n\n        # Check that the rendered output contains the expected log entries\n        self.assertIn(\"super Title {'changed': {'fields': ['Title and History.']}}\", rendered)\n        self.assertNotIn(\"user2 Title {'changed': {'fields': ['Changed something by user2']}}\", rendered)\n\n        # Render the template with the custom tag for a different user\n        template = Template(\n            \"{% load admin_log %}\"\n            \"{% get_admin_log 10 as admin_log for_user user2.pk %}\"\n            \"{% for entry in admin_log %}\"\n            \"{{ entry.user.username }} {{ entry.object_repr }} {{ entry.change_message }}<br>\"\n            \"{% endfor %}\"\n        )\n        context = Context({\"user\": self.user, \"", "def test_get_admin_log_tag(self):\n    \"\"\"\n    Test the 'get_admin_log' template tag with different combinations of arguments.\n    \"\"\"\n    # Test case with no user filter\n    response = self.client.get(reverse(\"admin:admin_utils_article_change\", args=(quote(self.a1.pk),)) + \"?get_admin_log=10&varname=admin_log\")\n    self.assertContains(response, \"Changed Title and History.\")\n    self.assertEqual(len(response.context['admin_log']), 1)\n\n    # Test case with user filter as an ID\n    response = self.client.get(reverse(\"admin:admin_utils_article_change\", args=(quote(self.a1.pk),)) + \"?get_admin_log=10&varname=admin_log&user=1\")\n    self.assertContains(response, \"Changed Title and History.\")\n    self.assertEqual(len(response.context['admin_log']), 1)\n\n    # Test case with user filter as a context variable\n    response = self.client.get(reverse(\"admin:admin_utils_article_change\", args=(quote(self.a1.pk),)) + \"?get_admin_log=10&varname=admin_log&user=user\")\n    self.assertContains(response, \"Changed Title and History.\")\n    self.assertEqual(len(response.context['admin_log']), 1)\n\n    # Test case with user filter as a context variable that doesn't exist\n    with self.assertRaises(KeyError):\n        response = self.client.get(reverse(\"admin:admin_utils_article_change\", args=(quote(self.a1.pk),)) + \"?get_admin_log=10&varname=admin_log&user=nonexistent\")\n\n    # Test case with limit that isn't a number\n    with self.assertRaises(ValueError):\n        response = self.client.get(reverse(\"admin:admin_utils_article_change\", args=(quote(self.a1.pk),)) + \"?get_admin_log=not_a_number&var", "def test_admin_log_node_with_user(self):\n    \"\"\"\n    Test the AdminLogNode with user filtering.\n    \"\"\"\n    from django.template import Context, Template\n\n    # Create additional user and log entry\n    user2 = User.objects.create_user(username=\"user2\", password=\"secret\")\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        user2.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something by user2\",\n    )\n\n    # Render template with user filter\n    template_str = \"{% load admin_utils_tags %}{% get_admin_log 10 as admin_log for_user user2 %}\"\n    template = Template(template_str)\n    context = Context({\"log_entries\": LogEntry.objects.all(), \"user2\": user2})\n    template.render(context)\n\n    # Check that only the log entry by user2 is in the context variable\n    self.assertEqual(len(context[\"admin_log\"]), 1)\n    self.assertEqual(context[\"admin_log\"][0].user, user2)\n"], "sample_856": ["def test_train_test_split_empty_testset():\n    X, = [[1]]  # 1 sample\n    with pytest.raises(\n            ValueError,\n            match='With n_samples=1, test_size=0.0 and train_size=None, '\n            'the resulting test set will be empty'):\n        train_test_split(X, test_size=0.0)\n\n    X = [[1], [1], [1]]  # 3 samples, ask for more than 2 thirds\n    with pytest.raises(\n            ValueError,\n            match='With n_samples=3, test_size=0.0 and train_size=None, '\n            'the resulting test set will be empty'):\n        train_test_split(X, test_size=0.0)\n", "def test_leave_one_p_group_out_empty_trainset():\n    X, y = [[1]], [0]  # 1 sample\n    groups = [1]\n\n    with pytest.raises(\n            ValueError,\n            match='The groups parameter contains fewer than 2 unique groups'):\n        next(LeaveOneGroupOut().split(X, y, groups))\n\n    with pytest.raises(\n            ValueError,\n            match='The groups parameter contains fewer than (or equal to) n_groups'):\n        next(LeavePGroupsOut(n_groups=2).split(X, y, groups))\n", "def test_shuffle_split_large_test_size():\n    cv = ShuffleSplit(test_size=1000)\n    X, y = np.arange(100).reshape(100, 1), np.ones(100)\n    with pytest.raises(\n            ValueError,\n            match='test_size=1000 should be either positive and smaller than the number of samples 100 or a float in the (0, 1) range'):\n        next(cv.split(X, y))\n", "def test_repeated_stratified_kfold_deterministic_split_different_random_state():\n    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n    y = [1, 1, 1, 0, 0]\n    random_state1 = 1944695409\n    random_state2 = 987654321\n    rskf1 = RepeatedStratifiedKFold(\n        n_splits=2,\n        n_repeats=2,\n        random_state=random_state1)\n    rskf2 = RepeatedStratifiedKFold(\n        n_splits=2,\n        n_repeats=2,\n        random_state=random_state2)\n\n    # split should produce different splits on each call\n    splits1 = rskf1.split(X, y)\n    splits2 = rskf2.split(X, y)\n    for (train1, test1), (train2, test2) in zip(splits1, splits2):\n        assert not np.array_equal(train1, train2)\n        assert not np.array_equal(test1, test2)\n", "def test_predefined_split_errors():\n    X = np.ones(10)\n    y = np.arange(10)\n    test_fold = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n\n    ps = PredefinedSplit(test_fold)\n\n    # Check that ValueError is raised if test_fold has less than 2 unique values\n    test_fold_single = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n    ps_single = PredefinedSplit(test_fold_single)\n    with pytest.raises(ValueError, match=\"The 'test_fold' parameter must have at least 2 unique values.\"):\n        next(ps_single.split(X, y))\n\n    # Check that ValueError is raised if test_fold has -1 and other unique values\n    test_fold_negative = np.array([0, 0, 1, 1, 2, 2, 3, 3, -1, -1])\n    ps_negative = PredefinedSplit(test_fold_negative)\n    with pytest.raises(ValueError, match=\"The 'test_fold' parameter contains -1 and other unique values.\"):\n        next(ps_negative.split(X, y))\n\n    # Check that ValueError is raised if test_fold has different lengths than X\n    test_fold_diff_length = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4])\n    ps_diff_length = PredefinedSplit(test_fold_diff_length)\n    with pytest.raises(ValueError, match=\"The 'test_fold' parameter should have the same number of samples as X.\"):\n        next(ps_diff_length.split(X, y))\n"], "sample_875": ["def test_classification_metric_labels_types(metric, classes):\n    \"\"\"Check that the metric works with different types of `labels`.\n\n    We can expect `labels` to be a tuple of bool, an integer, a float, a string.\n    No error should be raised for those types.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    n_samples = 10\n    y_true = rng.choice(classes, size=n_samples, replace=True)\n    if metric is brier_score_loss:\n        # brier score loss requires probabilities\n        y_pred = rng.uniform(size=n_samples)\n    else:\n        y_pred = y_true.copy()\n    result = metric(y_true, y_pred, labels=classes)\n    assert not np.any(np.isnan(result))\n", "def test_brier_score_loss_with_pos_label():\n    # Check brier_score_loss function with pos_label\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1.0, 0.95])\n    true_score = linalg.norm(y_true - y_pred) ** 2 / len(y_true)\n\n    assert_almost_equal(brier_score_loss(y_true, y_true, pos_label=1), 0.0)\n    assert_almost_equal(brier_score_loss(y_true, y_pred, pos_label=1), true_score)\n    assert_almost_equal(brier_score_loss(1.0 + y_true, y_pred, pos_label=1), true_score)\n    assert_almost_equal(brier_score_loss(2 * y_true - 1, y_pred, pos_label=1), true_score)\n\n    # ensure to raise an error for multiclass y_true\n    y_true = np.array([0, 1, 2, 0])\n    y_pred = np.array([0.8, 0.6, 0.4, 0.2])\n    error_message = (\n        \"Only binary classification is supported. The type of the target is multiclass\"\n    )\n\n    with pytest.raises(ValueError, match=error_message):\n        brier_score_loss(y_true, y_pred, pos_label=1)\n\n    # calculate correctly when there's only one class in y_true\n    assert_almost_equal(brier_score_loss([-1], [0.4], pos_label=0), 0.16)\n    assert_almost_equal(brier_score_loss([0], [0.4], pos_label=0), 0.16)\n    assert_almost_equal(", "def test_classification_metric_average_options(metric, average):\n    \"\"\"Check that the metric works with different values of `average`.\n\n    No error should be raised for any of the valid options.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    n_samples = 10\n    y_true = rng.choice([0, 1], size=n_samples, replace=True)\n    y_pred = y_true.copy()\n    result = metric(y_true, y_pred, average=average)\n    assert not np.any(np.isnan(result))\n", "def test_classification_metric_multi_label(metric):\n    \"\"\"Check that the metric works with multi-label classification.\"\"\"\n    rng = np.random.RandomState(42)\n    n_samples, n_labels = 10, 3\n    y_true = rng.randint(0, 2, size=(n_samples, n_labels))\n    y_pred = y_true.copy()\n    result = metric(y_true, y_pred, average=\"micro\")\n    assert not np.any(np.isnan(result))\n", "def test_classification_metric_classes_types(metric, classes):\n    \"\"\"Check that the metric works with different types of `classes`.\n\n    We can expect `classes` to be a tuple of bools, ints, floats, or strings.\n    No error should be raised for those types.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    n_samples = 10\n    y_true = rng.choice(classes, size=n_samples, replace=True)\n    if metric is log_loss:\n        # log loss requires probabilities\n        y_pred = rng.uniform(size=(n_samples, len(classes)))\n    else:\n        y_pred = y_true.copy()\n    result = metric(y_true, y_pred, classes=classes)\n    if isinstance(result, tuple):\n        assert all(not np.any(np.isnan(res)) for res in result)\n    else:\n        assert not np.any(np.isnan(result))\n"], "sample_980": ["def test_commutes_with():\n    a = Permutation([1, 4, 3, 0, 2, 5])\n    b = Permutation([0, 1, 2, 3, 4, 5])\n    assert a.commutes_with(b)\n    b = Permutation([2, 3, 5, 4, 1, 0])\n    assert not a.commutes_with(b)\n", "def test_commutes_with():\n    p = Permutation([1, 5, 2, 0, 3, 6, 4])\n    q = Permutation([[1, 2, 3, 5, 6], [0, 4]])\n    assert p.commutes_with(q) == False\n    r = Permutation([3, 2, 1, 0])\n    assert r.commutes_with(r) == True\n", "def test_inverse():\n    p = Permutation([0, 2, 1, 3])\n    assert ~p == Permutation([0, 2, 1, 3])\n    assert p * ~p == Permutation([0, 1, 2, 3])\n    assert ~(~p) == p\n    assert p ** -1 == ~p\n", "def test_pow_operator():\n    p = Permutation([0, 2, 1, 3])\n    assert p**2 == Permutation([0, 1, 2, 3])\n    assert p**3 == Permutation([0, 2, 1, 3])\n    assert p**0 == Permutation([0, 1, 2, 3])\n    assert p**-1 == Permutation([0, 1, 2, 3])\n    raises(NotImplementedError, lambda: p**p)\n", "def test_cycle_structure():\n    p = Permutation([[1, 2], [3, 5, 2]])\n    assert p.cycle_structure == {2: 1, 3: 1}\n    p = Permutation([[1, 2], [3, 5, 2], [7, 8]])\n    assert p.cycle_structure == {1: 1, 2: 1, 3: 1}\n    p = Permutation([[1, 2], [3, 5, 2], [7, 8], [9]])\n    assert p.cycle_structure == {1: 2, 2: 1, 3: 1}\n    p = Permutation([[1, 2], [3, 5, 2], [7, 8], [9, 10]])\n    assert p.cycle_structure == {2: 2, 3: 1}\n    p = Permutation([[1, 2], [3, 5, 2], [7, 8], [9, 10, 11]])\n    assert p.cycle_structure == {1: 1, 3: 2}\n    p = Permutation([[1, 2], [3, 5, 2], [7, 8], [9, 10, 11, 12]])\n    assert p.cycle_structure == {2: 1, 4: 1}\n    p = Permutation([[1, 2], [3, 5, 2], [7, 8], [9, 10, 11, 12, 13]])\n    assert p.cycle_structure == {2: 1, 5: 1}\n"], "sample_824": ["def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    X = [[0], [0]]\n    Y = [[1], [2]]\n    D = paired_cosine_distances(X, Y)\n    assert_array_almost_equal(D, [0.5, 1.0])\n", "def test_pairwise_distances_chunked_reduce_sparse():\n    X = np.arange(10).reshape(-1, 1)\n    good_reduce = lambda D, start: csr_matrix(D)\n    S_chunks = pairwise_distances_chunked(X, None, reduce_func=good_reduce,\n                                          working_memory=64)\n    next(S_chunks)\n", "def test_check_different_shapes_paired_distances():\n    # Ensure an error is raised if the shapes are different.\n    XA = np.resize(np.arange(45), (5, 9))\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, XB)\n", "def test_pairwise_distances_sparse_with_precomputed_metric():\n    # Test that a TypeError is raised if sparse matrices are used with\n    # precomputed metric\n    X_sparse = csr_matrix([[1, 2], [3, 4]])\n    Y_sparse = csr_matrix([[5, 6], [7, 8]])\n    assert_raises(TypeError, pairwise_distances, X_sparse, Y_sparse, metric=\"precomputed\")\n", "def test_pairwise_distances_with_precomputed_metric():\n    # Test that precomputed metric works correctly with pairwise_distances\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n    D = pairwise_distances(X, Y, metric='euclidean')\n    D_precomputed = pairwise_distances(D, Y=None, metric='precomputed')\n    assert_array_almost_equal(D, D_precomputed)\n"], "sample_438": ["def test_get_object_cache_after_update(self):\n    question = Question.objects.create(text=\"Who?\")\n    post = Post.objects.create(title=\"Answer\", parent=question)\n\n    question.text = \"Who is your favorite author?\"\n    question.save()\n\n    post = Post.objects.get(pk=post.pk)\n    with self.assertNumQueries(1):\n        self.assertEqual(post.parent.text, \"Who is your favorite author?\")\n", "def test_clear_cached_generic_foreign_key(self):\n    question = Question.objects.create(text=\"What is your favorite color?\")\n    answer = Answer.objects.create(text=\"Blue\", question=question)\n    old_entity = answer.question\n    question.delete()\n    answer.refresh_from_db()\n    new_entity = answer.question\n    self.assertIsNone(new_entity)\n    with self.assertRaises(Question.DoesNotExist):\n        old_entity.refresh_from_db()\n", "def test_generic_relation_get_attname(self):\n    self.assertEqual(Question.answer_set.field.get_attname(), \"answer\")\n", "def test_get_object_cache_updates_after_save(self):\n    question = Question.objects.create(text=\"Who?\")\n    post = Post.objects.create(title=\"Answer\", parent=question)\n\n    question.text = \"Who is your favorite teacher?\"\n    question.save()\n\n    post = Post.objects.get(pk=post.pk)\n    with self.assertNumQueries(1):\n        self.assertEqual(post.parent.text, \"Who is your favorite teacher?\")\n        self.assertEqual(post.parent.text, \"Who is your favorite teacher?\")\n", "def test_generic_relation_queryset_with_deleted_objects(self):\n    question = Question.objects.create(text=\"Who?\")\n    answer = Answer.objects.create(text=\"Answer\", question=question)\n\n    question_pk = question.pk\n    Question.objects.all().delete()\n\n    with self.assertNumQueries(1):\n        self.assertSequenceEqual(\n            list(Question.answer_set.field.get_queryset(question_pk)),\n            [answer],\n        )\n"], "sample_671": ["def test_mark_skip_item(testdir):\n    # Ensure pytest.mark.skip works with non-Python Item\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        class MyItem(pytest.Item):\n            nodeid = 'foo'\n                marker = pytest.mark.skip(reason=\"Skipping test\")\n                self.add_marker(marker)\n                assert False\n\n            return MyItem(\"foo\", parent)\n    \"\"\"\n    )\n    result = testdir.inline_run()\n    passed, skipped, failed = result.listoutcomes()\n    assert not failed\n    skipped_tests = [r for r in skipped if r.nodeid == \"foo\"]\n    assert skipped_tests\n", "def test_skipif_class_with_reason(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class TestClass(object):\n            pytestmark = pytest.mark.skipif(True, reason=\"class skip\")\n                assert 0\n                assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rs\")\n    result.stdout.fnmatch_lines([\"*SKIP*class skip*\", \"*2 skipped*\"])\n", "def test_marked_skipif_with_not_string(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(False, reason=\"Test skipif with boolean and reason\")\n            pass\n        \"\"\"\n    )\n    ev = MarkEvaluator(item, \"skipif\")\n    assert ev.istrue()\n    expl = ev.getexplanation()\n    assert expl == \"Test skipif with boolean and reason\"\n", "def test_skip_with_non_string_reason(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(123)\n            pass\n        \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception, pytest_runtest_setup, item) as excinfo:\n        assert \"Reason must be a string\" in str(excinfo.value)\n", "def test_skip_with_reason_and_allow_module_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.skip(\"skip_module_level\", allow_module_level=True, reason=\"module level skip\")\n\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*skip_module_level*\"])\n"], "sample_564": ["def test_scatter_size_keyword():\n    \"\"\"\n    Test size keyword usage with non-finite coordinate arrays.\n\n    GH#26236\n    \"\"\"\n\n    x = [np.nan, 1, 2,  1]\n    y = [0, np.inf, 2,  1]\n    z = [0, 1, -np.inf, 1]\n    sizes = [10, 20, 30, 40]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    path3d = ax.scatter(x, y, z, s=sizes)\n\n    # Assert sizes' equality\n    assert len(path3d.get_sizes()) == len(super(type(path3d), path3d).get_offsets())\n", "def test_text3d_modification_with_different_colors():\n    # Test that modifying the Text position after the fact works the same as\n    # setting it directly with different colors.\n    zdirs = (None, 'x', 'y', 'z', (1, 1, 0), (1, 1, 1))\n    xs = (2, 6, 4, 9, 7, 2)\n    ys = (6, 4, 8, 7, 2, 2)\n    zs = (4, 2, 5, 6, 1, 7)\n    colors = ['red', 'green', 'blue', 'yellow', 'cyan', 'magenta']\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.set_xlim3d(0, 10)\n    ax_test.set_ylim3d(0, 10)\n    ax_test.set_zlim3d(0, 10)\n    for zdir, x, y, z, color in zip(zdirs, xs, ys, zs, colors):\n        t = ax_test.text(0, 0, 0, f'({x}, {y}, {z}), dir={zdir}', color=color)\n        t.set_position_3d((x, y, z), zdir=zdir)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.set_xlim3d(0, 10)\n    ax_ref.set_ylim3d(0, 10)\n    ax_ref.set_zlim3d(0, 10)\n    for zdir, x, y, z, color in zip(zdirs, xs, ys, zs, colors):\n        ax_ref.text(x, y, z, f'({x}, {y}, {z}), dir={zdir}', zdir=zdir, color=color)\n", "def test_bar3d_color(fig_test, fig_ref):\n    \"\"\"Test color parameter usage with bar3d.\"\"\"\n    ax1 = fig_test.add_subplot(111, projection='3d')\n    x = np.arange(4)\n    y = np.arange(5)\n    x2d, y2d = np.meshgrid(x, y)\n    x2d, y2d = x2d.ravel(), y2d.ravel()\n    z = x2d + y2d + 1\n    color = [cm.coolwarm(i/20) for i in range(20)]\n    ax1.bar3d(x2d, y2d, x2d * 0, 1, 1, z, shade=True, color=color)\n\n    ax2 = fig_ref.add_subplot(111, projection='3d')\n    ax2.bar3d(x2d, y2d, x2d * 0, 1, 1, z, shade=True, color='blue')\n", "def test_mutating_input_arrays_x_y_and_z():\n    \"\"\"\n    Test to see if the `x`, `y` and `z` arrays do not get mutated\n    after a call to `Axes3D.plot`\n\n    This is a modification of test_mutating_input_arrays_y_and_z\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    x = [1, 2, 3]\n    y = [0.0, 0.0, 0.0]\n    z = [0.0, 0.0, 0.0]\n    ax.plot(x, y, z, 'o-')\n\n    # mutate x, y, z to get a nontrivial line\n    x[:] = [2, 3, 4]\n    y[:] = [1, 2, 3]\n    z[:] = [1, 2, 3]\n\n    # check if the data was not mutated\n    assert x == [2, 3, 4]\n    assert y == [1, 2, 3]\n    assert z == [1, 2, 3]\n", "def test_marker_draw_order_view_rotated_second(fig_test, fig_ref, azim):\n    \"\"\"\n    Test that the draw order changes with the direction for a second time.\n\n    If we rotate *azim* by 180 degrees and exchange the colors, the plot\n    plot should look the same again.\n    \"\"\"\n    x = [-1, 1]\n    y = [1, -1]\n    z = [0, 0]\n    color = ['b', 'y']\n    ax = fig_test.add_subplot(projection='3d')\n    # axis are not exactly invariant under 180 degree rotation -> deactivate\n    ax.set_axis_off()\n    ax.scatter(x, y, z, s=3500, c=color[::-1])  # color reversed\n    ax.view_init(elev=0, azim=azim - 180, roll=0)  # view rotated by 180 deg\n    ax = fig_ref.add_subplot(projection='3d')\n    ax.set_axis_off()\n    ax.scatter(x, y, z, s=3500, c=color)\n    ax.view_init(elev=0, azim=azim, roll=0)\n"], "sample_1078": ["def test_IndexedBase_strides():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', strides=(2, 3))\n    assert a.strides == (2, 3)\n    assert Indexed(a, i, j).strides == (2, 3)\n\n    b = IndexedBase('b')\n    assert b.strides is None\n    assert Indexed(b, i, j).strides is None\n\n    c = IndexedBase('c', strides='C')\n    assert c.strides == 'C'\n    assert Indexed(c, i, j).strides == 'C'\n\n    d = IndexedBase('d', strides='F')\n    assert d.strides == 'F'\n    assert Indexed(d, i, j).strides == 'F'\n", "def test_IndexedBase_offset():\n    i, j = symbols('i j', integer=True)\n    o = symbols('o', integer=True)\n    A = IndexedBase('A', offset=o)\n    assert A.offset == o\n    assert A[i, j].offset == o\n    assert Indexed('A', i, j).offset == o\n", "def test_Indexed_derivative_with_piecewise_function():\n    i = symbols('i', integer=True)\n    h = IndexedBase('h')\n    expr = Piecewise((sin(h[i]), i > 0), (cos(h[i]), True))\n    assert expr.diff(h[i]) == Piecewise((cos(h[i]), i > 0), (-sin(h[i]), True))\n", "def test_IndexedBase_strides():\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase('A', strides=(i, j, k))\n    assert A.strides == (i, j, k)\n", "def test_Indexed_derivative_with_constants():\n    A = IndexedBase(\"A\")\n    i = symbols(\"i\")\n    assert A[i].diff(A[i]) == 1\n    assert A[i].diff(A[i], A[i]) == 0\n    assert A[i].diff(A[i], A[i], A[i]) == 0\n    assert A[i].diff(i) == 0\n    assert A[i].diff(i, i) == 0\n"], "sample_1131": ["def test_log2_and_logaddexp():\n    from sympy import log2, logaddexp\n\n    expr1 = log2(x)\n    expr2 = logaddexp(x, y)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log2(x)'\n    assert prntr.doprint(expr2) == 'numpy.logaddexp(x, y)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(x)/numpy.log(2)'\n    assert prntr.doprint(expr2) == 'numpy.logaddexp(x, y)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log(x)/math.log(2)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python:\\n  # logaddexp\\nlogaddexp(x, y)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log(x)/mpmath.log(2)'\n    assert prntr.doprint(expr2) == 'mpmath.logaddexp(x, y)'\n", "def test_Pow_complex_base():\n    expr = (1 + I)**x\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '(1 + 1j)**x'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.exp(mpmath.ln((1 + 1j), True)*x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.exp((1 + 1j)*x*numpy.log(10)**(-1))'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.exp((1 + 1j)*x*numpy.log(10)**(-1))'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == '(1 + I)**x'\n", "def test_NumPyPrinter_print_MatMul():\n    from sympy import Matrix\n\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = Matrix([[9, 10], [11, 12]])\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(A*B) == 'numpy.dot(A, B)'\n    assert prntr.doprint(A*B*C) == 'numpy.dot(numpy.dot(A, B), C)'\n", "def test_LambertW():\n    from sympy import LambertW\n\n    expr = LambertW(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.lambertw(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # LambertW\\nLambertW(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # LambertW\\nLambertW(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) ==  'mpmath.lambertw(x)'\n", "def test_logaddexp_and_logaddexp2():\n    from sympy import logaddexp, logaddexp2\n\n    expr1 = logaddexp(x, y)\n    expr2 = logaddexp2(x, y)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.logaddexp(x, y)'\n    assert prntr.doprint(expr2) == 'numpy.logaddexp2(x, y)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.logaddexp(x, y)'\n    assert prntr.doprint(expr2) == 'numpy.logaddexp2(x, y)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log(numpy.exp(x) + numpy.exp(y))'\n    assert prntr.doprint(expr2) == 'math.log(2**x + 2**y)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.logaddexp(x, y)'\n    assert prntr.doprint(expr2) == 'mpmath.logaddexp2(x, y)'\n"], "sample_787": ["def test_balanced_accuracy_score_sample_weight():\n    y_true = ['a', 'b', 'a', 'b']\n    y_pred = ['a', 'a', 'a', 'b']\n    sample_weight = [0.25, 0.25, 0.25, 0.25]\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    assert balanced == pytest.approx(0.5)\n", "def test_balanced_accuracy_score_single_class():\n    # Test balanced_accuracy_score with a single class\n    y_true = [0, 0, 0]\n    y_pred = [0, 0, 0]\n    assert_equal(balanced_accuracy_score(y_true, y_pred), 1.0)\n", "def test_balanced_accuracy_score_inverse_labels():\n    y_true = ['a', 'b', 'a', 'b']\n    y_pred = ['b', 'a', 'b', 'a']\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == pytest.approx(0.5)\n", "def test_balanced_accuracy_score_unseen_predicted():\n    assert_warns_message(UserWarning, 'y_true contains classes not in y_pred',\n                         balanced_accuracy_score, [0, 0, 0, 1], [0, 0, 0])\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = [0, 1, 0, 1, 0]\n    y_pred = [0, 1, 0, 1, 1]\n    sample_weight = [1, 1, 2, 1, 1]\n    balanced_weighted = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    assert balanced_weighted == pytest.approx(0.8333333333333334)\n"], "sample_1108": ["def test_common_prefix_suffix_empty():\n    assert common_prefix([], []) == []\n    assert common_suffix([], []) == []\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3), (1, -2, 3), (1, 2, -3), (1, -2, -3), (-1, 2, 3), (-1, -2, 3), (-1, 2, -3), (-1, -2, -3)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n    assert list(permute_signs((0, 0, 1))) == [(0, 0, 1), (0, 0, -1)]\n", "def test_ibin_all():\n    assert list(ibin(0, 'all')) == [(0,)]\n    assert list(ibin(0, 'all', str=True)) == ['0']\n    assert list(ibin(1, 'all')) == [(0,), (1,)]\n    assert list(ibin(1, 'all', str=True)) == ['0', '1']\n    assert list(ibin(2, 'all')) == [(0, 0), (0, 1), (1, 0), (1, 1)]\n    assert list(ibin(2, 'all', str=True)) == ['00', '01', '10', '11']\n    assert list(ibin(3, 'all')) == [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]\n    assert list(ibin(3, 'all', str=True)) == ['000', '001', '010', '011', '100', '101', '110', '111']\n", "def test_has_variety():\n    assert has_variety((1, 2, 1))\n    assert not has_variety((1, 1, 1))\n", "def test_multiset():\n    assert multiset([1, 2, 2, 3, 3, 3]) == {1: 1, 2: 2, 3: 3}\n    assert multiset('aabbc') == {'a': 2, 'b': 2, 'c': 1}\n    assert multiset('aabbc', ordered=True) == [(2, 'a'), (2, 'b'), (1, 'c')]\n    assert multiset('aabbc', as_dict=False) == [('a', 2), ('b', 2), ('c', 1)]\n    assert multiset('aabbc', as_dict=True) == {'a': 2, 'b': 2, 'c': 1}\n    assert multiset('aabbc', ordered=True, as_dict=False) == [(2, 'a'), (2, 'b'), (1, 'c')]\n    assert multiset('aabbc', ordered=True, as_dict=True) == {2: ['a', 'b'], 1: ['c']}\n"], "sample_820": ["def test_sample_weight_regressor():\n    \"\"\"Tests sample_weight parameter of VotingRegressor\"\"\"\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = DummyRegressor(strategy='median')\n    ereg1 = VotingRegressor([('mean', reg1), ('median', reg2)], weights=[1, 2]).fit(X_r, y_r, sample_weight=np.ones((len(y_r),)))\n    ereg2 = VotingRegressor([('mean', reg1), ('median', reg2)], weights=[1, 2]).fit(X_r, y_r)\n    assert_array_equal(ereg1.predict(X_r), ereg2.predict(X_r))\n\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y_r),))\n    ereg3 = VotingRegressor([('mean', reg1)], weights=[1])\n    ereg3.fit(X_r, y_r, sample_weight)\n    reg1.fit(X_r, y_r, sample_weight)\n    assert_array_equal(ereg3.predict(X_r), reg1.predict(X_r))\n\n    # check that an error is raised and indicative if sample_weight is not\n    # supported.\n    reg3 = KNeighborsRegressor()\n    ereg3 = VotingRegressor([('mean', reg1), ('knn', reg3)], weights=[1, 2])\n    msg = ('Underlying estimator KNeighborsRegressor does not support '\n           'sample weights.')\n    with pytest.raises(ValueError, match=msg):\n        ereg3.fit(X_r, y_", "def test_sample_weight_regressor():\n    \"\"\"Tests sample_weight parameter of VotingRegressor\"\"\"\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = RandomForestRegressor(random_state=123)\n    ereg1 = VotingRegressor([('dr', reg1), ('rf', reg2)], weights=[1, 2]).fit(X_r, y_r, sample_weight=np.ones((len(y_r),)))\n    ereg2 = VotingRegressor([('dr', reg1), ('rf', reg2)], weights=[1, 2]).fit(X_r, y_r)\n    assert_array_equal(ereg1.predict(X_r), ereg2.predict(X_r))\n\n    sample_weight = np.random.RandomState(123).uniform(size=(len(y_r),))\n    ereg3 = VotingRegressor([('dr', reg1)], weights=[1])\n    ereg3.fit(X_r, y_r, sample_weight)\n    reg1.fit(X_r, y_r, sample_weight)\n    assert_array_equal(ereg3.predict(X_r), reg1.predict(X_r))\n", "def test_unfitted_transform():\n    \"\"\"Check transform method of unfitted VotingClassifier raises error.\"\"\"\n    eclf = VotingClassifier(estimators=[('lr', LogisticRegression()),\n                                        ('rf', RandomForestClassifier())],\n                            voting='soft')\n    msg = (\"This VotingClassifier instance is not fitted yet. Call 'fit'\"\n           \" with appropriate arguments before using this method.\")\n    assert_raise_message(NotFittedError, msg, eclf.transform, X)\n", "def test_transform_hard_voting():\n    \"\"\"Check transform method of VotingClassifier with hard voting on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    assert_array_equal(eclf.transform(X).shape, (4, 3))\n    assert_array_equal(eclf.transform(X),\n                       np.array([[1, 1, 1],\n                                 [1, 1, 1],\n                                 [2, 1, 2],\n                                 [2, 2, 2]]))\n", "def test_regressor_weights_format():\n    # Test regressor weights inputs as list and array\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = DummyRegressor(strategy='median')\n    ereg1 = VotingRegressor([('mean', reg1), ('median', reg2)], weights=[1, 2])\n    ereg2 = VotingRegressor([('mean', reg1), ('median', reg2)], weights=np.array((1, 2)))\n    ereg1.fit(X_r, y_r)\n    ereg2.fit(X_r, y_r)\n    assert_array_almost_equal(ereg1.predict(X_r), ereg2.predict(X_r))\n"], "sample_221": ["def test_annotation_with_expression(self):\n    qs = Happening.objects.annotate(next_day=models.F('when') + datetime.timedelta(days=1))\n    self.assert_pickles(qs)\n", "def test_raw_queryset(self):\n    Group.objects.create(id=1, name='Group 1')\n    Event.objects.create(id=1, title='Event 1', group_id=1)\n    raw_qs = Group.objects.raw('SELECT * FROM myapp_group')\n    self.assert_pickles(raw_qs)\n    raw_qs = Group.objects.raw('SELECT * FROM myapp_group WHERE id = %s', [1])\n    self.assert_pickles(raw_qs)\n    raw_qs = Group.objects.raw('SELECT * FROM myapp_group WHERE id IN %s', [[1]])\n    self.assert_pickles(raw_qs)\n", "def test_pickle_union_queryset_not_evaluated(self):\n    g1 = Group.objects.create(name='group1')\n    g2 = Group.objects.create(name='group2')\n    Event.objects.create(title='event1', group=g1)\n    Event.objects.create(title='event2', group=g2)\n    groups = Group.objects.filter(name='group1').union(Group.objects.filter(name='group2'))\n    list(groups)  # evaluate QuerySet.\n    with self.assertNumQueries(0):\n        self.assert_pickles(groups)\n", "def test_filter_with_subquery(self):\n    group = Group.objects.create(name='test group')\n    event1 = Event.objects.create(title='event 1', group=group)\n    event2 = Event.objects.create(title='event 2', group=group)\n    subquery = Event.objects.filter(title='event 1').values('id')\n    events = Event.objects.filter(id__in=subquery)\n\n    with self.assertNumQueries(0):\n        dumped = pickle.dumps(events.query)\n\n    with self.assertNumQueries(0):\n        reloaded = pickle.loads(dumped)\n        reloaded_events = Event.objects.none()\n        reloaded_events.query = reloaded\n\n    self.assertSequenceEqual(reloaded_events, [event1])\n", "def test_exclude_deferred(self):\n    qs = Happening.objects.all()\n    qs._defer_next_filter = True\n    qs = qs.exclude(id=0)\n    self.assert_pickles(qs)\n"], "sample_999": ["def test_Differential_printing():\n    from sympy.diffgeom.rn import R2\n    assert latex(R2.dx) == r\"\\mathrm{d}x\"\n    assert latex(R2.dy) == r\"\\mathrm{d}y\"\n", "def test_Quaternion_addition_subtraction():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    assert q1 + q2 == Quaternion(6, 8, 10, 12)\n    assert q1 - q2 == Quaternion(-4, -4, -4, -4)\n", "def test_Casoratian_printing():\n    from sympy.diffgeom import Casoratian\n    x, y, z = symbols('x y z')\n    f = Function('f')\n    g = Function('g')\n    h = Function('h')\n    casoratian = Casoratian(f(x, y, z), g(x, y, z), h(x, y, z))\n    assert latex(casoratian) == r\"\\left| \\begin{array}{ccc} f{\\left (x, y, z \\right )} & g{\\left (x, y, z \\right )} & h{\\left (x, y, z \\right )} \\\\ \\frac{\\partial}{\\partial x} f{\\left (x, y, z \\right )} & \\frac{\\partial}{\\partial x} g{\\left (x, y, z \\right )} & \\frac{\\partial}{\\partial x} h{\\left (x, y, z \\right )} \\\\ \\frac{\\partial}{\\partial y} f{\\left (x, y, z \\right )} & \\frac{\\partial}{\\partial y} g{\\left (x, y, z \\right )} & \\frac{\\partial}{\\partial y} h{\\left (x, y, z \\right )} \\end{array} \\right|\"\n", "def test_Transpose_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(Transpose(A)) == r\"A^{T}\"\n", "def test_LieDerivative_printing():\n    from sympy.diffgeom import Manifold, Patch, CoordSystem\n    from sympy.diffgeom.tensorfields import TensorField\n    from sympy.diffgeom.manifolds import SphereS2\n    from sympy.diffgeom import LieDerivative\n    m = Manifold('M', 2)\n    patch = Patch('P', m)\n    chart = CoordSystem('chart', patch, ['x', 'y'])\n    vector_field = TensorField(m, (1, 0), name='V')\n    tensor_field = TensorField(m, (2, 0), name='T')\n    lie_derivative = LieDerivative(vector_field, tensor_field)\n    assert latex(lie_derivative) == r'\\mathcal{L}_{V}T'\n\n    sphere = SphereS2()\n    assert latex(LieDerivative(sphere.r_phi, sphere.metric)) == r'\\mathcal{L}_{\\frac{\\partial}{\\partial{\\phi}}}\\left(g_{\\alpha\\beta}\\right)'\n"], "sample_821": ["def test_affinity_propagation_single_sample():\n    # Test AffinityPropagation with a single sample\n    X = np.array([[1, 2]])\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n\n    cluster_centers_indices, labels = affinity_propagation(S, preference=preference)\n\n    assert_array_equal([0], cluster_centers_indices)\n    assert_array_equal([0], labels)\n", "def test_affinity_propagation_single_sample():\n    # Test Affinity Propagation with a single sample\n    X = np.array([[0, 0]])\n    S = -euclidean_distances(X, squared=True)\n\n    # Expect a single cluster with the single sample as the center\n    cluster_center_indices, labels = affinity_propagation(S)\n    assert_array_equal([0], cluster_center_indices)\n    assert_array_equal([0], labels)\n\n    # Test AffinityPropagation with a single sample\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X)\n    assert_array_equal([0], af.cluster_centers_indices_)\n    assert_array_equal([0], labels)\n", "def test_affinity_propagation_with_single_sample():\n    # Test AffinityPropagation with a single sample\n    X = np.array([[0, 0]])\n    S = -euclidean_distances(X, squared=True)\n\n    # AffinityPropagation should return 1 cluster with the single sample as the center\n    cluster_center_indices, labels = affinity_propagation(S)\n    assert_array_equal([0], cluster_center_indices)\n    assert_array_equal([0], labels)\n\n    # Test with AffinityPropagation object\n    af = AffinityPropagation()\n    labels = af.fit(X).labels_\n    assert_array_equal([0], af.cluster_centers_indices_)\n    assert_array_equal([0], labels)\n", "def test_affinity_propagation_preference_none():\n    # Test AffinityPropagation with preference set to None\n    af = AffinityPropagation(preference=None)\n    labels = af.fit_predict(X)\n    n_clusters_ = len(af.cluster_centers_indices_)\n    assert_equal(np.unique(labels).size, n_clusters_)\n", "def test_affinity_propagation_invalid_inputs():\n    # Test invalid inputs for affinity_propagation function\n    S = np.array([[1, 2], [3, 4]])\n    preference = np.array([1, 2])\n\n    # Test non-square similarity matrix\n    with pytest.raises(ValueError):\n        affinity_propagation(S, preference)\n\n    # Test damping factor less than 0.5\n    with pytest.raises(ValueError):\n        affinity_propagation(S, preference, damping=0.4)\n\n    # Test damping factor greater than or equal to 1\n    with pytest.raises(ValueError):\n        affinity_propagation(S, preference, damping=1)\n\n    # Test preference shape mismatch\n    S = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    with pytest.raises(ValueError):\n        affinity_propagation(S, preference)\n"], "sample_284": ["def test_file_hash_consistency(self):\n    # Create a test file\n    filename = 'test_file.txt'\n    content = 'This is a test file.'\n    with open(self._get_filename_path(filename), 'w') as f:\n        f.write(content)\n\n    # Calculate the hash of the file content\n    hash1 = storage.staticfiles_storage.file_hash(filename, content)\n\n    # Calculate the hash of the file content a second time\n    hash2 = storage.staticfiles_storage.file_hash(filename, content)\n\n    # The hashes should be the same\n    self.assertEqual(hash1, hash2)\n", "def test_post_processing_custom_template(self):\n    \"\"\"\n    post_processing uses the custom template when provided.\n    \"\"\"\n    original_patterns = storage.staticfiles_storage._patterns\n    storage.staticfiles_storage._patterns = {\n        \"*.css\": [(re.compile(r\"url\\(['\\\"]{0,1}\\s*(.*?)['\\\"]{0,1}\\)\"), \"custom_template(%s)\")]\n    }\n\n    collectstatic_args = {\n        'interactive': False,\n        'verbosity': 0,\n        'link': False,\n        'clear': False,\n        'dry_run': False,\n        'post_process': True,\n        'use_default_ignore_patterns': True,\n        'ignore_patterns': ['*.ignoreme'],\n    }\n\n    collectstatic_cmd = CollectstaticCommand()\n    collectstatic_cmd.set_options(**collectstatic_args)\n    stats = collectstatic_cmd.collect()\n\n    with storage.staticfiles_storage.open(os.path.join('cached', 'styles.css')) as relfile:\n        self.assertIn(b\"custom_template(other.d41d8cd98f00.css)\", relfile.read())\n\n    storage.staticfiles_storage._patterns = original_patterns\n    self.assertPostCondition()\n", "def test_post_processing_with_symlinks(self):\n    \"\"\"\n    post_processing behaves correctly when a symlink exists in the source directory.\n    \"\"\"\n    # Create a symlink in the source directory\n    src_file = os.path.join(TEST_ROOT, 'project', 'test', 'file.txt')\n    symlink = os.path.join(TEST_ROOT, 'project', 'test', 'symlink.txt')\n    os.symlink(src_file, symlink)\n\n    collectstatic_args = {\n        'interactive': False,\n        'verbosity': 0,\n        'link': False,\n        'clear': False,\n        'dry_run': False,\n        'post_process': True,\n        'use_default_ignore_patterns': True,\n        'ignore_patterns': ['*.ignoreme'],\n    }\n\n    collectstatic_cmd = CollectstaticCommand()\n    collectstatic_cmd.set_options(**collectstatic_args)\n    stats = collectstatic_cmd.collect()\n    self.assertIn('test/symlink.txt', stats['post_processed'])\n    self.assertPostCondition()\n\n    # Remove the symlink after the test\n    os.remove(symlink)\n", "def test_template_tag_absolute_from_subdirectory(self):\n    relpath = self.hashed_file_path(\"cached/subdir/absolute.css\")\n    self.assertEqual(relpath, \"cached/subdir/absolute.3d6e8e4b4782.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertNotIn(b\"/static/cached/styles.css\", content)\n        self.assertIn(b\"/static/cached/styles.5e0040571e1a.css\", content)\n        self.assertNotIn(b\"/static/styles_root.css\", content)\n        self.assertIn(b\"/static/styles_root.401f2509a628.css\", content)\n        self.assertIn(b'/static/cached/img/relative.acae32e4532b.png', content)\n    self.assertPostCondition()\n", "    def test_post_process_max_passes(self):\n        \"\"\"\n        Test that post_process respects the max_post_process_passes setting.\n        \"\"\"\n        storage.staticfiles_storage.max_post_process_passes = 1\n        # Change a referenced file to trigger a post-processing loop.\n        referenced_file = 'cached/other.css'\n        referenced_file_path = os.path.join(settings.STATIC_ROOT, referenced_file)\n        with open(referenced_file_path, 'a') as f:\n            f.write('/* Add a comment to change the file hash */')\n\n        err = StringIO()\n        with self.assertRaises(RuntimeError):\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n        self.assertEqual(\"Post-processing 'All' failed!\\n\\n\", err.getvalue())\n\n        # Clean up the changes made to the referenced file.\n        with open(referenced_file_path, 'r') as f:\n            lines = f.readlines()\n        with open(referenced_file_path, 'w') as f:\n            f.writelines([line for line in lines if not line.startswith('/*')])\n\n        # Reset the max_post_process_passes setting.\n        storage.staticfiles_storage.max_post_process_passes = self._max_post_process_passes\n"], "sample_349": ["    def test_build_attrs_with_custom_attrs(self):\n        custom_attrs = {'data-ajax--delay': 500, 'class': 'custom-class'}\n        form = AlbumForm()\n        attrs = form['band'].field.widget.build_attrs(base_attrs=custom_attrs)\n        self.assertEqual(attrs['data-ajax--delay'], 500)\n        self.assertEqual(attrs['class'], 'custom-class admin-autocomplete')\n", "def test_i18n_support(self):\n    rel = Album._meta.get_field('band').remote_field\n    languages_and_codes = [\n        ('en', 'en'),\n        ('fr', 'fr'),\n        ('es', 'es'),\n        # Add more language-code pairs as needed\n    ]\n    for lang, code in languages_and_codes:\n        with self.subTest(lang=lang):\n            with translation.override(lang):\n                attrs = AutocompleteSelect(rel, admin.site).build_attrs({})\n                self.assertEqual(attrs.get('lang'), code)\n", "def test_render_options_with_initial_value(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    # With initial value for 'band'\n    form = AlbumForm(initial={'band': beatles.uuid})\n    output = form.as_p()\n    selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.uuid\n    option = '<option value=\"%s\">The Who</option>' % who.uuid\n    self.assertIn(selected_option, output)\n    self.assertNotIn(option, output)\n", "def test_media_debug(self):\n    \"\"\"\n    Test that the correct files are included in the media when DEBUG=True.\n    \"\"\"\n    rel = Album._meta.get_field('band').remote_field\n    base_files = (\n        'admin/js/vendor/jquery/jquery.js',\n        'admin/js/vendor/select2/select2.full.js',\n        # Language file is inserted here.\n        'admin/js/jquery.init.js',\n        'admin/js/autocomplete.js',\n    )\n    languages = (\n        ('de', 'de'),\n        ('00', None),\n        ('sr-cyrl', 'sr-Cyrl'),\n        ('zh-hans', 'zh-CN'),\n        ('zh-hant', 'zh-TW'),\n    )\n    for lang, select_lang in languages:\n        with self.subTest(lang=lang):\n            if select_lang:\n                expected_files = (\n                    base_files[:2] +\n                    (('admin/js/vendor/select2/i18n/%s.js' % select_lang),) +\n                    base_files[2:]\n                )\n            else:\n                expected_files = base_files\n            with self.settings(DEBUG=True):\n                with translation.override(lang):\n                    self.assertEqual(AutocompleteSelect(rel, admin.site).media._js, list(expected_files))\n", "    def test_i18n_name_not_available(self):\n        with patch('django.utils.translation.get_language', return_value='invalid_lang'):\n            rel = Album._meta.get_field('band').remote_field\n            attrs = AutocompleteSelect(rel, admin.site).build_attrs({})\n            self.assertNotIn('lang', attrs)\n"], "sample_960": ["def test_pyclass_with_union_type_operator(app):\n    text = (\".. py:class:: Class\\n\"\n            \"   :type: int | str\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0],\n                ([desc_annotation, \"class \"],\n                 [desc_name, \"Class\"],\n                 [desc_annotation, (\": \",\n                                    [pending_xref, \"int\"],\n                                    \" \",\n                                    [desc_sig_punctuation, \"|\"],\n                                    \" \",\n                                    [pending_xref, \"str\"])]))\n", "def test_pyfunction_with_default_arguments(app):\n    text = \".. py:function:: hello(name='World', greeting='Hello') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'World'\"])],\n                                      [desc_parameter, ([desc_sig_name, \"greeting\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'Hello'\"])])])\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\"><span class=\"pre\">Name</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert '<p><strong>name</strong> (<em>Name</em>) \u2013 blah blah</p>' in content\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n", "def test_pyexception_signature_with_base(app):\n    text = \".. py:exception:: builtins.IOError(Exception)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"builtins.\"],\n                                                    [desc_name, \"IOError\"],\n                                                    [desc_sig_punctuation, \"(\"],\n                                                    [desc_sig_name, \"Exception\"],\n                                                    [desc_sig_punctuation, \")\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pyclass_with_type_annotation(app):\n    text = (\".. py:class:: Class\\n\"\n            \"   :type: TypeVar('T')\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, ([desc_annotation, (\": \",\n                                                                     [pending_xref, \"TypeVar\"],\n                                                                     [desc_sig_punctuation, \"(\"],\n                                                                     [nodes.inline, \"'T'\"],\n                                                                     [desc_sig_punctuation, \")\"])])])]))\n    assert 'Class' in domain.objects\n    assert domain.objects['Class'] == ('index', 'Class', 'class', False)\n"], "sample_4": ["def test_write_html_table_bad_format(self, cosmo, tmp_path):\n    \"\"\"Test if the format argument is incorrect\"\"\"\n    fp = tmp_path / \"test_write_html_table_bad_format.html\"\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not csv\"):\n        write_html_table(cosmo, fp, format=\"csv\")\n", "def test_readwrite_html_table_bad_format(self, cosmo, write, tmp_path):\n    \"\"\"Test if the format argument is incorrect.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_table_bad_format.html\"\n\n    # Test with an unsupported format\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not csv\"):\n        write(fp, format=\"csv\")\n", "def test_readwrite_html_latex_names(self, cosmo, read, write, tmp_path, add_cu):\n    \"\"\"Test if latex_names argument works correctly.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_latex_names.html\"\n\n    # Write with latex_names=True\n    write(fp, format=\"ascii.html\", latex_names=True)\n\n    # Read with latex_names=True\n    got = read(fp, format=\"ascii.html\", latex_names=True)\n    assert got == cosmo\n\n    # Read with latex_names=False\n    got = read(fp, format=\"ascii.html\", latex_names=False)\n    assert got == cosmo\n\n    # Write with latex_names=False\n    write(fp, format=\"ascii.html\", latex_names=False)\n\n    # Read with latex_names=True should fail\n    with pytest.raises(KeyError):\n        read(fp, format=\"ascii.html\", latex_names=True)\n\n    # Read with latex_names=False\n    got = read(fp, format=\"ascii.html\", latex_names=False)\n    assert got == cosmo\n", "def test_readwrite_html_table_with_units(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n    \"\"\"Test writing and reading a cosmology with units.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_table_with_units.html\"\n\n    # Write cosmology to html table with units\n    write(fp, format=\"ascii.html\")\n\n    # Read cosmology from html table with units\n    got = read(fp, format=\"ascii.html\")\n\n    # Assert that the read cosmology is equal to the original one\n    assert got == cosmo\n", "def test_readwrite_html_table_with_units(self, cosmo, read, write, tmp_path, add_cu):\n    \"\"\"Test if table with units can be read and written correctly.\"\"\"\n    fp = tmp_path / \"test_readwrite_html_table_with_units.html\"\n\n    # Write cosmology to HTML table with units\n    write(fp, format=\"ascii.html\", latex_names=False)\n\n    # Read cosmology from HTML table with units\n    got = read(fp, format=\"ascii.html\")\n\n    # Check that the read cosmology is equal to the original cosmology\n    assert got == cosmo\n"], "sample_464": ["def test_streaming_content(self):\n    content = [\"streaming \", \"content\"]\n    response = FileResponse(io.BytesIO(b\"\".join(content)))\n    self.assertEqual(list(response.streaming_content), list(map(response.make_bytes, content)))\n", "def test_content_length_filelike_with_getbuffer(self):\n    class BufferWithGetbuffer:\n            self.buffer = io.BytesIO(content)\n\n            return self.buffer.read(n)\n\n            return self.buffer.getbuffer()\n\n            return self.buffer.tell()\n\n    buffer = BufferWithGetbuffer(b\"binary content\")\n    response = FileResponse(buffer)\n    self.assertEqual(response.headers[\"Content-Length\"], \"14\")\n", "def test_file_from_closed_file(self):\n    with open(__file__, \"rb\") as file:\n        file.close()\n        response = FileResponse(file)\n        self.assertRaises(ValueError, list, response)\n", "def test_content_type_buffer_compressed(self):\n    test_tuples = (\n        (\".tar.gz\", \"application/gzip\"),\n        (\".tar.br\", \"application/x-brotli\"),\n        (\".tar.bz2\", \"application/x-bzip\"),\n        (\".tar.xz\", \"application/x-xz\"),\n        (\".tar.Z\", \"application/x-compress\"),\n    )\n    for extension, mimetype in test_tuples:\n        with self.subTest(ext=extension):\n            buffer = io.BytesIO(b\"binary content\")\n            buffer.name = f\"test{extension}\"\n            response = FileResponse(buffer)\n            self.assertEqual(response.headers[\"Content-Type\"], mimetype)\n", "def test_file_response_close(self):\n    \"\"\"\n    The FileResponse closes the file handle after streaming is done.\n    \"\"\"\n    with tempfile.NamedTemporaryFile() as temp_file:\n        temp_file.write(b\"binary content\")\n        temp_file.seek(0)\n        response = FileResponse(temp_file)\n        content = b\"\".join(response)\n        response.close()\n        self.assertTrue(temp_file.closed)\n        self.assertEqual(content, b\"binary content\")\n"], "sample_1055": ["def test_decipher_elgamal():\n    prk = elgamal_private_key(5)\n    ek = elgamal_public_key(prk)\n    P = ek[0]\n    assert P - 1 == decipher_elgamal(encipher_elgamal(P - 1, ek), prk)\n", "def test_encipher_decipher_rot13():\n    assert encipher_rot13(\"ABC\") == \"NOP\"\n    assert decipher_rot13(\"NOP\") == \"ABC\"\n    assert encipher_rot13(\"NOP\") == \"ABC\"\n    assert decipher_rot13(\"ABC\") == \"NOP\"\n", "def test_encipher_decipher_gm_seed():\n    p, q = 131, 89\n    messages = [0, 32855, 34303]\n    seeds = [1, 2, 3]\n    for msg in messages:\n        pri = gm_private_key(p, q)\n        pub = gm_public_key(p, q)\n        for seed in seeds:\n            enc = encipher_gm(msg, pub, seed=seed)\n            dec = decipher_gm(enc, pri)\n            assert dec == msg\n", "def test_decipher_bg_errors():\n    pri = bg_private_key(23, 31)\n    pub = bg_public_key(23, 31)\n    enc = encipher_bg(123, pub)\n    raises(ValueError, lambda: decipher_bg((enc[0], -1), pri))\n    raises(ValueError, lambda: decipher_bg((enc[0], \"a\"), pri))\n", "def test_decipher_bifid():\n    assert decipher_bifid(\"AB\", \"\", \"AB\") == \"AB\"\n    assert decipher_bifid(\"CO\", \"\", \"CD\") == \"AB\"\n    assert decipher_bifid(\"ch\", \"\", \"c\") == \"AB\"\n    assert decipher_bifid(\"b ac\", \"\", \"b\") == \"ABC\"\n    assert decipher_bifid(\"AB\", \"AB\", \"\") == \"AB\"\n    assert decipher_bifid(\"AB\", \"CD\", \"\") == \"CO\"\n    assert decipher_bifid(\"ab\", \"c\", \"\") == \"ch\"\n    assert decipher_bifid(\"a bc\", \"b\", \"\") == \"b ac\"\n"], "sample_1070": ["def test_log_product_expand():\n    from sympy.abc import n\n    x, y = symbols('x,y', positive=True)\n    from sympy.concrete import Product\n\n    assert simplify(log(Product(x**n, (n, 1, n)))) == n*log(x)\n    assert simplify(log(Product(x**n*y**n, (n, 1, n)))) == n*(log(x) + log(y))\n\n    expr = log(Product(-2, (n, 0, 4)))\n    assert expand_log(expr) == 4*log(2) + 4*log(-1)\n", "def test_exp_is_finite():\n    x = Symbol('x', finite=True)\n    assert exp(x).is_finite is True\n\n    x = Symbol('x', infinite=True)\n    assert exp(x).is_finite is False\n    assert exp(-x).is_finite is True\n\n    x = Symbol('x', extended_real=True, negative=True)\n    assert exp(x).is_finite is True\n\n    x = Symbol('x', extended_real=True, positive=True)\n    assert exp(x).is_finite is True\n\n    x = Symbol('x', extended_real=True, zero=True)\n    assert exp(x).is_finite is True\n\n    x = Symbol('x', extended_real=False)\n    assert exp(x).is_finite is None\n", "def test_exp_pow():\n    x = Symbol('x')\n    assert exp(2*x)._eval_power(2) == exp(4*x)\n    assert exp(x)._eval_power(Rational(1, 2)) == exp(x/2)\n", "def test_exp_symbolic():\n    x, y = symbols('x y')\n    assert exp(x + y).expand() != exp(x)*exp(y)\n    assert exp(x + y).expand(deep=True) == exp(x)*exp(y)\n\n    assert exp(x*y).expand() != exp(x)*exp(y)\n    assert exp(x*y).expand(deep=True) == exp(x*y)\n\n    assert exp(2*x).expand() == exp(2*x)\n    assert exp(2*x).expand(deep=True) == exp(2*x)\n\n    assert exp(-x).expand() == exp(-x)\n    assert exp(-x).expand(deep=True) == exp(-x)\n\n    assert exp(x + log(y)).expand() != exp(x)*y\n    assert exp(x + log(y)).expand(deep=True) == exp(x)*y\n", "def test_exp_polar_evalf():\n    x, y = symbols('x y', real=True)\n\n    assert exp_polar(I*x).evalf() == cos(x) + I*sin(x)\n    assert exp_polar(y + I*x).evalf() == exp(y)*(cos(x) + I*sin(x))\n\n    # Test for special cases where evalf might be flaky\n    assert exp_polar(I*oo).evalf() == nan\n    assert exp_polar(I*(-oo)).evalf() == nan\n    assert exp_polar(I*2*pi).evalf() == exp_polar(I*2*pi)\n"], "sample_1127": ["def test_symmetricpermutationgroup_contains():\n    G = SymmetricPermutationGroup(4)\n    assert Permutation(1, 2, 3) in G\n    assert Permutation(1, 2, 3, 4, 5) not in G\n\n    with pytest.raises(TypeError):\n        assert 5 in G\n", "def test_polycyclic_group():\n    a = Permutation([0, 1, 2])\n    b = Permutation([2, 1, 0])\n    G = PermutationGroup([a, b])\n    PG = G.polycyclic_group()\n    assert PG.pc_series == [PermutationGroup([a, b]), PermutationGroup([a])]\n    assert PG.pc_sequence == [b]\n    assert PG.relative_order == [2]\n", "def test_coset_enumeration():\n    a = Permutation(0, 1, 2, 3)\n    b = Permutation(0, 3)(1, 2)\n    G = PermutationGroup([a, b])\n    H = PermutationGroup([a])\n    cosets = G.coset_enumeration(H)\n    assert cosets == {0: {0, 1}, 1: {2, 3}, 2: {4, 5}, 3: {6, 7}, 4: {8, 9}, 5: {10, 11}}\n\n    S = SymmetricGroup(3)\n    T = S.subgroup([Permutation(0, 1)])\n    cosets = S.coset_enumeration(T)\n    assert cosets == {0: {0, 1, 2}, 1: {3, 4, 5}}\n", "def test_normal_subgroups():\n    G = SymmetricGroup(4)\n    normal_subgroups = G.normal_subgroups()\n    expected_subgroups = [G, PermutationGroup([Permutation(4)]), PermutationGroup([Permutation(3, 2), Permutation(2, 1)])]\n    for subgroup in expected_subgroups:\n        assert subgroup in normal_subgroups\n", "def test_coset_class_exceptions():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    g = Permutation(0, 1, 2, 3, 4)  # permutation of size 5, not compatible with G\n    with pytest.raises(ValueError):\n        Coset(g, G, dir='+')\n    H = PermutationGroup([Permutation(0, 1), Permutation(2, 3)])  # not a subgroup of G\n    with pytest.raises(ValueError):\n        Coset(a, H, G, dir='+')\n    dir = 'invalid_dir'  # not a valid direction\n    with pytest.raises(ValueError):\n        Coset(a, G, dir=dir)\n"], "sample_518": ["def test_default_fill():\n    patch = Patch()\n    assert patch.get_fill() is True\n\n    patch.set_fill(False)\n    assert patch.get_fill() is False\n\n    patch.set_fill(True)\n    assert patch.get_fill() is True\n\n    patch.set_fill(None)\n    assert patch.get_fill() is True\n", "def test_annulus_edge():\n    fig, ax = plt.subplots()\n    annulus = Annulus((0.5, 0.5), 0.2, 0.05, fc='none', ec='k')\n    ax.add_patch(annulus)\n    ax.set_aspect('equal')\n", "def test_wedge_range_extended():\n    ax = plt.axes()\n\n    args = [(0, 360), (-90, 90), (180, 540), (0, 720)]\n\n    for i, (theta1, theta2) in enumerate(args):\n        x = i % 3\n        y = i // 3\n\n        wedge = mpatches.Wedge((x * 3, y * 3), 1, theta1, theta2,\n                               facecolor='none', edgecolor='k', lw=3)\n\n        ax.add_artist(wedge)\n\n    ax.set_xlim([-2, 8])\n    ax.set_ylim([-2, 9])\n", "def test_wedge_negative_theta():\n    # Test that a wedge with negative theta values works correctly\n    wedge = mpatches.Wedge((0, 0), 1, -45, 45)\n    assert wedge.theta1 == -45\n    assert wedge.theta2 == 45\n\n    # Test that the wedge is drawn correctly\n    fig, ax = plt.subplots()\n    ax.add_patch(wedge)\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    fig.canvas.draw()\n", "def test_arc3_connection_style():\n    fig, ax = plt.subplots()\n    ax.set_xlim([-0.5, 1.5])\n    ax.set_ylim([-0.5, 1.5])\n    patch = FancyArrowPatch((0, 0), (1, 1), connectionstyle=\"arc3\", arrowstyle=\"->\")\n    ax.add_patch(patch)\n"], "sample_654": ["def test_parametrize_root(argroot):\n    assert argroot == \"c\"\n", "def test_arg_shadowing(argroot, arg):\n    assert isinstance(arg, str)\n    assert argroot == arg\n", "def test_fixture_in_class(arg):\n    assert isinstance(arg, str)\n", "def test_finalizer_order_on_parametrization_with_fixture(self, testdir, scope, monkeypatch):\n    monkeypatch.setenv(\"PYTEST_FIXTURE_SCOPE\", scope)\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import os\n        values = []\n\n        @pytest.fixture(scope=os.environ.get('PYTEST_FIXTURE_SCOPE', 'function'), params=[\"1\"])\n            return request.param\n\n        @pytest.fixture(scope=os.environ.get('PYTEST_FIXTURE_SCOPE', 'function'))\n                assert not values, \"base should not have been finalized\"\n            request.addfinalizer(cleanup_fix2)\n\n        @pytest.fixture(scope=os.environ.get('PYTEST_FIXTURE_SCOPE', 'function'))\n                values.append(\"fin_base\")\n                print(\"finalizing base\")\n            request.addfinalizer(cleanup_base)\n\n            pass\n            pass\n            pass\n    \"\"\"\n    )\n    reprec = testdir.inline_run(\"-lvs\")\n    reprec.assertoutcome(passed=3)\n", "def test_fixture_redefinition(testdir):\n    \"\"\"Fixture redefinition should raise a ValueError (#5584)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 1\n\n        @pytest.fixture\n            return 2\n\n            assert fix == 2\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*ValueError: duplicate fixtures in same scope*\"])\n"], "sample_574": ["def test_tick_locator_input_check(self, t):\n\n    err = \"Tick locator must be an instance of .*?, not <class 'tuple'>.\"\n    with pytest.raises(TypeError, match=err):\n        Temporal().tick((1, 2))\n", "def test_label_formatter_input_check(self, x):\n\n    err = \"Label formatter must be an instance of .*?, not <class 'str'>.\"\n    with pytest.raises(TypeError, match=err):\n        Temporal().label(\"{x}\")\n", "def test_color_tuple_values_with_temporal(self, t, x):\n\n    cmap = color_palette((\"b\", \"g\"), as_cmap=True)\n    s = Temporal((\"b\", \"g\"))._setup(t, Color())\n    normed = (x - x.min()) / (x.max() - x.min())\n    assert_array_equal(s(t), cmap(normed)[:, :3])  # FIXME RGBA\n", "def test_label_formatter_input_check(self, x):\n    err = \"Label formatter must be an instance of .*?, not <class 'str'>.\"\n    with pytest.raises(TypeError, match=err):\n        Temporal().label(\"{x}\")\n", "def test_tick_locator_input_check_with_invalid_type(self, t):\n    err = \"Tick locator must be an instance of .*?, not <class 'str'>.\"\n    with pytest.raises(TypeError, match=err):\n        Temporal().tick(\"invalid_locator\")\n"], "sample_648": ["def test_node_keywords_len(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture(scope=\"session\", autouse=True)\n            request.keywords[\"hello\"] = 42\n            assert \"world\" not in request.keywords\n\n        @pytest.fixture(scope=\"function\", autouse=True)\n            assert \"world\" in request.keywords\n            assert \"hello\" in request.keywords\n            assert len(request.keywords) == 2\n\n        @pytest.mark.world\n            pass\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_parameterset_extract_from_force_tuple() -> None:\n    param_set = ParameterSet.extract_from((1, 2), force_tuple=True)\n    assert param_set.values == (1, 2)\n    assert param_set.marks == []\n    assert param_set.id is None\n", "def test_mark_mro_method() -> None:\n    xfail = pytest.mark.xfail\n\n    class TestMRO:\n        @xfail(\"a\")\n            pass\n\n        @xfail(\"b\")\n            pass\n\n    class SubTestMRO(TestMRO):\n        @xfail(\"c\")\n            pass\n\n    from _pytest.mark.structures import get_unpacked_marks\n\n    all_marks = get_unpacked_marks(SubTestMRO)\n\n    assert all_marks == [xfail(\"a\").mark, xfail(\"b\").mark]\n\n    all_marks = get_unpacked_marks(SubTestMRO.test_c)\n\n    assert all_marks == [xfail(\"c\").mark, xfail(\"a\").mark, xfail(\"b\").mark]\n\n    assert get_unpacked_marks(SubTestMRO.test_c, consider_mro=False) == [xfail(\"c\").mark]\n", "def test_mark_inheritance_with_mro():\n    class MarkA:\n        pytestmark = pytest.mark.a\n\n    class MarkB:\n        pytestmark = pytest.mark.b\n\n    class MarkC(MarkA, MarkB):\n        pytestmark = pytest.mark.c\n\n    class TestInheritance(MarkC):\n            assert \"a\" in self.request.keywords\n            assert \"b\" in self.request.keywords\n            assert \"c\" in self.request.keywords\n", "def test_mark_on_pseudo_function_without_args(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.r\n            pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n"], "sample_596": ["def test_concat_data_vars_different(self):\n    data = Dataset({\"foo\": (\"x\", np.random.randn(10)), \"bar\": (\"x\", np.random.randn(10))})\n    data_diff = data.copy()\n    data_diff[\"foo\"] = (\"x\", np.random.randn(10))  # different values\n    objs = [data, data_diff]\n    actual = concat(objs, dim=\"x\", data_vars=\"different\")\n    assert \"foo\" in actual.data_vars\n    assert_identical(data[\"bar\"], actual[\"bar\"])\n", "def test_concat_with_data_vars_as_list():\n    # Test concat with data_vars as a list\n    data = Dataset({\"foo\": (\"x\", [1, 2, 3]), \"bar\": (\"x\", [4, 5, 6])}, {\"x\": [0, 1, 2]})\n    split_data = [data.isel(x=slice(2)), data.isel(x=slice(2, None))]\n    actual = concat(split_data, \"x\", data_vars=[\"foo\"])\n    expected = data.drop_vars(\"bar\")\n    assert_identical(expected, actual)\n", "def test_concat_with_index_array():\n    da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": pd.Index([\"a\", \"b\", \"c\"])})\n    da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": pd.Index([\"d\", \"e\", \"f\"])})\n\n    expected = DataArray(\n        [1, 2, 3, 4, 5, 6],\n        dims=\"x\",\n        coords={\"x\": pd.Index([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])},\n    )\n\n    actual = concat([da1, da2], \"x\")\n    assert_identical(actual, expected)\n", "def test_concat_with_index_dimension():\n    # Test concatenating datasets where the dimension to concatenate along is also a coordinate\n    ds1 = Dataset({'A': ('x', [1, 2, 3])}, {'x': [10, 20, 30]})\n    ds2 = Dataset({'A': ('x', [4, 5, 6])}, {'x': [40, 50, 60]})\n    expected = Dataset({'A': ('x', [1, 2, 3, 4, 5, 6]), 'x': ('x', [10, 20, 30, 40, 50, 60])})\n    actual = concat([ds1, ds2], dim='x')\n    assert_identical(actual, expected)\n", "def test_concat_dim_order():\n    # Test that the dimension order is preserved\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.zeros((3, 4)))}, {\"x\": np.arange(3), \"y\": np.arange(4)})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), np.ones((3, 4)))}, {\"x\": np.arange(3), \"y\": np.arange(4)})\n    expected = Dataset({\"a\": ((\"z\", \"x\", \"y\"), np.concatenate([ds1[\"a\"].values, ds2[\"a\"].values], axis=0))},\n                       {\"z\": [0, 1], \"x\": np.arange(3), \"y\": np.arange(4)})\n    actual = concat([ds1, ds2], dim=\"z\")\n    assert_identical(actual, expected)\n"], "sample_891": ["def test_precision_recall_curve_with_probablity_estimates(global_random_seed):\n    \"\"\"Check that thresholds do not exceed 1.0 when `y_score` is a probability\n    estimate.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/26193\n    \"\"\"\n    rng = np.random.RandomState(global_random_seed)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)\n    _, _, thresholds = precision_recall_curve(y_true, y_score)\n    assert np.isinf(thresholds[0])\n", "def test_roc_curve_without_probability_estimates(global_random_seed):\n    \"\"\"Check that thresholds do not exceed 1.0 when `y_score` is not a probability\n    estimate.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/26193\n    \"\"\"\n    rng = np.random.RandomState(global_random_seed)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.randint(0, 100, size=10)\n    _, _, thresholds = roc_curve(y_true, y_score)\n    assert np.isinf(thresholds[0])\n", "def test_roc_curve_single_class():\n    y_true = [0] * 10\n    y_score = np.random.rand(10)\n    with pytest.raises(ValueError, match=\"y_true contains only one label\"):\n        roc_curve(y_true, y_score)\n", "def test_roc_auc_score_with_sample_weight(y_true, y_score, sample_weight, expected_auc):\n    auc = roc_auc_score(y_true, y_score, sample_weight=sample_weight)\n    assert_almost_equal(auc, expected_auc)\n", "def test_roc_curve_with_zero_positive_labels(global_random_seed):\n    \"\"\"Check that the ROC curve is computed correctly when all positive labels are zero.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/26193\n    \"\"\"\n    rng = np.random.RandomState(global_random_seed)\n    y_true = np.zeros(10)\n    y_score = rng.rand(10)\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    assert np.allclose(fpr, [0, 1])\n    assert np.allclose(tpr, [0, 0])\n"], "sample_229": ["def test_intersection_with_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=3, num__lte=7)\n    qs3 = Number.objects.filter(num__gte=4, num__lte=6)\n    self.assertNumbersEqual(qs1.intersection(qs2.union(qs3)), [4, 5], ordered=False)\n", "def test_unsupported_operations_after_filter(self):\n    qs = Number.objects.all()\n    msg = 'Calling QuerySet.%s() after filter() is not supported.'\n    operations = [\n        'annotate',\n        'defer',\n        'distinct',\n        'exclude',\n        'only',\n        'prefetch_related',\n        'select_related',\n    ]\n    for operation in operations:\n        with self.assertRaisesMessage(\n            NotSupportedError,\n            msg % operation,\n        ):\n            getattr(qs.filter(num=2), operation)()\n", "def test_order_by_with_values(self):\n    qs1 = Number.objects.filter(num__lte=1).values('num')\n    qs2 = Number.objects.filter(num__gte=2, num__lte=3).values('num')\n    self.assertNumbersEqual(qs1.union(qs2).order_by('-num'), [3, 2, 1, 0], ordered=True)\n", "def test_union_with_values_and_subquery(self):\n    qs1 = Number.objects.filter(num__gte=5).values('num')\n    qs2 = Number.objects.filter(num__lte=3).values('num')\n    subquery = Number.objects.filter(num__in=qs1).values('num')\n    result = qs2.union(subquery)\n    self.assertEqual(list(result), [{'num': 3}, {'num': 5}, {'num': 6}, {'num': 7}, {'num': 8}, {'num': 9}])\n", "def test_union_with_annotations(self):\n    qs1 = Number.objects.filter(num__lte=5).annotate(square=F('num') * F('num'))\n    qs2 = Number.objects.filter(num__gte=5).annotate(square=F('num') * F('num'))\n    union_qs = qs1.union(qs2)\n    self.assertEqual(union_qs.get(num=3).square, 9)\n    self.assertEqual(union_qs.get(num=7).square, 49)\n"], "sample_535": ["def test_cell_alignment():\n    fig = plt.figure()\n\n    ax1 = fig.add_subplot(3, 1, 1)\n    ax1.axis('off')\n    tb1 = ax1.table(\n        cellText=[['Left', 'Center', 'Right'],\n                  ['Wide Text', 'Wide Text', 'Wide Text']],\n        loc=\"center\")\n    tb1.auto_set_font_size(False)\n    tb1.set_fontsize(12)\n    tb1[0, 0].get_text().set_horizontalalignment('left')\n    tb1[0, 1].get_text().set_horizontalalignment('center')\n    tb1[0, 2].get_text().set_horizontalalignment('right')\n\n    ax2 = fig.add_subplot(3, 1, 2)\n    ax2.axis('off')\n    tb2 = ax2.table(\n        cellText=[['Top'],\n                  ['Bottom']],\n        loc=\"center\")\n    tb2.auto_set_font_size(False)\n    tb2.set_fontsize(12)\n    tb2[0, 0].get_text().set_verticalalignment('top')\n    tb2[1, 0].get_text().set_verticalalignment('bottom')\n\n    ax3 = fig.add_subplot(3, 1, 3)\n    ax3.axis('off')\n    tb3 = ax3.table(\n        cellText=[['Center']],\n        loc=\"center\")\n    tb3.auto_set_font_size(False)\n    tb3.set_fontsize(12)\n    tb3[0, 0].get_text().set_horizontalalignment('center')\n    tb3[0, 0].get_text().set_verticalalignment('center')\n", "def test_text_alignment():\n    alignments = ('left', 'center', 'right')\n    cellText = [['1'] * len(alignments)] * 2\n    colWidths = [0.1] * len(alignments)\n\n    _, axs = plt.subplots(nrows=len(alignments), figsize=(4, len(alignments)+1))\n    for ax, align in zip(axs, alignments):\n        ax.table(\n                colWidths=colWidths,\n                cellText=cellText,\n                loc='center',\n                cellLoc=align,\n                )\n        ax.axis('off')\n    plt.tight_layout()\n", "def test_auto_fontsize():\n    fig = plt.figure()\n\n    ax1 = fig.add_subplot(2, 1, 1)\n    ax1.axis('off')\n    tb1 = ax1.table(\n        cellText=[['Fit Text', 2],\n                  ['very long long text, Longer text than default', 1]],\n        rowLabels=[\"A\", \"B\"],\n        colLabels=[\"Col1\", \"Col2\"],\n        loc=\"center\")\n    tb1.auto_set_font_size(True)\n\n    ax2 = fig.add_subplot(2, 1, 2)\n    ax2.axis('off')\n    tb2 = ax2.table(\n        cellText=[['Fit Text', 2],\n                  ['very long long text, Longer text than default', 1]],\n        rowLabels=[\"A\", \"B\"],\n        colLabels=[\"Col1\", \"Col2\"],\n        loc=\"center\")\n    tb2.auto_set_font_size(False)\n    tb2.set_fontsize(12)\n", "def test_cell_alignment():\n    fig = plt.figure()\n\n    # left alignment\n    ax1 = fig.add_subplot(3, 1, 1)\n    ax1.axis('off')\n    tb1 = ax1.table(\n        cellText=[['left', 'left'],\n                  ['aligned', 'aligned']],\n        loc=\"center\",\n        cellLoc='left')\n\n    # center alignment\n    ax2 = fig.add_subplot(3, 1, 2)\n    ax2.axis('off')\n    tb2 = ax2.table(\n        cellText=[['center', 'center'],\n                  ['aligned', 'aligned']],\n        loc=\"center\",\n        cellLoc='center')\n\n    # right alignment\n    ax3 = fig.add_subplot(3, 1, 3)\n    ax3.axis('off')\n    tb3 = ax3.table(\n        cellText=[['right', 'right'],\n                  ['aligned', 'aligned']],\n        loc=\"center\",\n        cellLoc='right')\n", "def test_table_with_headers():\n    data = np.random.randn(3, 4)\n    columns = ('Column 1', 'Column 2', 'Column 3', 'Column 4')\n    rows = ('Row 1', 'Row 2', 'Row 3')\n\n    fig, ax = plt.subplots()\n    ax.axis('off')\n    ax.table(cellText=data, colLabels=columns, rowLabels=rows, loc='center')\n"], "sample_286": ["def test_refresh_from_db_with_fields(self):\n    a = Article.objects.create(pub_date=datetime.now(), headline='Original Headline')\n    new_headline = 'New Headline'\n    Article.objects.filter(pk=a.pk).update(headline=new_headline)\n    with self.assertNumQueries(1):\n        a.refresh_from_db(fields=['headline'])\n        self.assertEqual(a.headline, new_headline)\n", "def test_refresh_from_db_deleted_instance(self):\n    a = Article.objects.create(pub_date=datetime.now())\n    a.delete()\n    msg = \"Article matching query does not exist.\"\n    with self.assertRaisesMessage(ObjectDoesNotExist, msg):\n        a.refresh_from_db()\n", "def test_refresh_from_db_with_fields(self):\n    a = Article.objects.create(pub_date=datetime.now())\n    a2 = Article.objects.create(pub_date=datetime.now())\n    a.headline = 'New headline'\n    a.save()\n    a2.refresh_from_db(fields=['headline'])\n    self.assertNotEqual(a2.headline, a.headline)\n", "def test_refresh_with_deferred_fields(self):\n    a = Article.objects.create(pub_date=datetime.now(), headline='Test headline')\n    a_deferred = Article.objects.defer('headline').get(pk=a.pk)\n    a.headline = 'Updated headline'\n    a.save()\n    with self.assertNumQueries(1):\n        a_deferred.refresh_from_db()\n    self.assertEqual(a_deferred.headline, 'Test headline')\n    a_deferred.refresh_from_db(fields=['headline'])\n    self.assertEqual(a_deferred.headline, 'Updated headline')\n", "def test_get_absolute_url_override(self):\n    # Test if the get_absolute_url method is overridden correctly.\n        return '/custom/url/'\n\n    with mock.patch.dict(settings.ABSOLUTE_URL_OVERRIDES, {'basic.article': custom_get_absolute_url}):\n        a = Article.objects.create(headline='Test Article', pub_date=datetime.now())\n        self.assertEqual(a.get_absolute_url(), '/custom/url/')\n"], "sample_471": ["def test_integerfield_step_size_max_value(self):\n    f = IntegerField(step_size=3, max_value=10)\n    self.assertWidgetRendersTo(\n        f,\n        '<input name=\"f\" max=\"10\" step=\"3\" type=\"number\" id=\"id_f\" required>',\n    )\n    msg = (\n        \"Ensure this value is a multiple of step size 3, ending at 10, e.g. 3, 6, \"\n        \"9, and so on.\"\n    )\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"8\")\n    self.assertEqual(f.clean(\"3\"), 3)\n    self.assertEqual(f.clean(\"10\"), 10)\n    self.assertEqual(f.step_size, 3)\n", "def test_integerfield_step_size_max_value(self):\n    f = IntegerField(step_size=3, max_value=10)\n    self.assertWidgetRendersTo(\n        f,\n        '<input name=\"f\" max=\"10\" step=\"3\" type=\"number\" id=\"id_f\" required>',\n    )\n    msg = \"'Ensure this value is less than or equal to 10.' and 'Ensure this value is a multiple of step size 3.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"12\")\n    self.assertEqual(f.clean(\"8\"), 8)\n    self.assertEqual(f.clean(\"10\"), 10)\n    self.assertEqual(f.step_size, 3)\n    self.assertEqual(f.max_value, 10)\n", "def test_integerfield_step_size_with_min_value_zero(self):\n    f = IntegerField(step_size=3, min_value=0)\n    self.assertWidgetRendersTo(\n        f,\n        '<input name=\"f\" min=\"0\" step=\"3\" type=\"number\" id=\"id_f\" required>',\n    )\n    msg = (\n        \"Ensure this value is a multiple of step size 3, starting from 0, e.g. \"\n        \"0, 3, 6, and so on.\"\n    )\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"1\")\n    self.assertEqual(f.clean(\"0\"), 0)\n    self.assertEqual(f.clean(\"3\"), 3)\n    self.assertEqual(f.step_size, 3)\n", "def test_integerfield_decimal_input(self):\n    f = IntegerField()\n    with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n        f.clean(\"1.5\")\n", "def test_integerfield_custom_widget(self):\n    class CustomWidget(Textarea):\n        pass\n\n    f = IntegerField(widget=CustomWidget)\n    self.assertWidgetRendersTo(\n        f, '<textarea name=\"f\" id=\"id_f\" required></textarea>'\n    )\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(\"\")\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(None)\n    self.assertEqual(1, f.clean(\"1\"))\n    self.assertIsInstance(f.clean(\"1\"), int)\n    self.assertEqual(23, f.clean(\"23\"))\n    with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n        f.clean(\"a\")\n"], "sample_426": ["def test_years_edge(self):\n    t = datetime.datetime(2000, 1, 1)\n    tests = [\n        (datetime.datetime(2000, 12, 31), \"365\\xa0days\"),\n        (datetime.datetime(2001, 1, 1), \"1\\xa0year\"),\n        (datetime.datetime(2001, 12, 31), \"1\\xa0year, 365\\xa0days\"),\n        (datetime.datetime(2002, 1, 1), \"2\\xa0years\"),\n        (datetime.datetime(2002, 12, 31), \"2\\xa0years, 365\\xa0days\"),\n        (datetime.datetime(2003, 1, 1), \"3\\xa0years\"),\n        (datetime.datetime(2003, 12, 31), \"3\\xa0years, 365\\xa0days\"),\n        (datetime.datetime(2004, 1, 1), \"4\\xa0years\"),\n        (datetime.datetime(2004, 12, 31), \"4\\xa0years, 365\\xa0days\"),\n        # Leap year\n        (datetime.datetime(2004, 2, 29), \"4\\xa0years\"),\n        (datetime.datetime(2005, 2, 28), \"5\\xa0years\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(t, value), expected)\n", "def test_months_leap_year(self):\n    t = datetime.datetime(2020, 1, 1)\n    tests = [\n        (datetime.datetime(2020, 1, 31), \"4\\xa0weeks, 2\\xa0days\"),\n        (datetime.datetime(2020, 2, 29), \"1\\xa0month, 4\\xa0weeks, 3\\xa0days\"),\n        (datetime.datetime(2020, 3, 1), \"2\\xa0months\"),\n        (datetime.datetime(2020, 3, 31), \"2\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2020, 4, 1), \"3\\xa0months\"),\n        (datetime.datetime(2020, 4, 30), \"3\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2020, 5, 1), \"4\\xa0months\"),\n        (datetime.datetime(2020, 5, 31), \"4\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2020, 6, 1), \"5\\xa0months\"),\n        (datetime.datetime(2020, 6, 30), \"5\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2020, 7, 1), \"6\\xa0months\"),\n        (datetime.datetime(2020, 7, 31), \"6\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2020, 8, 1), \"7\\xa0months\"),\n        (datetime.datetime(2020, 8, 31), \"7\\xa0months, 4\\xa0weeks\"),\n        (datetime.datetime(2020, 9, 1), \"8\\xa0months\"),\n        (datetime.", "def test_future_datetime(self):\n    future_datetime = datetime.datetime(2050, 1, 1)\n    self.assertEqual(timesince(future_datetime), \"0\\xa0minutes\")\n    self.assertEqual(timeuntil(future_datetime), \"33\\xa0years\")\n", "def test_timesince_future_date_with_depth(self):\n    \"\"\"Timesince should work correctly with a future date and depth.\"\"\"\n    future_date = datetime.datetime.now() + 2 * self.oneweek + 3 * self.oneday + 4 * self.onehour\n    self.assertEqual(timesince(self.t, future_date, depth=2), \"2 weeks, 3 days\")\n", "def test_timesince_with_future_datetime(self):\n    \"\"\"timesince should return '0 minutes' when given a future datetime.\"\"\"\n    future_date = self.t + 2 * self.oneday\n    self.assertEqual(timesince(future_date, self.t), \"0\\xa0minutes\")\n"], "sample_801": ["def test_imputer():\n    # render a SimpleImputer\n    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n    expected = \"\"\"", "def test_set_params():\n    # Test the set_params method\n    lr = LogisticRegression(C=1.0, penalty='l2')\n    lr.set_params(C=10.0, penalty='l1')\n    assert lr.C == 10.0\n    assert lr.penalty == 'l1'\n\n    # Test set_params with a pipeline\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression(C=999))\n    pipeline.set_params(logisticregression__C=1000)\n    assert pipeline.steps[1][1].C == 1000\n\n    # Test set_params with a gridsearch\n    param_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                   'C': [1, 10, 100, 1000]},\n                  {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n    gs = GridSearchCV(SVC(), param_grid, cv=5)\n    gs.set_params(estimator__C=2000)\n    assert gs.estimator.C == 2000\n", "def test_long_array_parameter():\n    # make sure long array parameters are correctly truncated\n    lr = LogisticRegressionCV(Cs=np.arange(100))\n    expected = \"\"\"", "def test_simple_imputer():\n    # Test the SimpleImputer class\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", fill_value=None, verbose=0, copy=True)\n    expected = \"\"\"", "def test_simple_imputer():\n    # Render a SimpleImputer object\n    imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\", fill_value=None, verbose=0, copy=True)\n    expected = \"\"\""], "sample_283": ["def test_no_dbname_and_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'PASSWORD': 'somepassword',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\n                'USER': 'someuser',\n                'PASSWORD': 'somepassword',\n                'HOST': 'somehost',\n                'PORT': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "def test_no_dbname_or_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'PASSWORD': 'somepassword',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {'PGPASSWORD': 'somepassword'},\n        )\n    )\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {},\n        )\n    )\n", "def test_no_dbname_with_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({'OPTIONS': {'service': 'django_test'}}),\n        (['psql'], {'PGSERVICE': 'django_test'}),\n    )\n\n    self.assertEqual(\n        self.settings_to_cmd_args_env({'OPTIONS': {'service': 'django_test'}, 'NAME': ''}),\n        (['psql'], {'PGSERVICE': 'django_test'}),\n    )\n"], "sample_733": ["def test_countvectorizer_invalid_max_df():\n    # test for raising error message when max_df is less than 0\n    message = \"negative value for max_df\"\n    assert_raise_message(\n        ValueError, message, CountVectorizer, max_df=-1)\n", "def test_vectorizer_max_features_input_validation():\n    # Test that an error is raised when max_features is not a positive integer or None\n    with pytest.raises(ValueError):\n        CountVectorizer(max_features=0)\n    with pytest.raises(ValueError):\n        CountVectorizer(max_features=-1)\n    with pytest.raises(ValueError):\n        CountVectorizer(max_features=1.5)\n    with pytest.raises(ValueError):\n        CountVectorizer(max_features='a')\n", "def test_tfidfvectorizer_idf_on_unlearned_vector():\n    # Test that trying to transform with an unlearned IDF vector raises an error\n    t3 = TfidfTransformer(use_idf=True)\n    X = [[1, 1, 1],\n         [1, 1, 0]]\n    exception = ValueError\n    message = \"The tfidf vector is not fitted\"\n    assert_raise_message(exception, message, t3.transform, X)\n", "def test_tfidfvectorizer_invalid_params():\n    # Test for error message when max_df is less than 0\n    with pytest.raises(ValueError):\n        TfidfVectorizer(max_df=-0.5)\n\n    # Test for error message when min_df is less than 0\n    with pytest.raises(ValueError):\n        TfidfVectorizer(min_df=-0.5)\n\n    # Test for error message when max_features is not a positive integer\n    with pytest.raises(ValueError):\n        TfidfVectorizer(max_features=-1)\n    with pytest.raises(ValueError):\n        TfidfVectorizer(max_features=1.5)\n", "def test_countvectorizer_max_features_stop_words():\n    # Test that stop_words_ attribute is updated correctly when max_features is used\n    vect = CountVectorizer(stop_words=\"english\", max_features=2)\n    vect.fit(ALL_FOOD_DOCS)\n    assert_equal(len(vect.stop_words_), 7)\n    assert_equal(len(vect.vocabulary_), 2)\n"], "sample_716": ["def test_ridge_classifier_cv_sample_weight():\n    # Test sample weights for RidgeClassifierCV\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = [1, 1, 1, -1, -1]\n    sample_weight = [1, 1, 2, 1, 1]\n\n    reg = RidgeClassifierCV(alphas=[.01, .1, 1], sample_weight=sample_weight)\n    reg.fit(X, y)\n\n    assert_array_equal(reg.predict([[-.2, 2]]), np.array([-1]))\n", "def test_ridge_classifier_multilabel_support():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0, n_labels=2)\n    ridge_classifier = RidgeClassifier()\n    ridge_classifier.fit(X, y)\n    y_pred = ridge_classifier.predict(X)\n    assert y_pred.shape == y.shape\n", "def test_ridge_n_targets_not_match_alphas():\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([[1, 2], [1, 2], [1, 2], [-1, -2], [-1, -2]])\n    alphas = [1.0, 2.0]\n    ridge = Ridge(alpha=alphas)\n    assert_raises(ValueError, ridge.fit, X, y)\n", "def test_ridge_with_y_as_2d_array():\n    # Test Ridge regression with y as a 2D array\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n    n_samples, n_features, n_target = 6, 5, 2\n\n    y = rng.randn(n_samples, n_target)\n    X = rng.randn(n_samples, n_features)\n\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (n_target, n_features))\n    assert_greater(ridge.score(X, y), 0.47)\n", "def test_ridge_classifier_sample_weights():\n    rng = np.random.RandomState(0)\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3,\n                               random_state=rng)\n    sample_weight = rng.rand(100)\n\n    reg = RidgeClassifier(random_state=rng)\n    reg.fit(X, y, sample_weight=sample_weight)\n    assert_equal(reg.coef_.shape, (3, 20))\n\n    reg_no_weights = RidgeClassifier(random_state=rng)\n    reg_no_weights.fit(X, y)\n    assert_raises(AssertionError, np.testing.assert_array_equal,\n                  reg.coef_, reg_no_weights.coef_)\n"], "sample_833": ["def test_penalty_none_liblinear():\n    # Make sure penalty='none' raises an error with liblinear solver\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    msg = \"penalty='none' is not supported for the liblinear solver\"\n    lr = LogisticRegression(penalty='none', solver='liblinear')\n    assert_raise_message(ValueError, msg, lr.fit, X, y)\n", "def test_intercept_scaling_values(solver):\n    # Test intercept_scaling with different values\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    lr_default = LogisticRegression(solver=solver, random_state=0)\n    lr_scaled = LogisticRegression(solver=solver, random_state=0, intercept_scaling=2.0)\n\n    lr_default.fit(X, y)\n    lr_scaled.fit(X, y)\n\n    # Check that intercept_scaling affects the intercept, but not the coefficients\n    assert_array_almost_equal(lr_default.coef_, lr_scaled.coef_)\n    assert_array_almost_equal(lr_default.intercept_ / 2.0, lr_scaled.intercept_)\n", "def test_logistic_regression_path_intercept_scaling():\n    # Make sure that intercept_scaling is used in logistic_regression_path\n    X, y = make_classification(n_samples=1000, random_state=0)\n    Cs = [.00001, 1, 10000]\n    coefs1, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=Cs, solver='saga', random_state=0, intercept_scaling=1.0)\n    coefs2, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=Cs, solver='saga', random_state=0, intercept_scaling=2.0)\n\n    # Make sure coefs differ by more than a small margin\n    assert not np.allclose(coefs1, coefs2, rtol=0, atol=.1)\n", "def test_LogisticRegression_penalty_none(multi_class):\n    # Make sure error is raised when setting penalty='none' for\n    # multi_class='multinomial'\n    X, y = make_classification(n_samples=1000, n_classes=3, random_state=0)\n\n    msg = \"penalty='none' is not supported for the liblinear solver\"\n    lr = LogisticRegression(penalty='none', solver='liblinear',\n                            multi_class=multi_class)\n    if multi_class == 'multinomial':\n        assert_raise_message(ValueError, msg, lr.fit, X, y)\n    else:\n        lr.fit(X, y)\n\n    msg = \"penalty='none' is not supported for the newton-cg solver\"\n    lr = LogisticRegression(penalty='none', solver='newton-cg',\n                            multi_class=multi_class)\n    if multi_class == 'multinomial':\n        assert_raise_message(ValueError, msg, lr.fit, X, y)\n    else:\n        lr.fit(X, y)\n", "def test_logistic_regression_path_intercept_scaling():\n    # Make sure intercept_scaling parameter is passed to _logistic_regression_path\n    X, y = make_classification(n_samples=1000, random_state=0)\n    Cs = np.logspace(-4, 4, 3)\n\n    intercept_scaling = 2.0\n    coefs1, _, _ = _logistic_regression_path(X, y, Cs=Cs, fit_intercept=True,\n                                            solver='sag', intercept_scaling=intercept_scaling,\n                                            random_state=0)\n\n    lr = LogisticRegression(solver='sag', C=Cs[0], fit_intercept=True,\n                            intercept_scaling=intercept_scaling, random_state=0)\n    lr.fit(X, y)\n    coefs2 = lr.coef_\n\n    assert_array_almost_equal(coefs1[0], coefs2, decimal=6)\n"], "sample_986": ["def test_evalf_bernoulli():\n    from sympy.functions.combinatorial.numbers import bernoulli\n    assert bernoulli(0).evalf() == 1\n    assert bernoulli(1).evalf() == -1/2\n    assert bernoulli(2).evalf() == 1/6\n    raises(ValueError, lambda: bernoulli(x).evalf())\n", "def test_evalf_atan():\n    assert NS('atan(0)') == '0'\n    assert NS('atan(1)') == '0.785398163397448'\n    assert NS('atan(pi)') == '1.26262725567891'\n    assert NS('atan(-pi)') == '-1.26262725567891'\n    assert NS('atan(inf)') == '1.57079632679490'\n    assert NS('atan(-inf)') == '-1.57079632679490'\n", "def test_evalf_complex_precision():\n    e = exp(I*pi/3)\n    assert NS(e, 15) == \"0.500000000000000 + 0.866025403784439*I\"\n    assert NS(e, 2) == \"0.50 + 0.87*I\"\n", "def test_evalf_bernoulli():\n    from sympy import bernoulli\n    assert bernoulli(0).evalf() == 1\n    assert bernoulli(1).evalf() == -1/2\n    assert bernoulli(10).evalf(50) == -50521 / 63\n    raises(ValueError, lambda: bernoulli(1.5))\n", "def test_evalf_with_subs():\n    assert NS('sin(x)', subs={x: 1}) == '0.841470984807897'\n    assert NS('cos(x)', subs={x: 1}) == '0.540302305868140'\n    assert NS('exp(x)', subs={x: 1}) == '2.71828182845905'\n    assert NS('log(x)', subs={x: 1}) == '0.000000000000000'\n    assert NS('sqrt(x)', subs={x: 2}) == '1.41421356237310'\n    assert NS('x**2', subs={x: 2}) == '4.00000000000000'\n    assert NS('x**(1/2)', subs={x: 4}) == '2.00000000000000'\n    assert NS('atan(x)', subs={x: 1}) == '0.785398163397448'\n    assert NS('Abs(x)', subs={x: -3}) == '3.00000000000000'\n    assert NS('ceiling(x)', subs={x: 3.2}) == '4.00000000000000'\n    assert NS('floor(x)', subs={x: 3.7}) == '3.00000000000000'\n    assert NS('re(x)', subs={x: 3+2j}) == '3.00000000000000'\n    assert NS('im(x)', subs={x: 3+2j}) == '2.00000000000000'\n"], "sample_120": ["def test_serialize_models_model(self):\n    self.assertSerializedEqual(models.Model)\n", "def test_serialize_custom_type(self):\n    \"\"\"\n    Test serialization of a custom type.\n    \"\"\"\n    class CustomType:\n            self.value = value\n\n            return 'CustomType(%r)' % self.value\n\n    custom_instance = CustomType('test value')\n    self.assertSerializedResultEqual(\n        custom_instance,\n        (\"migrations.test_writer.CustomType('test value')\", {'import migrations.test_writer'})\n    )\n", "def test_serialize_custom_field(self):\n    class CustomField(models.CharField):\n            name, path, args, kwargs = super().deconstruct()\n            kwargs['custom_arg'] = 'custom_value'\n            return name, path, args, kwargs\n\n    field = CustomField(max_length=255, custom_arg='custom_value')\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"migrations.test_writer.CustomField(custom_arg='custom_value', max_length=255)\"\n    )\n    self.serialize_round_trip(field)\n", "def test_serialize_registered_custom_type(self):\n    class CustomType:\n            self.value = value\n\n            return ('CustomType', [self.value], {})\n\n    class CustomTypeSerializer(BaseSerializer):\n            value, imports = serializer_factory(self.value.value)\n            return f'CustomType({value})', imports\n\n    MigrationWriter.register(CustomType, CustomTypeSerializer)\n    self.assertSerializedEqual(CustomType('test_value'))\n    MigrationWriter.unregister(CustomType)\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize: CustomType(\\'test_value\\')'):\n        self.assertSerializedEqual(CustomType('test_value'))\n", "def test_serialize_default_with_callable(self):\n    \"\"\"\n    Test that a default value with a callable is serialized correctly.\n    \"\"\"\n        return \"default value\"\n\n    field = models.CharField(default=default_value)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.CharField(default=migrations.test_writer.default_value)\",\n    )\n\n    # Test with a partial function\n    partial_default = functools.partial(default_value)\n    field = models.CharField(default=partial_default)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.CharField(default=functools.partial(migrations.test_writer.default_value))\",\n    )\n"], "sample_594": ["def test_inline_variable_array_repr_sparse():\n    import sparse\n\n    data = sparse.COO(np.array([[0, 1, 2], [0, 0, 3]]), fill_value=0)\n    variable = xr.Variable(\"x\", data)\n\n    max_width = 30\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    expected = \"<COO: nnz=3, fill_value=0>\"\n    assert actual == expected\n", "def test_format_timedelta_out_of_bounds():\n    from datetime import timedelta\n\n    delta = timedelta(days=10000)\n    expected = \"1157 days 00:00:00\"\n    result = formatting.format_timedelta(delta)\n    assert result == expected\n\n    delta = timedelta(microseconds=-10000000000)\n    expected = \"-1157 days +00:00:00\"\n    result = formatting.format_timedelta(delta)\n    assert result == expected\n", "def test_array_repr_unindexed_dims():\n    da = xr.DataArray(np.random.randn(2, 3, 4), dims=(\"x\", \"y\", \"z\"), name=\"test\")\n    da = da.isel(z=0)\n    actual = formatting.array_repr(da)\n    expected = dedent(\n        \"\"\"\\\n        <xarray.DataArray 'test' (x: 2, y: 3, z: 1)>\n        array([[[ 0.123],\n                [-0.456],\n                [ 0.789]]])\n        Dimensions without coordinates: z\"\"\"\n    )\n    assert actual == expected\n", "def test_short_data_repr_large_dask_array():\n    import dask.array as da\n\n    large_dask_array = da.random.random((10000, 10000), chunks=(1000, 1000))\n    variable = xr.Variable(\"x\", large_dask_array)\n\n    result = formatting.short_data_repr(variable)\n\n    assert \"dask.array\" in result\n    assert \"chunksize\" in result\n    assert \"shape=(10000, 10000)\" in result\n\n    # Check that the repr is limited to a certain number of lines\n    line_count = result.count(\"\\n\") + 1\n    assert line_count <= 40\n", "def test_format_timedelta_format():\n    timedelta_format_cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"datetime\", \"10 days 01:00:00\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"time\", \"01:00:00\"),\n        (pd.Timedelta(\"10 days\"), \"datetime\", \"10 days 00:00:00\"),\n        (pd.Timedelta(\"10 days\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"10 days\"), \"time\", \"00:00:00\"),\n        (pd.Timedelta(\"1 hour\"), \"datetime\", \"0 days 01:00:00\"),\n        (pd.Timedelta(\"1 hour\"), \"date\", \"0 days\"),\n        (pd.Timedelta(\"1 hour\"), \"time\", \"01:00:00\"),\n        (pd.Timedelta(\"0 days 0 hours\"), \"datetime\", \"0 days 00:00:00\"),\n        (pd.Timedelta(\"0 days 0 hours\"), \"date\", \"0 days\"),\n        (pd.Timedelta(\"0 days 0 hours\"), \"time\", \"00:00:00\"),\n    ]\n    for item, timedelta_format, expected in timedelta_format_cases:\n        actual = formatting.format_timedelta(item, timedelta_format)\n        assert expected == actual\n"], "sample_1200": ["def test_physical_constants_are_dimensionless():\n    from sympy.physics.units import definitions\n\n    for name in dir(definitions):\n        quantity = getattr(definitions, name)\n        if not isinstance(quantity, PhysicalConstant):\n            continue\n        assert SI.get_dimension_system().is_dimensionless(quantity.dimension), f\"{name} is not dimensionless when it should be\"\n", "def test_physics_constant_property():\n    from sympy.physics.units.definitions import gravitational_constant, molar_gas_constant, vacuum_permittivity, speed_of_light, elementary_charge\n    from sympy.physics.units import meter, joule\n\n    assert gravitational_constant.is_physical_constant\n    assert molar_gas_constant.is_physical_constant\n    assert vacuum_permittivity.is_physical_constant\n    assert speed_of_light.is_physical_constant\n    assert elementary_charge.is_physical_constant\n\n    assert not meter.is_physical_constant\n    assert not joule.is_physical_constant\n", "def test_derived_units():\n    from sympy.physics.units.definitions.dimension_definitions import Dimension\n    from sympy.physics.units.quantities import Quantity\n\n    derived_unit = Quantity(\"derived_unit\")\n    derived_dimension = Dimension(length*time)\n\n    SI.extend([], derived_units={derived_dimension: derived_unit})\n\n    assert SI.derived_units[derived_dimension] == derived_unit\n", "def test_derived_units():\n    from sympy.physics.units.definitions.dimension_definitions import energy, length\n    from sympy.physics.units.systems import UnitSystem\n    from sympy.physics.units import joule, meter\n\n    # Test the derived_units property of UnitSystem\n    derived_units = SI.derived_units\n    assert energy in derived_units\n    assert derived_units[energy] == joule\n    assert length in derived_units\n    assert derived_units[length] == meter\n\n    # Test extending a UnitSystem with derived units\n    new_derived_units = {energy: 2*joule, length: 3*meter}\n    new_system = SI.extend(base=(), derived_units=new_derived_units)\n    assert new_system.derived_units[energy] == 2*joule\n    assert new_system.derived_units[length] == 3*meter\n", "def test_physics_constant_comparison():\n    from sympy.physics.units.definitions import elementary_charge, speed_of_light\n\n    assert elementary_charge != speed_of_light\n    assert elementary_charge != elementary_charge.scale_factor\n    assert elementary_charge != elementary_charge.dimension\n\n    # Comparison with a different PhysicalConstant\n    different_charge = PhysicalConstant('different_charge', 'DC', 1.602176634e-19 * coulomb, dimension=charge)\n    assert elementary_charge != different_charge\n\n    # Comparison with a different Quantity\n    different_quantity = Quantity('different_quantity')\n    different_quantity.set_global_relative_scale_factor(1, meter)\n    assert elementary_charge != different_quantity\n\n    # Comparison with a different dimension\n    different_dimension = Dimension((1, 0, -1, 0, 0, 0, 0))\n    assert elementary_charge != different_dimension\n\n    # Comparison with a number\n    assert elementary_charge != 1.602176634e-19\n"], "sample_13": ["def test_latitude_out_of_limits_degrees(value):\n    \"\"\"\n    Test that values slightly larger than 90 degrees are rejected.\n    Test cases for issue #13708\n    \"\"\"\n    with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n        Latitude(value, u.degree)\n", "def test_angle_to_string_latex_format():\n    \"\"\"\n    Test the to_string() method with the 'latex' format option.\n    \"\"\"\n    a = Angle(45.0, unit=u.degree)\n    assert a.to_string(format='latex') == '$45^\\circ$'\n\n    a = Angle(1.0, unit=u.hourangle)\n    assert a.to_string(format='latex') == '$1^{\\mathrm{h}}$'\n\n    a = Angle(0.5, unit=u.radian)\n    assert a.to_string(format='latex') == '$0.5^{\\\\mathrm{rad}}$'\n", "def test_dms_hms_conversion(value, unit, expected_dms, expected_hms):\n    angle = Angle(value, unit=unit)\n    assert angle.dms == expected_dms\n    assert angle.hms == expected_hms\n", "def test_angle_from_string():\n    \"\"\"\n    Tests creating an angle from a string with different separators\n    \"\"\"\n\n    # Test different separators\n    a = Angle('12d34m56.78s')\n    assert_allclose(a.degree, 12.5824444444)\n\n    a = Angle('12\u00b034\\'56.78\"')\n    assert_allclose(a.degree, 12.5824444444)\n\n    a = Angle('12:34:56.78')\n    assert_allclose(a.degree, 12.5824444444)\n\n    a = Angle('12 34 56.78')\n    assert_allclose(a.degree, 12.5824444444)\n\n    a = Angle('12h34m56.78s')\n    assert_allclose(a.hour, 12.5824444444)\n\n    a = Angle('12h 34m 56.78s')\n    assert_allclose(a.hour, 12.5824444444)\n\n    a = Angle('12:34:56.78h')\n    assert_allclose(a.hour, 12.5824444444)\n", "def test_angle_properties():\n    \"\"\"\n    Tests the properties of Angle, Longitude, and Latitude objects\n    \"\"\"\n\n    # Test hour property\n    a = Angle('3h45m15s')\n    assert a.hour == 3.7541666666666667\n\n    # Test hms property\n    assert a.hms == (3, 45, 15.0)\n\n    # Test dms property\n    a = Angle('54d30m15s')\n    assert a.dms == (54, 30, 15.0)\n\n    # Test signed_dms property\n    a = Angle('-54d30m15s')\n    assert a.signed_dms == (-1, 54, 30, 15.0)\n\n    # Test Longitude and Latitude objects\n    lon = Longitude('12h30m45s')\n    lat = Latitude('45d30m15s')\n    assert lon.wrap_angle == 360 * u.deg\n    assert lat._validate_angles is None  # Check if validation is done correctly\n\n    # Test Latitude validation\n    with pytest.raises(ValueError):\n        lat = Latitude('91d')\n"], "sample_921": ["def test_isabstractmethod(app):\n    from target.methods import Base, AbstractClass\n\n    assert inspect.isabstractmethod(AbstractClass.abstractmeth) is True\n    assert inspect.isabstractmethod(Base.meth) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n        pass\n\n    assert inspect.is_singledispatch_function(single_dispatch_func) is True\n    assert inspect.is_singledispatch_function(normal_func) is False\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @singledispatch_func.register(int)\n        pass\n\n        pass\n\n    assert inspect.is_singledispatch_function(singledispatch_func) is True\n    assert inspect.is_singledispatch_function(singledispatch_func.register(float)) is False\n    assert inspect.is_singledispatch_function(normal_func) is False\n", "def test_isabstractmethod(app):\n    from target.methods import Base, AbstractClass\n\n    assert inspect.isabstractmethod(AbstractClass.abstractmeth) is True  # abstract method\n    assert inspect.isabstractmethod(Base.meth) is False                  # method\n    assert inspect.isabstractmethod(Base.staticmeth) is False            # staticmethod\n    assert inspect.isabstractmethod(Base.classmeth) is False             # classmethod\n    assert inspect.isabstractmethod(Base.prop) is False                  # property\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @fun.register(int)\n        pass\n\n        pass\n\n    assert inspect.is_singledispatch_function(fun) is True\n    assert inspect.is_singledispatch_function(not_singledispatch) is False\n"], "sample_456": ["def test_formset_with_deletion_extra_forms(self):\n    \"\"\"FormSets with deletion and extra forms.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, extra=1)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-0-votes\" value=\"100\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-0-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-1-choice\" value=\"Fergie\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-1-votes\" value=\"900\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-1-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-2-choice\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-2-votes\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-2-DELETE\"></li>',\n    )\n    # Let's delete Fergie and fill in the extra form.\n    data = {\n        \"choices-TOTAL_FORMS\": \"3\",  # the number of forms rendered\n        \"choices-INITIAL_FORMS\": \"2\",  # the number of forms with initial data\n        \"choices-MIN_NUM_FORMS\": \"0\",  # min number of forms\n        \"choices-MAX_", "def test_formset_prefix(self):\n    \"\"\"Formsets have the correct prefix.\"\"\"\n    formset = self.make_choiceformset()\n    self.assertEqual(formset.prefix, \"choices\")\n    formset = self.make_choiceformset(prefix=\"different\")\n    self.assertEqual(formset.prefix, \"different\")\n", "def test_initial_form_count_with_max_num(self):\n    \"\"\"\n    initial_form_count() returns the number of forms that are required in this\n    FormSet, taking into account the max_num parameter.\n    \"\"\"\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n        {\"choice\": \"The Decemberists\", \"votes\": 500},\n    ]\n    ChoiceFormSet = formset_factory(Choice, max_num=2)\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertEqual(formset.initial_form_count(), 2)\n\n    ChoiceFormSet = formset_factory(Choice, max_num=4)\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertEqual(formset.initial_form_count(), 3)\n", "def test_formset_with_max_num_and_extra(self):\n    \"\"\"Formsets with max_num and extra parameters.\"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"4\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"2\",\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-1-choice\": \"Fergie\",\n        \"choices-1-votes\": \"900\",\n        \"choices-2-choice\": \"The Decemberists\",\n        \"choices-2-votes\": \"500\",\n        \"choices-3-choice\": \"Basia Bulat\",\n        \"choices-3-votes\": \"50\",\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=2, max_num=2)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(len(formset.forms), 4)\n    self.assertEqual(\n        [form.cleaned_data for form in formset.forms],\n        [\n            {\"votes\": 100, \"choice\": \"Calexico\"},\n            {\"votes\": 900, \"choice\": \"Fergie\"},\n            {\"votes\": 500, \"choice\": \"The Decemberists\"},\n            {\"votes\": 50, \"choice\": \"Basia Bulat\"},\n        ],\n    )\n", "def test_formset_with_initial_and_errors(self):\n    \"\"\"\n    Test that initial data and errors are handled correctly in a formset.\n    \"\"\"\n    initial_data = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n    ]\n    error_data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"2\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"1000\",\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-1-choice\": \"Fergie\",\n        \"choices-1-votes\": \"\",  # Invalid vote count\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(\n        data=error_data,\n        initial=initial_data,\n        auto_id=False,\n        prefix=\"choices\"\n    )\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.errors,\n        [{}, {\"votes\": [\"This field is required.\"]}]\n    )\n    self.assertEqual(\n        formset.cleaned_data,\n        [{\"votes\": 100, \"choice\": \"Calexico\"}, {\"choice\": \"Fergie\"}]\n    )\n"], "sample_585": ["def test_da_groupby_reduce_keep_attrs():\n\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], [('x', [1, 1, 1, 2, 2, 2])], attrs={'foo': 'bar'})\n\n    # Test keep_attrs=True\n    expected = xr.DataArray([3, 15], [('x', [1, 2])], attrs={'foo': 'bar'})\n    actual = array.groupby('x').sum(keep_attrs=True)\n    assert_identical(expected, actual)\n\n    # Test keep_attrs=False\n    expected = xr.DataArray([3, 15], [('x', [1, 2])])\n    actual = array.groupby('x').sum(keep_attrs=False)\n    assert_identical(expected, actual)\n", "def test_groupby_operations():\n    # test groupby operations like mean, sum, etc.\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], [('x', [1, 1, 1, 2, 2, 2])])\n\n    # mean\n    expected_mean = xr.DataArray([2, 5], [('x', [1, 2])])\n    actual_mean = array.groupby('x').mean()\n    assert_identical(expected_mean, actual_mean)\n\n    # sum\n    expected_sum = xr.DataArray([6, 15], [('x', [1, 2])])\n    actual_sum = array.groupby('x').sum()\n    assert_identical(expected_sum, actual_sum)\n", "def test_groupby_da_string():\n    # test groupby with a DataArray of dtype string for GH1132\n    # create test data\n    strings = ['a', 'b', 'c', 'd']\n    foo = xr.DataArray([1, 2, 3, 4], coords=dict(label=strings), dims='label')\n    # create test index\n    labels = ['a', 'b', 'b', 'c', 'c', 'c', 'd', 'd']\n    ind = xr.DataArray(labels, coords=dict(label=strings*2), dims='label',\n                       name='reference_label')\n    g = foo.groupby(ind)\n    actual = g.sum(dim='label')\n    expected = xr.DataArray([1, 5, 4],\n                            coords=dict(reference_label=['a', 'b', 'd']),\n                            dims='reference_label')\n    assert actual.equals(expected)\n", "def test_groupby_with_multi_dimensional_coords():\n    # Test groupby operation with multi-dimensional coordinates\n    # Create a DataArray with multi-dimensional coordinates\n    coords = {'x': ('a', 'b'), 'y': ('c', 'd')}\n    dims = ('a', 'b')\n    data = np.random.randn(2, 2)\n    da = xr.DataArray(data, coords=coords, dims=dims)\n\n    # Perform groupby operation\n    grouped = da.groupby('x')\n\n    # Check the result\n    assert grouped._restore_coord_dims is False  # Coordinates should not be restored\n    assert grouped._group_dim == 'x'  # Group dimension should be 'x'\n    assert len(grouped) == 2  # Number of unique groups should be 2\n", "def test_groupby_reduce_func_args():\n        return arg1 + arg2 + arg3\n\n    array = xr.DataArray([1, 1, 1], [('x', [1, 2, 3])])\n    expected = xr.DataArray([3], [('x', [1])])\n    actual = array.groupby('x').reduce(func, args=(1,), arg3=1)\n    assert_identical(expected, actual)\n"], "sample_147": ["def test_difference_with_subcompound_qs(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.union(Number.objects.filter(num__gt=5))\n    self.assertEqual(qs1.difference(qs2).count(), 5)\n", "def test_combining_multiple_models_with_exclude(self):\n    ReservedName.objects.create(name='99 little bugs', order=99)\n    qs1 = Number.objects.filter(num=1).exclude(num=2).values_list('num', flat=True)\n    qs2 = ReservedName.objects.exclude(order=0).values_list('order')\n    self.assertEqual(list(qs1.union(qs2).order_by('num')), [1, 99])\n", "def test_values_list_with_union_and_ordering(self):\n    qs1 = Number.objects.filter(num__gte=5).values_list('num', flat=True)\n    qs2 = Number.objects.filter(num__lte=3).values_list('num', flat=True)\n    union_qs = qs1.union(qs2)\n    self.assertEqual(list(union_qs.order_by('-num')), [5, 4, 3, 2, 1, 0])\n", "def test_ordering_with_subqueries(self):\n    qs1 = Number.objects.filter(num__lt=5).values('num')\n    qs2 = Number.objects.filter(num__gte=5).values('num')\n    union_qs = qs1.union(qs2)\n    # Test ordering with a subquery in the ordering clause\n    ordered_union_qs = union_qs.annotate(\n        doubled_num=F('num') * 2\n    ).order_by('doubled_num')\n    self.assertNumbersEqual(ordered_union_qs, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n", "def test_difference_with_subcompound_qs(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.union(Number.objects.filter(num__gt=1))\n    self.assertEqual(qs1.difference(qs2).count(), 2)\n"], "sample_160": ["def test_format_with_scientific_notation(self):\n    self.assertEqual(nformat(1e16, '.', grouping=3, thousand_sep=',', force_grouping=True), '10,000,000,000,000,000')\n    self.assertEqual(nformat(1e16, '.', decimal_pos=2, grouping=3, thousand_sep=',', force_grouping=True), '10,000,000,000,000,000.00')\n    self.assertEqual(nformat(1e-16, '.', decimal_pos=2), '0.00')\n    self.assertEqual(nformat(1e-16, '.', decimal_pos=16), '0.0000000000000001')\n", "def test_grouping_sequence(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2), thousand_sep=','), '12,34,567')\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,7')\n    self.assertEqual(nformat(1234567, '.', grouping=(0, 2), thousand_sep=','), '1,23,45,67')\n", "    def test_grouping_sequence(self):\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2), thousand_sep=','), '123,45,67,89')\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 3), thousand_sep=','), '12,34,56,789')\n", "    def test_exponent_formatting(self):\n        self.assertEqual(nformat(Decimal('9e9999'), '.'), '9e+9999')\n        self.assertEqual(nformat(Decimal('9e201'), '.'), '9e+201')\n        self.assertEqual(nformat(Decimal('9e200'), '.'), '9e+200')\n        self.assertEqual(nformat(Decimal('1.2345e999'), '.'), '1.2345e+999')\n        self.assertEqual(nformat(Decimal('9e-999'), '.'), '9e-999')\n        self.assertEqual(nformat(Decimal('1e-7'), '.', decimal_pos=8), '0.00000010')\n        self.assertEqual(nformat(Decimal('1e-8'), '.', decimal_pos=8), '0.00000001')\n        self.assertEqual(nformat(Decimal('1e-9'), '.', decimal_pos=8), '0.00000000')\n        self.assertEqual(nformat(Decimal('1e-10'), '.', decimal_pos=8), '0.00000000')\n        self.assertEqual(nformat(Decimal('1e-11'), '.', decimal_pos=8), '0.00000000')\n        self.assertEqual(nformat(Decimal('1' + ('0' * 300)), '.', decimal_pos=3), '1.000e+300')\n        self.assertEqual(nformat(Decimal('0.{}1234'.format('0' * 299)), '.', decimal_pos=3), '0.000')\n", "    def test_grouping_sequence(self):\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2), thousand_sep=','), '123,45,67,89')\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 0), thousand_sep=','), '123,456,789')\n        self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n"], "sample_1197": ["def test_derived_units():\n    # Testing the derived_units property of UnitSystem\n    derived_units = SI.derived_units\n    assert isinstance(derived_units, dict)\n    assert len(derived_units) > 0\n    for dimension, unit in derived_units.items():\n        assert isinstance(dimension, Dimension)\n        assert isinstance(unit, Quantity)\n", "def test_quantity_hash():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(10), meter)\n    w.set_global_relative_scale_factor(S(20), meter)\n\n    assert hash(u) == hash(v)\n    assert hash(u) != hash(w)\n", "def test_quantity_comparisons():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(20), meter)\n\n    assert u > v\n    assert u == w\n    assert u != v\n    assert v < u\n    assert not (u < v)\n    assert not (v > u)\n\n    with raises(TypeError):\n        u > s\n    with raises(TypeError):\n        u < s\n", "def test_unit_system_methods():\n    us = UnitSystem([meter, second], [kilometer, hour], \"TestSystem\")\n    assert str(us) == \"TestSystem\"\n    assert repr(us) == \"<UnitSystem: (meter, second)>\"\n\n    us2 = us.extend([centimeter], [minute], \"ExtendedSystem\")\n    assert str(us2) == \"ExtendedSystem\"\n    assert repr(us2) == \"<UnitSystem: (meter, second, centimeter)>\"\n\n    assert us2.get_dimension_system() == us.get_dimension_system()\n    assert us2.get_quantity_dimension(kilometer) == length\n    assert us2.get_quantity_scale_factor(kilometer) == 1000\n\n    assert us2.is_consistent == us.is_consistent\n    assert us2.derived_units == us.derived_units\n\n    assert us2.get_units_non_prefixed() == {kilometer, hour}\n", "def test_physical_constant_values():\n    from sympy.physics.units import definitions, convert_to, meter, second, kilogram\n\n    assert convert_to(definitions.speed_of_light, meter / second) == 299792458 * meter / second\n    assert convert_to(definitions.gravitational_constant, meter**3 / kilogram / second**2) == 6.67430e-11 * meter**3 / kilogram / second**2\n    assert convert_to(definitions.planck_constant, kilogram * meter**2 / second) == 6.62607015e-34 * kilogram * meter**2 / second\n"], "sample_751": ["def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work without sample_weights in the base estimator\n\n    The random weighted sampling is done internally in the _boost method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0])\n\n            return np.hstack([np.ones(X.shape[0]), np.zeros(X.shape[0])])\n\n    boost = AdaBoostClassifier(DummyEstimator(), n_estimators=3, algorithm='SAMME')\n    boost.fit(X, y_class)\n    assert_equal(len(boost.estimator_weights_), len(boost.estimator_errors_))\n", "def test_base_estimator_parameters():\n    # Test that base estimator parameters are passed correctly.\n    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2))\n    clf.fit(X, y_class)\n    for est in clf.estimators_:\n        assert_equal(est.max_depth, 2)\n\n    clf = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=2), random_state=0)\n    clf.fit(X, y_regr)\n    for est in clf.estimators_:\n        assert_equal(est.max_depth, 2)\n", "def test_adaboost_classifier_weight_validation():\n    # Test that AdaBoostClassifier raises an error with invalid weights\n    clf = AdaBoostClassifier()\n    X, y = datasets.make_classification(n_samples=10, n_features=2, random_state=0)\n    with pytest.raises(ValueError, match=\"Sample weights must be non-negative\"):\n        clf.fit(X, y, sample_weight=-np.ones(10))\n", "def test_adaboost_with_numpy_ints():\n    # Test Adaboost with numpy integer input\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    clf = AdaBoostClassifier(n_estimators=50, random_state=0)\n    clf.fit(X, y)\n    assert_equal(clf.predict(X).dtype, np.int64)\n", "def test_base_estimator_sample_weight():\n    \"\"\"\n    Test that base estimators support sample_weight.\n\n    Checks whether the base estimators support the `sample_weight` parameter\n    in their `fit` method. This is important for AdaBoost as it uses weighted\n    sampling in its boosting algorithm.\n    \"\"\"\n    # Create a simple base estimator that does not support sample_weight\n    class NoSampleWeightEstimator(BaseEstimator):\n            pass\n            return np.zeros(X.shape[0])\n\n    # Create an AdaBoostRegressor with the simple base estimator\n    boost = AdaBoostRegressor(NoSampleWeightEstimator(), n_estimators=3)\n\n    # Fitting should raise a TypeError because the base estimator does not support sample_weight\n    with pytest.raises(TypeError):\n        boost.fit(X, y_regr)\n\n    # Create a base estimator that supports sample_weight\n    class SampleWeightEstimator(BaseEstimator):\n            pass\n            return np.zeros(X.shape[0])\n\n    # Create an AdaBoostRegressor with the base estimator that supports sample_weight\n    boost = AdaBoostRegressor(SampleWeightEstimator(), n_estimators=3)\n\n    # Fitting should not raise an error\n    boost.fit(X, y_regr)\n"], "sample_892": ["def test_adaboost_classifier_sample_weight_shape_mismatch():\n    # Test that it gives proper exception on sample weight shape mismatch.\n    clf = AdaBoostClassifier()\n    msg = re.escape(\"sample_weight.shape == (1, 2), expected (6,)\")\n    with pytest.raises(ValueError, match=msg):\n        clf.fit(X, y_class, sample_weight=np.array([[-1, 1]]))\n", "def test_adaboost_classifier_base_estimator_no_predict_proba():\n    # Test that it gives proper exception when base estimator does not support predict_proba.\n    class NoPredictProbaEstimator:\n            pass\n\n            pass\n\n    clf = AdaBoostClassifier(estimator=NoPredictProbaEstimator(), algorithm=\"SAMME.R\")\n    msg = re.escape(\"AdaBoostClassifier with algorithm='SAMME.R' requires that the weak learner supports the calculation of class probabilities with a predict_proba method.\")\n    with pytest.raises(TypeError, match=msg):\n        clf.fit(X, y_class)\n", "def test_adaboost_estimator_without_predict_proba():\n    # check that an error is raised when using SAMME.R algorithm with an estimator\n    # that does not implement predict_proba\n\n    class EstimatorWithoutPredictProba:\n            pass\n\n            return np.zeros(X.shape[0])\n\n    boost = AdaBoostClassifier(EstimatorWithoutPredictProba(), algorithm=\"SAMME.R\")\n    err_msg = \"AdaBoostClassifier with algorithm='SAMME.R' requires that the weak learner supports the calculation of class probabilities with a predict_proba method\"\n    with pytest.raises(TypeError, match=err_msg):\n        boost.fit(X, y_class)\n", "def test_adaboost_classifier_without_predict_proba():\n    # Test that AdaBoostClassifier raises a TypeError when the base estimator does not have a predict_proba method.\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    model = AdaBoostClassifier(estimator=DecisionTreeClassifier(), algorithm=\"SAMME.R\")\n\n    err_msg = (\n        \"AdaBoostClassifier with algorithm='SAMME.R' requires that the weak learner supports the calculation of class \"\n        \"probabilities with a predict_proba method.\\n\"\n        \"Please change the base estimator or set algorithm='SAMME' instead.\"\n    )\n    with pytest.raises(TypeError, match=err_msg):\n        model.fit(X, y)\n", "def test_default_estimator_class(estimator):\n    # Check that the default estimator is DecisionTreeClassifier for\n    # AdaBoostClassifier and DecisionTreeRegressor for AdaBoostRegressor\n    if estimator == \"deprecated\":\n        clf = AdaBoostClassifier(base_estimator=estimator)\n        regr = AdaBoostRegressor(base_estimator=estimator)\n        with pytest.warns(FutureWarning, match=\"`base_estimator` was renamed\"):\n            clf.fit(X, y_class)\n            regr.fit(X, y_regr)\n    else:\n        clf = AdaBoostClassifier(estimator=estimator)\n        regr = AdaBoostRegressor(estimator=estimator)\n        clf.fit(X, y_class)\n        regr.fit(X, y_regr)\n\n    assert isinstance(clf.estimator_, DecisionTreeClassifier)\n    assert isinstance(regr.estimator_, DecisionTreeRegressor)\n"], "sample_860": ["def test_check_X_y_informative_error_with_estimator():\n    X = np.ones((2, 2))\n    y = None\n    msg = \"This MyEstimator instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n    with pytest.raises(ValueError, match=\"y cannot be None\"):\n        check_X_y(X, y, estimator=\"MyEstimator\")\n", "def test_check_array_numeric_dtype_casting():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\n    X_checked = check_array(X, dtype=\"numeric\")\n    assert X_checked.dtype == np.float32\n\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.int32)\n    X_checked = check_array(X, dtype=\"numeric\")\n    assert X_checked.dtype == np.float64\n\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.int64)\n    X_checked = check_array(X, dtype=\"numeric\")\n    assert X_checked.dtype == np.float64\n\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.object)\n    X_checked = check_array(X, dtype=\"numeric\")\n    assert X_checked.dtype == np.float64\n", "def test_allclose_dense_sparse_rtol_atol():\n    x = np.array([[1e-8, 1e-6], [1, 1]])\n    y = np.array([[1e-9, 1e-6], [1, 1]])\n    assert _allclose_dense_sparse(x, y, rtol=1e-7, atol=0)\n    assert not _allclose_dense_sparse(x, y, rtol=1e-9, atol=0)\n", "def test_check_X_y_multi_output_sparse():\n    # Test check_X_y with multi_output=True and sparse y\n    X = np.ones((2, 2))\n    y = sp.csr_matrix([[0], [1]])\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert sp.issparse(y_checked)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y.toarray(), y_checked.toarray())\n\n    # Test check_X_y with multi_output=True and sparse y with nan or inf values\n    y = sp.csr_matrix([[np.nan], [1]])\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity\"):\n        check_X_y(X, y, multi_output=True)\n\n    y = sp.csr_matrix([[np.inf], [1]])\n    with pytest.raises(ValueError, match=\"Input contains NaN, infinity\"):\n        check_X_y(X, y, multi_output=True)\n", "def test_check_array_dtype_object_conversion():\n    # test that data-frame like objects with dtype object\n    # get converted to float64\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.object)\n    X_df = MockDataFrame(X)\n    checked_arr = check_array(X_df, dtype=np.float64)\n    assert checked_arr.dtype == np.float64\n    checked_arr = check_array(X_df, dtype=np.float64, ensure_2d=False)\n    assert checked_arr.dtype == np.float64\n"], "sample_995": ["def test_Float_to_Integer():\n    assert Integer(Float(3.0)) == 3\n    assert Integer(Float(3.5)) == 3\n    assert Integer(Float(-3.5)) == -3\n    assert Integer(Float(0.0)) == 0\n", "def test_Float_approximation_interval():\n    assert Float(1.2345).approximation_interval(Integer) == (Integer(1), Integer(2))\n    assert Float(1.2345).approximation_interval(Rational) == (Rational(617, 500), Rational(1235, 1000))\n", "def test_NumberSymbol_complex_comparisons():\n    assert not (pi == I)\n    assert not (E == I)\n    assert not (GoldenRatio == I)\n    assert not (EulerGamma == I)\n    assert not (pi != I)\n    assert not (E != I)\n    assert not (GoldenRatio != I)\n    assert not (EulerGamma != I)\n\n    raises(TypeError, lambda: pi < I)\n    raises(TypeError, lambda: E < I)\n    raises(TypeError, lambda: GoldenRatio < I)\n    raises(TypeError, lambda: EulerGamma < I)\n    raises(TypeError, lambda: pi > I)\n    raises(TypeError, lambda: E > I)\n    raises(TypeError, lambda: GoldenRatio > I)\n    raises(TypeError, lambda: EulerGamma > I)\n    raises(TypeError, lambda: pi <= I)\n    raises(TypeError, lambda: E <= I)\n    raises(TypeError, lambda: GoldenRatio <= I)\n    raises(TypeError, lambda: EulerGamma <= I)\n    raises(TypeError, lambda: pi >= I)\n    raises(TypeError, lambda: E >= I)\n    raises(TypeError, lambda: GoldenRatio >= I)\n    raises(TypeError, lambda: EulerGamma >= I)\n", "def test_NumberSymbol_as_base_exp():\n    assert pi.as_base_exp() == (E, I*log(pi))\n    assert E.as_base_exp() == (E, 1)\n    assert GoldenRatio.as_base_exp() == (S.Half * (1 + sqrt(5)), 1)\n    assert EulerGamma.as_base_exp() == (exp(EulerGamma), 1)\n    assert oo.as_base_exp() == (exp(oo), 1)\n    assert (-oo).as_base_exp() == (exp(-oo), 1)\n    assert I.as_base_exp() == (exp(I*pi/2), 1)\n", "def test_Float_as_complex():\n    assert Float('1.0').as_real_imag() == (Float('1.0'), S.Zero)\n    assert Float('1.0').as_real_imag()[0].is_Float\n    assert Float('1.0').as_real_imag()[1].is_Zero\n    assert Float('1.0').as_coeff_mul()[0].is_Float\n    assert Float('1.0').as_coeff_mul()[1].is_One\n    assert Float('1.0').as_coeff_Mul()[0].is_Float\n    assert Float('1.0').as_coeff_Mul()[1].is_One\n"], "sample_1205": ["def test_PolyElement_mul_ground():\n    R, x = ring(\"x\", ZZ)\n    f = 3*x**2 + 2*x + 1\n\n    r = f.mul_ground(0)\n    assert r == 0 and isinstance(r, R.dtype)\n\n    r = f.mul_ground(1)\n    assert r == f and isinstance(r, R.dtype)\n\n    r = f.mul_ground(2)\n    assert r == 6*x**2 + 4*x + 2 and isinstance(r, R.dtype)\n\n    raises(CoercionFailed, lambda: f.mul_ground(QQ(1,7)))\n", "def test_PolyElement_trunc_ground():\n    _, x = ring(\"x\", ZZ)\n\n    f = 3*x**2 + 5*x - 7\n    g = f.trunc_ground(10)\n    assert g == 3*x**2 + 5*x - 7\n\n    g = f.trunc_ground(5)\n    assert g == 3*x**2 + x - 2\n", "def test_PolyElement_divides():\n    R, x, y = ring(\"x, y\", ZZ)\n    assert (x**2 - y**2).divides(x**4 - y**4)\n    assert not (x**2 - y**2).divides(x**3 - y**3)\n", "def test_PolyElement_imul_num():\n    R, x = ring(\"x\", ZZ)\n    f = x**3 + 4*x**2 + 2*x + 3\n\n    f.imul_num(2)\n    assert f == 2*x**3 + 8*x**2 + 4*x + 6\n\n    g = R(2)\n    g.imul_num(3)\n    assert g == 6\n\n    h = x\n    h.imul_num(0)\n    assert h == 0\n", "def test_PolyElement_cancel_with_field():\n    Fx, x = field(\"x\", QQ)\n    Rt, t = ring(\"t\", Fx)\n\n    f = (x**2 + 2)/2*t\n    g = t**2 + (x**2 + 4)/4\n\n    assert f.cancel(g) == ((x**2 + 2)*t, 2*t**2 + x**2 + 4)\n\n    assert (-f).cancel(g) == ((-x**2 - 2)*t, 2*t**2 + x**2 + 4)\n    assert f.cancel(-g) == ((x**2 + 2)*t, -2*t**2 - x**2 - 4)\n"], "sample_198": ["def test_expression_wrapper_output_field(self):\n    expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n    self.assertEqual(expr.output_field, IntegerField())\n", "def test_date_subtraction_with_none(self):\n    queryset = Experiment.objects.annotate(\n        difference=F('completed') - Value(None, output_field=DateField()),\n    )\n    self.assertIsNone(queryset.first().difference)\n", "def test_bitwise_not(self):\n    with self.assertRaisesMessage(NotImplementedError, 'Bitwise NOT is not implemented.'):\n        ~Combinable()\n", "def test_expression_wrapper_resolve_expression(self):\n    expr = ExpressionWrapper(F('field'), output_field=IntegerField())\n    resolved_expr = expr.resolve_expression(query=Company.objects.all())\n    self.assertIsInstance(resolved_expr, ExpressionWrapper)\n    self.assertIsInstance(resolved_expr.expression, Ref)\n    self.assertEqual(resolved_expr.expression.name, 'field')\n", "def test_expression_wrapper_output_field(self):\n    class TestModel(Model):\n        field = IntegerField()\n\n    expr = ExpressionWrapper(F('field'), output_field=IntegerField())\n    self.assertEqual(expr.output_field, IntegerField())\n    self.assertEqual(expr.expression.output_field, IntegerField())\n\n    expr = ExpressionWrapper(F('field'), output_field=CharField())\n    self.assertEqual(expr.output_field, CharField())\n    self.assertEqual(expr.expression.output_field, CharField())\n\n    expr = ExpressionWrapper(F('field'), output_field=TestModel._meta.get_field('field'))\n    self.assertEqual(expr.output_field, IntegerField())\n    self.assertEqual(expr.expression.output_field, IntegerField())\n"], "sample_191": ["def test_nonexistent_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    with self.tick_twice():\n        self.increment_mtime(self.nonexistent_file)\n    self.assertEqual(notify_mock.call_count, 0)\n", "    def test_run_loop_calls_stop(self):\n        reloader = autoreload.BaseReloader()\n        reloader.tick = mock.MagicMock()\n        reloader.tick.side_effect = StopIteration\n        with mock.patch.object(reloader, 'stop') as mock_stop:\n            reloader.run_loop()\n        mock_stop.assert_called_once()\n", "    def test_snapshot_files_updates_with_new_file(self):\n        new_file = self.ensure_file(self.tempdir / 'new_file.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, new_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertIn(new_file, snapshot1)\n            self.increment_mtime(new_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[new_file], snapshot2[new_file])\n", "def test_tick_triggers_on_new_files(self, mock_notify_file_changed):\n    new_file = self.tempdir / 'new_file.py'\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, new_file]):\n        ticker = self.reloader.tick()\n        next(ticker)\n        self.ensure_file(new_file)\n        next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n", "    def test_notify_file_changed(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.utils.autoreload.file_changed.send') as mocked_send:\n            mocked_send.return_value = [(None, False)]\n            with self.assertRaises(SystemExit):\n                reloader.notify_file_changed('/path/to/file')\n        mocked_send.assert_called_once_with(sender=reloader, file_path='/path/to/file')\n"], "sample_629": ["def test_expand_modules_with_ignore_list():\n    ignore_list = [\"unittest_lint.py\"]\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    files_or_modules = [Path(__file__).parent]\n    expected = [init_of_package, this_file_from_init]\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_ignore_list():\n    ignore_list = [\"unittest_lint.py\"]\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    files_or_modules = [Path(__file__).parent]\n    expected = [init_of_package, this_file_from_init]\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_ignore_list():\n    files_or_modules = [__file__, Path(__file__).parent]\n    ignore_list = [\"unittest_lint.py\"]\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    expected = [init_of_package, this_file_from_init]\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_with_ignore_list():\n    files_or_modules = [EXPAND_MODULES]\n    ignore_list = [\"unittest_expand_modules.py\"]\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    assert not modules\n    assert not errors\n", "def test_expand_modules_ignore_list():\n    files_or_modules = [__file__, Path(__file__).parent]\n    ignore_list = [\"unittest_lint.py\"]\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    expected = [this_file, init_of_package, this_file_from_init]\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n"], "sample_537": ["def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    assert_allclose(P, Su, atol=1e-06)\n", "def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    assert_allclose(P, Su, atol=1e-06)\n", "def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    assert_allclose(P, Su, atol=1e-06)\n", "def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    assert_allclose(P, Su, atol=1e-06)\n", "def test_csd_oversampling():\n    \"\"\"Test the case len(x) < NFFT for csd().\"\"\"\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    v = np.array([0, 2, 3, 4, 2, 3, 2])\n    dt = 1.0\n    Sxu = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    Syv = np.abs(np.fft.fft(v) * dt)**2 / (dt * v.size)\n    Sxy = np.conj(np.fft.fft(u) * dt) * np.fft.fft(v) * dt / (dt * u.size)\n    Pxy, f = mlab.csd(x=u, y=v, NFFT=u.size*2, Fs=1/dt, window=mlab.window_none,\n                      detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                      sides='onesided')\n    Sxy_1side = np.append([Sxy[0]], Sxy[1:4] + Sxy[4:][::-1])\n    assert_almost_equal(np.sum(Pxy), np.sum(Sxy_1side))  # same energy\n    assert_almost_equal(np.sum(Pxy / Sxu[1:4]), np.sum(Syv[1:4] / Sxu[1:4]))  # correct coherence\n"], "sample_607": ["def test_detect_parameters():\n    backend = DummyBackendEntrypoint1\n    parameters = plugins.detect_parameters(backend.open_dataset)\n    assert parameters == (\"decoder\",)\n\n    backend = DummyBackendEntrypointArgs\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(backend.open_dataset)\n\n    backend = DummyBackendEntrypointKwargs\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(backend.open_dataset)\n", "def test_get_backend_with_string():\n    backend = plugins.get_backend(\"dummy\")\n    assert isinstance(backend, DummyBackendEntrypoint1)\n", "def test_get_backend():\n    backend = plugins.get_backend(\"dummy\")\n    assert isinstance(backend, DummyBackendEntrypoint1)\n\n    with pytest.raises(ValueError, match=r\"unrecognized engine\"):\n        plugins.get_backend(\"unknown\")\n\n    with pytest.raises(TypeError, match=r\"engine must be a string or a subclass\"):\n        plugins.get_backend(123)\n", "def test_get_backend():\n    engine = \"dummy\"\n    backend = plugins.get_backend(engine)\n    assert isinstance(backend, DummyBackendEntrypoint1)\n\n    engine = DummyBackendEntrypoint2\n    backend = plugins.get_backend(engine)\n    assert isinstance(backend, DummyBackendEntrypoint2)\n\n    with pytest.raises(ValueError, match=r\"unrecognized engine\"):\n        plugins.get_backend(\"invalid_engine\")\n\n    with pytest.raises(TypeError, match=r\"engine must be a string or a subclass\"):\n        plugins.get_backend(123)\n", "def test_get_backend_str():\n    backend = plugins.get_backend(\"netcdf4\")\n    assert backend == \"DummyBackend\"\n"], "sample_164": ["    def test_page_found_no_warning(self):\n        self.client.get('/innocent/')\n        self.assertEqual(self.logger_output.getvalue(), '')\n", "    def get_admin_email_handler(self, logger):\n        return [\n            h for h in logger.handlers\n            if h.__class__.__name__ == \"AdminEmailHandler\"\n        ][0]\n", "    def test_emit_with_no_exc_info(self):\n        \"\"\"\n        AdminEmailHandler should handle records with no exc_info.\n        \"\"\"\n        handler = AdminEmailHandler()\n        record = self.logger.makeRecord('name', logging.ERROR, 'function', 'lno', 'message', None, None)\n        handler.emit(record)\n        self.assertEqual(len(mail.outbox), 1)\n        msg = mail.outbox[0]\n        self.assertEqual(msg.subject, \"[Django] ERROR: message\")\n        self.assertNotIn(\"Traceback (most recent call last):\", msg.body)\n", "    def test_server_formatter_status_codes(self):\n        self.assertLogsRequest(\n            url='/internal_server_error/?status=200',\n            level='INFO',\n            status_code=200,\n            msg='Success: /internal_server_error/',\n        )\n        self.assertLogsRequest(\n            url='/internal_server_error/?status=302',\n            level='INFO',\n            status_code=302,\n            msg='Redirect: /internal_server_error/',\n        )\n        self.assertLogsRequest(\n            url='/internal_server_error/?status=400',\n            level='WARNING',\n            status_code=400,\n            msg='Bad Request: /internal_server_error/',\n        )\n        self.assertLogsRequest(\n            url='/internal_server_error/?status=500',\n            level='ERROR',\n            status_code=500,\n            msg='Internal Server Error: /internal_server_error/',\n        )\n", "    def test_server_formatter_uses_server_time(self):\n        formatter = ServerFormatter(fmt='{server_time}')\n        self.assertTrue(formatter.uses_server_time())\n\n        formatter = ServerFormatter(fmt='no server_time here')\n        self.assertFalse(formatter.uses_server_time())\n"], "sample_106": ["def test_cache_versioning_delete_many(self):\n    cache.set('answer1', 37, version=1)\n    cache.set('answer1', 42, version=2)\n    cache.set('answer2', 37, version=1)\n    cache.set('answer2', 42, version=2)\n    cache.delete_many(['answer1', 'answer2'])\n    self.assertIsNone(cache.get('answer1', version=1))\n    self.assertIsNone(cache.get('answer1', version=2))\n    self.assertIsNone(cache.get('answer2', version=1))\n    self.assertIsNone(cache.get('answer2', version=2))\n\n    cache.set('answer3', 37, version=1)\n    cache.set('answer3', 42, version=2)\n    cache.set('answer4', 37, version=1)\n    cache.set('answer4', 42, version=2)\n    caches['v2'].delete_many(['answer3', 'answer4'])\n    self.assertIsNone(cache.get('answer3', version=1))\n    self.assertIsNone(cache.get('answer3', version=2))\n    self.assertIsNone(cache.get('answer4', version=1))\n    self.assertIsNone(cache.get('answer4', version=2))\n", "def test_cache_versioning_get_set_with_version_as_none(self):\n    cache.set('answer1', 42, version=None)\n    self.assertEqual(cache.get('answer1'), 42)\n    self.assertIsNone(cache.get('answer1', version=1))\n    self.assertIsNone(cache.get('answer1', version=2))\n", "def test_has_vary_header(self):\n    response = HttpResponse()\n    response['Vary'] = 'Accept-Encoding'\n    self.assertTrue(has_vary_header(response, 'Accept-Encoding'))\n    self.assertFalse(has_vary_header(response, 'Cookie'))\n", "def test_learn_cache_key_with_timeout(self):\n    request = self.factory.head(self.path)\n    response = HttpResponse()\n    response['Vary'] = 'Pony'\n    # Make sure that the Vary header is added to the key hash\n    learn_cache_key(request, response, timeout=10)\n\n    self.assertEqual(\n        get_cache_key(request),\n        'views.decorators.cache.cache_page.settingsprefix.GET.'\n        '18a03f9c9649f7d684af5db3524f5c99.d41d8cd98f00b204e9800998ecf8427e',\n        'Cache key should use the specified timeout'\n    )\n", "def test_custom_key_func_with_version(self):\n    cache.set('answer1', 42, version=1)\n    caches['custom_key'].set('answer2', 42, version=2)\n\n    self.assertEqual(cache.get('answer1'), 42)\n    self.assertIsNone(caches['custom_key'].get('answer1'))\n    self.assertIsNone(cache.get('answer2'))\n    self.assertEqual(caches['custom_key'].get('answer2'), 42)\n"], "sample_16": ["def test_rollaxis(self):\n    self.check(np.rollaxis, 0, 2)\n", "def test_rec_functions_structured_input():\n    # Test merge_arrays with structured input\n    arr = rfn.merge_arrays((self.q_pv, self.q_pv_t))\n    assert_array_equal(arr[\"f0\"], self.q_pv)\n    assert_array_equal(arr[\"f1\"], self.q_pv_t)\n    assert arr.unit == ((u.km, u.km / u.s), ((u.km, u.km / u.s), u.s))\n", "def test_array_split(self):\n    q = np.arange(10.0).reshape(2, 5) * u.m\n    out = np.array_split(q, 3)\n    expected = np.array_split(q.value, 3)\n    expected = [x * u.m for x in expected]\n    assert len(out) == len(expected)\n    assert all(o.shape == x.shape for o, x in zip(out, expected))\n    assert all(np.all(o == x) for o, x in zip(out, expected))\n", "def test_repeat_unitless_quantity():\n    q = np.repeat(1.0, 3) * u.dimensionless_unscaled\n    out = np.repeat(q, 2)\n    expected = np.repeat(q.value, 2) * u.dimensionless_unscaled\n    assert np.all(out == expected)\n", "def test_rec_functions_structured_unit_equality():\n    # Test that structured units with equal component units are equal\n    assert u.StructuredUnit((u.km, u.s)) == u.StructuredUnit((u.m * 1e3, u.s))\n\n    # Test that structured units with unequal component units are not equal\n    assert u.StructuredUnit((u.km, u.s)) != u.StructuredUnit((u.m, u.s))\n\n    # Test that structured units with equal nested component units are equal\n    assert u.StructuredUnit((u.StructuredUnit((u.km, u.s)), u.s)) == u.StructuredUnit((u.StructuredUnit((u.m * 1e3, u.s)), u.s))\n\n    # Test that structured units with unequal nested component units are not equal\n    assert u.StructuredUnit((u.StructuredUnit((u.km, u.s)), u.s)) != u.StructuredUnit((u.StructuredUnit((u.m, u.s)), u.s))\n"], "sample_897": ["def test_partial_dependence_display_kind_centered_single_feature(\n    pyplot,\n    kind,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_invalid_kind(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_sample_weight_and_centered(\n    pyplot,\n    kind,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_with_sample_weight(\n    pyplot,\n    clf_diabetes,\n    diabetes,\n    centered,\n    kind,", "def test_partial_dependence_display_with_sample_weight(pyplot, kind):\n    \"\"\"Check that the results are consistent between brute and recursive methods when\n    sample weights are provided.\n    \"\"\"\n    X, y = make_regression(n_samples=50, n_features=2, random_state=0)\n    estimator = GradientBoostingRegressor(random_state=0)\n    estimator.fit(X, y)\n\n    sample_weight = np.random.uniform(size=len(y))\n\n    disp_brute = PartialDependenceDisplay.from_estimator(\n        estimator,\n        X,\n        features=[0, 1],\n        kind=kind,\n        method=\"brute\",\n        sample_weight=sample_weight,\n    )\n\n    disp_recursive = PartialDependenceDisplay.from_estimator(\n        estimator,\n        X,\n        features=[0, 1],\n        kind=kind,\n        method=\"recursive\",\n        sample_weight=sample_weight,\n    )\n\n    for pd_brute, pd_recursive in zip(disp_brute.pd_results, disp_recursive.pd_results):\n        assert_allclose(pd_brute[\"average\"], pd_recursive[\"average\"])\n"], "sample_618": ["def test_cross_broadcasting(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=dim)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_skipna():\n    a = xr.DataArray([[1, np.nan], [2, 3]], dims=(\"x\", \"y\"))\n    b = xr.DataArray([[4, 5], [6, np.nan]], dims=(\"x\", \"y\"))\n\n    actual = xr.cross(a, b, dim=\"y\", skipna=True)\n    expected = xr.DataArray([-3, np.nan], dims=\"x\")\n\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_broadcasting(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=dim)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_coords_not_aligned(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a =", "def test_cross_with_coordinates(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    expected = np.cross(ae, be, axis=axis)\n\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=dim)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n"], "sample_992": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n", "def test_CustomFunctionPrinter():\n    prntr = PythonCodePrinter({'user_functions': {'CustomFunction': 'custom.CustomFunction'}})\n    expr = CustomFunction(x, y)\n    assert prntr.doprint(expr) == 'custom.CustomFunction(x, y)'\n    assert prntr.module_imports == {'custom': {'CustomFunction'}}\n", "def test_CustomFunction():\n        return x**2 + 2*x + 1\n\n    prntr = PythonCodePrinter({'user_functions': {'custom_func': 'custom_func'}})\n    expr = custom_func(x)\n    assert prntr.doprint(expr) == 'custom_func(x)'\n    assert prntr.module_imports == {}\n", "def test_CustomPrintedObject_SciPyPrinter():\n    obj = CustomPrintedObject()\n    p = SciPyPrinter()\n    assert p.doprint(obj) == 'numpy'\n    assert 'numpy' in p.module_imports\n", "def test_CustomPrintMethod():\n    obj = CustomPrintedObject()\n    prntr = PythonCodePrinter()\n    raises(AttributeError, lambda: prntr.doprint(obj))\n"], "sample_541": ["def test_polygon_selector_handle_props():\n    ax = get_ax()\n    onselect = mock.Mock(spec=noop, return_value=None)\n    tool = widgets.PolygonSelector(ax, onselect, handle_props=dict(markeredgecolor='r', markersize=10))\n    event_sequence = [\n        *polygon_place_vertex(50, 50),\n        *polygon_place_vertex(150, 50),\n        *polygon_place_vertex(50, 150),\n        *polygon_place_vertex(50, 50),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    for artist in tool._handles_artists:\n        assert artist.get_markeredgecolor() == 'r'\n        assert artist.get_markersize() == 10\n", "def test_rectangle_selector_update(ax):\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    tool = widgets.RectangleSelector(ax, onselect, interactive=True)\n    # move outside of axis\n    click_and_drag(tool, start=(100, 110), end=(150, 120))\n\n    onselect.assert_called_once()\n    assert tool.extents == (100.0, 150.0, 110.0, 120.0)\n\n    tool.update()\n    # Check that update doesn't trigger onselect\n    onselect.assert_called_once()\n", "def test_polygon_selector_draw_bounding_box(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that the bounding box was created\n    assert tool._box is not None\n    assert tool._box.get_visible()\n\n    # Check that the bounding box is around the polygon\n    np.testing.assert_allclose(\n        tool._box.extents, (0.0, 40.0, 0.0, 40.0))\n\n    # Remove the bounding box\n    tool._remove_box()\n    assert tool._box is None\n\n    # Check that the bounding box is not visible\n    tool.set_visible(True)\n    assert not tool.get_visible()\n\n    # Add the bounding box back\n    tool._add_box()\n    assert tool._box is not None\n    assert tool._box.get_visible()\n", "def test_polygon_selector_set_extents(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    tool = widgets.PolygonSelector(ax, onselect=noop)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    tool.extents = (20, 100, 20, 100)\n    assert tool.verts == [(20, 20), (100, 20), (20, 100)]\n", "def test_polygon_selector_bounds():\n    # Test that the polygon selector correctly handles vertices outside the axes bounds\n    ax = get_ax()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n    # Create a polygon with a vertex outside the axes bounds\n    expected_result = [(5, 5), (15, 5), (10, 15), (5, 5)]\n    event_sequence = [\n        *polygon_place_vertex(5, 5),\n        *polygon_place_vertex(15, 5),\n        *polygon_place_vertex(10, 15),\n        *polygon_place_vertex(5, 5),\n    ]\n    check_polygon_selector(event_sequence, expected_result, 1)\n"], "sample_330": ["    def test_null_reference(self):\n        ref = ObjectReference.objects.create(obj=None)\n        self.assertIsNone(ref.obj)\n\n        ref = ObjectReference.objects.get(obj=None)\n        self.assertIsNone(ref.obj)\n", "    def test_format_number(self):\n        from django.db.backends.base.operations import format_number\n        self.assertEqual(format_number(123.456, 6, 2), \"123.46\")\n        self.assertEqual(format_number(123.456, 4, 2), \"123.46\")\n        self.assertEqual(format_number(123.456, 3, 2), \"123.46\")\n        self.assertEqual(format_number(123.456, None, 2), \"123.46\")\n        self.assertEqual(format_number(123.456, None, None), \"123.456\")\n        self.assertEqual(format_number(None, 6, 2), None)\n", "    def test_debug_cursor_wrapper(self):\n        with self.assertLogs('django.db.backends', 'DEBUG') as cm:\n            obj = Object.objects.create()\n            self.assertEqual(len(cm.output), 1)\n            self.assertIn(\"INSERT INTO\", cm.output[0])\n\n        with self.assertLogs('django.db.backends', 'DEBUG') as cm:\n            obj = Object.objects.get(id=obj.id)\n            self.assertEqual(len(cm.output), 1)\n            self.assertIn(\"SELECT\", cm.output[0])\n", "    def test_callproc_without_kwargs(self):\n        with mock.patch.object(connection, 'validate_no_broken_transaction') as mock_validate:\n            with mock.patch.object(connection.cursor, 'callproc') as mock_callproc:\n                wrapper = CursorWrapper(connection.cursor, connection)\n                wrapper.callproc('procname')\n                mock_validate.assert_called_once()\n                mock_callproc.assert_called_once_with('procname')\n", "    def test_keyword_field_access(self):\n        # Create a Reporter instance\n        reporter = Reporter.objects.create(first_name='John', last_name='Doe')\n\n        # Create a SQLKeywordsModel instance with the reporter\n        model_instance = SQLKeywordsModel.objects.create(reporter=reporter)\n\n        # Access the reporter field using a keyword as field name\n        self.assertEqual(model_instance.reporter, reporter)\n"], "sample_481": ["def test_join_autoescape_on(self):\n    var_list = [\"<p>Hello World!</p>\", \"beta & me\", \"<script>Hi!</script>\"]\n    context = {\"var_list\": var_list, \"var_joiner\": \"<br/>\"}\n    output = self.engine.render_to_string(\"join_autoescape_on\", context)\n    expected_result = \"&lt;p&gt;Hello World!&lt;/p&gt;&lt;br/&gt;beta &amp; me&lt;br/&gt;&lt;script&gt;Hi!&lt;/script&gt;\"\n    self.assertEqual(output, expected_result)\n", "def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"alpha\", \"beta\", \"gamma\"], \"var\": mark_safe(\" & \")}\n    )\n    self.assertEqual(output, \"alpha & beta & gamma\")\n", "def test_join_autoescape_on(self):\n    var_list = [\"<p>Hello World!</p>\", \"beta & me\", \"<script>Hi!</script>\"]\n    context = {\"var_list\": var_list, \"var_joiner\": \"<br/>\"}\n    output = self.engine.render_to_string(\"join_autoescape_on\", context)\n    expected_result = \"&lt;p&gt;Hello World!&lt;/p&gt;&lt;br/&gt;beta &amp; me&lt;br/&gt;&lt;script&gt;Hi!&lt;/script&gt;\"\n    self.assertEqual(output, expected_result)\n", "def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"alpha\", \"beta\", \"gamma\"], \"var\": mark_safe(\"<br>\")}\n    )\n    self.assertEqual(output, \"alpha<br>beta<br>gamma\")\n", "def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"alpha\", \"beta\", \"gamma\"], \"var\": \" | \"}\n    )\n    self.assertEqual(output, \"alpha | beta | gamma\")\n"], "sample_499": ["def test_legend_title_fontsize_rcparam():\n    # test the rcParams legend.title_fontsize\n    plt.plot(range(10))\n    mpl.rcParams['legend.title_fontsize'] = 22\n    leg = plt.legend(title='Aardvark')\n    assert leg.get_title().get_fontsize() == 22\n", "def test_legend_labelcolor_valid_strings(labelcolor):\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), np.arange(10)*1, label='#1')\n    ax.plot(np.arange(10), np.arange(10)*2, label='#2')\n    ax.plot(np.arange(10), np.arange(10)*3, label='#3')\n\n    leg = ax.legend(labelcolor=labelcolor)\n    for text in leg.get_texts():\n        assert text.get_color() is not None\n", "def test_legend_edgecolor_inherit():\n    # Test that legend edgecolor inherits from axes edgecolor if not specified\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    ax.set_facecolor('lightgray')\n    ax.set_edgecolor('red')\n    leg = ax.legend(edgecolor='inherit')\n    assert leg.get_frame().get_edgecolor() == ax.spines['left'].get_edgecolor()\n", "def test_legend_with_multiple_axes():\n    # Test legend with multiple axes\n    fig, axs = plt.subplots(2)\n    axs[0].plot([1, 2], [3, 4], label='line1')\n    axs[1].plot([5, 6], [7, 8], label='line2')\n    fig.legend()\n    assert len(fig.legends) == 1\n    assert len(axs[0].get_legend_handles_labels()[0]) == 1\n    assert len(axs[1].get_legend_handles_labels()[0]) == 1\n", "def test_legend_bbox_to_anchor_transform():\n    # Test that bbox_to_anchor accepts transform parameter\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='Aardvark')\n    bbox_to_anchor = (1.05, 1)\n    transform = ax.transAxes\n    leg = ax.legend(bbox_to_anchor=bbox_to_anchor, bbox_transform=transform)\n    assert leg.get_bbox_to_anchor() == mtransforms.Bbox(np.array([[1.05, 1], [1, 0]]))\n"], "sample_858": ["def test_transform_hard_voting():\n    \"\"\"Check transform method of VotingClassifier with hard voting on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    assert_array_equal(eclf.transform(X).shape, (4, 3))\n    assert_array_equal(eclf.transform(X),\n                       np.array([[1, 1, 1],\n                                 [1, 1, 1],\n                                 [2, 2, 1],\n                                 [2, 2, 2]]))\n", "def test_transform_hardvoting():\n    \"\"\"Check transform method of VotingClassifier with hard voting on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    assert_array_equal(eclf.transform(X).shape, (4, 3))\n    assert_array_equal(eclf.transform(X), np.array([[1, 1, 1],\n                                                    [1, 1, 1],\n                                                    [2, 2, 1],\n                                                    [2, 2, 2]]))\n", "def test_set_estimator_invalid_name():\n    \"\"\"VotingClassifier should raise ValueError for invalid estimator names\"\"\"\n    clf = LogisticRegression(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf)])\n    msg = \"Estimator names must not contain __: got ['lr__invalid']\"\n    assert_raise_message(ValueError, msg, eclf.set_params, lr__invalid=clf)\n", "def test_predict_regressor():\n    \"\"\"Check regression prediction on boston dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = DecisionTreeRegressor(random_state=0)\n    ereg = VotingRegressor([('lr', reg1), ('tree', reg2)])\n\n    ereg.fit(X_r, y_r)\n    pred = ereg.predict(X_r)\n\n    # Ensure prediction array shape is correct\n    assert pred.shape == y_r.shape\n\n    # Ensure predictions are within a reasonable range for boston dataset\n    assert np.all(pred >= np.min(y_r))\n    assert np.all(pred <= np.max(y_r))\n", "def test_named_estimators():\n    \"\"\"Check named_estimators property of VotingClassifier on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft').fit(X, y)\n\n    assert eclf.named_estimators_['lr'] is eclf.estimators_[0]\n    assert eclf.named_estimators_['rf'] is eclf.estimators_[1]\n    assert eclf.named_estimators_['gnb'] is eclf.estimators_[2]\n"], "sample_1121": ["def test_floordiv():\n    assert x // y == floor(x/y)\n    assert x // 3 == floor(x/3)\n    assert 3 // x == floor(3/x)\n", "def test_divmod_with_zero_divisor():\n    raises(ZeroDivisionError, lambda: divmod(x, 0))\n", "def test_issue_18507_second():\n    assert Mul(zoo, 0, zoo) is nan\n", "def test_div():\n    eq = x/y\n    assert eq.is_Mul and eq.args == (x, y**(-1))\n    eq = x/3\n    assert eq.is_Mul and eq.args == (x, Rational(1, 3))\n    eq = 3/x\n    assert eq.is_Mul and eq.args == (Rational(3), x**(-1))\n", "def test_Add_is_negative():\n    x, y = symbols('x y', negative=True)\n    assert (x + y).is_negative\n\n    x, y = symbols('x y', negative=False)\n    assert (x + y).is_negative is None\n"], "sample_406": ["def test_create_method_with_defaults(self):\n    # You can create saved objects in a single step with defaults\n    a13 = Article.objects.create()\n    self.assertEqual(a13.headline, \"Default headline\")\n    self.assertIsNotNone(a13.pub_date)\n", "def test_save_update_fields(self):\n    a = Article.objects.create(headline=\"Original headline\", pub_date=datetime.now())\n    a.headline = \"Updated headline\"\n    with self.assertNumQueries(1):\n        a.save(update_fields=[\"headline\"])\n    updated_article = Article.objects.get(id=a.id)\n    self.assertEqual(updated_article.headline, \"Updated headline\")\n", "def test_save_update_fields(self):\n    a = Article.objects.create(\n        headline=\"Article 1\",\n        pub_date=datetime(2005, 7, 31, 12, 30, 45),\n    )\n    new_pub_date = datetime(2005, 8, 1, 12, 30, 45)\n    a.pub_date = new_pub_date\n    a.save(update_fields=['pub_date'])\n    a_refreshed = Article.objects.get(pk=a.pk)\n    self.assertEqual(a_refreshed.pub_date, new_pub_date)\n    self.assertEqual(a_refreshed.headline, \"Article 1\")\n", "def test_update_or_create(self):\n    # Test that update_or_create correctly updates an existing record and creates a new one if it doesn't exist\n    Article.objects.create(id=1, headline=\"Original headline\", pub_date=datetime(2005, 7, 28))\n    updated_article, created = Article.objects.update_or_create(id=1, defaults={'headline': 'Updated headline'})\n    self.assertFalse(created)\n    self.assertEqual(updated_article.headline, 'Updated headline')\n\n    new_article, created = Article.objects.update_or_create(id=2, defaults={'headline': 'New headline', 'pub_date': datetime(2005, 7, 29)})\n    self.assertTrue(created)\n    self.assertEqual(new_article.headline, 'New headline')\n", "def test_deleting_an_object_does_not_delete_its_related_objects_by_default(self):\n    a = Article.objects.create(headline=\"To be deleted\", pub_date=datetime(2005, 7, 28))\n    s = SelfRef.objects.create(article=a)\n    a.delete()\n    # The related object still exists\n    self.assertEqual(SelfRef.objects.count(), 1)\n"], "sample_1196": ["def test_contains_eval():\n    x = Symbol('x')\n    assert Contains(x, S.Naturals).doit() == (x >= 0) & x.is_integer\n    assert Contains(x, S.Integers).doit() == x.is_integer\n    assert Contains(x, S.Reals).doit() == True\n", "def test_contains_non_evaluable_set():\n    x = Symbol('x')\n    y = Symbol('y')\n    s = FiniteSet(x, y)\n    assert Contains(x, s) == Contains(x, s, evaluate=False)\n    assert Contains(y, s) == Contains(y, s, evaluate=False)\n    assert Contains(2, s) == Contains(2, s, evaluate=False)\n", "def test_contains_complex():\n    x = Symbol('x', complex=True)\n    assert Contains(x, S.Complexes) is S.true\n    assert Contains(x, S.Reals) != S.true\n    assert Contains(x, S.Integers) != S.true\n", "def test_non_boolean_sets():\n    x = Symbol('x')\n    class NonBooleanSet(Set):\n            return x if x > 0 else -x\n    non_bool_set = NonBooleanSet()\n    assert Contains(x, non_bool_set) == Contains(x, non_bool_set)\n", "def test_contains_complex_set():\n    x = Symbol('x')\n    z = Symbol('z')\n    complex_set = FiniteSet(1, 2, 3, Eq(z, 4), Interval(5, 6))\n    assert Contains(x, complex_set) == (x == 1) | (x == 2) | (x == 3) | (z == 4) | ((5 <= x) & (x <= 6))\n    assert Contains(1, complex_set) == S.true\n    assert Contains(4, complex_set) == S.false\n    assert Contains(z, complex_set) == (z == 4)\n"], "sample_93": ["def test_aggregation_with_distinct_values(self):\n    authors = Author.objects.annotate(distinct_books=Count('book__id', distinct=True))\n    for author in authors:\n        distinct_books = Book.objects.filter(authors=author).values('name').distinct().count()\n        self.assertEqual(author.distinct_books, distinct_books)\n", "def test_case_when_annotation(self):\n    books = Book.objects.annotate(\n        price_category=Case(\n            When(price__lt=Decimal(\"50.0\"), then=Value('Cheap')),\n            When(price__lt=Decimal(\"100.0\"), then=Value('Moderate')),\n            default=Value('Expensive')\n        )\n    )\n\n    cheap_books = books.filter(price_category='Cheap')\n    self.assertEqual(cheap_books.count(), 3)\n\n    moderate_books = books.filter(price_category='Moderate')\n    self.assertEqual(moderate_books.count(), 2)\n\n    expensive_books = books.filter(price_category='Expensive')\n    self.assertEqual(expensive_books.count(), 1)\n", "def test_annotation_with_default_value(self):\n    authors = Author.objects.annotate(friend_count=Count('friends', default=0))\n    a = authors.get(name=\"Brad Dayley\")\n    self.assertEqual(a.friend_count, 0)\n\n    authors = authors.filter(friend_count__gt=0).order_by('name')\n    self.assertQuerysetEqual(\n        authors, [\n            ('Adrian Holovaty', 2),\n            ('Jacob Kaplan-Moss', 2),\n            ('James Bennett', 1),\n            ('Jeffrey Forcier', 2),\n            ('Paul Bissex', 2),\n            ('Wesley J. Chun', 3)\n        ],\n        lambda a: (a.name, a.friend_count)\n    )\n", "def test_aggregate_expression_with_values(self):\n    books = Book.objects.values('rating').annotate(total_pages=Sum('pages')).order_by('rating')\n    avg_pages = books.aggregate(Avg('total_pages'))\n    self.assertEqual(avg_pages['total_pages__avg'], Approximate(437.0, places=1))\n", "def test_aggregate_annotation_with_filter(self):\n    publishers = Publisher.objects.annotate(\n        cheap_books_count=Count('book', filter=Q(book__price__lt=Decimal(\"40.0\")))\n    ).filter(cheap_books_count__gt=0).order_by(\"pk\")\n    self.assertQuerysetEqual(publishers, ['Apress', 'Prentice Hall'], lambda p: p.name)\n\n    authors = Author.objects.annotate(\n        young_friends_count=Count('friends', filter=Q(friends__age__lt=30))\n    ).filter(young_friends_count__gt=0).order_by(\"pk\")\n    self.assertQuerysetEqual(authors, ['Adrian Holovaty', 'Jacob Kaplan-Moss', 'Jeffrey Forcier', 'Paul Bissex', 'Wesley J. Chun'], lambda a: a.name)\n"], "sample_810": ["def test_pipeline_fit_transform_with_fit_params():\n    # tests that Pipeline passes fit_params to intermediate steps\n    # when fit_transform is invoked\n    pipe = Pipeline([('transf', TransfFitParams()), ('clf', FitParamT())])\n    pipe.fit_transform(X=None,\n                       y=None,\n                       transf__should_get_this=True,\n                       clf__should_succeed=True)\n    assert pipe.named_steps['transf'].fit_params['should_get_this']\n    assert 'should_succeed' not in pipe.named_steps['transf'].fit_params\n", "def test_pipeline_with_transform_only_final_estimator():\n    # Test that pipeline works with a transformer as the final estimator\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    transf = Transf()\n    pipeline = Pipeline([('mock', transf), ('final_transf', transf)])\n\n    # test transform:\n    X_trans = pipeline.fit(X, y).transform(X)\n    X_trans2 = transf.fit(X, y).transform(X)\n    assert_array_almost_equal(X_trans, X_trans2)\n\n    # test inverse_transform:\n    X_back = pipeline.inverse_transform(X_trans)\n    X_back2 = transf.inverse_transform(X_trans)\n    assert_array_almost_equal(X_back, X_back2)\n", "def test_pipeline_with_final_estimator_as_none():\n    # Test the pipeline with the final estimator as None\n    X = np.array([[1, 2], [3, 4]])\n    pipeline = Pipeline([('transf', Transf()), ('final', None)])\n    pipeline.fit(X)\n    assert_array_equal(pipeline.transform(X), X)\n    assert_raises(AttributeError, getattr, pipeline, 'predict')\n    assert_raises(AttributeError, getattr, pipeline, 'predict_proba')\n    assert_raises(AttributeError, getattr, pipeline, 'predict_log_proba')\n    assert_raises(AttributeError, getattr, pipeline, 'decision_function')\n    assert_raises(AttributeError, getattr, pipeline, 'score')\n    assert_raises(AttributeError, getattr, pipeline, 'classes_')\n", "def test_pipeline_fit_params_error():\n    # Test that an error is raised when fit_params are passed to the wrong step\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    assert_raises_regex(ValueError,\n                        \"Pipeline.fit does not accept the wrong_step__param parameter. \"\n                        \"You can pass parameters to specific steps of your pipeline \"\n                        \"using the stepname__parameter format, e.g. \"\n                        \"`Pipeline.fit(X, y, logisticregression__sample_weight=\"\n                        \"sample_weight)`.\",\n                        pipe.fit, X, None, wrong_step__param=True)\n", "def test_pipeline_set_params_error():\n    # Test pipeline raises error message for invalid parameters\n    pipe = Pipeline([('cls', LinearRegression())])\n\n    # expected error message\n    error_msg = \"Invalid parameter cls__fake for estimator Pipeline.\"\n\n    assert_raise_message(ValueError,\n                         error_msg,\n                         pipe.set_params,\n                         cls__fake='nope')\n\n    # invalid step name\n    assert_raise_message(ValueError,\n                         \"Invalid parameter invalid__estimator for estimator Pipeline.\",\n                         pipe.set_params,\n                         invalid__estimator='nope')\n"], "sample_433": ["def test_rename_field_and_index_together_with_db_column(self):\n    \"\"\"Fields are renamed before updating index_together with db_column.\"\"\"\n    book_index_together_3_db_column = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"newfield\", models.IntegerField(db_column=\"new_field\")),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"title\", \"newfield\")},\n        },\n    )\n    book_index_together_4_db_column = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"newfield2\", models.IntegerField(db_column=\"new_field_2\")),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"title\", \"newfield2\")},\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, book_index_together_3_db_column],\n        [AutodetectorTests.author_empty, book_index_together_4_db_column],\n        MigrationQuestioner({\"ask_rename\": True}),\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"RenameField\", \"AlterIndexTogether\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        name=\"book\",\n        index_together={(\"title\", \"newfield2\")},\n    )\n", "    def test_add_index(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddIndex(\n                    model_name=\"Person\",\n                    index=models.Index(fields=[\"name\"], name=\"person_name_idx\"),\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_name_idx\")\n", "def test_remove_field_with_related_model(self):\n    related_model = ModelState(\n        \"testapp\",\n        \"RelatedModel\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        ],\n    )\n    author_with_related_field = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"related_field\", models.CharField(max_length=200)),\n        ],\n    )\n    author_without_related_field = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n    )\n    changes = self.get_changes(\n        [related_model, author_with_related_field],\n        [related_model, author_without_related_field],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"RemoveField\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        model_name=\"author\",\n        name=\"related_field\",\n    )\n", "def test_alter_index_together_dependency(self):\n    \"\"\"\n    #27186 - AlterIndexTogether correctly depends on other apps' models.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_name, self.book_index_together],\n        [self.author_with_book_index_together, self.book_index_together],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterIndexTogether\"])\n    self.assertMigrationDependencies(\n        changes, \"testapp\", 0, [(\"otherapp\", \"__first__\")]\n    )\n", "def test_alter_unique_together_fk_to_m2m_2(self):\n    author_with_unique_id = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"unique_together\": {(\"id\", \"name\")},\n        },\n    )\n    book_with_author_m2m = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ManyToManyField(\"testapp.Author\")),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n    )\n    changes = self.get_changes(\n        [self.author_name, self.book_unique_together],\n        [self.author_name, book_with_author_m2m],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"AlterUniqueTogether\", \"RemoveField\", \"AddField\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        unique_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        model_name=\"book\",\n        name=\"author\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        2,\n        model_name=\"book\",\n        name=\"author\",\n    )\n"], "sample_83": ["    def setUp(self):\n        self.library = Library()\n", "    def setUp(self):\n        self.library = Library()\n", "    def setUp(self):\n        self.library = Library()\n        self.context = {'key': 'value'}\n\n        @self.library.simple_tag\n            return {'arg1': arg1, 'arg2': arg2, 'context': context, 'kwarg1': kwarg1}\n\n        self.tag_helper_node = self.library.tags['func'](None, \"func arg1 arg2 kwarg1='custom'\")\n", "    def setUp(self):\n        self.library = Library()\n", "    def setUp(self):\n        self.library = Library()\n"], "sample_43": ["def test_non_one_dimensional_input():\n    rng = np.random.RandomState(0)\n    t = rng.rand(10, 10)\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='events')\n", "def test_fitness_function_exceptions():\n    \"\"\"Test for exceptions raised by fitness functions\"\"\"\n    t = np.random.rand(100)\n\n    # Unknown fitness function\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='unknown')\n\n    # ncp_prior cannot be computed if neither gamma nor p0 is defined\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness=FitnessFunc())\n", "def test_events_fitness_with_sigma():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n    x = np.ones_like(t)\n    sigma = 0.5 * np.ones_like(t)\n\n    # sigma should not affect the result for events fitness\n    bins1 = bayesian_blocks(t, x, sigma, fitness='events')\n    bins2 = bayesian_blocks(t, x, fitness='events')\n\n    assert_allclose(bins1, bins2)\n", "def test_events_fitness_with_counts():\n    t = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    x = np.array([1, 2, 0, 1, 0, 2, 1, 0, 1, 2])\n\n    bins = bayesian_blocks(t, x, fitness='events')\n\n    # The expected bins based on the known behavior of the events fitness function\n    expected_bins = [1, 3, 6, 10]\n\n    assert_allclose(bins, expected_bins)\n", "def test_fitness_function_errors():\n    \"\"\"Test errors for several fitness functions\"\"\"\n    rng = np.random.RandomState(42)\n    t = rng.rand(100)\n\n    # Invalid fitness function\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='invalid')\n\n    # Gamma and p0 not specified for measures\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='measures', x=t, sigma=0.1, gamma=None, p0=None)\n\n    # p0, gamma, and ncp_prior all specified for measures\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='measures', x=t, sigma=0.1, gamma=0.5, p0=0.01, ncp_prior=4)\n\n    # Multidimensional t input\n    with pytest.raises(ValueError):\n        bayesian_blocks(t.reshape(10, 10), fitness='events')\n\n    # x and sigma specified without x\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, sigma=0.1, fitness='events')\n\n    # x with non-integer values for events\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, x=t, fitness='events')\n\n    # x with non-binary values for regular_events\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, x=t, fitness='regular_events', dt=0.01)\n\n    # sigma with shape not matching x for measures\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, x=t, sigma=t[:-1], fitness='measures')\n"], "sample_861": ["def test_grid_search_bad_cv():\n    # Use global X, y\n\n    class BrokenKFold(KFold):\n            return 1\n\n    # create bad cv\n    cv = BrokenKFold(n_splits=3)\n\n    train_size = 100\n    grid = GridSearchCV(\n        SVC(random_state=0), param_grid={'C': [10]}, cv=cv\n    )\n\n    # assert that this raises an error\n    with pytest.raises(ValueError,\n                       match='cv.split and cv.get_n_splits returned '\n                             'inconsistent results. Expected \\\\d+ '\n                             'splits, got \\\\d+'):\n        grid.fit(X[:train_size], y[:train_size])\n", "def test_grid_search_no_fit():\n    # Test that GridSearchCV raises an error when fit is not called\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]})\n    with pytest.raises(ValueError, match='This GridSearchCV instance was initialized with'):\n        grid_search.predict(X)\n", "def test_grid_search_with_pre_dispatch():\n    # Test that GridSearchCV can handle pre_dispatch values\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    clf = SVC(random_state=0)\n    param_grid = {'C': [0.1, 1, 10]}\n\n    # pre_dispatch = '2*n_jobs'\n    grid_search = GridSearchCV(clf, param_grid, cv=3, pre_dispatch='2*n_jobs')\n    grid_search.fit(X, y)\n\n    # pre_dispatch = an integer\n    grid_search = GridSearchCV(clf, param_grid, cv=3, pre_dispatch=2)\n    grid_search.fit(X, y)\n\n    # pre_dispatch = 'all'\n    grid_search = GridSearchCV(clf, param_grid, cv=3, pre_dispatch='all')\n    grid_search.fit(X, y)\n\n    # pre_dispatch = None\n    grid_search = GridSearchCV(clf, param_grid, cv=3, pre_dispatch=None)\n    grid_search.fit(X, y)\n", "def test_grid_search_score_method_with_callable_scoring():\n    # Test grid-search on classifier that has no score function\n    # and using a callable scoring function\n    X, y = make_classification(n_samples=100, n_classes=2, flip_y=.2,\n                               random_state=0)\n    clf = LinearSVC(random_state=0)\n    grid = {'C': [.1]}\n\n    # Define a custom scoring function\n        return f1_score(y_true, y_pred)\n\n    grid_search = GridSearchCV(clf, grid, scoring=custom_scorer)\n    grid_search.fit(X, y)\n\n    # Check that the best estimator contains the right value for C\n    assert grid_search.best_estimator_.C == .1\n\n    # Check that the score is equal to the custom_scorer result\n    assert_almost_equal(grid_search.score(X, y), custom_scorer(y, grid_search.predict(X)))\n", "def test_grid_search_with_custom_scoring():\n    # Test grid-search with a custom scoring function\n        return np.mean(y_true == y_pred)\n\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    grid_search = GridSearchCV(SVC(), param_grid=params, scoring=custom_scoring)\n    grid_search.fit(X, y)\n\n    # Test that the best estimator is found correctly\n    assert grid_search.best_params_ in [{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'},\n                                        {'degree': 2, 'kernel': 'poly'}]\n"], "sample_711": ["def test_node_parent_initialization() -> None:\n    parent_node = nodes.Node(\"parent_node\")\n    child_node = nodes.Node(\"child_node\", parent=parent_node)\n    assert child_node.parent == parent_node\n", "def test_fs_collector_from_parent_disallowed_arguments():\n    with pytest.raises(TypeError, match=\"config is\"):\n        nodes.FSCollector.from_parent(None, config=None)\n    with pytest.raises(TypeError, match=\"session is\"):\n        nodes.FSCollector.from_parent(None, session=None)\n", "def test_node_repr_failure_fulltrace(pytester: Pytester) -> None:\n    \"\"\"\n    Test that repr_failure() respects the fulltrace option.\n    \"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--fulltrace\")\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\", \"*def test_fulltrace()*\"])\n\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([str(p) + \":*: AssertionError\", \"*1 failed in *\"])\n    assert \"def test_fulltrace()\" not in result.stdout.str()\n", "def test_node_getparent() -> None:\n    class Parent(nodes.Node):\n        pass\n\n    class Child(nodes.Node):\n        pass\n\n    parent = Parent(name=\"parent\")\n    child = Child(name=\"child\", parent=parent)\n\n    assert child.getparent(Parent) == parent\n    assert child.getparent(Child) == child\n    assert child.getparent(nodes.Node) == child\n    assert child.getparent(str) is None\n", "def test_node_location_changed_cwd_with_absolute_path(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import os\n        import pytest\n\n        @pytest.fixture\n            out_dir = 'ddd'\n            os.mkdir(out_dir)\n            old_dir = os.getcwd()\n            os.chdir(out_dir)\n            yield out_dir\n            os.chdir(old_dir)\n\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([str(p.absolute()) + \":*: AssertionError\", \"*1 failed in *\"])\n"], "sample_658": ["def test_doctest_continue_on_failure_pdb(self, testdir):\n    testdir.maketxtfile(\n        test_something=\"\"\"\n        >>> i = 5\n        >>> def foo():\n        ...     raise ValueError('error1')\n        >>> foo()\n        >>> i\n        >>> i + 2\n        7\n        >>> i + 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\", \"--pdb\")\n    result.assert_outcomes(failed=1)\n    # The lines that contains the failure are 4, 5, and 8.  The first one\n    # is a stack trace and the other two are mismatches.\n    result.stdout.fnmatch_lines(\n        [\"*4: UnexpectedException*\", \"*5: DocTestFailure*\", \"*8: DocTestFailure*\"]\n    )\n", "def test_doctest_continue_on_failure(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n            '''\n            >>> foo()\n            'foo'\n            >>> bar()\n            'bar'\n            '''\n            return 'foo'\n\n            raise ValueError('error')\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\")\n    result.assert_outcomes(passed=1, failed=1)\n    # The lines that contains the failure is 8.\n    result.stdout.fnmatch_lines(\n        [\"*8: UnexpectedException*\"]\n    )\n", "def test_doctest_report_choices_in_config(self, testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        doctest_report = ndiff\n        \"\"\"\n    )\n    result = self._run_doctest_report(testdir, \"none\")\n    result.stdout.fnmatch_lines(\n        [\n            \"         a  b\",\n            \"      0  1  4\",\n            \"    - 1  2  4\",\n            \"    ?       ^\",\n            \"    + 1  2  5\",\n            \"    ?       ^\",\n            \"      2  3  6\",\n        ]\n    )\n", "def test_doctest_fixture_in_class_scope(self, testdir):\n    \"\"\"Test that fixtures with class scope work properly with doctest modules.\"\"\"\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"class\")\n            request.cls.class_scope_value = 99\n    \"\"\"\n    )\n    testdir.makepyfile(\n        test_1='''\n        class TestClass:\n                \"\"\"\n                >>> self.class_scope_value + 1\n                100\n                \"\"\"\n        '''\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"*=== 1 passed in *\"])\n", "def test_doctest_unexpected_exception_with_pdb(self, testdir):\n    testdir.maketxtfile(\n        \"\"\"\n        >>> i = 0\n        >>> 0 / i\n        2\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--pdb\")\n    # This is an assertion to check if the debugger was not started\n    assert \"Pdb\" not in result.stdout.str()\n"], "sample_663": ["def test_collector_respects_tbstyle_short(testdir):\n    p1 = testdir.makepyfile(\"assert 0\")\n    result = testdir.runpytest(p1, \"--tb=short\")\n    assert result.ret == ExitCode.INTERRUPTED\n    result.stdout.fnmatch_lines(\n        [\n            \"*_ ERROR collecting test_collector_respects_tbstyle_short.py _*\",\n            '  File \"*/test_collector_respects_tbstyle_short.py\", line 1, in <module>',\n            \"    assert 0\",\n            \"AssertionError: assert 0\",\n            \"*! Interrupted: 1 error during collection !*\",\n            \"*= 1 error in *\",\n        ]\n    )\n", "def test_collect_pyargs_with_multiple_testpaths(testdir, monkeypatch):\n    testmod1 = testdir.mkdir(\"testmod1\")\n    testmod1.ensure(\"test_file1.py\").write(\"def test_func1(): pass\")\n\n    testmod2 = testdir.mkdir(\"testmod2\")\n    testmod2.ensure(\"test_file2.py\").write(\"def test_func2(): pass\")\n\n    root = testdir.mkdir(\"root\")\n    root.ensure(\"pytest.ini\").write(\n        textwrap.dedent(\n            \"\"\"\n        [pytest]\n        addopts = --pyargs\n        testpaths = testmod1 testmod2\n    \"\"\"\n        )\n    )\n    monkeypatch.setenv(\"PYTHONPATH\", str(testdir.tmpdir), prepend=os.pathsep)\n    with root.as_cwd():\n        result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*2 passed in*\"])\n", "def test_collect_pkg_init_only_with_tbstyle(testdir):\n    subdir = testdir.mkdir(\"sub\")\n    init = subdir.ensure(\"__init__.py\")\n    init.write(\"assert 0\")\n\n    result = testdir.runpytest(\"-v\", \"-o\", \"python_files=*.py\", str(init), \"--tb=native\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*_ ERROR collecting __init__.py _*\",\n            \"Traceback (most recent call last):\",\n            '  File \"*/__init__.py\", line 1, in <module>',\n            \"    assert 0\",\n            \"AssertionError: assert 0\",\n            \"*1 error in *\",\n        ]\n    )\n", "def test_collector_respects_tbstyle_with_long_traceback(testdir):\n    p1 = testdir.makepyfile(\"\\n\".join([\"def func():\" + \"    assert 0\" * 100]))\n    result = testdir.runpytest(p1, \"--tb=native\")\n    assert result.ret == ExitCode.INTERRUPTED\n    result.stdout.fnmatch_lines(\n        [\n            \"*_ ERROR collecting test_collector_respects_tbstyle_with_long_traceback.py _*\",\n            \"Traceback (most recent call last):\",\n            '  File \"*/test_collector_respects_tbstyle_with_long_traceback.py\", line 2, in <module>',\n            \"    def func():\",\n            '  File \"*/test_collector_respects_tbstyle_with_long_traceback.py\", line 3, in func',\n            \"    assert 0\",\n            \"AssertionError: assert 0\",\n            \"*...*\",\n            \"AssertionError: assert 0\",\n            \"*! Interrupted: 1 error during collection !*\",\n            \"*= 1 error in *\",\n        ]\n    )\n", "def test_collect_init_tests_with_no_python_files_config(testdir):\n    \"\"\"Check that we do not collect files from __init__.py files when no 'python_files' config is specified (#3773)\"\"\"\n    p = testdir.copy_example(\"collect/collect_init_tests\")\n    result = testdir.runpytest(p, \"--collect-only\", \"-o\", \"python_files=\")\n    result.stdout.fnmatch_lines(\n        [\n            \"collected 0 items\"\n        ]\n    )\n    result = testdir.runpytest(\"./tests\", \"--collect-only\", \"-o\", \"python_files=\")\n    result.stdout.fnmatch_lines(\n        [\n            \"collected 0 items\"\n        ]\n    )\n    result = testdir.runpytest(\"./tests/test_foo.py\", \"--collect-only\", \"-o\", \"python_files=\")\n    result.stdout.fnmatch_lines(\n        []\n    )\n    result = testdir.runpytest(\"./tests/__init__.py\", \"--collect-only\", \"-o\", \"python_files=\")\n    result.stdout.fnmatch_lines(\n        []\n    )\n"], "sample_1198": ["def test_parser_mathematica_trig_functions():\n    parser = MathematicaParser()\n\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    assert chain(\"Sin[x]\") == [\"Sin\", \"x\"]\n    assert chain(\"Cos[x]\") == [\"Cos\", \"x\"]\n    assert chain(\"Tan[x]\") == [\"Tan\", \"x\"]\n    assert chain(\"Cot[x]\") == [\"Cot\", \"x\"]\n    assert chain(\"Sec[x]\") == [\"Sec\", \"x\"]\n    assert chain(\"Csc[x]\") == [\"Csc\", \"x\"]\n    assert chain(\"ArcSin[x]\") == [\"ArcSin\", \"x\"]\n    assert chain(\"ArcCos[x]\") == [\"ArcCos\", \"x\"]\n    assert chain(\"ArcTan[x]\") == [\"ArcTan\", \"x\"]\n    assert chain(\"ArcCot[x]\") == [\"ArcCot\", \"x\"]\n    assert chain(\"ArcSec[x]\") == [\"ArcSec\", \"x\"]\n    assert chain(\"ArcCsc[x]\") == [\"ArcCsc\", \"x\"]\n    assert chain(\"Sinh[x]\") == [\"Sinh\", \"x\"]\n    assert chain(\"Cosh[x]\") == [\"Cosh\", \"x\"]\n    assert chain(\"Tanh[x]\") == [\"Tanh\", \"x\"]\n    assert chain(\"Coth[x]\") == [\"Coth\", \"x\"]\n    assert chain(\"Sech[x]\") == [\"Sech\", \"x\"]\n    assert chain(\"Csch[x]\") == [\"Csch\", \"x\"]\n    assert chain(\"ArcSinh[x]\") == [\"ArcSinh\", \"x\"]\n    assert chain(\"ArcCosh[x]\") == [\"ArcCosh\", \"x\"]\n    assert chain(\"ArcTanh[x]\") == [\"ArcTanh\", \"x\"]\n    assert chain(\"ArcCoth[x]\") == [\"ArcCoth\", \"x\"]\n    assert chain(\"ArcSech[x", "def test_parser_mathematica_other_functions():\n    parser = MathematicaParser()\n\n    convert = lambda expr: parser.parse(expr)\n\n    assert convert('ArcCosh[x]') == acosh(x)\n    assert convert('ArcTanh[x]') == atanh(x)\n    assert convert('ArcCoth[x]') == acoth(x)\n    assert convert('ArcSech[x]') == asech(x)\n    assert convert('ArcCsch[x]') == acsch(x)\n    assert convert('Expand[x]') == expand(x)\n    assert convert('Im[x]') == im(x)\n    assert convert('Re[x]') == sympy.re(x)\n    assert convert('Flatten[x]') == flatten(x)\n    assert convert('Polylog[2, x]') == polylog(2, x)\n    assert convert('Cancel[x]') == cancel(x)\n    assert convert('TrigExpand[x]') == expand_trig(x)\n    assert convert('Sign[x]') == sign(x)\n    assert convert('Simplify[x]') == simplify(x)\n    assert convert('Defer[x]') == UnevaluatedExpr(x)\n    assert convert('Identity[x]') == x\n    assert convert('Null') == S.Zero\n    assert convert('Pochhammer[x, y]') == rf(x, y)\n    assert convert('LogIntegral[x]') == li(x)\n    assert convert('PrimePi[x]') == primepi(x)\n    assert convert('Prime[x]') == prime(x)\n    assert convert('PrimeQ[x]') == isprime(x)\n    assert convert('List[1, 2, 3]') == (1, 2, 3)\n    assert convert('Greater[x, y]') == x > y\n    assert convert('GreaterEqual[x, y]') == x >= y\n    assert convert('Less[x, y]') == x < y\n    assert convert('LessEqual[x, y]')", "def test_parser_mathematica_special_functions():\n    parser = MathematicaParser()\n\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    assert chain(\"ArcTan[x, y]\") == [\"ArcTan\", \"x\", \"y\"]\n    assert chain(\"Pochhammer[x, y]\") == [\"Pochhammer\", \"x\", \"y\"]\n    assert chain(\"ExpIntegralEi[x]\") == [\"ExpIntegralEi\", \"x\"]\n    assert chain(\"SinIntegral[x]\") == [\"SinIntegral\", \"x\"]\n    assert chain(\"CosIntegral[x]\") == [\"CosIntegral\", \"x\"]\n    assert chain(\"AiryAi[x]\") == [\"AiryAi\", \"x\"]\n    assert chain(\"AiryAiPrime[5]\") == [\"AiryAiPrime\", \"5\"]\n    assert chain(\"AiryBi[x]\") == [\"AiryBi\", \"x\"]\n    assert chain(\"AiryBiPrime[7]\") == [\"AiryBiPrime\", \"7\"]\n    assert chain(\"LogIntegral[4]\") == [\"LogIntegral\", \"4\"]\n    assert chain(\"PrimePi[7]\") == [\"PrimePi\", \"7\"]\n    assert chain(\"Prime[5]\") == [\"Prime\", \"5\"]\n    assert chain(\"PrimeQ[5]\") == [\"PrimeQ\", \"5\"]\n", "def test_parser_mathematica_unsupported_functions():\n    parser = MathematicaParser()\n\n    # Test cases for unsupported functions\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"UnsupportedFunction[x]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"InvalidFunction[x, y, z]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"FunctionWithMissingBracket[x\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"FunctionWithMissingArgument]\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"FunctionWithMismatchedBrackets[x])\"))\n    raises(ValueError, lambda: parser._from_mathematica_to_tokens(\"FunctionWithInvalidArguments[x, , y]\"))\n", "def test_parser_mathematica_divide_by():\n    parser = MathematicaParser()\n\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # DivideBy operator\n    assert chain(\"a /= b\") == [\"DivideBy\", \"a\", \"b\"]\n    assert chain(\"a /= b /= c\") == [\"DivideBy\", \"a\", [\"DivideBy\", \"b\", \"c\"]]\n    assert chain(\"a /= b /= c /= d\") == [\"DivideBy\", \"a\", [\"DivideBy\", \"b\", [\"DivideBy\", \"c\", \"d\"]]]\n\n    # Left priority operator:\n    assert chain(\"a /= b\") == [\"DivideBy\", \"a\", \"b\"]\n    assert chain(\"a /= b /= c\") == [\"DivideBy\", [\"DivideBy\", \"a\", \"b\"], \"c\"]\n    assert chain(\"a /= b /= c /= d\") == [\"DivideBy\", [\"DivideBy\", [\"DivideBy\", \"a\", \"b\"], \"c\"], \"d\"]\n"], "sample_1017": ["def test_BooleanFunction_diff_multivariate():\n    assert And(x, y).diff(z) == S.Zero\n", "def test_BooleanFunction_eval_derivative():\n    assert And(x, y).diff(x).doit() == Piecewise((0, Eq(y, 0)), (1, True))\n", "def test_BooleanFunction_diff_with_relational():\n    assert And(x > 0, y).diff(x) == Piecewise((0, Eq(0, y)), (1, True))\n", "def test_BooleanFunction_to_nnf():\n    assert And(x, y).to_nnf() == And(x, y)\n    assert Or(x, y).to_nnf() == Or(x, y)\n    assert Not(And(x, y)).to_nnf() == Or(Not(x), Not(y))\n    assert Not(Or(x, y)).to_nnf() == And(Not(x), Not(y))\n    assert Not(Not(x)).to_nnf() == x\n    assert Implies(x, y).to_nnf() == Or(Not(x), y)\n    assert Equivalent(x, y).to_nnf() == And(Or(Not(x), y), Or(x, Not(y)))\n    assert Xor(x, y).to_nnf() == Or(And(x, Not(y)), And(Not(x), y))\n    assert ITE(x, y, z).to_nnf() == And(Or(Not(x), y), Or(x, z))\n", "def test_boolean_function_evaluate():\n    assert Not(True) == S.false\n    assert Not(False) == S.true\n    assert Not(And(True, True)) == S.false\n    assert Not(And(True, False)) == S.true\n    assert Or(True, True) == S.true\n    assert Or(True, False) == S.true\n    assert Or(False, False) == S.false\n    assert And(True, True) == S.true\n    assert And(True, False) == S.false\n    assert And(False, False) == S.false\n    assert Xor(True, True) == S.false\n    assert Xor(True, False) == S.true\n    assert Xor(False, False) == S.false\n    assert Implies(True, True) == S.true\n    assert Implies(True, False) == S.false\n    assert Implies(False, True) == S.true\n    assert Implies(False, False) == S.true\n    assert Equivalent(True, True) == S.true\n    assert Equivalent(True, False) == S.false\n    assert Equivalent(False, False) == S.true\n"], "sample_1015": ["def test_ccode_AddAugmentedAssignment():\n    assert ccode(aug_assign(x, '+', y)) == 'x += y;'\n    assert ccode(aug_assign(x, '-', y)) == 'x -= y;'\n    assert ccode(aug_assign(x, '*', y)) == 'x *= y;'\n    assert ccode(aug_assign(x, '/', y)) == 'x /= y;'\n    assert ccode(aug_assign(x, '%', y)) == 'x %= y;'\n    assert ccode(aug_assign(x, '&', y)) == 'x &= y;'\n    assert ccode(aug_assign(x, '|', y)) == 'x |= y;'\n    assert ccode(aug_assign(x, '^', y)) == 'x ^= y;'\n    assert ccode(aug_assign(x, '<<', y)) == 'x <<= y;'\n    assert ccode(aug_assign(x, '>>', y)) == 'x >>= y;'\n", "def test_ccode_aug_assign():\n    assert ccode(aug_assign(x, '+', y)) == 'x += y;'\n    assert ccode(aug_assign(x, '-', y)) == 'x -= y;'\n    assert ccode(aug_assign(x, '*', y)) == 'x *= y;'\n    assert ccode(aug_assign(x, '/', y)) == 'x /= y;'\n    assert ccode(aug_assign(x, '%', y)) == 'x %= y;'\n    assert ccode(aug_assign(x, '&', y)) == 'x &= y;'\n    assert ccode(aug_assign(x, '|', y)) == 'x |= y;'\n    assert ccode(aug_assign(x, '^', y)) == 'x ^= y;'\n    assert ccode(aug_assign(x, '<<', y)) == 'x <<= y;'\n    assert ccode(aug_assign(x, '>>', y)) == 'x >>= y;'\n", "def test_ccode_complex_numbers():\n    z = symbols('z', complex=True)\n    assert ccode(z.real) == \"creal(z)\"\n    assert ccode(z.imag) == \"cimag(z)\"\n", "def test_ccode_complex_numbers():\n    i = symbols('i', complex=True)\n    assert ccode(i) == 'I'\n    assert ccode(2 + 3*i) == '2 + 3*I'\n    assert ccode(2 + 3*i, type_aliases={complex_: complex64}) == '2 + 3*I'\n    assert ccode(2 + 3*i, type_aliases={complex_: complex128}) == '2 + 3*I'\n    z = symbols('z', complex=True)\n    assert ccode(z + 2 + 3*i) == 'z + 2 + 3*I'\n    assert ccode(z + 2 + 3*i, type_aliases={complex_: complex64}) == 'z + 2 + 3*I'\n    assert ccode(z + 2 + 3*i, type_aliases={complex_: complex128}) == 'z + 2 + 3*I'\n    assert ccode(z*i) == 'z*I'\n    assert ccode(z*i, type_aliases={complex_: complex64}) == 'z*I'\n    assert ccode(z*i, type_aliases={complex_: complex128}) == 'z*I'\n    assert ccode(z*I) == 'z*I'\n    assert ccode(z*I, type_aliases={complex_: complex64}) == 'z*I'\n    assert ccode(z*I, type_aliases={complex_: complex128}) == 'z*I'\n    w = symbols('w', complex=True)\n    assert ccode(w*z) == 'w*z'\n    assert ccode(w*z, type_aliases={complex_: complex64}) == 'w*z'\n    assert ccode(w*z, type_aliases={complex_: complex128}) == 'w*z'\n    assert ccode(w/z) == 'w/z'\n    assert ccode(w/z, type_aliases={", "def test_ccode_float_constants():\n    assert ccode(Float(0.5, precision=24)) == '0.5'\n    assert ccode(Float(0.5, precision=53)) == '0.5'\n    assert ccode(Float(0.5, precision=64)) == '0.5L'\n    assert ccode(Float(0.5, precision=80)) == '0.5Q'\n    assert ccode(Float(0.5, precision=113)) == '0.5Q'\n    assert ccode(Float(0.5, precision=128)) == '0.5Q'\n"], "sample_1149": ["def test_singleton_registration():\n    # Test that the SingletonRegistry correctly registers and retrieves singleton instances\n    class TestSingleton1(Basic, metaclass=Singleton):\n        pass\n\n    class TestSingleton2(Basic, metaclass=Singleton):\n        pass\n\n    assert hasattr(S, 'TestSingleton1')\n    assert hasattr(S, 'TestSingleton2')\n    assert S.TestSingleton1 is TestSingleton1()\n    assert S.TestSingleton2 is TestSingleton2()\n    assert S.TestSingleton1 is not S.TestSingleton2\n", "def test_singleton_registry():\n    # Test the registration and retrieval of a class in the SingletonRegistry\n    class TestClass(Basic, metaclass=Singleton):\n        pass\n\n    assert TestClass() is S.TestClass\n    assert 'TestClass' not in dir(S)  # TestClass should not be installed yet\n    assert S.TestClass is S.TestClass  # Accessing it twice should return the same instance\n    assert 'TestClass' in dir(S)  # TestClass should be installed now\n", "def test_singleton_registration_order():\n    # Test that the singletons are registered in the order they are defined\n    # This is to ensure that a singleton that is redefined later will overwrite the previous one\n\n    class FirstSingleton(Basic, metaclass=Singleton):\n        pass\n\n    class SecondSingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert S.FirstSingleton is FirstSingleton()\n    assert S.SecondSingleton is SecondSingleton()\n\n    class FirstSingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert S.FirstSingleton is FirstSingleton()\n    assert S.SecondSingleton is SecondSingleton()\n    assert S.FirstSingleton is not SecondSingleton()\n", "def test_singleton_access_before_instantiation():\n    # Test that accessing a singleton attribute before instantiation still\n    # returns the correct singleton instance.\n\n    # Define a new singleton class\n    class NewSingleton(Basic, metaclass=Singleton):\n        pass\n\n    # Access the attribute before instantiation\n    singleton_instance = S.NewSingleton\n\n    # Force instantiation\n    NewSingleton()\n\n    # Check that the attribute still returns the same instance\n    assert singleton_instance is S.NewSingleton\n", "def test_singleton_reuse():\n    # Test that the singleton instance is reused after being garbage collected\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert TestSingleton() is S.TestSingleton\n\n    obj = S.TestSingleton\n    del TestSingleton\n    del S.TestSingleton\n    import gc\n    gc.collect()\n    assert S.TestSingleton is obj\n"], "sample_323": ["def test_minimize_rollbacks_branchy_forward(self):\n    r\"\"\"\n    Minimize rollbacks when target has multiple in-app children in forward direction.\n\n    a: 1 -> 2 -> 3\n    b:    \\-> 1 -> 2\n    c:         \\-> 1\n\n    If a1, b1, and c1 are applied already and a2 and b2 are not, and we're asked to migrate to\n    a3, don't apply or unapply c1, regardless of its current state.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    a3_impl = FakeMigration('a3')\n    a3 = ('a', '3')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    b2_impl = FakeMigration('b2')\n    b2 = ('b', '2')\n    c1_impl = FakeMigration('c1')\n    c1 = ('c', '1')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(a3, a3_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(b2, b2_impl)\n    graph.add_node(c1, c1_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, a3, a2)\n    graph.add_dependency(None, b1, a1)\n    graph.add_dependency(None, b2, b1)\n    graph.add_dependency(None, c1, b1)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n       ", "def test_minimize_rollbacks_across_apps(self):\n    r\"\"\"\n    Minimize rollbacks across apps.\n\n    a: 1 <--- 2\n    b: 1 <--- 2\n\n    If a1 and b1 are applied, and we're asked to migrate to a1, don't apply\n    or unapply b2, regardless of its current state.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    b2_impl = FakeMigration('b2')\n    b2 = ('b', '2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_node(b2, b2_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, b2, b1)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        b1: b1_impl,\n    })\n\n    plan = executor.migration_plan({a1})\n\n    self.assertEqual(plan, [])\n", "    def test_minimize_rollbacks_branchy_multiple_targets(self):\n        r\"\"\"\n        Minimize rollbacks when target has multiple in-app children and multiple targets.\n\n        a: 1 <---- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n        b:      \\- 1 <--- 2\n        c:      - 1 <--- 2\n\n        If we're asked to migrate to a1 and c1, don't apply or unapply b2,\n        regardless of its current state.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a1)\n", "def test_alter_fk_with_id_type(self):\n    try:\n        executor = MigrationExecutor(connection)\n        self.assertTableNotExists(\"author_app_author\")\n        self.assertTableNotExists(\"book_app_book\")\n        # Apply initial migrations\n        executor.migrate([\n            (\"author_app\", \"0001_initial\"),\n            (\"book_app\", \"0001_initial\"),\n        ])\n        self.assertTableExists(\"author_app_author\")\n        self.assertTableExists(\"book_app_book\")\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n\n        # Apply FK alteration\n        executor.migrate([(\"book_app\", \"0002_alter_fk\")])\n\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n    finally:\n        # We can't simply unapply the migrations here because there is no\n        # implicit cast from VARCHAR to INT on the database level.\n        with connection.schema_editor() as editor:\n            editor.execute(editor.sql_delete_table % {\"table\": \"book_app_book\"})\n            editor.execute(editor.sql_delete_table % {\"table\": \"author_app_author\"})\n        self.assertTableNotExists(\"author_app_author\")\n        self.assertTableNotExists(\"book_app_book\")\n        executor.migrate([(\"author_app\", None)], fake=True)\n        executor.migrate([(\"book_app\", None)], fake=True)\n", "def test_unapply_migration(self):\n    \"\"\"\n    Test that unapply_migration() properly unapplies a migration.\n\n    This test checks that the unapply_migration() method correctly unapplies a\n    migration, rolling back any changes it made to the database schema.\n    \"\"\"\n    class FakeSchemaEditor:\n            return self\n\n            pass\n\n            # Simulate the unapply operation\n            pass\n\n    class FakeMigration(migrations.Migration):\n        operations = [\n            migrations.CreateModel('TestModel', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n            ]),\n        ]\n\n    executor = MigrationExecutor(connection)\n    state = ProjectState()\n    migration = FakeMigration('0001_initial', 'test_app')\n\n    # Apply the migration\n    state = executor.apply_migration(state, migration)\n    self.assertTableExists('test_app_testmodel')\n\n    # Unapply the migration\n    executor.unapply_migration(state, migration)\n    self.assertTableNotExists('test_app_testmodel')\n"], "sample_766": ["def test_dict_learning_unknown_transform_algorithm():\n    n_components = 5\n    dico = DictionaryLearning(n_components, transform_algorithm='<unknown>')\n    assert_raises(ValueError, dico.fit, X)\n", "def test_sparse_encode_lars_positive_parameter():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    err_msg = \"Positive constraint not supported for 'lars' coding method.\"\n    with pytest.raises(ValueError, match=err_msg):\n        sparse_encode(X, V, algorithm=\"lars\", positive=True)\n", "def test_dict_learning_online_readonly_data():\n    n_components = 8\n    with TempMemmap(X) as X_read_only:\n        dico = MiniBatchDictionaryLearning(n_components, n_iter=1, random_state=0)\n        dico.fit(X_read_only)\n        code = dico.transform(X_read_only)\n        assert_array_almost_equal(np.dot(code, dico.components_), X_read_only, decimal=2)\n", "def test_sparse_encode_shapes_1d(algorithm):\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    X_1d = X[0]\n    code = sparse_encode(X_1d, V, algorithm=algorithm)\n    assert_equal(code.shape, (n_components,))\n", "def test_sparse_encode_shapes_parallel():\n    rng = np.random.RandomState(0)\n    algorithms = ['omp', 'lasso_lars', 'lasso_cd', 'lars', 'threshold']\n    for n_components, n_samples in itertools.product([1, 5], [1, 9]):\n        X_ = rng.randn(n_samples, n_features)\n        dictionary = rng.randn(n_components, n_features)\n        for algorithm in algorithms:\n            code = sparse_encode(X_, dictionary, algorithm=algorithm, n_jobs=2)\n            assert_equal(code.shape, (n_samples, n_components))\n"], "sample_886": ["def test_set_output_method_chaining():\n    \"\"\"Check that the set_output method can be chained.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X)\n\n    est.set_output(transform=\"pandas\").set_output(transform=\"default\")\n\n    X_trans = est.transform(X)\n    assert isinstance(X_trans, np.ndarray)\n", "def test_set_output_with_sparse_data():\n    \"\"\"Check that _wrap_in_pandas_container raises error for sparse data.\"\"\"\n    X_csr = csr_matrix([[1, 0, 3], [0, 0, 1]])\n\n    with pytest.raises(ValueError, match=\"Pandas output does not support sparse data\"):\n        _wrap_in_pandas_container(X_csr, columns=[\"a\", \"b\", \"c\"])\n", "def test__wrap_in_pandas_container_no_columns():\n    \"\"\"Check _wrap_in_pandas_container when columns is None.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n\n    dense_no_columns = _wrap_in_pandas_container(X, columns=None)\n    assert isinstance(dense_no_columns, pd.DataFrame)\n    assert_array_equal(dense_no_columns.columns, range(X.shape[1]))\n", "def test_set_output_mixin_inheritance():\n    \"\"\"Check that _SetOutputMixin is correctly inherited.\"\"\"\n\n    class ParentEstimator(_SetOutputMixin):\n            return X\n\n            return input_features\n\n    class ChildEstimator(ParentEstimator):\n        pass\n\n    est = ChildEstimator()\n    assert hasattr(est, \"set_output\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est.set_output(transform=\"pandas\")\n    X_trans_pd = est.transform(X)\n    assert isinstance(X_trans_pd, pd.DataFrame)\n", "def test_set_output_mixin_transform_method_not_defined():\n    \"\"\"Check that the transform method is not wrapped if it's not defined.\"\"\"\n\n    class EstimatorWithSetOutputNoTransform(_SetOutputMixin):\n            return self\n\n    est = EstimatorWithSetOutputNoTransform()\n    assert not hasattr(est, \"transform\")\n\n    # The estimator is not wrapped because the transform method is not defined\n    assert not hasattr(est, \"_sklearn_auto_wrap_output_keys\")\n"], "sample_557": ["def test_set_constrained_layout_pads():\n    fig = plt.figure(layout='constrained')\n    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, wspace=0.03, hspace=0.04)\n    layout_engine = fig.get_layout_engine()\n    assert layout_engine._params['w_pad'] == 0.01\n    assert layout_engine._params['h_pad'] == 0.02\n    assert layout_engine._params['wspace'] == 0.03\n    assert layout_engine._params['hspace'] == 0.04\n", "def test_tight_layout_pad():\n    fig, axs = plt.subplots(2, 2)\n    fig.tight_layout(pad=2.0)\n    assert fig.get_layout_engine().pad == 2.0\n", "def test_get_constrained_layout_pads_relative():\n    params = {'w_pad': 0.01, 'h_pad': 0.02, 'wspace': 0.03, 'hspace': 0.04}\n    expected = tuple([*params.values()])\n    fig = plt.figure(layout=mpl.layout_engine.ConstrainedLayoutEngine(**params))\n    fig.draw_without_rendering()\n    with pytest.warns(PendingDeprecationWarning, match=\"will be deprecated\"):\n        pads = fig.get_constrained_layout_pads(relative=True)\n    assert all(0 <= pad <= 1 for pad in pads)\n", "def test_add_subplot_label():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, label='test')\n    assert fig.axes[0].get_label() == 'test'\n", "def test_set_layout_engine():\n    fig = Figure()\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    fig.set_layout_engine('constrained', w_pad=0.05, h_pad=0.05)\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    assert fig.get_layout_engine().w_pad == 0.05\n    assert fig.get_layout_engine().h_pad == 0.05\n"], "sample_1146": ["def test_latex_array_expressions():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    assert latex(A) == \"A\"\n\n    B = ArrayElement(\"A\", (2, 1/(1-x), 0))\n    assert latex(B) == \"{{A}_{2, \\\\frac{1}{1 - x}, 0}}\"\n", "def test_printing_latex_array_expressions_with_symbols():\n    x = symbols(\"x\")\n    assert latex(ArraySymbol(\"A\", x, 3, 4)) == \"A\"\n    assert latex(ArrayElement(\"A\", (x, 1/(1-x), 0))) == \"{{A}_{x, \\\\frac{1}{1 - x}, 0}}\"\n", "def test_latex_IndexedBase():\n    A = IndexedBase('A')\n    assert latex(A[2]) == r'A_{2}'\n    assert latex(A[2, 3]) == r'A_{2, 3}'\n    assert latex(A[2, 3, 4]) == r'A_{2, 3, 4}'\n    assert latex(A[2, 3, 4, 5]) == r'A_{2, 3, 4, 5}'\n\n    B = IndexedBase('B', shape=(2, 3))\n    assert latex(B[1, 2]) == r'B_{1, 2}'\n\n    C = IndexedBase('C', shape=(2, 3, 4))\n    assert latex(C[0, 1, 2]) == r'C_{0, 1, 2}'\n", "def test_printing_latex_array_expressions_with_symbols():\n    x, y, z = symbols('x y z')\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    assert latex(ArrayElement(\"A\", (x, y, z))) == \"{{A}_{x, y, z}}\"\n", "def test_printing_latex_array_slicing():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    assert latex(A[0:1, 1:2, :]) == \"{{A}_{0:1, 1:2, :}}\"\n    assert latex(A[0, 1:2, 2:3]) == \"{{A}_{0, 1:2, 2:3}}\"\n"], "sample_358": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            opclasses=['varchar_pattern_ops', 'text_pattern_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            opclasses=['text_ops', 'varchar_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'],\n            lambda column: column.upper(),\n            ['ASC', 'DESC'],\n            ['opclass1', 'opclass2'],\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            opclasses=['varchar_ops', 'int_ops']\n        )\n", "    def test_rename_column_references_without_alias(self):\n        compiler = Query(Person, alias_cols=False).get_compiler(connection=connection)\n        table = Person._meta.db_table\n        expressions = Expressions(\n            table=table,\n            expressions=ExpressionList(\n                IndexExpression(Upper('last_name')),\n                IndexExpression(F('first_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        expressions.rename_column_references(table, 'first_name', 'other')\n        self.assertIs(expressions.references_column(table, 'other'), True)\n        self.assertIs(expressions.references_column(table, 'first_name'), False)\n        expected_str = '(UPPER(%s)), %s' % (\n            self.editor.quote_name('last_name'),\n            self.editor.quote_name('other'),\n        )\n        self.assertEqual(str(expressions), expected_str)\n"], "sample_69": ["def test_watch_dir_with_absolute(self):\n    self.reloader.watch_dir(self.tempdir, '*.py')\n    self.assertIn(self.tempdir, self.reloader.directory_globs)\n", "def test_watch_dir_without_absolute(self):\n    with self.assertRaisesMessage(ValueError, 'test_dir must be absolute.'):\n        self.reloader.watch_dir('test_dir', '*.py')\n", "def test_file_deleted(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.existing_file)\n    self.existing_file.unlink()\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "def test_snapshot_files_with_subdirectories(self):\n    subdir_file = self.ensure_file(self.tempdir / 'subdir' / 'test.py')\n    with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, subdir_file]):\n        snapshot1 = dict(self.reloader.snapshot_files())\n        self.assertIn(self.existing_file, snapshot1)\n        self.assertIn(subdir_file, snapshot1)\n        self.increment_mtime(subdir_file)\n        snapshot2 = dict(self.reloader.snapshot_files())\n        self.assertNotEqual(snapshot1[subdir_file], snapshot2[subdir_file])\n        self.assertEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n", "def test_iter_all_python_module_files_called(self, mocked_modules, notify_mock):\n    mocked_modules.return_value = [self.existing_file]\n    with self.tick_twice():\n        self.increment_mtime(self.existing_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertEqual(mocked_modules.call_count, 2)\n"], "sample_515": ["def test_colorbar_set_label():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    cb.set_label('Colorbar label')\n    assert cb.ax.get_ylabel() == 'Colorbar label'\n", "def test_colorbar_ticklabels():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    cb.set_ticklabels(['Zero', 'One', 'Two', 'Three'])\n    fig.canvas.draw()\n    assert cb.ax.yaxis.get_ticklabels()[0].get_text() == 'Zero'\n    assert cb.ax.yaxis.get_ticklabels()[1].get_text() == 'One'\n    assert cb.ax.yaxis.get_ticklabels()[2].get_text() == 'Two'\n    assert cb.ax.yaxis.get_ticklabels()[3].get_text() == 'Three'\n", "def test_colorbar_set_ticks_update_ticks():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc, ax=ax)\n    cb.set_ticks([20, 40, 60], update_ticks=True)\n    assert cb.get_ticks().tolist() == [20, 40, 60]\n    cb.set_ticks([10, 30, 50], update_ticks=False)\n    assert cb.get_ticks().tolist() == [20, 40, 60]\n", "def test_colorbar_add_lines():\n    fig, ax = plt.subplots()\n    CS = ax.contour(np.random.randn(10, 10), levels=[0.5, 1.5], colors='k')\n    cb = fig.colorbar(CS)\n    levels = [0.2, 0.6, 1.0]\n    colors = ['r', 'g', 'b']\n    linewidths = [1, 2, 3]\n    cb.add_lines(levels, colors, linewidths)\n    assert len(cb.lines) == 1\n    assert isinstance(cb.lines[0], mpl.collections.LineCollection)\n", "def test_colorbar_with_ticklabels():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cbar = fig.colorbar(im, ticks=[0, 1, 2, 3], orientation='vertical')\n    cbar.ax.set_yticklabels(['Low', 'Medium', 'High', 'Extreme'])\n"], "sample_510": ["def test_close_all_figures():\n    plt.figure()\n    plt.figure()\n    plt.close('all')\n    assert plt.get_fignums() == []\n", "def test_subplot_mosaic():\n    fig, axd = plt.subplot_mosaic({'A': [1, 2], 'B': [3, 4]})\n    assert axd['A'].get_subplotspec().get_topmost_subplotspec() == fig.add_subplot(121).get_subplotspec().get_topmost_subplotspec()\n    assert axd['B'].get_subplotspec().get_topmost_subplotspec() == fig.add_subplot(122).get_subplotspec().get_topmost_subplotspec()\n", "def test_imread_imsave():\n    x = np.arange(100).reshape((10, 10))\n    fname = 'test.png'\n    plt.imsave(fname, x)\n    x1 = plt.imread(fname)\n    np.testing.assert_array_equal(x, x1)\n", "def test_subplot_mosaic():\n    fig, ax_dict = plt.subplot_mosaic({'a': ['b', 'c']})\n    assert isinstance(ax_dict['a'], np.ndarray)\n    assert isinstance(ax_dict['b'], mpl.axes.SubplotBase)\n    assert isinstance(ax_dict['c'], mpl.axes.SubplotBase)\n    assert ax_dict['b'].get_gridspec() == ax_dict['c'].get_gridspec()\n    assert ax_dict['a'][0] == ax_dict['b']\n    assert ax_dict['a'][1] == ax_dict['c']\n", "def test_clf():\n    # create a figure and axes\n    fig, ax = plt.subplots()\n\n    # check that the figure is initially empty\n    assert len(fig.axes) == 1\n\n    # draw something on the axes\n    ax.plot([1, 2, 3], [4, 5, 6])\n\n    # check that the figure is no longer empty\n    assert len(fig.axes) > 0\n\n    # clear the figure\n    plt.clf()\n\n    # check that the figure is now empty\n    assert len(fig.axes) == 0\n"], "sample_341": ["def test_formset_with_custom_error_messages(self):\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '',\n    }\n    error_messages = {'missing_management_form': 'Custom missing management form error.'}\n    ChoiceFormSet = formset_factory(Choice, error_messages=error_messages)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.error_messages['missing_management_form'], 'Custom missing management form error.')\n", "def test_formset_custom_prefix(self):\n    \"\"\"Custom prefix is used for the management form.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=1)\n    formset = ChoiceFormSet(prefix='custom_prefix')\n    self.assertEqual(formset.management_form.prefix, 'custom_prefix')\n", "def test_formset_validation_with_required_files(self):\n    \"\"\"Test formset validation with required files.\"\"\"\n    class FileForm(Form):\n        file = FileField(required=True)\n\n    data = {\n        'file-TOTAL_FORMS': '1',\n        'file-INITIAL_FORMS': '0',\n        'file-MIN_NUM_FORMS': '0',\n        'file-MAX_NUM_FORMS': '1',\n        'file-0-file': '',  # Empty file input\n    }\n    files = {}  # Empty files dictionary\n    FileFormSet = formset_factory(FileForm)\n    formset = FileFormSet(data, files, prefix='file')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.errors, [{'file': ['This field is required.']}])\n", "def test_custom_error_messages(self):\n    \"\"\"Custom error messages are used in the non_form_errors() method.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    ChoiceFormSet = formset_factory(Choice, error_messages={'missing_management_form': 'Custom missing management form error'})\n    formset = ChoiceFormSet(data=data, prefix='choices')\n    formset.management_form = None  # simulate missing management form\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Custom missing management form error'],\n    )\n", "def test_custom_error_list_rendering(self):\n    class CustomErrorList(ErrorList):\n            return '<div class=\"custom-errorlist\">%s</div>' % ''.join([str(e) for e in self])\n\n    formset = FavoriteDrinksFormSet(error_class=CustomErrorList)\n    self.assertEqual(str(formset.forms[0].errors), '<div class=\"custom-errorlist\"></div>')\n"], "sample_223": ["def test_ticket_24605_exclude(self):\n    \"\"\"\n    Subquery table names should be quoted with exclude.\n    \"\"\"\n    i1 = Individual.objects.create(alive=True)\n    RelatedIndividual.objects.create(related=i1)\n    i2 = Individual.objects.create(alive=False)\n    RelatedIndividual.objects.create(related=i2)\n    i3 = Individual.objects.create(alive=True)\n    i4 = Individual.objects.create(alive=False)\n\n    self.assertSequenceEqual(\n        Individual.objects.exclude(Q(alive=False), Q(related_individual__isnull=True)).order_by('pk'),\n        [i1, i2, i3]\n    )\n", "def test_exclude_with_circular_fk_relation_reverse(self):\n    qs = ObjectA.objects.filter(objectb__objecta__name=F('name'))\n    self.assertEqual(qs.count(), 0)\n", "    def test_ticket_24605_with_double_negation(self):\n        \"\"\"\n        Make sure __pk__in and __in work the same for related fields when\n        using a double negation subquery.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        qx = (\n            ~Q(ticket23605b__pk__in=Ticket23605B.objects.filter(field_b0=567)) &\n            Q(ticket23605b__field_b0__gte=100)\n        )\n        qy = (\n            ~Q(ticket23605b__in=Ticket23605B.objects.filter(field_b0=567)) &\n            Q(ticket23605b__field_b0__gte=100)\n        )\n        self.assertEqual(\n            set(Ticket23605A.objects.filter(qx).values_list('pk', flat=True)),\n            set(Ticket23605A.objects.filter(qy).values_list('pk', flat=True))\n        )\n        self.assertSequenceEqual(Ticket23605A.objects.filter(qx), [a1])\n", "    def test_ticket_24886(self):\n        \"\"\"\n        Make sure the ORed condition doesn't trim the join if the ORed filter\n        uses a relation that is inside the join.\n        \"\"\"\n        i1 = Individual.objects.create()\n        ri1 = RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create()\n        RelatedIndividual.objects.create(related=i2)\n        qs = Individual.objects.filter(Q(pk=i1.pk) | Q(related_individual__related=i1))\n        self.assertEqual(str(qs.query).count('INNER JOIN'), 1)\n        self.assertSequenceEqual(qs, [i1])\n", "    def test_ticket_23978(self):\n        \"\"\"\n        #23978 - Make sure that a combined query can be built using a mixture\n        of related object filters.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=100.0)\n        Ticket23605B.objects.create(\n            field_b0=100.0, field_b1=True,\n            modelc_fk=c1, modela_fk=a1)\n        Ticket23605B.objects.create(\n            field_b0=200.0, field_b1=False,\n            modelc_fk=c1, modela_fk=a2)\n        complex_q = Q(ticket23605b__field_b0__gte=F('ticket23605b__modelc_fk__field_c0')) & Q(ticket23605b__field_b1=True)\n        qs = Ticket23605A.objects.filter(complex_q)\n        self.assertSequenceEqual(qs, [a1])\n"], "sample_1162": ["def test_MatrixSymbol_kind():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.kind is MatrixKind(NumberKind)\n", "def test_Function_kind():\n    f = Function('f')\n    g = Function('g', commutative=False)\n    assert f(comm_x).kind is NumberKind\n    assert g(comm_x).kind is UndefinedKind\n", "def test_Derivative_noncommutative_kind():\n    assert Derivative(noncomm_x, noncomm_x).kind is UndefinedKind\n", "def test_MatMul_commutative():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatMul(A, B).is_commutative is False\n    assert MatMul(comm_x, B).is_commutative is True\n", "def test_Matrix_kind_with_symbol():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.kind is MatrixKind(UndefinedKind)\n"], "sample_146": ["def test_consistent_language_settings_with_valid_code(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings_with_valid_code(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_valid_script_consistent_language_settings(self):\n        tests = [\n            # language + script.\n            'sr-Latn',\n            'sr-Cyrl',\n        ]\n        for tag in tests:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_languages_bidi_subset_of_languages(self):\n        msg = (\n            'You have provided an invalid language code in the LANGUAGES_BIDI setting: '\n            '%r. It is not present in the LANGUAGES setting.'\n        )\n        valid_languages = [('en', 'English'), ('ar', 'Arabic')]\n        invalid_languages_bidi = ['fr', 'ja']\n        with self.settings(LANGUAGES=valid_languages, LANGUAGES_BIDI=invalid_languages_bidi):\n            errors = check_setting_languages_bidi(None)\n            self.assertEqual(errors, [\n                Error(msg % tag, id='translation.E003') for tag in invalid_languages_bidi\n            ])\n", "def test_consistent_language_settings_with_variants(self):\n    for tag in ['de-AT', 'de-ch', 'de-de']:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_291": ["    def test_setup_adds_attributes(self):\n        request = self.rf.get('/')\n        args = ('arg1', 'arg2')\n        kwargs = {'key1': 'value1', 'key2': 'value2'}\n\n        view = View()\n        view.setup(request, *args, **kwargs)\n\n        self.assertEqual(view.request, request)\n        self.assertEqual(view.args, args)\n        self.assertEqual(view.kwargs, kwargs)\n", "def test_redirect_view_with_query_string(self):\n    response = RedirectView.as_view(url='/bar/', query_string=True)(self.rf.get('/foo/?param=value'))\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, '/bar/?param=value')\n", "def test_setup_args_kwargs_request(self):\n    \"\"\"\n    Test that setup() properly assigns args, kwargs, and request to the instance.\n    \"\"\"\n    request = self.rf.get('/')\n    args = ('arg1', 'arg2')\n    kwargs = {'key1': 'value1', 'key2': 'value2'}\n\n    view = View()\n    view.setup(request, *args, **kwargs)\n\n    self.assertEqual(view.request, request)\n    self.assertEqual(view.args, args)\n    self.assertEqual(view.kwargs, kwargs)\n", "    def test_setup_method_calls(self):\n        \"\"\"\n        Test that setup() is called correctly and sets attributes on the view instance.\n        \"\"\"\n        request = self.rf.get('/')\n        args = ('arg1', 'arg2')\n        kwargs = {'kwarg1': 'value1', 'kwarg2': 'value2'}\n\n        view = View()\n        view.setup(request, *args, **kwargs)\n\n        self.assertEqual(view.request, request)\n        self.assertEqual(view.args, args)\n        self.assertEqual(view.kwargs, kwargs)\n", "    def test_direct_instantiation_with_args(self):\n        \"\"\"\n        It should be possible to use the view by directly instantiating it\n        with args and kwargs without going through .as_view() (#21564).\n        \"\"\"\n        view = SimpleView()\n        request = self.rf.get('/')\n        args = ('arg1', 'arg2')\n        kwargs = {'kwarg1': 1, 'kwarg2': 'test'}\n        view.setup(request, *args, **kwargs)\n        response = view.dispatch(request)\n        self.assertEqual(response.status_code, 200)\n"], "sample_754": ["def test_sparse_pca_error_shape(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng, normalize_components=norm_comp)\n    spca.fit(X)\n    assert_equal(spca.error_.shape, (spca.max_iter,))\n", "def test_spca_transform_without_fitting(norm_comp):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    spca = SparsePCA(n_components=3, normalize_components=norm_comp)\n\n    with pytest.raises(ValueError, match=\"This SparsePCA instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\"):\n        spca.transform(Y)\n", "def test_sparse_pca_transform_normalize(norm_comp):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array\n    spca = SparsePCA(n_components=3, normalize_components=norm_comp, random_state=rng)\n    spca.fit(Y)\n    X_transformed_norm = spca.transform(Y)\n\n    Y_centered = Y - Y.mean(axis=0)\n    spca_centered = SparsePCA(n_components=3, normalize_components=False, random_state=rng)\n    spca_centered.fit(Y_centered)\n    X_transformed_centered = spca_centered.transform(Y_centered)\n\n    assert_array_almost_equal(X_transformed_norm, X_transformed_centered)\n", "def test_sparse_pca_with_different_methods(norm_comp):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    spca_lars = SparsePCA(n_components=3, method='lars', random_state=0, normalize_components=norm_comp)\n    spca_cd = SparsePCA(n_components=3, method='cd', random_state=0, normalize_components=norm_comp)\n    spca_lars.fit(Y)\n    spca_cd.fit(Y)\n    assert_array_almost_equal(spca_lars.components_, spca_cd.components_, decimal=2)\n", "def test_sparse_pca_components_orthogonal(norm_comp):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    spca = SparsePCA(n_components=3, random_state=rng, normalize_components=norm_comp)\n    spca.fit(Y)\n    components = spca.components_\n    # Check that components are orthogonal to each other\n    assert_allclose(components.T.dot(components), np.eye(3), atol=1e-5)\n"], "sample_29": ["def test_write_latex_invalid_format(self, write):\n    \"\"\"Test passing an invalid format\"\"\"\n    invalid_format = \"csv\"\n    with pytest.raises(ValueError, match=\"format must be 'latex', not\"):\n        write(None, format=invalid_format)\n", "def test_write_latex_with_kwargs(self, write, tmp_path):\n    \"\"\"Test passing additional kwargs to table.write\"\"\"\n    fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n    write(fp, format=\"latex\", latex_dollar_align=True)\n    # Assert that the file was written with the correct LaTeX formatting\n", "def test_write_latex_custom_format(self, write, tmp_path):\n    \"\"\"Test passing a custom format to write_latex\"\"\"\n    fp = tmp_path / \"test_write_latex_custom_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex', not custom\"):\n        write(fp, format=\"custom\")\n", "def test_write_latex_custom_kwargs(self, write, tmp_path):\n    \"\"\"Test passing custom kwargs to table.write\"\"\"\n    fp = tmp_path / \"test_write_latex_custom_kwargs.tex\"\n    write(fp, format=\"latex\", latex_names=True, latexdict={\"preamble\": \"\\\\documentclass{article}\"})\n    with open(fp, 'r') as file:\n        data = file.read().replace('\\n', '')\n        assert \"\\\\documentclass{article}\" in data\n", "def test_write_latex_valid_format(self, write, tmp_path):\n    \"\"\"Test to write a LaTeX file with a valid format\"\"\"\n    fp = tmp_path / \"test_write_latex_valid_format.tex\"\n    write(fp, format=\"latex\")\n    assert fp.exists()\n    assert fp.is_file()\n    # Check if the file contains LaTeX format\n    with open(fp, 'r') as f:\n        content = f.read()\n        assert \"\\\\documentclass\" in content\n        assert \"\\\\begin{document}\" in content\n        assert \"\\\\end{document}\" in content\n"], "sample_799": ["def test_fit_and_score_return_estimator():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n\n    # test return_estimator option\n    fit_and_score_args = [clf, X, y, dict(), train, test, 0]\n    fit_and_score_kwargs = {'return_estimator': True}\n    result = _fit_and_score(*fit_and_score_args, **fit_and_score_kwargs)\n    assert isinstance(result[-1], SVC)\n", "def test_cross_val_predict_allow_nans():\n    # Check that cross_val_predict allows input data with NaNs\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    p = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean', missing_values=np.nan)),\n        ('classifier', MockClassifier()),\n    ])\n    cross_val_predict(p, X, y, cv=5)\n", "def test_fit_and_score_with_predict():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n    fit_and_score_args = [clf, X, y, dict(), train, test, 0]\n    fit_and_score_kwargs = {'parameters': {'max_iter': 100, 'tol': 0.1},\n                            'fit_params': None,\n                            'return_train_score': True,\n                            'return_estimator': True}\n    _, _, _, _, est = _fit_and_score(*fit_and_score_args, **fit_and_score_kwargs)\n    assert hasattr(est, 'predict')\n", "def test_score_custom_callable():\n        return ((y_true == y_pred).sum() / len(y_true))\n\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1, 2, 3, 3, 5])\n    assert_almost_equal(_score(None, y_true, y_pred, custom_scorer), 0.8)\n\n    y_true = np.array([0, 1, 0, 1, 0])\n    y_pred = np.array([0, 1, 1, 1, 0])\n    assert_almost_equal(_score(None, y_true, y_pred, custom_scorer), 0.6)\n", "def test_learning_curve_no_cv():\n    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,\n                               n_redundant=0, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    estimator = MockImprovingEstimator(20)\n    with warnings.catch_warnings(record=True) as w:\n        train_sizes, train_scores, test_scores = learning_curve(\n            estimator, X, y, cv='warn', train_sizes=np.linspace(0.1, 1.0, 10))\n    if len(w) > 0:\n        raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n    assert_array_equal(train_sizes, np.linspace(2, 20, 10))\n    assert_array_almost_equal(train_scores.mean(axis=1),\n                              np.linspace(1.9, 1.0, 10))\n    assert_array_almost_equal(test_scores.mean(axis=1),\n                              np.linspace(0.1, 1.0, 10))\n"], "sample_114": ["def test_mti_inheritance_model_addition(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal], [Animal, Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "def test_add_non_blank_textfield_and_charfield_with_default(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Adding a NOT NULL and non-blank `CharField` or `TextField`\n    with a default should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_biography_non_blank_default])\n    self.assertEqual(mocked_ask_method.call_count, 0)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "    def test_mti_inheritance_model_creation(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal], [Animal, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "def test_mti_inheritance_field_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"species\", models.CharField(max_length=50)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='species', model_name='animal')\n", "def test_add_non_blank_fk(self, mocked_ask_method):\n    \"\"\"\n    Adding a NOT NULL and non-blank `ForeignKey` without default should prompt for a default.\n    \"\"\"\n    Publisher = ModelState('app', 'Publisher', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Book = ModelState('app', 'Book', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"publisher\", models.ForeignKey('app.Publisher', models.CASCADE)),\n    ])\n    changes = self.get_changes([Book], [Book, Publisher])\n    self.assertEqual(mocked_ask_method.call_count, 1)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Publisher')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='publisher', model_name='book')\n"], "sample_804": ["def test_one_hot_encoder_drop_auto():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 2, 55],\n         ['def', 1, 55],\n         ['def', 3, 59]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [1, 1, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 1, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_sparse_with_drop():\n    X = [[3, 2, 1], [0, 1, 1]]\n    enc = OneHotEncoder(sparse=True, drop='first')\n    X_trans = enc.fit_transform(X)\n    assert_equal(X_trans.shape, (2, 4))\n    assert_array_equal(X_trans.toarray(), [[0., 1., 1.], [1., 0., 1.]])\n", "def test_one_hot_encoder_drop_first_category():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 0],\n           [1, 0, 1],\n           [1, 0, 0]]\n    assert_array_equal(trans, exp)\n    assert_array_equal(enc.drop_idx_, 0)\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_valid_categories(drop):\n    enc = OneHotEncoder(drop=drop)\n    X = [['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]]\n    trans = enc.fit_transform(X).toarray()\n    if len(drop) == 3:\n        exp = [[1, 1, 0], [0, 0, 1], [0, 0, 0]]\n        assert_array_equal(trans, exp)\n        dropped_cats = [cat[feature]\n                        for cat, feature in zip(enc.categories_,\n                                                enc.drop_idx_)]\n        assert_array_equal(dropped_cats, drop)\n        assert_array_equal(np.array(X, dtype=object),\n                           enc.inverse_transform(trans))\n    else:\n        assert_raises_regex(\n            ValueError,\n            \"The following categories were supposed\",\n            enc.fit, X)\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n"], "sample_509": ["def test_concise_formatter_usetex_newline():\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    formatter = mdates.ConciseDateFormatter(locator, usetex=True)\n    year_1996 = 9861.0\n    strings = formatter.format_ticks([year_1996])\n    assert strings == [r'{\\fontfamily{\\familydefault}\\selectfont 1996}']\n", "def test_ConciseDateFormatter_usetex_newline():\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    formatter = mdates.ConciseDateFormatter(locator, usetex=True)\n    year_1996 = 9861.0\n    strings = formatter.format_ticks([\n        year_1996,\n        year_1996 + 500 / mdates.MUSECONDS_PER_DAY,\n        year_1996 + 900 / mdates.MUSECONDS_PER_DAY])\n    expected = [\n        r'{\\fontfamily{\\familydefault}\\selectfont 01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 00.0005}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 00.0009}'\n    ]\n    assert strings == expected\n", "def test_microsecond_locator_out_of_range():\n    locator = mdates.MicrosecondLocator(interval=1)\n    with pytest.raises(ValueError, match=\"Data has no positive values\"):\n        locator.tick_values(0, 10)\n\n    with pytest.raises(ValueError, match=\"Data has no negative values\"):\n        locator.tick_values(-10, 0)\n", "def test_microsecondlocator():\n    locator = mdates.MicrosecondLocator()\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(0, 1)\n    locator.axis.set_data_interval(0, 1)\n    ticks = locator.tick_values(0, 1)\n    assert np.allclose(ticks, np.arange(0, 1, 1/mdates.MUSECONDS_PER_DAY))\n\n    locator = mdates.MicrosecondLocator(interval=2)\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(0, 1)\n    locator.axis.set_data_interval(0, 1)\n    ticks = locator.tick_values(0, 1)\n    assert np.allclose(ticks, np.arange(0, 1, 2/mdates.MUSECONDS_PER_DAY))\n\n    with pytest.raises(ValueError):\n        mdates.MicrosecondLocator(interval=-1)\n\n    with pytest.raises(ValueError):\n        mdates.MicrosecondLocator(interval=0)\n\n    with pytest.raises(ValueError):\n        mdates.MicrosecondLocator(interval=1.5)\n", "def test_concise_formatter_usetex_newline():\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(mdates.date2num(np.datetime64('2022-01-01')), mdates.date2num(np.datetime64('2022-01-10')))\n\n    formatter = mdates.ConciseDateFormatter(locator, usetex=True)\n    expected = [\n        r'{\\fontfamily{\\familydefault}\\selectfont 01/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 02/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 03/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 04/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 05/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 06/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 07/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 08/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 09/01}',\n        r'{\\fontfamily{\\familydefault}\\selectfont 10/01}'\n    ]\n    assert formatter.format_ticks(locator()) == expected\n"], "sample_872": ["def test_label_ranking_loss_pos_label_types():\n    \"\"\"Check that label_ranking_loss works with different types of `pos_label`.\n\n    We can expect `pos_label` to be a bool, an integer, a float, a string.\n    No error should be raised for those types.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    n_samples, pos_label = 10, \"one\"\n    y_true = rng.choice([False, True], size=n_samples, replace=True)\n    y_score = rng.rand(n_samples)\n    result = label_ranking_loss(y_true, y_score, pos_label=pos_label)\n    assert not np.isnan(result)\n", "def test_top_k_accuracy_score_error_labels_unordered(y_true, y_score, labels, msg):\n    with pytest.raises(ValueError, match=msg):\n        top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n", "def test_label_ranking_avg_precision_score_with_sample_weight():\n    # Test that label_ranking_avg_precision_score works with sample_weight.\n    # Non-regression test for #22575\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n    sample_weight = np.array([1, 2])\n    result = label_ranking_average_precision_score(y_true, y_score, sample_weight=sample_weight)\n    assert result == pytest.approx((2 / 3 + 0) / 2)\n", "def test_top_k_accuracy_score_error_sample_size(y_true, y_score, labels, msg):\n    with pytest.raises(ValueError, match=msg):\n        top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n", "def test_ranking_metric_consistency(metric, classes):\n    \"\"\"Check that the metric produces consistent results with integer and boolean labels.\n\n    The metric should produce the same results when given integer labels (0, 1) as it does when given boolean labels\n    (False, True).\n    \"\"\"\n    rng = np.random.RandomState(42)\n    n_samples = 10\n    y_true_bool = rng.choice(classes, size=n_samples, replace=True)\n    y_true_int = y_true_bool.astype(int)\n    y_proba = rng.rand(n_samples)\n    result_bool = metric(y_true_bool, y_proba)\n    result_int = metric(y_true_int, y_proba)\n    if isinstance(result_bool, float):\n        assert result_bool == result_int\n    else:\n        metric_1_bool, metric_2_bool, thresholds_bool = result_bool\n        metric_1_int, metric_2_int, thresholds_int = result_int\n        assert_allclose(metric_1_bool, metric_1_int)\n        assert_allclose(metric_2_bool, metric_2_int)\n        assert_allclose(thresholds_bool, thresholds_int)\n"], "sample_1079": ["def test_project():\n    a = Point(1, 2)\n    b = Point(2, 5)\n    z = a.origin\n    p = Point.project(a, b)\n    assert Line(p, a).is_perpendicular(Line(p, b))\n    assert Point.is_collinear(z, p, b)\n\n    # Test projecting to the zero vector\n    raises(ValueError, lambda: Point.project(a, z))\n", "def test_is_nonzero():\n    assert Point(0, 0).is_nonzero is False\n    assert Point(1, 0).is_nonzero is True\n    assert Point(0, 1).is_nonzero is True\n    assert Point(1, 1).is_nonzero is True\n\n    x = Symbol('x', real=True)\n    p = Point(x, 1)\n    assert p.is_nonzero is None\n\n    p = Point(0, x)\n    assert p.is_nonzero is None\n\n    p = Point(x, x)\n    assert p.is_nonzero is None\n", "def test_point_normalize_dimension():\n    assert Point._normalize_dimension(Point(1, 2), Point(3, 4, 5), dim=3) == [\n        Point(1, 2, 0), Point(3, 4, 5)]\n    assert Point._normalize_dimension(Point(1, 2), Point(3, 4, 5), dim=4) == [\n        Point(1, 2, 0, 0), Point(3, 4, 5, 0)]\n    raises(ValueError, lambda: Point._normalize_dimension(Point(1, 2), Point(3, 4, 5), dim=2, on_morph='error'))\n    raises(ValueError, lambda: Point._normalize_dimension(Point(1, 2, 3), Point(4, 5), dim=3, on_morph='error'))\n    raises(ValueError, lambda: Point._normalize_dimension(Point(1, 2, 3), Point(4, 5), dim=2, on_morph='error'))\n", "def test_project():\n    p1 = Point(3, 4)\n    p2 = Point(2, 1)\n\n    assert Point.project(p1, p2) == Point(2.24, 1.76)\n\n    # Check zero vector\n    p3 = Point(0, 0)\n    with raises(ValueError):\n        Point.project(p1, p3)\n\n    # Check 3D points\n    p4 = Point3D(1, 2, 3)\n    p5 = Point3D(4, 5, 6)\n\n    assert Point3D.project(p4, p5) == Point3D(3.125, 3.75, 4.375)\n", "def test_canberra_distance():\n    p1 = Point(0, 0)\n    p2 = Point(3, 4)\n    p3 = Point(1, 2)\n    p4 = Point(2, 2)\n\n    assert p1.canberra_distance(p2) == 1\n    assert p2.canberra_distance(p1) == 1\n    assert p1.canberra_distance(p3) == 2\n    assert p1.canberra_distance(p4) == 1\n    assert p2.canberra_distance(p3) == 1\n    assert p3.canberra_distance(p4) == 0.5\n\n    # Testing canberra_distance with zero vectors\n    p5 = Point(0, 0)\n    p6 = Point(0, 0)\n    with raises(ValueError):\n        p5.canberra_distance(p6)\n"], "sample_1194": ["def test_julia_user_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_julia_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert julia_code(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n           'existing_julia_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_julia_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert julia_code(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        'existing_julia_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_HadamardPower():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    v = MatrixSymbol('v', 3, 1)\n    h = MatrixSymbol('h', 1, 3)\n    C = HadamardProduct(A, B)\n    assert julia_code(C**2) == \"(A .* B) .^ 2\"\n    assert julia_code(C**v) == \"(A .* B) .^ v\"\n    assert julia_code(h*C**v*v) == \"h * (A .* B) .^ v * v\"\n    assert julia_code(C**A) == \"(A .* B) .^ A\"\n    # mixing HadamardPower and scalar\n    assert julia_code(C**x*y) == \"((A .* B) .^ x .* y)\"\n", "def test_MatMul():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert julia_code(A @ B) == \"A * B\"\n    assert julia_code(A @ 2) == \"A * 2\"\n    assert julia_code(2 @ A) == \"2 * A\"\n    assert julia_code(A @ A @ B) == \"A * A * B\"\n    assert julia_code((A @ B) @ C) == \"A * B * C\"\n", "def test_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"custom_fcn_f\",\n        \"g\": [(lambda x: x.is_Matrix, \"custom_mat_fcn_g\"),\n              (lambda x: not x.is_Matrix, \"custom_fcn_g\")]\n    }\n    mat = Matrix([[1, x]])\n    assert julia_code(f(x) + g(x) + g(mat), user_functions=custom_functions) == \\\n        'custom_fcn_f(x) + custom_fcn_g(x) + custom_mat_fcn_g([1 x])'\n"], "sample_176": ["def test_add_model_with_field_added_to_base_model(self):\n    \"\"\"\n    Adding a base field takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AddField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "def test_add_non_blank_foreignkey(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Adding a NOT NULL and non-blank `ForeignKey` without default\n    should prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_book_non_blank])\n    self.assertEqual(mocked_ask_method.call_count, 1)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_create_model_with_db_index(self):\n    \"\"\"Test creation of new model with db_index already defined.\"\"\"\n    author = ModelState('otherapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200, db_index=True)),\n    ])\n    changes = self.get_changes([], [author])\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n    self.assertOperationFieldAttributes(changes, 'otherapp', 0, 0, db_index=True)\n", "def test_add_model_with_field_removed_from_base_model_multistep(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name. The base field is removed in a\n    separate migration to avoid violating the dependency constraint.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 2)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n    self.assertOperationTypes(changes, 'app', 1, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 1, 0, name='book')\n    self.assertMigrationDependencies(changes, 'app', 1, [('app', 'auto_1')])\n", "def test_add_model_with_field_removed_from_base_model_inheritance(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name. This test checks for the correct\n    order of operations when there's an inheritance chain.\n    \"\"\"\n    before = [\n        ModelState('app', 'base', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'derived', [], bases=('app.base',)),\n    ]\n    after = [\n        ModelState('app', 'base', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'derived', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.base',)),\n        ModelState('app', 'newmodel', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.derived',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='base')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='newmodel')\n    self.assertOperationAttributes(changes, 'app', 0, 2, name='derived')\n"], "sample_805": ["def test_multioutput_regression_errors():\n    y_true = np.array([[1, 2], [3, 4]])\n    y_pred = np.array([[2, 3], [4, 5]])\n\n    # Testing ValueError for multi-output metrics\n    metrics = [median_absolute_error, max_error]\n    for metric in metrics:\n        with pytest.raises(ValueError, match=\"Multioutput not supported\"):\n            metric(y_true, y_pred)\n", "def test_regression_multioutput_errors():\n    y_true = np.array([[1, 2], [2.5, -1], [4.5, 3], [5, 7]])\n    y_pred = np.array([[1, 1], [2, -1], [5, 4]])\n\n    # Check for errors when y_true and y_pred have different number of output\n    with pytest.raises(ValueError):\n        mean_squared_error(y_true, y_pred)\n\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred)\n\n    with pytest.raises(ValueError):\n        r2_score(y_true, y_pred)\n\n    with pytest.raises(ValueError):\n        explained_variance_score(y_true, y_pred)\n\n    # Check for error when multioutput is not valid\n    invalid_multioutput = 'invalid'\n    with pytest.raises(ValueError):\n        mean_squared_error(y_true, y_pred, multioutput=invalid_multioutput)\n", "def test_regression_metrics_with_sample_weights():\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.5, 2.2, 3.1, 3.9, 5.1])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.2, 0.2])\n\n    assert_almost_equal(mean_squared_error(y_true, y_pred, sample_weight=sample_weight), 0.054, decimal=2)\n    assert_almost_equal(mean_absolute_error(y_true, y_pred, sample_weight=sample_weight), 0.14, decimal=2)\n    assert_almost_equal(r2_score(y_true, y_pred, sample_weight=sample_weight), 0.96, decimal=2)\n    assert_almost_equal(explained_variance_score(y_true, y_pred, sample_weight=sample_weight), 0.98, decimal=2)\n", "def test_regression_metrics_with_sample_weight():\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.2, 2.3, 3.1, 4.2, 5.3])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.2, 0.2])\n\n    assert_almost_equal(mean_squared_error(y_true, y_pred, sample_weight=sample_weight), 0.016, decimal=2)\n    assert_almost_equal(mean_absolute_error(y_true, y_pred, sample_weight=sample_weight), 0.09, decimal=2)\n    assert_almost_equal(median_absolute_error(y_true, y_pred), 0.1, decimal=2)\n    assert_almost_equal(r2_score(y_true, y_pred, sample_weight=sample_weight), 0.993, decimal=2)\n    assert_almost_equal(explained_variance_score(y_true, y_pred, sample_weight=sample_weight), 0.995, decimal=2)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight, p=0), 0.016, decimal=2)\n", "def test_regression_metrics_with_sample_weight():\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.5, 2.2, 3.1, 4.0, 5.3])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.2, 0.2])\n\n    assert_almost_equal(mean_squared_error(y_true, y_pred, sample_weight=sample_weight), 0.022, 3)\n    assert_almost_equal(mean_absolute_error(y_true, y_pred, sample_weight=sample_weight), 0.08, 2)\n    assert_almost_equal(r2_score(y_true, y_pred, sample_weight=sample_weight), 0.990, 3)\n    assert_almost_equal(explained_variance_score(y_true, y_pred, sample_weight=sample_weight), 0.994, 3)\n"], "sample_885": ["def test_generate_valid_param_for_real_not_int_interval():\n    \"\"\"Check that the value generated for the 'real_not_int' interval type is not an integer.\"\"\"\n    constraint = Interval(\"real_not_int\", None, None, closed=\"both\")\n    value = generate_valid_param(constraint)\n    assert constraint.is_satisfied_by(value)\n    assert not isinstance(value, Integral)\n", "def test_interval_real_not_int_valid():\n    \"\"\"Check that the \"real_not_int\" type in the Interval constraint accepts valid values.\"\"\"\n    constraint = Interval(\"real_not_int\", 0, 1, closed=\"both\")\n    assert constraint.is_satisfied_by(0.5)\n    assert constraint.is_satisfied_by(1.0)\n", "def test_generate_invalid_param_val_real_not_int():\n    \"\"\"Check that the value generated for a \"real_not_int\" interval constraint\n    does not satisfy the constraint.\n    \"\"\"\n    real_not_int_interval = Interval(\"real_not_int\", None, None, closed=\"neither\")\n    bad_value = generate_invalid_param_val(real_not_int_interval)\n    assert not real_not_int_interval.is_satisfied_by(bad_value)\n", "def test_interval_type_real_not_int():\n    \"\"\"Check for the type \"real_not_int\" in the Interval constraint.\"\"\"\n    constraint = Interval(\"real_not_int\", None, None, closed=\"both\")\n    assert constraint.is_satisfied_by(1.0)\n    assert not constraint.is_satisfied_by(1)\n    assert not constraint.is_satisfied_by(\"a\")\n", "def test_boolean_constraint_invalid_value():\n    \"\"\"Check that validate_params raises an InvalidParameterError when the parameter\n    value is not a boolean-like.\n    \"\"\"\n\n    @validate_params({\"param\": [\"boolean\"]})\n        pass\n\n    # True/False and np.bool_(True/False) are valid params\n    f(True)\n    f(np.bool_(False))\n\n    # an int is also valid but deprecated\n    with pytest.warns(\n        FutureWarning, match=\"Passing an int for a boolean parameter is deprecated\"\n    ):\n        f(1)\n\n    # invalid values should raise an InvalidParameterError\n    with pytest.raises(InvalidParameterError, match=\"The 'param' parameter\"):\n        f(param=\"invalid\")\n"], "sample_168": ["def test_contenttypes_not_removed_if_user_cancels(self):\n    \"\"\"Stale content types aren't removed if the user cancels.\"\"\"\n    with mock.patch('builtins.input', return_value='no'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', verbosity=2)\n    self.assertNotIn('Deleting stale content type', stdout.getvalue())\n    self.assertIn('Stale content types remain.', stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "def test_contenttypes_removed_for_removed_apps(self):\n    ContentType.objects.create(app_label='contenttypes_tests', model='Fake')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n    self.assertIn(\n        \"Deleting stale content type 'contenttypes_tests | Fake'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_contenttypes_not_removed_for_unavailable_database(self):\n    \"\"\"A ContentType isn't deleted if the database isn't available.\"\"\"\n    with mock.patch('django.db.router.allow_migrate_model', return_value=False):\n        with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', verbosity=2)\n    self.assertNotIn(\"Deleting stale content type\", stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_contenttypes_not_removed_when_router_disallows_migration(self):\n        \"\"\"\n        ContentTypes are not removed when router disallows migration for the database.\n        \"\"\"\n        with mock.patch('django.db.router.BaseDatabaseOperations.allow_migrate_model', return_value=False):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n        self.assertEqual(stdout.getvalue(), '')\n", "    def test_noinput_argument(self):\n        \"\"\"--noinput argument deletes stale content types without prompting.\"\"\"\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', '--noinput', verbosity=2)\n        self.assertIn('Deleting stale content type', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n"], "sample_348": ["def test_autocomplete_is_manytomany(self):\n    class UserAdmin(ModelAdmin):\n        search_fields = ('name',)\n\n    class Admin(ModelAdmin):\n        autocomplete_fields = ('users',)\n\n    site = AdminSite()\n    site.register(User, UserAdmin)\n    self.assertIsValid(Admin, ValidationTestModel, admin_site=site)\n", "    def test_not_string(self):\n        class TestModelAdmin(ModelAdmin):\n            empty_value_display = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'empty_value_display' must be a string.\",\n            'admin.E120'\n        )\n", "    def test_actions_on_save_without_has_change_permission(self):\n        @admin.action(on_save=True)\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action_on_save,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"BandAdmin must define a has_change_permission() method for the \"\n            \"action_on_save action.\",\n            id='admin.E131',\n        )\n", "    def test_actions_are_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('not_a_function',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"All actions defined in BandAdmin must be callable.\",\n            id='admin.E131',\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E120'\n        )\n"], "sample_377": ["    def test_template_path(self):\n        reporter = ExceptionReporter(None, None, None, None)\n        self.assertTrue(reporter.html_template_path.exists())\n        self.assertTrue(reporter.text_template_path.exists())\n", "    def test_repr_exception(self):\n        class WrappedCallable:\n                raise Exception(\"Exception in repr\")\n\n                pass\n\n        actual = repr(CallableSettingWrapper(WrappedCallable()))\n        self.assertEqual(actual, \"<CallableSettingWrapper object at 0x...>\")\n", "    def test_multivalue_dict_with_sensitive_key(self):\n        \"\"\"\n        Sensitive POST parameters cannot be seen in the error reports for\n        requests even if they are present in a MultiValueDict.\n        \"\"\"\n        request = self.rf.post(\"/some_url/\", {\"sausage\": \"sausage-value\", \"eggs\": \"scrambled\"})\n        request.POST = request.POST.copy()\n        request.POST.sensitive_post_parameters = {\"sausage\"}\n\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_view, request)\n            self.verify_unsafe_email(sensitive_view, request)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_view, request)\n            self.verify_safe_email(sensitive_view, request)\n", "    def test_non_sensitive_request(self):\n        \"\"\"\n        Everything (request info and frame variables) can be seen in the\n        default error reports for non-sensitive requests.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(non_sensitive_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_unsafe_response(non_sensitive_view)\n", "    def test_encoding_error_in_source(self):\n        try:\n            raise ValueError(\"Test value error\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        with mock.patch(\n            \"django.views.debug.ExceptionReporter._get_source\",\n            return_value=[b\"\\xc3\\x28\"],\n        ):\n            html = reporter.get_traceback_html()\n            text = reporter.get_traceback_text()\n        self.assertIn(\"&lt;source code not available&gt;\", html)\n        self.assertIn(\"<source code not available>\", text)\n"], "sample_1043": ["def test_CustomFunction():\n        return x**2 + 2*x + 1\n\n    settings = {'user_functions': {'custom_func': 'CustomFunction'}}\n    assert mcode(custom_func(x), **settings) == \"CustomFunction[x]\"\n", "def test_custom_functions():\n        return x**2 + 2*x + 1\n\n    printer = MCodePrinter({'user_functions': {'custom_func': 'CustomFunction'}})\n    expr = custom_func(x)\n    assert printer.doprint(expr) == \"CustomFunction[x]\"\n", "def test_Complex():\n    assert mcode(x + 2*S.ImaginaryUnit) == \"x + 2*I\"\n    assert mcode(S.ImaginaryUnit*x) == \"I*x\"\n    assert mcode(S.ImaginaryUnit*S.ImaginaryUnit) == \"-1\"\n    assert mcode((1 + S.ImaginaryUnit)/(1 - S.ImaginaryUnit)) == \"I\"\n", "def test_Log():\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(log(x, 10)) == \"Log[10, x]\"\n    assert mcode(log(x**2, exp(1))) == \"Log[x^2]\"\n", "def test_custom_function():\n        return x**2 + 2*x + 1\n    settings = {'user_functions': {'custom_func': 'MyFunction'}}\n    assert mcode(custom_func(x), **settings) == \"MyFunction[x]\"\n"], "sample_1123": ["def test_condset_as_relational():\n    c = ConditionSet(x, x < 1, S.Reals)\n    assert c.as_relational(0.5) == And(0.5 < 1, S.Reals.contains(0.5))\n    assert c.as_relational(2) == And(2 < 1, S.Reals.contains(2))\n", "def test_ConditionSet_as_relational():\n    c = ConditionSet(x, x > 5, Interval(1, 7))\n    assert c.as_relational(6) == (x > 5)\n    assert c.as_relational(8) is S.false\n    assert c.as_relational(w) == And(w > 5, Contains(w, Interval(1, 7)))\n", "def test_CondSet_union():\n    input_conditionset1 = ConditionSet(x, x > 0, Interval(1, 5))\n    input_conditionset2 = ConditionSet(x, x < 0, Interval(-5, -1))\n    output_conditionset = Interval(-5, 5)\n    assert Union(input_conditionset1, input_conditionset2) == output_conditionset\n", "def test_subs_CondSet_with_expr():\n    expr = x + 1\n    c = ConditionSet(expr, expr < 2, S.Reals)\n    assert c.subs(x, y) == ConditionSet(y + 1, y + 1 < 2, S.Reals)\n", "def test_issue_17651():\n    with warns_deprecated_sympy():\n        assert ConditionSet(x, {x > 1, x < 5}, S.Reals) == ConditionSet(x, (x > 1) & (x < 5), S.Reals)\n\n    c = ConditionSet(x, (x > 1) & (x < 5), S.Reals)\n    assert c.condition == (x > 1) & (x < 5)\n"], "sample_1107": ["def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_permutations():\n    assert list(permutations([1, 2, 3], 0)) == [()]\n    assert list(permutations([1, 2, 3], 1)) == [(1,), (2,), (3,)]\n    assert list(permutations([1, 2, 3], 2)) == [(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)]\n    assert list(permutations([1, 2, 3], 3)) == [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n\n    assert list(permutations([1, 2, 3], 0, repetition=True)) == [()]\n    assert list(permutations([1, 2, 3], 1, repetition=True)) == [(1,), (2,), (3,)]\n    assert list(permutations([1, 2, 3], 2, repetition=True)) == [(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)]\n\n    assert len(list(permutations(range(5), 3))) == 60\n    assert len(list(permutations(range(5), 3, repetition=True))) == 125\n", "def test_generate_involutions_permute_signs_signed_permutations():\n    assert set(generate_involutions(3)) == {(0, 1, 2), (0, 2, 1), (1, 0, 2), (2, 1, 0)}\n    assert set(permute_signs((0, 1, 2))) == {(0, 1, 2), (0, 1, -2), (0, -1, 2), (0, -1, -2)}\n    assert set(signed_permutations((0, 1, 2))) == {\n        (0, 1, 2), (0, 1, -2), (0, -1, 2), (0, -1, -2), (1, 0, 2), (1, 0, -2),\n        (1, -2, 0), (-1, 2, 0), (-1, 0, 2), (-1, 0, -2), (2, 0, 1), (2, 0, -1),\n        (2, -1, 0), (-2, 1, 0), (-2, 0, 1), (-2, 0, -1)\n    }\n", "def test_multiset():\n    assert multiset([1, 2, 3]) == {1: 1, 2: 1, 3: 1}\n    assert multiset([1, 2, 2, 3, 3, 3]) == {1: 1, 2: 2, 3: 3}\n    assert multiset([]) == {}\n    assert multiset('aaabbc') == {'a': 3, 'b': 2, 'c': 1}\n", "def test_permutations():\n    assert list(permutations('abc')) == [('a', 'b', 'c'), ('a', 'c', 'b'),\n                                         ('b', 'a', 'c'), ('b', 'c', 'a'),\n                                         ('c', 'a', 'b'), ('c', 'b', 'a')]\n    assert list(permutations('a')) == [('a',)]\n    assert list(permutations('')) == [()]\n\n    assert list(permutations(range(3))) == [(0, 1, 2), (0, 2, 1),\n                                            (1, 0, 2), (1, 2, 0),\n                                            (2, 0, 1), (2, 1, 0)]\n    assert list(permutations(range(1))) == [(0,)]\n    assert list(permutations(range(0))) == [()]\n\n    assert list(permutations('abc', 2)) == [('a', 'b'), ('a', 'c'),\n                                            ('b', 'a'), ('b', 'c'),\n                                            ('c', 'a'), ('c', 'b')]\n    assert list(permutations('abc', 0)) == [()]\n\n    assert list(permutations('abc', repetition=True, length=2)) == [('a', 'a'), ('a', 'b'), ('a', 'c'),\n                                                                   ('b', 'a'), ('b', 'b'), ('b', 'c'),\n                                                                   ('c', 'a'), ('c', 'b'), ('c', 'c')]\n\n    assert list(permutations('ab', repetition=False, length=3)) == []\n\n    assert list(permutations('ab', repetition=True, length=3)) == [('a', 'a', 'a'), ('a', 'a', 'b'),\n                                                                   ('a', 'b', 'a'), ('a', 'b', 'b'),\n                                                                   ('b', 'a', 'a'), ('b', 'a', 'b'),\n                "], "sample_129": ["    def test_floatformat03(self):\n        output = self.engine.render_to_string('floatformat03', {\"a\": \"34.23234\", \"b\": \"34.00000\", \"c\": \"34.26000\"})\n        self.assertEqual(output, \"34.232 34.000 34.260\")\n", "def test_floatformat03(self):\n    output = self.engine.render_to_string('floatformat03', {\"a\": 1.42, \"b\": mark_safe(1.42)})\n    self.assertEqual(output, \"1.4 1.4\")\n", "def test_floatformat_decimal_input(self):\n    output = self.engine.render_to_string('floatformat_decimal_input', {\"a\": Decimal('1.423'), \"b\": Decimal('2.56789')})\n    self.assertEqual(output, \"1.42 2.568\")\n", "def test_floatformat03(self):\n    output = self.engine.render_to_string('floatformat03', {\"a\": \"34.23234\", \"b\": \"34.00000\", \"c\": \"34.26000\"})\n    self.assertEqual(output, \"34.232 34 34.260\")\n", "def test_floatformat_decimal_place_format(self):\n    output = self.engine.render_to_string('floatformat_decimal_place_format', {\"a\": 123.4567, \"b\": 123.4567, \"c\": 1234.567})\n    self.assertEqual(output, \"123.457 123.457 1234.5670\")\n"], "sample_189": ["    def test_cache_with_timeout_zero(self):\n        cache.set('zero_timeout', 'value', timeout=0)\n        self.assertIsNone(cache.get('zero_timeout'))\n", "def test_get_with_expiration(self):\n    cache.set('expire1', 'value', timeout=1)\n    self.assertEqual(cache.get('expire1'), 'value')\n    time.sleep(2)\n    self.assertIsNone(cache.get('expire1'))\n", "    def test_set_many_expiration_with_timeout_param(self):\n        # set_many takes a second ``timeout`` parameter\n        cache.set_many({\"key1\": \"spam\", \"key2\": \"eggs\"}, timeout=1)\n        time.sleep(2)\n        self.assertIsNone(cache.get(\"key1\"))\n        self.assertIsNone(cache.get(\"key2\"))\n", "def test_cache_key_with_method(self):\n    request = self.factory.get(self.path, method='POST')\n    response = HttpResponse()\n    # Set headers to an empty list.\n    learn_cache_key(request, response)\n    # The request method is taken into account.\n    self.assertEqual(\n        get_cache_key(request),\n        'views.decorators.cache.cache_page.settingsprefix.POST.'\n        '18a03f9c9649f7d684af5db3524f5c99.d41d8cd98f00b204e9800998ecf8427e'\n    )\n", "def test_set_many_invalid_key(self):\n    msg = KEY_ERRORS_WITH_MEMCACHED_MSG % ':1:key with spaces'\n    with self.assertWarnsMessage(CacheKeyWarning, msg):\n        cache.set_many({'key with spaces': 'foo'})\n"], "sample_136": ["def test_http_headers_getitem(self):\n    environ = {\n        'CONTENT_TYPE': 'text/html',\n        'CONTENT_LENGTH': '100',\n        'HTTP_HOST': 'example.com',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(headers['Content-Type'], 'text/html')\n    self.assertEqual(headers['content-type'], 'text/html')\n    self.assertEqual(headers['Content-Length'], '100')\n    self.assertEqual(headers['content_length'], '100')\n    self.assertEqual(headers['Host'], 'example.com')\n    self.assertEqual(headers['host'], 'example.com')\n", "def test_httprequest_headers(self):\n    request = HttpRequest()\n    request.META = {\n        'SERVER_NAME': 'internal.com',\n        'SERVER_PORT': 80,\n        'HTTP_ACCEPT': '*',\n        'HTTP_HOST': 'example.com',\n        'HTTP_USER_AGENT': 'python-requests/1.2.0',\n        'CONTENT_TYPE': 'text/html',\n        'CONTENT_LENGTH': '100',\n    }\n    self.assertEqual(dict(request.headers), {\n        'Accept': '*',\n        'Host': 'example.com',\n        'User-Agent': 'python-requests/1.2.0',\n        'Content-Type': 'text/html',\n        'Content-Length': '100',\n    })\n", "def test_wsgirequest_http_x_forwarded_proto(self):\n    \"\"\"\n    The HTTP_X_FORWARDED_PROTO header is used to determine the scheme\n    \"\"\"\n    request = WSGIRequest({\n        'PATH_INFO': '/somepath/',\n        'REQUEST_METHOD': 'get',\n        'wsgi.input': BytesIO(b''),\n        'HTTP_X_FORWARDED_PROTO': 'https',\n    })\n    self.assertTrue(request.is_secure())\n    self.assertEqual(request.scheme, 'https')\n", "def test_get_signed_cookie(self):\n    factory = RequestFactory()\n    request = factory.get('/')\n    request.COOKIES = {'signed_cookie': 'signed_value:signature'}\n    self.assertEqual(request.get_signed_cookie('signed_cookie'), 'signed_value')\n\n    with self.assertRaises(KeyError):\n        request.get_signed_cookie('missing_cookie')\n\n    request.COOKIES['invalid_cookie'] = 'invalid_signature'\n    with self.assertRaises(BadSignature):\n        request.get_signed_cookie('invalid_cookie')\n\n    self.assertEqual(request.get_signed_cookie('invalid_cookie', default='default_value'), 'default_value')\n", "def test_wsgirequest_headers(self):\n    request = WSGIRequest({\n        'PATH_INFO': '/somepath/',\n        'REQUEST_METHOD': 'get',\n        'wsgi.input': BytesIO(b''),\n        'SERVER_NAME': 'internal.com',\n        'SERVER_PORT': 80,\n        'CONTENT_TYPE': 'text/html',\n        'CONTENT_LENGTH': '100',\n        'HTTP_ACCEPT': '*',\n        'HTTP_HOST': 'example.com',\n        'HTTP_USER_AGENT': 'python-requests/1.2.0',\n    })\n    self.assertEqual(dict(request.headers), {\n        'Content-Type': 'text/html',\n        'Content-Length': '100',\n        'Accept': '*',\n        'Host': 'example.com',\n        'User-Agent': 'python-requests/1.2.0',\n    })\n"], "sample_447": ["def test_alias_forbidden_chars(self):\n    tests = [\n        'al\"ias',\n        \"a'lias\",\n        \"ali`as\",\n        \"alia s\",\n        \"alias\\t\",\n        \"ali\\nas\",\n        \"alias--\",\n        \"ali/*as\",\n        \"alias*/\",\n        \"alias;\",\n        # [] are used by MSSQL.\n        \"alias[\",\n        \"alias]\",\n    ]\n    msg = (\n        \"Column aliases cannot contain whitespace characters, quotation marks, \"\n        \"semicolons, or SQL comments.\"\n    )\n    for crafted_alias in tests:\n        with self.subTest(crafted_alias):\n            with self.assertRaisesMessage(ValueError, msg):\n                Book.objects.alias(**{crafted_alias: Value(1)})\n", "def test_alias_forbidden_chars(self):\n    tests = [\n        'al\"ias',\n        \"a'lias\",\n        \"ali`as\",\n        \"alia s\",\n        \"alias\\t\",\n        \"ali\\nas\",\n        \"alias--\",\n        \"ali/*as\",\n        \"alias*/\",\n        \"alias;\",\n    ]\n    msg = (\n        \"Column aliases cannot contain whitespace characters, quotation marks, \"\n        \"semicolons, or SQL comments.\"\n    )\n    for crafted_alias in tests:\n        with self.subTest(crafted_alias):\n            with self.assertRaisesMessage(ValueError, msg):\n                Book.objects.alias(**{crafted_alias: Value(1)})\n", "def test_alias_forbidden_chars(self):\n    tests = [\n        'al\"ias',\n        \"a'lias\",\n        \"ali`as\",\n        \"alia s\",\n        \"alias\\t\",\n        \"ali\\nas\",\n        \"alias--\",\n        \"ali/*as\",\n        \"alias*/\",\n        \"alias;\",\n    ]\n    msg = (\n        \"Column aliases cannot contain whitespace characters, quotation marks, \"\n        \"semicolons, or SQL comments.\"\n    )\n    for crafted_alias in tests:\n        with self.subTest(crafted_alias):\n            with self.assertRaisesMessage(ValueError, msg):\n                Book.objects.alias(**{crafted_alias: Value(1)})\n", "def test_alias_with_aggregate_values_chaining(self):\n    qs = (\n        Book.objects.values(\"publisher\")\n        .alias(multiplier=Value(3))\n        .annotate(multiplied_value_sum=Sum(F(\"multiplier\") * F(\"publisher__num_awards\")))\n        .order_by()\n    )\n    self.assertCountEqual(\n        qs,\n        [\n            {\"multiplied_value_sum\": 9, \"publisher\": self.p1.pk},\n        ],\n    )\n", "def test_annotation_filter_with_subquery_on_annotated_field(self):\n    qs = Publisher.objects.annotate(\n        total_books=Count(\"book\"),\n    ).filter(\n        total_books=Subquery(\n            Publisher.objects.annotate(\n                book_count=Count(\"book\")\n            ).filter(\n                name=OuterRef(\"name\")\n            ).values(\"book_count\")\n        )\n    )\n    self.assertCountEqual(\n        qs,\n        [self.p1, self.p2, self.p3, self.p4, self.p5],\n    )\n"], "sample_80": ["def test_foreign_key_with_allow_joins(self):\n    query = Query(Item)\n    where = query.build_where(Q(creator__num__gt=2), allow_joins=True)\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_complex_query_with_transform(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(num__gt=2) | ~Q(name__lower='foo'))\n    self.assertEqual(where.connector, OR)\n\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n\n    negated_lookup = where.children[1]\n    self.assertTrue(negated_lookup.negated)\n    lookup = negated_lookup.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, SimpleCol)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "def test_transform_with_unsupported_lookup(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        with self.assertRaisesMessage(FieldError, \"Unsupported lookup 'unsupported' for CharField\"):\n            query.build_where(Q(name__lower__unsupported='foo'))\n", "def test_annotation(self):\n    query = Query(Author)\n    query.add_annotation(F('num') + 1, 'annotated_num', is_summary=False)\n    where = query.build_where(Q(annotated_num__gt=5))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 5)\n    self.assertIn('annotated_num', query.annotation_select)\n    annotation = query.annotation_select['annotated_num']\n    self.assertIsInstance(annotation, F)\n    self.assertEqual(annotation.name, 'num')\n", "def test_foreign_key_lookup_with_transform(self):\n    query = Query(Item)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(~Q(creator__name__lower='foo'))\n    self.assertTrue(where.negated)\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, SimpleCol)\n    self.assertEqual(lookup.lhs.lhs.target.related_model, Author)\n"], "sample_257": ["def test_key_transform_in_exclude(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.exclude(value__c__in=[14]),\n        self.objs[:3] + self.objs[5:],\n    )\n", "def test_key_transform_sql_injection(self):\n    with CaptureQueriesContext(connection) as queries:\n        self.assertIs(\n            NullableJSONModel.objects.filter(**{\n                f\"value__test' = 'a') OR 1 = 1 OR ('d\": 'x',\n            }).exists(),\n            False,\n        )\n    self.assertIn(\n        \"\"\".\"value\" #> '{\"test\\'\": \"a\\'\") OR 1 = 1 OR (''d\"}' = '\"x\"' \"\"\",\n        queries[0]['sql'],\n    )\n", "def test_key_transform_lookup_expression(self):\n    # Test that a KeyTransform lookup can be used in an expression.\n    obj = NullableJSONModel.objects.create(value={'a': 10, 'b': 5})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__lt=F('value__b') + 5),\n        [obj],\n    )\n", "def test_key_transform_annotation_expression_with_joins(self):\n    related_obj = RelatedJSONModel.objects.create(\n        value={'d': ['f', 'e']},\n        json_model=self.objs[4],\n    )\n    RelatedJSONModel.objects.create(\n        value={'d': ['e', 'f']},\n        json_model=self.objs[4],\n    )\n    self.assertSequenceEqual(\n        RelatedJSONModel.objects.annotate(\n            key=F('value__d'),\n            related_key=F('json_model__value__d'),\n            chain=KeyTransform('1', 'key'),\n            expr=KeyTransform('0', 'related_key'),\n        ).filter(chain=F('expr')),\n        [related_obj],\n    )\n", "def test_key_transform_raw_expression_with_param(self):\n    expr = RawSQL(self.raw_sql, ['{\"x\": \"bar\"}'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr), value__bar='bar'),\n        [self.objs[7]],\n    )\n"], "sample_1030": ["def test_ordered_points():\n    p1, p2, p3 = Point2D(0, 0), Point2D(1, 1), Point2D(2, 2)\n    assert _ordered_points((p2, p1, p3)) == (p1, p2, p3)\n    assert _ordered_points((p3, p2, p1)) == (p1, p2, p3)\n    assert _ordered_points((p1, p2)) == (p1, p2)\n    assert _ordered_points((p2, p1)) == (p1, p2)\n", "def test_ordered_points():\n    points = [Point2D(3, 4), Point2D(1, 2), Point2D(2, 1)]\n    ordered = _ordered_points(points)\n    assert ordered == (Point2D(1, 2), Point2D(2, 1), Point2D(3, 4))\n", "def test_ordered_points():\n    points = [Point2D(1, 1), Point2D(2, 2), Point2D(3, 3)]\n    ordered_points = _ordered_points(points)\n    assert ordered_points == (Point2D(1, 1), Point2D(2, 2), Point2D(3, 3))\n\n    points = [Point2D(3, 3), Point2D(2, 2), Point2D(1, 1)]\n    ordered_points = _ordered_points(points)\n    assert ordered_points == (Point2D(1, 1), Point2D(2, 2), Point2D(3, 3))\n\n    points = [Point2D(1, 3), Point2D(2, 2), Point2D(3, 1)]\n    ordered_points = _ordered_points(points)\n    assert ordered_points == (Point2D(1, 3), Point2D(2, 2), Point2D(3, 1))\n", "def test_ordered_points():\n    p = Point2D(2, 1)\n    q = Point2D(1, 2)\n    r = Point2D(2, 1)\n    assert _ordered_points((p, q, r)) == (p, r, q)\n", "def test_ordered_points():\n    p1 = Point2D(1, 2)\n    p2 = Point2D(3, 4)\n    p3 = Point2D(2, 3)\n    assert _ordered_points((p1, p2, p3)) == (p1, p3, p2)\n    p4 = Point2D(1, 2)\n    assert _ordered_points((p1, p2, p3, p4)) == (p1, p4, p3, p2)\n"], "sample_895": ["def test_column_transformer_drop_all_columns():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n\n    ct = ColumnTransformer([(\"trans1\", \"drop\", [0, 1, 2])], remainder=\"passthrough\")\n\n    assert_array_equal(ct.fit_transform(X_array), np.array([]).reshape(3, 0))\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.array([]).reshape(3, 0))\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert isinstance(ct.transformers_[-1][1], str)\n    assert ct.transformers_[-1][1] == \"passthrough\"\n    assert_array_equal(ct.transformers_[-1][2], [])\n", "def test_column_transformer_drop_and_passthrough_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_res_both = X_array.copy()\n\n    # second and third columns are doubled when remainder = DoubleTrans\n    X_res_both[:, 1:3] *= 2\n\n    ct = ColumnTransformer([(\"trans1\", \"drop\", [0])], remainder=DoubleTrans())\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n\n    ct = ColumnTransformer([(\"trans1\", \"drop\", [0])], remainder=\"passthrough\")\n\n    assert_array_equal(ct.fit_transform(X_array), X_array[:, 1:3])\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array[:, 1:3])\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == \"remainder\"\n    assert ct.transformers_[-1][1] == \"passthrough\"\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_with_make_column_selector_array_input():\n    # Functional test for column transformer + column selector with array input\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([[0, 1, 2]]).T\n\n    cat_selector = make_column_selector(dtype_include=[int])\n    num_selector = make_column_selector(dtype_include=[float])\n\n    scaler = StandardScaler()\n\n    ct_selector = ColumnTransformer([(\"scaler\", scaler, num_selector)])\n    ct_direct = ColumnTransformer([(\"scaler\", scaler, [1])])\n\n    X_selector = ct_selector.fit_transform(X_array)\n    X_direct = ct_direct.fit_transform(X_array)\n\n    assert_allclose(X_selector, X_direct)\n", "def test_column_transformer_get_feature_names_out_with_dropped_columns():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    ct = ColumnTransformer(\n        [\n            (\"trans1\", \"passthrough\", [0]),\n            (\"trans2\", \"drop\", [1]),\n            (\"trans3\", \"passthrough\", [2]),\n        ],\n        verbose_feature_names_out=False\n    )\n    ct.fit(X)\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(feature_names_out, [0, 2])\n", "def test_column_transformer_error_messages():\n    # Test that column transformer raises appropriate errors\n    # when the input is not valid\n\n    # Non-array input\n    X = \"not an array\"\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0])])\n    with pytest.raises(ValueError, match=\"Input should be of type ndarray, DataFrame or sparse matrix\"):\n        ct.fit(X)\n\n    # Non-array column transformer input\n    X = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([(\"trans1\", Trans(), \"not a valid column specifier\")])\n    with pytest.raises(ValueError, match=\"No valid specification of columns\"):\n        ct.fit(X)\n\n    # Non-integer column index\n    X = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([(\"trans1\", Trans(), [\"non-integer\"])])\n    with pytest.raises(ValueError, match=\"Specifying the columns using strings is only supported for pandas DataFrames\"):\n        ct.fit(X)\n"], "sample_199": ["def test_annotation_aggregate_with_m2m(self):\n    qs = Author.objects.filter(age__lt=30).annotate(\n        total_books=Count('book'),\n    ).values('name', 'total_books')\n    self.assertCountEqual(qs, [\n        {'name': 'James Bennett', 'total_books': 1},\n        {'name': 'Paul Bissex', 'total_books': 1},\n        {'name': 'Wesley J. Chun', 'total_books': 2},\n    ])\n", "def test_annotation_aggregate_with_m2m(self):\n    qs = Author.objects.filter(age__lt=30).annotate(\n        total_pages=Sum(F('book_set__pages')),\n    ).values('name', 'total_pages')\n    self.assertCountEqual(qs, [\n        {'name': 'James Bennett', 'total_pages': 300},\n        {'name': 'Paul Bissex', 'total_pages': 300},\n        {'name': 'Wesley J. Chun', 'total_pages': 628},\n    ])\n", "def test_annotation_with_multiple_values(self):\n    books = Book.objects.annotate(\n        author_age=F('authors__age'),\n        publisher_name=F('publisher__name')\n    ).filter(pk=self.b1.pk).order_by('author_age')\n    self.assertEqual(books[0].author_age, 34)\n    self.assertEqual(books[0].publisher_name, 'Apress')\n    self.assertEqual(books[1].author_age, 35)\n    self.assertEqual(books[1].publisher_name, 'Apress')\n", "def test_annotation_aggregate_with_m2m(self):\n    qs = Author.objects.filter(age__lt=30).annotate(\n        total_books=Count('book'),\n    ).filter(\n        total_books__gt=0,\n    ).annotate(\n        max_pages=Max('book__pages'),\n    ).values('name', 'max_pages')\n    self.assertCountEqual(qs, [\n        {'name': 'James Bennett', 'max_pages': 300},\n        {'name': 'Paul Bissex', 'max_pages': 350},\n        {'name': 'Wesley J. Chun', 'max_pages': 300},\n    ])\n", "def test_annotation_with_multiple_conditions(self):\n    books = Book.objects.annotate(\n        is_expensive=Case(\n            When(price__gt=50, then=Value('very expensive')),\n            When(price__gt=25, then=Value('expensive')),\n            default=Value('cheap'),\n            output_field=CharField(),\n        )\n    )\n    expensive_book_count = books.filter(is_expensive='expensive').count()\n    very_expensive_book_count = books.filter(is_expensive='very expensive').count()\n    cheap_book_count = books.filter(is_expensive='cheap').count()\n    self.assertEqual(expensive_book_count, 1)\n    self.assertEqual(very_expensive_book_count, 1)\n    self.assertEqual(cheap_book_count, 6)\n"], "sample_292": ["def test_csrf_token_in_header_with_incorrect_value(self):\n    \"\"\"\n    The token in the header with an incorrect value is not accepted.\n    \"\"\"\n    req = self._get_POST_csrf_cookie_request()\n    req.META['HTTP_X_CSRFTOKEN'] = 'incorrect_token'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        resp = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(403, resp.status_code)\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_BAD_TOKEN)\n", "def test_csrf_token_in_header_with_customized_name_invalid(self):\n    \"\"\"\n    A request with an invalid CSRF token in a header with a customized name\n    should be rejected.\n    \"\"\"\n    req = self._get_POST_csrf_cookie_request()\n    req.META['HTTP_X_CSRFTOKEN_CUSTOMIZED'] = 'invalid_token'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n        resp = mw.process_view(req, post_form_view, (), {})\n    self.assertEqual(403, resp.status_code)\n    self.assertEqual(cm.records[0].getMessage(), 'Forbidden (%s): ' % REASON_BAD_TOKEN)\n", "def test_csrf_token_reset_on_session_change(self):\n    \"\"\"\n    A new token is sent if the session changes.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    req.session.cycle_key()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = token_view(req)\n    token = get_token(req)\n    self.assertIsNotNone(token)\n    self._check_token_present(resp, token)\n", "def test_good_origin_csrf_trusted_origin_allowed_with_port(self):\n    \"\"\"\n    A POST request with an origin added to the CSRF_TRUSTED_ORIGINS\n    setting is accepted when the origin includes a port.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://foo.example.com:443'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n    self.assertEqual(mw.allowed_origins_exact, set())\n    self.assertEqual(mw.allowed_origin_subdomains, {'https': ['.example.com']})\n", "def test_https_no_referer(self):\n    \"\"\"\n    A POST HTTPS request with no referer is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n"], "sample_460": ["def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n    user = User.objects.create_user(\n        username=\"user\",\n        password=\"secret\",\n        email=\"user@example.com\",\n        is_staff=False,\n    )\n    self.client.force_login(user)\n    known_url = reverse(\"admin10:admin_views_article_changelist\")\n    response = self.client.get(known_url[:-1])\n    self.assertRedirects(\n        response,\n        \"/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article\",\n    )\n", "def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username=\"staff\",\n        password=\"secret\",\n        email=\"staff@example.com\",\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse(\"admin10:admin_views_article_changelist\")\n    response = self.client.get(known_url[:-1], SCRIPT_NAME=\"/prefix/\")\n    self.assertRedirects(\n        response,\n        \"/prefix\" + known_url,\n        status_code=301,\n        fetch_redirect_response=False,\n    )\n", "def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n    superuser = User.objects.create_user(\n        username=\"staff\",\n        password=\"secret\",\n        email=\"staff@example.com\",\n        is_staff=True,\n    )\n    self.client.force_login(superuser)\n    known_url = reverse(\"admin10:admin_views_article_changelist\")\n    response = self.client.get(known_url[:-1], SCRIPT_NAME=\"/prefix/\")\n    self.assertRedirects(\n        response,\n        \"/prefix\" + known_url,\n        status_code=301,\n        fetch_redirect_response=False,\n    )\n", "def test_admin_view_on_site_custom_admin_site(self):\n    model_admin = ModelAdmin(City, customadmin.site)\n    content_type_pk = ContentType.objects.get_for_model(City).pk\n    redirect_url = model_admin.get_view_on_site_url(self.c1)\n    expected_url = reverse(\n        f\"{customadmin.site.name}:view_on_site\",\n        kwargs={\n            \"content_type_id\": content_type_pk,\n            \"object_id\": self.c1.pk,\n        },\n    )\n    self.assertEqual(redirect_url, expected_url)\n", "def test_change_form_widgets(self):\n    response = self.client.get(reverse(\"admin:admin_views_post_change\", args=(self.p1.pk,)))\n    # Check that the widget is a Textarea\n    self.assertContains(response, '<textarea name=\"title\" cols=\"40\" rows=\"10\" id=\"id_title\">')\n    # Check that the widget is a CheckboxInput\n    self.assertContains(response, '<input type=\"checkbox\" name=\"published\" id=\"id_published\" checked>')\n    # Check that the widget is a DateInput\n    self.assertContains(response, '<input type=\"text\" name=\"published_date\" value=\"2022-01-01\" id=\"id_published_date\">')\n    # Check that the widget is a Select\n    self.assertContains(response, '<select name=\"category\" id=\"id_category\">')\n"], "sample_769": ["def test_balanced_accuracy_score_empty():\n    assert_raises(ValueError, balanced_accuracy_score, [], [])\n    assert_raises(ValueError, balanced_accuracy_score, ['a'], [])\n    assert_raises(ValueError, balanced_accuracy_score, [], ['a'])\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 2, 1, 0, 1, 2])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    per_class_recall = recall_score(y_true, y_pred, average=None)\n    expected_balanced = np.mean(per_class_recall)\n    assert balanced == pytest.approx(expected_balanced)\n", "def test_balanced_accuracy_score_multilabel():\n    y_true = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n    y_pred = np.array([[0, 1], [0, 1], [1, 0], [1, 1]])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    expected_balanced = (0.5 + 0.5) / 2\n    assert balanced == pytest.approx(expected_balanced)\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = np.array([0, 0, 1, 1, 2, 2])\n    y_pred = np.array([0, 1, 1, 2, 2, 2])\n    sample_weight = np.array([1, 1, 2, 2, 1, 1])\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    expected = (2 * 0.5 + 1 * 0.6666667) / 3\n    assert balanced == pytest.approx(expected)\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = [0, 0, 1, 1, 1]\n    y_pred = [0, 0, 1, 1, 0]\n    sample_weight = [1, 1, 2, 2, 3]\n\n    # Calculate balanced accuracy with sample weights\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n\n    # Calculate balanced accuracy without sample weights for comparison\n    balanced_no_weight = balanced_accuracy_score(y_true, y_pred)\n\n    # The result with sample weights should be different from the result without sample weights\n    assert balanced != balanced_no_weight\n"], "sample_36": ["def test_biweight_location_axis_1d():\n    \"\"\"Test a 1D array with the axis keyword.\"\"\"\n    with NumpyRNGContext(12345):\n        data = normal(5, 2, 100)\n        bw = biweight_location(data, axis=0)\n        bwi = biweight_location(data)\n        assert_allclose(bw, bwi)\n\n        with pytest.raises(ValueError):\n            biweight_location(data, axis=1)\n", "def test_biweight_location_with_initial_guess():\n    \"\"\"Test biweight_location with a specified initial guess.\"\"\"\n    with NumpyRNGContext(12345):\n        ny = 100\n        nx = 200\n        data = normal(5, 2, (ny, nx))\n        initial_guess = 4.5\n\n        bw = biweight_location(data, M=initial_guess, axis=0)\n        bwi = []\n        for i in range(nx):\n            bwi.append(biweight_location(data[:, i], M=initial_guess))\n        bwi = np.array(bwi)\n        assert_allclose(bw, bwi)\n", "def test_biweight_location_M_input():\n    \"\"\"Test biweight_location with a specified M input.\"\"\"\n    data = [1, 3, 5, 500, 2]\n    M = 2.0\n    cbl = biweight_location(data, M=M)\n    assert abs(cbl - 2.745) < 1e-3\n", "def test_biweight_location_large_outlier():\n    \"\"\"Test biweight_location with a large outlier.\"\"\"\n    data = np.ones(100)\n    data[0] = 1000000  # large outlier\n    loc = biweight_location(data)\n    # The outlier should not significantly affect the location estimate\n    assert_allclose(loc, 1.0, atol=0.1)\n", "def test_biweight_midcovariance_vs_biweight_midvariance():\n    \"\"\"\n    Test that biweight_midcovariance agrees with biweight_midvariance\n    when the input data has the same variable multiple times.\n    \"\"\"\n\n    rng = np.random.RandomState(1)\n    data = rng.normal(0, 2, size=(100,))\n    data_dup = np.vstack([data, data])\n\n    cov = biweight_midcovariance(data_dup)\n    var = biweight_midvariance(data)\n    assert_allclose(cov[0, 0], var)\n    assert_allclose(cov[0, 1], var)\n    assert_allclose(cov[1, 0], var)\n    assert_allclose(cov[1, 1], var)\n"], "sample_768": ["def test_group_kfold_reproducible():\n    # Check that iterating twice on the GroupKFold gives the same\n    # sequence of train-test when the random_state is given\n    gkf = GroupKFold(n_splits=3, random_state=42)\n    groups = np.array([0, 0, 1, 1, 2, 2])\n    X = np.ones(len(groups))\n    assert_array_equal(list(a for a, b in gkf.split(X, groups=groups)),\n                       list(a for a, b in gkf.split(X, groups=groups)))\n", "def test_leave_one_p_group_out_no_groups():\n    # Check that LeaveOneGroupOut and LeavePGroupsOut raise an error when no groups are provided\n    X = y = np.ones(5)\n    with pytest.raises(ValueError, match=\"The 'groups' parameter should not be None.\"):\n        next(LeaveOneGroupOut().split(X, y, groups=None))\n    with pytest.raises(ValueError, match=\"The 'groups' parameter should not be None.\"):\n        next(LeavePGroupsOut(n_groups=2).split(X, y, groups=None))\n", "def test_shuffle_split_deterministic():\n    # Check that shuffling is deterministic when random_state is an integer\n    ss1 = ShuffleSplit(test_size=0.2, random_state=0)\n    ss2 = ShuffleSplit(test_size=0.2, random_state=0)\n    for split1, split2 in zip(ss1.split(X), ss2.split(X)):\n        assert_array_equal(split1[0], split2[0])\n        assert_array_equal(split1[1], split2[1])\n", "def test_repeated_stratified_kfold_deterministic_split_with_invalid_group():\n    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n    y = [1, 1, 1, -1, -1]\n    random_state = 1944695409\n    rskf = RepeatedStratifiedKFold(\n        n_splits=2,\n        n_repeats=2,\n        random_state=random_state)\n\n    # split should raise ValueError if groups contains nan or inf\n    groups_with_nan = np.array([1, 1, 1, np.nan, 0])\n    groups_with_inf = np.array([1, 1, 1, np.inf, 0])\n\n    assert_raises(ValueError, list, rskf.split(X, y, groups_with_nan))\n    assert_raises(ValueError, list, rskf.split(X, y, groups_with_inf))\n", "def test_predefinedsplit_warnings():\n    # Test that warnings are raised when test_fold contains values outside\n    # the range of -1 to n_samples, and when the number of unique folds is not\n    # equal to n_splits.\n    X = np.ones(10)\n    test_fold = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 10])  # contains a value greater than n_samples\n    with pytest.warns(UserWarning, match=\"Found array with 11 sample(s) while the cross-validator is expected to use 10 sample(s). Make sure that the test_fold is the correct one.\"):\n        predefined_split = PredefinedSplit(test_fold)\n        list(predefined_split.split(X))\n\n    test_fold = np.array([0, 1, 2, 2, 2, 2, 2, 2, 2, 2])  # only 1 unique fold\n    with pytest.warns(UserWarning, match=\"The test_fold contains only one unique split. This will result in an empty train set.\"):\n        predefined_split = PredefinedSplit(test_fold)\n        list(predefined_split.split(X))\n\n    test_fold = np.array([0, 1, 2, -2, -2, -2, -2, -2, -2, -2])  # contains a value less than -1\n    with pytest.warns(UserWarning, match=\"The test_fold contains negative values. This will result in undefined behaviour.\"):\n        predefined_split = PredefinedSplit(test_fold)\n        list(predefined_split.split(X))\n"], "sample_235": ["def test_hooks_run_on_set_autocommit(self):\n    with transaction.atomic():\n        self.do(1)\n        connection.set_autocommit(True)\n        self.assertNotified([1])\n    self.assertDone([1])\n", "def test_hooks_run_in_same_savepoint(self):\n    with transaction.atomic():\n        self.do(1)\n        with transaction.atomic():\n            self.do(2)\n            transaction.on_commit(lambda: self.notify(20))\n        self.do(3)\n        transaction.on_commit(lambda: self.notify(30))\n\n    self.assertDone([1, 2, 3])\n    self.assertNotified([20, 30])\n", "def test_hooks_cleared_after_exception(self):\n    try:\n        with transaction.atomic():\n            self.do(1)\n            raise ForcedError()\n    except ForcedError:\n        pass\n\n    with transaction.atomic():\n        self.do(2)\n\n    self.assertDone([2])\n", "def test_hook_with_savepoint_rollback(self):\n        with transaction.atomic():\n            Thing.objects.create(num=num)\n            self.notify(num)\n            raise ForcedError()\n\n    try:\n        with transaction.atomic():\n            transaction.on_commit(lambda: on_commit(1))\n            transaction.on_commit(lambda: on_commit(2))\n    except ForcedError:\n        pass\n\n    self.assertDone([])\n", "def test_hooks_cleared_after_rollback_with_error_in_hook(self):\n        raise ForcedError()\n\n    try:\n        with transaction.atomic():\n            transaction.on_commit(on_commit_error)\n            self.do(1)\n    except ForcedError:\n        pass\n\n    with transaction.atomic():\n        self.do(2)\n\n    self.assertDone([2])\n"], "sample_646": ["def test_unittest_skip_reason_attribute(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n                raise unittest.SkipTest(\"skipping due to reasons\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"-rs\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *SKIP*[1]*skipping due to reasons*\n        *1 skipped*\n    \"\"\"\n    )\n", "def test_raising_unittest_skiptest_during_collection_for_class(\n    pytester: Pytester,", "def test_skip_class_with_fixture(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        @pytest.fixture(scope=\"class\")\n            request.cls.class_fixture_value = \"class_fixture_value\"\n\n        @unittest.skip(\"skipping class\")\n        @pytest.mark.usefixtures(\"class_fixture\")\n        class MySkippedTestCase(unittest.TestCase):\n                pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(skipped=1)\n", "def test_teardown_order(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                pass\n                pass\n                self.values.append(self._testMethodName)\n            assert MyTestCase.values == ['test_one', 'test_two']\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 0\n    assert passed == 3\n", "def test_raising_unittest_skiptest_during_collection_of_test_case(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestIt(unittest.TestCase):\n                raise unittest.SkipTest()\n\n                pass\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 1\n    assert skipped == 1\n    assert failed == 0\n    assert reprec.ret == ExitCode.OK\n"], "sample_33": ["def test_indent():\n    s = \"Hello\\nWorld\"\n    indented = misc.indent(s)\n    assert indented == \"    Hello\\n    World\"\n\n    indented = misc.indent(s, shift=2)\n    assert indented == \"        Hello\\n        World\"\n\n    indented = misc.indent(s, width=2)\n    assert indented == \"  Hello\\n  World\"\n\n    s = \"Hello\\nWorld\\n\"\n    indented = misc.indent(s)\n    assert indented == \"    Hello\\n    World\\n\"\n", "def test_indent():\n    s = \"Hello\\nWorld\\n\"\n    indented = misc.indent(s, shift=2, width=4)\n    assert indented == \"        Hello\\n        World\\n        \\n\"\n", "def test_indent():\n    s = \"This is a test string.\\nIt has multiple lines.\\n\"\n    indented = misc.indent(s, shift=2)\n    expected = \"        This is a test string.\\n        It has multiple lines.\\n        \"\n    assert indented == expected\n", "def test_ordered_descriptor_container():\n    class ExampleDescriptor(misc.OrderedDescriptor):\n        _class_attribute_ = 'examples'\n        _name_attribute_ = 'name'\n        name = '<unbound>'\n\n            super().__init__()\n            self.value = value\n\n    class ExampleClass(metaclass=misc.OrderedDescriptorContainer):\n        attr1 = ExampleDescriptor('first')\n        attr2 = ExampleDescriptor('second')\n\n    assert len(ExampleClass.examples) == 2\n    assert ExampleClass.examples['attr1'].value == 'first'\n    assert ExampleClass.examples['attr2'].value == 'second'\n\n    class SubExampleClass(ExampleClass):\n        attr3 = ExampleDescriptor('third')\n\n    assert len(SubExampleClass.examples) == 1\n    assert SubExampleClass.examples['attr3'].value == 'third'\n\n    class SubExampleClassWithInheritance(ExampleClass):\n        _inherit_descriptors_ = (ExampleDescriptor,)\n        attr3 = ExampleDescriptor('third')\n\n    assert len(SubExampleClassWithInheritance.examples) == 3\n    assert SubExampleClassWithInheritance.examples['attr1'].value == 'first'\n    assert SubExampleClassWithInheritance.examples['attr2'].value == 'second'\n    assert SubExampleClassWithInheritance.examples['attr3'].value == 'third'\n", "def test_indent():\n    text = \"Hello\\nWorld\\n\"\n    indented = misc.indent(text, shift=2, width=2)\n    expected = \"    Hello\\n    World\\n\"\n    assert indented == expected\n\n    text = \"Hello\\nWorld\"\n    indented = misc.indent(text, shift=1, width=4)\n    expected = \"    Hello\\n    World\"\n    assert indented == expected\n"], "sample_87": ["def test_glob_non_existing_directory(self, mocked_modules, notify_mock):\n    non_py_file = self.ensure_file(self.tempdir / 'does_not_exist' / 'non_py_file')\n    self.reloader.watch_dir(self.tempdir / 'does_not_exist', '*.py')\n    with self.tick_twice():\n        self.increment_mtime(non_py_file)\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_nonexistent_file_does_not_cause_error(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.nonexistent_file)\n    with self.tick_twice():\n        # This should not cause an error\n        pass\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_deleted_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.existing_file)\n    with self.tick_twice():\n        self.existing_file.unlink()\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "    def setUp(self):\n        self._tempdir = tempfile.TemporaryDirectory()\n        self.tempdir = Path(self._tempdir.name).resolve().absolute()\n        self.existing_file = self.ensure_file(self.tempdir / 'test.py')\n        self.reloader = autoreload.StatReloader()\n", "    def test_tick_with_changed_file(self):\n        mtimes = {self.existing_file: 12345}\n        self.reloader.snapshot_files = mock.MagicMock(return_value=iter([(self.existing_file, 12346)]))\n        self.reloader.notify_file_changed = mock.MagicMock()\n\n        with mock.patch.object(self.reloader, 'snapshot_files') as mock_snapshot, \\\n             mock.patch.dict(autoreload.StatReloader.__dict__, {'SLEEP_TIME': 0}):\n            ticker = self.reloader.tick()\n            next(ticker)  # Run the first iteration of the loop\n\n        self.reloader.notify_file_changed.assert_called_once_with(self.existing_file)\n"], "sample_931": ["def test_pyfunction_signature_with_annotations(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"str\"])])\n", "def test_pyfunction_signature_with_module_option(app):\n    text = (\".. py:function:: hello(name: str) -> str\\n\"\n            \"   :module: mymodule\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"mymodule.\"],\n                                                    [desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][0], desc_addname, \"mymodule.\")\n", "def test_pyfunction_signature_with_defaults(app):\n    text = \".. py:function:: hello(name='world', greeting='Hello') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'world'\"])],\n                                      [desc_parameter, ([desc_sig_name, \"greeting\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'Hello'\"])])])\n", "def test_pycurrentmodule_directive(app):\n    text = (\".. py:currentmodule:: sphinx\\n\"\n            \".. py:function:: func\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([desc, ([desc_signature, ([desc_addname, \"sphinx.\"],\n                                                    [desc_name, \"func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert 'sphinx.func' in domain.objects\n    assert domain.objects['sphinx.func'] == ('index', 'sphinx.func', 'function')\n", "def test_pyfunction_signature_with_annotation(app):\n    text = \".. py:function:: hello(name: str) -> str: A function that greets a person.\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, desc_parameter, ([desc_sig_name, \"name\"],\n                                                      [desc_sig_punctuation, \":\"],\n                                                      \" \",\n                                                      [nodes.inline, pending_xref, \"str\"])])\n    assert doctree[1][1].astext().strip() == \"A function that greets a person.\"\n"], "sample_1167": ["def test_printing_latex_array_expressions_indexing():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    assert latex(A[0, 1, 2]) == \"{{A}_{0, 1, 2}}\"\n", "def test_printing_latex_array_expressions_with_complex_indices():\n    assert latex(ArrayElement(\"A\", (2, 1/(1-x), 0, i))) == \"{{A}_{2, \\\\frac{1}{1 - x}, 0, i}}\"\n", "def test_printing_latex_array_expressions_with_derivative():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    x = Symbol(\"x\")\n    expr = Derivative(ArrayElement(\"A\", (2, 1/(1-x), 0)), x)\n    assert latex(expr) == r\"\\frac{d}{d x} {{A}_{2, \\frac{1}{1 - x}, 0}}\"\n", "def test_latex_expand_multiline():\n    a, b, c, d, e, f = symbols('a b c d e f')\n    expr = -a + 2*b -3*c +4*d -5*e\n    expected = r\"\\begin{eqnarray}\" + \"\\n\"\\\n        r\"f & = &- a \\nonumber\\\\\" + \"\\n\"\\\n        r\"& + & 2 b \\nonumber\\\\\" + \"\\n\"\\\n        r\"& - & 3 c \\nonumber\\\\\" + \"\\n\"\\\n        r\"& + & 4 d \\nonumber\\\\\" + \"\\n\"\\\n        r\"& - & 5 e \" + \"\\n\"\\\n        r\"\\end{eqnarray}\"\n    assert multiline_latex(f, expr, environment=\"eqnarray\", expand_multiline=True) == expected\n", "def test_latex_printing_with_dimensions():\n    x = symbols('x', real=True)\n    assert latex(x*meter) == r'x \\text{m}'\n    assert latex(x*meter**2) == r'x \\text{m}^{2}'\n    assert latex(x*meter**-1) == r'\\frac{x}{\\text{m}}'\n    assert latex(x*meter**-2) == r'\\frac{x}{\\text{m}^{2}}'\n    assert latex(x*second) == r'x \\text{s}'\n    assert latex(x*second**2) == r'x \\text{s}^{2}'\n    assert latex(x*second**-1) == r'\\frac{x}{\\text{s}}'\n    assert latex(x*second**-2) == r'\\frac{x}{\\text{s}^{2}}'\n    assert latex(x*meter/second) == r'\\frac{x \\text{m}}{\\text{s}}'\n"], "sample_44": ["def test_inplace_multiplication_division_unit_checks(self):\n    lu1 = u.mag(u.Jy)\n    lq1 = u.Magnitude(np.arange(1., 10.), lu1)\n    with pytest.raises(u.UnitsError):\n        lq1 *= u.m\n\n    assert np.all(lq1.value == np.arange(1., 10.))\n    assert lq1.unit == lu1\n\n    with pytest.raises(u.UnitsError):\n        lq1 /= u.m\n\n    assert np.all(lq1.value == np.arange(1., 10.))\n    assert lq1.unit == lu1\n", "def test_neg_pos_inplace(self):\n    lq = u.Magnitude(np.arange(1., 10.)*u.Jy)\n    lq_neg = -lq\n    assert not np.all(lq_neg == lq)\n    assert np.all(lq_neg.value == -lq.value)\n    assert lq_neg.unit == lq.unit\n\n    lq_neg *= -1\n    assert np.all(lq_neg == lq)\n\n    lq_pos = +lq\n    assert np.all(lq_pos == lq)\n    assert lq_pos is not lq\n", "def test_neg_pos_logquantity(self):\n    lq = u.Magnitude(np.arange(1., 4.) * u.Jy)\n    neg_lq = -lq\n    assert neg_lq != lq\n    assert neg_lq.unit.physical_unit == u.Jy**-1\n    assert -neg_lq == lq\n    pos_lq = +lq\n    assert pos_lq is not lq\n    assert pos_lq == lq\n", "def test_logarithmic_quantities_physical_unit_conversion():\n    \"\"\"Check that conversion between different physical units is possible.\"\"\"\n    lq1 = u.Magnitude(np.arange(1., 10.) * u.Jy)\n    lq2 = lq1.to(u.STmag)\n    assert lq2.unit == u.STmag\n    assert np.allclose(lq2.value, lq1.value + 8.9)\n\n    lq3 = lq1.to(u.ABmag)\n    assert lq3.unit == u.ABmag\n    assert np.allclose(lq3.value, lq1.value + 48.6)\n", "def test_inverse(self):\n    lq = u.Magnitude(np.arange(1., 10.) * u.Jy)\n    lq_inv = u.Quantity(1 / lq)\n    assert isinstance(lq_inv, u.Magnitude)\n    assert lq_inv.unit.function_unit == u.mag\n    assert lq_inv.unit.physical_unit == lq.unit.physical_unit ** -1\n    assert_allclose(lq_inv.to_value(u.Jy ** -1), 1 / lq.to_value(u.Jy))\n"], "sample_866": ["def test_affinity_propagation_single_sample():\n    # Test AffinityPropagation with a single sample\n    X = np.array([[0, 0]])\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X)\n    assert_array_equal(labels, np.array([0]))\n    assert_array_equal(af.cluster_centers_indices_, np.array([0]))\n    assert_array_equal(af.cluster_centers_, X)\n", "def test_affinity_propagation_preference_array_shape_mismatch():\n    # Test ValueError is raised when preference is an array of shape different from n_samples\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    preference = np.array([1, 2, 3, 4])\n    with pytest.raises(ValueError):\n        affinity_propagation(X, preference=preference)\n", "def test_affinity_propagation_precomputed_input_validation():\n    # Test exception in AffinityPropagation when precomputed input is not a square matrix\n    X = np.array([[0, 0, 0], [1, 1, 1]])\n    af = AffinityPropagation(affinity=\"precomputed\")\n    with pytest.raises(ValueError):\n        af.fit(X)\n\n    # Test exception in AffinityPropagation when precomputed input is not symmetric\n    X = np.array([[0, 1], [2, 0]])\n    af = AffinityPropagation(affinity=\"precomputed\")\n    with pytest.raises(ValueError):\n        af.fit(X)\n", "def test_affinity_propagation_degenerate_case():\n    # Degenerate case where all samples have equal similarities.\n    X = np.array([[0, 0], [0, 0], [0, 0]])\n    S = -euclidean_distances(X, squared=True)\n\n    # Setting preference < similarity\n    cluster_center_indices, labels = assert_warns_message(\n        UserWarning, \"mutually equal\", affinity_propagation, S, preference=-1)\n\n    # Expect one cluster, with arbitrary (first) sample as exemplar\n    assert_array_equal([0], cluster_center_indices)\n    assert_array_equal([0, 0, 0], labels)\n\n    # Setting preference > similarity\n    cluster_center_indices, labels = assert_warns_message(\n        UserWarning, \"mutually equal\", affinity_propagation, S, preference=1)\n\n    # Expect every sample to become an exemplar\n    assert_array_equal([0, 1, 2], cluster_center_indices)\n    assert_array_equal([0, 1, 2], labels)\n", "def test_affinity_propagation_with_different_damping():\n    # Test AffinityPropagation with different damping values\n    af = AffinityPropagation(damping=0.7)\n    labels_damped = af.fit(X).labels_\n    af = AffinityPropagation(damping=0.9)\n    labels_less_damped = af.fit(X).labels_\n    assert not np.array_equal(labels_damped, labels_less_damped)\n"], "sample_342": ["def test_multiple_to_field_resolution_with_mti(self):\n    \"\"\"\n    to_field resolution should correctly resolve for target models using\n    MTI with multiple levels of inheritance.\n    \"\"\"\n    tests = [\n        (Employee, Manager, 'employee_ptr'),\n        (Manager, Bonus, 'recipient_ptr'),\n    ]\n    for Target, Remote, related_name in tests:\n        with self.subTest(target_model=Target, remote_model=Remote, related_name=related_name):\n            o = Target.objects.create(name=\"Vincent Van Gogh\", gender=1, code=\"painter\", alive=False)\n            opts = {\n                'app_label': Remote._meta.app_label,\n                'model_name': Remote._meta.model_name,\n                'field_name': related_name,\n            }\n            request = self.factory.get(self.url, {'term': 'vincent', **opts})\n            request.user = self.superuser\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n            self.assertEqual(response.status_code, 200)\n            data = json.loads(response.content.decode('utf-8'))\n            self.assertEqual(data, {\n                'results': [{'id': str(o.pk), 'text': o.name}],\n                'pagination': {'more': False},\n            })\n", "def test_serialize_result_custom_to_field(self):\n    custom_to_field = 'custom_id'\n    Author.objects.create(name='Author 1', custom_id='C1')\n    Author.objects.create(name='Author 2', custom_id='C2')\n    opts = {\n        'app_label': Authorship._meta.app_label,\n        'model_name': Authorship._meta.model_name,\n        'field_name': 'author',\n    }\n    request = self.factory.get(self.url, {'term': 'Author', **opts})\n    request.user = self.superuser\n\n    class CustomToFieldAutocompleteJsonView(AutocompleteJsonView):\n            return {\n                'id': str(getattr(obj, custom_to_field)),\n                'text': obj.name,\n            }\n\n    response = CustomToFieldAutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data, {\n        'results': [\n            {'id': 'C1', 'text': 'Author 1'},\n            {'id': 'C2', 'text': 'Author 2'},\n        ],\n        'pagination': {'more': False},\n    })\n", "def test_to_field_resolution_with_foreign_key(self):\n    \"\"\"\n    to_field resolution should correctly resolve for target models using\n    a ForeignKey field.\n    \"\"\"\n    a = Author.objects.create(name=\"Leo Tolstoy\")\n    b = Book.objects.create(title=\"War and Peace\")\n    Authorship.objects.create(author=a, book=b)\n    opts = {\n        'app_label': Book._meta.app_label,\n        'model_name': Book._meta.model_name,\n        'field_name': 'authorship__author',\n    }\n    request = self.factory.get(self.url, {'term': 'leo', **opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data, {\n        'results': [{'id': str(a.pk), 'text': a.name}],\n        'pagination': {'more': False},\n    })\n", "def test_superuser_permission(self):\n    \"\"\"\n    Superusers can access the autocomplete view without the view or change permissions.\n    \"\"\"\n    Question.objects.create(question='Is this a question?')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n", "def test_get_queryset_with_related_filter(self):\n    \"\"\"\n    Ensure get_queryset() returns only related objects when a related filter is used.\n    \"\"\"\n    author1 = Author.objects.create(name='Author 1')\n    author2 = Author.objects.create(name='Author 2')\n    book1 = Book.objects.create(title='Book 1')\n    book2 = Book.objects.create(title='Book 2')\n    book1.authors.add(author1)\n    book2.authors.add(author2)\n\n    request = self.factory.get(self.url, {'term': 'author', 'app_label': 'admin_views', 'model_name': 'author', 'field_name': 'authors'})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(len(data['results']), 2)\n    self.assertEqual(data['results'][0]['text'], 'Author 1')\n    self.assertEqual(data['results'][1]['text'], 'Author 2')\n"], "sample_61": ["    def test_ascii_validator_messages(self):\n        invalid_username = \"o'connell\"\n        v = validators.ASCIIUsernameValidator()\n        with self.assertRaises(ValidationError) as cm:\n            v(invalid_username)\n        self.assertEqual(\n            cm.exception.message,\n            \"Enter a valid username. This value may contain only English letters, \"\n            \"numbers, and @/./+/-/_ characters.\"\n        )\n        self.assertEqual(cm.exception.error_list[0].code, 'invalid')\n", "    def test_ascii_validator(self):\n        valid_usernames = ['glenn', 'GLEnN', 'jean-marc']\n        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", 'trailingnewline\\n']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_validate(self):\n        valid_usernames = ['glenn', 'GLEnN', 'jean-marc', 'user.name+tag', 'user@example', 'user_name']\n        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", 'trailingnewline\\n', 'user,name', 'user:name', 'user;name']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['glenn', 'GLEnN', 'jean-marc', 'user.name', 'user+name', 'user-name', 'user_name']\n        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", 'trailingnewline\\n', 'user@name']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_validate(self):\n        valid_usernames = ['glenn', 'GLEnN', 'jean-marc', 'joe.smith+test@example.com', 'joe_test']\n        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", 'trailingnewline\\n', 'non@scii#char']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_1179": ["def test_MatPow():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert str(A**3) == \"A**3\"\n", "def test_MatPow():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert str(A**2) == \"A**2\"\n    assert str(A**-1) == \"A**(-1)\"\n", "def test_Predicate_printing():\n    assert sstr(Q.even(x)) == 'Q.even(x)'\n    assert sstr(Q.odd(x)) == 'Q.odd(x)'\n    assert sstr(Q.integer(x)) == 'Q.integer(x)'\n    assert sstr(Q.positive(x)) == 'Q.positive(x)'\n    assert sstr(Q.negative(x)) == 'Q.negative(x)'\n", "def test_issue_22021():\n    assert sstr(Mul(Pow(x,-2, evaluate=False), Pow(3,-1,evaluate=False), evaluate=False), full_prec=True) == \"0.333333333333333*x**(-2)\"\n", "def test_issue_23684():\n    assert sstr(MatrixSymbol(\"A\", 2, 2), sympy_integers=True) == \"A\"\n    assert sstr(MatrixSlice(MatrixSymbol(\"A\", 2, 2), (S(0), None, None), (S(0), None, None)), sympy_integers=True) == \"A[:, :]\"\n"], "sample_201": ["def test_empty_cookie(self):\n    \"\"\"\n    If there is no cookie data, the _get method should return an empty list of messages.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    messages, all_retrieved = storage._get()\n    self.assertEqual(messages, [])\n    self.assertTrue(all_retrieved)\n", "def test_encode_decode_empty(self):\n    \"\"\"\n    The encoder and decoder should handle empty messages correctly.\n    \"\"\"\n    storage = self.get_storage()\n    # Encode and decode an empty message list.\n    encoded_empty = storage._encode([])\n    decoded_empty = storage._decode(encoded_empty)\n    self.assertEqual(decoded_empty, [])\n\n    # Encode and decode an empty message list with encode_empty=True.\n    encoded_empty_explicit = storage._encode([], encode_empty=True)\n    decoded_empty_explicit = storage._decode(encoded_empty_explicit)\n    self.assertEqual(decoded_empty_explicit, [])\n", "def test_max_cookie_length_remove_newest(self):\n    \"\"\"\n    If the data exceeds what is allowed in a cookie, newest messages are\n    removed before saving if remove_oldest is False.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 4 messages into the cookie, but not 5.\n    # See also FallbackTest.test_session_fallback\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        storage.add(constants.INFO, str(i) * msg_size)\n    unstored_messages = storage.update(response, remove_oldest=False)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, '4' * msg_size)\n", "def test_legacy_hash_decode_invalid(self):\n    # Test that decoding an invalid message using the legacy hash returns None\n    storage = self.storage_class(self.get_request())\n    invalid_encoded_messages = 'invalidhash$invalidmessage'\n    decoded_messages = storage._decode(invalid_encoded_messages)\n    self.assertIsNone(decoded_messages)\n", "def test_cookie_size_limit(self):\n    \"\"\"\n    If the data exceeds what is allowed in a cookie, remaining messages are\n    not lost after saving (and are returned in the next request).\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 4 messages into the cookie, but not 5.\n    # See also FallbackTest.test_session_fallback\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        storage.add(constants.INFO, str(i) * msg_size)\n    storage.update(response)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    # Clear loaded data and retrieve messages again\n    if hasattr(storage, '_loaded_data'):\n        del storage._loaded_data\n    messages = list(storage)\n\n    self.assertEqual(len(messages), 1)\n    self.assertEqual(messages[0].message, '0' * msg_size)\n"], "sample_429": ["def test_url_validator_schemes(self):\n    url_validator = URLValidator(schemes=[\"http\", \"https\"])\n    self.assertIsNone(url_validator(\"http://example.com\"))\n    self.assertIsNone(url_validator(\"https://example.com\"))\n    with self.assertRaises(ValidationError):\n        url_validator(\"ftp://example.com\")\n    with self.assertRaises(ValidationError):\n        url_validator(\"git://example.com\")\n", "compilation error", "compilation error", "    def test_url_validator_schemes(self):\n        v = URLValidator(schemes=[\"http\", \"https\"])\n        with self.assertRaises(ValidationError):\n            v(\"ftp://example.com/\")\n        with self.assertRaises(ValidationError):\n            v(\"file://localhost/path\")\n        v(\"http://example.com/\")\n        v(\"https://example.com/\")\n", "    def test_prohibit_null_characters_validator_custom_message_and_code(self):\n        validator = ProhibitNullCharactersValidator(message=\"Custom error message\", code=\"custom_code\")\n        with self.assertRaisesMessage(ValidationError, \"Custom error message\"):\n            validator(\"\\x00something\")\n        with self.assertRaises(ValidationError) as cm:\n            validator(\"\\x00something\")\n        self.assertEqual(cm.exception.code, \"custom_code\")\n"], "sample_540": ["def test_no_init_func_warning(anim):\n    frames = []\n    match_target = \"Can not start iterating the frames for the initial draw.\"\n    with pytest.warns(UserWarning, match=match_target):\n        anim = animation.FuncAnimation(**{**anim, 'frames': frames})\n    anim._init_draw()\n", "def test_funcanimation_kwargs(anim):\n        assert isinstance(frame, int)\n        assert a == 1\n        assert b == 2\n        assert c == 3\n        return []\n\n    with pytest.raises(TypeError, match=\"func() missing 1 required positional argument: 'c'\"):\n        animation.FuncAnimation(**anim, func=func, fargs=(1, 2))\n\n    anim = animation.FuncAnimation(**anim, func=func, fargs=(1, 2, 3))\n    anim._init_draw()\n", "def test_no_frames(anim):\n    with pytest.warns(UserWarning, match=\"exhausted\"):\n        anim._start()\n", "def test_save_count_override_warnings_generator(anim):\n    save_count = 5\n    frames = lambda: iter(range(2))\n    match_target = (\n        f'You passed in an explicit {save_count=} '\n        \"which is being ignored in favor of \"\n        f\"{save_count=}.\"\n    )\n\n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames, 'save_count': save_count}\n        )\n\n    assert anim._save_count == save_count\n    anim._init_draw()\n", "def test_movie_writer_invalid_format(anim):\n    with pytest.raises(ValueError, match=\"Invalid format 'invalid'\"):\n        _ = anim.save(\"test.invalid\", writer=animation.FFMpegFileWriter())\n"], "sample_395": ["def test_django_path_excluded(self):\n    self.assertEqual(autoreload.get_template_directories(), set())\n", "    def test_template_dirs_ignore_nonexistent_path(self):\n        self.assertEqual(autoreload.get_template_directories(), set())\n", "    def test_custom_loader_support(self, mock_custom_loader_reset, mock_custom_loader_get_dirs):\n        mock_custom_loader_get_dirs.return_value = [EXTRA_TEMPLATES_DIR]\n        template_path = EXTRA_TEMPLATES_DIR / \"custom_template.html\"\n\n        self.assertTrue(autoreload.template_changed(None, template_path))\n        mock_custom_loader_reset.assert_called_once()\n", "def test_template_dirs_empty(self, mock_reset):\n    self.assertEqual(autoreload.get_template_directories(), set())\n    mock_reset.assert_not_called()\n", "def test_jinja2_template_changed(self):\n    template_path = EXTRA_TEMPLATES_DIR / \"index.html\"\n    self.assertTrue(autoreload.template_changed(None, template_path))\n    self.assertEqual(self.mock_reset.call_count, 1)\n"], "sample_1191": ["def test_hermite_normal_modulo_D():\n    m = DM([[2, 7, 17, 29, 41], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    hnf = DM([[1, 0, 0], [0, 2, 1], [0, 0, 1]], ZZ)\n    assert _hermite_normal_form_modulo_D(m, ZZ(96)) == hnf\n", "def test_hermite_normal_zero_matrix():\n    m = DM([[0, 0], [0, 0]], ZZ)\n    hnf = DM([], (2, 0), ZZ)\n    assert hermite_normal_form(m) == hnf\n", "def test_hermite_normal_failure_cases():\n    m = DM([[1, 0], [0, 1]], ZZ)\n    hnf = DM([[1, 0], [0, 1]], ZZ)\n    assert hermite_normal_form(m, D=ZZ(0)) == hnf  # D should be positive\n    raises(DMDomainError, lambda: hermite_normal_form(m, D=QQ(2)))  # D should be an integer\n", "def test_hermite_normal_modulo_D():\n    m = DM([[8, 28, 68, 116, 164], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    hnf = DM([[4, 0, 0], [0, 2, 1], [0, 0, 1]], ZZ)\n    assert hermite_normal_form(m, D=ZZ(8)) == hnf\n", "def test_hermite_normal_modulo_D():\n    m = DM([[8, 28, 68, 116, 164], [3, 11, 19, 31, 43], [5, 13, 23, 37, 47]], ZZ)\n    hnf = DM([[4, 0, 0], [0, 2, 1], [0, 0, 1]], ZZ)\n    assert _hermite_normal_form_modulo_D(m, ZZ(8)) == hnf\n    assert hermite_normal_form(m, D=ZZ(8), check_rank=True) == hnf\n\n    m = DM([[10, 8, 6, 30, 2], [45, 36, 27, 18, 9], [5, 4, 3, 2, 1]], ZZ)\n    hnf = DM([[26, 2], [0, 9], [0, 1]], ZZ)\n    assert _hermite_normal_form_modulo_D(m, ZZ(3250)) == hnf\n    assert hermite_normal_form(m, D=ZZ(3250), check_rank=True) == hnf\n"], "sample_877": ["def test_isotonic_regression_output_transform():\n    \"\"\"Check that `transform` does return the expected output type.\n\n    We need to check that `transform` will output a DataFrame and a NumPy array\n    when we set `transform_output` to `pandas`.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/25499\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n    regressor = IsotonicRegression()\n    with sklearn.config_context(transform_output=\"pandas\"):\n        regressor.fit(X, y)\n        X_trans = regressor.transform(X)\n\n    assert isinstance(X_trans, pd.DataFrame)\n    assert isinstance(regressor.transform(X), np.ndarray)\n", "def test_isotonic_regression_increasing_auto():\n    # Test the auto-detection of increasing parameter\n    X = np.array([0, 1, 2, 3, 4, 5])\n    y_increasing = np.array([0, 1, 2, 3, 4, 5])\n    y_decreasing = np.array([5, 4, 3, 2, 1, 0])\n\n    ir_increasing = IsotonicRegression(increasing=\"auto\").fit(X, y_increasing)\n    ir_decreasing = IsotonicRegression(increasing=\"auto\").fit(X, y_decreasing)\n\n    assert ir_increasing.increasing_ is True\n    assert ir_decreasing.increasing_ is False\n", "def test_isotonic_regression_increasing_auto_decimal_values():\n    # Set y and x for increasing with decimal values\n    y = np.array([5.1, 6.2, 7.3, 8.4, 9.5, 10.6])\n    x = np.arange(len(y))\n\n    # Create model and fit_transform\n    ir = IsotonicRegression(increasing=\"auto\")\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        y_ = ir.fit_transform(x, y)\n        # work-around for pearson divide warnings in scipy <= 0.17.0\n        assert all([\"invalid value encountered in \" in str(warn.message) for warn in w])\n\n    # Check that relationship increases\n    is_increasing = y_[0] < y_[-1]\n    assert is_increasing\n", "def test_isotonic_regression_increasing_false():\n    # Test for increasing=False in IsotonicRegression\n    rng = np.random.RandomState(42)\n    n_samples = 20\n    X = np.sort(rng.uniform(size=n_samples))\n    y = np.sort(rng.uniform(size=n_samples))[::-1]\n    ireg = IsotonicRegression(increasing=False).fit(X, y)\n    y_pred = ireg.predict(X)\n    assert_array_equal(y_pred, y[::-1])\n", "def test_isotonic_regression_sample_weight_zero():\n    \"\"\"Check that IsotonicRegression handles sample weights of zero correctly.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/21007\n    \"\"\"\n    X = np.array([0, 1, 2, 3, 4, 5])\n    y = np.array([0, 0, 0, 0, 0, 0])\n    sample_weight = np.array([0, 0, 0, 1, 0, 0])\n\n    ir = IsotonicRegression().fit(X, y, sample_weight=sample_weight)\n    y_pred = ir.predict(X)\n\n    assert_array_equal(y_pred, [0, 0, 0, 0, 0, 0])\n"], "sample_920": ["    def test_attributes_without_type(self):\n        docstring = \"\"\"\\", "def test_code_block_in_parameters_section(self):\n    docstring = \"\"\"", "    def test_method_with_class_reference(self):\n        docstring = \"\"\"\\", "    def test_keywords_with_types(self):\n        docstring = \"\"\"\\", "def test_strip_signature_backslash(self):\n    docstring = \"\"\""], "sample_299": ["def test_non_file_based_cache(self):\n    with self.settings(CACHES={\n        'default': {\n            'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            'LOCATION': pathlib.Path.cwd() / 'cache',\n        },\n    }):\n        self.assertEqual(check_file_based_cache_is_absolute(None), [])\n", "    def test_no_warning_for_non_filebased_cache(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n            'MEDIA_ROOT': pathlib.Path.cwd() / 'media',\n            'STATIC_ROOT': pathlib.Path.cwd() / 'static',\n        }):\n            self.assertEqual(check_cache_location_not_exposed(None), [])\n", "    def test_non_file_based_cache_backend(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n        }):\n            self.assertEqual(check_cache_location_not_exposed(None), [])\n", "    def test_cache_path_not_conflict_with_multiple_staticfiles_dirs(self):\n        root = pathlib.Path.cwd()\n        settings = {\n            'CACHES': {\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': root / 'cache',\n                },\n            },\n            'STATICFILES_DIRS': [root / 'other1', root / 'other2'],\n        }\n        with self.settings(**settings):\n            self.assertEqual(check_cache_location_not_exposed(None), [])\n", "    def test_non_file_based_cache_backend(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n"], "sample_73": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        self._file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n            ('xyz.png', 'xyz'),\n        )\n        for filename, content in self._file_contents:\n            with open(os.path.join(temp_dir, filename), 'w') as f:\n                f.write(content)\n        self.patched_settings = self.settings(\n            STATICFILES_DIRS=settings.STATICFILES_DIRS + [temp_dir],\n        )\n        self.patched_settings.enable()\n        self.addCleanup(shutil.rmtree, temp_dir)\n        self._manifest_strict = storage.staticfiles_storage.manifest_strict\n", "    def test_template_tag_non_ascii_content(self):\n        relpath = self.hashed_file_path(\"test/nonascii.css\")\n        self.assertEqual(relpath, \"test/nonascii.25b0925a052f.css\")\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read().decode('utf-8')\n            self.assertIn('url(\"nonascii_characters.d41d8cd98f00.png\")', content)\n        self.assertPostCondition()\n", "    def test_manifest_invalid_version(self):\n        invalid_manifest = {\n            'version': '2.0',\n            'paths': {'cached/styles.css': 'cached/styles.5e0040571e1a.css'}\n        }\n        storage.staticfiles_storage.save_manifest()\n        with open(storage.staticfiles_storage.path('staticfiles.json'), 'w') as f:\n            json.dump(invalid_manifest, f)\n\n        err_msg = \"Couldn't load manifest 'staticfiles.json' (version 1.0)\"\n        with self.assertRaisesMessage(ValueError, err_msg):\n            storage.staticfiles_storage.load_manifest()\n", "    def setUp(self):\n        super().setUp()\n        self.temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(self.temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, self.temp_dir)\n\n        # Create a file with intermediate files\n        self.intermediate_file_name = 'test/intermediate.css'\n        self.intermediate_file_content = '@import url(\"other.css\");\\nbody { background: url(\"image.png\"); }'\n        self.intermediate_file_path = os.path.join(self.temp_dir, self.intermediate_file_name)\n        with open(self.intermediate_file_path, 'w') as f:\n            f.write(self.intermediate_file_content)\n\n        self.patched_settings = self.settings(\n            STATICFILES_DIRS=settings.STATICFILES_DIRS + [self.temp_dir],\n        )\n        self.patched_settings.enable()\n", "    def test_manifest_entry_without_extension(self):\n        \"\"\"\n        Test that a manifest entry without an extension raises a ValueError\n        \"\"\"\n        missing_file_name = 'cached/missing'\n        configured_storage = storage.staticfiles_storage\n        configured_storage.manifest_strict = False\n\n        # File doesn't exist on disk\n        err_msg = \"The file '%s' could not be found with %r.\" % (missing_file_name, configured_storage._wrapped)\n        with self.assertRaisesMessage(ValueError, err_msg):\n            self.hashed_file_path(missing_file_name)\n\n        # File exists on disk\n        content = StringIO()\n        content.write('Found')\n        configured_storage.save(missing_file_name, content)\n\n        # File name without extension in manifest should raise a ValueError\n        err_msg = \"Missing staticfiles manifest entry for '%s'\" % missing_file_name\n        with self.assertRaisesMessage(ValueError, err_msg):\n            self.hashed_file_path(missing_file_name)\n\n        # Clean up\n        configured_storage.delete(missing_file_name)\n"], "sample_808": ["def test_iforest_contamination_error():\n    X = [[0.0], [1.0]]\n    clf = IsolationForest(behaviour='old')\n    clf.fit(X)\n    with pytest.raises(ValueError, match=\"contamination parameter cannot be set to 'auto' when behaviour == 'old'.\"):\n        clf.set_params(contamination='auto')\n        clf.fit(X)\n", "def test_iforest_sample_weight():\n    \"\"\"Test Isolation Forest with sample weights.\"\"\"\n    X = np.array([[0, 1], [1, 2]])\n    sample_weight = np.array([1, 2])\n\n    # Test with equal sample weights\n    clf = IsolationForest(random_state=rng).fit(X)\n    scores_equal = clf.decision_function(X)\n\n    # Test with unequal sample weights\n    clf = IsolationForest(random_state=rng).fit(X, sample_weight=sample_weight)\n    scores_unequal = clf.decision_function(X)\n\n    # Scores should be different for weighted and unweighted data\n    assert not np.allclose(scores_equal, scores_unequal)\n", "def test_iforest_sample_weight():\n    \"\"\"Test Isolation Forest with sample weights\"\"\"\n\n    # Generate train/test data\n    rng = check_random_state(2)\n    X = 0.3 * rng.randn(120, 2)\n    X_train = np.r_[X + 2, X - 2]\n    X_train = X[:100]\n\n    # Generate some abnormal novel observations\n    X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n    X_test = np.r_[X[100:], X_outliers]\n    y_test = np.array([0] * 20 + [1] * 20)\n\n    # Assign higher weights to outliers\n    sample_weight = np.ones(len(X_train))\n    sample_weight[100:] = 2\n\n    # fit the model with sample weights\n    clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train, sample_weight=sample_weight)\n\n    # predict scores (the lower, the more normal)\n    y_pred = - clf.decision_function(X_test)\n\n    # check that there is at most 6 errors (false positive or false negative)\n    assert_greater(roc_auc_score(y_test, y_pred), 0.98)\n", "def test_iforest_sample_weight():\n    \"\"\"Test Isolation Forest with sample weights.\"\"\"\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n    sample_weight = rng.rand(X_train.shape[0])\n    clf1 = IsolationForest(random_state=rng).fit(X_train, sample_weight=sample_weight)\n    clf2 = IsolationForest(random_state=rng).fit(X_train)\n    assert_array_almost_equal(clf1.predict(X_test), clf2.predict(X_test), decimal=3)\n", "def test_iforest_sample_weight():\n    \"\"\"Test Isolation Forest with sample weights\"\"\"\n\n    # Generate train/test data\n    rng = check_random_state(2)\n    X = 0.3 * rng.randn(120, 2)\n    X_train = np.r_[X + 2, X - 2]\n    X_train = X[:100]\n\n    # Generate some abnormal novel observations\n    X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n    X_test = np.r_[X[100:], X_outliers]\n    y_test = np.array([0] * 20 + [1] * 20)\n\n    # Sample weights for training data\n    sample_weight = np.ones(X_train.shape[0])\n    sample_weight[::2] = 2  # Double weight for every other sample\n\n    # fit the model with sample weights\n    clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train, sample_weight=sample_weight)\n\n    # predict scores (the lower, the more normal)\n    y_pred = - clf.decision_function(X_test)\n\n    # check that there is at most 6 errors (false positive or false negative)\n    assert_greater(roc_auc_score(y_test, y_pred), 0.98)\n"], "sample_548": ["def test_colorbar_labelpad():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cbar = fig.colorbar(im, label='cbar', labelpad=10)\n    assert cbar.ax.get_ylabel() == 'cbar'\n    assert cbar.ax.yaxis.labelpad == 10\n", "def test_colorbar_set_alpha():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.arange(100).reshape(10, 10), alpha=0.5)\n    cb = fig.colorbar(im)\n    assert cb.alpha == 0.5\n    cb.set_alpha(0.8)\n    assert cb.alpha == 0.8\n    cb.set_alpha(None)\n    assert cb.alpha is None\n", "def test_colorbar_lognorm_extendfrac():\n    # Test that colorbar with lognorm and extendfrac is handled correctly\n    fig, ax = plt.subplots()\n    cb = Colorbar(ax, norm=LogNorm(vmin=0.1, vmax=1000.0),\n                  orientation='vertical', extend='both', extendfrac=0.2)\n    assert cb._values[0] >= 0.0\n", "def test_colorbar_extend_locator():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    im = ax.contourf(data, levels=levels, extend='both')\n    cbar = fig.colorbar(im, ticks=levels, extendfrac=0.1)\n    ticks = cbar.ax.yaxis.get_major_ticks()\n    assert len(ticks) == len(levels) + 2  # +2 for the extended ticks\n    assert ticks[0].get_loc() == levels[0] - 200  # extend lower\n    assert ticks[-1].get_loc() == levels[-1] + 200  # extend upper\n", "def test_colorbar_extend_inverted_axis(orientation, extend):\n    \"\"\"Test extension color with an inverted axis\"\"\"\n    data = np.arange(12).reshape(3, 4)\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps[\"viridis\"].with_extremes(under=(0, 0, 0, 1),\n                                                  over=(1, 1, 1, 1))\n    im = ax.imshow(data, cmap=cmap)\n    cbar = fig.colorbar(im, orientation=orientation, extend=extend)\n    if orientation == \"horizontal\":\n        cbar.ax.invert_xaxis()\n    else:\n        cbar.ax.invert_yaxis()\n    assert len(cbar._extend_patches) == 1\n    if extend == \"min\":\n        assert cbar._extend_patches[0].get_facecolor() == (0, 0, 0, 1)\n    else:\n        assert cbar._extend_patches[0].get_facecolor() == (1, 1, 1, 1)\n"], "sample_950": ["def test_pyproperty_with_union_type_operator(app):\n    text = \".. py:property:: prop\\n      :type: int | str\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0],\n                ([desc_annotation, \"property \"],\n                 [desc_name, \"prop\"],\n                 [desc_annotation, (\": \",\n                                    [pending_xref, \"int\"],\n                                    \" \",\n                                    [desc_sig_punctuation, \"|\"],\n                                    \" \",\n                                    [pending_xref, \"str\"])]))\n", "def test_domain_py_xrefs_abbreviations_no_module(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'abbr_no_module.html').read_text()\n    assert re.search(r'normal: <a .* href=\"roles.html#NestedParentA.NestedChildA.subchild_1\" .*>'\n                     r'<.*>NestedParentA.NestedChildA.subchild_1\\(\\)<.*></a>',\n                     content)\n    assert re.search(r'relative: <a .* href=\"roles.html#NestedParentA.NestedChildA.subchild_1\" .*>'\n                     r'<.*>NestedChildA.subchild_1\\(\\)<.*></a>',\n                     content)\n    assert re.search(r'short name: <a .* href=\"roles.html#NestedParentA.NestedChildA.subchild_1\" .*>'\n                     r'<.*>subchild_1\\(\\)<.*></a>',\n                     content)\n    assert re.search(r'relative \\+ short name: <a .* href=\"roles.html#NestedParentA.NestedChildA.subchild_1\" .*>'\n                     r'<.*>subchild_1\\(\\)<.*></a>',\n                     content)\n    assert re.search(r'short name \\+ relative: <a .* href=\"roles.html#NestedParentA.NestedChildA.subchild_1\" .*>'\n                     r'<.*>subchild_1\\(\\)<.*></a>',\n                     content)\n", "def test_pyattribute_with_union_type_operator(app):\n    text = (\".. py:attribute:: attr\\n\"\n            \"   :type: Optional[str | None]\\n\"\n            \"   :value: None\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"Optional\"],\n                                                                       [desc_sig_punctuation, \"[\"],\n                                                                       [pending_xref, \"str\"],\n                                                                       [desc_sig_punctuation, \" | \"],\n                                                                       [pending_xref, \"None\"],\n                                                                       [desc_sig_punctuation, \"]\"])],\n                                                    [desc_annotation, \" = None\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1][0], ([desc_name, \"attr\"],\n                                [desc_annotation, (\": \",\n                                                   [pending_xref, \"Optional\"],\n                                                   [desc_sig_punctuation, \"[\"],\n                                                   [pending_xref, \"str\"],\n                                                   [desc_sig_punctuation, \" | \"],\n                                                   [pending_xref, \"None\"],\n                                                   [desc_sig_punctuation, \"]\"])],\n                                [desc_annotation, \" = None\"]))\n    assert_node(doctree[1][0][1][3], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n    assert_node(doctree[1][0][1][5], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"None\")\n", "def test_pyfunction_with_positional_only_argument(app):\n    text = \".. py:function:: hello(name, /, age: int)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"name\"],\n                                      [desc_parameter, desc_sig_operator, \"/\"],\n                                      [desc_parameter, ([desc_sig_name, \"age\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"int\"])])])\n", "def test_resolve_xref_for_duplicate_objects(app, status, warning):\n    app.builder.build_all()\n\n    # Check that the correct reference is chosen when there are duplicate objects\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<a class=\"reference internal\" href=\"#duplicate-object\" title=\"duplicate-object\">'\n            '<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">'\n            'DuplicateObject</span></code></a>' in content)\n    assert warning.getvalue() == ''\n"], "sample_1094": ["def test_find():\n    x, y, z = symbols('x y z')\n    expr = x + y + z\n    assert expr.find(x) == {x}\n    assert expr.find(x, group=True) == {x: 1}\n    assert expr.find(x + y) == {x + y}\n    assert expr.find(x + y, group=True) == {x + y: 1}\n    assert expr.find(x + z) == set()\n    assert expr.find(x + z, group=True) == {}\n", "def test_replace_exact():\n    from sympy import Function, symbols\n    f = Function('f')\n    x = symbols('x')\n    e = f(1) + f(0)\n    q = f(x), lambda x: f(x + 1)\n    assert e.replace(*q, exact=False) == f(1) + f(2)\n    assert e.replace(*q, exact=True) == f(0) + f(2)\n", "def test_find():\n    x, y, z = symbols('x y z')\n    e = sin(x + y) + cos(x - z)\n    assert e.find(sin) == {sin(x + y)}\n    assert e.find(cos) == {cos(x - z)}\n    assert e.find(sin, group=True) == {sin(x + y): 1}\n    assert e.find(cos, group=True) == {cos(x - z): 1}\n    assert e.find(x) == {x}\n    assert e.find(x, group=True) == {x: 2}\n    assert e.find(x + y) == {x + y}\n    assert e.find(x + z) == set()\n    assert e.find(lambda expr: expr.is_Add) == {x + y, x - z}\n    assert e.find(lambda expr: expr.is_Add, group=True) == {x + y: 1, x - z: 1}\n", "def test_replace():\n    x, y, z = symbols('x y z')\n    e = (x + y)**(x + y)\n    assert e.replace(x + y, z) == z**z\n\n    p = Wild('p')\n    q = Wild('q')\n    assert e.replace(p**p, q**q) == (x + y)**(x + y)\n\n    assert (x - 2).replace(p - x, p - x, exact=False) == 2*x - 2\n    assert (2/x).replace(p*x, p*x, exact=False) == 2/x**2\n", "def test_replace_patterns():\n    x, y, z = symbols('x y z')\n    a, b = Wild('a'), Wild('b')\n\n    e = (x + y)**(x + y)\n    assert e.replace(a**a, b, map=True) == ((x + y)**(x + y), {(x + y): b})\n    assert e.replace(a**a, b) == b\n\n    e = (2*x)**2\n    assert e.replace(a*b**z, z*a*b, map=True) == ((2*x)**2, {(2*x): 2*(2*x)})\n    assert e.replace(a*b**z, z*a*b) == 2*(2*x)\n\n    e = x - 2\n    assert e.replace(a - x, b, map=True, exact=True) == (x - 2, {})\n    assert e.replace(a - x, b, exact=True) == x - 2\n\n    e = x**(1 + y)\n    assert e.replace(x**(a + 1), z**(-a - 1), map=True, exact=False) == (x**(1 + y), {})\n    assert e.replace(x**(a + 1), z**(-a - 1), exact=False) == x**(1 - y)\n\n    e = 2*x\n    assert e.replace(a*x, a + 1, map=True, exact=True) == (2*x, {})\n    assert e.replace(a*x, a + 1, exact=True) == 2*x\n"], "sample_822": ["def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    X = [[0], [0]]\n    Y = [[1], [2]]\n    D = paired_cosine_distances(X, Y)\n    assert_array_almost_equal(D, [0.5, 1.0])\n", "def test_paired_cosine_distances():\n    # Check the paired cosine distances computation\n    X = [[0], [0]]\n    Y = [[1], [2]]\n    D = paired_cosine_distances(X, Y)\n    assert_array_almost_equal(D, [0.5, 1.0])\n", "def test_check_paired_arrays():\n    # Ensure that if XA and XB are given correctly, they return as equal.\n    XA = np.resize(np.arange(40), (5, 8))\n    XB = np.resize(np.arange(40), (5, 8))\n    XA_checked, XB_checked = check_paired_arrays(XA, XB)\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)\n\n    # Ensure an error is raised if the dimensions are different.\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, XB)\n", "def test_pairwise_distances_precomputed():\n    # Test the pairwise_distance helper function with precomputed metric.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n    D = pairwise_distances(X, Y, metric=\"euclidean\")\n\n    # Test with precomputed metric\n    D_precomputed = pairwise_distances(D, Y, metric=\"precomputed\")\n    assert_array_almost_equal(D_precomputed, D)\n", "def test_check_different_dimensions_paired_arrays():\n    # Ensure an error is raised if the dimensions are different.\n    XA = np.resize(np.arange(45), (5, 9))\n    XB = np.resize(np.arange(32), (4, 8))\n    assert_raises(ValueError, check_paired_arrays, XA, XB)\n"], "sample_664": ["def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`*\"]\n    )\n", "def test_fixture_positional_arguments(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture('function')\n            pass\n\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them *\",\n        ]\n    )\n", "def test_fixture_positional_arguments_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(1, 2, 3)\n            pass\n\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\"\n        ]\n    )\n", "def test_funcargnames_is_deprecated(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n        @pytest.fixture\n            pass\n\n            if 'funcargnames' in metafunc.fixturenames:\n                raise ValueError(\"funcargnames is deprecated\")\n    \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n            assert 'funcargnames' not in fixture1.funcargnames\n    \"\"\"\n    )\n\n    with pytest.warns(deprecated.FUNCARGNAMES):\n        testdir.runpytest()\n", "def test_deprecated_warnings(pytestconfig, argnames, warning_message):\n    \"\"\"Check deprecated warnings for FUNCARGNAMES and FIXTURE_POSITIONAL_ARGUMENTS.\"\"\"\n    with pytest.warns(pytest.PytestDeprecationWarning, match=warning_message):\n        pytestconfig.funcargnames = argnames\n"], "sample_1086": ["def test_Inverse():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert str(A**-1) == \"A**(-1)\"\n    assert str(Inverse(A)) == \"A**(-1)\"\n", "def test_DMP_printing():\n    from sympy.polys import DMP, ZZ\n    dmp = DMP([[1, 2], [3, 4]], 3, ZZ)\n    assert str(dmp) == 'DMP([[1, 2], [3, 4]], ZZ, lex)'\n", "def test_issue_14805():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    expr = Tr(A*B)\n    assert str(expr) == 'Tr(A*B)'\n", "def test_issue_16963():\n    A, B = symbols('A B', commutative=False)\n    expr = log(A * B * A**-1)\n    assert str(expr) == 'log(A*B) + log(A**(-1))'\n", "def test_issue_14567():\n    # Additional test for issue #14567 to cover more scenarios\n    assert factorial(Sum(0, (x, 0, 0))) + y == y\n    assert factorial(Sum(1, (x, 0, 0))) + y == y + 1\n    assert factorial(Sum(x, (x, 0, 0))) + y == y\n"], "sample_624": ["def test_format_timedelta_invalid_pandas_format() -> None:\n    expected = \"10 days 1 hour 00\"\n    with pytest.raises(ValueError):\n        formatting.format_timedelta(expected)\n", "def test_nondefault_indexes() -> None:\n    from xarray.core.indexes import Index\n\n    class CustomIndex(Index):\n            self.names = names\n\n    indexes = {\n        \"default\": np.array([1, 2, 3]),\n        \"custom\": CustomIndex([\"x\", \"y\"]),\n    }\n\n    expected = {\"custom\": CustomIndex([\"x\", \"y\"])}\n    actual = formatting.nondefault_indexes(indexes)\n    assert expected == actual\n", "def test_unindexed_dims_repr() -> None:\n    dims = [\"x\", \"y\", \"z\"]\n    coords = {\"x\": np.array([1, 2, 3])}\n    max_rows = 3\n\n    actual = formatting.unindexed_dims_repr(dims, coords, max_rows)\n    expected = \"Dimensions without coordinates: y, z\"\n    assert actual == expected\n\n    dims = [\"x\", \"y\", \"z\", \"w\", \"v\"]\n    coords = {\"x\": np.array([1, 2, 3]), \"y\": np.array([4, 5])}\n    max_rows = 2\n\n    actual = formatting.unindexed_dims_repr(dims, coords, max_rows)\n    expected = \"Dimensions without coordinates: z, ...\"\n    assert actual == expected\n\n    dims = [\"x\", \"y\", \"z\", \"w\", \"v\"]\n    coords = {\"x\": np.array([1, 2, 3]), \"y\": np.array([4, 5]), \"z\": np.array([6, 7, 8])}\n    max_rows = 3\n\n    actual = formatting.unindexed_dims_repr(dims, coords, max_rows)\n    expected = \"Dimensions without coordinates: w, v\"\n    assert actual == expected\n\n    dims = [\"x\", \"y\", \"z\", \"w\", \"v\"]\n    coords = {\"x\": np.array([1, 2, 3]), \"y\": np.array([4, 5]), \"z\": np.array([6, 7, 8]), \"w\": np.array([9, 10])}\n    max_rows = 2\n\n    actual = formatting.unindexed_dims_repr(dims, coords, max_rows)\n    expected = \"Dimensions without coordinates: v\"\n    assert actual == expected\n\n    dims = [\"x\"]\n    coords = {\"x\": np.array([1, 2, 3])}\n    max_rows = 1\n\n", "def test_diff_array_repr_with_memory_cached_array() -> None:\n    from xarray.core.indexing import MemoryCachedArray\n\n    a = xr.DataArray(np.array([1, 2, 3]), name=\"a\")\n    b = xr.DataArray(np.array([1, 2, 3]), name=\"a\")\n\n    # Apply MemoryCachedArray to one of the data arrays\n    a._data = MemoryCachedArray(a._data)\n\n    with xr.set_options(display_expand_data=True):\n        expected = dedent(\n            \"\"\"\\\n        Left and right DataArray objects are not identical\n        Differing values:\n        L\n            array([1, 2, 3], dtype=int64)\n        R\n            array([1, 2, 3], dtype=int64)\n        Differing data variables:\n        L   a     (dim_0) int64 1 2 3\n        R   a     (dim_0) int64 1 2 3\"\"\"\n        )\n        actual = formatting.diff_array_repr(a, b, \"identical\")\n        assert actual == expected\n", "def test_short_data_repr() -> None:\n    # Test numpy array\n    arr = np.array([1, 2, 3])\n    expected = \"[1 2 3]\"\n    assert formatting.short_data_repr(arr) == expected\n\n    # Test xarray DataArray\n    da = xr.DataArray([1, 2, 3], dims=\"x\")\n    expected = \"[3 values with dtype=int64]\"\n    assert formatting.short_data_repr(da) == expected\n\n    # Test dask array\n    import dask.array as da\n    darr = da.from_array(np.array([1, 2, 3]), chunks=2)\n    da_xr = xr.DataArray(darr)\n    expected = \"[3 values with dtype=int64]\"\n    assert formatting.short_data_repr(da_xr) == expected\n\n    # Test custom array with __array_function__\n    class CustomArray:\n            self.data = data\n\n            return NotImplemented\n\n    custom_arr = CustomArray(np.array([1, 2, 3]))\n    custom_da = xr.DataArray(custom_arr)\n    expected = \"<...>\"  # You can adjust this based on the actual output\n    assert formatting.short_data_repr(custom_da) == expected\n"], "sample_214": ["def test_key_transform_null_lookup(self):\n    obj = NullableJSONModel.objects.create(value={'a': None})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__isnull=True),\n        self.objs[:3] + self.objs[5:] + [obj],\n    )\n", "def test_key_transform_with_empty_string(self):\n    obj = NullableJSONModel.objects.create(value={'': 'empty key value'})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__has_key=''),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__='empty key value'),\n        [obj],\n    )\n", "def test_key_lt_gt_lte_gte(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lt=15),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gt=13),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lte=14),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gte=14),\n        [self.objs[3], self.objs[4]],\n    )\n", "def test_key_transform_exact(self):\n    tests = [\n        (KeyTransform('c', 'value'), 14),\n        (KeyTransform('f', KeyTransform('1', KeyTransform('d', 'value'))), 'g'),\n        (KeyTransform('0', KeyTransform('bar', 'value')), 'foo'),\n        (KeyTransform('foo', 'value'), 'bar'),\n        (KeyTransform('foo', 'value'), 'BaR'),  # Test case-sensitivity\n        (KeyTransform('baz', 'value'), {'a': 'b', 'c': 'd'}),\n        (KeyTransform('bar', 'value'), ['foo', 'bar']),\n        (KeyTransform('bax', 'value'), {'foo': 'bar'}),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertIs(NullableJSONModel.objects.filter(\n                **{lookup.name: value},\n            ).exists(), True)\n", "def test_key_transform_lookup_with_sql_injection(self):\n    with CaptureQueriesContext(connection) as queries:\n        self.assertIs(\n            NullableJSONModel.objects.filter(**{\n                \"\"\"value__test' = '\"a\"') OR 1 = 1 OR ('d\"\"\": KeyTransform('x', 'value'),\n            }).exists(),\n            False,\n        )\n    self.assertIn(\n        \"\"\".\"value\" #> '{\"test\\'\": \"x\"}) OR 1 = 1 OR (''d\"'\") = NULL \"\"\",\n        queries[0]['sql'],\n    )\n"], "sample_1033": ["def test_Mul_does_not_distribute_infinity_2():\n    a, b = symbols('a b')\n    assert ((1 + I)*oo*a).is_Mul\n    assert ((a + b)*(-oo*a)).is_Mul\n    assert ((a + 1)*zoo*a).is_Mul\n    assert ((1 + I)*oo*a).is_finite is False\n    z = (1 + I)*oo*a\n    assert ((1 - I)*z).expand() is oo*a\n", "def test_Mul_does_not_cancel_zeros():\n    a, b = symbols('a b')\n    assert ((a + b)/(a + b)) is not 1\n    assert ((b - a)/(b - a)) is not 1\n    # issue 13904\n    expr = (1/(a+b) + 1/(a-b))/(1/(a+b) - 1/(a-b))\n    assert expr.subs(b, a) is not 1\n", "def test_Mod_is_zero():\n    assert (x % x).is_zero\n    assert (x % 1).is_zero is None\n    assert (x % y).is_zero is None\n    assert (3 % x).is_zero is None\n", "def test_issue_15911():\n    assert Pow(2, 1, evaluate=False) == 2\n    assert Pow(2, 2, evaluate=False) == 4\n    assert Pow(2, 3, evaluate=False) == 8\n    assert Pow(2, -1, evaluate=False) == 0.5\n    assert Pow(2, -2, evaluate=False) == 0.25\n    assert Pow(2, -3, evaluate=False) == 0.125\n    assert Pow(2, x, evaluate=False) == 2**x\n    assert Pow(2, -x, evaluate=False) == 2**(-x)\n", "def test_Mul_is_hermitian():\n    a = Symbol('a', hermitian=True)\n    b = Symbol('b', hermitian=True)\n    c = Symbol('c', hermitian=False)\n    d = Symbol('d', antihermitian=True)\n    e1 = Mul(a, b, c, evaluate=False)\n    e2 = Mul(b, a, c, evaluate=False)\n    e3 = Mul(a, b, c, d, evaluate=False)\n    e4 = Mul(b, a, c, d, evaluate=False)\n    e5 = Mul(a, c, evaluate=False)\n    e6 = Mul(a, c, d, evaluate=False)\n    assert e1.is_hermitian is None\n    assert e2.is_hermitian\n    assert e1.is_antihermitian is None\n    assert e2.is_antihermitian is False\n    assert e3.is_antihermitian is None\n    assert e4.is_antihermitian\n    assert e5.is_antihermitian is None\n    assert e6.is_antihermitian\n"], "sample_1093": ["def test_NumPyPrinter_print_sinc():\n    from sympy import sinc\n\n    expr = sinc(x)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.sinc(x/numpy.pi)'\n", "def test_LogGamma():\n    from sympy import loggamma\n\n    expr = loggamma(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.gammaln(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'math.log(math.gamma(x))'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(math.gamma(x))'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.gammaln(x)'\n", "def test_issue_16537():\n    from sympy import log1p, log2\n\n    expr1 = log1p(x)\n    expr2 = log2(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.log1p(x)'\n    assert prntr.doprint(expr2) == 'scipy.special.log2(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(x + 1)'\n    assert prntr.doprint(expr2) == 'numpy.log(x)/numpy.log(2)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log1p(x)'\n    assert prntr.doprint(expr2) == 'math.log(x)/math.log(2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log1p(x)'\n    assert prntr.doprint(expr2) == 'mpmath.log(x)/mpmath.log(2)'\n", "def test_NumPyPrinter_log1p():\n    expr = log1p(x)\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n", "def test_additional_functions():\n    from sympy import digamma, LambertW, log1p, exp2\n\n    expr1 = digamma(x)\n    expr2 = LambertW(x)\n    expr3 = log1p(x)\n    expr4 = exp2(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.psi(x)'\n    assert prntr.doprint(expr2) == 'scipy.special.lambertw(x)'\n    assert prntr.doprint(expr3) == 'scipy.special.log1p(x)'\n    assert prntr.doprint(expr4) == 'scipy.special.exp2(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr3) == 'numpy.log1p(x)'\n    assert prntr.doprint(expr4) == '2**x'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr3) == 'math.log1p(x)'\n    assert prntr.doprint(expr4) == '2**x'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.psi(x)'\n    assert prntr.doprint(expr2) == 'mpmath.lambertw(x)'\n    assert prntr.doprint(expr3) == 'mpmath.log1p(x)'\n    assert prntr.doprint(expr4) == 'mpmath.exp2(x)'\n"], "sample_728": ["def test_make_classification_flip_y():\n    flip_y = 0.2\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=1, n_repeated=1, n_classes=3,\n                               n_clusters_per_class=1, hypercube=False,\n                               shift=None, scale=None, flip_y=flip_y,\n                               random_state=0)\n\n    # Check that the correct proportion of labels were flipped\n    assert_almost_equal(np.mean(y != make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=1, n_repeated=1, n_classes=3,\n                               n_clusters_per_class=1, hypercube=False,\n                               shift=None, scale=None, flip_y=0.0,\n                               random_state=0)[1]), flip_y, decimal=1)\n", "def test_make_classification_flip_y():\n    X, y = make_classification(n_samples=100, n_classes=2, flip_y=0.1, random_state=0)\n    assert_true(np.abs(np.mean(y) - 0.5) < 0.05, \"Unexpected class balance\")\n    assert_true(np.sum(y == 0) > 10, \"Unexpected number of samples in class #0\")\n    assert_true(np.sum(y == 1) > 10, \"Unexpected number of samples in class #1\")\n    assert_true(np.sum(y == 0) < 90, \"Unexpected number of samples in class #0\")\n    assert_true(np.sum(y == 1) < 90, \"Unexpected number of samples in class #1\")\n", "def test_make_classification_flip_y():\n    \"\"\"Test the flipping of labels in make_classification\"\"\"\n    n_samples = 1000\n    flip_y = 0.5\n    X, y = make_classification(n_samples=n_samples, n_classes=2,\n                               flip_y=flip_y, random_state=0)\n\n    assert_equal(X.shape, (n_samples, 20))\n    assert_equal(y.shape, (n_samples,))\n\n    # Check that the correct number of labels have been flipped\n    original_labels = np.bincount(y)\n    X, y = make_classification(n_samples=n_samples, n_classes=2,\n                               flip_y=flip_y, random_state=0)\n    flipped_labels = np.bincount(y)\n\n    assert_almost_equal(np.abs(original_labels - flipped_labels) / n_samples,\n                        [flip_y, flip_y], decimal=1)\n", "def test_make_classification_flip_y():\n    \"\"\"Test the flipping of labels in make_classification.\"\"\"\n    # Create a dataset with a single class, and flip all labels\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=1, n_repeated=1, n_classes=1,\n                               n_clusters_per_class=1, hypercube=False,\n                               shift=None, scale=None, flip_y=1.0,\n                               random_state=0)\n\n    assert_equal(X.shape, (100, 20), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (1,), \"Unexpected number of classes\")\n    assert_equal(sum(y == 0), 0, \"All labels should have been flipped\")\n\n    # Test the flip_y parameter with a multi-class problem\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=1, n_repeated=1, n_classes=3,\n                               n_clusters_per_class=1, hypercube=False,\n                               shift=None, scale=None, flip_y=0.5,\n                               random_state=0)\n\n    assert_equal(X.shape, (100, 20), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n    assert_true(sum(y == 0) < 50, \"Unexpected number of samples in class #0\")\n    assert_true(sum(y == 1) < 50, \"Unexpected number of samples in class #1\")\n    assert_true(sum(y == 2)", "def test_make_classification_flip_y():\n    \"\"\"Test the flip_y parameter in make_classification\"\"\"\n    flip_y = 0.5\n    X, y = make_classification(n_samples=1000, flip_y=flip_y, random_state=0)\n\n    assert_equal(np.unique(y).shape, (2,), \"Unexpected number of classes\")\n    assert_array_almost_equal(np.bincount(y) / len(y), [0.5, 0.5], decimal=1,\n                              err_msg=\"Wrong number of samples per class\")\n"], "sample_702": ["def test_runresult_assertion_on_warnings(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n\n            warnings.warn(UserWarning(\"some custom warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, warnings=1)\n", "def test_pytester_run_with_stdin(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\"\"\"\n        import sys\n\n            data = sys.stdin.read()\n            assert data == 'test input'\n    \"\"\")\n    result = pytester.runpytest_subprocess(testfile, stdin='test input')\n    assert result.ret == ExitCode.OK\n", "def test_linematcher_no_fnmatch_line(pytester: Pytester) -> None:\n    lm = LineMatcher([\"foo\", \"bar\", \"baz\"])\n    with pytest.raises(pytest.fail.Exception) as e:\n        lm.no_fnmatch_line(\"bar\")\n    assert str(e.value).splitlines() == [\n        \"nomatch: 'bar'\",\n        \"    and: 'foo'\",\n        \"fnmatch: 'bar'\",\n        \"   with: 'bar'\",\n    ]\n", "def test_pytester_syspathinsert_pathlib_path(pytester: Pytester) -> None:\n    path = pytester.path / \"subdir\"\n    path.mkdir()\n    pytester.syspathinsert(path)\n    assert path in sys.path\n", "def test_linematcher_no_matching_consecutive() -> None:\n    lm = LineMatcher([\"1\", \"2\", \"3\"])\n    with pytest.raises(pytest.fail.Exception) as e:\n        lm.no_fnmatch_line(\"1\\n3\")\n    assert str(e.value).splitlines() == [\"fnmatch: '1\\\\n3'\", \"   with: '1'\"]\n"], "sample_1020": ["def test_TrigonometricFunctions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sin(x) + cos(x)) == \"Cos[x] + Sin[x]\"\n    assert mcode(sin(x)**2 + cos(x)**2) == \"Cos[x]^2 + Sin[x]^2\"\n", "def test_Log():\n    assert mcode(exp(x)) == \"E^x\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(log(x, y)) == \"Log[x, y]\"\n    assert mcode(log(x, 10)) == \"Log[x, 10]\"\n", "def test_Trigonometric_Functions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(sin(x) + cos(x)) == \"Cos[x] + Sin[x]\"\n    assert mcode(sin(x)**2 + cos(x)**2) == \"Cos[x]^2 + Sin[x]^2\"\n    assert mcode(sin(x*y)) == \"Sin[x*y]\"\n    assert mcode(cos(x*y)) == \"Cos[x*y]\"\n", "def test_custom_function():\n    custom_func = {'g': [(lambda *x: True, 'CustomFunction')]}\n    assert mcode(g(x), user_functions=custom_func) == \"CustomFunction[x]\"\n", "def test_UnknownFunction():\n    g = Function('g')\n    assert mcode(g(x, y)) == \"g[x, y]\"\n"], "sample_237": ["def test_username_unique(self):\n    \"\"\"\n    A unique USERNAME_FIELD should not raise any errors.\n    \"\"\"\n    class CustomUserUniqueUsername(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_is_anonymous_authenticated_properties(self):\n    \"\"\"\n    <User Model>.is_anonymous/is_authenticated must not be properties.\n    \"\"\"\n    class CustomUserIsAnonymousProperty(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        @property\n            return True\n\n        @property\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Critical(\n            '%s.is_anonymous must be an attribute or property rather than '\n            'a method. Ignoring this is a security issue as anonymous '\n            'users will be treated as authenticated!' % CustomUserIsAnonymousProperty,\n            obj=CustomUserIsAnonymousProperty,\n            id='auth.C009',\n        ),\n        checks.Critical(\n            '%s.is_authenticated must be an attribute or property rather '\n            'than a method. Ignoring this is a security issue as anonymous '\n            'users will be treated as authenticated!' % CustomUserIsAnonymousProperty,\n            obj=CustomUserIsAnonymousProperty,\n            id='auth.C010',\n        ),\n    ])\n", "def test_user_model_good(self):\n    \"\"\"A good user model with REQUIRED_FIELDS as a list and USERNAME_FIELD not in it.\"\"\"\n    class CustomUserGood(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        email = models.EmailField(unique=True)\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = ['email']\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "    def test_is_anonymous_authenticated_attributes(self):\n        \"\"\"\n        <User Model>.is_anonymous/is_authenticated must be attributes or properties.\n        \"\"\"\n        class CustomUserUniqueUsername(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            is_anonymous = False\n            is_authenticated = True\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "def test_username_field_not_exist(self):\n    \"\"\"USERNAME_FIELD should be an existing field in the model.\"\"\"\n    class CustomUserBadUsernameField(AbstractBaseUser):\n        username_field = models.CharField(max_length=30, unique=True)\n        date_of_birth = models.DateField()\n\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = ['date_of_birth']\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The field named as the 'USERNAME_FIELD' for a custom user model does not exist.\",\n            hint=(\n                \"The 'USERNAME_FIELD' is currently set to 'username', but there is no such field defined in the model.\"\n            ),\n            obj=CustomUserBadUsernameField,\n            id='auth.E004',\n        ),\n    ])\n"], "sample_978": ["def test_non_integer_knots():\n    d = 2\n    knots = [0, 0.5, 1, 2, 2.5, 3, 4, 4.5]\n    splines = bspline_basis_set(d, knots, x)\n\n    assert splines[0] == Piecewise(((-3*x**2/4 + x), And(x <= 1, x >= 0)),\n                                   (x**2/4 - 0.5*x + 0.25, And(x <= 2, x >= 1)),\n                                   (0, True))\n    assert splines[1] == Piecewise((x**2/4, And(x <= 1, x >= 0.5)),\n                                   (-3*x**2/4 + 1.5*x - 0.75, And(x <= 2, x >= 1.5)),\n                                   (0, True))\n    assert splines[2] == Piecewise((x**2 - 1.5*x + 0.5625, And(x <= 2, x >= 1)),\n                                   (x**2 - 4.5*x + 7.5625, And(x <= 3, x >= 2)),\n                                   (0, True))\n    assert splines[3] == Piecewise((-3*x**2/4 + 3*x - 1.125, And(x <= 2.5, x >= 2)),\n                                   (x**2/4 - 2*x + 2.25, And(x <= 3, x >= 2.5)),\n                                   (0, True))\n    assert splines[4] == Piecewise((x**2/4 - 1.5*x + 0.5625, And(x <= 3, x >= 2.5)),\n                                   (-3*x**2/4 + 5*x - 5.625, And(x <= 4, x >= 3.5)),\n                                ", "def test_basic_degree_0_with_float_knots():\n    d = 0\n    knots = [0.0, 1.0, 2.0, 3.0, 4.0]\n    splines = bspline_basis_set(d, knots, x)\n    for i in range(len(splines)):\n        assert splines[i] == Piecewise((1, Interval(i, i + 1).contains(x)),\n                                       (0, True))\n", "def test_basic_degree_4():\n    d = 4\n    knots = range(6)\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise(\n        (x**4/24, Interval(0, 1).contains(x)),\n        (Rational(1, 24) - x**3/6 + x**4/4, Interval(1, 2).contains(x)),\n        (Rational(11, 24) - 7*x/6 + x**2/2 - x**3/6 + x**4/12, Interval(2, 3).contains(x)),\n        (Rational(43, 24) - 4*x + 5*x**2/2 - x**3/2 + x**4/24, Interval(3, 4).contains(x)),\n        (Rational(23, 24) - 13*x/6 + 7*x**2/2 - 7*x**3/6 + x**4/24, Interval(4, 5).contains(x)),\n        (0, True)\n    )\n    assert splines[0] == b0\n", "def test_basic_degree_4():\n    d = 4\n    knots = range(7)\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise(\n        (x**4/24, Interval(0, 1).contains(x)),\n        (Rational(1, 6) + x - x**2/2 + x**3/3 - x**4/24, Interval(1, 2).contains(x)),\n        (Rational(11, 12) - 5*x/2 + 5*x**2/3 - x**3/2 + x**4/24, Interval(2, 3).contains(x)),\n        (Rational(5, 6) + x - 3*x**2 + 3*x**3/2 - x**4/24, Interval(3, 4).contains(x)),\n        (Rational(25, 12) - 13*x/2 + 15*x**2/2 - 3*x**3/2 + x**4/24, Interval(4, 5).contains(x)),\n        (Rational(7, 6) - 2*x + 3*x**2 - 3*x**3/2 + x**4/24, Interval(5, 6).contains(x)),\n        (0, True)\n    )\n    assert splines[0] == b0\n", "def test_basic_degree_4():\n    d = 4\n    knots = range(7)\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise(\n        (x**4/24, Interval(0, 1).contains(x)),\n        (Rational(1, 6) - x/2 + x**2/2 - x**3/3 + x**4/6, Interval(1, 2).contains(x)),\n        (Rational(11, 24) + x/2 - x**2 + 5*x**3/6 - x**4/6, Interval(2, 3).contains(x)),\n        (Rational(43, 24) - 7*x/2 + 5*x**2/2 - 5*x**3/6 + x**4/6, Interval(3, 4).contains(x)),\n        (Rational(35, 24) - 11*x/2 + 9*x**2/2 - 7*x**3/3 + x**4/2, Interval(4, 5).contains(x)),\n        (Rational(7, 24) - x/2 + x**2 - 3*x**3/2 + x**4, Interval(5, 6).contains(x)),\n        (0, True)\n    )\n    assert splines[0] == b0\n"], "sample_66": ["def test_POST_multipart_with_non_ascii_boundary(self):\n    \"\"\"\n    Multipart POST requests with non-ASCII boundary are valid and should be handled.\n    \"\"\"\n    payload = FakePayload(\"\\r\\n\".join([\n        '--boundary\u00e9',\n        'Content-Disposition: form-data; name=\"name\"',\n        '',\n        'value',\n        '--boundary\u00e9--'\n        '']))\n    request = WSGIRequest({\n        'REQUEST_METHOD': 'POST',\n        'CONTENT_TYPE': 'multipart/form-data; boundary=boundary\u00e9',\n        'CONTENT_LENGTH': len(payload),\n        'wsgi.input': payload,\n    })\n    self.assertEqual(request.POST, {'name': ['value']})\n", "def test_wsgirequest_is_secure(self):\n    # Test is_secure() without SECURE_PROXY_SSL_HEADER\n    request = WSGIRequest({'REQUEST_METHOD': 'get', 'wsgi.input': BytesIO(b'')})\n    self.assertFalse(request.is_secure())\n\n    # Test is_secure() with SECURE_PROXY_SSL_HEADER\n    with override_settings(SECURE_PROXY_SSL_HEADER=('HTTP_X_FORWARDED_PROTO', 'https')):\n        request = WSGIRequest({\n            'REQUEST_METHOD': 'get',\n            'wsgi.input': BytesIO(b''),\n            'HTTP_X_FORWARDED_PROTO': 'https',\n        })\n        self.assertTrue(request.is_secure())\n\n    # Test is_secure() with invalid SECURE_PROXY_SSL_HEADER\n    with override_settings(SECURE_PROXY_SSL_HEADER='INVALID'):\n        request = WSGIRequest({'REQUEST_METHOD': 'get', 'wsgi.input': BytesIO(b'')})\n        with self.assertRaises(ImproperlyConfigured):\n            request.is_secure()\n", "def test_http_headers_case_insensitive(self):\n    environ = {\n        'HTTP_User-Agent': 'python-requests/1.2.0',\n        'HTTP_ACCEPT': '*',\n        'HTTP_hOst': 'example.com',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(headers['User-Agent'], 'python-requests/1.2.0')\n    self.assertEqual(headers['user-agent'], 'python-requests/1.2.0')\n    self.assertEqual(headers['USER_AGENT'], 'python-requests/1.2.0')\n    self.assertEqual(headers['Accept'], '*')\n    self.assertEqual(headers['accept'], '*')\n    self.assertEqual(headers['HOST'], 'example.com')\n    self.assertEqual(headers['HoSt'], 'example.com')\n", "def test_httpheaders_getitem(self):\n    environ = {\n        'CONTENT_TYPE': 'text/html',\n        'CONTENT_LENGTH': '100',\n        'HTTP_HOST': 'example.com',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(headers['host'], 'example.com')\n    self.assertEqual(headers['Host'], 'example.com')\n    self.assertEqual(headers['HOST'], 'example.com')\n    self.assertEqual(headers['CONTENT_LENGTH'], '100')\n    self.assertEqual(headers['content-length'], '100')\n    self.assertEqual(headers['Content-Length'], '100')\n", "def test_wsgirequest_headers(self):\n    environ = {\n        'CONTENT_TYPE': 'text/html',\n        'CONTENT_LENGTH': '100',\n        'HTTP_HOST': 'example.com',\n        'HTTP_USER_AGENT': 'python-requests/1.2.0',\n    }\n    request = WSGIRequest(environ)\n    self.assertEqual(dict(request.headers), {\n        'Content-Type': 'text/html',\n        'Content-Length': '100',\n        'Host': 'example.com',\n        'User-Agent': 'python-requests/1.2.0',\n    })\n"], "sample_24": ["def test_nan_to_num(self):\n    self.check(np.nan_to_num)\n    ma = Masked([np.nan, 1.0], mask=[True, False])\n    o = np.nan_to_num(ma, copy=False)\n    assert_masked_equal(o, Masked([0.0, 1.0], mask=[True, False]))\n    assert ma is o\n", "    def check(self, func, *args, **kwargs):\n        o = func(self.ma, *args, **kwargs)\n        expected = func(self.a, *args, **kwargs)\n        expected_mask = func(self.mask_a, *args, **kwargs)\n        assert_array_equal(o.unmasked, expected)\n        assert_array_equal(o.mask, expected_mask)\n", "def test_pad(self):\n    pad_width = ((1, 2), (3, 4))\n    constant_values = (10, 20)\n    out = np.pad(self.ma, pad_width, constant_values=constant_values)\n    expected = np.pad(self.a, pad_width, constant_values=constant_values)\n    expected_mask = np.pad(self.mask_a, pad_width, constant_values=True)\n    assert_array_equal(out.unmasked, expected)\n    assert_array_equal(out.mask, expected_mask)\n", "def test_nan_to_num_non_scalar(self):\n    # Test when nan_to_num is used with a non-scalar value\n    o = np.nan_to_num(self.ma, nan=2.0)\n    expected = np.nan_to_num(self.a, nan=2.0)\n    assert_array_equal(o.unmasked, expected)\n    assert_array_equal(o.mask, self.mask_a)\n", "def test_pad(self):\n    pad_width = ((1, 1), (2, 2))\n    mode = 'constant'\n    constant_values = (0, 0)\n    out = np.pad(self.ma, pad_width, mode=mode, constant_values=constant_values)\n    expected = np.pad(self.a, pad_width, mode=mode, constant_values=constant_values)\n    expected_mask = np.pad(self.mask_a, pad_width, mode=mode, constant_values=True)\n    assert_array_equal(out.unmasked, expected)\n    assert_array_equal(out.mask, expected_mask)\n"], "sample_218": ["def test_extract_week_func_with_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n    melb = pytz.timezone('Australia/Melbourne')\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            extracted=ExtractWeek('start_datetime', tzinfo=melb),\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, start_datetime.astimezone(melb).isocalendar()[1]),\n            (end_datetime, end_datetime.astimezone(melb).isocalendar()[1]),\n        ],\n        lambda m: (m.start_datetime, m.extracted)\n    )\n", "def test_trunc_func_with_invalid_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    invalid_tzinfo = pytz.timezone('Invalid/Timezone')\n\n    msg = 'Database returned an invalid datetime value. Are time zone definitions for your database installed?'\n    with self.assertRaisesMessage(ValueError, msg):\n        list(DTModel.objects.annotate(truncated=Trunc('start_datetime', 'year', tzinfo=invalid_tzinfo)))\n", "def test_trunc_func_with_timezone_boundaries(self):\n    \"\"\"\n    Test Trunc function with timezone at the boundary between two timezone offsets.\n    \"\"\"\n    start_datetime = datetime(2015, 10, 25, 1, 30, 50, 321)\n    end_datetime = datetime(2016, 3, 27, 1, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('day')\n    test_datetime_kind('week')\n    test_datetime_kind('month')\n    test_datetime_kind('quarter')\n", "def test_trunc_func_explicit_timezone_priority(self):\n    start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n    end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    melb = pytz.timezone('Australia/Melbourne')\n\n    with timezone.override(melb):\n        model = DTModel.objects.annotate(\n            day_melb=Trunc('start_datetime', 'day'),\n            day_utc=Trunc('start_datetime', 'day', tzinfo=timezone.utc),\n        ).order_by('start_datetime').get()\n        self.assertEqual(model.day_melb, 16)\n        self.assertEqual(model.day_utc, 15)\n", "def test_trunc_func_with_timezone_invalid_datetime(self):\n    \"\"\"\n    The trunc function should raise an error if the datetime is invalid or ambiguous.\n    \"\"\"\n    invalid_datetime = datetime(2015, 3, 29, 2, 30, 50, 321, tzinfo=pytz.timezone('America/Sao_Paulo'))\n    self.create_model(invalid_datetime, invalid_datetime)\n\n    with self.assertRaises(pytz.AmbiguousTimeError):\n        DTModel.objects.annotate(\n            truncated=Trunc('start_datetime', 'hour', output_field=DateTimeField(), tzinfo=pytz.timezone('America/Sao_Paulo'))\n        ).get()\n"], "sample_813": ["def test_intercept_bayesian_ridge():\n    # Test BayesianRidge with fit_intercept=True\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([2, 3, 7, 9, 11])\n    clf = BayesianRidge(fit_intercept=True)\n    clf.fit(X, Y)\n\n    # Check that the model could approximately learn the identity function with an intercept\n    test = [[1], [3], [4]]\n    assert_array_almost_equal(clf.predict(test), [2, 4, 5], 2)\n", "def test_prediction_bayesian_ridge_ard_with_zeros():\n    # Test BayesianRidge and ARDRegression predictions for edge case of\n    # target vector with all zeros\n    n_samples = 4\n    n_features = 5\n    X = np.zeros((n_samples, n_features))\n    y = np.zeros(n_samples)\n    expected = np.zeros(n_samples)\n\n    for clf in [BayesianRidge(), ARDRegression()]:\n        y_pred = clf.fit(X, y).predict(X)\n        assert_array_almost_equal(y_pred, expected)\n", "def test_bayesian_ridge_normalize_option():\n    # Test BayesianRidge normalize option\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n\n    clf_no_norm = BayesianRidge()\n    clf_no_norm.fit(X, y)\n\n    clf_norm = BayesianRidge(normalize=True)\n    clf_norm.fit(X, y)\n\n    # Check that the coefficients are not the same when normalize is True\n    assert not np.allclose(clf_no_norm.coef_, clf_norm.coef_)\n\n    # Check that the coefficients are the same when normalize is False\n    clf_no_norm = BayesianRidge(normalize=False)\n    clf_no_norm.fit(X, y)\n    assert np.allclose(clf_no_norm.coef_, clf_norm.coef_)\n", "def test_normalization():\n    # Test normalization option for both Bayesian regressors\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([1, 2, 3])\n\n    # Without normalization\n    clf1 = BayesianRidge(normalize=False)\n    clf1.fit(X, Y)\n\n    # With normalization\n    clf2 = BayesianRidge(normalize=True)\n    clf2.fit(X, Y)\n\n    # The coefficients should be different due to normalization\n    assert_array_less(np.abs(clf1.coef_ - clf2.coef_), 1e-8)\n\n    # For ARDRegression\n    clf3 = ARDRegression(normalize=False)\n    clf3.fit(X, Y)\n\n    clf4 = ARDRegression(normalize=True)\n    clf4.fit(X, Y)\n\n    # The coefficients should be different due to normalization\n    assert_array_less(np.abs(clf3.coef_ - clf4.coef_), 1e-8)\n", "def test_bayesian_ridge_convergence():\n    \"\"\"Check convergence of BayesianRidge with a simple case.\n\n    Test that the coefficients and the log marginal likelihood converge to the\n    expected values for a simple case where the solution is known.\n    \"\"\"\n    # Define a simple case where the solution is known\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([3, 7, 11])\n    expected_coef = np.array([1, 2])\n\n    # Compute the log marginal likelihood for the expected solution\n    n_samples, n_features = X.shape\n    eigen_vals_ = np.linalg.eigh(np.dot(X.T, X))[0]\n    alpha_ = 1. / np.var(y)\n    lambda_ = 1.\n    score = -0.5 * (n_features * np.log(lambda_) +\n                    n_samples * np.log(alpha_) -\n                    alpha_ * np.sum((y - np.dot(X, expected_coef)) ** 2) +\n                    np.sum(np.log(eigen_vals_ / (lambda_ + alpha_ * eigen_vals_))) -\n                    n_samples * np.log(2 * np.pi))\n\n    # Fit the model and check convergence\n    clf = BayesianRidge(n_iter=100, tol=1e-6, compute_score=True)\n    clf.fit(X, y)\n    assert_almost_equal(clf.coef_, expected_coef, decimal=6)\n    assert_almost_equal(clf.scores_[-1], score, decimal=6)\n"], "sample_736": ["def test_logreg_l2_sparse_data():\n    # Test that L2 penalty works for sparse data.\n    rng = np.random.RandomState(42)\n    n_samples = 50\n    X, y = make_classification(n_samples=n_samples, n_features=20,\n                               random_state=0)\n    X_noise = rng.normal(scale=0.1, size=(n_samples, 3))\n    X_constant = np.ones(shape=(n_samples, 2))\n    X = np.concatenate((X, X_noise, X_constant), axis=1)\n    X[X < 1] = 0\n    X = sparse.csr_matrix(X)\n\n    lr_liblinear = LogisticRegression(penalty=\"l2\", C=1.0, solver='liblinear',\n                                      fit_intercept=False,\n                                      tol=1e-10)\n    lr_liblinear.fit(X, y)\n\n    lr_saga = LogisticRegression(penalty=\"l2\", C=1.0, solver='saga',\n                                 fit_intercept=False,\n                                 max_iter=1000, tol=1e-10)\n    lr_saga.fit(X, y)\n    assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)\n\n    # Check that solving on the sparse and dense data yield the same results\n    lr_saga_dense = LogisticRegression(penalty=\"l2\", C=1.0, solver='saga',\n                                       fit_intercept=False,\n                                       max_iter=1000, tol=1e-10)\n    lr_saga_dense.fit(X.toarray(), y)\n    assert_array_almost_equal(lr_saga.coef_, lr_saga_dense.coef_)\n", "def test_logreg_l1_intercept_scaling():\n    # Test that intercept_scaling is considered in the L1 penalty\n    rng = np.random.RandomState(42)\n    n_samples = 50\n    X, y = make_classification(n_samples=n_samples, n_features=20,\n                               random_state=0)\n    X_intercept = np.ones(shape=(n_samples, 1))\n    X = np.concatenate((X, X_intercept), axis=1)\n    lr_liblinear = LogisticRegression(penalty=\"l1\", C=1.0, solver='liblinear',\n                                      fit_intercept=False, intercept_scaling=10.0,\n                                      tol=1e-10)\n    lr_liblinear.fit(X, y)\n    # The intercept should be regularized to zero by the l1 penalty\n    assert_array_almost_equal(lr_liblinear.coef_[0, -1], 0.0)\n", "def test_logistic_regression_intercept_scaling():\n    # Test that intercept_scaling works correctly\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n    clf = LogisticRegression(intercept_scaling=2.0)\n    clf.fit(X, y)\n    assert_equal(clf.intercept_.shape, (1,))\n    assert_equal(clf.intercept_scaling, 2.0)\n    assert_equal(clf.coef_.shape, (1, 5))\n    X_with_intercept_scaling = np.hstack((X, np.ones((10, 1)) * 2.0))\n    expected_coef = np.linalg.inv(X_with_intercept_scaling.T.dot(X_with_intercept_scaling)).dot(X_with_intercept_scaling.T).dot(y)\n    assert_array_almost_equal(clf.coef_.ravel(), expected_coef[:-1])\n    assert_almost_equal(clf.intercept_, expected_coef[-1] * 2.0)\n", "def test_logistic_regression_refit():\n    # Test the refit option in LogisticRegressionCV\n    X, y = make_classification(n_samples=10, n_features=20, random_state=0)\n\n    # With refit=True, the score should be close to the best score\n    clf_refit = LogisticRegressionCV(Cs=[1.0], cv=2, refit=True)\n    clf_refit.fit(X, y)\n    refit_score = clf_refit.score(X, y)\n    best_score = clf_refit.scores_[clf_refit.classes_[0]].max()\n    assert_almost_equal(refit_score, best_score, decimal=2)\n\n    # With refit=False, the score should be close to the average score\n    clf_no_refit = LogisticRegressionCV(Cs=[1.0], cv=2, refit=False)\n    clf_no_refit.fit(X, y)\n    no_refit_score = clf_no_refit.score(X, y)\n    avg_score = clf_no_refit.scores_[clf_no_refit.classes_[0]].mean()\n    assert_almost_equal(no_refit_score, avg_score, decimal=2)\n", "def test_logreg_intercept_scaling_values():\n    # Test that intercept_scaling affects the intercept but not the coefficients\n    clf_default = LogisticRegression(fit_intercept=True)\n    clf_default.fit(X, Y1)\n    clf_scaled = LogisticRegression(fit_intercept=True, intercept_scaling=10)\n    clf_scaled.fit(X, Y1)\n    assert_almost_equal(clf_scaled.intercept_ / clf_default.intercept_, 10)\n    assert_array_almost_equal(clf_scaled.coef_, clf_default.coef_)\n"], "sample_110": ["def test_pickle_values_subquery_queryset_not_evaluated(self):\n    group = Group.objects.create(name='group')\n    Event.objects.create(title='event', group=group)\n    groups = Group.objects.annotate(\n        event_title=models.Subquery(\n            Event.objects.filter(group_id=models.OuterRef('id')).values('title'),\n        ),\n    )\n    list(groups)  # evaluate QuerySet.\n    with self.assertNumQueries(0):\n        self.assert_pickles(groups)\n", "def test_update_with_subquery(self):\n    group = Group.objects.create(name='group')\n    Event.objects.create(title='event', group=group)\n    updated = Group.objects.update(\n        name=models.Subquery(\n            Event.objects.filter(group_id=models.OuterRef('id')).values('title'),\n        ),\n    )\n    self.assertEqual(updated, 1)\n    group.refresh_from_db()\n    self.assertEqual(group.name, 'event')\n", "def test_subquery_queryset_with_aggregate(self):\n    group = Group.objects.create(name='group')\n    Event.objects.create(title='event1', group=group)\n    Event.objects.create(title='event2', group=group)\n    groups = Group.objects.annotate(\n        event_count=models.Subquery(\n            Event.objects.filter(group_id=models.OuterRef('id')).annotate(count=models.Count('id')).values('count'),\n        ),\n    )\n    list(groups)  # evaluate QuerySet.\n    with self.assertNumQueries(0):\n        self.assert_pickles(groups)\n", "def test_date_trunc_queryset_still_usable(self):\n    event = Event.objects.create(title='event', start_date=datetime.date(2022, 1, 1))\n    events = Event.objects.annotate(\n        start_month=models.functions.Trunc('start_date', 'month'),\n    )\n    events2 = pickle.loads(pickle.dumps(events))\n    self.assertSequenceEqual(events2.filter(start_month='2022-01-01'), [event])\n", "def test_window_expression(self):\n    # Test pickling and unpickling of a QuerySet with a window expression\n    happening = Happening.objects.create(name='test', number1=5, number2=10)\n    qs = Happening.objects.annotate(rank=models.Window(expression=models.F('number1'), order_by=models.F('number2').desc()))\n    self.assert_pickles(qs)\n"], "sample_166": ["def test_get_random_string_length(self):\n    length = 20\n    random_string = get_random_string(length)\n    self.assertEqual(len(random_string), length)\n", "def test_get_random_string_length(self):\n    length = 16\n    result = get_random_string(length)\n    self.assertEqual(len(result), length)\n", "def test_get_random_string_length(self):\n    for length in range(1, 25):\n        random_string = get_random_string(length)\n        self.assertEqual(len(random_string), length)\n", "def test_get_random_string_length(self):\n    length = 22\n    random_string = get_random_string(length)\n    self.assertEqual(len(random_string), length)\n", "def test_get_random_string_length(self):\n    length = 22\n    random_string = get_random_string(length)\n    self.assertEqual(len(random_string), length)\n"], "sample_209": ["def test_model_with_multiple_pks(self):\n    \"\"\"\n    Test that a model with multiple primary keys can be instantiated and saved correctly.\n    \"\"\"\n    event1 = Event.objects.create(id1=1, id2='a', when=datetime.datetime(2000, 1, 1, 12, 0, 0))\n    event2 = Event.objects.get(id1=1, id2='a')\n    self.assertEqual(event1, event2)\n", "def test_save_base_raw(self):\n    # Test that raw=True does not call clean_fields()\n    worker = Worker(name='worker', department_id=1)\n    worker.save_base(raw=True)\n    self.assertEqual(worker._state.adding, True)\n", "def test_ordering_with_transform(self):\n    # Test ordering with transform\n    department = Department.objects.create(id=1, name='department')\n    Worker.objects.create(name='worker1', department=department, hire_date=datetime.date(2022, 1, 1))\n    Worker.objects.create(name='worker2', department=department, hire_date=datetime.date(2021, 1, 1))\n    Worker.objects.create(name='worker3', department=department, hire_date=None)\n\n    ordered_workers = Worker.objects.order_by('hire_date__year')\n    self.assertQuerysetEqual(\n        ordered_workers,\n        ['<Worker: worker3>', '<Worker: worker2>', '<Worker: worker1>'],\n        transform=str,\n    )\n", "def test_model_fields_cache(self):\n    # Test that the fields_cache is reset when creating a new instance\n    department = Department.objects.create(id=1, name='Test Department')\n    worker = Worker.objects.create(name='Test Worker', department=department)\n    copied_worker = copy.copy(worker)\n    self.assertEqual(copied_worker._state.fields_cache, {})\n\n    # Test that the fields_cache is updated when accessing a field\n    self.assertEqual(copied_worker.name, 'Test Worker')\n    self.assertEqual(copied_worker._state.fields_cache, {'name': 'Test Worker'})\n\n    # Test that the fields_cache is updated when setting a field\n    copied_worker.name = 'Updated Worker'\n    self.assertEqual(copied_worker._state.fields_cache, {'name': 'Updated Worker'})\n\n    # Test that the fields_cache is updated when saving the instance\n    copied_worker.save()\n    self.assertEqual(copied_worker._state.fields_cache, {'name': 'Updated Worker', 'department': department})\n", "def test_model_with_get_next_or_previous_in_order(self):\n    \"\"\"\n    Test get_next_or_previous_in_order method in the model\n    \"\"\"\n    Article.objects.create(headline=\"First\", pub_date=datetime.datetime(2021, 1, 1), _order=1)\n    Article.objects.create(headline=\"Second\", pub_date=datetime.datetime(2021, 1, 2), _order=2)\n    Article.objects.create(headline=\"Third\", pub_date=datetime.datetime(2021, 1, 3), _order=3)\n\n    article = Article.objects.get(headline=\"Second\")\n    next_article = article.get_next_or_previous_in_order(is_next=True)\n    previous_article = article.get_next_or_previous_in_order(is_next=False)\n\n    self.assertEqual(next_article.headline, \"Third\")\n    self.assertEqual(previous_article.headline, \"First\")\n"], "sample_277": ["def test_resolve_expression(self):\n    q = Q(price__gt=F('discounted_price'))\n    from django.db.models.sql.query import Query\n    query = Query(None)\n    result = q.resolve_expression(query)\n    self.assertIsNotNone(result)\n", "def test_deconstruct_complex_query(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q3 = Q(name='test')\n    q = q1 | ~(q2 & q3)\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price__gt', F('discounted_price')),\n        ~Q(\n            Q(price=F('discounted_price')),\n            Q(name='test')\n        ),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n", "    def test_deconstruct_complex(self):\n        q1 = Q(price__gt=F('discounted_price'))\n        q2 = Q(price=F('discounted_price'))\n        q3 = Q(discounted_price__isnull=True)\n        q = (q1 & q2) | q3\n        path, args, kwargs = q.deconstruct()\n        self.assertEqual(args, (\n            (('price', F('discounted_price')), ('price__gt', F('discounted_price'))),\n            ('discounted_price__isnull', True),\n        ))\n        self.assertEqual(kwargs, {'_connector': 'OR'})\n", "def test_deconstruct_nested_or(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q3 = Q(discount__gt=F('min_discount'))\n    q = (q1 | q2) & q3\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price', F('discounted_price')),\n        ('price__gt', F('discounted_price')),\n        ('discount__gt', F('min_discount')),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n", "def test_combine_or_with_duplicates(self):\n    q1 = Q(price=10)\n    q2 = Q(price=20)\n    q3 = Q(price=10)\n    q = q1 | q2 | q3\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (('price', 10), ('price', 20)))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n"], "sample_41": ["def test_compare_with_zero():\n    # Ensure that equality comparisons with zero work, and don't\n    # raise exceptions.\n    assert not (u.m == 0)\n    assert u.m != 0\n", "def test_raise_to_zero_power():\n    \"\"\"Test that raising to zero power returns dimensionless unit.\"\"\"\n    m2s2 = u.m ** 2 / u.s ** 2\n    assert m2s2 ** 0 == u.dimensionless_unscaled\n", "def test_unit_division_with_none():\n    \"\"\"\n    Test that division with None raises TypeError.\n\n    Regression test for https://github.com/astropy/astropy/issues/9356\n    \"\"\"\n    with pytest.raises(TypeError):\n        u.m / None\n", "def test_unit_conversion_with_quantity():\n    \"\"\"Test unit conversion with a Quantity object.\n\n    Regression test for https://github.com/astropy/astropy/issues/9036\n    \"\"\"\n    from astropy.units import Quantity\n\n    q = Quantity(1, u.m)\n    converted = q.to(u.cm)\n    assert converted.value == 100\n    assert converted.unit == u.cm\n", "def test_decompose_with_different_bases():\n    \"\"\"Test that decompose works correctly when given a set of bases.\n\n    Regression test for https://github.com/astropy/astropy/issues/8260\n    \"\"\"\n    from astropy.units import cgs\n\n    m2s2 = u.m ** 2 / u.s ** 2\n    spm_cgs = m2s2.decompose(bases=cgs.bases)\n    assert spm_cgs.bases == [cgs.cm, cgs.s]\n    assert spm_cgs.powers == [1, -1]\n    assert spm_cgs == cgs.cm / cgs.s\n\n    spm_si = m2s2.decompose(bases=u.si.bases)\n    assert spm_si.bases == [u.m, u.s]\n    assert spm_si.powers == [1, -1]\n    assert spm_si == u.m / u.s\n"], "sample_592": ["def test_format_timedelta_out_of_bounds():\n    from datetime import timedelta\n\n    delta = timedelta(days=100000)\n    expected = \"11574 days 00:00:00\"\n    result = formatting.format_timedelta(delta)\n    assert result == expected\n\n    delta = timedelta(days=-100000)\n    expected = \"-11574 days +00:00:00\"\n    result = formatting.format_timedelta(delta)\n    assert result == expected\n", "def test_limit_lines():\n    long_string = \"Line 1\\nLine 2\\nLine 3\\nLine 4\\nLine 5\\nLine 6\"\n    expected = \"Line 1\\n...\\nLine 6\"\n    actual = formatting.limit_lines(long_string, limit=4)\n    assert actual == expected\n\n    short_string = \"Short string\"\n    expected = \"Short string\"\n    actual = formatting.limit_lines(short_string, limit=4)\n    assert actual == expected\n", "def test_format_timedelta_out_of_bounds():\n    from datetime import timedelta\n\n    delta = timedelta(days=10000)\n    expected = \"10000 days 0:00:00\"\n    result = formatting.format_timedelta(delta)\n    assert result == expected\n\n    delta = timedelta(hours=1000000)\n    expected = \"1157 days 13:46:40\"\n    result = formatting.format_timedelta(delta)\n    assert result == expected\n", "def test_last_item_empty_array():\n    array = np.array([])\n    result = formatting.last_item(array)\n    assert result == []\n", "def test_inline_dask_repr():\n    import dask.array as da\n    array = da.from_array(np.random.randn(10, 10), chunks=(5, 5))\n    expected = \"dask.array<chunksize=(5, 5)> \"\n    actual = formatting.inline_dask_repr(array)\n    assert expected == actual\n\n    array_with_meta = da.from_array(np.array([1, 2, 3]), chunks=(3,), meta=np.ndarray)\n    expected = \"dask.array<chunksize=(3,), meta=np.ndarray>\"\n    actual = formatting.inline_dask_repr(array_with_meta)\n    assert expected == actual\n"], "sample_526": ["def test_num2date_nan():\n    assert np.isnan(mdates.num2date(np.nan))\n", "def test_num2date_non_scalar():\n    times = np.array([100000.0000578702, 100000.0000578703])\n    expected_dates = [datetime.datetime(2243, 10, 17, 0, 0, 4, 999980, tzinfo=datetime.timezone.utc),\n                      datetime.datetime(2243, 10, 17, 0, 0, 5, tzinfo=datetime.timezone.utc)]\n    assert np.all(mdates.num2date(times) == expected_dates)\n", "def test_date2num_out_of_range():\n    # Test date2num with a date outside the valid range for numpy.datetime64\n    with pytest.raises(OverflowError, match=\"date value out of range\"):\n        mdates.date2num(datetime.datetime(100000, 1, 1))\n", "def test_date2num_datetime_with_tzinfo():\n    # Test date2num with datetime objects with tzinfo\n    tz = dateutil.tz.gettz('America/New_York')\n    dt = datetime.datetime(2017, 3, 12, 1, 30, tzinfo=tz)\n    num = mdates.date2num(dt)\n    # Expected result is the number of days since the epoch\n    expected_num = 736682.0416666667\n    assert np.isclose(num, expected_num)\n", "def test_date_numpyx_tzinfo():\n    # Test that numpy dates work properly with tzinfo\n    base = datetime.datetime(2017, 1, 1, tzinfo=mdates.UTC)\n    time = [base + datetime.timedelta(days=x) for x in range(0, 3)]\n    timenp = np.array(time, dtype='datetime64[ns]')\n    data = np.array([0., 2., 1.])\n    fig = plt.figure(figsize=(10, 2))\n    ax = fig.add_subplot(1, 1, 1)\n    h, = ax.plot(time, data)\n    hnp, = ax.plot(timenp, data)\n    np.testing.assert_equal(h.get_xdata(orig=False), hnp.get_xdata(orig=False))\n"], "sample_289": ["    def test_setlist(self):\n        x = MultiValueDict()\n        x.setlist('a', [1, 2])\n        self.assertEqual(list(x.lists()), [('a', [1, 2])])\n", "def test_setlist(self):\n    x = MultiValueDict({'a': [1], 'b': [2], 'c': [3]})\n    x.setlist('b', [4, 5])\n    self.assertEqual(list(x.lists()), [('a', [1]), ('b', [4, 5]), ('c', [3])])\n", "    def test_setlist(self):\n        x = MultiValueDict()\n        x.setlist('a', [1, 2])\n        self.assertEqual(list(x.lists()), [('a', [1, 2])])\n", "    def test_setlist(self):\n        x = MultiValueDict({'a': [1, 2]})\n        x.setlist('a', [3, 4])\n        self.assertEqual(list(x.lists()), [('a', [3, 4])])\n", "    def test_setlistdefault(self):\n        x = MultiValueDict({'a': [1, 2]})\n        a = x.setlistdefault('a', [3, 4])\n        b = x.setlistdefault('b', [3, 4])\n        self.assertEqual(a, [1, 2])\n        self.assertEqual(b, [3, 4])\n        self.assertEqual(list(x.lists()), [('a', [1, 2]), ('b', [3, 4])])\n\n        c = x.setlistdefault('c')\n        self.assertEqual(c, [])\n        self.assertEqual(list(x.lists()), [('a', [1, 2]), ('b', [3, 4]), ('c', [])])\n"], "sample_470": ["def test_lazy_mod(self):\n    \"\"\"\n    % works correctly for Promises.\n    \"\"\"\n    lazy_str = lazy(lambda: \"Hello, %s!\", str)\n    self.assertEqual(lazy_str() % \"World\", \"Hello, World!\")\n", "def test_lazy_comparison(self):\n    \"\"\"\n    Comparison operators work correctly for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertTrue(lazy_a() <= lazy_b())\n    self.assertTrue(lazy_a() >= lazy_b())\n    self.assertFalse(lazy_b() < lazy_c())\n    self.assertFalse(lazy_b() > lazy_c())\n", "    def test_lazy_str_mod(self):\n        \"\"\"\n        __mod__ works correctly for Promises.\n        \"\"\"\n        lazy_obj = lazy(lambda: \"test\", str)\n        self.assertEqual(lazy_obj() % \"test\", \"testtest\")\n", "def test_lazy_string_modulo(self):\n    \"\"\"\n    Lazy objects should support string modulo operation.\n    \"\"\"\n    lazy_obj = lazy(lambda: \"test %s\", str)\n    self.assertEqual(lazy_obj() % \"value\", \"test value\")\n", "    def test_classproperty_inheritance(self):\n        class Base:\n            base_attr = 123\n\n            @classproperty\n                return cls.base_attr\n\n        class Derived(Base):\n            derived_attr = 456\n\n            @classproperty\n                return cls.derived_attr\n\n        self.assertEqual(Derived.base, 123)\n        self.assertEqual(Derived().base, 123)\n        self.assertEqual(Derived.derived, 456)\n        self.assertEqual(Derived().derived, 456)\n"], "sample_121": ["def test_check_constraints_not_required_db_features(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n\n        class Meta:\n            required_db_features = {'supports_something_else'}\n            constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n    self.assertEqual(Model.check(), [\n        Warning(\n            '%s does not support check constraints.' % connection.display_name,\n            hint=(\n                \"A constraint won't be created. Silence this warning if you \"\n                \"don't care about it.\"\n            ),\n            obj=Model,\n            id='models.W027',\n        )\n    ])\n", "    def test_check_constraints_with_database_support(self):\n        with self.settings(DATABASES={'default': {'ENGINE': 'django.db.backends.mysql'}}):\n            class Model(models.Model):\n                age = models.IntegerField()\n\n                class Meta:\n                    constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n            self.assertEqual(Model.check(), [])\n", "    def test_check_constraints_in_database_without_support(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n        with self.settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}):\n            errors = Model.check()\n            self.assertEqual(errors, [\n                Warning(\n                    'sqlite does not support check constraints.',\n                    hint=(\n                        \"A constraint won't be created. Silence this warning if \"\n                        \"you don't care about it.\"\n                    ),\n                    obj=Model,\n                    id='models.W027',\n                ),\n                Warning(\n                    'sqlite does not support check constraints.',\n                    hint=(\n                        \"A constraint won't be created. Silence this warning if \"\n                        \"you don't care about it.\"\n                    ),\n                    obj=Model,\n                    id='models.W027',\n                ),\n            ])\n", "def test_unique_constraints(self):\n    class Model(models.Model):\n        field1 = models.CharField(unique=True, max_length=100)\n\n        class Meta:\n            constraints = [models.UniqueConstraint(fields=['field1'], name='unique_field1')]\n\n    self.assertEqual(Model.check(), [\n        Error(\n            \"The fields field1, field1 are referenced by both a unique constraint and an index in the model Model.\",\n            obj=Model,\n            id='models.E007',\n        )\n    ])\n", "    def test_pointing_to_non_local_index(self):\n        class Foo(models.Model):\n            field1 = models.IntegerField()\n\n        class Bar(Foo):\n            field2 = models.IntegerField()\n\n            class Meta:\n                indexes = [models.Index(fields=['field2', 'field1'], name='name')]\n                index_together = [['field2', 'field1']]\n                unique_together = [['field2', 'field1']]\n\n        self.assertEqual(Bar.check(), [\n            Error(\n                \"'indexes' refers to field 'field1' which is not local to \"\n                \"model 'Bar'.\",\n                hint='This issue may be caused by multi-table inheritance.',\n                obj=Bar,\n                id='models.E016',\n            ),\n            Error(\n                \"'index_together' refers to field 'field1' which is not local to \"\n                \"model 'Bar'.\",\n                hint='This issue may be caused by multi-table inheritance.',\n                obj=Bar,\n                id='models.E016',\n            ),\n            Error(\n                \"'unique_together' refers to field 'field1' which is not local to \"\n                \"model 'Bar'.\",\n                hint='This issue may be caused by multi-table inheritance.',\n                obj=Bar,\n                id='models.E016',\n            ),\n        ])\n"], "sample_1206": ["def test_exponentiation_of_1():\n    x = Symbol('x')\n    assert 1**x == 1\n    assert unchanged(Pow, 1, x)\n", "def test_exponentiation_of_1():\n    x = Symbol('x')\n    assert 1**-x == S.One\n    assert 1**x == S.One\n", "def test_exponentiation_of_0_with_positive_exponent():\n    x = Symbol('x', positive=True)\n    assert 0**x == S.Zero\n    assert unchanged(Pow, 0, x)\n", "def test_Integer_exponentiation():\n    assert Integer(2)**Integer(3) == 8\n    assert Integer(3)**Integer(2) == 9\n    assert Integer(2)**Integer(0) == 1\n    assert Integer(0)**Integer(0) == 1\n    assert Integer(-2)**Integer(3) == -8\n    assert Integer(-3)**Integer(2) == 9\n    assert Integer(-2)**Integer(0) == 1\n", "def test_integer_power():\n    x = Integer(2)\n    assert x**2 == 4\n    assert x**-1 == S.Half\n    assert x**S.Zero == 1\n    assert x**S.One == x\n    assert x**S.NegativeOne == S.Half\n    assert x**x == 4\n    assert x**-x == S.Rational(1, 4)\n"], "sample_929": ["def test_pyattribute_old(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :annotation: = ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \" = ''\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_pyfunction_noindex(app):\n    text = \".. py:function:: func\\n\"\n    text += \"   :noindex:\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[])\n", "def test_pydecorator_noindex(app):\n    text = \".. py:decorator:: deco\\n   :noindex:\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc,))\n    assert_node(doctree[0], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=True)\n\n    assert 'deco' not in domain.objects\n", "def test_pycurrentmodule(app):\n    text = (\".. py:currentmodule:: sphinx\\n\"\n            \".. py:function:: func\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"sphinx.\"],\n                                                    [desc_name, \"func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert 'sphinx.func' in domain.objects\n    assert domain.objects['sphinx.func'] == ('index', 'sphinx.func', 'function')\n\n    text = (\".. py:currentmodule:: None\\n\"\n            \".. py:function:: func\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, ()])]))\n    assert 'func' in domain.objects\n    assert domain.objects['func'] == ('index', 'func', 'function')\n", "def test_pycurrentmodule(app):\n    text = (\".. py:currentmodule:: sphinx.builders\\n\"\n            \".. py:function:: ManualBuilder\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][0], desc_signature, **{\"py:module\": \"sphinx.builders\"})\n    assert 'sphinx.builders.ManualBuilder' in domain.objects\n    assert domain.objects['sphinx.builders.ManualBuilder'] == ('index', 'sphinx.builders.ManualBuilder', 'function')\n"], "sample_674": ["def test_node_repr():\n    node = nodes.Node(\"test_node\", parent=None, config=None, session=None, fspath=None, nodeid=\"test_node\")\n    assert repr(node) == \"<Node test_node>\"\n", "def test_node_constructor_validation():\n    with pytest.raises(TypeError, match=\"config or parent must be provided\"):\n        nodes.Node(\"test\")\n    with pytest.raises(TypeError, match=\"session or parent must be provided\"):\n        nodes.Node(\"test\", parent=nodes.Node(\"parent\"))\n    with pytest.raises(TypeError, match=\"nodeid or parent must be provided\"):\n        nodes.Node(\"test\", parent=nodes.Node(\"parent\", session=object(), config=object()))\n", "def test_file_collectfile(testdir):\n    \"\"\"Test _collectfile method of FSCollector class.\"\"\"\n    test_file = testdir.makepyfile(\n        \"\"\"\n            assert True\n    \"\"\"\n    )\n    parent = nodes.FSCollector(testdir)\n    collector = nodes.File(test_file, parent=parent)\n    items = collector._collectfile(test_file)\n    assert len(items) == 1\n    assert isinstance(items[0], nodes.Item)\n    assert items[0].name == \"test_example\"\n", "def test_get_fslocation_from_item():\n    class FakeItem:\n        pass\n\n    item = FakeItem()\n    item.location = (\"/path/to/file.py\", 123, \"function_name\")\n    assert nodes.get_fslocation_from_item(item) == (\"/path/to/file.py\", 123)\n\n    item = FakeItem()\n    item.obj = lambda: None\n    item.obj.__globals__ = {\"__file__\": \"/path/to/file.py\"}\n    item.obj.__code__ = compile(\"\", \"\", \"exec\", code_object=type(item.obj.__code__)(1, 0, 1, 0, 0, 0, 0, (), (), (), \"\", \"\", 123, \"\", (), ()))\n    assert nodes.get_fslocation_from_item(item) == (\"/path/to/file.py\", 123)\n\n    item = FakeItem()\n    item.fspath = \"/path/to/file.py\"\n    assert nodes.get_fslocation_from_item(item) == (\"/path/to/file.py\", -1)\n\n    item = FakeItem()\n    assert nodes.get_fslocation_from_item(item) == (\"unknown location\", -1)\n", "def test_node_from_parent_required_arguments():\n    class MockNode:\n        config = \"mock_config\"\n        session = \"mock_session\"\n\n    parent = MockNode()\n    node = nodes.Node.from_parent(parent, name=\"test_node\")\n    assert node.name == \"test_node\"\n    assert node.config == \"mock_config\"\n    assert node.session == \"mock_session\"\n"], "sample_887": ["def test_calibration_display_no_ref_line(pyplot, iris_data_binary):\n    # Check that `ref_line` is not plotted when `ref_line=False`\n    X, y = iris_data_binary\n    lr = LogisticRegression().fit(X, y)\n\n    viz = CalibrationDisplay.from_estimator(lr, X, y, ref_line=False)\n\n    labels = viz.ax_.get_legend_handles_labels()[1]\n    assert \"Perfectly calibrated\" not in labels\n", "def test_calibration_with_unsupported_fit_param(data):\n    \"\"\"Check that CalibratedClassifierCV does not pass unsupported fit parameters to the estimator.\"\"\"\n\n    class TestClassifier(LogisticRegression):\n            assert unsupported_param is None\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    CalibratedClassifierCV(estimator=TestClassifier()).fit(\n        *data, unsupported_param=np.ones(len(data[1]))\n    )\n", "def test_calibration_with_sample_weight_decision_function(data):\n    \"\"\"Tests that sample_weight is passed to the underlying base\n    estimator when decision_function is used.\"\"\"\n    X, y = data\n    sample_weight = np.ones_like(y)\n\n    class ClfWithSampleWeightDecisionFunction(CheckingClassifier):\n            return np.ones(X.shape[0])\n\n    clf = ClfWithSampleWeightDecisionFunction(expected_sample_weight=True)\n    pc_clf = CalibratedClassifierCV(clf)\n\n    pc_clf.fit(X, y, sample_weight=sample_weight)\n", "def test_calibrated_classifier_cv_feature_names_in(data):\n    \"\"\"Check that the feature_names_in_ attribute is propagated.\"\"\"\n    X, y = data\n    estimator = LogisticRegression()\n    estimator.fit(X, y)\n    assert hasattr(estimator, \"feature_names_in_\")\n\n    calibrated_clf = CalibratedClassifierCV(estimator)\n    calibrated_clf.fit(X, y)\n    assert hasattr(calibrated_clf, \"feature_names_in_\")\n    assert_array_equal(calibrated_clf.feature_names_in_, estimator.feature_names_in_)\n", "def test_calibration_predict_proba_nan(data):\n    \"\"\"Check that _CalibratedClassifier.predict_proba returns NaN when\n    base_estimator.predict_proba returns NaN.\"\"\"\n    X, y = data\n    # Create a classifier that returns NaN for some samples\n    class NaNClassifier(BaseEstimator):\n            return self\n            proba = np.ones((X.shape[0], 2))\n            proba[0, :] = np.nan\n            return proba\n\n    calibrated_clf = CalibratedClassifierCV(estimator=NaNClassifier(), cv=\"prefit\")\n    calibrated_clf.fit(X, y)\n    proba = calibrated_clf.predict_proba(X)\n    assert np.isnan(proba[0, :]).all()\n"], "sample_957": ["def test_restify_type_Annotated():\n    from typing import Annotated  # type: ignore\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":class:`str`\"  # NOQA\n", "def test_restify_type_Annotated():\n    from typing import Annotated  # type: ignore\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == (\":class:`str`\"  # NOQA)\n", "def test_restify_type_TypeVar():\n    T1 = TypeVar('T1')\n    T2 = TypeVar('T2', bound=int)\n    T3 = TypeVar('T3', bound=Union[int, str])\n\n    assert restify(T1) == \":obj:`tests.test_util_typing.T1`\"\n    assert restify(T2) == \":obj:`tests.test_util_typing.T2`\"\n    assert restify(T3) == \":obj:`tests.test_util_typing.T3`\"\n\n    assert restify(List[T1]) == \":class:`~typing.List`\\\\ [:obj:`tests.test_util_typing.T1`]\"\n    assert restify(List[T2]) == \":class:`~typing.List`\\\\ [:obj:`tests.test_util_typing.T2`]\"\n    assert restify(List[T3]) == \":class:`~typing.List`\\\\ [:obj:`tests.test_util_typing.T3`]\"\n", "def test_restify_type_ForwardRef_nested():\n    from typing import ForwardRef  # type: ignore\n    assert restify(ForwardRef(\"List['int']\")) == \":class:`typing.List`\\\\ [:class:`int`]\"\n", "def test_restify_type_Annotated():\n    from typing import Annotated  # type: ignore\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":class:`str`\"  # NOQA\n"], "sample_25": ["def test_update_rvkc_with_different_field_specifier(self):\n    \"\"\"Test updating a RVKC with a different field specifier.\"\"\"\n    self._test_header.set(\"DP1\", \"AXIS.3: 1\")\n    self._test_header[\"DP1.AXIS.3\"] = \"AXIS.4: 2\"\n    assert self._test_header[\"DP1.AXIS.4\"] == 2.0\n    assert \"DP1.AXIS.3\" not in self._test_header\n", "def test_append_commentary_card(self):\n    \"\"\"\n    Test appending commentary cards to a header.\n    \"\"\"\n    header = fits.Header()\n    header.add_comment(\"This is a comment\")\n    assert len(header) == 1\n    assert header[\"COMMENT\"] == \"This is a comment\"\n\n    header.add_comment(\"This is another comment\")\n    assert len(header) == 2\n    assert header[\"COMMENT\"] == [\"This is a comment\", \"This is another comment\"]\n\n    header.add_history(\"This is a history\")\n    assert len(header) == 3\n    assert header[\"HISTORY\"] == \"This is a history\"\n\n    header.add_history(\"This is another history\")\n    assert len(header) == 4\n    assert header[\"HISTORY\"] == [\"This is a history\", \"This is another history\"]\n", "def test_header_insert_with_index_and_comment(self):\n    \"\"\"\n    Test inserting a new card with just the keyword, value, and comment, and with an index.\n    \"\"\"\n\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n    header.insert(1, (\"E\", \"F\", \"G\"))\n    assert len(header) == 3\n    assert list(header)[1] == \"E\"\n    assert header[1] == \"F\"\n    assert header.comments[\"E\"] == \"G\"\n", "def test_verify_fix_rvkc(self):\n    \"\"\"\n    Test that the verify='fix' option works correctly for RVKCs.\n    \"\"\"\n\n    hdr = fits.Header()\n    hdr[\"DP1\"] = \"AXIS.1: 1.0\"\n    hdr[\"DP1\"] = \"AXIS.2: 2.0\"\n    hdr[\"DP1\"] = \"AXIS.3: INF\"\n    hdr[\"DP1\"] = \"AXIS.4: NaN\"\n\n    hdr.verify(\"fix\")\n\n    assert hdr[\"DP1.AXIS.1\"] == 1.0\n    assert hdr[\"DP1.AXIS.2\"] == 2.0\n    assert hdr[\"DP1.AXIS.3\"] == np.nan\n    assert hdr[\"DP1.AXIS.4\"] == np.nan\n", "def test_subclass_append_with_defaults():\n    class MyHeader(fits.Header):\n            if isinstance(card, tuple) and len(card) == 1:\n                # Just for our checks we add a default value and comment if they are not provided.\n                card += (0.0, \"no comment\")\n\n            return super().append(card, *args, **kwargs)\n\n    my_header = MyHeader()\n    my_header.append((\"a\",))\n    my_header.append((\"b\", 2.0))\n    my_header.append((\"c\", 3.0, \"third\"))\n\n    assert my_header[\"a\"] == 0.0\n    assert my_header[\"b\"] == 2.0\n    assert my_header[\"c\"] == 3.0\n    assert my_header.comments[\"a\"] == \"no comment\"\n    assert my_header.comments[\"b\"] == \"\"\n    assert my_header.comments[\"c\"] == \"third\"\n"], "sample_151": ["def test_mti_inheritance_model_addition(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([Animal], [Animal, self.dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "def test_mti_inheritance_model_removal_with_fk_dependency(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    Cat = ModelState('app', 'Cat', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"friend\", models.ForeignKey('app.Dog', models.CASCADE)),\n    ])\n    changes = self.get_changes([Animal, Dog, Cat], [Animal, Cat])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n    self.assertMigrationDependencies(changes, 'app', 0, [('app', 'auto_1')])\n", "    def test_add_nullable_textfield_and_charfield(self, mocked_ask_method):\n        \"\"\"\n        #23405 - Adding a NULL `CharField` or `TextField` should not prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_nullable_biography])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "def test_alter_field_to_not_null_with_default_charfield(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Tests autodetection of nullable to non-nullable alterations for CharField.\n    \"\"\"\n    changes = self.get_changes([self.author_name_null], [self.author_name_default_charfield])\n    self.assertEqual(mocked_ask_method.call_count, 0)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default='Ada Lovelace')\n", "def test_add_booleanfield_default_true(self):\n    \"\"\"\n    #23405 - Adding a BooleanField with a default of True should not prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_is_active])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"is_active\")\n"], "sample_697": ["def test_tmp_path_factory_with_explicit_basetemp_is_used(\n    tmp_path: Path, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_handles_existing_directories(tmp_path_factory: TempPathFactory) -> None:\n    \"\"\"Verify that if a --basetemp directory already exists, it is removed and recreated.\"\"\"\n    existing_dir = tmp_path_factory.getbasetemp()\n    assert existing_dir.exists()\n\n    # Create a file in the existing directory to ensure it gets removed.\n    (existing_dir / \"test_file\").touch()\n\n    new_dir = tmp_path_factory.getbasetemp()\n    assert new_dir.exists()\n    assert new_dir != existing_dir\n    assert not (existing_dir / \"test_file\").exists()\n", "def test_tmp_path_factory_owner(tmp_path: Path, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Verify that pytest raises an error if the temporary directory is not owned by the current user.\"\"\"\n    # Use the test's tmp_path as the system temproot (/tmp).\n    monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(tmp_path))\n    # Change the ownership of the tmp_path to a different user.\n    uid = os.getuid()\n    gid = os.getgid()\n    os.chown(tmp_path, uid + 1, gid)\n    # Create a TempPathFactory and expect an OSError.\n    with pytest.raises(OSError, match=\"is not owned by the current user\"):\n        tmp_factory = TempPathFactory(None, lambda *args: None, _ispytest=True)\n", "def test_tmp_path_factory_raises_error_for_unowned_directory(\n    tmp_path: Path, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_fails_if_temproot_is_not_owned_by_current_user(\n    tmp_path: Path, monkeypatch: MonkeyPatch"], "sample_844": ["def test_precomputed_dists_kd_tree():\n    redX = X[::2]\n    dists = pairwise_distances(redX, metric='euclidean')\n    clust1 = OPTICS(min_samples=10, algorithm='kd_tree',\n                    metric='precomputed').fit(dists)\n    clust2 = OPTICS(min_samples=10, algorithm='kd_tree',\n                    metric='euclidean').fit(redX)\n\n    assert_allclose(clust1.reachability_, clust2.reachability_)\n    assert_array_equal(clust1.labels_, clust2.labels_)\n", "def test_precomputed_dists_large_data():\n    # testing with a large dataset to check memory efficiency\n    rng = np.random.RandomState(0)\n    n_points = 1000\n    X = rng.randn(n_points, 10)\n\n    dists = pairwise_distances(X, metric='euclidean')\n    clust1 = OPTICS(min_samples=10, algorithm='brute',\n                    metric='precomputed').fit(dists)\n    clust2 = OPTICS(min_samples=10, algorithm='brute',\n                    metric='euclidean').fit(X)\n\n    assert_allclose(clust1.reachability_, clust2.reachability_)\n    assert_array_equal(clust1.labels_, clust2.labels_)\n", "def test_min_samples_float():\n    # Test that min_samples is correctly rounded up to the nearest integer\n    # when it is a float between 0 and 1.\n    X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n    clust = OPTICS(min_samples=0.6).fit(X)\n    assert clust.min_samples == 3  # 60% of 5 samples is 3 samples\n", "def test_xi_parameter(xi):\n    redX = X[::2]  # reduce for speed\n    clust = OPTICS(min_samples=9, xi=xi).fit(redX)\n    # check that xi parameter affects the number of clusters found\n    assert len(np.unique(clust.labels_)) > 1\n", "def test_min_samples_float():\n    # Test that min_samples can be a float between 0 and 1\n    clust = OPTICS(min_samples=0.5, max_eps=1.0).fit(X)\n    assert len(clust.labels_) == len(X)\n    assert max(clust.labels_) == 3  # X is composed of 4 clusters\n\n    # Test that min_samples raises an error if it's not a positive integer or a float between 0 and 1\n    clust = OPTICS(min_samples=-1)\n    with pytest.raises(ValueError, match=\"min_samples must be a positive integer or a float between 0 and 1\"):\n        clust.fit(X)\n\n    clust = OPTICS(min_samples=1.5)\n    with pytest.raises(ValueError, match=\"min_samples must be a positive integer or a float between 0 and 1\"):\n        clust.fit(X)\n"], "sample_890": ["def test_scoring_parameter():\n    \"\"\"Check that the scoring parameter is working correctly\"\"\"\n    X, y = make_classification(random_state=0)\n\n    knc = KNeighborsClassifier(n_neighbors=5)\n\n    sfs = SequentialFeatureSelector(knc, n_features_to_select=5, scoring='accuracy')\n    sfs.fit(X, y)\n    assert sfs.score(X, y) == accuracy_score(y, sfs.predict(X))\n\n    sfs = SequentialFeatureSelector(knc, n_features_to_select=5, scoring='f1')\n    sfs.fit(X, y)\n    assert sfs.score(X, y) == f1_score(y, sfs.predict(X))\n", "def test_feature_names_in_support():\n    \"\"\"Check that feature names are preserved when available\"\"\"\n\n    n_features = 10\n    X, y = make_regression(n_features=n_features, random_state=0)\n    feature_names = [f'feature_{i}' for i in range(n_features)]\n    X = pd.DataFrame(X, columns=feature_names)\n\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=5,\n        direction=\"forward\",\n        cv=2,\n    )\n    sfs.fit(X, y)\n\n    assert sfs.feature_names_in_ == np.array(feature_names)\n    assert sfs.get_support().sum() == 5\n    assert all(name in X.columns for name in sfs.get_support(indices=False))\n", "def test_feature_names_in_and_out_shape():\n    # Test that feature names are preserved in the output of transform\n\n    n_features = 10\n    feature_names = [f\"f{i}\" for i in range(n_features)]\n    X = np.random.randn(50, n_features)\n    X = pd.DataFrame(X, columns=feature_names)\n    y = np.random.randint(0, 2, size=50)\n\n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(),\n        n_features_to_select=5,\n        cv=2,\n    )\n    sfs.fit(X, y)\n    X_transformed = sfs.transform(X)\n\n    assert X_transformed.shape[1] == sfs.n_features_to_select_\n    assert_array_equal(X_transformed.columns, feature_names[sfs.get_support()])\n", "def test_n_jobs_support(n_jobs):\n    \"\"\"Check that the n_jobs parameter is correctly passed to the underlying estimator\"\"\"\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        direction=\"forward\",\n        n_jobs=n_jobs,\n    )\n    sfs.fit(X, y)\n    # Assuming that the underlying estimator stores the n_jobs parameter\n    assert sfs.estimator.n_jobs == n_jobs\n", "def test_support_mask():\n    # Test that the support mask is returned correctly\n\n    X, y = make_classification(n_features=20, n_informative=5, random_state=0)\n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(n_neighbors=3),\n        n_features_to_select=5,\n        direction=\"forward\",\n        cv=2,\n    )\n    sfs.fit(X, y)\n    support_mask = sfs._get_support_mask()\n\n    assert isinstance(support_mask, np.ndarray)\n    assert support_mask.shape == (20,)\n    assert support_mask.dtype == bool\n    assert support_mask.sum() == 5\n    assert np.all(support_mask == sfs.support_)\n"], "sample_599": ["def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), [0, 1, 2], encoding={\"_Unsigned\": \"true\"})\n    expected = xr.Variable((\"x\",), np.array([0, 1, 2], dtype=np.int32))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_CFMaskCoder_encode_with_missing_value():\n    original = xr.Variable((\"x\",), [0, np.nan, 1], {\"missing_value\": np.nan})\n    expected = xr.Variable((\"x\",), [0, -9999, 1], {\"missing_value\": -9999})\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_CFMaskCoder_multiple_fill_values():\n    original = xr.Variable((\"x\",), [0, -1, 1], {\"_FillValue\": [-1, 2]})\n    expected = xr.Variable((\"x\",), [0, np.nan, np.nan])\n    coder = variables.CFMaskCoder()\n    with pytest.warns(variables.SerializationWarning):\n        encoded = coder.decode(original)\n    assert_identical(expected, encoded)\n", "def test_CFScaleOffsetCoder_decode():\n    original_data = np.array([2.0, 3.0, 4.0], dtype=np.float64)\n    original = xr.Variable((\"x\",), original_data, attrs={\"scale_factor\": 2.0, \"add_offset\": 1.0})\n    expected = xr.Variable((\"x\",), [1.0, 2.0, 3.0])\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.decode(original)\n    assert_identical(expected, encoded)\n", "def test_UnsignedIntegerCoder_decode():\n    original = xr.Variable((\"x\",), np.array([0, 1, 255], dtype=np.uint8), {\"_Unsigned\": \"true\"})\n    expected = xr.Variable((\"x\",), [0, 1, 255])\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.decode(original)\n    assert_identical(expected, encoded)\n"], "sample_1018": ["def test_fcode_BooleanTrue():\n    assert fcode(True, standard=95, source_format='free') == '.true.'\n", "def test_fcode_Assignment():\n    x = symbols('x')\n    assert fcode(Assignment(x, 42), source_format='free') == 'x = 42'\n", "def test_fcode_BooleanTrueFalse():\n    assert fcode(True, standard=95, source_format='free') == \".true.\"\n    assert fcode(False, standard=95, source_format='free') == \".false.\"\n", "def test_fcode_Infinity():\n    x = symbols('x')\n    assert fcode(S.Infinity, source_format='free') == \"(huge(0d0) + 1)\"\n    assert fcode(S.NegativeInfinity, source_format='free') == \"-(huge(0d0) + 1)\"\n    assert fcode(x + S.Infinity, source_format='free') == \"x + (huge(0d0) + 1)\"\n    assert fcode(x + S.NegativeInfinity, source_format='free') == \"x - (huge(0d0) + 1)\"\n", "def test_fcode_Infinity():\n    x = symbols('x')\n    assert fcode((1.0 + 1e100) * x) == \"      (huge(0d0) + 1)*x\"\n"], "sample_138": ["def test_template_tag_multiple_imports(self):\n    relpath = self.hashed_file_path(\"cached/multiple_imports.css\")\n    self.assertEqual(relpath, \"cached/multiple_imports.33a292e8d693.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertIn(b\"\"\"import url(\"styles.5e0040571e1a.css\"), url(\"other.d41d8cd98f00.css\")\"\"\", content)\n    self.assertPostCondition()\n", "    def setUp(self):\n        super().setUp()\n\n        temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, temp_dir)\n        self.patched_settings = self.settings(\n            STATICFILES_DIRS=settings.STATICFILES_DIRS + [temp_dir],\n        )\n        self.patched_settings.enable()\n\n        # Create a file to be post-processed\n        self.intermediate_file_name = os.path.join(temp_dir, 'intermediate.css')\n        with open(self.intermediate_file_name, 'w') as f:\n            f.write('@import url(\"other.css\");')\n", "def test_cache_busting_of_unchanged_files(self):\n    \"\"\"\n    Unchanged files should not be recreated with a new hash during a second collectstatic run.\n    \"\"\"\n    # First collectstatic run.\n    self.run_collectstatic()\n    original_hashed_files = storage.staticfiles_storage.hashed_files.copy()\n\n    # Second collectstatic run without changing any files.\n    self.run_collectstatic()\n    new_hashed_files = storage.staticfiles_storage.hashed_files\n\n    # The hashed filenames should be the same for unchanged files.\n    self.assertEqual(original_hashed_files, new_hashed_files)\n", "def test_template_tag_non_ascii(self):\n    relpath = self.hashed_file_path(\"test/nonascii.css\")\n    self.assertEqual(relpath, \"test/nonascii.8d7841e30988.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertIn(b'@charset \"ISO-8859-15\";', content)\n        self.assertIn(b'url(\"nonascii.png\");', content)\n        self.assertIn(b'url(\"nonascii_query.png?spam=eggs\");', content)\n        self.assertIn(b'url(\"nonascii_fragment.png#eggs\");', content)\n        self.assertIn(b'url(\"nonascii_query_fragment.png?spam=eggs#spam\");', content)\n    self.assertPostCondition()\n", "def test_template_tag_unicode(self):\n    relpath = self.hashed_file_path(\"cached/nonascii.css\")\n    self.assertEqual(relpath, \"cached/nonascii.88b55f667230.css\")\n    with storage.staticfiles_storage.open(relpath) as relfile:\n        content = relfile.read()\n        self.assertIn(b\"url(\\\"img/\\xc3\\xa0propos.acae32e4532b.png\\\")\", content)\n    self.assertPostCondition()\n"], "sample_750": ["def test_omp_gram_tol():\n    tol = 0.5\n    norms_squared = np.sum(y ** 2, axis=0)\n    gamma_tol = orthogonal_mp_gram(G, Xy, tol=tol, norms_squared=norms_squared)\n    assert_true(np.sum((y - np.dot(X, gamma_tol)) ** 2, axis=0) <= tol).all()\n", "def test_omp_path_with_tol():\n    tol = 0.5\n    path = orthogonal_mp(X, y, tol=tol, return_path=True)\n    last = orthogonal_mp(X, y, tol=tol, return_path=False)\n    assert_true(np.sum((y[:, 0] - np.dot(X, last)) ** 2) <= tol)\n    assert_array_almost_equal(path[:, :, -1], last)\n    path = orthogonal_mp_gram(G, Xy, tol=tol, norms_squared=np.sum(y ** 2, axis=0), return_path=True)\n    last = orthogonal_mp_gram(G, Xy, tol=tol, norms_squared=np.sum(y ** 2, axis=0), return_path=False)\n    assert_true(np.sum((y[:, 0] - np.dot(X, last)) ** 2) <= tol)\n    assert_array_almost_equal(path[:, :, -1], last)\n", "def test_omp_with_intercept():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs, fit_intercept=True)\n    omp.fit(X, y[:, 0])\n    omp_no_intercept = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs, fit_intercept=False)\n    omp_no_intercept.fit(X - omp.intercept_, y[:, 0])\n    assert_array_almost_equal(omp.coef_, omp_no_intercept.coef_)\n", "def test_omp_n_iter():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n    omp.fit(X, y)\n    assert_equal(omp.n_iter_, n_nonzero_coefs)\n    omp.fit(X, y[:, 0])\n    assert_equal(omp.n_iter_, n_nonzero_coefs)\n\n    omp_cv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                         max_iter=10, cv=5)\n    omp_cv.fit(X, y)\n    assert_equal(omp_cv.n_iter_, omp_cv.n_nonzero_coefs_)\n", "def test_omp_cv_multi_target():\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y)\n    assert_equal(ompcv.n_nonzero_coefs_, n_nonzero_coefs)\n    assert_array_almost_equal(ompcv.coef_, gamma)\n    omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=False,\n                                    n_nonzero_coefs=ompcv.n_nonzero_coefs_)\n    omp.fit(X, y)\n    assert_array_almost_equal(ompcv.coef_, omp.coef_)\n"], "sample_1002": ["def test_issue_14367():\n    assert Float(1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "def test_Float_min_max():\n    assert Float(3.5).min(2.0) == 2.0\n    assert Float(3.5).max(5.0) == 5.0\n    assert Float(3.5).min(6.0) == 3.5\n    assert Float(3.5).max(2.0) == 3.5\n", "def test_issue_10230():\n    assert mod_inverse(5, 6) == 5\n", "def test_Float_float_conversion():\n    # Test converting Float to float\n    f1 = Float(3.14159, precision=6)\n    assert float(f1) == 3.14159\n    assert type(float(f1)) is float\n\n    f2 = Float('inf')\n    assert float(f2) == float('inf')\n    assert type(float(f2)) is float\n\n    f3 = Float('-inf')\n    assert float(f3) == float('-inf')\n    assert type(float(f3)) is float\n\n    f4 = Float('nan')\n    assert math.isnan(float(f4))\n    assert type(float(f4)) is float\n\n    # Test converting float to Float\n    f5 = Float(float(2.71828))\n    assert f5 == Float(2.71828)\n    assert type(f5) is Float\n\n    f6 = Float(float('inf'))\n    assert f6 == Float('inf')\n    assert type(f6) is Float\n\n    f7 = Float(float('-inf'))\n    assert f7 == Float('-inf')\n    assert type(f7) is Float\n\n    f8 = Float(float('nan'))\n    assert math.isnan(float(f8))\n    assert type(f8) is Float\n", "def test_Float_sage_conversion():\n    from sympy.external import import_module\n    sage = import_module('sage.all')\n    if not sage:\n        skip('sage not installed. Abort sage tests.')\n\n    assert Float(sage.RDF(2.718281828459045))._mpf_ == Float(2.718281828459045)._mpf_\n    assert Float(sage.RR(2.718281828459045))._mpf_ == Float(2.718281828459045)._mpf_\n    assert Float(sage.CIF(1 + 2j))._mpc_ == (Float(1)._mpf_, Float(2)._mpf_)\n    assert Float(sage.CC(1 + 2j))._mpc_ == (Float(1)._mpf_, Float(2)._mpf_)\n"], "sample_324": ["def test_https_no_origin_header(self):\n    \"\"\"A POST HTTPS request without an Origin header is accepted.\"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_https_good_referer_matches_cookie_domain_port_specified(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted from a\n    subdomain that's allowed by CSRF_COOKIE_DOMAIN when the port is specified\n    in the referer.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_REFERER'] = 'https://foo.example.com:443/'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_csrf_cookie_reset_on_different_token(self):\n    \"\"\"\n    The csrf token used in posts is changed on every request (although\n    stays equivalent). The csrf cookie should be reset if a new token\n    is used.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, None)\n    if csrf_cookie:\n        self.assertNotEqual(\n            csrf_cookie.value, self._csrf_id_cookie,\n            \"CSRF cookie was not reset on a request with a different token\"\n        )\n", "def test_https_good_referer_matches_cookie_domain_without_port(self):\n    \"\"\"\n    A POST HTTPS request with a good referer should be accepted from a\n    subdomain that's allowed by CSRF_COOKIE_DOMAIN without a port.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_REFERER'] = 'https://foo.example.com'\n    req.META['SERVER_PORT'] = '443'\n    mw = CsrfViewMiddleware(post_form_view)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_good_origin_with_custom_header_and_trusted_origin_allowed(self):\n    \"\"\"\n    A POST request with a custom header and an origin added to the CSRF_TRUSTED_ORIGINS\n    setting is accepted.\n    \"\"\"\n    req = self._get_POST_csrf_cookie_request(meta_token=self._csrf_id, token_header='HTTP_X_CSRFTOKEN')\n    req._is_secure_override = True\n    req.META['HTTP_ORIGIN'] = 'https://trusted.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n    self.assertEqual(mw.allowed_origins_exact, {'https://trusted.example.com'})\n    self.assertEqual(mw.allowed_origin_subdomains, {})\n"], "sample_179": ["def test_unique_constraint_name_constraints(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(fields=['age'], name='_unique_age'),\n                models.UniqueConstraint(fields=['age'], name='5unique_age'),\n            ]\n\n    self.assertEqual(Model.check(), [\n        Error(\n            \"The constraint name '%sunique_age' cannot start with an \"\n            \"underscore or a number.\" % prefix,\n            obj=Model,\n            id='models.E033',\n        ) for prefix in ('_', '5')\n    ])\n", "def test_check_unique_together_required_db_features(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n        name = models.CharField(max_length=50)\n\n        class Meta:\n            required_db_features = {'supports_unique_constraints'}\n            unique_together = (('age', 'name'),)\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_deferrable_unique_constraint_invalid_value(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(\n                    fields=['age'],\n                    name='unique_age_deferrable',\n                    deferrable='invalid_value',\n                ),\n            ]\n\n    errors = Model.check(databases=self.databases)\n    expected = [\n        Error(\n            'Invalid value for deferrable: \"invalid_value\". Expected one of '\n            'Deferrable.DEFERRED, Deferrable.IMMEDIATE, or Deferrable.NOT_DEFERRED.',\n            obj=Model,\n            id='models.E041',\n        ),\n    ]\n    self.assertEqual(errors, expected)\n", "def test_multiple_primary_keys(self):\n    class Model(models.Model):\n        key1 = models.IntegerField(primary_key=True)\n        key2 = models.IntegerField(primary_key=True)\n\n    self.assertEqual(Model.check(), [\n        Error(\n            \"The model cannot have more than one field with 'primary_key=True'.\",\n            obj=Model,\n            id='models.E026',\n        )\n    ])\n", "def test_check_constraints_complex(self):\n    class Model(models.Model):\n        age = models.IntegerField()\n        name = models.CharField(max_length=50)\n\n        class Meta:\n            constraints = [\n                models.CheckConstraint(\n                    check=models.Q(age__gte=18) | ~models.Q(name__startswith='John'),\n                    name='is_adult_or_not_john'\n                )\n            ]\n\n    errors = Model.check(databases=self.databases)\n    warn = Warning(\n        '%s does not support check constraints.' % connection.display_name,\n        hint=(\n            \"A constraint won't be created. Silence this warning if you \"\n            \"don't care about it.\"\n        ),\n        obj=Model,\n        id='models.W027',\n    )\n    expected = [] if connection.features.supports_table_check_constraints else [warn]\n    self.assertCountEqual(errors, expected)\n"], "sample_451": ["def test_replace_metacharacters(self):\n    self.assertEqual(replace_metacharacters(r\"a+b*c?d$e^f\"), \"a+b*cd$e^f\")\n    self.assertEqual(replace_metacharacters(r\"a\\\\+b*c\\\\?d$e^f\"), r\"a\\\\+b*cd$e^f\")\n    self.assertEqual(replace_metacharacters(r\"a\\\\\\?b*c\\\\?d$e^f\"), r\"a\\?b*cd$e^f\")\n", "def test_replace_metacharacters(self):\n    self.assertEqual(replace_metacharacters(r\"\\\\?+*^$[]\"), \"?+*^$[]\")\n    self.assertEqual(replace_metacharacters(r\"\\?\\+\\*\\^\\$\\[\\]\"), \"?+*^$[]\")\n", "    def test_replace_named_groups(self):\n        from django.contrib.admindocs.utils import replace_named_groups\n        pattern = \"^(?P<a>\\w+)/b/(?P<c>\\w+)/$\"\n        replaced_pattern = replace_named_groups(pattern)\n        self.assertEqual(replaced_pattern, \"^<a>/b/<c>/$\")\n", "def test_replace_metacharacters(self):\n    self.assertEqual(replace_metacharacters(r'^(?P<a>\\w+)/b/\\+\\*$'), r'^(?P<a>\\w+)/b/+*$')\n    self.assertEqual(replace_metacharacters(r'^(?P<a>\\w+)/b/\\?$'), r'^(?P<a>\\w+)/b/\\?$')\n    self.assertEqual(replace_metacharacters(r'^(?P<a>\\w+)/b/\\.$'), r'^(?P<a>\\w+)/b/\\.$')\n    self.assertEqual(replace_metacharacters(r'^(?P<a>\\w+)/b/\\^$'), r'^(?P<a>\\w+)/b/\\^$')\n    self.assertEqual(replace_metacharacters(r'^(?P<a>\\w+)/b/\\\\Z$'), r'^(?P<a>\\w+)/b/\\\\Z$')\n", "def test_replace_metacharacters(self):\n    self.assertEqual(replace_metacharacters(r'abc\\\\?*+^$def'), r'abc?*+^$def')\n    self.assertEqual(replace_metacharacters(r'abc\\\\\\?*+^$def'), r'abc\\?*+^$def')\n    self.assertEqual(replace_metacharacters(r'abc\\?\\*\\+\\\\\\^\\\\$def'), r'abc?*+\\^$def')\n    self.assertEqual(replace_metacharacters(r'abc\\\\b\\\\B\\\\A\\\\Zdef'), r'abc\\b\\B\\A\\Zdef')\n"], "sample_608": ["def test_inline_variable_array_repr_dask_array() -> None:\n    import dask.array as da\n\n    value = da.from_array(np.array([20, 40]), chunks=(2,))\n    variable = xr.Variable(\"x\", value)\n\n    max_width = 50\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    assert actual == \"dask.array<chunksize=(2), meta=np.ndarray>\"\n", "def test_format_array_flat_long_string() -> None:\n    long_str = [\" \".join([\"hello world\" for _ in range(100)])]\n    actual = formatting.format_array_flat(np.asarray([long_str]), 21)\n    expected = \"'hello world hello...'\n    assert expected == actual\n", "def test_format_timedelta_out_of_bounds():\n    from datetime import timedelta\n\n    td = timedelta(days=30000)\n    expected = \"30000 days 00:00:00\"\n    result = formatting.format_timedelta(td)\n    assert result == expected\n\n    td = timedelta(days=-30000)\n    expected = \"-30000 days 00:00:00\"\n    result = formatting.format_timedelta(td)\n    assert result == expected\n", "def test_short_data_repr_duck_array() -> None:\n    class DuckArray:\n            self.data = data\n\n            return \"DuckArray({})\".format(repr(self.data))\n\n    duck_array = DuckArray(np.array([1, 2, 3]))\n    data_array = xr.DataArray(duck_array)\n\n    actual = formatting.short_data_repr(data_array)\n    expected = \"DuckArray(array([1, 2, 3]))\"\n    assert actual == expected\n", "def test_short_data_repr() -> None:\n    # Test short_data_repr function with different data types and sizes\n    # Test with numpy array\n    data = np.array([[1, 2, 3], [4, 5, 6]])\n    da = xr.DataArray(data)\n    result = formatting.short_data_repr(da)\n    assert result == \"array([[1, 2, 3],\\n       [4, 5, 6]])\"\n\n    # Test with large numpy array\n    data = np.random.rand(1000, 1000)\n    da = xr.DataArray(data)\n    result = formatting.short_data_repr(da)\n    assert result == \"[1000000 values with dtype=float64]\"\n\n    # Test with xarray DataArray that is not in memory\n    data = xr.DataArray(np.random.rand(1000, 1000), chunks=(100, 100))\n    result = formatting.short_data_repr(data)\n    assert result == \"dask.array<chunksize=(100, 100), meta=np.ndarray>\"\n\n    # Test with xarray Variable that is not in memory\n    data = xr.Variable((\"x\", \"y\"), xr.DataArray(np.random.rand(1000, 1000), chunks=(100, 100)))\n    result = formatting.short_data_repr(data)\n    assert result == \"dask.array<chunksize=(100, 100), meta=np.ndarray>\"\n"], "sample_1041": ["def test_matrixelement_diff_inverse():\n    A = MatrixSymbol('A', n, n)\n    i, j, k = symbols('i j k')\n    expr = diff(A.inv()[i, j], A[k, l])\n    assert expr == -A.inv()[i, k]*A.inv()[l, j]\n", "def test_issue_9000():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, p)\n    expr1 = (A*B)*C\n    expr2 = A*(B*C)\n    assert expr1 != expr2\n    assert MatMul.is_associative\n    assert expr1 == expr2.doit()\n", "def test_MatrixElement_simplify():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert (A[0, 0] * B[0, 0]).simplify() == A[0, 0] * B[0, 0]\n    assert (A[0, 0] * B[1, 0]).simplify() != A[0, 0] * B[0, 0]\n", "def test_MatrixElement_derivative():\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', n, n)\n    i, j = symbols('i j')\n\n    assert (X[i, j]).diff(X[i, j]) == 1\n    assert (X[i, j]).diff(X[j, i]) == 0\n    assert (X[i, j]).diff(Y[i, j]) == 0\n\n    # Test derivative of MatrixElement with respect to MatrixSymbol\n    assert (X[i, j]).diff(X) == KroneckerDelta(i, j) * Identity(n)\n\n    # Test derivative of MatrixElement with respect to Inverse\n    assert (X.I[i, j]).diff(X) == -X.I[i, k] * X[k, j] * X.I[k, k]\n", "def test_MatrixElement_properties():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert MatrixElement(A, 0, 0).is_symbol\n    assert not MatrixElement(A, 0, 0).is_commutative\n    assert not MatrixElement(A, 0, 0).is_number\n    assert MatrixElement(A, 0, 0).is_symbol\n    assert MatrixElement(A, 0, 0).args == (A, 0, 0)\n    assert MatrixElement(A, 0, 0).parent == A\n    assert MatrixElement(A, 0, 0).i == 0\n    assert MatrixElement(A, 0, 0).j == 0\n    assert MatrixElement(A, 0, 0).indices == (0, 0)\n    assert MatrixElement(A, 0, 0)._diff_wrt\n    assert MatrixElement(A, 0, 0).doit() == A[0, 0]\n    assert MatrixElement(A, 0, 0).doit(deep=False) == A[0, 0]\n    assert MatrixElement(A, n, m).doit() == A[n, m]\n    assert MatrixElement(A, n, m).doit(deep=False) == A[n, m]\n"], "sample_298": ["def test_token_with_changed_password(self):\n    \"\"\"Changing the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_different_algorithm(self):\n        \"\"\"\n        A valid token can be created with an algorithm other than the default\n        by using the PasswordResetTokenGenerator.algorithm attribute.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        new_algorithm = 'sha512'\n        # Create and check a token with a different algorithm.\n        p0 = PasswordResetTokenGenerator()\n        p0.algorithm = new_algorithm\n        tk0 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk0), True)\n        # Create and check a token with the default algorithm.\n        p1 = PasswordResetTokenGenerator()\n        self.assertNotEqual(p1.algorithm, new_algorithm)\n        tk1 = p1.make_token(user)\n        # Tokens created with a different algorithm don't validate.\n        self.assertIs(p0.check_token(user, tk1), False)\n        self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_different_last_login(self):\n    \"\"\"Updating the user last_login field invalidates the token.\"\"\"\n    user = User.objects.create_user('lastloginuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now()\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with a different algorithm by using the\n    PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'sha512'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n"], "sample_94": ["def test_fields_with_fk_validation_error(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    invalid_email = 'invalid@example.com'\n\n    msg = 'email instance with email %r does not exist.' % invalid_email\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=email.pk,\n            email=invalid_email,\n            group=group.pk,\n            stdout=new_io,\n        )\n", "def test_swappable_user_required_field_validation(self):\n    new_io = StringIO()\n    with self.assertRaisesMessage(CommandError, 'Invalid date: 1976-13-01'):\n        call_command(\n            \"createsuperuser\",\n            interactive=False,\n            email=\"joe@somewhere.org\",\n            date_of_birth=\"1976-13-01\",\n            first_name='Joe',\n            stdout=new_io,\n        )\n", "def test_custom_user_custom_input(self):\n    new_io = StringIO()\n    first_name = 'john'\n    date_of_birth = '1990-01-01'\n    email = 'john@example.com'\n\n    @mock_inputs({\n        'password': 'password',\n        'username': 'john123',\n        'first_name': first_name,\n        'date_of_birth': date_of_birth,\n        'email': email,\n    })\n        call_command(\n            'createsuperuser',\n            interactive=True,\n            stdin=MockTTY(),\n            stdout=new_io,\n            stderr=new_io,\n        )\n        self.assertEqual(\n            new_io.getvalue().strip(),\n            'Superuser created successfully.'\n        )\n        u = CustomUser._default_manager.get(username='john123')\n        self.assertEqual(u.first_name, first_name)\n        self.assertEqual(u.date_of_birth, date(1990, 1, 1))\n        self.assertEqual(u.email, email)\n\n    test(self)\n", "def test_fields_with_fk_non_interactive(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n    call_command(\n        'createsuperuser',\n        interactive=False,\n        username=email.pk,\n        email=email.email,\n        group=group.pk,\n        stdout=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertEqual(command_output, 'Superuser created successfully.')\n    u = CustomUserWithFK._default_manager.get(email=email)\n    self.assertEqual(u.username, email)\n    self.assertEqual(u.group, group)\n\n    # Test with non-existent group\n    non_existent_group_id = group.pk + 1\n    msg = 'group instance with id %d does not exist.' % non_existent_group_id\n    with self.assertRaisesMessage(CommandError, msg):\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username=email.pk,\n            email=email.email,\n            group=non_existent_group_id,\n            stdout=new_io,\n        )\n", "def test_fields_with_fk_invalid_input(self):\n    new_io = StringIO()\n    group = Group.objects.create(name='mygroup')\n    email = Email.objects.create(email='mymail@gmail.com')\n\n    @mock_inputs({\n        'password': 'nopasswd',\n        'Username (Email.id): ': 'invalid_input',\n        'Email (Email.email): ': email.email,\n        'Group (Group.id): ': group.pk,\n    })\n        with self.assertRaisesMessage(CommandError, 'Invalid value for username: invalid_input'):\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdout=new_io,\n                stdin=MockTTY(),\n            )\n\n    test(self)\n"], "sample_1095": ["def test_permutation_length():\n    p = Permutation([0, 1, 2, 3, 4, 5])\n    assert p.length() == 6\n    p = Permutation([[0, 1], [2, 3], [4, 5]])\n    assert p.length() == 6\n    p = Permutation([[0, 1], [2, 3]])\n    assert p.length() == 4\n", "def test_permutation_inverse():\n    p = Permutation(0, 1, 2)\n    assert p**-1 == Permutation(1, 2, 0)\n    assert ~p == Permutation(1, 2, 0)\n    assert p * ~p == Permutation(3)\n    assert ~p * p == Permutation(3)\n    assert p * p**-1 == Permutation(3)\n    assert p**-1 * p == Permutation(3)\n", "def test_permutation_apply_integer_arguments():\n    p = Permutation(0, 1, 2)\n    assert p.apply(0) == 1\n    assert p.apply(1) == 2\n    assert p.apply(2) == 0\n\n    q = Permutation(0, 2, 1)\n    assert q.apply(0) == 1\n    assert q.apply(1) == 0\n    assert q.apply(2) == 2\n\n    raises(TypeError, lambda: p.apply('a'))\n    raises(TypeError, lambda: p.apply(3))\n", "def test_permutation_apply_composition():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    q = Permutation(2, 0, 1)\n    assert p.apply(q.apply(x)) == q.apply(p.apply(x))\n    assert p.apply(q.apply(0)) == 1\n", "def test_permutation_properties():\n    p = Permutation(0, 1, 2)\n    assert p.is_even\n    assert not p.is_odd\n    assert p.is_identity is False\n    assert p.is_Identity is False\n    assert p.size == 3\n    assert p.cycles == 1\n    assert p.cardinality == 6\n    assert p.length() == 2\n    assert p.order() == 3\n    assert p.max() == 2\n    assert p.min() == 0\n    assert p.ascents() == [0, 1]\n    assert p.descents() == [2]\n    assert p.inversions() == 2\n    assert p.signature() == 1\n    assert p.index() == 2\n    assert p.cycle_structure == {2: 1}\n    assert p.full_cyclic_form == [[0, 1]]\n    assert p.transpositions() == [(0, 1)]\n    assert p.inversion_vector() == [1, 0]\n    assert p.runs() == [[0, 1], [2]]\n    assert p.support() == [0, 1]\n    assert p.atoms() == {0, 1, 2}\n"], "sample_638": ["def test_no_arguments(capsys):\n    \"\"\"Test that help message is displayed when no arguments are provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    assert __doc__ in capsys.readouterr().out\n    assert wrapped_sysexit.value.code == 1\n", "def test_no_arguments(mock_writer, capsys):\n    \"\"\"Test that help message is shown when no arguments are provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    assert \"usage:\" in capsys.readouterr().out\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n", "def test_no_arguments(capsys):\n    \"\"\"Test that the help message is printed when no arguments are provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([])\n    # Check that the help message is printed\n    assert \"pyreverse [options] <packages>.\" in capsys.readouterr().out\n    # Check that we exited with the expected error code\n    assert wrapped_sysexit.value.code == 1\n", "def test_run_no_arguments(mock_writer, capsys):\n    \"\"\"Test that the help message is displayed if no arguments are provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([])\n    # Check that the help message is displayed\n    assert \"__main__.py\" in capsys.readouterr().out\n    # Check that pyreverse didn't try to create the diagram\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n", "def test_run_with_no_args(capsys):\n    \"\"\"Test that the help message is printed when no arguments are provided.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([])\n    # Check that the help message is printed to the user\n    assert main.__doc__ in capsys.readouterr().out\n    # Check that we exited with the expected error code\n    assert wrapped_sysexit.value.code == 1\n"], "sample_288": ["def test_key_transform_exact(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__0__exact='e'),\n        [self.objs[4]],\n    )\n", "def test_key_transform_list_lookup(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__0='e'),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__1__f='g'),\n        [self.objs[4]],\n    )\n", "def test_key_exact_with_transform(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__1__f=KeyTransform('f', KeyTransform('1', 'value__d'))),\n        [self.objs[4]],\n    )\n", "def test_nested_key_transform_with_integer(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__0__isnull=False).annotate(\n            key=KeyTransform('d', 'value'),\n            chain=KeyTransform(0, KeyTransform('1', 'key')),\n            expr=KeyTransform(0, KeyTransform('1', Cast('key', models.JSONField()))),\n        ).filter(chain='f'),\n        [self.objs[4]],\n    )\n", "def test_key_transform_expression_wrapper(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.annotate(\n            expr=ExpressionWrapper(\n                KeyTransform('c', 'value'),\n                output_field=IntegerField(),\n            ),\n        ).filter(expr__lt=15),\n        self.objs[3:5],\n    )\n"], "sample_489": ["def test_update_conflicts_unique_fields_pk(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(rank=1, name=\"a\"),\n            FieldsWithDbColumns(rank=2, name=\"b\"),\n        ]\n    )\n\n    obj1 = FieldsWithDbColumns.objects.get(rank=1)\n    obj2 = FieldsWithDbColumns.objects.get(rank=2)\n    conflicting_objects = [\n        FieldsWithDbColumns(pk=obj1.pk, rank=3, name=\"c\"),\n        FieldsWithDbColumns(pk=obj2.pk, rank=4, name=\"d\"),\n    ]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"pk\"],\n        update_fields=[\"name\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"rank\", \"name\"),\n        [\n            {\"rank\": 1, \"name\": \"c\"},\n            {\"rank\": 2, \"name\": \"d\"},\n        ],\n    )\n", "def test_update_conflicts_update_fields_db_column(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(rank=1, name=\"a\"),\n            FieldsWithDbColumns(rank=2, name=\"b\"),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n\n    conflicting_objects = [\n        FieldsWithDbColumns(rank=1, name=\"c\", db_column=\"x\"),\n        FieldsWithDbColumns(rank=2, name=\"d\", db_column=\"y\"),\n    ]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"rank\"],\n        update_fields=[\"name\", \"db_column\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"rank\", \"name\", \"db_column\"),\n        [\n            {\"rank\": 1, \"name\": \"c\", \"db_column\": \"x\"},\n            {\"rank\": 2, \"name\": \"d\", \"db_column\": \"y\"},\n        ],\n    )\n", "def test_update_conflicts_with_none_value(self):\n    TwoFields.objects.bulk_create(\n        [\n            TwoFields(f1=1, f2=1, name=\"a\"),\n            TwoFields(f1=2, f2=2, name=\"b\"),\n        ]\n    )\n    self.assertEqual(TwoFields.objects.count(), 2)\n\n    conflicting_objects = [\n        TwoFields(f1=1, f2=1, name=None),\n        TwoFields(f1=2, f2=2, name=\"d\"),\n    ]\n    results = TwoFields.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"f1\"],\n        update_fields=[\"name\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(TwoFields.objects.count(), 2)\n    self.assertCountEqual(\n        TwoFields.objects.values(\"f1\", \"f2\", \"name\"),\n        [\n            {\"f1\": 1, \"f2\": 1, \"name\": None},\n            {\"f1\": 2, \"f2\": 2, \"name\": \"d\"},\n        ],\n    )\n", "def test_bulk_create_with_db_column_with_pk(self):\n    data = [\n        FieldsWithDbColumns(id=1, rank=1, name='a'),\n        FieldsWithDbColumns(id=2, rank=2, name='b'),\n    ]\n    FieldsWithDbColumns.objects.bulk_create(data)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertEqual(FieldsWithDbColumns.objects.get(id=1).name, 'a')\n    self.assertEqual(FieldsWithDbColumns.objects.get(id=2).name, 'b')\n", "def test_returned_columns(self):\n    fields = [\n        \"id\",\n        \"name\",\n        \"iso_two_letter\",\n    ]\n    data = [\n        Country(name=\"United States of America\", iso_two_letter=\"US\"),\n        Country(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n    ]\n    results = Country.objects.bulk_create(data, return_fields=fields)\n    self.assertEqual(len(results), len(data))\n    for i, country in enumerate(results):\n        self.assertEqual(country.id, data[i].id)\n        self.assertEqual(country.name, data[i].name)\n        self.assertEqual(country.iso_two_letter, data[i].iso_two_letter)\n"], "sample_445": ["def test_years_edge(self):\n    t = datetime.datetime(2000, 1, 1)\n    tests = [\n        (datetime.datetime(2000, 1, 31), \"1\\xa0month\"),\n        (datetime.datetime(2001, 1, 1), \"1\\xa0year\"),\n        (datetime.datetime(2001, 1, 31), \"1\\xa0year, 1\\xa0month\"),\n        (datetime.datetime(2002, 1, 1), \"2\\xa0years\"),\n        (datetime.datetime(2002, 1, 31), \"2\\xa0years, 1\\xa0month\"),\n        (datetime.datetime(2003, 1, 1), \"3\\xa0years\"),\n        (datetime.datetime(2003, 1, 31), \"3\\xa0years, 1\\xa0month\"),\n        (datetime.datetime(2004, 1, 1), \"4\\xa0years\"),\n        (datetime.datetime(2004, 1, 31), \"4\\xa0years, 1\\xa0month\"),\n        (datetime.datetime(2005, 1, 1), \"5\\xa0years\"),\n        (datetime.datetime(2005, 1, 31), \"5\\xa0years, 1\\xa0month\"),\n        (datetime.datetime(2006, 1, 1), \"6\\xa0years\"),\n        (datetime.datetime(2006, 1, 31), \"6\\xa0years, 1\\xa0month\"),\n        (datetime.datetime(2007, 1, 1), \"7\\xa0years\"),\n        (datetime.datetime(2007, 1, 31), \"7\\xa0years, 1\\xa0month\"),\n        (datetime.datetime(2008, 1, 1), \"8\\xa0years\"),\n        (datetime.", "def test_custom_time_strings(self):\n    custom_time_strings = {\n        \"year\": \"Years: %(num)d\",\n        \"month\": \"Months: %(num)d\",\n        \"week\": \"Weeks: %(num)d\",\n        \"day\": \"Days: %(num)d\",\n        \"hour\": \"Hours: %(num)d\",\n        \"minute\": \"Minutes: %(num)d\",\n    }\n    t = (\n        self.t\n        + self.oneyear\n        + self.onemonth\n        + self.oneweek\n        + self.oneday\n        + self.onehour\n        + self.oneminute\n    )\n    expected = \"Years: 1, Months: 1, Weeks: 1, Days: 1, Hours: 1\"\n    self.assertEqual(timesince(self.t, t, time_strings=custom_time_strings), expected)\n", "def test_timesince_with_custom_time_strings(self):\n    custom_time_strings = {\n        \"year\": \"%(num)d y\",\n        \"month\": \"%(num)d m\",\n        \"week\": \"%(num)d w\",\n        \"day\": \"%(num)d d\",\n        \"hour\": \"%(num)d h\",\n        \"minute\": \"%(num)d min\",\n    }\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings),\n        \"1 w\",\n    )\n    self.assertEqual(\n        timeuntil(self.t, self.t - self.oneweek, time_strings=custom_time_strings),\n        \"1 w\",\n    )\n", "def test_time_strings(self):\n    custom_strings = {\n        \"year\": npgettext_lazy(\"%(num)d an\", \"%(num)d ans\", \"num\"),\n        \"month\": npgettext_lazy(\"%(num)d mois\", \"%(num)d mois\", \"num\"),\n        \"week\": npgettext_lazy(\"%(num)d semaine\", \"%(num)d semaines\", \"num\"),\n        \"day\": npgettext_lazy(\"%(num)d jour\", \"%(num)d jours\", \"num\"),\n        \"hour\": npgettext_lazy(\"%(num)d heure\", \"%(num)d heures\", \"num\"),\n        \"minute\": npgettext_lazy(\"%(num)d minute\", \"%(num)d minutes\", \"num\"),\n    }\n    with translation.override(\"fr\"):\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_strings), \"1 minute\")\n        self.assertEqual(timesince(self.t, self.t + 2 * self.oneday + 6 * self.onehour, time_strings=custom_strings), \"2 jours, 6 heures\")\n", "def test_time_strings(self):\n    \"\"\"\n    Test the use of custom time strings.\n    \"\"\"\n    custom_time_strings = {\n        \"year\": \"Anno %(num)d\",\n        \"month\": \"Mense %(num)d\",\n        \"week\": \"Septimana %(num)d\",\n        \"day\": \"Dies %(num)d\",\n        \"hour\": \"Hora %(num)d\",\n        \"minute\": \"Minuta %(num)d\",\n    }\n    self.assertEqual(\n        timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings),\n        \"Septimana 1\",\n    )\n"], "sample_278": ["def test_expression_wrapper_in_subquery(self):\n    inner = Company.objects.annotate(\n        lower_name=ExpressionWrapper(Lower('name'), output_field=CharField()),\n    ).values('lower_name')\n    outer = Company.objects.filter(name__in=Subquery(inner))\n    self.assertCountEqual(outer, Company.objects.all())\n", "def test_expression_wrapper_with_none(self):\n    queryset = Employee.objects.annotate(\n        salary_wrapper=ExpressionWrapper(Value(None, output_field=IntegerField()), output_field=IntegerField())\n    )\n    self.assertIsNone(queryset.first().salary_wrapper)\n", "def test_expression_wrapper_converts_value(self):\n    expr = ExpressionWrapper(Value('3'), output_field=IntegerField())\n    self.assertEqual(expr.convert_value(1), 3)\n", "    def test_output_field_can_be_omitted(self):\n        expr = ExpressionWrapper(Value(3))\n        self.assertIsInstance(expr.output_field, IntegerField)\n", "def test_expression_wrapper_with_outerref(self):\n    inner = Company.objects.filter(ceo=OuterRef('pk')).values('num_employees')\n    qs = Employee.objects.annotate(\n        ceo_company_size=ExpressionWrapper(\n            Subquery(inner),\n            output_field=IntegerField()\n        ),\n    )\n    self.assertEqual(qs.get(firstname='Max').ceo_company_size, 32)\n"], "sample_807": ["def test_calibration_binary_inputs():\n    \"\"\"Test calibration with binary inputs\"\"\"\n    n_samples = 100\n    X = np.random.RandomState(42).randn(n_samples, 1)\n    y = (X.squeeze() > 0).astype(np.int)\n\n    clf = LinearSVC()\n    clf.fit(X, y)\n    prob_pos_clf = clf.decision_function(X)\n\n    for method in ['isotonic', 'sigmoid']:\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        pc_clf.fit(X, y)\n        prob_pos_pc_clf = pc_clf.predict_proba(X)[:, 1]\n\n        # Check that brier score has improved after calibration\n        assert_greater(brier_score_loss(y, prob_pos_clf),\n                       brier_score_loss(y, prob_pos_pc_clf))\n", "def test_calibration_weight_warning():\n    \"\"\"Test warning when base_estimator does not support sample_weight\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    sample_weight = np.random.RandomState(42).rand(100)\n\n    class MockClassifier(BaseEstimator):\n        \"\"\"Mock classifier that does not support sample_weight\"\"\"\n\n            return self\n\n            return np.zeros((X.shape[0], 2))\n\n    with pytest.warns(UserWarning):\n        calibrated_clf = CalibratedClassifierCV(base_estimator=MockClassifier())\n        calibrated_clf.fit(X, y, sample_weight=sample_weight)\n", "def test_calibration_with_missing_values():\n    \"\"\"Test calibration with missing values\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                               random_state=42)\n    X[::2, 0] = np.nan\n\n    # split train and test\n    X_train, y_train = X[:n_samples], y[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    # Linear SVC\n    clf = LinearSVC(random_state=42)\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        clf.fit(X_train, y_train)\n\n    # CalibratedClassifierCV\n    cal_clf = CalibratedClassifierCV(clf, cv=2)\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        cal_clf.fit(X_train, y_train)\n", "def test_calibration_decision_function_binary():\n    \"\"\"Test calibration for binary decision function\"\"\"\n    n_samples = 50\n    X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                               random_state=42)\n\n    X_train, y_train = X[:n_samples], y[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    # Linear SVC with linear kernel (decision function)\n    clf = LinearSVC()\n    clf.fit(X_train, y_train)\n    prob_pos_clf = clf.decision_function(X_test)\n    prob_pos_clf = (prob_pos_clf - prob_pos_clf.min()) / (prob_pos_clf.max() - prob_pos_clf.min())\n\n    # Linear SVC with calibration\n    for method in ['isotonic', 'sigmoid']:\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        pc_clf.fit(X_train, y_train)\n        prob_pos_pc_clf = pc_clf.predict_proba(X_test)[:, 1]\n\n        # Check that brier score has improved after calibration\n        assert_greater(brier_score_loss(y_test, prob_pos_clf),\n                       brier_score_loss(y_test, prob_pos_pc_clf))\n", "def test_calibration_cv_stratified():\n    \"\"\"Test if StratifiedKFold is used for calibration when cv is None or int\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples, n_features=6,\n                               n_informative=2, n_redundant=2,\n                               n_classes=2, random_state=42)\n\n    # Test with None and int values of cv\n    for cv in [None, 5]:\n        pc_clf = CalibratedClassifierCV(method='isotonic', cv=cv)\n        pc_clf.fit(X, y)\n\n        # Check if StratifiedKFold is used in the cv object\n        assert isinstance(pc_clf.cv, StratifiedKFold)\n"], "sample_32": ["def test_de_densityscale_z_zero():\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.50, w0=-1, wz=0.5)\n\n    assert cosmo.de_density_scale(0) == 1\n", "def test_de_density_scale_units():\n    cosmo = w0wzCDM(H0=70 * u.km / (u.Mpc * u.s), Om0=0.3, Ode0=0.50, w0=-1, wz=0.5)\n\n    z = np.array([0.1, 0.2, 0.5, 1.5, 2.5])\n    assert u.allclose(\n        cosmo.de_density_scale(z),\n        [1.00705953, 1.02687239, 1.15234885, 2.40022841, 6.49384982],\n        rtol=1e-4,\n    )\n    assert cosmo.de_density_scale(z).unit == u.dimensionless_unscaled\n", "def test_de_densityscale_zero():\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.0, w0=-1, wz=0.5)\n\n    z = np.array([0.1, 0.2, 0.5, 1.5, 2.5])\n    assert u.allclose(cosmo.de_density_scale(z), np.ones_like(z), rtol=1e-7)\n\n    cosmo = Flatw0wzCDM(H0=70, Om0=0.3, w0=-1, wz=0.5)\n    assert u.allclose(cosmo.de_density_scale(z), np.ones_like(z), rtol=1e-7)\n", "def test_w_with_array_input():\n    cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.50, w0=-1, wz=0.5)\n\n    z_array = np.array([0.1, 0.2, 0.5, 1.5, 2.5])\n    expected_output = np.array([-1.05, -1.10, -0.75, 0.0, 0.75])\n\n    assert np.allclose(cosmo.w(z_array), expected_output)\n", "def test_de_density_scale_at_z_zero(H0, Om0, Ode0, w0, wz):\n    \"\"\"Test that de_density_scale at z=0 is 1.0 for all parameter combinations.\"\"\"\n    cosmo = w0wzCDM(H0=H0, Om0=Om0, Ode0=Ode0, w0=w0, wz=wz)\n    assert u.allclose(cosmo.de_density_scale(0), 1.0, atol=1e-7)\n"], "sample_771": ["def test_power_transformer_2d_sparse():\n    X = np.abs(X_2d)\n    X_sparse = sparse.csr_matrix(X)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X_sparse)\n        X_trans_func = power_transform(\n            X_sparse, method='box-cox',\n            standardize=standardize\n        )\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.boxcox(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j].toarray(), X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv.toarray(), X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_sparse_input(method, standardize):\n    # Check that PowerTransformer handles sparse input correctly\n    X = sparse.csr_matrix(X_2d)\n    if method == 'box-cox':\n        X.data = np.abs(X.data)\n\n    pt = PowerTransformer(method, standardize, copy=True)\n    X_trans = pt.fit_transform(X)\n    assert sparse.issparse(X_trans)\n    assert_array_almost_equal(X_trans.toarray(), pt.transform(X.toarray()))\n", "def test_power_transformer_axis1():\n    X = np.array([[0, 25, 50, 75, 100],\n                  [2, 4, 6, 8, 10],\n                  [2.6, 4.1, 2.3, 9.5, 0.1]])\n\n    pt = PowerTransformer(method='box-cox', standardize=True)\n    X_trans_a0 = pt.fit_transform(X.T, method='box-cox', axis=0)\n    X_trans_a1 = pt.fit_transform(X, method='box-cox', axis=1)\n    assert_array_almost_equal(X_trans_a0, X_trans_a1.T)\n", "def test_power_transformer_sparse(method):\n    # check that the transformer works with sparse input\n    X = np.abs(X_2d)\n    X_sparse = sparse.csr_matrix(X)\n\n    pt = PowerTransformer(method=method)\n    X_trans = pt.fit_transform(X_sparse)\n    assert sparse.issparse(X_trans)\n    assert_array_almost_equal(X_trans.toarray(), power_transform(X, method=method))\n\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert sparse.issparse(X_inv_trans)\n    assert_array_almost_equal(X_inv_trans.toarray(), X)\n", "def test_power_transformer_transform_single_value():\n    # Check that transform() works with single values\n    X = np.abs(X_1col)\n\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    pt.fit(X)\n    X_trans = pt.transform(X[0])\n    assert X_trans.shape == (1, 1)\n\n    pt_standardize = PowerTransformer(method='box-cox', standardize=True)\n    pt_standardize.fit(X)\n    X_trans_standardize = pt_standardize.transform(X[0])\n    assert X_trans_standardize.shape == (1, 1)\n"], "sample_11": ["def test_world_to_pixel_values_different_int_types():\n    int_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n    np64_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, np.int64(0), :])\n    world_arrays = ([10, 10.1], [20, 20.1])\n    for int_coord, np64_coord in zip(int_sliced.world_to_pixel_values(*world_arrays),\n                                     np64_sliced.world_to_pixel_values(*world_arrays)):\n        assert all(int_coord == np64_coord)\n", "def test_nested_coupled_world_slicing():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl1 = SlicedLowLevelWCS(fits_wcs, 0)\n    sl2 = SlicedLowLevelWCS(sl1, 0)\n    world = fits_wcs.pixel_to_world_values(0,0,0)\n    out_pix = sl2.world_to_pixel_values(world[0], world[1])\n\n    assert np.allclose(out_pix[0], 0)\n", "def test_coupled_pixel_slicing():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, slice(None), slice(None), 0)\n    pixel = (0, 0, 0)\n    out_world = sl.pixel_to_world_values(*pixel)\n\n    assert np.allclose(out_world[2], fits_wcs.pixel_to_world_values(*pixel)[2])\n", "def test_coupled_world_slicing_multiple_dimensions():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, (0, slice(None), 1))\n    world = fits_wcs.pixel_to_world_values(0, 0, 1)\n    out_pix = sl.world_to_pixel_values(world[0], world[1])\n\n    assert np.allclose(out_pix, (0, 0))\n", "def test_world_to_pixel_values_different_int_types():\n    int_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n    np64_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, np.int64(0), :])\n    world_arrays = ([10, 10.1], [25, 25.1])\n    for int_coord, np64_coord in zip(int_sliced.world_to_pixel_values(*world_arrays),\n                                     np64_sliced.world_to_pixel_values(*world_arrays)):\n        assert all(int_coord == np64_coord)\n"], "sample_1065": ["def test_subfactorial_rewrite():\n    x = Symbol('x')\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1) / S.Exp1\n    assert subfactorial(x).rewrite(uppergamma).rewrite(subfactorial) == subfactorial(x)\n", "def test_binomial_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert binomial(n, k).series(n, 0, 3) == \\\n        binomial(0, k) + k*binomial(0, k - 1)*n/1 + (k*(k - 1)*binomial(0, k - 2)*n**2/2 + O(n**3))/2\n", "def test_rf_ff_symmetry():\n    x, y = symbols('x, y')\n    n, k = symbols('n k', integer=True, nonnegative=True)\n    assert rf(x, k) == ff(x + k - 1, k)\n    assert ff(x, k) == rf(x - k + 1, k)\n    assert rf(n, k) == ff(n + k - 1, k)\n    assert ff(n, k) == rf(n - k + 1, k)\n", "def test_rf_ff_rewrite():\n    x, y = symbols('x, y')\n    n, k = symbols('n k', integer=True)\n    m = Symbol('m', integer=True, nonnegative=True)\n\n    assert rf(x, k).rewrite(gamma) == gamma(x + k) / gamma(x)\n    assert ff(x, k).rewrite(gamma) == (-1)**k * gamma(k - x) / gamma(-x)\n    assert rf(x, k).rewrite(ff) == ff(x + k - 1, k)\n    assert ff(x, k).rewrite(rf) == rf(-k + x + 1, k)\n    assert rf(x, k).rewrite(binomial) == factorial(k) * binomial(x + k - 1, k)\n    assert ff(x, k).rewrite(binomial) == factorial(k) * binomial(x, k)\n    assert rf(n, m).rewrite(factorial) == factorial(n + m - 1) / factorial(n - 1)\n    assert ff(n, m).rewrite(factorial) == factorial(n) / factorial(n - m)\n", "def test_subfactorial_is_odd_even():\n    # Testing is_even and is_odd properties of subfactorial function\n    x = Symbol('x', integer=True, nonnegative=True)\n    y = Symbol('y', integer=True, positive=True)\n    z = Symbol('z', integer=True, negative=True)\n\n    assert subfactorial(x + 1).is_odd is True\n    assert subfactorial(x + 2).is_even is True\n    assert subfactorial(y).is_odd is False\n    assert subfactorial(y + 1).is_odd is True\n    assert subfactorial(z).is_even is None\n"], "sample_86": ["def test_lazy_text_cast(self):\n    \"\"\"\n    __text_cast method works correctly for Promises.\n    \"\"\"\n    original_object = 'Lazy translation text'\n    lazy_obj = lazy(lambda: original_object, str)\n    self.assertEqual(original_object, lazy_obj().__text_cast())\n", "def test_lazy_hash_equality(self):\n    \"\"\"\n    Hash and equality work correctly for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n    self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n", "def test_lazy_hash(self):\n    \"\"\"\n    The hash of a Promise is the hash of its evaluated value.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n    self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n", "    def test_lazy_ordering(self):\n        \"\"\"\n        <, <=, >, >=, and sort() work correctly for Promises.\n        \"\"\"\n        lazy_a = lazy(lambda: 4, int)\n        lazy_b = lazy(lambda: 4, int)\n        lazy_c = lazy(lambda: 5, int)\n\n        self.assertLess(lazy_c(), lazy_a())\n        self.assertLessEqual(lazy_a(), lazy_b())\n        self.assertGreater(lazy_c(), lazy_b())\n        self.assertGreaterEqual(lazy_c(), lazy_a())\n\n        lazy_list = [lazy_c(), lazy_b(), lazy_a()]\n        lazy_list.sort()\n        self.assertEqual(lazy_list, [lazy_a(), lazy_b(), lazy_c()])\n", "    def test_lazy_deepcopy(self):\n        \"\"\"\n        deepcopy works correctly for Promises.\n        \"\"\"\n        import copy\n\n        class Foo:\n            pass\n\n        obj = Foo()\n        lazy_obj = lazy(lambda: obj, Foo)\n        copied_lazy_obj = copy.deepcopy(lazy_obj)\n\n        self.assertIsNot(lazy_obj(), copied_lazy_obj())\n        self.assertEqual(lazy_obj(), obj)\n        self.assertEqual(copied_lazy_obj(), obj)\n"], "sample_1199": ["def test_tensor_product_associativity():\n    assert TensorProduct(A, TensorProduct(B, C)) == TensorProduct(TensorProduct(A, B), C)\n", "def test_tensor_product_matrix():\n    # Testing tensor product with matrices\n    mat1 = Matrix([[1, 2], [3, 4]])\n    mat2 = Matrix([[5, 6], [7, 8]])\n    assert TensorProduct(mat1, mat2) == Matrix([[5, 6, 10, 12], [7, 8, 14, 16], [15, 18, 20, 24], [21, 24, 28, 32]])\n    assert TensorProduct(mat2, mat1) == Matrix([[5, 10, 6, 12], [14, 20, 18, 24], [7, 14, 8, 16], [21, 28, 24, 32]])\n", "def test_tensor_product_matrices():\n    assert TensorProduct(mat1, mat2) == Matrix([[2*I, 3*I, 6, 9],\n                                               [4*I, 2, 9, 6],\n                                               [1, 3, 3 + 3*I, 2 + 6*I],\n                                               [1 + I, 3 + 3*I, 6 + 9*I, 2 + 6*I]])\n    assert TensorProduct(mat2, mat1) == Matrix([[2*I, 1, 3*I, 1 + I],\n                                               [3, 1 + I, 9, 3 + 3*I],\n                                               [6*I, 2, 3*I, 1 + I],\n                                               [9*I, 2 + I, 6*I, 3 + 3*I]])\n", "def test_tensor_product_with_matrices():\n    mat1 = Matrix([[1, 2*I], [1 + I, 3]])\n    mat2 = Matrix([[2*I, 3], [4*I, 2]])\n    expected = Matrix([[2*I, 3*I, 6, 12],\n                      [4*I+2, 6+2*I, 12+4*I, 6],\n                      [1+2*I, 3+2*I, 3+6*I, 9+6*I],\n                      [4+3*I, 6+3*I, 12+9*I, 6+18*I]])\n    assert TensorProduct(mat1, mat2) == expected\n", "def test_tensor_product_matrices():\n    assert TensorProduct(mat1, mat2) == Matrix([[2*I, 6*I, 3, 0],\n                                               [2*I + 4*I, 6, 4*I, 6],\n                                               [1, 2*I, 3, 0],\n                                               [1 + I, 3, 0, 6*I]])\n    assert TensorProduct(mat2, mat1) == Matrix([[2*I, 6, 3*I, 0],\n                                               [3, 6*I, 3, 0],\n                                               [4*I, 2, 6*I, 3],\n                                               [0, 4*I, 1 + 3*I, 3]])\n"], "sample_1080": ["def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.real(x) & Q.positive(x)) == 1\n    assert refine(sign(x), Q.real(x) & Q.negative(x)) == -1\n    assert refine(sign(x), Q.real(x) & Q.zero(x)) == 0\n    assert refine(sign(x), Q.imaginary(x) & Q.positive(im(x))) == S.ImaginaryUnit\n    assert refine(sign(x), Q.imaginary(x) & Q.negative(im(x))) == -S.ImaginaryUnit\n", "def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.zero(re(x)) & Q.zero(im(x))) == nan\n    assert refine(sign(x), Q.zero(re(x)) & Q.positive(im(x))) == S.ImaginaryUnit\n    assert refine(sign(x), Q.zero(re(x)) & Q.negative(im(x))) == -S.ImaginaryUnit\n    assert refine(sign(x), Q.positive(re(x))) == 1\n    assert refine(sign(x), Q.negative(re(x))) == -1\n", "def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.positive(x) & Q.real(x)) == 1\n    assert refine(sign(x), Q.negative(x) & Q.real(x)) == -1\n    assert refine(sign(x), Q.positive(x) & Q.imaginary(x)) == I / abs(I * x)\n    assert refine(sign(x), Q.negative(x) & Q.imaginary(x)) == -I / abs(I * x)\n", "def test_refine_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.positive(x)) == 1\n    assert refine(sign(x), Q.negative(x)) == -1\n    assert refine(sign(x), Q.zero(x)) == 0\n    assert refine(sign(x), True) == sign(x)\n", "def test_refine_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.positive(re(x))) == sign(x)\n    assert refine(sign(x), Q.negative(re(x))) == sign(x)\n    assert refine(sign(x), Q.zero(re(x)) & Q.positive(im(x))) == S.ImaginaryUnit\n    assert refine(sign(x), Q.zero(re(x)) & Q.negative(im(x))) == -S.ImaginaryUnit\n"], "sample_783": ["def test_imputation_constant_fill_value_override():\n    # Test imputation using the constant strategy with fill_value override\n    X = np.array([\n        [np.nan, 1.1, 0, np.nan],\n        [1.2, np.nan, 1.3, np.nan],\n        [0, 0, np.nan, np.nan],\n        [1.4, 1.5, 0, np.nan]\n    ])\n\n    X_true = np.array([\n        [100, 1.1, 0, 100],\n        [1.2, 100, 1.3, 100],\n        [0, 0, 100, 100],\n        [1.4, 1.5, 0, 100]\n    ])\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=100)\n    X_trans = imputer.fit_transform(X)\n\n    assert_array_equal(X_trans, X_true)\n", "def test_imputation_constant_error_invalid_fill_value(X, missing_values, strategy, fill_value, err_msg):\n    imputer = SimpleImputer(missing_values=missing_values, strategy=strategy, fill_value=fill_value)\n    with pytest.raises(ValueError, match=err_msg):\n        imputer.fit_transform(X)\n", "def test_imputation_error_invalid_shape():\n    # Test that an error is raised when the number of features in the transform\n    # data does not match the number of features in the fit data\n    X_fit = np.array([[1, 2], [3, 4]])\n    X_transform = np.array([[1, 2, 3]])\n\n    imputer = SimpleImputer(strategy=\"mean\")\n    imputer.fit(X_fit)\n\n    with pytest.raises(ValueError, match=\"X has a different number of features\"):\n        imputer.transform(X_transform)\n", "def test_imputation_with_different_missing_values():\n    # Test imputation using different missing value representations\n    X = np.array([\n        [np.nan, 1.1, 0, np.nan],\n        [1.2, np.inf, 1.3, np.nan],\n        [0, 0, np.nan, -np.inf],\n        [1.4, 1.5, 0, np.nan]\n    ])\n\n    X_true = np.array([\n        [-1, 1.1, 0, -1],\n        [1.2, -1, 1.3, -1],\n        [0, 0, -1, -1],\n        [1.4, 1.5, 0, -1]\n    ])\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=-1, missing_values=np.nan)\n    X_trans = imputer.fit_transform(X)\n    assert_allclose(X_trans, X_true)\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=-1, missing_values=np.inf)\n    X_trans = imputer.fit_transform(X)\n    X_true[1, 1] = 1.1\n    X_true[2, 3] = 0\n    assert_allclose(X_trans, X_true)\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=-1, missing_values=-np.inf)\n    X_trans = imputer.fit_transform(X)\n    X_true[1, 1] = 1.3\n    X_true[2, 3] = -1\n    assert_allclose(X_trans, X_true)\n", "def test_imputation_with_empty_array(X):\n    # Test imputation when the input array is empty\n    imputer = SimpleImputer(strategy=\"mean\")\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X_trans, X.toarray())  # No change to the array as it was empty\n"], "sample_563": ["def test_anchoredoffsetbox():\n    fig, ax = plt.subplots()\n\n    ta = TextArea(\"foo\")\n    ao = AnchoredOffsetbox('upper left', child=ta)\n    ax.add_artist(ao)\n\n    ta = TextArea(\"bar\")\n    ao = AnchoredOffsetbox('upper right', child=ta)\n    ax.add_artist(ao)\n\n    ta = TextArea(\"foobar\")\n    ao = AnchoredOffsetbox('lower right', child=ta)\n    ax.add_artist(ao)\n", "def test_offsetimage():\n    fig, ax = plt.subplots()\n\n    arr = np.random.rand(10, 10)\n    oi = OffsetImage(arr, zoom=2)\n    ab = AnnotationBbox(oi, (0.5, 0.5))\n    ax.add_artist(ab)\n\n    fig.canvas.draw()\n", "def test_offsetbox_custom_offset():\n    fig, ax = plt.subplots()\n    da = DrawingArea(10, 10)\n    da.set_offset((50, 50))\n    ax.add_artist(da)\n    fig.canvas.draw()\n    assert da.get_offset() == (50, 50)\n", "def test_anchoredtext_vertical_alignment():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test long text\", loc=\"lower left\",\n                         pad=0.2, prop={\"va\": \"bottom\"})\n    ax.add_artist(text0)\n    text1 = AnchoredText(\"test long text\", loc=\"center left\",\n                         pad=0.2, prop={\"va\": \"center\"})\n    ax.add_artist(text1)\n    text2 = AnchoredText(\"test long text\", loc=\"upper left\",\n                         pad=0.2, prop={\"va\": \"top\"})\n    ax.add_artist(text2)\n\n    fig.canvas.draw()\n", "def test_offsetimage():\n    fig, ax = plt.subplots()\n    img_data = np.random.rand(10, 10)\n    im = OffsetImage(img_data, zoom=2)\n    ab = AnnotationBbox(im, (0.5, 0.5))\n    ax.add_artist(ab)\n"], "sample_211": ["    def test_get_method(self):\n        test_view = views.SingleObjectTemplateResponseMixinView()\n        test_view.template_name = 'test_template.html'\n        test_view.object = 'test_object'\n        request = self.rf.get('/')\n        response = test_view.get(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.template_name, [test_view.template_name])\n        self.assertEqual(response.context_data['object'], test_view.object)\n", "def test_options_for_redirect_view(self):\n    \"\"\"\n    A RedirectView responds to OPTIONS with the same status code and\n    Allow header as if it were GET.\n    \"\"\"\n    request = self.rf.options('/')\n    view = RedirectView.as_view(url='/bar/')\n    response = view(request)\n    self._assert_allows(response, 'GET', 'HEAD', 'POST')\n", "    def test_template_mixin_with_template(self):\n        \"\"\"\n        Test that a template is correctly returned when using a template mixin and specifying a template.\n        \"\"\"\n        view = views.TemplateResponseWithTemplate()\n        self.assertEqual(view.get_template_names(), ['generic_views/test_template.html'])\n", "def test_options_for_redirect_view(self):\n    \"\"\"\n    A RedirectView also allows OPTIONS.\n    \"\"\"\n    request = self.rf.options('/')\n    view = RedirectView.as_view(url='/bar/')\n    response = view(request)\n    self._assert_allows(response, 'GET', 'HEAD', 'POST', 'PUT', 'PATCH', 'DELETE', 'OPTIONS')\n", "def test_head_with_get_view(self):\n    \"\"\"\n    Test a view which supplies a GET method responds correctly to HEAD.\n    \"\"\"\n    self._assert_simple(SimpleView.as_view()(self.rf.head('/')))\n    view = SimpleView.as_view()\n    response = view(self.rf.head('/'))\n    self.assertEqual(response.status_code, 200)\n"], "sample_740": ["def test_check_array_memmap_copy_on_write():\n    X = np.ones((4, 4))\n    with TempMemmap(X, mmap_mode='c') as X_memmap:\n        X_checked = check_array(X_memmap, copy=False)\n        assert np.may_share_memory(X_memmap, X_checked)\n        assert not X_checked.flags['WRITEABLE']\n        X_checked[0, 0] = 2\n        assert X_memmap[0, 0] == 1\n", "def test_check_array_nan_conversion():\n    X = np.array([[1, np.nan], [2, 3]], dtype=float)\n    X_checked = check_array(X, force_all_finite='allow-nan')\n    assert np.isnan(X_checked[0, 1])\n", "def test_check_array_accept_sparse_list():\n    X = [[1, 2], [3, 4]]\n    X_csr = sp.csr_matrix(X)\n    X_csc = sp.csc_matrix(X)\n    X_coo = sp.coo_matrix(X)\n\n    X_checked = check_array(X_csr, accept_sparse=['csr', 'csc'])\n    assert_equal(X_checked.format, 'csr')\n\n    X_checked = check_array(X_csc, accept_sparse=['csr', 'csc'])\n    assert_equal(X_checked.format, 'csc')\n\n    X_checked = check_array(X_coo, accept_sparse=['csr', 'csc', 'coo'])\n    assert_equal(X_checked.format, 'csr')\n\n    msg = \"Parameter 'accept_sparse' should be a string, boolean or list of strings. You provided 'accept_sparse={}'.\"\n    assert_raise_message(ValueError, msg.format(123),\n                         check_array, X_csr, accept_sparse=123)\n", "def test_check_array_with_large_sparse_matrix_non_supported_scipy_version():\n    # Large sparse matrices should not be accepted for scipy<0.14.0\n    if not LARGE_SPARSE_SUPPORTED:\n        X = sp.rand(20, 10, format='csr', dtype=np.float32)\n        X.indices = X.indices.astype('int64')\n        msg = (\"Scipy version %s does not support large\"\n               \" indices, please upgrade your scipy\"\n               \" to 0.14.0 or above\" % scipy_version)\n        assert_raise_message(ValueError, msg, check_array, X, accept_sparse='csr')\n", "def test_check_array_complex_data_warning():\n    X = np.array([[1 + 1j, 3, 5], [2, 4, 6]], dtype=complex)\n    with pytest.warns(UserWarning, match=\"Complex data not supported\"):\n        check_array(X, warn_on_dtype=True)\n\n    X = sp.csr_matrix([[0, 1 + 1j], [0, 0]], dtype=complex)\n    with pytest.warns(UserWarning, match=\"Complex data not supported\"):\n        check_array(X, accept_sparse=True, warn_on_dtype=True)\n"], "sample_595": ["def test_str_accessor_with_empty_bytes():\n    empty_bytes = xr.DataArray(np.empty(shape=(0,), dtype=\"S\"))\n\n    # Test decode method\n    decoded = empty_bytes.str.decode(\"utf-8\")\n    expected = xr.DataArray(np.empty(shape=(0,), dtype=\"U\"))\n    assert_equal(decoded, expected)\n\n    # Test encode method\n    encoded = empty_bytes.str.encode(\"utf-8\")\n    assert_equal(encoded, empty_bytes)\n", "def test_slice_replace_empty_input():\n    values = xr.DataArray([\"\", \"abcdef\", \"ghijkl\"])\n\n    result = values.str.slice_replace(2, 5, \"xyz\")\n    expected = xr.DataArray([\"xyz\", \"abxyzef\", \"ghijkl\"])\n    assert_equal(result, expected)\n\n    result = values.str.slice_replace(10, 20, \"xyz\")\n    expected = xr.DataArray([\"\", \"abcdefxyz\", \"ghijklxyz\"])\n    assert_equal(result, expected)\n", "def test_case_preserved(dtype):\n    da = xr.DataArray([\"SOme word\"]).astype(dtype)\n    original = xr.DataArray([\"SOme word\"]).astype(dtype)\n    capitalized = xr.DataArray([\"Some word\"]).astype(dtype)\n    lowered = xr.DataArray([\"some word\"]).astype(dtype)\n    swapped = xr.DataArray([\"soME WORD\"]).astype(dtype)\n\n    da.str.capitalize()\n    assert_equal(da, original)\n\n    da.str.lower()\n    assert_equal(da, original)\n\n    da.str.swapcase()\n    assert_equal(da, original)\n\n    da.str.title()\n    assert_equal(da, original)\n\n    da.str.upper()\n    assert_equal(da, original)\n", "def test_swapcase_with_unicode(dtype):\n    values = xr.DataArray([\"\\u0131I\", \"i\\u0307\", \"M\\u030C\", \"m\\u0328\"]).astype(dtype)\n    result = values.str.swapcase()\n    expected = xr.DataArray([\"\\u0130i\", \"I\\u0307\", \"m\\u030C\", \"M\\u0328\"]).astype(dtype)\n    assert_equal(result, expected)\n", "def test_replace_errors():\n    values = xr.DataArray([\"fooBAD__barBAD\"])\n    # test with unicode\n    pat = \"BAD[_]+\"\n    repl = \"UNICODE_\\xe4\"\n    result = values.str.replace(pat, repl)\n    expected = xr.DataArray([\"fooUNICODE_\\xe4__barUNICODE_\\xe4\"])\n    assert_equal(result, expected)\n\n    # test with non-string repl\n    msg = \"repl must be a string or callable\"\n    with pytest.raises(TypeError, match=msg):\n        values.str.replace(\"BAD[_]+\", 123)\n"], "sample_123": ["    def test_parsing(self):\n        self.assertEqual(\n            limited_parse_qsl('a=1&b=2'),\n            [('a', '1'), ('b', '2')]\n        )\n        self.assertEqual(\n            limited_parse_qsl('a=1&b=2&c=3', fields_limit=2),\n            [('a', '1'), ('b', '2')]\n        )\n        self.assertEqual(\n            limited_parse_qsl('a=1&b=2&c=3', keep_blank_values=True),\n            [('a', '1'), ('b', '2'), ('c', '3')]\n        )\n", "    def test_url_with_control_chars(self):\n        # URLs starting with control characters are considered invalid by Chrome\n        self.assertIs(url_has_allowed_host_and_scheme('\\x01example.com', allowed_hosts={'example.com'}), False)\n", "    def test_limited_fields(self):\n        qs = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(qs, fields_limit=2), [('a', '1'), ('b', '2')])\n", "    def test_parsing(self):\n        self.assertEqual(\n            limited_parse_qsl('a=1&b=2&c=3'),\n            [('a', '1'), ('b', '2'), ('c', '3')]\n        )\n", "    def test_valid_ipv6_url(self):\n        self.assertTrue(url_has_allowed_host_and_scheme('http://[2001:db8:85a3:8d3:1319:8a2e:370:7348]:8080/', allowed_hosts={'[2001:db8:85a3:8d3:1319:8a2e:370:7348]:8080'}))\n        self.assertTrue(url_has_allowed_host_and_scheme('https://[2001:db8:85a3:8d3:1319:8a2e:370:7348]:8080/', allowed_hosts={'[2001:db8:85a3:8d3:1319:8a2e:370:7348]:8080'}, require_https=True))\n"], "sample_735": ["def test_init_params_kmeans():\n    # Test init_params='kmeans'\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n    X = rand_data.X['full']\n\n    gmm = GaussianMixture(n_components=n_components, init_params='kmeans',\n                          random_state=rng)\n    gmm.fit(X)\n\n    # Check that the means are not too far from the k-means clusters\n    kmeans = KMeans(n_clusters=n_components, random_state=rng)\n    kmeans.fit(X)\n    assert_array_almost_equal(np.sort(gmm.means_, axis=0),\n                              np.sort(kmeans.cluster_centers_, axis=0),\n                              decimal=1)\n", "def test_gaussian_mixture_precisions_cholesky():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        # Check the shape of precisions_cholesky_\n        if covar_type == 'full':\n            expected_shape = (n_components, rand_data.n_features, rand_data.n_features)\n        elif covar_type == 'tied':\n            expected_shape = (rand_data.n_features, rand_data.n_features)\n        else:\n            expected_shape = (n_components, rand_data.n_features)\n        assert_equal(gmm.precisions_cholesky_.shape, expected_shape)\n\n        # Check the precision matrices are symmetric\n        if covar_type == 'full':\n            for prec_chol in gmm.precisions_cholesky_:\n                assert_array_almost_equal(prec_chol, prec_chol.T)\n        elif covar_type == 'tied':\n            assert_array_almost_equal(gmm.precisions_cholesky_, gmm.precisions_cholesky_.T)\n", "def test_gaussian_mixture_invalid_parameters():\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 2)\n\n    # Test invalid covariance_type\n    covariance_type_bad = 'invalid_covariance_type'\n    gmm = GaussianMixture(covariance_type=covariance_type_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'covariance_type': %s \"\n                         \"'covariance_type' should be in \"\n                         \"['spherical', 'tied', 'diag', 'full']\"\n                         % covariance_type_bad,\n                         gmm.fit, X)\n\n    # Test invalid init_params\n    init_params_bad = 'invalid_method'\n    gmm = GaussianMixture(init_params=init_params_bad)\n    assert_raise_message(ValueError,\n                         \"Unimplemented initialization method '%s'\"\n                         % init_params_bad,\n                         gmm.fit, X)\n\n    # Test invalid tol\n    tol_bad = -1\n    gmm = GaussianMixture(tol=tol_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'tol': %.5f \"\n                         \"Tolerance used by the EM must be non-negative\"\n                         % tol_bad, gmm.fit, X)\n\n    # Test invalid reg_covar\n    reg_covar_bad = -1\n    gmm = GaussianMixture(reg_covar=reg_covar_bad)\n    assert_raise_message(ValueError,\n                         \"Invalid value for 'reg_covar': %.5f \"\n                         \"regularization on covariance must be \"\n                         \"non-negative\" % reg_covar_bad, gmm.fit, X)\n", "def test_gaussian_mixture_singleton_input():\n    # Test with a single data point\n    X = np.array([[1.0]])\n    g = GaussianMixture(n_components=1, covariance_type='full', random_state=0)\n    g.fit(X)\n    assert_array_equal(g.means_, X)\n    assert_array_equal(g.covariances_, [[[1e-06]]])  # Due to reg_covar\n", "def test_init_params():\n    # We check that 'kmeans' and 'random' initialization methods work correctly\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=1)\n    n_components = rand_data.n_components\n    X = rand_data.X['full']\n\n    gmm_kmeans = GaussianMixture(n_components=n_components, init_params='kmeans', random_state=rng).fit(X)\n    gmm_random = GaussianMixture(n_components=n_components, init_params='random', random_state=rng).fit(X)\n\n    # Check that the weights are initialized correctly\n    assert_array_almost_equal(gmm_kmeans.weights_, gmm_random.weights_)\n\n    # Check that the means are different because of different initialization methods\n    assert_raises(AssertionError, np.testing.assert_array_almost_equal, gmm_kmeans.means_, gmm_random.means_)\n"], "sample_932": ["def test_namespace_definitions():\n    check('namespace', 'N', {2: '1N'})\n    check('namespace', 'N::M', {2: 'N1M'})\n", "def test_build_domain_cpp_tparam_qualified_name(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"tparam-qualified-name\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:function reference target not found: f<T::typeWarn>\" in ws[0]\n    assert \"WARNING: cpp:function reference target not found: g<T::U::typeWarn>\" in ws[1]\n", "def test_anon_definitions_in_template():\n    check('class', 'template<typename T> {key}A<T>::@b', {3: \"NUt1_bEI1TE1AE\"}, asTextOutput='class template<typename T> [anonymous]::A<T>::@b')\n", "def test_build_domain_cpp_duplicate_declaration(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"duplicate-declaration\")\n    assert len(ws) == 1\n    assert \"WARNING: Duplicate C++ declaration\" in ws[0]\n", "def test_namespace_definitions():\n    check('namespace', 'A', {2: \"1A\"})\n    check('namespace', 'A::B', {2: \"N1A1BE\"})\n    check('function', 'void f(A::B a)', {1: \"f__A::B\", 2: \"1fN1A1BE\"})\n"], "sample_99": ["def test_trunc_func_with_different_timezones(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n    london = pytz.timezone('Europe/London')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated_melb=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb),\n                truncated_london=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=london)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb), truncate_to(start_datetime.astimezone(london), kind, london)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb), truncate_to(end_datetime.astimezone(london), kind, london))\n            ],\n            lambda m: (m.start_datetime, m.truncated_melb, m.truncated_london)\n        )\n\n    test_datetime_kind('year')\n    test_datetime_kind('quarter')\n    test_datetime_kind('month')\n    test_datetime_kind('week')\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n    test_datetime_kind('minute')\n", "def test_trunc_with_time_and_date_fields(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    if settings.USE_TZ:\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    # Test truncation with DateField and TimeField\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            date_truncated=Trunc('start_date', 'month', output_field=DateField())\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, truncate_to(start_datetime.date(), 'month')),\n            (end_datetime, truncate_to(end_datetime.date(), 'month'))\n        ],\n        lambda m: (m.start_datetime, m.date_truncated)\n    )\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            time_truncated=Trunc('start_time', 'hour', output_field=TimeField())\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, truncate_to(start_datetime.time(), 'hour')),\n            (end_datetime, truncate_to(end_datetime.time(), 'hour'))\n        ],\n        lambda m: (m.start_datetime, m.time_truncated)\n    )\n", "    def test_extract_func_with_new_york_timezone(self):\n        start_datetime = datetime(2015, 12, 31, 20, 0, 0, 0)\n        end_datetime = datetime(2016, 1, 1, 4, 0, 0, 0)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        qs = DTModel.objects.annotate(\n            year=Extract('start_datetime', 'year'),\n            month=Extract('start_datetime', 'month'),\n            day=Extract('start_datetime', 'day'),\n            hour=ExtractHour('start_datetime'),\n            minute=ExtractMinute('start_datetime'),\n            second=ExtractSecond('start_datetime'),\n        ).order_by('start_datetime')\n        model = qs.get()\n        self.assertEqual(model.year, 2015)\n        self.assertEqual(model.month, 12)\n        self.assertEqual(model.day, 31)\n        self.assertEqual(model.hour, 20)\n        self.assertEqual(model.minute, 0)\n        self.assertEqual(model.second, 0)\n", "def test_trunc_tz_invalid_datetime_values(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321, tzinfo=pytz.FixedOffset(-1440))\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123, tzinfo=pytz.FixedOffset(-1440))\n    self.create_model(start_datetime, end_datetime)\n    with self.assertRaisesMessage(\n        ValueError,\n        'Database returned an invalid datetime value. Are time zone definitions for your database installed?'\n    ):\n        list(DTModel.objects.annotate(truncated=Trunc('start_datetime', 'year')))\n", "def test_trunc_none_with_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(None, None)\n    melb = pytz.timezone('Australia/Melbourne')\n    for t in (Trunc('start_datetime', 'year', tzinfo=melb), Trunc('start_date', 'year', tzinfo=melb), Trunc('start_time', 'hour', tzinfo=melb)):\n        with self.subTest(t):\n            self.assertIsNone(DTModel.objects.annotate(truncated=t).first().truncated)\n"], "sample_378": ["def test_related_object_update(self):\n    related_objects = [RelatedObject.objects.create() for _ in range(10)]\n    single_objects = [SingleObject.objects.create() for _ in range(10)]\n    for related, single in zip(related_objects, single_objects):\n        related.single = single\n    RelatedObject.objects.bulk_update(related_objects, ['single'])\n    self.assertCountEqual(\n        RelatedObject.objects.values_list('single', flat=True),\n        [obj.single_id for obj in related_objects]\n    )\n", "def test_datetime_field_with_timezone(self):\n    timezone = datetime.timezone(datetime.timedelta(hours=1))\n    articles = [\n        Article.objects.create(name=str(i), created=datetime.datetime.now(timezone))\n        for i in range(10)\n    ]\n    point_in_time = datetime.datetime(1991, 10, 31, tzinfo=timezone)\n    for article in articles:\n        article.created = point_in_time\n    Article.objects.bulk_update(articles, ['created'])\n    self.assertCountEqual(Article.objects.filter(created=point_in_time), articles)\n", "def test_bulk_create_with_empty_list(self):\n    objects = []\n    result = Note.objects.bulk_create(objects)\n    self.assertEqual(len(result), 0)\n", "def test_unique_field(self):\n    unique_fields = [\n        CustomPk.objects.create(name='unique-%s' % i, extra='extra')\n        for i in range(10)\n    ]\n    for model in unique_fields:\n        model.name = 'updated-%s' % model.pk\n    CustomPk.objects.bulk_update(unique_fields, ['name'], unique_fields=['name'])\n    self.assertCountEqual(\n        CustomPk.objects.values_list('name', flat=True),\n        [model.name for model in unique_fields]\n    )\n", "def test_bulk_update_foreign_keys(self):\n    # Create related objects\n    tags = [Tag.objects.create(name=str(i)) for i in range(10)]\n    # Create notes with foreign keys\n    notes = [Note.objects.create(note=str(i), misc=str(i), tag=tags[i % 10]) for i in range(10)]\n\n    # Update the foreign keys\n    for note, tag in zip(notes, tags[5:] + tags[:5]):  # shift the tags by 5 positions\n        note.tag = tag\n    Note.objects.bulk_update(notes, ['tag'])\n\n    # Check the results\n    self.assertCountEqual(\n        Note.objects.values_list('tag', flat=True),\n        [note.tag.id for note in notes]\n    )\n"], "sample_130": ["def test_negated_transform(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(~Q(name__lower__exact='foo'))\n    self.assertTrue(where.negated)\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, SimpleCol)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n", "def test_f_expression_with_transform(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(num__gt=F('name__lower')))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertIsInstance(lookup.rhs, Lower)\n    self.assertIsInstance(lookup.rhs.lhs, SimpleCol)\n    self.assertEqual(lookup.rhs.lhs.target, Author._meta.get_field('name'))\n", "def test_transform_with_invalid_lookup(self):\n    query = Query(Author)\n    with self.assertRaises(FieldError):\n        with register_lookup(CharField, Lower):\n            query.build_where(Q(name__invalid_lookup='foo'))\n", "def test_transform_foreign_key(self):\n    query = Query(Item)\n    msg = \"Cannot resolve keyword 'creator__name__lower' into field\"\n    with self.assertRaisesMessage(FieldError, msg), register_lookup(CharField, Lower):\n        query.build_where(Q(creator__name__lower='foo'))\n", "def test_lookup_expression_in_query(self):\n    query = Query(Author)\n    where = query.build_where(Q(num__gt=F('rank__rank_value')))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertIsInstance(lookup.rhs, SimpleCol)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n    self.assertEqual(lookup.rhs.target, Ranking._meta.get_field('rank_value'))\n"], "sample_23": ["def test_angle_unicode_formatting():\n    \"\"\"\n    Tests string formatting for Angle objects with unicode separators\n    \"\"\"\n    angle = Angle(\"54.12412\", unit=u.degree)\n\n    res = \"Angle as HMS: 3h36m29.7888s\"\n    assert f\"Angle as HMS: {angle.to_string(unit=u.hour, format='unicode')}\" == res\n\n    res = \"Angle as DMS: 54d07m26.832s\"\n    assert f\"Angle as DMS: {angle.to_string(unit=u.degree, format='unicode')}\" == res\n\n    res = \"Angle as rad: 0.9446441 rad\"\n    assert f\"Angle as rad: {angle.to_string(unit=u.radian, format='unicode')}\" == res\n", "def test_angle_float_conversion():\n    \"\"\"\n    Test conversion of Angle to float\n    \"\"\"\n    a = Angle(3.60827466667, unit=u.hour)\n    assert float(a) == 3.60827466667\n\n    a = Angle(\"54:07:26.832\", unit=u.degree)\n    assert float(a) == 54.12412\n\n    a = Angle(0.944644098745, unit=u.radian)\n    assert float(a) == 0.944644098745\n\n    a = Angle(np.array([1.0, 2.0]), unit=u.degree)\n    with pytest.raises(TypeError):\n        float(a)\n", "def test_angle_hours_to_string():\n    \"\"\"\n    Test the to_string method for hourangle units.\n    \"\"\"\n    a = Angle(2.5, unit=u.hourangle)\n    assert a.to_string() == \"2h30m00s\"\n    assert a.to_string(decimal=True) == \"2.5 h\"\n    assert a.to_string(sep=':') == \"2:30:00\"\n    assert a.to_string(precision=1) == \"2h30m0.0s\"\n    assert a.to_string(alwayssign=True) == \"+2h30m00s\"\n    assert a.to_string(pad=True) == \"02h30m00s\"\n    assert a.to_string(fields=2) == \"2h30m\"\n    assert a.to_string(format='latex') == \"$2^{\\\\mathrm{h}}30^{\\\\mathrm{m}}00^{\\\\mathrm{s}}$\"\n    assert a.to_string(format='unicode') == \"2\u02b030\u1d5000\u02b3\"\n", "def test_angle_scalar():\n    \"\"\"\n    Test the creation of a scalar Angle object.\n    \"\"\"\n    a = Angle(45, unit=u.degree)\n    assert a.value == 45\n    assert a.unit == u.degree\n\n    # Check that the value can be set\n    a.value = 90\n    assert a.value == 90\n    assert a.unit == u.degree\n\n    # Check that the unit can be set\n    a.unit = u.radian\n    assert a.value == np.pi / 2\n    assert a.unit == u.radian\n", "def test_angle_from_float():\n    \"\"\"\n    Test creating Angle object from float\n    \"\"\"\n    a1 = Angle(54.12412, unit=u.degree)\n    a2 = Angle(54.12412)\n\n    assert a1 == a2\n    assert a1.unit == u.degree\n    assert a2.unit == u.degree\n"], "sample_1135": ["def test_issue_18507_2():\n    assert Mul(zoo, 0, zoo) is nan\n", "def test_issue_18507_2():\n    assert Mul(zoo, zoo, 1) is zoo\n", "def test_div_floor():\n    assert div(x, y) == x // y\n    assert div(x, 3) == x // 3\n    assert div(3, x) == 3 // x\n    assert div(x, y) == floor(x / y)\n    assert div(x, 3) == floor(x / 3)\n    assert div(3, x) == floor(3 / x)\n", "def test_div():\n    e = a/b\n    assert e == a*b**(-1)\n    e = a/b + c/2\n    assert e == a*b**(-1) + Rational(1)/2*c\n    e = (1 - b)/(b - 1)\n    assert e == (1 + -b)*((-1) + b)**(-1)\n\n    # next test\n    e = (x + y)/(x - y)\n    assert e == (x + y)*(x - y)**(-1)\n    e = (x + y)/(y - x)\n    assert e == (x + y)*(y - x)**(-1)\n", "def test_mul_doesnt_expand_exp_with_complex():\n    x, y = symbols('x y', complex=True)\n    assert unchanged(Mul, exp(x), exp(y))\n    assert unchanged(Mul, 2**x, 2**y)\n    assert x**2*x**3 == x**5\n    assert 2**x*3**x != 6**x\n    assert x**(2*y) == (x**2)**y\n    assert sqrt(2)*sqrt(2) == 2\n    assert 2**x*2**(2*x) == 2**(3*x)\n    assert sqrt(2)*2**Rational(1, 4)*5**Rational(3, 4) == 10**Rational(3, 4)\n    assert (x**(-log(5)/log(3))*x)/(x*x**( - log(5)/log(3))) == sympify(1)\n"], "sample_556": ["def test_supxlabel_supylabel_fontproperties(fig_test, fig_ref):\n    fps = mpl.font_manager.FontProperties(size='large', weight='bold')\n    txt_x = fig_test.supxlabel('fontprops xlabel', fontproperties=fps)\n    txt_y = fig_test.supylabel('fontprops ylabel', fontproperties=fps)\n    assert txt_x.get_fontsize() == fps.get_size_in_points()\n    assert txt_x.get_weight() == fps.get_weight()\n    assert txt_y.get_fontsize() == fps.get_size_in_points()\n    assert txt_y.get_weight() == fps.get_weight()\n", "def test_add_subplot_kwargs_error():\n    fig = plt.figure()\n    with pytest.raises(TypeError, match=\"projection must be a string\"):\n        fig.add_subplot(1, 1, 1, projection=123)\n    with pytest.raises(TypeError, match=\"projection must be a string\"):\n        fig.add_axes([0, 0, 1, 1], projection=123)\n", "def test_savefig_pil_kwargs():\n    fig = Figure()\n    buffer = io.BytesIO()\n    fig.savefig(buffer, format='png', pil_kwargs={\"optimize\": True})\n    buffer.seek(0)\n    img = Image.open(buffer)\n    assert img.info.get('optimize') == 'True'\n", "def test_axes_labelsize():\n    fig, ax = plt.subplots()\n    ax.set_xlabel('X Label', size=12)\n    ax.set_ylabel('Y Label', size=14)\n    ax.set_title('Title', size=16)\n\n    assert ax.xaxis.label.get_size() == 12\n    assert ax.yaxis.label.get_size() == 14\n    assert ax.title.get_size() == 16\n", "def test_set_visible():\n    fig = plt.figure()\n    ax = fig.add_subplot()\n\n    # Test setting visible to False\n    fig.set_visible(False)\n    assert not fig.get_visible()\n    assert not ax.get_visible()\n\n    # Test setting visible to True\n    fig.set_visible(True)\n    assert fig.get_visible()\n    assert ax.get_visible()\n"], "sample_371": ["def test_template_override_exception_reporter_text(self):\n    with self.assertLogs('django.request', 'ERROR'):\n        response = self.client.get('/raises500/', HTTP_ACCEPT='text/plain')\n    self.assertContains(response, 'Oh dear, an error occurred!', status_code=500)\n", "def test_unicode_error_display(self):\n    try:\n        raise UnicodeError('unicode error occurred', b'\\x80abc', 1, 2, 'ordinal not in range(128)')\n    except Exception:\n        exc_type, exc_value, tb = sys.exc_info()\n    request = self.rf.get('/test_view/')\n    reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n    html = reporter.get_traceback_html()\n    self.assertIn('Unicode error hint', html)\n    self.assertIn('The string that could not be encoded/decoded was: <strong>\\\\x80abc</strong>', html)\n    text = reporter.get_traceback_text()\n    self.assertIn('Unicode error hint\\nThe string that could not be encoded/decoded was: \\\\x80abc', text)\n", "def test_sensitive_variables_with_non_sensitive_view(self):\n    \"\"\"\n    The sensitive_variables decorator should not obfuscate variables in a\n    non-sensitive view.\n    \"\"\"\n    request = self.rf.post('/some_url/', self.breakfast_data)\n    response = non_sensitive_view(request)\n    # Non-sensitive variable's name and value are shown.\n    self.assertContains(response, 'cooked_eggs', status_code=500)\n    self.assertContains(response, 'scrambled', status_code=500)\n    # Non-sensitive POST parameters' values are shown.\n    self.assertContains(response, 'sausage-value', status_code=500)\n    self.assertContains(response, 'baked-beans-value', status_code=500)\n    self.assertContains(response, 'hash-brown-value', status_code=500)\n    self.assertContains(response, 'bacon-value', status_code=500)\n", "    def test_non_sensitive_email(self):\n        with self.settings(ADMINS=[('Admin', 'admin@example.com')]):\n            mail.outbox = []  # Empty outbox\n            self.client.get('/raises500/')\n            self.assertEqual(len(mail.outbox), 1)\n            email = mail.outbox[0]\n            self.assertIn('sausage-value', str(email.body))\n            self.assertIn('bacon-value', str(email.body))\n            self.assertIn('sausage-value', str(email.alternatives[0][0]))\n            self.assertIn('bacon-value', str(email.alternatives[0][0]))\n", "    def test_unicode_error_out_of_range(self):\n        try:\n            \"foobar\\u1234barfoo\".encode('ascii')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('<h2>Unicode error hint</h2>', html)\n        self.assertIn('The string that could not be encoded/decoded was: ', html)\n        self.assertIn('<strong>foobar&#x1234;barfoo</strong>', html)\n"], "sample_384": ["def test_functions_with_multiple_fields(self):\n    for note in self.notes:\n        note.note = \"test-%s\" % note.id\n        note.misc = Lower(\"note\")\n    Note.objects.bulk_update(self.notes, [\"note\", \"misc\"])\n    self.assertEqual(set(Note.objects.values_list(\"note\", flat=True)), {str(i) for i in range(10)})\n    self.assertEqual(set(Note.objects.values_list(\"misc\", flat=True)), {str(i).lower() for i in range(10)})\n", "def test_update_functions_with_null(self):\n    Note.objects.bulk_create([Note(note=\"TEST\", misc=\"test\") for _ in range(10)])\n    notes = Note.objects.all()\n    for note in notes:\n        note.note = Lower(F(\"note\")) if note.id % 2 == 0 else None\n    Note.objects.bulk_update(notes, [\"note\"])\n    self.assertCountEqual(\n        Note.objects.filter(note__isnull=True), notes[1::2]\n    )\n    self.assertCountEqual(\n        Note.objects.filter(note=\"test\"), notes[::2]\n    )\n", "def test_update_with_join(self):\n    notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(10)]\n    tags = [Tag.objects.create(name=str(i)) for i in range(10)]\n    for note, tag in zip(notes, tags):\n        note.tag = tag\n    Note.objects.bulk_update(notes, [\"note\"])\n    self.assertCountEqual(\n        Note.objects.values_list(\"note\", flat=True),\n        [cat.note for cat in notes],\n    )\n    with self.assertNumQueries(1):\n        Note.objects.filter(tag__in=tags).update(note=\"updated\")\n    self.assertCountEqual(\n        Note.objects.filter(tag__in=tags).values_list(\"note\", flat=True),\n        [\"updated\"] * len(tags),\n    )\n", "def test_bulk_update_with_functions_and_joins(self):\n    # Create some notes and related tags\n    tags = [Tag.objects.create(name=str(i)) for i in range(10)]\n    notes = [Note.objects.create(note=str(i), misc=str(i), tag=tags[i]) for i in range(10)]\n\n    # Update the notes' notes to be lowercase and their misc to be the tag's name\n    for note in notes:\n        note.note = Lower('note')\n        note.misc = 'tag__name'\n\n    # Perform the bulk update\n    Note.objects.bulk_update(notes, ['note', 'misc'])\n\n    # Check that the updates were successful\n    for note, tag in zip(notes, tags):\n        self.assertEqual(note.note, str(note.id).lower())\n        self.assertEqual(note.misc, tag.name)\n", "    def test_update_many_to_many_field(self):\n        food1 = Food.objects.create(name=\"Banana\")\n        food2 = Food.objects.create(name=\"Apple\")\n        tag1 = Tag.objects.create(name=\"Tag1\")\n        tag2 = Tag.objects.create(name=\"Tag2\")\n        food1.tags.add(tag1)\n        food2.tags.add(tag2)\n        food1.tags.add(tag2)\n        food2.tags.add(tag1)\n\n        # Test that updating many-to-many field raises ValueError\n        with self.assertRaisesMessage(ValueError, \"bulk_update() can only be used with concrete fields.\"):\n            Food.objects.bulk_update([food1, food2], [\"tags\"])\n\n        # Test that updating a field with a many-to-many relation raises ValueError\n        with self.assertRaisesMessage(ValueError, \"bulk_update() can only be used with concrete fields.\"):\n            Food.objects.bulk_update([food1, food2], [\"single_tag\"])\n"], "sample_203": ["    def test_url_validator_schemes(self):\n        url_validator = validators.URLValidator(schemes=['http', 'https'])\n        valid_url = 'https://www.example.com'\n        invalid_url = 'ftp://www.example.com'\n        self.assertIsNone(url_validator(valid_url))\n        with self.assertRaises(ValidationError):\n            url_validator(invalid_url)\n", "    def test_base_validator_compare_method(self):\n        base_validator = validators.BaseValidator(limit_value=10)\n        self.assertTrue(base_validator.compare(11, 10))\n        self.assertFalse(base_validator.compare(9, 10))\n", "    def test_file_extension_validator(self):\n        validator = validators.FileExtensionValidator(allowed_extensions=['txt', 'pdf'])\n        valid_file = SimpleUploadedFile('file.txt', b'content')\n        invalid_file = SimpleUploadedFile('file.doc', b'content')\n\n        self.assertIsNone(validator(valid_file))\n\n        with self.assertRaises(ValidationError) as e:\n            validator(invalid_file)\n        self.assertEqual(e.exception.message, 'File extension \u201cdoc\u201d is not allowed. Allowed extensions are: txt, pdf.')\n\n        # Testing custom message and code\n        custom_validator = validators.FileExtensionValidator(allowed_extensions=['jpg'], message='Custom message', code='custom_code')\n        with self.assertRaises(ValidationError) as e:\n            custom_validator(invalid_file)\n        self.assertEqual(e.exception.message, 'Custom message')\n        self.assertEqual(e.exception.code, 'custom_code')\n", "    def test_url_validator_with_long_domain(self):\n        long_domain = '.'.join(['a' * 63 for _ in range(4)] + ['com'])\n        long_url = f'http://{long_domain}/'\n        validator = validators.URLValidator()\n        with self.assertRaises(ValidationError):\n            validator(long_url)\n", "def test_value_placeholder_with_email_field(self):\n    cases = [\n        ('invalid_email', 'invalid'),\n        ('user@example', 'invalid'),\n        ('user@.com', 'invalid'),\n        ('user@domain.c', 'invalid'),\n        ('user@domain..com', 'invalid'),\n    ]\n    for value, code in cases:\n        with self.subTest(value=value):\n            class MyForm(forms.Form):\n                field = forms.EmailField(\n                    validators=[validators.EmailValidator()],\n                    error_messages={code: '%(value)s'},\n                )\n\n            form = MyForm({'field': value})\n            self.assertIs(form.is_valid(), False)\n            self.assertEqual(form.errors, {'field': [value]})\n"], "sample_918": ["def test_pyfunction_signature_with_empty_args(app):\n    text = \".. py:function:: hello()\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist)],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1], desc_parameterlist)\n", "def test_pyfunction_signature_with_annotations(app):\n    text = \".. py:function:: func(a: int, b: str = 'hello') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"a\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"int\"])],\n                                      [desc_parameter, ([desc_sig_name, \"b\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"str\"],\n                                                        \" \",\n                                                        [desc_sig_operator, \"=\"],\n                                                        \" \",\n                                                        [nodes.inline, \"'hello'\"])])])\n", "def test_pydecoratormethod_signature_prefix(app):\n    text = \".. py:currentmodule:: module\\n\\n.. py:decoratormethod:: Class.deco\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_addname, \"module.\"],\n                                                    [desc_name, \"Class.deco\"])],\n                                  desc_content)]))\n    assert_node(doctree[2], addnodes.desc, desctype=\"method\",\n                domain=\"py\", objtype=\"method\", noindex=False)\n\n    assert 'module.Class.deco' in domain.objects\n    assert domain.objects['module.Class.deco'] == ('index', 'module.Class.deco', 'method')\n", "def test_pyfunction_signature_full_with_default_value(app):\n    text = \".. py:function:: hello(a: str = 'default') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, [desc_parameter, ([desc_sig_name, \"a\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"str\"],\n                                                        \" \",\n                                                        [desc_sig_operator, \"=\"],\n                                                        \" \",\n                                                        [nodes.inline, \"'default'\"])])\n", "def test_pyclass_with_slots(app):\n    text = (\".. py:class:: ClassWithSlots\\n\"\n            \"   :metaclass: FooMeta\\n\"\n            \"   __slots__ = ['__dict__']\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"ClassWithSlots\"],\n                                                    [desc_annotation, \" (metaclass=FooMeta)\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"class\",\n                domain=\"py\", objtype=\"class\", noindex=False)\n\n    assert 'ClassWithSlots' in domain.objects\n    assert domain.objects['ClassWithSlots'] == ('index', 'ClassWithSlots', 'class')\n"], "sample_369": ["def test_add_custom_fk_with_hardcoded_to_field(self):\n    class HardcodedForeignKey(models.ForeignKey):\n            kwargs['to_field'] = 'name'\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs['to_field']\n            return name, path, args, kwargs\n\n    book_hardcoded_to_field = ModelState('testapp', 'Book', [\n        ('author', HardcodedForeignKey('testapp.Author', on_delete=models.CASCADE)),\n    ])\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_empty, book_hardcoded_to_field],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book')\n", "    def test_alter_db_column_with_model_change(self):\n        \"\"\"\n        Tests when model and db_column changes, autodetector must create two\n        operations.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_db_column_options],\n            [self.author_renamed_with_new_db_column_options],\n            MigrationQuestioner({\"ask_rename_model\": True}),\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"name\", db_column=\"new_column\")\n", "def test_add_custom_fk_with_to_from_db_constraint(self):\n    class CustomForeignKey(models.ForeignKey):\n            kwargs['to_field'] = models.F('id')\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs['to_field']\n            return name, path, args, kwargs\n\n    book_custom_fk_to_from_db_constraint = ModelState('testapp', 'Book', [\n        ('author', CustomForeignKey('testapp.Author', on_delete=models.CASCADE)),\n    ])\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_empty, book_custom_fk_to_from_db_constraint],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book')\n", "def test_add_custom_fk_with_hardcoded_to_field(self):\n    class HardcodedForeignKey(models.ForeignKey):\n            kwargs['to_field'] = 'custom_id'\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs['to_field']\n            return name, path, args, kwargs\n\n    book_hardcoded_fk_to_field = ModelState('testapp', 'Book', [\n        ('author', HardcodedForeignKey('testapp.Author', on_delete=models.CASCADE)),\n    ])\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_empty, book_hardcoded_fk_to_field],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book')\n", "def test_add_index_with_name_set(self):\n    author_with_index = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200, db_index=True)),\n    ], options={'indexes': [models.Index(fields=['name'], name='custom_index')]})\n    changes = self.get_changes([self.author_empty], [author_with_index])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AddIndex'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author', index=models.Index(fields=['name'], name='custom_index'))\n"], "sample_974": ["def test_ccode_AugmentedAssignment():\n    expr = aug_assign(x, '+=', y)\n    assert ccode(expr) == 'x += y;'\n    expr = aug_assign(x, '-=', y)\n    assert ccode(expr) == 'x -= y;'\n    expr = aug_assign(x, '*=', y)\n    assert ccode(expr) == 'x *= y;'\n    expr = aug_assign(x, '/=', y)\n    assert ccode(expr) == 'x /= y;'\n", "def test_ccode_matrix_multiplication():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    C = MatrixSymbol('C', 2, 2)\n    expr = A * B\n    assert ccode(expr, C) == (\n        \"for (int i=0; i<2; i++){\\n\"\n        \"   for (int j=0; j<2; j++){\\n\"\n        \"      C[i*2 + j] = 0;\\n\"\n        \"      for (int k=0; k<3; k++){\\n\"\n        \"         C[i*2 + j] += A[i*3 + k]*B[k*2 + j];\\n\"\n        \"      }\\n\"\n        \"   }\\n\"\n        \"}\"\n    )\n", "def test_ccode_Assignment_with_dereference():\n    y = symbols('y')\n    z = symbols('z', cls=Dummy)\n    expr = Assignment(y, z)\n    assert ccode(expr, dereference=[z]) == 'y = (*z);'\n", "def test_ccode_MatrixElement():\n    from sympy import MatrixSymbol, Matrix, Idx\n    n = symbols('n', integer=True)\n    i = Idx('i', n)\n    j = Idx('j', n)\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    mat = Matrix([A[i, j] * B[j, i] for i in range(n) for j in range(n)])\n    assert ccode(mat, C) == (\n        \"for (int i=0; i<n; i++){\\n\"\n        \"   for (int j=0; j<n; j++){\\n\"\n        \"      C[i*n + j] = A[i*n + j]*B[j*n + i];\\n\"\n        \"   }\\n\"\n        \"}\"\n    )\n", "def test_ccode_AugmentedAssignment():\n    expr = aug_assign(x, '+=', y)\n    assert ccode(expr) == \"x += y;\"\n    expr = aug_assign(x, '-=', y)\n    assert ccode(expr) == \"x -= y;\"\n    expr = aug_assign(x, '*=', y)\n    assert ccode(expr) == \"x *= y;\"\n    expr = aug_assign(x, '/=', y)\n    assert ccode(expr) == \"x /= y;\"\n    expr = aug_assign(x, '//=', y)\n    assert ccode(expr) == \"x /= y;\"\n    expr = aug_assign(x, '%=', y)\n    assert ccode(expr) == \"x %= y;\"\n    expr = aug_assign(x, '**=', y)\n    assert ccode(expr) == \"x = pow(x, y);\"\n"], "sample_388": ["    def test_email_configuration(self):\n        \"\"\"\n        Test that the email address is configured correctly for known and unknown users.\n        \"\"\"\n        User.objects.create(username=\"knownuser\")\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user, self.email_header: \"knownuser@example.com\"})\n        knownuser = User.objects.get(username=\"knownuser\")\n        self.assertEqual(knownuser.email, \"knownuser@example.com\")\n\n        response = self.client.get(\"/remote_user/\", **{self.header: \"newuser\", self.email_header: \"newuser@example.com\"})\n        newuser = User.objects.get(username=\"newuser\")\n        self.assertEqual(newuser.email, \"newuser@example.com\")\n", "    def configure_user(self, request, user, created=True):\n        \"\"\"\n        Sets the user's email address from the REMOTE_EMAIL header.\n        \"\"\"\n        user.email = request.META.get(RemoteUserTest.email_header, \"\")\n        user.save()\n        return user\n", "    def test_email_header(self):\n        \"\"\"\n        Tests the case where the email header is set for a known user.\n        \"\"\"\n        user = User.objects.create(username=\"knownuser\", email=\"old@example.com\")\n        num_users = User.objects.count()\n        response = self.client.get(\n            \"/remote_user/\",\n            **{\n                self.header: self.known_user,\n                self.email_header: \"new@example.com\",\n            },\n        )\n        self.assertEqual(response.context[\"user\"].username, \"knownuser\")\n        self.assertEqual(response.context[\"user\"].email, \"new@example.com\")\n        self.assertEqual(User.objects.count(), num_users)\n        user = User.objects.get(username=\"knownuser\")\n        self.assertEqual(user.email, \"new@example.com\")\n", "    def configure_user(self, request, user, created=True):\n        user.email = request.META.get(RemoteUserTest.email_header, \"\")\n        user.save()\n        return user\n", "    def test_unknown_user_without_header(self):\n        \"\"\"\n        The unknown user should not be created if the specific header is not\n        present in the request.\n        \"\"\"\n        num_users = User.objects.count()\n        response = self.client.get(\"/remote_user/\", **{self.header: \"newuser\"})\n        self.assertTrue(response.context[\"user\"].is_anonymous)\n        self.assertEqual(User.objects.count(), num_users)\n"], "sample_817": ["def test_sparse_matrix_threshold():\n    # Test VarianceThreshold with a custom variance for sparse matrices.\n    X = bsr_matrix(data)\n    X = VarianceThreshold(threshold=0.4).fit_transform(X)\n    assert_equal((len(data), 1), X.shape)\n", "def test_variance_threshold_with_constant_feature():\n    # Test VarianceThreshold with a constant feature\n    X = [[1, 2, 3], [1, 2, 3], [1, 2, 3]]\n    sel = VarianceThreshold().fit(X)\n    assert_array_equal([], sel.get_support(indices=True))\n", "def test_sparse_matrix_with_threshold():\n    # Test VarianceThreshold with custom variance on a sparse matrix.\n    X = VarianceThreshold(threshold=0.5).fit_transform(csr_matrix(data))\n    assert_equal((len(data), 2), X.shape)\n\n    # Test that the returned sparse matrix is csr_matrix\n    assert isinstance(X, csr_matrix)\n", "def test_sparse_matrix_transform():\n    # Test VarianceThreshold with sparse matrix input.\n    X = csc_matrix(data)\n    sel = VarianceThreshold().fit(X)\n    X_transformed = sel.transform(X)\n    assert_array_equal(X_transformed.toarray(), np.array(data)[:, sel.get_support()])\n\n    # Test with custom variance.\n    X_transformed = VarianceThreshold(threshold=.4).fit_transform(X)\n    assert_equal((len(data), 1), X_transformed.shape)\n", "def test_sparse_matrix_non_zero_variance():\n    # Test VarianceThreshold with non-zero variance on sparse matrices.\n    X = csc_matrix([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\n    sel = VarianceThreshold(threshold=2.0).fit(X)\n    assert_array_equal([1, 2], sel.get_support(indices=True))\n"], "sample_259": ["def test_prefetch_object_with_queryset_twice(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    book2 = Book.objects.get(id=self.book2.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', queryset=Author.objects.filter(id=self.author1.id)),\n        )\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1, book2],\n            Prefetch('authors', queryset=Author.objects.filter(id__in=[self.author1.id, self.author2.id])),\n        )\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book2.authors.all(), [self.author1])\n", "def test_prefetch_object_with_queryset_and_to_attr(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', queryset=Author.objects.filter(name='Charlotte'), to_attr='lead_author')\n        )\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.lead_author, [self.author1])\n", "def test_m2m_then_foreignkey(self):\n    \"\"\"A m2m can be followed through a foreignkey.\"\"\"\n    authors = list(Author.objects.all())\n    with self.assertNumQueries(2):\n        prefetch_related_objects(authors, 'first_book__read_by')\n\n    with self.assertNumQueries(0):\n        self.assertEqual(\n            [\n                [str(r) for r in a.first_book.read_by.all()]\n                for a in authors\n            ],\n            [\n                ['Amy'],     # Charlotte - Poems\n                ['Amy'],     # Anne - Poems\n                ['Amy'],     # Emily - Poems\n                ['Amy', 'Belinda'],   # Jane - Sense and Sense\n            ]\n        )\n", "def test_m2m_then_foreignkey(self):\n    \"\"\"A foreignkey can be followed through a m2m.\"\"\"\n    authors = list(Author.objects.all())\n    with self.assertNumQueries(2):\n        prefetch_related_objects(authors, 'books__first_time_authors')\n\n    with self.assertNumQueries(0):\n        self.assertEqual(\n            [\n                [list(b.first_time_authors.all()) for b in a.books.all()]\n                for a in authors\n            ],\n            [\n                [[self.author1], []],  # Charlotte - Poems, Jane Eyre\n                [[self.author1]],       # Anne - Poems\n                [[self.author1], []],   # Emily - Poems, Wuthering Heights\n                [[self.author4]],       # Jane - Sense and Sense\n            ]\n        )\n", "def test_prefetch_object_conflict(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertRaises(ValueError):\n        prefetch_related_objects(\n            [book1],\n            Prefetch('authors', to_attr='id'),\n        )\n"], "sample_169": ["def test_key_transform_in_annotation(self):\n    qs = NullableJSONModel.objects.annotate(\n        key=KeyTransform('a', 'value'),\n    ).filter(key__isnull=False)\n    self.assertSequenceEqual(qs, [self.objs[3], self.objs[4]])\n", "def test_key_transform_expression_with_aggregate(self):\n    self.assertEqual(\n        NullableJSONModel.objects.filter(value__d__0__isnull=False).annotate(\n            key=KeyTransform('d', 'value'),\n            chain=KeyTransform('0', 'key'),\n        ).aggregate(count=Count('chain'))['count'],\n        1,\n    )\n", "def test_key_transform_lookup_with_raw_sql(self):\n    expr = RawSQL(self.raw_sql, ['{\"x\": \"bar\"}'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr)),\n        [self.objs[7]],\n    )\n", "def test_key_transform_with_aggregates(self):\n    # Test key transforms in aggregates.\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__0__isnull=False).annotate(\n            key=KeyTransform('d', 'value'),\n        ).aggregate(max_key=Max('key__0')),\n        {'max_key': 'e'},\n    )\n", "def test_key_transform_with_list(self):\n    obj = NullableJSONModel.objects.create(value=[{'a': 1}, {'b': 'x'}])\n    self.assertIs(NullableJSONModel.objects.filter(value__has_key=KeyTransform('b', KeyTransform('1', 'value'))).exists(), True)\n    self.assertIs(NullableJSONModel.objects.filter(value__contains=KeyTransform('1', 'value')).exists(), True)\n"], "sample_561": ["def test_marker_joinstyle():\n    marker = markers.MarkerStyle(\"o\")\n    assert marker.get_joinstyle() == 'round'\n\n    marker = markers.MarkerStyle(\"o\", joinstyle='miter')\n    assert marker.get_joinstyle() == 'miter'\n\n    marker = markers.MarkerStyle(\"o\", joinstyle=markers.JoinStyle.bevel)\n    assert marker.get_joinstyle() == 'bevel'\n\n    with pytest.raises(ValueError):\n        markers.MarkerStyle(\"o\", joinstyle='invalid')\n", "def test_marker_half_fill():\n    marker = markers.MarkerStyle('o', fillstyle='right')\n    assert marker._half_fill()\n    marker = markers.MarkerStyle('o', fillstyle='top')\n    assert marker._half_fill()\n    marker = markers.MarkerStyle('o', fillstyle='bottom')\n    assert marker._half_fill()\n    marker = markers.MarkerStyle('o', fillstyle='left')\n    assert marker._half_fill()\n    marker = markers.MarkerStyle('o', fillstyle='full')\n    assert not marker._half_fill()\n    marker = markers.MarkerStyle('o', fillstyle='none')\n    assert not marker._half_fill()\n", "def test_marker_joinstyle():\n    marker = markers.MarkerStyle(\"o\")\n    assert marker.get_joinstyle() == 'round'\n\n    styled_marker = markers.MarkerStyle(\"o\", joinstyle='miter')\n    assert styled_marker.get_joinstyle() == 'miter'\n\n    assert styled_marker.get_joinstyle() != marker.get_joinstyle()\n", "def test_marker_fillstyle_update():\n    marker = markers.MarkerStyle(\"o\", fillstyle=\"full\")\n    marker._set_fillstyle(\"none\")\n    assert marker.get_fillstyle() == \"none\"\n    assert not marker.is_filled()\n\n    marker._set_fillstyle(\"left\")\n    assert marker.get_fillstyle() == \"left\"\n    assert marker.is_filled()\n\n    with pytest.raises(ValueError):\n        marker._set_fillstyle(\"invalid\")\n", "def test_marker_scaled_with_transform():\n    marker = markers.MarkerStyle(\"1\", transform=Affine2D().translate(1, 1))\n    new_marker = marker.scaled(2)\n    assert new_marker is not marker\n    expected = Affine2D().translate(1, 1).scale(2)\n    assert new_marker.get_user_transform() == expected\n    assert marker._user_transform is not new_marker._user_transform\n\n    new_marker = marker.scaled(2, 3)\n    assert new_marker is not marker\n    expected = Affine2D().translate(1, 1).scale(2, 3)\n    assert new_marker.get_user_transform() == expected\n    assert marker._user_transform is not new_marker._user_transform\n"], "sample_374": ["def test_nested_prefetch_with_select_related(self):\n    # Test that nested prefetches work with select_related().\n    with self.assertNumQueries(3):\n        qs = Room.objects.select_related('house').prefetch_related('house__occupants')\n        rooms = list(qs)\n        self.assertEqual(len(rooms), 1)\n        self.assertEqual(rooms[0].house.name, 'Big house')\n        self.assertEqual(len(rooms[0].house.occupants.all()), 0)\n", "def test_nested_prefetch_with_limit(self):\n    # Create more rooms for the house to test limit\n    for i in range(5):\n        Room.objects.create(name=f'Room {i+2}', house=self.house)\n\n    queryset = House.objects.only('name').prefetch_related(\n        Prefetch('rooms', queryset=Room.objects.prefetch_related(\n            Prefetch('house', queryset=House.objects.only('address')),\n        ).all()[:3]),\n    )\n    with self.assertNumQueries(3):\n        house = queryset.first()\n\n    # Check that only 3 rooms are prefetched\n    self.assertEqual(len(house.rooms.all()), 3)\n    self.assertIs(Room.house.is_cached(house.rooms.first()), True)\n    with self.assertNumQueries(0):\n        house.rooms.first().house.address\n", "def test_nested_prefetch_with_to_attr(self):\n    # Test nested prefetch with to_attr\n    room_qs = Room.objects.prefetch_related(\n        Prefetch('house', to_attr='home')\n    )\n    with self.assertNumQueries(2):\n        room = room_qs.get(pk=self.room.pk)\n\n    self.assertIs(Room.house.is_cached(room), False)\n    self.assertIs(Room.house.get_cached_value(room), None)\n    with self.assertNumQueries(0):\n        room.home.address\n", "def test_nested_prefetch_with_reverse_relation(self):\n    \"\"\"\n    Nested prefetching through a reverse relation doesn't cause an infinite\n    recursion of queries.\n    \"\"\"\n    teachers = Teacher.objects.all()\n    departments = Department.objects.prefetch_related(\n        Prefetch('teachers', queryset=teachers.prefetch_related('department_set')),\n    )\n    with self.assertNumQueries(3):\n        list(departments)\n", "def test_nested_prefetch_multiple_levels(self):\n    \"\"\"\n    Test that nested prefetches work correctly for multiple levels.\n    \"\"\"\n    with self.assertNumQueries(5):\n        rooms = Room.objects.prefetch_related(\n            Prefetch('house', queryset=House.objects.prefetch_related(\n                Prefetch('owner', queryset=Person.objects.only('name')),\n            )),\n        )\n        room = rooms.first()\n        self.assertEqual(room.house.owner.name, self.house1.owner.name)\n"], "sample_910": ["def test_warning_with_type_and_subtype(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1', type='test', subtype='logging')\n    assert 'WARNING: message1' in warning.getvalue()\n\n    app.config.suppress_warnings = ['test']\n    warning.truncate(0)\n    logger.warning('message2', type='test', subtype='logging')\n    assert 'WARNING: message2' not in warning.getvalue()\n\n    app.config.suppress_warnings = ['test.logging']\n    warning.truncate(0)\n    logger.warning('message3', type='test', subtype='logging')\n    assert 'WARNING: message3' not in warning.getvalue()\n", "def test_prefix_with_existing_prefix(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1')\n    with prefixed_warnings('PREFIX:'):\n        logger.warning('PREFIX: message2')\n        with prefixed_warnings('Another PREFIX:'):\n            logger.warning('Another PREFIX: message3')\n        logger.warning('PREFIX: message4')\n    logger.warning('message5')\n\n    assert 'WARNING: message1' in warning.getvalue()\n    assert 'WARNING: PREFIX: message2' in warning.getvalue()\n    assert 'WARNING: Another PREFIX: PREFIX: message3' in warning.getvalue()\n    assert 'WARNING: PREFIX: message4' in warning.getvalue()\n    assert 'WARNING: message5' in warning.getvalue()\n", "def test_logging_colors(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.debug('debug message')\n    logger.info('info message')\n    logger.verbose('verbose message')\n    logger.warning('warning message')\n    logger.error('error message')\n\n    assert colorize('darkgray', 'debug message') in status.getvalue()\n    assert 'info message' in status.getvalue()\n    assert 'verbose message' in status.getvalue()\n    assert colorize('red', 'WARNING: warning message') in warning.getvalue()\n    assert colorize('darkred', 'WARNING: error message') in warning.getvalue()\n", "def test_warning_with_type_and_subtype(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1', type='test', subtype='logging')\n    logger.warning('message2', type='test', subtype='crash')\n    logger.warning('message3', type='actual', subtype='logging')\n    assert 'message1' in warning.getvalue()\n    assert 'message2' in warning.getvalue()\n    assert 'message3' in warning.getvalue()\n\n    warning.truncate(0)\n    logger.warning('message4', type='test', subtype='logging', once=True)\n    logger.warning('message4', type='test', subtype='logging', once=True)\n    assert 'WARNING: message4' in strip_escseq(warning.getvalue())\n    assert warning.getvalue().count('WARNING: message4') == 1\n", "def test_skip_warningiserror_with_warning(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    app.warningiserror = False\n    with logging.skip_warningiserror():\n        logger.warning('message')\n\n    # warning should not raise SphinxWarning exception\n    try:\n        with logging.skip_warningiserror(False):\n            logger.warning('message')\n    except SphinxWarning:\n        pytest.fail(\"Unexpected SphinxWarning raised\")\n"], "sample_720": ["def test_power_transformer_yeojohnson():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson', standardize=standardize)\n\n        for X_trans in [X_trans_class, X_trans_func]:\n            for j in range(X_trans.shape[1]):\n                X_expected, lmbda = stats.yeojohnson(X[:, j].flatten())\n\n                if standardize:\n                    X_expected = scale(X_expected)\n\n                assert_almost_equal(X_trans[:, j], X_expected)\n                assert_almost_equal(lmbda, pt.lambdas_[j])\n\n            # Test inverse transformation\n            X_inv = pt.inverse_transform(X_trans)\n            assert_array_almost_equal(X_inv, X)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_standardize_exception():\n    pt = PowerTransformer(method='box-cox', standardize='yes')\n    X = np.abs(X_2d)\n\n    # An exception should be raised if PowerTransformer.standardize isn't valid\n    bad_standardize_message = \"'standardize' must be either True or False\"\n    assert_raise_message(ValueError, bad_standardize_message,\n                         pt.fit, X)\n", "def test_power_transformer_yellow_brick_exception():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_2d)\n\n    # An exception should be raised if PowerTransformer is used with yellowbrick\n    yellow_brick_message = \"PowerTransformer does not support yellowbrick visualization\"\n    assert_raise_message(ValueError, yellow_brick_message,\n                         pt.fit_transform, X, visualize=True)\n", "def test_power_transformer_standardize_exception():\n    pt = PowerTransformer(method='yeo-johnson')\n    X = np.abs(X_2d)\n\n    # An exception should be raised if PowerTransformer.standardize isn't valid\n    bad_standardize_message = \"'standardize' must be boolean.\"\n    pt.standardize = 'True'\n    assert_raise_message(ValueError, bad_standardize_message,\n                         pt.fit, X)\n", "def test_power_transformer_yeo_johnson_transform():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    pt = PowerTransformer(method='yeo-johnson')\n    pt.fit(X)\n    X_trans = pt.transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n    assert len(pt.lambdas_) == X.shape[1]\n    assert isinstance(pt.lambdas_, np.ndarray)\n"], "sample_792": ["def test_gnb_predict_proba():\n    # Test GaussianNB's probability scores\n\n    # Test multiclass case (2-d output, must sum to one)\n    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    clf = GaussianNB()\n    clf.fit(X, y)\n    assert_equal(clf.predict_proba(X[0:1]).shape, (1, 2))\n    assert_equal(clf.predict_proba(X[:2]).shape, (2, 2))\n    assert_almost_equal(np.sum(clf.predict_proba([X[1]])), 1)\n    assert_almost_equal(np.sum(clf.predict_proba([X[-1]])), 1)\n    assert_almost_equal(np.sum(np.exp(clf.class_log_prior_)), 1)\n", "def test_complement_nb_predict_proba():\n    # Test Complement NB classes' probability scores\n\n    # The 100s below distinguish Bernoulli from multinomial.\n    # FIXME: write a test to show this.\n    X_bernoulli = [[1, 100, 0], [0, 1, 0], [0, 100, 1]]\n    X_multinomial = [[0, 1], [1, 3], [4, 0]]\n\n    # test binary case (1-d output)\n    y = [0, 0, 2]   # 2 is regression test for binary case, 02e673\n    for cls, X in zip([ComplementNB], [X_bernoulli, X_multinomial]):\n        clf = cls().fit(X, y)\n        assert_equal(clf.predict(X[-1:]), 2)\n        assert_equal(clf.predict_proba([X[0]]).shape, (1, 2))\n        assert_array_almost_equal(clf.predict_proba(X[:2]).sum(axis=1),\n                                  np.array([1., 1.]), 6)\n\n    # test multiclass case (2-d output, must sum to one)\n    y = [0, 1, 2]\n    for cls, X in zip([ComplementNB], [X_bernoulli, X_multinomial]):\n        clf = cls().fit(X, y)\n        assert_equal(clf.predict_proba(X[0:1]).shape, (1, 3))\n        assert_equal(clf.predict_proba(X[:2]).shape, (2, 3))\n        assert_almost_equal(np.sum(clf.predict_proba([X[1]])), 1)\n        assert_almost_equal(np.sum(clf.predict_proba([X[-1]])), 1)\n", "def test_cnb_single_class():\n    # Test ComplementNB when there is only one class in the training set.\n    X = np.array([[1, 0], [1, 1], [0, 1], [0, 0]])\n    y = np.array([1, 1, 1, 1])\n\n    # Check that weights are correct.\n    theta = np.array([[0.25, 0.75]])\n    weights = -np.log(theta)\n\n    clf = ComplementNB(alpha=1.0)\n    clf.fit(X, y)\n\n    assert_array_almost_equal(clf.feature_log_prob_, weights)\n", "def test_complementnb_single_class():\n    # Test ComplementNB when there is only one class\n    X = np.array([[1, 0], [1, 1], [0, 1]])\n    y = np.array([0, 0, 0])\n\n    clf = ComplementNB(alpha=1.0)\n    clf.fit(X, y)\n\n    # Check that counts/weights are correct.\n    feature_count = np.array([[1, 2]])\n    assert_array_equal(clf.feature_count_, feature_count)\n    class_count = np.array([3])\n    assert_array_equal(clf.class_count_, class_count)\n    feature_all = np.array([3, 3])\n    assert_array_equal(clf.feature_all_, feature_all)\n\n    # Check that the class prior is set to the prior given in the constructor\n    class_prior = np.array([1.0])\n    assert_array_almost_equal(np.exp(clf.class_log_prior_), class_prior)\n", "def test_discrete_nb_sample_weight():\n    # Test whether sample weights are properly used in discrete NB classes\n    X = [[0, 1], [1, 0], [0, 1], [1, 0]]\n    y = [0, 0, 1, 1]\n    sw = np.array([1, 1, 2, 2], dtype=np.float64)\n    sw /= sw.sum()\n\n    for cls in [BernoulliNB, MultinomialNB]:\n        clf1 = cls().fit(X, y, sample_weight=sw)\n        clf2 = cls().fit(X, y)\n\n        # Check that sample weight is correctly applied\n        assert_array_almost_equal(clf1.feature_log_prob_, clf2.feature_log_prob_)\n\n        # Check that class_count_ is updated correctly\n        assert_array_almost_equal(clf1.class_count_, np.array([2, 2]))\n"], "sample_955": ["def test_unparse_multiple_indices(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_set_with_starred(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_assign(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_slice(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_assign(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n"], "sample_869": ["def test_jaccard_similarity_score_validation():\n    y_true = np.array([0, 1, 0, 1, 1])\n    y_pred = np.array([0, 1, 0, 1, 1])\n    err_msg = r\"pos_label=2 is not a valid label: array\\(\\[0, 1\\]\\)\"\n    with pytest.raises(ValueError, match=err_msg):\n        jaccard_similarity_score(y_true, y_pred, average='binary', pos_label=2)\n\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[1, 1, 1], [1, 0, 1]])\n    msg1 = (r\"Target is multilabel-indicator but average='binary'. \"\n            r\"Please choose another average setting, one of \\[None, \"\n            r\"'micro', 'macro', 'weighted', 'samples'\\].\")\n    with pytest.raises(ValueError, match=msg1):\n        jaccard_similarity_score(y_true, y_pred, average='binary', pos_label=-1)\n\n    y_true = np.array([0, 1, 1, 0, 2])\n    y_pred = np.array([1, 1, 1, 1, 0])\n    msg2 = (r\"Target is multiclass but average='binary'. Please choose \"\n            r\"another average setting, one of \\[None, 'micro', 'macro', \"\n            r\"'weighted'\\].\")\n    with pytest.raises(ValueError, match=msg2):\n        jaccard_similarity_score(y_true, y_pred, average='binary')\n    msg3 = (\"Samplewise metrics are not available outside of multilabel \"\n            \"classification.\")\n    with pytest.raises(ValueError, match=msg3):\n        jaccard_similarity_score(y_true, y_pred,", "def test_multilabel_jaccard_similarity_score_with_sample_weight():\n    # Dense label indicator matrix format\n    y1 = np.array([[0, 1, 1], [1, 0, 1]])\n    y2 = np.array([[0, 0, 1], [1, 0, 1]])\n    w = np.array([1, 3])\n\n    assert jaccard_similarity_score(y1, y2, sample_weight=w) == 0.875\n    assert jaccard_similarity_score(y1, y1, sample_weight=w) == 1\n    assert jaccard_similarity_score(y2, y2, sample_weight=w) == 1\n    assert jaccard_similarity_score(y2, np.logical_not(y2), sample_weight=w) == 0\n    assert jaccard_similarity_score(y1, np.logical_not(y1), sample_weight=w) == 0\n    assert jaccard_similarity_score(y1, np.zeros(y1.shape), sample_weight=w) == 0\n    assert jaccard_similarity_score(y2, np.zeros(y1.shape), sample_weight=w) == 0\n", "def test_jaccard_similarity_score_sparse():\n    # Sparse label indicator matrix format\n    y1 = csr_matrix([[0, 1, 1], [1, 0, 1]])\n    y2 = csr_matrix([[0, 0, 1], [1, 0, 1]])\n\n    # size(y1 \\inter y2) = [1, 2]\n    # size(y1 \\union y2) = [2, 2]\n\n    assert jaccard_similarity_score(y1, y2) == 0.75\n    assert jaccard_similarity_score(y1, y1) == 1\n    assert jaccard_similarity_score(y2, y2) == 1\n    assert jaccard_similarity_score(y2, csr_matrix(np.logical_not(y2.toarray()))) == 0\n    assert jaccard_similarity_score(y1, csr_matrix(np.logical_not(y1.toarray()))) == 0\n    assert jaccard_similarity_score(y1, csr_matrix(np.zeros(y1.shape))) == 0\n    assert jaccard_similarity_score(y2, csr_matrix(np.zeros(y1.shape))) == 0\n", "def test_multilabel_jaccard_similarity_score_average():\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[0, 0, 1], [1, 1, 1]])\n\n    # average='micro'\n    assert_almost_equal(jaccard_similarity_score(y_true, y_pred, average='micro'), 2. / 5)\n\n    # average='macro'\n    assert_almost_equal(jaccard_similarity_score(y_true, y_pred, average='macro'), 2. / 3)\n\n    # average='weighted'\n    assert_almost_equal(jaccard_similarity_score(y_true, y_pred, average='weighted'), 7. / 8)\n\n    # average='samples'\n    assert_almost_equal(jaccard_similarity_score(y_true, y_pred, average='samples'), 2. / 3)\n", "def test_balanced_accuracy_score_with_sample_weight():\n    y_true = ['a', 'b', 'a', 'b']\n    y_pred = ['a', 'a', 'a', 'b']\n    sample_weight = [1, 1, 2, 1]\n    macro_recall = recall_score(y_true, y_pred, average='macro',\n                                labels=np.unique(y_true), sample_weight=sample_weight)\n    with ignore_warnings():\n        # Warnings are tested in test_balanced_accuracy_score_unseen\n        balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    assert balanced == pytest.approx(macro_recall)\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True, sample_weight=sample_weight)\n    chance = balanced_accuracy_score(y_true, np.full_like(y_true, y_true[0]), sample_weight=sample_weight)\n    assert adjusted == (balanced - chance) / (1 - chance)\n"], "sample_1177": ["def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', real=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == f(x)/Abs(f(x))\n", "def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', real=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x)) * Derivative(f(x), x)\n", "def test_Abs_complex_assumptions():\n    c = Symbol('c', complex=True, finite=True)\n    assert Abs(c).is_zero is False\n    assert Abs(c).is_real is True\n    assert Abs(c).is_extended_real is True\n    assert Abs(c).is_positive is True\n    assert Abs(c).is_nonnegative is True\n    assert Abs(c).is_extended_positive is True\n    assert Abs(c).is_extended_nonnegative is True\n    assert Abs(c).is_rational is None\n    assert Abs(c).is_integer is None\n    assert Abs(c).is_even is None\n    assert Abs(c).is_odd is None\n", "def test_abs_symbol_properties():\n    x = Symbol('x', real=True)\n    assert Abs(x).is_real is True\n    assert Abs(x).is_finite is True\n    assert Abs(x).is_complex is False\n    assert Abs(x).is_algebraic is None\n\n    x = Symbol('x', imaginary=True)\n    assert Abs(x).is_real is True\n    assert Abs(x).is_finite is True\n    assert Abs(x).is_complex is False\n    assert Abs(x).is_algebraic is None\n\n    x = Symbol('x', complex=True)\n    assert Abs(x).is_real is True\n    assert Abs(x).is_finite is None\n    assert Abs(x).is_complex is False\n    assert Abs(x).is_algebraic is None\n", "def test_Abs_nseries():\n    x = Symbol('x')\n    assert Abs(x).nseries(x, n=2) == x + O(x**2)\n\n    y = Symbol('y', positive=True)\n    assert Abs(y).nseries(y, n=2) == y + O(y**2)\n\n    z = Symbol('z', negative=True)\n    assert Abs(z).nseries(z, n=2) == -z + O(z**2)\n"], "sample_965": ["def test_isabstractmethod(app):\n    from target.methods import Base, Inherited\n\n    assert inspect.isabstractmethod(Base.abstractmeth) is True\n    assert inspect.isabstractmethod(Base.meth) is False\n    assert inspect.isabstractmethod(Inherited.abstractmeth) is True\n    assert inspect.isabstractmethod(Inherited.meth) is False\n", "def test_signature_type_aliases():\n    class MyModule:\n        class MyClass:\n            pass\n\n    type_aliases = {'MyModule.MyClass': 'AliasClass'}\n    sig = inspect.signature(MyModule.MyClass, type_aliases=type_aliases)\n    assert stringify_signature(sig) == '()'\n    assert sig.return_annotation == 'AliasClass'\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    sig = inspect.evaluate_signature(sig)\n\n    assert sig.parameters['x'].annotation == int\n    assert sig.parameters['y'].annotation == List[str]\n    assert sig.return_annotation == Optional[int]\n", "def test_getorigbases():\n    class Base:\n        pass\n\n    class Derived(Base):\n        pass\n\n    class Generic(GenericBase):\n        pass\n\n    assert inspect.getorigbases(Derived) == (Base,)\n    assert inspect.getorigbases(Generic) == (GenericBase,)\n    assert inspect.getorigbases(int) is None  # int is not a class\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig, globals())\n    assert evaluated_sig.parameters['a'].annotation == list[int]\n    assert evaluated_sig.parameters['b'].annotation == dict[str, int]\n    assert evaluated_sig.parameters['c'].annotation == Optional[Callable]\n    assert evaluated_sig.return_annotation == Union[str, int]\n"], "sample_775": ["def test_dict_n_max_elements_to_show():\n\n    n_max_elements_to_show = 30\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    # No ellipsis\n    dictionary = {i: i for i in range(n_max_elements_to_show)}\n    expected = \"\"\"", "def test_compact_representation():\n    # Test the compact representation\n    lr = LogisticRegression(C=99, class_weight=0.4, fit_intercept=False, tol=1234, verbose=True)\n    pp = _EstimatorPrettyPrinter(compact=True)\n    expected = \"LogisticRegression(C=99, class_weight=0.4, fit_intercept=False, tol=1234, verbose=True)\"\n    assert pp.pformat(lr) == expected\n", "def test_nested_dict_in_estimator():\n    # Test the representation of an estimator with a nested dictionary\n    params = {'param1': 1, 'param2': {'nested_param1': 2, 'nested_param2': 3}}\n    class NestedDictEstimator:\n            self.params = params\n            return self.params\n\n    estimator = NestedDictEstimator(**params)\n    expected = \"\"\"", "def test_nested_dict():\n    # Render a nested dict\n    nested_dict = {'a': 1, 'b': {'c': 2, 'd': {'e': 3, 'f': 4}}}\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1)\n    expected = \"\"\"", "def test_indent_at_name():\n    # Test indent_at_name parameter of _EstimatorPrettyPrinter\n    pp = _EstimatorPrettyPrinter(compact=True, indent=2, indent_at_name=False)\n\n    lr = LogisticRegression(C=99)\n    expected = \"\"\"\n  LogisticRegression(\n    C=99,\n    class_weight=None,\n    dual=False,\n    fit_intercept=True,\n    intercept_scaling=1,\n    l1_ratio=None,\n    max_iter=100,\n    multi_class='warn',\n    n_jobs=None,\n    penalty='l2',\n    random_state=None,\n    solver='warn',\n    tol=0.0001,\n    verbose=0,\n    warm_start=False)\"\"\"\n    expected = expected[1:]  # remove first \\n\n    assert pp.pformat(lr) == expected\n\n    # Check with a repr that doesn't fit on a single line\n    lr = LogisticRegression(C=99, class_weight=.4, fit_intercept=False,\n                            tol=1234, verbose=True)\n    expected = \"\"\"\n  LogisticRegression(\n    C=99,\n    class_weight=0.4,\n    dual=False,\n    fit_intercept=False,\n    intercept_scaling=1,\n    l1_ratio=None,\n    max_iter=100,\n    multi_class='warn',\n    n_jobs=None,\n    penalty='l2',\n    random_state=None,\n    solver='warn',\n    tol=1234,\n    verbose=True,\n    warm_start=False)\"\"\"\n    expected = expected[1:]  # remove first \\n\n    assert pp.pformat(lr) == expected\n\n    # Test with indent_at_name=True, should be the same as test_basic\n    pp = _EstimatorPrettyPrinter(compact=True, indent="], "sample_325": ["def test_form_subclassing_with_renderer(self):\n    class CustomForm(Form):\n        default_renderer = CustomRenderer()\n\n    form = CustomForm()\n    self.assertIsInstance(form.renderer, CustomRenderer)\n\n    custom = DjangoTemplates()\n    form = CustomForm(renderer=custom)\n    self.assertEqual(form.renderer, custom)\n", "def test_field_deep_copy_validators(self):\n    class CustomValidator(Validator):\n            self.message = message\n\n            if value != 'valid':\n                raise ValidationError(self.message)\n\n    field = CharField(validators=[CustomValidator('Form custom error message.')])\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsNot(field_copy.validators, field.validators)\n    self.assertIsNot(field_copy.validators[0], field.validators[0])\n", "    def test_attribute_error(self):\n        class CustomForm(Form):\n            default_renderer = 'not a renderer'\n\n        with self.assertRaisesMessage(ImproperlyConfigured, \"Form renderer must be a renderer instance.\"):\n            CustomForm()\n", "def test_render_with_custom_renderer(self):\n    class CustomRenderer(DjangoTemplates):\n            return \"Custom rendered output\"\n\n    form = Form(renderer=CustomRenderer())\n    output = form.render('template_name', {'form': form})\n    self.assertEqual(output, \"Custom rendered output\")\n", "    def test_error_list_escaping(self):\n        e = ErrorList()\n        e.append('Foo & \"bar\"!')\n        e.append(ValidationError('Foo%(bar)s', code='foobar', params={'bar': ' & \"baz\"!'}))\n\n        self.assertEqual(e.as_text(), '* Foo &amp; &quot;bar&quot;!\\n* Foo &amp; &quot;baz&quot;!')\n        self.assertEqual(e.as_ul(), '<ul class=\"errorlist\"><li>Foo &amp; &quot;bar&quot;!</li><li>Foo &amp; &quot;baz&quot;!</li></ul>')\n        errors = e.get_json_data()\n        self.assertEqual(errors, [{\"message\": \"Foo &amp; &quot;bar&quot;!\", \"code\": \"\"}, {\"message\": \"Foo &amp; &quot;baz&quot;!\", \"code\": \"foobar\"}])\n"], "sample_205": ["def test_iter(self):\n    error_dict = {'field1': ['E1', 'E2'], 'field2': ['E3']}\n    exception = ValidationError(error_dict)\n    error_iter = iter(exception)\n    self.assertEqual(next(error_iter), ('field1', ['E1', 'E2']))\n    self.assertEqual(next(error_iter), ('field2', ['E3']))\n    with self.assertRaises(StopIteration):\n        next(error_iter)\n\n    error_list = ['E1', ValidationError('E2')]\n    exception = ValidationError(error_list)\n    error_iter = iter(exception)\n    self.assertEqual(next(error_iter), 'E1')\n    self.assertEqual(next(error_iter), 'E2')\n    with self.assertRaises(StopIteration):\n        next(error_iter)\n\n    exception = ValidationError('E1')\n    error_iter = iter(exception)\n    self.assertEqual(next(error_iter), 'E1')\n    with self.assertRaises(StopIteration):\n        next(error_iter)\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError('message')\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {NON_FIELD_ERRORS: ['message']})\n\n        error_dict = {}\n        exception = ValidationError({'field1': 'error1'})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1']})\n\n        error_dict = {'field2': ['error2']}\n        exception = ValidationError({'field1': 'error1'})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n        error_dict = {'field1': ['error1'], 'field2': ['error2']}\n        exception = ValidationError({'field2': 'error3'})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1'], 'field2': ['error2', 'error3']})\n", "    def test_message_dict_raises_attribute_error(self):\n        error = ValidationError('message')\n        with self.assertRaises(AttributeError):\n            error.message_dict\n", "    def test_message_dict_property(self):\n        error_dict = {\n            'field1': ['E1', 'E2'],\n            'field2': ['E3', 'E4']\n        }\n        exception = ValidationError(error_dict)\n        self.assertEqual(exception.message_dict, error_dict)\n\n        with self.assertRaises(AttributeError):\n            exception = ValidationError('message')\n            exception.message_dict\n", "def test_message_dict(self):\n    message_dict = {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']}\n    exception = ValidationError(message_dict)\n    self.assertEqual(exception.message_dict, message_dict)\n\n    with self.assertRaises(AttributeError):\n        ValidationError('message').message_dict\n"], "sample_85": ["def test_fast_delete_cascade(self):\n    a = Avatar.objects.create()\n    u = User.objects.create(avatar=a)\n    # 1 query to fast-delete the user\n    # 1 query to delete the avatar\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n", "def test_fast_delete_reverse_m2m_through(self):\n    m = M.objects.create()\n    r = R.objects.create()\n    MR.objects.create(m=m, r=r)\n    # 1 to delete r, 1 to fast-delete MR for r\n    self.assertNumQueries(2, r.delete)\n    self.assertFalse(MR.objects.exists())\n", "def test_fast_delete_hidden_related(self):\n    r = R.objects.create()\n    h = HiddenUser.objects.create(r=r)\n    HiddenUserProfile.objects.create(user=h)\n\n    # 1 for r, 1 for fast delete of related hidden users\n    self.assertNumQueries(2, r.delete)\n    self.assertEqual(HiddenUserProfile.objects.count(), 0)\n", "def test_fast_delete_self_referential_fk(self):\n    parent = Parent.objects.create()\n    child = Child.objects.create(parent_ptr=parent)\n    # 1 for deleting child, 1 for fast deleting child's parent reference\n    self.assertNumQueries(2, child.delete)\n    self.assertFalse(Parent.objects.exists())\n    self.assertFalse(Child.objects.exists())\n", "def test_fast_delete_multi_table_inheritance(self):\n    \"\"\"\n    Test fast deletion on models with multi-table inheritance.\n    \"\"\"\n    class Base(models.Model):\n        name = models.CharField(max_length=10)\n\n        class Meta:\n            abstract = True\n\n    class Child(Base):\n        extra_field = models.CharField(max_length=20)\n\n    Base.objects.create(name='base')\n    Child.objects.create(name='child', extra_field='extra')\n\n    # Fast delete should work for the Child model\n    self.assertNumQueries(1, Child.objects.all().delete)\n    self.assertEqual(Base.objects.count(), 1)\n    self.assertEqual(Child.objects.count(), 0)\n\n    # Fast delete should work for the Base model, cascading to the Child model\n    self.assertNumQueries(2, Base.objects.all().delete)\n    self.assertEqual(Base.objects.count(), 0)\n    self.assertEqual(Child.objects.count(), 0)\n"], "sample_634": ["def test_expand_modules_import_error(self, files_or_modules, expected):\n    \"\"\"Test expand_modules when an ImportError is raised\"\"\"\n    ignore_list, ignore_list_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    assert modules == []\n    assert errors == expected\n", "def test_get_python_path(filepath, expected):\n    \"\"\"Test the get_python_path function with various file paths\"\"\"\n    assert get_python_path(filepath) == expected\n", "def test_expand_modules_file_does_not_exist(self, files_or_modules, expected):\n    \"\"\"Test expand_modules with a file that does not exist\"\"\"\n    ignore_list, ignore_list_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self.checker, \"ignore-paths\"),\n    )\n    assert modules == expected\n    assert len(errors) == 1\n    assert errors[0][\"key\"] == \"fatal\"\n    assert errors[0][\"mod\"] == os.path.splitext(os.path.basename(files_or_modules[0]))[0]\n    assert isinstance(errors[0][\"ex\"], ImportError)\n", "def test_expand_modules_import_error(self, files_or_modules, expected):\n    \"\"\"Test expand_modules with an import error\"\"\"\n    ignore_list, ignore_list_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    assert modules == []\n    assert errors == expected\n", "def test_expand_modules_single_module(self, files_or_modules, expected):\n    \"\"\"Test expand_modules with single modules and a default value of ignore-paths\"\"\"\n    ignore_list, ignore_list_re = [], []\n    modules, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        get_global_option(self, \"ignore-paths\"),\n    )\n    assert modules == expected\n    assert errors == [] if expected else [{\"key\": \"fatal\", \"mod\": files_or_modules[0], \"ex\": Exception}]\n"], "sample_909": ["def test_attributes_with_ivar(self):\n    docstring = \"\"\"\\", "def test_kwargs_with_types(self):\n    docstring = \"\"\"\\", "def test_attributes_with_asterisk_prefix(self):\n    docstring = \"\"\"\\", "    def test_code_block_in_keyword_arguments(self):\n        docstring = \"\"\"", "def test_usage_section(self):\n    docstring = \"\"\""], "sample_612": ["def test_groupby_multidim_map_apply():\n    array = make_groupby_multidim_example_array()\n    actual = array.groupby(\"lon\").apply(lambda x: x - x.mean())\n    expected = DataArray(\n        [[[-2.5, -6.0], [-5.0, -8.5]], [[2.5, 3.0], [8.0, 8.5]]],\n        coords=array.coords,\n        dims=array.dims,\n    )\n    assert_identical(expected, actual)\n", "def test_groupby_bins_multidim_float_coords():\n    array = make_groupby_multidim_example_array()\n    bins = [0.0, 15.5, 20.0]\n    bin_coords = pd.cut(array[\"lat\"].values.flat, bins).categories\n    expected = DataArray([16, 40], dims=\"lat_bins\", coords={\"lat_bins\": bin_coords})\n    actual = array.groupby_bins(\"lat\", bins).map(lambda x: x.sum())\n    assert_identical(expected, actual)\n    # modify the array coordinates to be non-monotonic after unstacking\n    array[\"lat\"].data = np.array([[10.0, 20.0], [20.0, 10.0]])\n    expected = DataArray([28, 28], dims=\"lat_bins\", coords={\"lat_bins\": bin_coords})\n    actual = array.groupby_bins(\"lat\", bins).map(lambda x: x.sum())\n    assert_identical(expected, actual)\n", "def test_ds_groupby_last(self):\n    ds = xr.Dataset({'foo': ('x', [1, 2, 3, 4])}, {'bar': ('x', [1, 1, 2, np.nan])})\n    actual = ds.groupby('bar').last()\n    expected = xr.Dataset({'foo': ('bar', [2, 4]), 'bar': [1, 2]})\n    assert_identical(actual, expected)\n", "def test_groupby_bins_datetime64(dataset):\n    # test groupby_bins with a DataArray of dtype datetime64 for GH1132\n    # create test data\n    times = pd.date_range(\"2000-01-01\", periods=4)\n    foo = xr.DataArray([1, 2, 3, 4], coords=dict(time=times), dims=\"time\")\n    # create test index\n    dd = times.to_pydatetime()\n    reference_dates = [dd[0], dd[2]]\n    labels = reference_dates[0:1] * 2 + reference_dates[1:2] * 2\n    ind = xr.DataArray(\n        labels, coords=dict(time=times), dims=\"time\", name=\"reference_date\"\n    )\n    g = foo.groupby_bins(ind, bins=[dd[0], dd[1], dd[3]])\n    actual = g.sum(dim=\"time\")\n    expected = xr.DataArray(\n        [3, 7], coords=dict(reference_date_bins=[pd.Interval(dd[0], dd[1], closed='right'), pd.Interval(dd[1], dd[3], closed='right')]), dims=\"reference_date_bins\"\n    )\n    assert_equal(expected, actual)\n", "def test_first_or_last(self):\n    array = DataArray([1, 2, 3, 4, 5], dims=\"x\")\n    by = DataArray([\"a\"] * 2 + [\"b\"] * 3, dims=\"x\", name=\"ab\")\n\n    expected = DataArray([1, 3], [(\"ab\", [\"a\", \"b\"])])\n    actual = array.groupby(by)._first_or_last(duck_array_ops.first)\n    assert_identical(expected, actual)\n\n    array = DataArray(np.random.randn(5, 3), dims=[\"x\", \"y\"])\n    expected = DataArray(array[[0, 2]], {\"ab\": [\"a\", \"b\"]}, [\"ab\", \"y\"])\n    actual = array.groupby(by)._first_or_last(duck_array_ops.first)\n    assert_identical(expected, actual)\n"], "sample_399": ["def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum(\"price\", default=0.0),\n    )\n    self.assertEqual(result[\"value\"], 0.0)\n", "def test_avg_date_field(self):\n    with self.assertRaisesMessage(FieldError, \"Cannot compute Avg('pubdate'): 'pubdate' is a DateField.\"):\n        Publisher.objects.aggregate(avg_pubdate=Avg(\"pubdate\"))\n", "def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum(\"rating\", default=0.0),\n    )\n    self.assertEqual(result[\"value\"], 0.0)\n", "def test_aggregation_with_aggregate(self):\n    result = Publisher.objects.aggregate(\n        sum_awards=Sum(\"num_awards\"),\n        avg_awards=Avg(\"num_awards\"),\n    )\n    self.assertEqual(result[\"sum_awards\"], 20)\n    self.assertAlmostEqual(result[\"avg_awards\"], 2.5, places=1)\n", "def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum(\"rating\", default=0.0),\n    )\n    self.assertEqual(result[\"value\"], 0.0)\n"], "sample_91": ["    def test_permission_denied(self):\n        \"The permission_denied view raises a 403 status\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception('Permission Denied'))\n        self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "def test_permission_denied_custom_template(self):\n    \"\"\"\n    403.html template is picked by permission_denied handler and contains exception.\n    \"\"\"\n    request = self.request_factory.get('/')\n    exception = Exception('Permission Denied')\n    response = permission_denied(request, exception)\n    self.assertContains(response, 'exception: Permission Denied', status_code=403)\n", "    def test_exception_message_in_404(self):\n        \"\"\"\n        The 404 page should display the exception message if it's a string\n        \"\"\"\n        exception = Http404(\"Testing technical 404.\")\n        request = self.request_factory.get('/technical404/')\n        response = page_not_found(request, exception)\n        self.assertContains(response, \"Testing technical 404.\", status_code=404)\n", "def test_csrf_token_in_error_pages(self):\n    \"\"\"\n    Error pages should have the csrf_token available in the context\n    \"\"\"\n    request = self.request_factory.get('/')\n    for error_func in (bad_request, permission_denied, page_not_found, server_error):\n        response = error_func(request, Exception())\n        self.assertNotEqual(response.content, b'NOTPROVIDED')\n        self.assertNotEqual(response.content, b'')\n", "def test_permission_denied_custom_template(self):\n    \"\"\"\n    403.html template is picked by permission_denied handler and displays the exception.\n    \"\"\"\n    request = self.request_factory.get('/')\n    exception_message = 'Permission Denied: Access is not allowed.'\n    response = permission_denied(request, Exception(exception_message))\n    self.assertContains(response, exception_message, status_code=403)\n"], "sample_1104": ["def test_MatPow():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert str(A**2) == \"A**2\"\n    assert str(A**-1) == \"A**(-1)\"\n    assert str(A**-2) == \"A**(-2)\"\n", "def test_Issue_14567_2():\n    assert Sum(factorial(x), (x, 0, 0)) + y == y + 1\n", "def test_MatrixDeterminant():\n    A = Matrix([[1, 2], [3, 4]])\n    assert str(A.det()) == \"-2\"\n\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert str(B.det()) == \"Det(B)\"\n", "def test_AlgebraicNumber():\n    from sympy import sqrt, AlgebraicNumber\n    alpha = sqrt(2) + sqrt(3)\n    assert str(AlgebraicNumber(alpha, sqrt(2) + sqrt(3))) == \"sqrt(2) + sqrt(3)\"\n    assert str(AlgebraicNumber(alpha, sqrt(2) - sqrt(3))) == \"(sqrt(2) + sqrt(3))**2\"\n", "def test_AlgebraicNumber():\n    from sympy import sqrt, AlgebraicNumber\n    n = sqrt(2) + sqrt(3)\n    assert str(AlgebraicNumber(n, n.expand())) == \"sqrt(2) + sqrt(3)\"\n    assert str(AlgebraicNumber(n, n.expand(), alias=True)) == \"sqrt(2) + sqrt(3)\"\n"], "sample_293": ["    def test_valid_resolve(self):\n        test_urls = [\n            '/route-pattern/test/123/',\n            '/route-pattern-converter/test/123/',\n        ]\n        for test_url in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, {'param': '123'})\n", "    def test_resolver_match_with_args_and_kwargs(self):\n        # Test that ResolverMatch works correctly when both args and kwargs are present\n        match = resolve('/mixed_args/42/37/')\n        self.assertEqual(match.url_name, 'mixed-args')\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'arg2': '37'})\n", "def test_include_non_string(self):\n    with self.assertRaisesMessage(ValueError, \"include() argument must be the urlconf module, a URL pattern object, or a tuple in the form (<urlconf_module>, <app_namespace>).\"):\n        include(123)\n", "    def test_pattern_name_with_colon(self):\n        \"\"\"\n        Names containing a colon are not allowed.\n        \"\"\"\n        msg = \"Your URL pattern '<int:id>/' has a name including a ':'. Remove the colon to avoid ambiguous namespace references.\"\n        with self.assertRaisesMessage(Warning, msg):\n            pattern = RegexPattern(r'^(?P<int:id>[0-9]+)/$', name='invalid:name')\n            URLPattern(pattern, views.empty_view)\n", "    def test_nested_view_class(self):\n        test_urls = [\n            ('nested_view_class', [], {}, '/nested_view_class/'),\n            ('nested_view_class', [37, 42], {}, '/nested_view_class/37/42/'),\n            ('nested_view_class', [], {'arg1': 42, 'arg2': 37}, '/nested_view_class/42/37/'),\n        ]\n        for name, args, kwargs, expected in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs):\n                self.assertEqual(reverse(name, args=args, kwargs=kwargs), expected)\n"], "sample_56": ["def test_search_fields_not_a_list_or_tuple(self):\n    class SongAdmin(admin.ModelAdmin):\n        search_fields = 'test'\n\n    self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'search_fields' must be a list or tuple.\",\n            obj=SongAdmin,\n            id='admin.E126',\n        )\n    ])\n", "def test_autocomplete_fields_not_list_or_tuple(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = 'test'\n\n    self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'autocomplete_fields' must be a list or tuple.\",\n            obj=SongAdmin,\n            id='admin.E036',\n        )\n    ])\n", "def test_invalid_list_display(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = ('title', 'invalid_field')\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_display[1]' refers to 'invalid_field', which is not a callable, \"\n            \"an attribute of 'SongAdmin', or an attribute or method on 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E108',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_display_links_invalid_field(self):\n    \"\"\"\n    Test that list_display_links contains a valid field.\n    \"\"\"\n    class SongAdmin(admin.ModelAdmin):\n        list_display = (\"title\", \"original_release\")\n        list_display_links = (\"invalid_field\",)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_display_links[0]' refers to 'invalid_field', \"\n            \"which is not defined in 'list_display'.\",\n            obj=SongAdmin,\n            id='admin.E111',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_filter_vertical_and_horizontal_with_non_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        filter_vertical = ['title', 'nonexistent']\n        filter_horizontal = ['title', 'nonexistent']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'filter_vertical[0]' refers to 'title', which is not a many-to-many field.\",\n            obj=SongAdmin,\n            id='admin.E003',\n        ),\n        checks.Error(\n            \"The value of 'filter_vertical[1]' refers to 'nonexistent', which is not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E019',\n        ),\n        checks.Error(\n            \"The value of 'filter_horizontal[0]' refers to 'title', which is not a many-to-many field.\",\n            obj=SongAdmin,\n            id='admin.E003',\n        ),\n        checks.Error(\n            \"The value of 'filter_horizontal[1]' refers to 'nonexistent', which is not an attribute of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E019',\n        ),\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_260": ["def test_create_alter_options_field(self):\n    options = {'verbose_name': 'New Name'}\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AlterModelOptions(\"Foo\", options),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))], options=options),\n        ],\n    )\n", "def test_create_model_add_constraint(self):\n    \"\"\"\n    AddConstraint should optimize into CreateModel.\n    \"\"\"\n    managers = [('objects', EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={'verbose_name': 'Foo'},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AddConstraint(\"Foo\", models.CheckConstraint(check=models.Q(age__gte=18), name=\"age_gte_18\")),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n                options={'verbose_name': 'Foo', 'constraints': [models.CheckConstraint(check=models.Q(age__gte=18), name=\"age_gte_18\")]},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_create_model_with_unique_together(self):\n    \"\"\"\n    CreateModel with unique_together should optimize into CreateModel with unique_together.\n    \"\"\"\n    unique_together = [(\"name\", \"age\")]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [\n                (\"name\", models.CharField(max_length=255)),\n                (\"age\", models.IntegerField()),\n            ]),\n            migrations.AlterUniqueTogether(\"Foo\", unique_together),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [\n                (\"name\", models.CharField(max_length=255)),\n                (\"age\", models.IntegerField()),\n            ], options={'unique_together': unique_together}),\n        ],\n    )\n", "def test_create_model_alter_together(self):\n    \"\"\"\n    AlterUniqueTogether and AlterIndexTogether should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [\n                (\"a\", models.IntegerField()),\n                (\"b\", models.IntegerField()),\n            ]),\n            migrations.AlterUniqueTogether(\"Foo\", [[\"a\", \"b\"]]),\n            migrations.AlterIndexTogether(\"Foo\", [[\"a\", \"b\"]]),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [\n                (\"a\", models.IntegerField()),\n                (\"b\", models.IntegerField()),\n            ], options={\n                'unique_together': {('a', 'b')},\n                'index_together': {('a', 'b')},\n            }),\n        ],\n    )\n", "def test_create_model_add_m2m_field(self):\n    \"\"\"\n    AddField should optimize into CreateModel for M2M fields.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.CreateModel(\"Bar\", [(\"title\", models.CharField(max_length=255))]),\n            migrations.AddField(\"Foo\", \"bars\", models.ManyToManyField(\"Bar\")),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [\n                (\"name\", models.CharField(max_length=255)),\n                (\"bars\", models.ManyToManyField(\"Bar\")),\n            ]),\n            migrations.CreateModel(\"Bar\", [(\"title\", models.CharField(max_length=255))]),\n        ],\n    )\n"], "sample_889": ["def test_calibration_with_sample_weight_and_fit_params(data):\n    \"\"\"Tests that sample_weight is passed to the underlying base\n    estimator along with fit_params.\"\"\"\n    X, y = data\n    sample_weight = np.ones_like(y)\n    fit_params = {\"a\": _convert_container(y, \"list\")}\n\n    clf = CheckingClassifier(expected_sample_weight=True, expected_fit_params=[\"a\"])\n    pc_clf = CalibratedClassifierCV(clf)\n\n    pc_clf.fit(X, y, sample_weight=sample_weight, **fit_params)\n", "def test_calibration_with_fit_params_non_sample_aligned(data):\n    \"\"\"Check that CalibratedClassifierCV does not enforce sample alignment\n    for fit parameters, even when using a custom splitter.\"\"\"\n\n    class TestClassifier(LogisticRegression):\n            assert fit_param is not None\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    cv = KFold(n_splits=2)\n    CalibratedClassifierCV(estimator=TestClassifier(), cv=cv).fit(\n        *data, fit_param=np.ones(len(data[1]) + 1)\n    )\n", "def test_calibration_with_sample_weight_and_fit_params(data):\n    \"\"\"Check that CalibratedClassifierCV handles both sample_weight and\n    fit_params correctly.\"\"\"\n\n    class TestClassifier(LogisticRegression):\n            assert sample_weight is not None\n            assert fit_param is not None\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    X, y = data\n    sample_weight = np.ones(len(y))\n    fit_param = np.ones(len(y))\n    calibrated_classifier = CalibratedClassifierCV(estimator=TestClassifier())\n    calibrated_classifier.fit(X, y, sample_weight=sample_weight, fit_param=fit_param)\n", "def test_calibration_predict_consistency(data):\n    # Test that the `predict` method returns consistent results\n    # with `predict_proba`\n    X, y = data\n    clf = LinearSVC(random_state=7)\n    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n    cal_clf.fit(X, y)\n\n    y_pred = cal_clf.predict(X)\n    y_proba = cal_clf.predict_proba(X)\n\n    # Check that the predicted class is the argmax of the predicted probabilities\n    assert np.array_equal(y_pred, np.argmax(y_proba, axis=1))\n", "def test_calibration_with_no_target_classes(data):\n    \"\"\"Check that CalibratedClassifierCV raises an error when the target has no classes.\"\"\"\n    X, y = data\n    y = np.ones_like(y)  # All samples have the same target\n    calibrated_classifier = CalibratedClassifierCV(base_estimator=LogisticRegression())\n    with pytest.raises(ValueError, match=\"y contains no classes.\"):\n        calibrated_classifier.fit(X, y)\n"], "sample_1175": ["def test_diffgeom_metric():\n    from sympy.diffgeom import MetricTensor\n    g = MetricTensor('g', rect)\n    assert pretty(g) == \"g\"\n", "def test_diffgeom_metrics():\n    from sympy.diffgeom import metric_to_Riemann_components\n    m, n, p = symbols('m n p', real=True)\n    g = Matrix([[m, n], [n, p]])\n    assert metric_to_Riemann_components(g) == ((m, n), (n, p))\n", "def test_issue_21830():\n    assert pretty(Piecewise((0, Eq(x, 0)), (1, True))) == \\\n    '/0, for x = 0\\n'\\\n    '\\1, otherwise'\n", "def test_issue_18399():\n    A = MatrixSymbol(\"A\", 2, 2)\n    expr = A ** 2\n    result = 'A*A'\n    assert pretty(expr) == result\n\n    expr = A * A ** 2\n    result = 'A*A*A'\n    assert pretty(expr) == result\n", "def test_integrals_issue_19018():\n    assert pretty(Integral(x*y, x, y)) == \\\n    ' /   / x\\\\ \\n'\\\n    '|     |   |\\n'\\\n    '|     |   | x*y dx\\n'\\\n    '|     |   |\\n'\\\n    '\\\\   dy /\\n'\n    assert upretty(Integral(x*y, x, y)) == \\\n    ' \u2320     \\n'\\\n    ' \u23ae     \\n'\\\n    ' \u23ae     \\n'\\\n    ' \u23ae x*y dx\\n'\\\n    ' \u23ae     \\n'\\\n    '\u2321 dy   '\n"], "sample_389": ["    def test_improperly_configured(self):\n        msg = \"The SECURE_PROXY_SSL_HEADER setting must be a tuple containing two values.\"\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            HttpRequest().scheme\n", "    def setUp(self):\n        self.request = HttpRequest()\n", "    def setUp(self):\n        self.request = HttpRequest()\n", "    def test_headers(self):\n        environ = {\"HTTP_USER_AGENT\": \"Test/1.0\", \"CONTENT_TYPE\": \"application/json\"}\n        request = HttpRequest()\n        request.META = environ\n        headers = request.headers\n        self.assertEqual(headers[\"User-Agent\"], \"Test/1.0\")\n        self.assertEqual(headers[\"Content-Type\"], \"application/json\")\n", "    def test_set_with_xheader_right_custom_value(self):\n        req = HttpRequest()\n        req.META[\"HTTP_X_FORWARDED_PROTO\"] = \"trustme\"\n        self.assertIs(req.is_secure(), True)\n"], "sample_336": ["    def test_include_urlconf_module(self):\n        urlconf_module = URLResolver(RegexPattern(r'^$'), 'urlpatterns_reverse.namespace_urls')\n        self.assertEqual(urlconf_module.urlconf_module.__name__, 'urlpatterns_reverse.namespace_urls')\n", "    def test_valid_path_converter(self):\n        test_urls = [\n            ('path-converter', {'path': 'example/path'}, '/path-converter/example/path/'),\n            ('path-converter-with-slug', {'slug': 'example-slug'}, '/path-converter-with-slug/example-slug/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n", "def test_invalid_regex_pattern_name(self):\n    # Regex pattern name contains an invalid character (refs #22014)\n    msg = \"URL route 'invalid/<name>' contains invalid characters.\"\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        URLPattern(r'invalid/<name>', views.empty_view)\n", "    def test_redirect_with_lang_code(self):\n        with self.settings(APPEND_SLASH=True, LANGUAGE_CODE='en'):\n            response = self.client.get('/places/1')\n            self.assertRedirects(response, \"/places/1/\", status_code=301)\n            response = self.client.get('/places/1/')\n            self.assertEqual(response.status_code, 200)\n\n        with self.settings(APPEND_SLASH=True, LANGUAGE_CODE='de'):\n            response = self.client.get('/places/1')\n            self.assertRedirects(response, \"/de/places/1/\", status_code=301)\n            response = self.client.get('/places/1/')\n            self.assertEqual(response.status_code, 200)\n", "    def test_special_characters_in_url(self):\n        test_urls = [\n            ('special', '/special_chars/~@+%5C$*%7C/', ['~@+\\\\$*|'], {}),\n            ('special', '/special_chars/some%20resource/', ['some resource'], {}),\n            ('special', '/special_chars/10%25%20complete/', ['10% complete'], {}),\n            ('special', '/special_chars/some%20resource/', [], {'chars': 'some resource'}),\n            ('special', '/special_chars/10%25%20complete/', [], {'chars': '10% complete'}),\n        ]\n        for name, url, args, kwargs in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs):\n                self.assertEqual(reverse(name, args=args, kwargs=kwargs), url)\n"], "sample_276": ["    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_status_count'), 'Integer')\n        self.assertEqual(get_return_data_type('get_groups_list'), 'List')\n        self.assertEqual(get_return_data_type('some_function'), '')\n", "def test_get_return_data_type(self):\n    \"\"\"\n    Test the get_return_data_type function.\n    \"\"\"\n    self.assertEqual(get_return_data_type('get_status_count'), 'Integer')\n    self.assertEqual(get_return_data_type('get_groups_list'), 'List')\n    self.assertEqual(get_return_data_type('get_full_name'), '')\n    self.assertEqual(get_return_data_type('add_image'), '')\n    self.assertEqual(get_return_data_type('delete_image'), '')\n", "def test_model_field_descriptions(self):\n    \"\"\"\n    The field descriptions should render correctly for each field type.\n    \"\"\"\n    response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n    self.assertContains(response, \"first name - The person's first name\")\n    self.assertContains(response, \"last name - The person's last name\")\n    self.assertContains(response, \"A custom field type\")\n    self.assertContains(response, \"Field of type: DescriptionLackingField\")\n", "    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_list'), 'List')\n        self.assertEqual(get_return_data_type('get_count'), 'Integer')\n        self.assertEqual(get_return_data_type('some_other_method'), '')\n", "def test_get_view_func_class_in_mod_func(self):\n    \"\"\"\n    Test _get_view_func() when class name is included in the mod_func.\n    \"\"\"\n    view = 'admin_docs.views.XViewClass.as_view'\n    view_func = views.ViewDetailView._get_view_func(view)\n    self.assertEqual(view_func, views.XViewClass.as_view)\n"], "sample_757": ["def test_one_hot_encoder_n_values_deprecation():\n    X = np.array([[1, 2, 1], [0, 1, 1]])\n    enc = OneHotEncoder(n_values=3)\n    with assert_warns(DeprecationWarning):\n        enc.fit_transform(X)\n    X_trans = enc.transform(X)\n    assert_equal(X_trans.shape, (2, 3 * 3))\n    assert_array_equal(enc.feature_indices_, [0, 3, 6, 9])\n", "def test_one_hot_encoder_n_values(X, n_values):\n    enc = OneHotEncoder(n_values=n_values)\n    with pytest.warns(DeprecationWarning, match='n_values'):\n        exp = np.array([[0., 1., 0.],\n                        [0., 0., 1.]])\n        assert_array_equal(enc.fit_transform(X).toarray(), exp)\n", "def test_one_hot_encoder_categories_none():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OneHotEncoder(categories=None)\n    exp = np.array([[1., 0., 1., 0., 1.], [0., 1., 0., 1., 1.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n", "def test_one_hot_encoder_handle_unknown_param(handle_unknown):\n    X = np.array([[1, 2], [3, 4]], dtype='int64')\n    X_unknown = np.array([[5, 6]], dtype='int64')\n    enc = OneHotEncoder(handle_unknown=handle_unknown)\n    enc.fit(X)\n    if handle_unknown == 'ignore':\n        enc.transform(X_unknown)\n    elif handle_unknown == 'error':\n        with pytest.raises(ValueError, match=\"Found unknown categories\"):\n            enc.transform(X_unknown)\n", "def test_one_hot_encoder_fit_transform_consistency():\n    X = np.array([['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]])\n    enc = OneHotEncoder()\n    enc.fit(X)\n    X_tr1 = enc.transform(X)\n    X_tr2 = enc.fit_transform(X)\n    assert_array_equal(X_tr1.toarray(), X_tr2.toarray())\n"], "sample_1166": ["def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n"], "sample_128": ["    def test_ops_class_covering_index(self):\n        index = Index(\n            name='test_ops_class_covering',\n            fields=['body'],\n            opclasses=['text_pattern_ops'],\n            include=['headline'],\n        )\n        with connection.schema_editor() as editor:\n            editor.add_index(IndexedArticle2, index)\n        with editor.connection.cursor() as cursor:\n            cursor.execute(self.get_opclass_query % 'test_ops_class_covering')\n            self.assertCountEqual(cursor.fetchall(), [('text_pattern_ops', 'test_ops_class_covering')])\n        with connection.cursor() as cursor:\n            constraints = connection.introspection.get_constraints(\n                cursor=cursor, table_name=IndexedArticle2._meta.db_table,\n            )\n            self.assertIn(index.name, constraints)\n            self.assertEqual(\n                constraints[index.name]['columns'],\n                ['body', 'headline'],\n            )\n", "    def test_include_with_partial_index(self):\n        index = Index(\n            name='covering_partial_headline_include_idx',\n            fields=['headline'],\n            include=['pub_date', 'published'],\n            condition=Q(published=True),\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s) INCLUDE (%s, %s) WHERE %s' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "    def test_no_condition(self):\n        with connection.schema_editor() as editor:\n            index = Index(\n                name='no_condition_idx',\n                fields=['headline'],\n            )\n            sql = str(index.create_sql(Article, schema_editor=editor))\n            where = sql.find('WHERE')\n            self.assertEqual(where, -1, \"Condition found in SQL when it shouldn't be\")\n            editor.add_index(index=index, model=Article)\n            with connection.cursor() as cursor:\n                self.assertIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n            editor.remove_index(index=index, model=Article)\n", "    def test_expression_index(self):\n        index = Index(\n            name='expression_idx',\n            fields=['-headline', 'body'],\n            condition=Q(body__icontains='Django'),\n            include=['pub_date'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s DESC)' % editor.quote_name('headline'),\n                str(index.create_sql(Article, editor)),\n            )\n            self.assertIn(\n                'WHERE UPPER(%s) LIKE %%s' % editor.quote_name('body'),\n                str(index.create_sql(Article, editor)),\n            )\n            self.assertIn(\n                'INCLUDE (%s)' % editor.quote_name('pub_date'),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'body', 'pub_date'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "    def test_descending_index_together(self):\n        \"\"\"Test descending index_together.\"\"\"\n        editor = connection.schema_editor()\n        index_sql = [str(statement) for statement in editor._model_indexes_sql(Article)]\n        self.assertIn('\"pub_date\" DESC', index_sql[0])\n"], "sample_803": ["def test_roc_auc_score_pos_label_errors():\n    # Raise an error when pos_label is not in binary y_true\n    y_true = np.array([0, 1])\n    y_pred = np.array([0, 1])\n    error_message = (\"pos_label=2 is invalid. Set it to a label in y_true.\")\n    assert_raise_message(ValueError, error_message, roc_auc_score,\n                         y_true, y_pred, pos_label=2)\n    # Raise an error for multilabel-indicator y_true with\n    # pos_label other than 1\n    y_true = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])\n    y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2], [0.2, 0.8]])\n    error_message = (\"Parameter pos_label is fixed to 1 for multilabel\"\n                     \"-indicator y_true. Do not set pos_label or set \"\n                     \"pos_label to 1.\")\n    assert_raise_message(ValueError, error_message, roc_auc_score,\n                         y_true, y_pred, pos_label=0)\n", "def test_roc_curve_sample_weight():\n    # Test Area under Receiver Operating Characteristic (ROC) curve with sample weight\n    y_true, _, probas_pred = make_prediction(binary=True)\n    sample_weight = np.ones_like(y_true)\n    sample_weight[::2] = 2  # double weight of every other sample\n    expected_auc = _auc(y_true, probas_pred, sample_weight=sample_weight)\n\n    fpr, tpr, thresholds = roc_curve(y_true, probas_pred, sample_weight=sample_weight)\n    roc_auc = auc(fpr, tpr)\n    assert_array_almost_equal(roc_auc, expected_auc, decimal=2)\n    assert_almost_equal(roc_auc, roc_auc_score(y_true, probas_pred, sample_weight=sample_weight))\n    assert_equal(fpr.shape, tpr.shape)\n    assert_equal(fpr.shape, thresholds.shape)\n", "def test_roc_auc_score_max_fpr():\n    # Test roc_auc_score with max_fpr\n    y_true = [0, 0, 1, 1]\n    y_scores = [0.1, 0.4, 0.35, 0.8]\n    roc_auc = roc_auc_score(y_true, y_scores, max_fpr=0.6)\n    assert_almost_equal(roc_auc, 0.5, decimal=2)\n", "def test_roc_auc_score_errors():\n    # Test errors for roc_auc_score\n    y_true = np.array([0, 0, 1, 1])\n    y_pred = np.array([0.1, 0.4, 0.35, 0.8])\n\n    # Invalid max_fpr value\n    with pytest.raises(ValueError, match=\"Expected max_frp in range ]0, 1], got: -0.1\"):\n        roc_auc_score(y_true, y_pred, max_fpr=-0.1)\n\n    with pytest.raises(ValueError, match=\"Expected max_frp in range ]0, 1], got: 1.1\"):\n        roc_auc_score(y_true, y_pred, max_fpr=1.1)\n\n    with pytest.raises(ValueError, match=\"Expected max_frp in range ]0, 1], got: 0\"):\n        roc_auc_score(y_true, y_pred, max_fpr=0)\n", "def test_roc_auc_score_sparse():\n    # Test `roc_auc_score` with sparse inputs\n    y_true = csr_matrix(np.array([[0, 1], [0, 1]]))\n    y_score = np.array([[0.1, 0.9], [0.2, 0.8]])\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='micro'), 1.0)\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='macro'), 1.0)\n"], "sample_28": ["def test_subclass_insert():\n    \"\"\"Check that subclasses don't get ignored on inserting.\"\"\"\n\n    class MyHeader(fits.Header):\n            if isinstance(card, tuple) and len(card) == 2:\n                # Just for our checks we add a comment if there is none.\n                card += (\"no comment\",)\n\n            return super().insert(index, card, *args, **kwargs)\n\n    my_header = MyHeader(\n        (\n            (\"a\", 1.0, \"first\"),\n            (\"b\", 2.0, \"second\"),\n            (\n                \"c\",\n                3.0,\n            ),\n        )\n    )\n\n    my_header.insert(1, (\"d\", 4.0))\n    assert my_header.comments[\"d\"] == \"no comment\"\n", "def test_subclass_slice_assignment(self):\n    \"\"\"Check that subclasses don't get ignored on slice assignment.\"\"\"\n\n    class MyHeader(fits.Header):\n            if isinstance(value, float):\n                value = int(value)\n            super().__setitem__(key, value)\n\n    my_header = MyHeader([(\"A\", 1.0), (\"B\", 2.0), (\"C\", 3.0)])\n    my_header[1:] = 4.0\n    assert my_header[1] == 4\n    assert my_header[2] == 4\n", "def test_header_insert_invalid_position(self):\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n\n    # Insert a new card at an invalid position\n    with pytest.raises(IndexError):\n        header.insert(10, (\"E\", \"F\"))\n\n    # Insert a new card at an invalid keyword\n    with pytest.raises(KeyError):\n        header.insert(\"E\", (\"F\", \"G\"))\n\n    # Insert a new card at an invalid tuple\n    with pytest.raises(KeyError):\n        header.insert((\"E\", 1), (\"F\", \"G\"))\n", "def test_insert_blank_card(self):\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\"), (\"E\", \"F\")])\n    header.insert(2, ())\n    assert len(header) == 4\n    assert header[2] == \"\"\n    assert header[\"\"] == \"\"\n    assert header.comments[\"\"] == \"\"\n\n    header.insert(2, (\"G\", \"H\"))\n    assert len(header) == 4\n    assert header[2] == \"H\"\n    assert header[\"G\"] == \"H\"\n    assert header.comments[\"G\"] == \"\"\n", "def test_verify_no_ignore_blanks_false(self):\n    \"\"\"Test verify with ignore_blanks=False.\"\"\"\n    header = fits.Header([(\"A\", \"B\"), (\"C\", \"D\")])\n    header.verify(\"fix\", ignore_blanks=False)\n    assert header[\"A\"] == \"B\"\n    assert header[\"C\"] == \"D\"\n"], "sample_883": ["def test_bayesian_ridge_max_iter():\n    # Test BayesianRidge max_iter parameter\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(max_iter=10)\n    clf.fit(X, Y)\n\n    # Check that the model could approximately learn the identity function\n    test = [[1], [3], [4]]\n    assert_array_almost_equal(clf.predict(test), [1, 3, 4], 2)\n\n    # Check that the max_iter parameter is respected\n    assert clf.n_iter_ == 10\n", "def test_bayesian_ridge_max_iter_default():\n    \"\"\"Check default value of max_iter when not explicitly set.\"\"\"\n    X, y = diabetes.data, diabetes.target\n    model = BayesianRidge()\n    model.fit(X, y)\n\n    # Check if max_iter is set to 300 if not explicitly provided\n    assert model.max_iter == 300\n", "def test_bayesian_ridge_max_iter_default():\n    \"\"\"Check the default value of `max_iter`.\"\"\"\n    X, y = diabetes.data, diabetes.target\n    model = BayesianRidge()\n\n    assert model.max_iter is None\n    model.fit(X, y)\n    assert model.max_iter == 300\n", "def test_bayesian_ridge_fit_intercept_false():\n    # Test BayesianRidge when fit_intercept is set to False\n    X = np.array([[1], [2], [3]])\n    Y = np.array([2, 4, 6])\n    clf = BayesianRidge(fit_intercept=False)\n    clf.fit(X, Y)\n\n    # Check that the intercept is set to 0 and the model learned the right weights\n    assert clf.intercept_ == 0\n    assert_array_almost_equal(clf.coef_, [2], 2)\n", "def test_bayesian_ridge_scores_increase():\n    \"\"\"Check that the scores attribute increases during training\"\"\"\n    X, y = diabetes.data, diabetes.target\n\n    clf = BayesianRidge(compute_score=True, max_iter=10)\n    clf.fit(X, y)\n\n    assert np.all(np.diff(clf.scores_) >= 0)\n"], "sample_511": ["def test_pyplot_title():\n    plt.figure()\n    plt.title(\"Test Title\")\n    assert plt.gca().get_title() == \"Test Title\"\n", "def test_tight_layout():\n    fig, axs = plt.subplots(2, 2)\n    fig.tight_layout()\n    assert fig.get_tightbbox() is not None\n", "def test_subplot_mosaic_empty_sentinel():\n    fig, ax_dict = plt.subplot_mosaic('A\\nB', empty_sentinel=' ')\n    assert 'A' in ax_dict\n    assert 'B' in ax_dict\n    assert ' ' not in ax_dict\n", "def test_polar_twinx():\n    # the first call creates the axes with polar projection\n    ax1 = plt.subplot(111, projection='polar')\n    # the second call should create a new axes with polar projection and twinx\n    ax2 = plt.twinx(ax1)\n    assert isinstance(ax2, mpl.projections.polar.PolarAxes)\n    assert ax1 is not ax2\n    assert ax1.get_shared_y_axes()[0] is ax2\n", "def test_figure_management():\n    # Test creating a figure\n    fig1 = plt.figure()\n    assert plt.gcf() == fig1\n\n    # Test creating another figure\n    fig2 = plt.figure()\n    assert plt.gcf() == fig2\n\n    # Test switching back to fig1\n    plt.figure(fig1.number)\n    assert plt.gcf() == fig1\n\n    # Test creating a new figure with no arguments\n    plt.figure()\n    fig3 = plt.gcf()\n    assert fig3 != fig1\n    assert fig3 != fig2\n\n    # Test closing fig1\n    plt.close(fig1.number)\n    with pytest.raises(Exception):\n        plt.figure(fig1.number)\n\n    # Test closing all figures\n    plt.close('all')\n    with pytest.raises(Exception):\n        plt.gcf()\n"], "sample_676": ["def test_skip_reasons_folding_with_global_pytestmark():\n    path = \"xyz\"\n    lineno = 3\n    message = \"justso\"\n    longrepr = (path, lineno, message)\n\n    class X(object):\n        pass\n\n    ev1 = X()\n    ev1.when = \"setup\"\n    ev1.skipped = True\n    ev1.longrepr = longrepr\n    ev1.keywords = {\"skip\": \"\", \"pytestmark\": \"some mark\"}\n\n    values = _folded_skips([ev1])\n    assert len(values) == 1\n    num, fspath, lineno, reason = values[0]\n    assert num == 1\n    assert fspath == path\n    assert lineno is None\n    assert reason == message\n", "def test_line_with_reprcrash_unicode_safety():\n    class config(object):\n        pass\n\n    class rep(object):\n            return \"FAILED\"\n\n        class longrepr:\n            class reprcrash:\n                message = \"Unicode: \u4f60\u597d, \u4e16\u754c\"\n\n    width = 80\n    expected = \"FAILED some::nodeid - Unicode: \u4f60\u597d, ...\"\n\n    actual = _get_line_with_reprcrash_message(config, rep(), width)\n    assert actual == expected\n", "def test_line_with_reprcrash_multibyte_characters(monkeypatch):\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n\n    mocked_pos = \"some::nodeid::with::emoji\"\n\n        return mocked_pos\n\n    monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n    class config(object):\n        pass\n\n    class rep(object):\n            return mocked_verbose_word\n\n        class longrepr:\n            class reprcrash:\n                pass\n\n    msg = \"ILED \u03b2\u009c", "def test_line_with_reprcrash_unicode_surrogate(monkeypatch):\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n\n    mocked_pos = \"some::nodeid\"\n\n        return mocked_pos\n\n    monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n    class config(object):\n        pass\n\n    class rep(object):\n            return mocked_verbose_word\n\n        class longrepr:\n            class reprcrash:\n                pass\n\n    # Test unicode surrogate safety.\n    rep.longrepr.reprcrash.message = u\"\ud83d\udc36\ud83d\udc36\ud83d\udc36\ud83d\udc36\ud83d\udc36\\n2nd line\"\n    expected = u\"FAILED some::nodeid - \ud83d\udc36\ud83d\udc36\ud83d\udc36\ud83d\udc36...\"\n    actual = _get_line_with_reprcrash_message(config, rep(), 29)\n    assert actual == expected\n    assert wcswidth(actual) == len(expected)\n", "def test_long_msg_truncation():\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n    mocked_pos = \"some::nodeid\"\n\n    class config(object):\n        pass\n\n    class rep(object):\n            return mocked_verbose_word\n\n        class longrepr:\n            class reprcrash:\n                pass\n\n    rep.longrepr.reprcrash.message = \"a\" * 100\n\n    actual = _get_line_with_reprcrash_message(config, rep(), 80)\n\n    assert actual == \"FAILED some::nodeid - \" + \"a\" * 75 + \"...\"\n    assert wcswidth(actual) <= 80\n\n    rep.longrepr.reprcrash.message = \"b\" * 100\n\n    actual = _get_line_with_reprcrash_message(config, rep(), 100)\n\n    assert actual == \"FAILED some::nodeid - \" + \"b\" * 95 + \"...\"\n    assert wcswidth(actual) <= 100\n"], "sample_152": ["def test_fast_delete_multiple_objects(self):\n    users = [User.objects.create() for _ in range(5)]\n    self.assertNumQueries(1, User.objects.filter(pk__in=[u.pk for u in users]).delete)\n    self.assertEqual(User.objects.count(), 0)\n", "def test_fast_delete_with_set_null(self):\n    avatar = Avatar.objects.create()\n    user = User.objects.create(avatar=avatar)\n    avatar.delete()\n    user.refresh_from_db()\n    self.assertIsNone(user.avatar)\n", "def test_fast_delete_signal_disables_fast_delete(self):\n    \"\"\"\n    Fast deletion of a model with a pre_delete signal should not be used.\n    \"\"\"\n    u = User.objects.create()\n    a = Avatar.objects.get(pk=u.avatar_id)\n\n        pass\n\n    models.signals.pre_delete.connect(receiver, sender=User)\n\n    # 1 query to select users for the avatar\n    # 1 query to delete the user\n    # 1 query to delete the avatar\n    self.assertNumQueries(3, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n\n    models.signals.pre_delete.disconnect(receiver, sender=User)\n", "def test_fast_delete_related_with_signals(self):\n    # Test fast delete of related objects when deletion signals are connected\n    # for the related model.\n    a = Avatar.objects.create()\n    User.objects.create(avatar=a)\n\n        pass\n\n    models.signals.pre_delete.connect(receiver, sender=User)\n\n    # With a signal, User can't be fast-deleted.\n    collector = Collector(using='default')\n    self.assertFalse(collector.can_fast_delete(User.objects.get(pk=a.user.pk)))\n\n    with self.assertNumQueries(2):\n        a.delete()\n\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n\n    models.signals.pre_delete.disconnect(receiver, sender=User)\n", "def test_fast_delete_signals(self):\n    # If signals are connected, objects cannot be fast-deleted.\n        pass\n\n    models.signals.pre_delete.connect(receiver, sender=User)\n    try:\n        u = User.objects.create(avatar=Avatar.objects.create())\n        a = Avatar.objects.get(pk=u.avatar_id)\n        self.assertNumQueries(3, a.delete)\n    finally:\n        models.signals.pre_delete.disconnect(receiver, sender=User)\n"], "sample_247": ["def test_alias_filter_with_double_f(self):\n    qs = Book.objects.alias(\n        other_rating=F('rating') + 1,\n    ).filter(rating=F('other_rating') - 1)\n    self.assertIs(hasattr(qs.first(), 'other_rating'), False)\n    self.assertEqual(qs.count(), Book.objects.count())\n", "def test_alias_exists_aggregate_values_chaining(self):\n    qs = Book.objects.alias(\n        has_authors=Exists(Book.authors.through.objects.filter(book=OuterRef('pk'))),\n        max_pubdate=Max('pubdate'),\n    ).values('max_pubdate', 'has_authors').order_by('max_pubdate')\n    self.assertIs(hasattr(qs.first(), 'max_pubdate'), False)\n    self.assertIs(hasattr(qs.first(), 'has_authors'), False)\n    self.assertCountEqual(qs, [\n        {'has_authors': True, 'max_pubdate': datetime.date(1991, 10, 15)},\n        {'has_authors': True, 'max_pubdate': datetime.date(2007, 12, 6)},\n        {'has_authors': True, 'max_pubdate': datetime.date(2008, 6, 23)},\n        {'has_authors': True, 'max_pubdate': datetime.date(2008, 11, 3)},\n    ])\n", "def test_aggregate_over_alias(self):\n    qs = Author.objects.alias(other_age=F('age')).aggregate(otherage_sum=Sum('age'))\n    other_agg = Author.objects.aggregate(age_sum=Sum('age'))\n    self.assertEqual(qs['otherage_sum'], other_agg['age_sum'])\n", "def test_alias_in_subquery(self):\n    qs = Book.objects.alias(\n        pub_year=ExtractYear('pubdate'),\n    ).filter(\n        pub_year__in=Subquery(\n            Book.objects.filter(\n                pubdate__year=OuterRef('pub_year'),\n            ).order_by().values('pubdate__year')\n        ),\n    ).values('pub_year')\n    self.assertCountEqual(qs, [\n        {'pub_year': 1991},\n        {'pub_year': 1995},\n        {'pub_year': 2007},\n        {'pub_year': 2008},\n    ])\n", "def test_alias_in_f_grouped_by_alias(self):\n    qs = Publisher.objects.alias(multiplier=Value(3)).values('name').annotate(\n        multiplied_value_sum=Sum(F('multiplier') * F('num_awards'))\n    ).order_by()\n    self.assertCountEqual(\n        qs, [\n            {'multiplied_value_sum': 9, 'name': 'Apress'},\n            {'multiplied_value_sum': 0, 'name': \"Jonno's House of Books\"},\n            {'multiplied_value_sum': 27, 'name': 'Morgan Kaufmann'},\n            {'multiplied_value_sum': 21, 'name': 'Prentice Hall'},\n            {'multiplied_value_sum': 3, 'name': 'Sams'},\n        ]\n    )\n"], "sample_715": ["def test_permutation_test_score_allow_nans_with_groups():\n    # Check that permutation_test_score allows input data with NaNs and groups\n    X = np.arange(200, dtype=np.float64).reshape(10, -1)\n    X[2, :] = np.nan\n    y = np.repeat([0, 1], X.shape[0] / 2)\n    groups = np.repeat([0, 1], X.shape[0] / 2)\n    p = Pipeline([\n        ('imputer', Imputer(strategy='mean', missing_values='NaN')),\n        ('classifier', MockClassifier()),\n    ])\n    permutation_test_score(p, X, y, groups=groups, cv=5)\n", "def test_cross_val_predict_method_custom_scoring():\n    # Test that cross_val_predict works with custom scoring functions\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    est = LogisticRegression()\n    methods = ['decision_function', 'predict_proba', 'predict_log_proba']\n    for method in methods:\n        predictions = cross_val_predict(est, X, y, method=method,\n                                        scoring=make_scorer(accuracy_score))\n        assert_equal(len(predictions), len(y))\n", "def test_permutation_test_score_multilabel():\n    X = np.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1],\n                  [-2, 1], [0, 0], [-2, -1], [-1, -2], [1, -2]])\n    y = np.array([[1, 1], [0, 1], [0, 1], [0, 1], [1, 1],\n                  [0, 1], [1, 0], [1, 1], [1, 0], [0, 0]])\n    clf = KNeighborsClassifier(n_neighbors=1)\n    score, _, pvalue = permutation_test_score(\n        clf, X, y, n_permutations=100, scoring='accuracy', random_state=0)\n    assert_almost_equal(score, 0.95, 2)\n    assert_almost_equal(pvalue, 0.0, 1)\n", "def test_fit_and_score_scorer_callable():\n    # Test case for scorer as callable\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=2, random_state=42)\n    clf = SVC()\n    scorer = make_scorer(accuracy_score)\n    score, fit_time, score_time = _fit_and_score(clf, X, y, scorer, False, None, 0, None, None)\n    assert isinstance(score, float)\n    assert isinstance(fit_time, float)\n    assert isinstance(score_time, float)\n", "def test_cross_val_score_sparse_X():\n    # Test cross_val_score with sparse input\n    X = csr_matrix([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    clf = Ridge()\n    scores = cross_val_score(clf, X, y, cv=2)\n    expected_scores = np.array([0.94184492, 0.97457627])\n    assert_array_almost_equal(scores, expected_scores)\n"], "sample_544": ["def test_figureimage_get_shape():\n    fig = plt.gcf()\n    im = FigureImage(fig)\n    with pytest.raises(RuntimeError, match=\"You must first set the image array\"):\n        im.get_shape()\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im.set_data(z)\n    assert im.get_shape() == (4, 3)\n    assert im.get_size() == im.get_shape()\n", "def test_figureimage_get_shape():\n    fig = plt.gcf()\n    im = FigureImage(fig)\n    with pytest.raises(RuntimeError, match=\"You must first set the image array\"):\n        im.get_shape()\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im.set_data(z)\n    assert im.get_shape() == (4, 3)\n    assert im.get_size() == im.get_shape()\n", "def test_figureimage_get_shape():\n    fig = plt.gcf()\n    im = FigureImage(fig)\n    with pytest.raises(RuntimeError, match=\"You must first set the image array\"):\n        im.get_shape()\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im.set_data(z)\n    assert im.get_shape() == (4, 3)\n    assert im.get_size() == im.get_shape()\n", "def test_figureimage_get_shape():\n    # generate dummy image to test get_shape method\n    fig = plt.gcf()\n    im = FigureImage(fig)\n    with pytest.raises(RuntimeError, match=\"You must first set the image array\"):\n        im.get_shape()\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im.set_data(z)\n    assert im.get_shape() == (4, 3)\n    assert im.get_size() == im.get_shape()\n", "def test_large_image_clip(dim, size, origin):\n    # Check that Matplotlib clips images that are too big for AGG\n    fig, ax = plt.subplots()\n\n    array = np.zeros((1, size + 2))\n    array[:, array.size // 2:] = 1\n    if dim == 'col':\n        array = array.T\n    im = ax.imshow(array, vmin=0, vmax=1,\n                   aspect='auto', extent=(0, 1, 0, 1),\n                   interpolation='none',\n                   origin=origin)\n\n    fig.canvas.draw()\n    # Check that the image is clipped\n    assert im.get_array().shape == (1, 2**20)\n"], "sample_545": ["def test_set_constrained_layout_pads():\n    fig = plt.figure(constrained_layout=True)\n    fig.set_constrained_layout_pads(w_pad=10, h_pad=20, wspace=0.1, hspace=0.2)\n    info = fig.get_constrained_layout_pads()\n    assert info['w_pad'] == 10\n    assert info['h_pad'] == 20\n    assert info['wspace'] == 0.1\n    assert info['hspace'] == 0.2\n", "def test_change_layout_engine():\n    fig = plt.figure()\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    fig.set_layout_engine(\"constrained\")\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig.set_layout_engine(\"none\")\n    assert fig.get_layout_engine() is None\n", "def test_subplots_adjust(fig_test, fig_ref):\n    fig_test.subplots(2, 2)\n    fig_test.subplots_adjust(wspace=0.5, hspace=0.5)\n\n    fig_ref.subplots(2, 2)\n    gs = fig_ref.get_constrained_layout_pads()\n    fig_ref.subplots_adjust(wspace=gs[0]*2, hspace=gs[1]*2)\n", "def test_pickle():\n    fig = Figure()\n    fig.add_subplot()\n    fig.suptitle(\"Title\")\n    fig.supxlabel(\"xlabel\")\n    fig.supylabel(\"ylabel\")\n\n    fig2 = pickle.loads(pickle.dumps(fig))\n\n    assert fig2.axes[0] is not fig.axes[0]\n    assert fig2.get_label() == fig.get_label()\n    assert fig2.get_suptitle() == fig.get_suptitle()\n    assert fig2.supxlabel.get_text() == fig.supxlabel.get_text()\n    assert fig2.supylabel.get_text() == fig.supylabel.get_text()\n", "def test_draw_without_rendering():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3], [4, 5, 6])\n    fig.draw_without_rendering()\n    # Add some assertions to check that the drawing process completed successfully\n    assert fig.stale == False\n"], "sample_640": ["def test_is_node_in_typing_guarded_import_block() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import TYPE_CHECKING\n    if TYPE_CHECKING:\n        import os  #@\n    else:\n        import sys  #@\n    \"\"\"\n    )\n    assert isinstance(code, list) and len(code) == 2\n\n    assert isinstance(code[0], nodes.Import)\n    assert utils.is_node_in_typing_guarded_import_block(code[0]) is True\n    assert isinstance(code[1], nodes.Import)\n    assert utils.is_node_in_typing_guarded_import_block(code[1]) is False\n", "def test_parse_format_string() -> None:\n    fmt = \"{:5}{}{{}}{}\"\n    keys, num_args, key_types, pos_types = utils.parse_format_string(fmt)\n    assert not keys\n    assert num_args == 3\n    assert key_types == {}\n    assert pos_types == [\"\", \"\", \"\"]\n\n    fmt = \"{field:10}\"\n    keys, num_args, key_types, pos_types = utils.parse_format_string(fmt)\n    assert \"field\" in keys\n    assert num_args == 0\n    assert key_types == {\"field\": \"d\"}\n    assert pos_types == []\n\n    fmt = \"{:5}{!r:10}\"\n    keys, num_args, key_types, pos_types = utils.parse_format_string(fmt)\n    assert not keys\n    assert num_args == 2\n    assert key_types == {}\n    assert pos_types == [\"d\", \"r\"]\n", "def test_get_node_first_ancestor_of_type() -> None:\n    node = astroid.extract_node(\n        \"\"\"\n        class OuterClass:\n                    pass  #@\n        \"\"\"\n    )\n    assert isinstance(node, nodes.FunctionDef)\n    assert utils.get_node_first_ancestor_of_type(node, nodes.ClassDef) == node.parent.parent\n    assert utils.get_node_first_ancestor_of_type(node, nodes.Module) is None\n", "def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import overload\n\n    @overload\n        pass\n\n        return a\n\n        return a\n    \"\"\"\n    )\n    assert isinstance(code, list) and len(code) == 3\n\n    assert isinstance(code[0], nodes.FunctionDef)\n    assert utils.is_overload_stub(code[0]) is True\n    assert isinstance(code[1], nodes.FunctionDef)\n    assert utils.is_overload_stub(code[1]) is False\n    assert isinstance(code[2], nodes.FunctionDef)\n    assert utils.is_overload_stub(code[2]) is False\n", "def test_is_inside_abstract_class() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from abc import ABC, abstractmethod\n\n    class AbstractClass(ABC):\n        @abstractmethod\n            pass\n\n    class SubClass(AbstractClass):\n        pass\n\n    class NotAbstractClass:\n        pass\n\n        class LocalClass(AbstractClass):\n            pass\n\n        assert utils.is_inside_abstract_class(SubClass())\n        assert utils.is_inside_abstract_class(LocalClass())\n        assert not utils.is_inside_abstract_class(NotAbstractClass())\n\n    function()\n    \"\"\"\n    )\n\n    assert isinstance(code, list)\n    assert len(code) == 1\n\n    assert isinstance(code[0], nodes.FunctionDef)\n    assert code[0].name == \"function\"\n"], "sample_698": ["def test_coloredlogformatter_with_width_precision_and_short_level() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-1.1s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[32mI\\x1b[0m Test Message\"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 I Test Message\")\n", "def test_get_option_ini():\n    class MockConfig:\n            if name == \"log_level\":\n                return \"DEBUG\"\n            return None\n\n            if name == \"log_level\":\n                return \"INFO\"\n            return None\n\n    config = MockConfig()\n    result = get_option_ini(config, \"log_level\")\n    assert result == \"DEBUG\"\n\n    config = MockConfig()\n    result = get_option_ini(config, \"nonexistent_option\")\n    assert result == \"INFO\"\n", "def test_log_capture_fixture():\n    class MockItem:\n            self._store = {}\n\n    class MockHandler(logging.Handler):\n            super().__init__()\n            self.records = []\n            self.stream = StringIO()\n\n            self.records.append(record)\n            super().emit(record)\n\n    item = MockItem()\n    item._store[caplog_handler_key] = MockHandler()\n    fixture = LogCaptureFixture(item, _ispytest=True)\n\n    # Test set_level method\n    fixture.set_level(logging.DEBUG, \"test_logger\")\n    assert logging.getLogger(\"test_logger\").level == logging.DEBUG\n\n    # Test _finalize method\n    fixture._finalize()\n    assert logging.getLogger(\"test_logger\").level != logging.DEBUG\n\n    # Test at_level context manager\n    with fixture.at_level(logging.ERROR, \"test_logger\"):\n        assert logging.getLogger(\"test_logger\").level == logging.ERROR\n    assert logging.getLogger(\"test_logger\").level != logging.ERROR\n", "def test_get_auto_indent() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    # Test with None\n    assert PercentStyleMultiline._get_auto_indent(None) == 0\n\n    # Test with boolean values\n    assert PercentStyleMultiline._get_auto_indent(True) == -1\n    assert PercentStyleMultiline._get_auto_indent(False) == 0\n\n    # Test with integer values\n    assert PercentStyleMultiline._get_auto_indent(5) == 5\n\n    # Test with string values\n    assert PercentStyleMultiline._get_auto_indent(\"True\") == -1\n    assert PercentStyleMultiline._get_auto_indent(\"False\") == 0\n    assert PercentStyleMultiline._get_auto_indent(\"5\") == 5\n    assert PercentStyleMultiline._get_auto_indent(\"junk\") == 0\n\n    # Test with non-string, non-integer, non-boolean values\n    assert PercentStyleMultiline._get_auto_indent(dict()) == 0\n", "def test_coloredlogformatter_with_levelname_precision() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname).5s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[32mINFO  \\x1b[0m Test Message\"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 INFO   Test Message\")\n"], "sample_423": ["def test_create_model_with_index_together(self):\n    author = ModelState(\n        \"otherapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"name\",)},\n        },\n    )\n    changes = self.get_changes([], [author])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"Author\",\n        options={\n            \"index_together\": {(\"name\",)},\n        },\n    )\n", "def test_add_index_together_with_db_tablespace(self):\n    book_index_together_db_tablespace = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"author\", \"title\")},\n            \"db_tablespace\": \"test_tablespace\",\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, AutodetectorTests.book],\n        [AutodetectorTests.author_empty, book_index_together_db_tablespace],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        index_together={(\"author\", \"title\")},\n        db_tablespace=\"test_tablespace\",\n    )\n", "def test_rename_index_together_to_index_db_tablespace(self):\n    # Indexes with a different db_tablespace don't match indexes in index_together.\n    book_db_tablespace = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(\n                    fields=[\"author\", \"title\"],\n                    db_tablespace=\"indexes\",\n                    name=\"book_title_author_idx\",\n                )\n            ],\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, self.book_index_together],\n        [AutodetectorTests.author_empty, book_db_tablespace],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"AlterIndexTogether\", \"AddIndex\"],\n    )\n", "def test_set_alter_order_with_respect_to_unique_together(self):\n    after = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ],\n        options={\n            \"order_with_respect_to\": \"book\",\n            \"unique_together\": {(\"id\", \"_order\")},\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.book, AutodetectorTests.author_with_book],\n        [AutodetectorTests.book, after],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterOrderWithRespectTo\", \"AlterUniqueTogether\"],\n    )\n", "def test_unique_together_remove_field_order(self):\n    \"\"\"\n    unique_together field removal and unique_together removal\n    are ordered correctly.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.book_unique_together],\n        [self.author_empty],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"otherapp\",\n        0,\n        [\"AlterUniqueTogether\", \"RemoveField\", \"RemoveField\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        unique_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        model_name=\"book\",\n        name=\"author\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        2,\n        model_name=\"book\",\n        name=\"title\",\n    )\n"], "sample_911": ["def test_nested_templates():\n    check('class', 'template<template<typename...> typename T> A', {2: 'II0EDpE0E1A'})\n    check('class', 'template<template<int...> typename T> A', {2: 'II_DpiE0E1A'})\n    check('class', 'template<template<typename...> typename T = std::tuple> A', {2: 'II0EDpE0E1A'})\n    check('class', 'template<template<int...> typename T = std::integer_sequence> A', {2: 'II_DpiE0E1A'})\n", "def test_enumerators_in_namespace():\n    check('enumerator', 'enum_member', {1: 'enum_member', 2: '11enum_member'})\n    check('enumerator', 'namespace::enum_member', {1: 'namespace::enum_member', 2: 'N9namespace11enum_memberE'})\n", "def test_xref_explicit_title():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n        assert str(ast) == title\n    check('f', 'f')\n    check('f()', 'f')\n    check('void f()', 'f')\n    check('T f()', 'f')\n    check('T f()', 'f')\n    check('void f(int)', 'f')\n    check('void f(int) const', 'f')\n    check('void f(int) const volatile', 'f')\n    check('void f(int) const volatile &', 'f')\n    check('void f(int) const volatile &&', 'f')\n    check('void f(int) const volatile noexcept', 'f')\n    check('void f(int) const volatile noexcept = delete', 'f')\n    check('void f(int) &&', 'f')\n    check('void f(int) &', 'f')\n    check('void f(int) const &', 'f')\n    check('void f(int) const', 'f')\n    check('void f(int) volatile', 'f')\n    check('void f(int) const volatile', 'f')\n    check('void f(int) noexcept', 'f')\n    check('void f(int) const noexcept', 'f')\n    check('void f(int) noexcept = delete', 'f')\n    check('void f(int) const noexcept = delete', 'f')\n    check('void f(int) noexcept(true)', 'f')\n    check('void f(int) const noexcept(true)', 'f')\n    check('void f(int) noexcept(false)', 'f')\n    check('void f(int) const noexcept(false)', 'f')\n    check('void f(int) noexcept(noexcept(true", "def test_template_args_with_brackets():\n    # from #3833\n    check('type', \"template<typename T, typename U = std::vector<T>> A\",\n          {2: \"I00E1AI1TNSt6vectorITEE\"})\n    check('type', \"template<typename T, typename U = std::vector<T> > A\",\n          {2: \"I00E1AI1TNSt6vectorITEE\"})\n    check('type', \"template<typename T, typename U = std::vector<T > > A\",\n          {2: \"I00E1AI1TNSt6vectorITEE\"})\n    check('type', \"template<typename T, typename U = std::vector< T > > A\",\n          {2: \"I00E1AI1TNSt6vectorITEE\"})\n    check('type', \"template<typename T, typename U = std::vector< T >> A\",\n          {2: \"I00E1AI1TNSt6vectorITEE\"})\n    check('type', \"template<typename T, typename U = std::vector< T > > A\",\n          {2: \"I00E1AI1TNSt6vectorITEE\"})\n    check('type', \"template<typename T, typename U = std::vector< T >> A\",\n          {2: \"I00E1AI1TNSt6vectorITEE\"})\n", "def test_explicit_instantiation():\n    check('member', 'explicit instantiate int foo<int>(int);', {1: 'foo__iC', 2: '13fooIiEEi'},\n          output='explicit instantiate int foo<int>(int);')\n    check('member', 'explicit instantiate int foo<int>;', {1: 'foo__i', 2: '13fooIiEE'},\n          output='explicit instantiate int foo<int>;')\n    check('member', 'template<typename T> explicit instantiate T foo<T>;', {2: 'I0E13fooI1TEE'},\n          output='template<> explicit instantiate T foo<T>;')\n"], "sample_1169": ["def test_canonical_ordering_AntiSymmetricTensor_above_below():\n    v = symbols(\"v\")\n\n    c, d = symbols(('c','d'), above_fermi=True, cls=Dummy)\n    k, l = symbols(('k','l'), below_fermi=True, cls=Dummy)\n\n    # the above indices should be ordered before the below indices\n    assert AntiSymmetricTensor(v, (k, c), (d, l)\n        ) == AntiSymmetricTensor(v, (c, k), (d, l))\n", "def test_canonical_ordering_CreateFermion():\n    i, j = symbols(('i','j'), below_fermi=True, cls=Dummy)\n\n    # formerly, the left was always considered greater\n    assert CreateFermion(i) > CreateFermion(j)\n", "def test_canonical_ordering_AntiSymmetricTensor_indices_ordering():\n    v = symbols(\"v\")\n\n    c, d = symbols(('c','d'), above_fermi=True, cls=Dummy)\n    k, l = symbols(('k','l'), below_fermi=True, cls=Dummy)\n\n    # Index ordering should be canonicalized\n    assert AntiSymmetricTensor(v, (c, d), (k, l)\n        ) == AntiSymmetricTensor(v, (d, c), (l, k))\n", "def test_canonical_ordering_AntiSymmetricTensor_with_indices():\n    v = symbols(\"v\")\n\n    c, d = symbols(('c','d'), above_fermi=True)\n    k, l = symbols(('k','l'), below_fermi=True)\n\n    # test with explicit indices\n    assert AntiSymmetricTensor(v, (k, l), (d, c)\n        ) == -AntiSymmetricTensor(v, (l, k), (d, c))\n\n    # test with explicit indices and dummy variables\n    c_dummy, d_dummy = symbols(('c','d'), above_fermi=True, cls=Dummy)\n    k_dummy, l_dummy = symbols(('k','l'), below_fermi=True, cls=Dummy)\n\n    assert AntiSymmetricTensor(v, (k_dummy, l_dummy), (d_dummy, c_dummy)\n        ) == -AntiSymmetricTensor(v, (l_dummy, k_dummy), (d_dummy, c_dummy))\n", "def test_issue_24709():\n    i, j, k, l = symbols('i j k l', below_fermi=True)\n    a, b, c, d = symbols('a b c d', above_fermi=True)\n    p, q, r, s = symbols('p q r s', cls=Dummy)\n\n    assert evaluate_deltas(contraction(Fd(p), F(q))) == KroneckerDelta(p, q)\n    assert evaluate_deltas(contraction(Fd(i), F(j))) == KroneckerDelta(i, j)\n"], "sample_660": ["def test_logging_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    assert len(node.find_by_tag(\"system-err\")) == 1\n    assert len(node.find_by_tag(\"system-out\")) == 1\n", "def test_logging_passing_tests_disabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=False\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n            assert True\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    assert len(node.find_by_tag(\"system-err\")) == 1\n    assert len(node.find_by_tag(\"system-out\")) == 1\n", "def test_logging_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    assert len(node.find_by_tag(\"system-err\")) == 1\n    assert len(node.find_by_tag(\"system-out\")) == 1\n    systemout = node.find_first_by_tag(\"system-out\")\n    assert \"This is stdout\" in systemout.text\n    assert \"hello\" in systemout.text\n    systemerr = node.find_first_by_tag(\"system-err\")\n    assert \"This is stderr\" in systemerr.text\n    assert \"hello\" not in systemerr.text\n", "def test_record_xml_attribute_xunit2(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_family = xunit2\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            record_xml_attribute(\"bar\", 1)\n            record_xml_attribute(\"foo\", \"<1\");\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rw\")\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    assert \"foo\" not in tnode.attributes\n    assert \"bar\" not in tnode.attributes\n", "def test_record_testsuite_property_type_checking_name_parameter(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(1, \"value\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\"*TypeError: name parameter needs to be a string, but int given\"]\n    )\n"], "sample_798": ["def test_ridge_solver_auto():\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Test auto solver\n    ridge_auto = Ridge(alpha=alpha, solver='auto')\n    ridge_auto.fit(X, y)\n\n    # Test cholesky solver for comparison\n    ridge_cholesky = Ridge(alpha=alpha, solver='cholesky')\n    ridge_cholesky.fit(X, y)\n\n    # Check that auto and cholesky solvers give the same result\n    assert_array_almost_equal(ridge_auto.coef_, ridge_cholesky.coef_)\n", "def test_ridge_solver_sag_saga(solver):\n    # Test Ridge regression with sag and saga solvers\n    n_samples, n_features = 100, 50\n    X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=42)\n    ridge = Ridge(alpha=1.0, solver=solver, random_state=42)\n    ridge.fit(X, y)\n    y_pred = ridge.predict(X)\n    assert_greater(ridge.score(X, y), 0.9)\n", "def test_ridge_regression_alpha_input():\n    rng = np.random.RandomState(42)\n    X = rng.randn(100, 3)\n    true_coefs = [1, 2, 0.1]\n    y = np.dot(X, true_coefs)\n    true_intercept = 10000.\n    y += true_intercept\n\n    alpha_array = np.array([0.1, 0.2, 0.3])\n    alpha_list = [0.1, 0.2, 0.3]\n    alpha_scalar = 0.1\n\n    coef_array = ridge_regression(X, y, alpha=alpha_array, return_intercept=True)[0]\n    coef_list = ridge_regression(X, y, alpha=alpha_list, return_intercept=True)[0]\n    coef_scalar = ridge_regression(X, y, alpha=alpha_scalar, return_intercept=True)[0]\n\n    assert_array_almost_equal(coef_array, coef_list)\n    assert_array_almost_equal(coef_array, coef_scalar)\n", "def test_ridge_regression_multi_target():\n    rng = np.random.RandomState(42)\n    X = rng.rand(100, 5)\n    y = rng.rand(100, 2)\n\n    # Test with multiple targets and a single alpha value\n    coefs_single_alpha = ridge_regression(X, y, alpha=1.0)\n    assert_equal(coefs_single_alpha.shape, (2, 5))\n\n    # Test with multiple targets and individual alpha values\n    alphas = [0.5, 1.0]\n    coefs_individual_alphas = ridge_regression(X, y, alpha=alphas)\n    assert_equal(coefs_individual_alphas.shape, (2, 5))\n", "def test_ridge_multi_target(solver):\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_targets = 6, 5, 3\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    alpha = [1.0] * n_targets\n\n    ridge = Ridge(alpha=alpha, solver=solver)\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (n_targets, n_features))\n    assert_greater(ridge.score(X, y), 0.47)\n"], "sample_1188": ["def test_pretty_print_unicode_d():\n    assert upretty(d[0]) == '(0|0)'\n    assert upretty(d[1]) == '(i_N|k_N)'\n    assert upretty(d[4]) == '(a) (i_N|k_N)'\n    assert upretty(d[5]) == '(a) (i_N|k_N) + (-b) (j_N|k_N)'\n    assert upretty(d[7]) == upretty_d_7\n    assert upretty(d[10]) == '(cos(a)) (i_C|k_N) + (-sin(a)) (j_C|k_N)'\n", "def test_pretty_print_unicode_d():\n    assert upretty(d[0]) == '(0|0)'\n    assert upretty(d[5]) == '(a) (i_N|k_N) + (-b) (j_N|k_N)'\n    assert upretty(d[7]) == upretty_d_7\n    assert upretty(d[10]) == '(cos(a)) (i_C|k_N) + (-sin(a)) (j_C|k_N)'\n", "def test_pretty_print_unicode_d():\n    assert upretty(d[0]) == '(0|0)'\n    assert upretty(d[5]) == '(a) (i_N|k_N) + (-b) (j_N|k_N)'\n    assert upretty(d[7]) == upretty_d_7\n    assert upretty(d[10]) == '(cos(a)) (i_C|k_N) + (-sin(a)) (j_C|k_N)'\n", "def test_custom_names_different_ordering():\n    B = CoordSys3D('B', vector_names=['y', 'z', 'x'],\n                   variable_names=['j', 'k', 'i'])\n    assert B.i.__str__() == 'B.i'\n    assert B.x.__str__() == 'B.x'\n    assert B.i._pretty_form == 'i_B'\n    assert B.x._pretty_form == 'x_B'\n    assert B.i._latex_form == r'\\mathbf{{i}_{B}}'\n    assert B.x._latex_form == r\"\\mathbf{\\hat{x}_{B}}\"\n", "def test_differential():\n    from sympy import symbols, diff, cos, sin\n    from sympy.vector import CoordSys3D, Differential\n\n    N = CoordSys3D('N')\n    x, y, z, t = symbols('x y z t')\n\n    f = cos(x)*sin(y)\n    df = diff(f, N.x) + diff(f, N.y)\n    df_str = \"sin(x)*sin(y) + cos(x)*cos(y)*(d x_N)\"\n\n    assert str(df) == df_str\n    assert upretty(df) == \"cos(x) cos(y) + sin(x) sin(y) d x_N\"\n    assert latex(df) == r\"cos\\left(x\\right) cos\\left(y\\right) + sin\\left(x\\right) sin\\left(y\\right) \\frac{\\partial}{\\partial x_{N}}\"\n\n    dx = Differential(N.x)\n    dy = Differential(N.y)\n    dz = Differential(N.z)\n\n    assert str(dx) == \"d x_N\"\n    assert upretty(dx) == \"d x_N\"\n    assert latex(dx) == r\"\\frac{\\partial}{\\partial x_{N}}\"\n\n    assert str(dy) == \"d y_N\"\n    assert upretty(dy) == \"d y_N\"\n    assert latex(dy) == r\"\\frac{\\partial}{\\partial y_{N}}\"\n\n    assert str(dz) == \"d z_N\"\n    assert upretty(dz) == \"d z_N\"\n    assert latex(dz) == r\"\\frac{\\partial}{\\partial z_{N}}\"\n"], "sample_97": ["def test_file_deleted(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.existing_file)\n    self.existing_file.unlink()\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "def test_non_existing_glob_directory(self, mocked_modules, notify_mock):\n    non_existing_dir = self.tempdir / 'does_not_exist'\n    self.reloader.watch_dir(non_existing_dir, '*.py')\n    with self.tick_twice():\n        # No changes should be detected since the directory does not exist\n        pass\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_file_deleted(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.existing_file)\n    os.remove(self.existing_file)\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "def test_deleted_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.existing_file)\n    with self.tick_twice():\n        self.existing_file.unlink()\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "    def test_notify_file_changed_without_signal(self):\n        reloader = autoreload.StatReloader()\n        with mock.patch('django.utils.autoreload.trigger_reload') as mock_trigger_reload:\n            reloader.notify_file_changed(self.existing_file)\n            self.assertEqual(mock_trigger_reload.call_count, 1)\n            self.assertEqual(mock_trigger_reload.call_args[0], (self.existing_file,))\n"], "sample_851": ["def test_regression_multioutput_invalid_shape():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1], [2], [5], [5]]\n\n    with pytest.raises(ValueError, match=\"have different number of output\"):\n        _check_reg_targets(y_true, y_pred, None)\n", "def test_multioutput_regression_errors():\n    y_true = np.array([[1, 0, 0, 1], [0, 1, 1, 1], [1, 1, 0, 1]])\n    y_pred = np.array([[0, 0, 0, 1], [1, 0, 1, 1], [0, 0, 0, 1]])\n\n    with pytest.raises(ValueError, match=\"Multioutput not supported in max_error\"):\n        max_error(y_true, y_pred)\n\n    with pytest.raises(ValueError, match=\"Multioutput not supported in mean_tweedie_deviance\"):\n        mean_tweedie_deviance(y_true, y_pred)\n", "def test_regression_multioutput_string_input():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n\n    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n    mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    r = r2_score(y_true, y_pred, multioutput='raw_values')\n    evs = explained_variance_score(y_true, y_pred, multioutput='raw_values')\n\n    mse_uniform = mean_squared_error(y_true, y_pred, multioutput='uniform_average')\n    mae_uniform = mean_absolute_error(y_true, y_pred, multioutput='uniform_average')\n    r_uniform = r2_score(y_true, y_pred, multioutput='uniform_average')\n    evs_uniform = explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n\n    mse_variance = mean_squared_error(y_true, y_pred, multioutput='variance_weighted')\n    r_variance = r2_score(y_true, y_pred, multioutput='variance_weighted')\n    evs_variance = explained_variance_score(y_true, y_pred, multioutput='variance_weighted')\n\n    # Add assertions to compare the results with expected values or other metrics\n    # For example:\n    # assert_array_almost_equal(mse_uniform, expected_mse_uniform, decimal=2)\n    # assert_array_almost_equal(r_variance, expected_r_variance, decimal=2)\n", "def test_regression_metrics_multioutput_at_limits():\n    y_true = np.array([[0, 0], [0, 0]])\n    y_pred = np.array([[0, 0], [0, 0]])\n    assert_almost_equal(mean_squared_error(y_true, y_pred), 0.00, 2)\n    assert_almost_equal(mean_squared_error(y_true, y_pred, squared=False), 0.00, 2)\n    assert_almost_equal(mean_absolute_error(y_true, y_pred), 0.00, 2)\n    assert_almost_equal(median_absolute_error(y_true, y_pred), 0.00, 2)\n    assert_almost_equal(max_error(y_true, y_pred), 0.00, 2)\n    assert_almost_equal(explained_variance_score(y_true, y_pred), 1.00, 2)\n    assert_almost_equal(r2_score(y_true, y_pred), 1.00, 2)\n    err_msg = \"Multioutput not supported in mean_squared_log_error\"\n    with pytest.raises(ValueError, match=err_msg):\n        mean_squared_log_error(y_true, y_pred)\n    err_msg = \"Multioutput not supported in max_error\"\n    with pytest.raises(ValueError, match=err_msg):\n        max_error([[0, 0], [1, 1]], [[0, 0], [2, 2]])\n\n    # Tweedie deviance error\n    power = 0\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, power=power),\n                    0.00, atol=1e-8)\n    power = 1\n    err_msg = \"Multioutput not supported in mean_tweedie_deviance\"\n    with", "def test_regression_metrics_with_sample_weight():\n    y_true = np.array([1, 2, 3, 4, 5])\n    y_pred = np.array([1.2, 2.1, 3.3, 3.8, 4.9])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.2, 0.2])\n\n    weighted_mse = mean_squared_error(y_true, y_pred, sample_weight=sample_weight)\n    weighted_mae = mean_absolute_error(y_true, y_pred, sample_weight=sample_weight)\n    weighted_r2 = r2_score(y_true, y_pred, sample_weight=sample_weight)\n\n    # Manually calculate the weighted metrics for comparison\n    weighted_sum = np.sum(sample_weight)\n    weighted_mean_true = np.sum(y_true * sample_weight) / weighted_sum\n    weighted_mean_pred = np.sum(y_pred * sample_weight) / weighted_sum\n    total_error = np.sum(sample_weight * (y_true - y_pred) ** 2)\n    total_variance = np.sum(sample_weight * (y_true - weighted_mean_true) ** 2)\n    total_weighted_mae = np.sum(sample_weight * np.abs(y_true - y_pred))\n    expected_weighted_mse = total_error / weighted_sum\n    expected_weighted_mae = total_weighted_mae / weighted_sum\n    expected_weighted_r2 = 1 - (total_error / total_variance)\n\n    assert_almost_equal(weighted_mse, expected_weighted_mse, decimal=5)\n    assert_almost_equal(weighted_mae, expected_weighted_mae, decimal=5)\n    assert_almost_equal(weighted_r2, expected_weighted_r2, decimal="], "sample_449": ["    def test_ipv6_support(self):\n        \"\"\"WSGIServer supports IPv6 when ipv6=True is passed to the constructor.\"\"\"\n        try:\n            server = WSGIServer((\"::1\", 0), WSGIRequestHandler, ipv6=True)\n            self.assertEqual(server.address_family, socket.AF_INET6)\n        finally:\n            server.server_close()\n", "def test_ipv6_support(self):\n    \"\"\"WSGIServer supports IPv6.\"\"\"\n    with self.settings(ALLOWED_HOSTS=['::1']):\n        request = WSGIRequest(self.request_factory.get(\"/\", REMOTE_ADDR='::1').environ)\n        client_address = (\"::1\", 8080)\n        server = WSGIServer((\"::1\", 0), WSGIRequestHandler, ipv6=True)\n        try:\n            server.handle_request(request)\n            self.assertTrue(server.socket.family == socket.AF_INET6)\n        finally:\n            server.server_close()\n", "def test_ipv6_address_family(self):\n    \"\"\"WSGIServer uses AF_INET6 socket family when ipv6 is True.\"\"\"\n    server = WSGIServer((\"localhost\", 0), WSGIRequestHandler, ipv6=True)\n    self.assertEqual(server.address_family, socket.AF_INET6)\n    server.server_close()\n", "def test_close_request_threaded_server(self):\n    \"\"\"ThreadedWSGIServer closes connections on request.\"\"\"\n    server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler)\n    server.connections_override = {\"default\": \"fake_connection\"}\n    request = WSGIRequest(self.request_factory.get(\"/\").environ)\n    client_address = (\"192.168.2.0\", 8080)\n    with patch(\"django.db.connections.close_all\") as mock_close_all:\n        server.close_request(request)\n        mock_close_all.assert_called_once()\n", "def test_threaded_wsgi_server_connections_override(self):\n    \"\"\"ThreadedWSGIServer overrides database connections in new threads.\"\"\"\n    from django.db import connections\n\n    # Stub a database connection.\n    class FakeConnection:\n            self.closed = False\n\n            self.closed = True\n\n    fake_connection = FakeConnection()\n\n    # Stub out the WSGI application to return a response.\n        start_response(\"200 OK\", [])\n        return [b\"Hello, world.\"]\n\n    rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n    rfile.seek(0)\n\n    wfile = UnclosableBytesIO()\n\n        if mode == \"rb\":\n            return rfile\n        elif mode == \"wb\":\n            return wfile\n\n    request = Stub(makefile=makefile)\n    server = ThreadedWSGIServer(\n        (\"localhost\", 0),\n        WSGIRequestHandler,\n        connections_override={\"default\": fake_connection},\n    )\n    server.set_app(test_app)\n\n    # Instantiating a handler runs the request as side effect.\n    WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n    # The database connection in the new thread should be closed.\n    self.assertTrue(fake_connection.closed)\n\n    # The original database connection should not be closed.\n    self.assertFalse(connections[\"default\"].closed)\n"], "sample_756": ["def test_precomputed_dists_error():\n    redX = X[::2]\n    dists = pairwise_distances(redX, metric='euclidean')\n    dists_wrong_shape = dists[:-1, :]\n\n    with pytest.raises(ValueError, match=\"Precomputed metric requires shape\"):\n        OPTICS(min_samples=10, algorithm='brute', metric='precomputed').fit(dists_wrong_shape)\n", "def test_nearest_neighbors_invalid():\n    clust = OPTICS(min_samples=len(X) + 1)\n    with pytest.raises(ValueError, match=\"Number of neighbors must be less than the number of samples\"):\n        clust.fit(X)\n", "def test_invalid_min_samples():\n    # Test that an invalid min_samples value raises an error\n    msg = \"Number of training samples (n_samples=5) must be greater than min_samples (min_samples=10) used for clustering.\"\n    clust = OPTICS(max_eps=5.0 * 0.3, min_samples=10)\n    X = np.random.rand(5, 2)\n    assert_raise_message(ValueError, msg, clust.fit, X)\n", "def test_empty_input():\n    # Test the case where the input dataset is empty\n    X = np.empty((0, 2))\n    clust = OPTICS(min_samples=10)\n    assert_raise_message(ValueError, \"X.shape[0] == 0\", clust.fit, X)\n", "def test_core_distances_computation():\n    # Test the _compute_core_distances_ function\n    nbrs = NearestNeighbors(n_neighbors=10)\n    nbrs.fit(X)\n    expected_core_distances = np.sort(pairwise_distances(X, metric='euclidean'))[:, 9]\n    clust = OPTICS(min_samples=10)\n    computed_core_distances = clust._compute_core_distances_(X, nbrs)\n    assert_allclose(expected_core_distances, computed_core_distances)\n"], "sample_115": ["def test_unicode_error(self):\n    \"\"\"\n    A UnicodeError displays a portion of the problematic string.\n    \"\"\"\n    try:\n        ''.encode('ascii')\n    except Exception:\n        exc_type, exc_value, tb = sys.exc_info()\n    reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n    html = reporter.get_traceback_html()\n    self.assertIn('<h2>Unicode error hint</h2>', html)\n    self.assertIn('The string that could not be encoded/decoded was: ', html)\n    self.assertIn('<strong></strong>', html)\n", "def test_get_default_exception_reporter_filter(self):\n    \"\"\"\n    The default exception reporter filter is cached after the first call.\n    \"\"\"\n    with mock.patch('django.views.debug.import_string') as import_string:\n        import_string.return_value = SafeExceptionReporterFilter\n        filter1 = get_default_exception_reporter_filter()\n        filter2 = get_default_exception_reporter_filter()\n        import_string.assert_called_once_with('django.views.debug.SafeExceptionReporterFilter')\n        self.assertIs(filter1, filter2)\n", "    def test_get_safe_settings(self):\n        \"\"\"\n        Test the get_safe_settings method of the SafeExceptionReporterFilter class.\n        \"\"\"\n        with self.settings(DEBUG=True, SECRET_KEY='test_secret_key'):\n            reporter_filter = SafeExceptionReporterFilter()\n            safe_settings = reporter_filter.get_safe_settings()\n            self.assertNotIn('SECRET_KEY', safe_settings)\n            self.assertEqual(safe_settings['SECRET_KEY'], reporter_filter.cleansed_substitute)\n", "def test_multivalue_dict_key_error_paranoid(self):\n    \"\"\"\n    #21098 -- No sensitive POST parameters' values are displayed in the\n    error reports for if request.POST['nonexistent_key'] throws an error\n    with a paranoid request.\n    \"\"\"\n    self.verify_paranoid_response(multivalue_dict_key_error)\n    self.verify_paranoid_email(multivalue_dict_key_error)\n", "    def test_exception_without_traceback(self):\n        \"\"\"\n        ExceptionReporter should still generate a report if the exception doesn't have a traceback.\n        \"\"\"\n        request = self.client.get('/')\n        try:\n            raise Exception(\"Test exception\")\n        except Exception as e:\n            e.__traceback__ = None\n            reporter = ExceptionReporter(request, type(e), e, None)\n            html = reporter.get_traceback_html()\n            self.assertIn('<h1>Exception at /</h1>', html)\n            self.assertIn('<pre class=\"exception_value\">Test exception</pre>', html)\n"], "sample_778": ["def test_nmf_zero_input():\n    # Test that NMF handles input with all zeros\n    X = np.zeros((10, 10))\n    m = NMF(n_components=5, init='random', random_state=0)\n    W = m.fit_transform(X)\n    H = m.components_\n    assert_array_equal(W, np.zeros((10, 5)))\n    assert_array_equal(H, np.zeros((5, 10)))\n", "def test_nmf_max_iter_tolerance(solver):\n    # Test that NMF raises ConvergenceWarning if max_iter is not reached\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(6, 5))\n    model = NMF(solver=solver, n_components=3, init='random', random_state=0, max_iter=1, tol=1e-10)\n    with pytest.warns(ConvergenceWarning):\n        model.fit(A)\n", "def test_nmf_input_validation():\n    # Test that the input validation in NMF.fit and NMF.fit_transform works correctly\n    rng = np.random.mtrand.RandomState(42)\n    A = rng.randn(10, 10)\n\n    # Test that an error is raised for invalid solver\n    with pytest.raises(ValueError) as excinfo:\n        NMF(solver='invalid').fit(A)\n    assert \"Invalid solver parameter: got 'invalid' instead of one of\" in str(excinfo.value)\n\n    # Test that an error is raised for invalid beta_loss\n    with pytest.raises(ValueError) as excinfo:\n        NMF(solver='mu', beta_loss='invalid').fit(A)\n    assert \"Invalid beta_loss parameter: got 'invalid' instead of one of\" in str(excinfo.value)\n\n    # Test that an error is raised for invalid init\n    with pytest.raises(ValueError) as excinfo:\n        NMF(init='invalid').fit(A)\n    assert \"Invalid init parameter: got 'invalid' instead of one of\" in str(excinfo.value)\n\n    # Test that an error is raised for invalid regularization\n    with pytest.raises(ValueError) as excinfo:\n        nmf._compute_regularization(0.5, 0.5, 'invalid')\n    assert \"Invalid regularization parameter: got 'invalid' instead of one of\" in str(excinfo.value)\n", "def test_nmf_with_nndsvd_init():\n    # Test that NMF with 'nndsvd' initialization does not raise warnings\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(10, 10))\n    A[:, 2 * np.arange(5)] = 0\n\n    for solver in ('cd', 'mu'):\n        with assert_no_warnings():\n            NMF(solver=solver, n_components=5, init='nndsvd', random_state=0, tol=1e-2).fit(A)\n", "def test_nmf_zero_input():\n    # Test that NMF handles zero input correctly\n    A = np.zeros((10, 10))\n    for solver in ('cd', 'mu'):\n        model = NMF(solver=solver, n_components=5, init='random',\n                    random_state=0, tol=1e-2)\n        W = model.fit_transform(A)\n        H = model.components_\n        # All elements of W and H should be zero for zero input\n        assert not np.any(W)\n        assert not np.any(H)\n"], "sample_72": ["def test_serialize_models_model(self):\n    self.assertSerializedEqual(models.Model)\n    self.assertSerializedResultEqual(\n        models.Model,\n        (\"models.Model\", {\"from django.db import models\"})\n    )\n", "def test_serialize_model_field_with_custom_serializer(self):\n    class CustomField(models.CharField):\n            super().__init__(*args, **kwargs)\n            self.custom_arg = custom_arg\n\n    field = CustomField(max_length=255, custom_arg='custom_value_changed')\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"migrations.test_writer.CustomField(custom_arg='custom_value_changed', max_length=255)\"\n    )\n", "def test_serialize_model_field_serializer(self):\n    field = models.FileField(upload_to='static/uploads/')\n    serializer = ModelFieldSerializer(field)\n    string, imports = serializer.serialize()\n    self.assertEqual(string, \"models.FileField(upload_to='static/uploads/')\")\n    self.assertEqual(imports, {'from django.db import models'})\n", "def test_serialize_operations(self):\n    operation = custom_migration_operations.more_operations.CustomOperation('test')\n    buff, imports = OperationWriter(operation, indentation=0).serialize()\n    self.assertEqual(imports, {'import custom_migration_operations.more_operations'})\n    self.assertEqual(\n        buff,\n        \"custom_migration_operations.more_operations.CustomOperation(\\n\"\n        \"    name='test',\\n\"\n        \"),\"\n    )\n", "def test_serialize_custom_model(self):\n    \"\"\"\n    Test serialization of a custom model.\n    \"\"\"\n    class CustomModel(models.Model):\n        name = models.CharField(max_length=100)\n\n    migration = type(\"Migration\", (migrations.Migration,), {\n        \"operations\": [\n            migrations.CreateModel(\"CustomModel\", (\"name\", CustomModel._meta.get_field(\"name\")), bases=(models.Model,)),\n        ],\n        \"dependencies\": []\n    })\n    writer = MigrationWriter(migration)\n    output = writer.as_string()\n    result = self.safe_exec(output)\n    self.assertIn(\"CustomModel\", result)\n"], "sample_846": ["def test_column_transformer_with_empty_transformer_list():\n    X = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([])\n    assert_array_equal(ct.fit_transform(X), X)\n    assert_array_equal(ct.fit(X).transform(X), X)\n    assert len(ct.transformers_) == 0\n", "def test_column_transformer_callable_specifier_with_transformer_weights():\n    # Test that transformer_weights works with callable specifiers\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([[0, 1, 2]]).T * 0.5\n\n        return [0]\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop',\n                           transformer_weights={'trans': 0.5})\n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == [0]\n", "def test_column_transformer_transformers_order():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([[0, 1, 2]]).T\n    X_res_second = np.array([[2, 4, 6]]).T\n    X_res_both = X_array\n\n    ct = ColumnTransformer([('trans1', Trans(), [1]), ('trans2', Trans(), [0])])\n    assert_array_equal(ct.fit_transform(X_array), X_res_both[:, ::-1])\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both[:, ::-1])\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[0][0] == 'trans1'\n    assert ct.transformers_[1][0] == 'trans2'\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])], remainder='passthrough')\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[0][0] == 'trans1'\n    assert ct.transformers_[1][0] == 'trans2'\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[0][0] == 'trans1'\n    assert ct.transformers_[1][0] == 'trans2'", "def test_column_transformer_remainder_estimator():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n\n    # second and third columns are doubled when remainder = DoubleTrans\n    X_res_both = X_array.copy()\n    X_res_both[:, 1:3] *= 2\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans())\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n\n    # test with a custom remainder estimator\n    class CustomRemainderTransformer(BaseEstimator, TransformerMixin):\n            return self\n\n            return X * 3\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=CustomRemainderTransformer())\n\n    X_res_both_custom = X_array.copy()\n    X_res_both_custom[:, 1:3] *= 3\n\n    assert_array_equal(ct.fit_transform(X_array), X_res_both_custom)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both_custom)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], CustomRemainderTransformer)\n    assert_", "def test_column_transformer_with_array_like_columns():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # Using array-like of strings for columns\n    ct = ColumnTransformer([('trans1', Trans(), ['0', '1'])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n\n    # Using array-like of integers for columns\n    ct = ColumnTransformer([('trans1', Trans(), [0, 1])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n\n    # Using array-like of boolean for columns\n    ct = ColumnTransformer([('trans1', Trans(), [True, True, False])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_array[:, :2])\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array[:, :2])\n"], "sample_538": ["def test_transform_bbox():\n    bbox = mtransforms.Bbox.from_extents(0, 0, 1, 1)\n    transform = mtransforms.Affine2D().translate(1, 2)\n    transformed_bbox = transform.transform_bbox(bbox)\n    expected_bbox = mtransforms.Bbox.from_extents(1, 2, 2, 3)\n    assert_array_equal(transformed_bbox.bounds, expected_bbox.bounds)\n", "def test_transformwrapper_dimensions():\n    t = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    with pytest.raises(ValueError, match=(\n            r\"The input and output dims of the new child \\(3, 3\\) \"\n            r\"do not match those of current child \\(2, 2\\)\")):\n        t.set(mtransforms.TransformedPath(Path([[0, 0], [1, 1], [2, 2]]), mtransforms.Affine2D()))\n", "def test_transformed_path_non_affine():\n    points = [(0, 0), (1, 0), (1, 1), (0, 1)]\n    path = Path(points, closed=True)\n\n    trans = mtransforms.NonAffineForTest(mtransforms.Affine2D().scale(2))\n    trans_path = mtransforms.TransformedPath(path, trans)\n\n    # The non-affine part of the transform should be applied to the path.\n    expected_path = Path(np.array(points) * 2, closed=True)\n    assert_allclose(trans_path.get_transformed_path_and_affine()[0].vertices, expected_path.vertices)\n\n    # The affine part of the transform should be returned separately.\n    assert_allclose(trans_path.get_transformed_path_and_affine()[1].get_matrix(), mtransforms.Affine2D().get_matrix())\n", "def test_affine_delta_transform():\n    t = mtransforms.Affine2D()\n    t.translate(1, 2)\n    t_delta = mtransforms.AffineDeltaTransform(t)\n    t_delta_inverse = mtransforms.AffineDeltaTransform(t.inverted())\n    a = np.array([[1, 0], [0, 1]])\n    b = np.array([[0, 0], [0, 0]])\n    assert np.allclose(t_delta.transform(a), t.transform(a))\n    assert np.allclose(t_delta.transform(b), b)\n    assert np.allclose(t_delta_inverse.transform(a), t.inverted().transform(a))\n    assert np.allclose(t_delta_inverse.transform(b), b)\n", "def test_transform_non_affine_invalidation():\n    # Tests that the non-affine part of a transform is correctly invalidated\n    # when the transform changes.\n    t = NonAffineForTest(mtransforms.Affine2D())\n    p = np.array([[0, 0], [1, 1]])\n\n    # Initial non-affine transform should be an identity transform.\n    assert_array_equal(t.transform_non_affine(p), p)\n\n    # Changing the underlying transform should invalidate the non-affine part.\n    t.real_trans.translate(1, 2)\n    assert_array_equal(t.transform_non_affine(p), p + [1, 2])\n\n    # Changing the non-affine transform itself should also invalidate it.\n    t.real_trans = mtransforms.Affine2D().scale(3)\n    assert_array_equal(t.transform_non_affine(p), p * 3)\n"], "sample_850": ["def test_nystroem_warning():\n    # Test that a warning is raised when n_components > n_samples\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    with pytest.warns(UserWarning, match=\"n_components > n_samples. This is not possible.\"):\n        nystroem = Nystroem(n_components=20)\n        X_transformed = nystroem.fit_transform(X)\n", "def test_nystroem_kernel_params():\n    # Test Nystroem with custom kernel_params\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n        return np.exp(-param1 * np.sum((X - Y) ** 2, axis=1)) + param2\n\n    K = custom_kernel(X[:, None], X[None, :], param1=0.5, param2=0.1)\n    nystroem = Nystroem(kernel=custom_kernel, n_components=X.shape[0],\n                        kernel_params={'param1': 0.5, 'param2': 0.1})\n    X_transformed = nystroem.fit_transform(X)\n    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)\n", "def test_nystroem_sparse_input():\n    # Test Nystroem with sparse input\n    rnd = np.random.RandomState(42)\n    X = csr_matrix(rnd.uniform(size=(10, 4)))\n\n    # rbf kernel should behave as gamma=None by default\n    # aka gamma = 1 / n_features\n    nystroem = Nystroem(n_components=10)\n    X_transformed = nystroem.fit_transform(X)\n    K = rbf_kernel(X.toarray(), gamma=None)\n    K2 = np.dot(X_transformed, X_transformed.T)\n    assert_array_almost_equal(K, K2)\n", "def test_rbf_sampler_sparse_input():\n    # test that RBFSampler can handle sparse input\n    rng = np.random.RandomState(42)\n    X = rng.uniform(size=(10, 20))\n    X_sparse = csr_matrix(X)\n\n    gamma = 10.\n    kernel = rbf_kernel(X, X, gamma=gamma)\n\n    rbf_transform = RBFSampler(gamma=gamma, n_components=1000, random_state=42)\n    X_trans = rbf_transform.fit_transform(X_sparse)\n    kernel_approx = np.dot(X_trans, X_trans.T)\n\n    assert_array_almost_equal(kernel, kernel_approx, decimal=1)\n", "def test_skewed_chi2_sampler_neg_skewedness():\n    # test that SkewedChi2Sampler works with negative skewedness\n    c = -0.03\n    Y_copy = Y.copy()\n    Y_copy[0, 0] = c / 2.\n\n    # approximate kernel mapping\n    transform = SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42)\n    X_trans = transform.fit_transform(X)\n    Y_trans = transform.transform(Y_copy)\n\n    kernel_approx = np.dot(X_trans, Y_trans.T)\n    assert np.isfinite(kernel_approx).all(), 'NaNs found in the approximate Gram matrix'\n"], "sample_174": ["def test_adapt_ipaddressfield_value(self):\n    self.assertIsNone(self.ops.adapt_ipaddressfield_value(None))\n    self.assertEqual(self.ops.adapt_ipaddressfield_value('192.168.0.1'), '192.168.0.1')\n", "def test_force_no_ordering(self):\n    self.assertEqual(self.ops.force_no_ordering(), [])\n", "def test_subtract_temporals_supported(self):\n    duration_field = DurationField()\n    duration_field_internal_type = duration_field.get_internal_type()\n    lhs = ('INTERVAL %s SECOND', [100])\n    rhs = ('INTERVAL %s SECOND', [50])\n    result = self.ops.subtract_temporals(duration_field_internal_type, lhs, rhs)\n    self.assertEqual(result, ('(INTERVAL %s SECOND - INTERVAL %s SECOND)', [100, 50]))\n", "def test_adapt_unknown_value_datetime(self):\n    value = timezone.now()\n    self.assertEqual(self.ops.adapt_unknown_value(value), self.ops.adapt_datetimefield_value(value))\n", "def test_execute_sql_flush_with_savepoint(self):\n    with transaction.atomic(savepoint=True):\n        author = Author.objects.create(name='J.R.R. Tolkien')\n        Book.objects.create(author=author)\n        self.assertIs(Author.objects.exists(), True)\n        self.assertIs(Book.objects.exists(), True)\n\n    sql_list = connection.ops.sql_flush(\n        no_style(),\n        [Author._meta.db_table, Book._meta.db_table],\n    )\n    connection.ops.execute_sql_flush(sql_list)\n\n    with transaction.atomic(savepoint=True):\n        self.assertIs(Author.objects.exists(), True)\n        self.assertIs(Book.objects.exists(), True)\n"], "sample_88": ["def test_send_messages_connection_error(self):\n    \"\"\"A socket connection error is raised with fail_silently=False.\"\"\"\n    self.backend.fail_silently = False\n    with self.assertRaises(ConnectionError):\n        self.backend.open()\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.backend = smtp.EmailBackend(username='', password='')\n        cls.server.stop()\n", "    def test_send_messages_with_fail_silently(self):\n        \"\"\"\n        send_messages() returns the number of successfully delivered messages\n        when fail_silently=True.\n        \"\"\"\n        backend = smtp.EmailBackend()\n        backend.fail_silently = True\n        backend.connection = True\n        email1 = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        email2 = EmailMessage('Subject', 'Content', 'from@example.com', ['invalid@'])\n        sent = backend.send_messages([email1, email2])\n        self.assertEqual(sent, 1)\n", "def test_email_backend_fail_silently_on_send_error(self):\n    \"\"\"\n    A socket send error is silenced with fail_silently=True.\n    \"\"\"\n    with self.backend:\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        self.backend.fail_silently = True\n        self.assertEqual(self.backend.send_messages([email]), 0)\n", "def test_send_messages_with_fail_silently(self):\n    backend = smtp.EmailBackend()\n    # Simulate connection initialization success and a subsequent\n    # connection error during send_messages.\n    backend.connection = True\n    backend.open = lambda: None\n    email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    backend.fail_silently = True\n    sent = backend.send_messages([email])\n    self.assertEqual(sent, 0)\n"], "sample_552": ["def test_savefig_pil_kwargs(fmt):\n    fig = plt.figure()\n    fig.savefig(io.BytesIO(), format=fmt, pil_kwargs={\"quality\": 95})\n", "def test_figure_close_axes_clear():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    fig.close()\n    assert fig.axes == []\n", "def test_add_subplot_invalid_kwargs():\n    fig = plt.figure()\n    with pytest.raises(TypeError, match=\"unexpected keyword argument 'invalid_kwarg'\"):\n        fig.add_subplot(1, 1, 1, invalid_kwarg=True)\n", "def test_tightbbox_edgecolor():\n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.plot([0, 1], [0, 1])\n    ax1.set_edgecolor('red')\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.plot([0, 1], [0, 1])\n    ax2.set_edgecolor('blue')\n    ax2.set_facecolor('none')\n    renderer = fig.canvas.get_renderer()\n    fig.tight_layout()\n    bbox = fig.get_tightbbox(renderer)\n    assert bbox.p0[0] == 0\n    assert bbox.p1[0] == fig.bbox.width\n    assert bbox.p0[1] == 0\n    assert bbox.p1[1] == fig.bbox.height\n", "def test_figure_init():\n    fig = Figure()\n    assert isinstance(fig, Figure)\n    assert fig.get_figwidth() == 6.4\n    assert fig.get_figheight() == 4.8\n    assert fig.get_dpi() == 100\n    assert fig.get_facecolor() == 'white'\n    assert fig.get_edgecolor() == 'white'\n"], "sample_706": ["def test_advanced_cases(expr: str, expected: bool) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_expressions(expr: str) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is eval(expr, {\"True\": True, \"False\": False})\n", "def test_special_characters_in_idents(ident: str) -> None:\n    assert evaluate(ident, {ident: True}.__getitem__)\n", "def test_parentheses_precedence(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is True\n"], "sample_315": ["    def test_en_url_with_special_characters(self):\n        response = self.client.get('/account/register/with-special-characters@example.com/', HTTP_ACCEPT_LANGUAGE='en')\n        self.assertRedirects(response, '/en/account/register/with-special-characters@example.com/')\n\n        response = self.client.get(response.headers['location'])\n        self.assertEqual(response.status_code, 200)\n", "    def test_prefixed_with_slash(self):\n        with translation.override('en'):\n            self.assertEqual(reverse('prefixed-slash'), '/en/prefixed/')\n        with translation.override('nl'):\n            self.assertEqual(reverse('prefixed-slash'), '/nl/prefixed/')\n        with translation.override(None):\n            self.assertEqual(reverse('prefixed-slash'), '/%s/prefixed/' % settings.LANGUAGE_CODE)\n", "    def test_wrong_prefix(self):\n        response = self.client.get('/xx/account/register/')\n        self.assertRedirects(response, '/en/account/register/')\n\n        response = self.client.get(response.headers['location'])\n        self.assertEqual(response.status_code, 404)\n", "    def test_default_language_used(self):\n        response = self.client.get('/prefixed/')\n        self.assertRedirects(response, '/pt/prefixed/')\n\n        response = self.client.get(response.headers['location'])\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers['content-language'], 'pt')\n        self.assertEqual(response.context['LANGUAGE_CODE'], 'pt')\n", "    def test_prefix_default_language(self):\n        with translation.override('en-us'):\n            self.assertEqual(reverse('prefixed'), '/prefixed/')\n        with translation.override('nl'):\n            self.assertEqual(reverse('prefixed'), '/nl/prefixed/')\n"], "sample_601": ["def test_cftime_strftime_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    expected = xr.DataArray(\n        [\n            [\"0001-01-01 00:00:01\", \"0001-01-01 00:15:00\"],\n            [\"0001-01-01 00:23:00\", \"0001-01-02 00:00:01\"],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_identical(result, expected)\n", "def test_cftime_strftime_accessor(cftime_rounding_dataarray, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y%m%d%H\"\n    expected = xr.DataArray(\n        [\n            [\"0001010101\", \"0001010115\"],\n            [\"0001010123\", \"0001010201\"],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_identical(result, expected)\n", "def test_cftime_strftime_accessor(cftime_rounding_dataarray, cftime_date_type):\n    import dask.array as da\n\n    date_format = \"%Y%m%d%H\"\n    expected = xr.DataArray(\n        [\n            [\"0001010101\", \"0001010115\"],\n            [\"0001010123\", \"0001010201\"],\n        ],\n        name=\"strftime\",\n    )\n\n    # Currently a compute is done to inspect a single value of the array\n    # if it is of object dtype to check if it is a cftime.datetime (if not\n    # we raise an error when using the dt accessor).\n    with raise_if_dask_computes(max_computes=1):\n        result = cftime_rounding_dataarray.chunk({\"dim_0\": 1}).dt.strftime(date_format)\n\n    expected = expected.chunk({\"dim_0\": 1})\n    assert isinstance(result.data, da.Array)\n    assert result.chunks == expected.chunks\n    assert_identical(result, expected)\n", "def test_dask_accessor_method_cftime(cftime_rounding_dataarray, cftime_date_type, method, freq):\n    import dask.array as da\n\n    if method == \"floor\":\n        expected = xr.DataArray(\n            [\n                [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n                [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            ],\n            name=method,\n        )\n    elif method == \"ceil\":\n        expected = xr.DataArray(\n            [\n                [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n                [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 3, 0)],\n            ],\n            name=method,\n        )\n    elif method == \"round\":\n        expected = xr.DataArray(\n            [\n                [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n                [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n            ],\n            name=method,\n        )\n\n    chunks = {\"dim_0\": 1}\n    with raise_if_dask_computes(max_computes=1):\n        result = getattr(cftime_rounding_dataarray.chunk(", "def test_strftime_cftime(data):\n    \"\"\"Test strftime method with cftime.\"\"\"\n    date_format = \"%Y-%m-%d\"\n    result = data.time.dt.strftime(date_format)\n    expected = xr.DataArray(\n        xr.coding.cftimeindex.CFTimeIndex(data.time.values).strftime(date_format),\n        name=\"strftime\",\n        coords=data.time.coords,\n        dims=data.time.dims,\n    )\n    assert_equal(result, expected)\n"], "sample_1092": ["def test_issue_18203_with_evaluate_false():\n    eq = Mul(x + y, x + y, evaluate=False)\n    assert cse(eq) == ([(x0, x + y)], [x0**2])\n", "def test_cse_with_functions():\n    f = Function('f')\n    g = Function('g')\n    eq = f(x) + g(x)\n    substs, reduced = cse([eq, f(x), g(x)])\n    assert substs == [(x0, f(x)), (x1, g(x))]\n    assert reduced == [x0 + x1, x0, x1]\n", "def test_cse_with_matrices():\n    A = Matrix([[x, y], [z, w]])\n    B = Matrix([[1, 2], [3, 4]])\n    C = A + B\n    D = A * B\n    substs, reduced = cse([C, D])\n    assert substs == [(x0, A)]\n    assert reduced == [x0 + B, x0 * B]\n", "def test_issue_18203_with_evaluate_false():\n    eq = Mul(x + y, x + y, evaluate=False)\n    cse_eq = cse(eq)[1][0]\n    assert cse_eq == Pow(x + y, 2)\n", "def test_cse_with_large_expressions():\n    nterms = 100\n    x = symbols('x:%d' % nterms)\n    exprs = [\n        reduce(add, [x[j]*(-1)**(i+j) for j in range(nterms)])\n        for i in range(nterms)\n    ]\n    assert all((exprs[i] + exprs[i+1]).simplify() == 0 for i in range(0, nterms, 2))\n    subst, red = cse(exprs)\n    assert len(subst) > 0, \"exprs[i] == -exprs[i+1], i.e. a CSE\"\n    for i, e in enumerate(red):\n        assert (e.subs(reversed(subst)) - exprs[i]).simplify() == 0\n"], "sample_600": ["def test_CFScaleOffsetCoder_encode_decode_roundtrip():\n    original = xr.Variable((\"x\",), np.arange(10.0), encoding=dict(scale_factor=10, add_offset=0.1))\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    decoded = coder.decode(encoded)\n    assert_allclose(original, decoded)\n", "def test_CFMaskCoder_decode_multiple_fill_values():\n    original = xr.Variable((\"x\",), [0, -1, 1], {\"_FillValue\": [-1, 2]})\n    with pytest.warns(variables.SerializationWarning):\n        encoded = variables.CFMaskCoder().decode(original)\n    expected = xr.Variable((\"x\",), [0, np.nan, 1])\n    assert_identical(expected, encoded)\n", "def test_CFMaskCoder_encode_with_fill_value():\n    original = xr.Variable((\"x\",), [0, 1, np.nan], {\"_FillValue\": -9999})\n    expected = xr.Variable((\"x\",), [0, 1, -9999], {\"_FillValue\": -9999})\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=\"uint8\"), attrs={\"_Unsigned\": \"true\", \"_FillValue\": 255})\n    expected = xr.Variable((\"x\",), np.array([1, 2, -1], dtype=\"int8\"), attrs={\"_FillValue\": -1})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_CFMaskCoder_encode_missing_fill_value():\n    original = xr.Variable((\"x\",), [0.0, np.nan, 1.0], encoding={\"_FillValue\": 9999.0})\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    expected = xr.Variable((\"x\",), [0.0, 9999.0, 1.0], encoding={\"_FillValue\": 9999.0})\n    assert_identical(encoded, expected)\n"], "sample_1031": ["def test_unit_conversion():\n    from sympy.physics.units import meter, centimeter\n    assert meter.convert_to(centimeter) == 100\n    assert centimeter.convert_to(meter) == 0.01\n", "def test_quantity_conversion():\n    # Test conversion between different units of the same dimension\n    assert (2 * m).convert_to(km) == 0.002 * km\n    assert (3 * kg).convert_to(g) == 3000 * g\n    assert (4 * s).convert_to(ms) == 4000 * ms\n", "def test_quantity_scale_factor_dimension_mismatch():\n    mismatch_quantity = Quantity(\"mismatch_quantity\")\n    mismatch_quantity.set_dimension(mass)  # setting dimension to mass\n    mismatch_quantity.set_scale_factor(m)  # but scale factor is a length unit\n    with raises(ValueError, match=\"quantity value and dimension mismatch\"):\n        Quantity.get_dimensional_expr(mismatch_quantity.scale_factor)\n", "def test_quantity_conversion():\n    assert Quantity(2, \"meter\").to(\"centimeter\") == Quantity(200, \"centimeter\")\n    assert Quantity(2, \"centimeter\").to(\"meter\") == Quantity(0.02, \"meter\")\n    assert Quantity(2, \"second\").to(\"millisecond\") == Quantity(2000, \"millisecond\")\n", "def test_quantity_scale_factors():\n    # Testing the scale factors and their corresponding dimensions\n    for quantity, dimension in Quantity.SI_quantity_dimension_map.items():\n        dimex = Quantity.get_dimensional_expr(quantity.scale_factor)\n        assert DimensionSystem.default().equivalent_dims(dimension, Dimension(dimex)), f\"Mismatch in dimension for {quantity.name}\"\n"], "sample_764": ["def test_column_transformer_with_empty_list():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', Trans(), [])])\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[0][0] != 'remainder'\n", "def test_column_transformer_invalid_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    transformer_weights = {'trans1': .1, 'trans3': 10}  # 'trans3' is not defined\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           transformer_weights=transformer_weights)\n    with pytest.raises(KeyError):\n        ct.fit_transform(X_array)\n", "def test_column_transformer_drop_when_empty_columns():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([[0], [2]])\n\n    ct = ColumnTransformer([('trans', Trans(), [])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n    assert ct.transformers_[-1][1] == 'drop'\n    assert_array_equal(ct.transformers_[-1][2], [0])\n\n    ct = ColumnTransformer([('trans1', Trans(), []), ('trans2', Trans(), [1])])\n    assert_array_equal(ct.fit_transform(X_array), np.array([[1], [4]]))\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.array([[1], [4]]))\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_empty_transformers():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([], remainder='drop')\n    with pytest.raises(ValueError):\n        ct.fit_transform(X_array)\n    with pytest.raises(ValueError):\n        ct.fit(X_array).transform(X_array)\n", "def test_column_transformer_transformer_weights_invalid():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # Test that an error is raised if transformer_weights contains a key that is not in the transformers\n    ct = ColumnTransformer([('trans1', Trans(), [0])], transformer_weights={'trans2': 0.5})\n    with pytest.raises(ValueError):\n        ct.fit_transform(X_array)\n\n    # Test that an error is raised if transformer_weights contains a value that is not a number\n    ct = ColumnTransformer([('trans1', Trans(), [0])], transformer_weights={'trans1': 'invalid'})\n    with pytest.raises(TypeError):\n        ct.fit_transform(X_array)\n"], "sample_836": ["def test_ovr_decision_function_invalid_input():\n    # test that the function raises an error for invalid input\n    predictions = np.array([[0, 1, 1],\n                            [0, 1, 0],\n                            [0, 1, 1]])\n\n    confidences = np.array([[-1e16, 0, -1e16],\n                            [1., 2., -3.],\n                            [-5., 2., 5.]])\n\n    n_classes = 4\n\n    msg = \"n_classifiers must be n_classes * (n_classes - 1 ) / 2\"\n    with pytest.raises(ValueError, match=msg):\n        _ovr_decision_function(predictions, confidences, n_classes)\n", "def test_ovr_decision_function_invalid_inputs():\n    # Test _ovr_decision_function with invalid inputs\n    predictions = np.array([[0, 1], [0, 1]])  # Invalid number of classifiers\n    confidences = np.array([[0.5, 0.6], [0.7, 0.8]])\n    n_classes = 3\n\n    assert_raises(ValueError, _ovr_decision_function, predictions, confidences, n_classes)\n\n    predictions = np.array([[0, 1, 1], [0, 1, 0]])  # Invalid number of samples\n    confidences = np.array([[-1e16, 0, -1e16], [1., 2., -3.]])\n\n    assert_raises(ValueError, _ovr_decision_function, predictions, confidences, n_classes)\n", "def test_ovr_decision_function_shape():\n    # test that the output shape of _ovr_decision_function is correct\n\n    n_samples = 5\n    n_classes = 3\n    n_classifiers = n_classes * (n_classes - 1) // 2\n\n    predictions = np.random.randint(0, 2, size=(n_samples, n_classifiers))\n    confidences = np.random.randn(n_samples, n_classifiers)\n\n    dec_values = _ovr_decision_function(predictions, confidences, n_classes)\n\n    assert dec_values.shape == (n_samples, n_classes)\n", "def test_ovr_decision_function_invalid_input():\n    # test with invalid input\n    predictions = np.array([[0, 1], [0, 1, 0]])  # wrong shape\n    confidences = np.array([[-1e16, 0], [1., 2., -3.]])  # wrong shape\n    n_classes = 3\n\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes)\n\n    # test with non-binary predictions\n    predictions = np.array([[0, 1, 2], [1, 1, 0]])\n    confidences = np.array([[-1e16, 0, -1e16], [1., 2., -3.]])\n\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes)\n\n    # test with non-numerical predictions\n    predictions = np.array([['0', '1'], ['0', '1']])\n    confidences = np.array([[-1e16, 0], [1., 2.]])\n\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes)\n", "def test_type_of_target_with_empty_sequences():\n    # Test type_of_target with empty sequences\n    assert type_of_target([[]]) == 'unknown'\n    assert type_of_target([[], [1, 2]]) == 'unknown'\n    assert type_of_target(np.array([[], [1, 2]])) == 'unknown'\n"], "sample_560": ["def test_loc_validation_outside():\n    fig, ax = plt.subplots()\n    ax.legend(loc='outside upper right')\n    ax.legend(loc='outside lower left')\n    ax.legend(loc='outside center right')\n    ax.legend(loc='outside center left')\n    with pytest.raises(ValueError, match=\"'wrong' is not a valid value for\"):\n        ax.legend(loc='outside wrong')\n", "def test_loc_validation_outside_value():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.legend(loc='outside upper right')\n    ax.legend(loc='outside lower left')\n    ax.legend(loc='outside center right')\n    ax.legend(loc='outside left upper')\n    ax.legend(loc='outside right lower')\n    with pytest.raises(ValueError, match=\"'outside wrong' is not a valid value for loc\"):\n        ax.legend(loc='outside wrong')\n", "def test_loc_validation_outside():\n    fig = plt.figure()\n    ax = fig.add_subplot()\n    ax.legend(loc='outside upper left')\n    ax.legend(loc='outside upper right')\n    ax.legend(loc='outside lower left')\n    ax.legend(loc='outside lower right')\n    ax.legend(loc='outside right upper')\n    ax.legend(loc='outside right lower')\n    ax.legend(loc='outside left upper')\n    ax.legend(loc='outside left lower')\n    with pytest.raises(ValueError, match=\"'outside wrong' is not a valid value for loc\"):\n        ax.legend(loc='outside wrong')\n", "def test_loc_validation_outside_value():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=\"'outside' option for loc='right' keyword argument only works for figure legends\"):\n        ax.legend(loc='outside right')\n\n    fig.legend(loc='outside right')\n    fig.legend(loc='outside upper right')\n    fig.legend(loc='outside left upper')\n    fig.legend(loc='outside lower left')\n    with pytest.raises(ValueError, match=\"'outside' is not a valid value for loc\"):\n        fig.legend(loc='outside')\n", "def test_legend_multiple_handles_labels():\n    fig, ax = plt.subplots()\n    handles = [mlines.Line2D([0], [0], color='r', label='Red line'),\n               mlines.Line2D([0], [0], color='b', label='Blue line'),\n               mpatches.Patch(color='g', label='Green patch')]\n    labels = [handle.get_label() for handle in handles]\n    ax.legend(handles=handles, labels=labels)\n    legend_texts = [text.get_text() for text in ax.get_legend().get_texts()]\n    assert legend_texts == labels\n"], "sample_685": ["def test_log_level_override_with_invalid_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            with pytest.raises(pytest.UsageError):\n                caplog.set_level('INVALID_LEVEL')\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*'INVALID_LEVEL' is not recognized as a logging level name*\"]\n    )\n    assert result.ret == 1\n", "def test_log_report_captures_according_to_config_option_upon_success(testdir):\n    \"\"\" Test that upon success:\n    (1) `caplog` succeeded to capture the DEBUG message and assert on it => No `Exception` is raised\n    (2) The `DEBUG` message does NOT appear in the `Captured log call` report\n    (3) No messages appear in the test reports due to `--log-level=CRITICAL`\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug('DEBUG log ' + 'message')\n            logging.info('INFO log ' + 'message')\n            logging.warning('WARNING log ' + 'message')\n            print('Print ' + 'message')\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.CRITICAL\n\n            with caplog.at_level(logging.DEBUG):\n                function_that_logs()\n\n            if 'DEBUG log ' + 'message' not in caplog.text:\n                raise Exception('caplog failed to ' + 'capture DEBUG')\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-level=CRITICAL\")\n    result.stdout.no_fnmatch_line(\"*Exception: caplog failed to capture DEBUG*\")\n    result.stdout.no_fnmatch_line(\"*DEBUG log message*\")\n    result.stdout.no_fnmatch_line([\"*Print message*\", \"*INFO log message*\", \"*WARNING log message*\"])\n    assert result.ret == 0\n", "def test_log_file_path(testdir):\n    log_file = \"test.log\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            plugin.set_log_path('{}')\n            logger = logging.getLogger('filelog')\n            logger.info(\"INFO message to file\")\n    \"\"\".format(log_file)\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    with open(log_file, \"r\") as f:\n        assert \"INFO message to file\" in f.read()\n", "def test_caplog_records_for_each_stage(caplog, logging_during_setup_and_teardown):\n    # Test the get_records method for each test phase\n    assert len(caplog.get_records(\"setup\")) == 1\n    assert len(caplog.get_records(\"call\")) == 1\n    assert len(caplog.get_records(\"teardown\")) == 1\n\n    # Test the contents of the records for each test phase\n    assert caplog.get_records(\"setup\")[0].message == \"a_setup_log\"\n    assert caplog.get_records(\"call\")[0].message == \"a_call_log\"\n    assert caplog.get_records(\"teardown\")[0].message == \"a_teardown_log\"\n", "def test_caplog_with_warning_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logger = logging.getLogger('catchlog')\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.WARNING\n\n            logger.warning(\"WARNING message will be shown\")\n\n            with caplog.at_level(logging.INFO, logger.name):\n                logger.info(\"INFO message won't be shown\")\n\n            logger.info(\"INFO message will be shown\")\n\n            assert \"message won't be shown\" not in caplog.text\n            assert \"WARNING message will be shown\" in caplog.text\n            assert \"INFO message will be shown\" in caplog.text\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=WARNING\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n"], "sample_843": ["def test_kernel_operator_associativity(kernel):\n    # Test associativity of kernel operators.\n    if isinstance(kernel, KernelOperator):\n        k1, k2, k3 = kernel.k1, kernel.k2, kernel.k3\n        op = type(kernel)\n        assert_almost_equal((op(k1, op(k2, k3)))(X),\n                            (op(op(k1, k2), k3))(X))\n", "def test_kernel_bounds():\n    # Test that kernel.bounds returns consistent results.\n    kernel = RBF(length_scale=2.0)\n    bounds = kernel.bounds\n    assert_array_equal(bounds, np.log([[0.5, 10.0]]))\n\n    kernel = RBF(length_scale=[2.0, 3.0])\n    bounds = kernel.bounds\n    assert_array_equal(bounds, np.log([[0.5, 10.0], [0.5, 10.0]]))\n\n    kernel = RBF(length_scale=[2.0, 3.0], length_scale_bounds=(1.0, 5.0))\n    bounds = kernel.bounds\n    assert_array_equal(bounds, np.log([[1.0, 5.0], [1.0, 5.0]]))\n\n    kernel = RBF(length_scale=[2.0, 3.0], length_scale_bounds=[(1.0, 5.0), (2.0, 6.0)])\n    bounds = kernel.bounds\n    assert_array_equal(bounds, np.log([[1.0, 5.0], [2.0, 6.0]]))\n", "def test_kernel_theta_setter(kernel):\n    # Test that setting kernel.theta correctly updates the hyperparameters\n    theta = kernel.theta\n    new_theta = theta + np.log(2)\n    kernel.theta = new_theta\n    assert_array_almost_equal(kernel.theta, new_theta)\n    for i, hyperparameter in enumerate(kernel.hyperparameters):\n        if not hyperparameter.fixed:\n            assert_almost_equal(getattr(kernel, hyperparameter.name), np.exp(new_theta[i]))\n", "def test_exponentiation_kernel():\n    # Test that exponentiation of a kernel is consistent.\n    kernel = RBF(length_scale=2.0) ** 2\n    K1 = kernel(X)\n    K2 = RBF(length_scale=2.0)(X) ** 2\n    assert_almost_equal(K1, K2)\n\n    # Test that gradient of exponentiation of a kernel is consistent.\n    K, K_gradient = kernel(X, eval_gradient=True)\n    K2, K2_gradient = RBF(length_scale=2.0)(X, eval_gradient=True)\n    assert_almost_equal(K, K2 ** 2)\n    assert_almost_equal(K_gradient, 2 * K2[:, :, np.newaxis] * K2_gradient)\n", "def test_exponentiation_kernel():\n    # Test Exponentiation kernel\n    base_kernel = RBF(length_scale=2.0)\n    exp_kernel = Exponentiation(base_kernel, 2.0)\n\n    K_base = base_kernel(X)\n    K_exp = exp_kernel(X)\n    assert_almost_equal(K_exp, K_base**2)\n\n    # Check gradient\n    K_exp, K_gradient = exp_kernel(X, eval_gradient=True)\n    assert K_gradient.shape[0] == X.shape[0]\n    assert K_gradient.shape[1] == X.shape[0]\n    assert K_gradient.shape[2] == base_kernel.theta.shape[0] + 1  # +1 for the exponent\n\n        base_kernel_clone = base_kernel.clone_with_theta(theta_and_exponent[:-1])\n        exp_kernel_clone = Exponentiation(base_kernel_clone, np.exp(theta_and_exponent[-1]))\n        K = exp_kernel_clone(X, eval_gradient=False)\n        return K\n\n    K_gradient_approx = _approx_fprime(np.append(base_kernel.theta, np.log(2.0)), eval_kernel_for_theta_and_exponent, 1e-10)\n    assert_almost_equal(K_gradient, K_gradient_approx, 4)\n"], "sample_1158": ["def test_issue_21536_evaluate_mixed():\n    # test to check evaluate=False and evaluate=True in case of iterable input\n    u = sympify(\"x+3*x+2\", evaluate=False)\n    v = sympify(\"2*x+4*x+2+4\", evaluate=True)\n\n    assert u.is_Add and set(u.args) == {x, 3*x, 2}\n    assert v.is_Add and set(v.args) == {6*x, 6}\n    assert sympify([\"x+3*x+2\", \"2*x+4*x+2+4\"], evaluate=False) == [u, v]\n", "def test_issue_21536_with_numpy():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    # test to check evaluate=False in case of iterable input with numpy arrays\n    u = sympify(numpy.array([\"x+3*x+2\"]), evaluate=False)\n    v = sympify(numpy.array([\"2*x+4*x+2+4\"]), evaluate=False)\n\n    assert u[0].is_Add and set(u[0].args) == {x, 3*x, 2}\n    assert v[0].is_Add and set(v[0].args) == {2*x, 4*x, 2, 4}\n    assert sympify(numpy.array([\"x+3*x+2\", \"2*x+4*x+2+4\"]), evaluate=False) == [u[0], v[0]]\n\n    # test to check evaluate=True in case of iterable input with numpy arrays\n    u = sympify(numpy.array([\"x+3*x+2\"]), evaluate=True)\n    v = sympify(numpy.array([\"2*x+4*x+2+4\"]), evaluate=True)\n\n    assert u[0].is_Add and set(u[0].args) == {4*x, 2}\n    assert v[0].is_Add and set(v[0].args) == {6*x, 6}\n    assert sympify(numpy.array([\"x+3*x+2\", \"2*x+4*x+2+4\"]), evaluate=True) == [u[0], v[0]]\n\n    # test to check evaluate with no input in case of iterable input with numpy arrays\n    u = sympify(numpy.array([\"x+3*x+2\"]))\n    v = sympify(numpy.array([\"2*x+4*x+2+4\"]))\n\n    assert u[0].is_Add and set(u[0].args) == {4*x, 2}\n   ", "def test_issue_21537():\n    # Test for the case when evaluate=False and convert_xor=True\n    u = sympify(\"x^3*x+2\", evaluate=False, convert_xor=True)\n    v = sympify(\"2*x^4*x+2+4\", evaluate=False, convert_xor=True)\n\n    assert u.is_Add and set(u.args) == {x**4, 2}\n    assert v.is_Add and set(v.args) == {x**5, 2, 4}\n    assert sympify([\"x^3*x+2\", \"2*x^4*x+2+4\"], evaluate=False, convert_xor=True) == [u, v]\n\n    # Test for the case when evaluate=True and convert_xor=True\n    u = sympify(\"x^3*x+2\", evaluate=True, convert_xor=True)\n    v = sympify(\"2*x^4*x+2+4\", evaluate=True, convert_xor=True)\n\n    assert u.is_Add and set(u.args) == {x**4, 2}\n    assert v.is_Add and set(v.args) == {x**5, 6}\n    assert sympify([\"x^3*x+2\", \"2*x^4*x+2+4\"], evaluate=True, convert_xor=True) == [u, v]\n", "def test_issue_22017():\n    # Test for complex numbers with sympify\n    complex_num = sympify(\"1 + 2j\")\n    assert complex_num == S(1) + S(2)*I\n    assert isinstance(complex_num, Add)\n    assert isinstance(complex_num.args[1], Mul)\n    assert complex_num.args[1].args[0] == 2\n    assert complex_num.args[1].args[1] == I\n", "def test_sympify_numpy_arrays():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    a = numpy.array([1, 2, 3])\n    a_sym = sympify(a)\n    assert isinstance(a_sym, ImmutableDenseNDimArray)\n    assert a_sym.shape == (3,)\n    assert a_sym[0] == 1\n    assert a_sym[1] == 2\n    assert a_sym[2] == 3\n\n    b = numpy.array([[1, 2], [3, 4]])\n    b_sym = sympify(b)\n    assert isinstance(b_sym, ImmutableDenseNDimArray)\n    assert b_sym.shape == (2, 2)\n    assert b_sym[0, 0] == 1\n    assert b_sym[0, 1] == 2\n    assert b_sym[1, 0] == 3\n    assert b_sym[1, 1] == 4\n"], "sample_587": ["def test_merge_override(self):\n    ds1 = xr.Dataset({\"x\": 0})\n    ds2 = xr.Dataset({\"x\": 1})\n    expected = xr.Dataset({\"x\": 1})\n    assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"override\"))\n", "def test_merge_override(self):\n    ds1 = xr.Dataset({\"x\": 0})\n    ds2 = xr.Dataset({\"x\": 1})\n    expected = xr.Dataset({\"x\": 1})\n    assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"override\"))\n", "def test_merge_override(self):\n    ds1 = xr.Dataset({\"x\": (\"y\", [1, 2])})\n    ds2 = xr.Dataset({\"x\": (\"y\", [3, 4])})\n    expected = xr.Dataset({\"x\": (\"y\", [3, 4])})\n    assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"override\"))\n", "    def test_merge_override(self):\n        ds1 = xr.Dataset({\"x\": 0})\n        ds2 = xr.Dataset({\"x\": 1})\n        expected = xr.Dataset({\"x\": 1})\n        assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n        assert expected.identical(ds2.merge(ds1, compat=\"override\"))\n", "def test_merge_explicit_coords(self):\n    data = create_test_data()\n    ds1 = data[[\"var1\"]]\n    ds2 = data[[\"var3\"]]\n    expected = data[[\"var1\", \"var3\"]].set_coords(\"var3\")\n    actual = ds1.merge(ds2, compat=\"no_conflicts\", join=\"outer\", explicit_coords=[\"var3\"])\n    assert expected.identical(actual)\n"], "sample_970": ["def test_isabstractmethod():\n    from abc import ABC, abstractmethod\n\n    class Foo(ABC):\n        @abstractmethod\n            pass\n\n    class Bar(Foo):\n            pass\n\n    class Baz(Foo):\n        pass\n\n    assert inspect.isabstractmethod(Foo.meth) is True\n    assert inspect.isabstractmethod(Bar.meth) is False\n    assert inspect.isabstractmethod(Baz.meth) is True\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @func.register(int)\n        pass\n\n    assert inspect.is_singledispatch_function(func) is True\n    assert inspect.is_singledispatch_function(_) is False\n\n        pass\n\n    assert inspect.is_singledispatch_function(not_singledispatch) is False\n", "def test_isabstractmethod(app):\n    from target.methods import Base, Inherited, AbstractBase\n\n    assert inspect.isabstractmethod(AbstractBase.abstractmeth) is True\n    assert inspect.isabstractmethod(Base.meth) is False\n    assert inspect.isabstractmethod(Inherited.abstractmeth) is False\n    assert inspect.isabstractmethod(Inherited.meth) is False\n", "def test_signature_type_aliases(app):\n    from target.type_aliases import MyDict, func\n\n    sig = inspect.signature(func, type_aliases={'MyDict': 'dict'})\n    assert stringify_signature(sig) == '(x: dict) -> None'\n", "def test_isabstractmethod(app):\n    from target.methods import Base\n    from abc import ABC, abstractmethod\n\n    assert inspect.isabstractmethod(abstractmethod(lambda: None)) is True\n    assert inspect.isabstractmethod(Base.abstractmeth) is True\n    assert inspect.isabstractmethod(Base().abstractmeth) is False\n    assert inspect.isabstractmethod(ABC.register(int)) is False\n    assert inspect.isabstractmethod(Base.meth) is False\n"], "sample_150": ["def test_command_check_called(self, mocked_check):\n    from django.core.management.base import BaseCommand\n    command = BaseCommand()\n    command.requires_system_checks = True\n    command.execute()\n    self.assertTrue(mocked_check.called)\n", "def test_check_call_in_execute(self, mocked_check):\n    command = BaseCommand()\n    command.requires_system_checks = True\n    command.execute(skip_checks=False)\n    self.assertTrue(mocked_check.called)\n    command.execute(skip_checks=True)\n    self.assertFalse(mocked_check.called)\n", "def test_command_check_called(self, mocked_check):\n    command = BaseCommand()\n    command.requires_system_checks = True\n    command.execute()\n    self.assertTrue(mocked_check.called)\n", "def test_postgresql_integer_datatype(self):\n    good_datatypes = ['bigint', 'integer', 'smallint']\n    for datatype in good_datatypes:\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=(datatype,)\n        ):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n    bad_datatypes = ['int', 'mediumint', 'serial']\n    for datatype in bad_datatypes:\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=(datatype,)\n        ):\n            # One warning for each database alias\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n", "def test_postgresql_check_database_backends(self):\n    with mock.patch('django.db.backends.utils.CursorWrapper.fetchone') as mock_fetchone:\n        # Simulate no search_path set\n        mock_fetchone.return_value = (None,)\n        result = check_database_backends(databases=self.databases)\n        # One warning for each database alias\n        self.assertEqual(len(result), 2)\n        self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n\n        # Simulate search_path set to public\n        mock_fetchone.return_value = ('public',)\n        self.assertEqual(check_database_backends(databases=self.databases), [])\n\n        # Simulate search_path set to something else\n        mock_fetchone.return_value = ('something_else',)\n        result = check_database_backends(databases=self.databases)\n        # One warning for each database alias\n        self.assertEqual(len(result), 2)\n        self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n"], "sample_972": ["def test_restify_type_hints_user_defined_generic():\n    class UserDefinedGeneric(Generic[T]):\n        pass\n\n    assert restify(UserDefinedGeneric) == \":py:class:`tests.test_util_typing.UserDefinedGeneric`\"\n    assert restify(UserDefinedGeneric[int]) == \":py:class:`tests.test_util_typing.UserDefinedGeneric`\\\\ [:py:class:`int`]\"\n", "def test_restify_type_instantiated_generic():\n    from typing import Dict, List\n    assert restify(Dict[str, List[int]]) == (\":py:class:`~typing.Dict`\\\\ \"\n                                             \"[:py:class:`str`, :py:class:`~typing.List`\\\\ \"\n                                             \"[:py:class:`int`]]\")\n", "def test_restify_type_instantiated_generic():\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert restify(MyGeneric[int]) == \":py:class:`tests.test_util_typing.MyGeneric`\\\\ [:py:class:`int`]\"\n    assert restify(MyGeneric[str]) == \":py:class:`tests.test_util_typing.MyGeneric`\\\\ [:py:class:`str`]\"\n\n    class MyGeneric2(Generic[T, U]):\n        pass\n\n    assert restify(MyGeneric2[int, str]) == \":py:class:`tests.test_util_typing.MyGeneric2`\\\\ [:py:class:`int`, :py:class:`str`]\"\n", "def test_restify_type_hints_Annotated():\n    from typing import Annotated  # type: ignore\n    assert restify(Annotated[str, \"foo\", \"bar\"]) == \":py:class:`str`\"  # NOQA\n", "def test_restify_type_hints_union_operator():\n    from typing import Union  # type: ignore\n    assert restify(int | None) == \":py:class:`int` | :py:obj:`None`\"  # type: ignore\n    assert restify(int | str) == \":py:class:`int` | :py:class:`str`\"  # type: ignore\n    assert restify(int | str | None) == (\":py:class:`int` | :py:class:`str` | \"  # type: ignore\n                                         \":py:obj:`None`\")\n\n    # Additional test cases\n    assert restify(Union[int, str] | None) == (\":py:obj:`~typing.Optional`\\\\ \"  # type: ignore\n                                               \"[Union[int, str]]\")\n    assert restify(None | Union[int, str]) == (\":py:obj:`~typing.Optional`\\\\ \"  # type: ignore\n                                               \"[Union[int, str]]\")\n"], "sample_1105": ["def test_matmul_commutative_scalars():\n    a, b = symbols('a b', commutative=True)\n    assert MatMul(a, b, A, A.T) == MatMul(b, a, A, A.T)\n", "def test_noncommutative_scalars():\n    a, b = symbols('a b', commutative=False)\n    try:\n        MatMul(a, A, b)\n    except NotImplementedError:\n        pass\n    else:\n        assert False, \"Noncommutative scalars in MatMul should raise NotImplementedError\"\n", "def test_matmul_noncommutative_scalars():\n    a, b = symbols('a b', commutative=False)\n    assert MatMul(a, b, evaluate=False).args == (a, b)\n    assert MatMul(a, b, evaluate=False).doit() == a*b\n    assert MatMul(a, b, A, evaluate=False).args == (a, b, A)\n    assert MatMul(a, b, A, evaluate=False).doit() == a*b*A\n", "def test_matmul_multiplication_with_different_shape_matrices():\n    X = MatrixSymbol('X', 2, 3)\n    Y = MatrixSymbol('Y', 4, 2)\n    try:\n        MatMul(X, Y)\n    except ShapeError:\n        pass\n    else:\n        assert False, \"Expected ShapeError\"\n", "def test_matmul_different_shapes():\n    A = MatrixSymbol('A', 3, 2)\n    B = MatrixSymbol('B', 2, 3)\n    C = MatrixSymbol('C', 3, 4)\n\n    # These should raise a ShapeError\n    with pytest.raises(ShapeError):\n        MatMul(A, B)\n\n    with pytest.raises(ShapeError):\n        MatMul(A, C)\n\n    with pytest.raises(ShapeError):\n        MatMul(A, B, C)\n"], "sample_916": ["def test_initializer_lists():\n    idsFunction = {1: 'f__T.T', 2: '1f1T1T'}\n    idsTemplate = {2: 'I_1TE_1TE1fv', 4: 'I_1TE_1TE1fvv'}\n    # with '=', initializer-list\n    check('function', 'void f(T v = {42, 42, 42})', idsFunction)\n    check('function', 'template<T v = {42, 42, 42}> void f()', idsTemplate)\n    check('function', 'void f(T v = {42, 42, 42,})', idsFunction)\n    check('function', 'template<T v = {42, 42, 42,}> void f()', idsTemplate)\n    check('function', 'void f(T v = {42, 42, args...})', idsFunction)\n    check('function', 'template<T v = {42, 42, args...}> void f()', idsTemplate)\n", "def test_xref_consistency_cpp_texpr(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_multi_declaration():\n    check('class', 'class A { public: int a; private: int b; };',\n          {1: 'A', 2: '1A'})\n    check('function', 'int f(); int g();',\n          {1: 'f', 2: '1fv'})\n    check('member', 'int a; int b;',\n          {1: 'a__i', 2: '1a'})\n    check('member', 'int a; int b; int c;',\n          {1: 'a__i', 2: '1a'})\n", "def test_template_default_args():\n    check('function', 'template<typename T, T value = {}> void f()',\n          {2: 'I0_1TE1fv', 4: 'I0_1TE1fvv'})\n    check('function', 'template<typename T, T value = {42}> void f()',\n          {2: 'I0_1TE1fv', 4: 'I0_1TE1fvv'})\n    check('function', 'template<typename T, T value = {42, 42}> void f()',\n          {2: 'I0_1TE1fv', 4: 'I0_1TE1fvv'})\n", "def test_namespace_definitions():\n    check('namespace', 'A', {2: \"N1A\"})\n    check('namespace', 'A::B::C', {2: \"N1A1B1C\"})\n    check('namespace', '::A::B::C', {2: \"N1A1B1C\"})\n"], "sample_320": ["def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\",\n            models.CASCADE,\n            limit_choices_to={\"field\": models.F(\"other_field\")},\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"other_field\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"field\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"other_field\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "    def test_references_field_by_through_fields_reverse(self):\n        operation = FieldOperation(\n            \"Other\",\n            \"field\",\n            models.ManyToManyField(\n                \"Model\", reverse=\"reverse_field\", through=\"Through\", through_fields=(\"first\", \"second\")\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Model\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"first\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"second\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Model\", \"reverse_field\", \"migrations\"), True\n        )\n", "    def test_references_model_base_model(self):\n        operation = migrations.CreateModel(\n            \"name\",\n            fields=[],\n            options={\"base_manager_name\": \"custom_manager\"},\n        )\n        self.assertIs(operation.references_model(\"custom_manager\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"missing_manager\", \"migrations\"), False)\n", "    def test_references_model(self):\n        operation = migrations.AlterField(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n        )\n        # Model name match.\n        self.assertIs(operation.references_model(\"mOdEl\", \"migrations\"), True)\n        # Referenced field.\n        self.assertIs(operation.references_model(\"oTher\", \"migrations\"), True)\n        # Doesn't reference.\n        self.assertIs(operation.references_model(\"Whatever\", \"migrations\"), False)\n", "def test_rename_index_referenced_by_unique_together(self):\n    \"\"\"\n    Tests the RenameIndex operation when the index is referenced by\n    unique_together.\n    \"\"\"\n    app_label = \"test_rninrefbyunto\"\n    project_state = self.set_up_test_model(app_label, unique_together=True)\n    table_name = app_label + \"_pony\"\n    self.assertIndexNameExists(table_name, \"pony_pink_weight_idx\")\n    self.assertIndexNameNotExists(table_name, \"new_pony_test_idx\")\n    operation = migrations.RenameIndex(\n        \"Pony\", new_name=\"new_pony_test_idx\", old_name=\"pony_pink_weight_idx\"\n    )\n    self.assertEqual(\n        operation.describe(),\n        \"Rename index pony_pink_weight_idx on Pony to new_pony_test_idx\",\n    )\n    self.assertEqual(\n        operation.migration_name_fragment,\n        \"rename_pony_pink_weight_idx_new_pony_test_idx\",\n    )\n\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    # Rename index.\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertIndexNameNotExists(table_name, \"pony_pink_weight_idx\")\n    self.assertIndexNameExists(table_name, \"new_pony_test_idx\")\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    self.assertIndexNameExists(table_name, \"pony_pink_weight_idx\")\n    self.assertIndexNameNotExists(table_name, \"new_pony_test_idx\")\n    # Deconstruction.\n    definition = operation.deconstruct"], "sample_1157": ["def test_issue_19501_with_functions():\n    x = Symbol('x')\n    f = Function('f')\n    eq = parse_expr('f(x)(1+x)', local_dict={'x': x, 'f': f}, transformations=(\n        standard_transformations +\n        (implicit_multiplication_application,)))\n    assert eq.free_symbols == {x}\n", "def test_lambda_notation():\n    x = Symbol('x')\n    f = parse_expr(\"lambda x: x + 1\", transformations=(standard_transformations + (lambda_notation,)))\n    assert f(x) == x + 1\n\n    g = parse_expr(\"lambda: x + 1\", transformations=(standard_transformations + (lambda_notation,)))\n    assert g() == x + 1\n\n    h = parse_expr(\"lambda x, y: x + y\", transformations=(standard_transformations + (lambda_notation,)))\n    assert h(x, 2) == x + 2\n", "def test_parse_expr_with_different_evaluate_values():\n    x = Symbol('x')\n    # evaluate=True (default)\n    expr1 = parse_expr('sin(x) + cos(x)')\n    assert expr1 == sin(x) + cos(x)\n    # evaluate=False\n    expr2 = parse_expr('sin(x) + cos(x)', evaluate=False)\n    assert str(expr2) == 'Add(sin(x), cos(x))'\n", "def test_issue_19501_implicit_multiplication_application():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    eq = parse_expr('E x(1+x)', local_dict={'x': x}, transformations=transformations)\n    assert eq == exp(x)*(1+x)\n", "def test_split_symbols_function_exponent():\n    transformations = standard_transformations + \\\n                      (split_symbols, implicit_multiplication, function_exponentiation)\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Symbol('a')\n    f = Function('f')\n\n    assert parse_expr(\"ay^2(x+1)\", transformations=transformations) == a*y**2*(x+1)\n    assert parse_expr(\"af^y(x+1)\", transformations=transformations, local_dict={'f':f}) == a*f(x+1)**y\n"], "sample_947": ["def test_member_decl_specs():\n    # test decl specs on left\n    check('member', \"auto int a\", {1: 'a'})\n    check('member', \"register int a\", {1: 'a'})\n    check('member', \"extern int a\", {1: 'a'})\n    check('member', \"static int a\", {1: 'a'})\n    check('member', \"thread_local int a\", {1: 'a'})\n    check('member', \"_Thread_local int a\", {1: 'a'})\n    check('member', \"extern thread_local int a\", {1: 'a'})\n    check('member', \"thread_local extern int a\", {1: 'a'}, \"extern thread_local int a\")\n    check('member', \"static thread_local int a\", {1: 'a'})\n    check('member', \"thread_local static int a\", {1: 'a'}, \"static thread_local int a\")\n", "def test_build_domain_c_enum_dup(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"enum-dup\")\n    assert len(ws) == 2\n    assert \"WARNING: Duplicate C declaration, also defined at index.rst:48.\" in ws[0]\n    assert \"WARNING: Duplicate C declaration, also defined at index.rst:49.\" in ws[1]\n", "def test_function_pointer_params():\n    check('function', 'void f(void (*func)(int))', {1: 'f'})\n    check('function', 'void f(void (*func[10])(int))', {1: 'f'})\n    check('function', 'void f(void (*(*func)(int))(int))', {1: 'f'})\n    check('function', 'void f(void (*func(int))(int))', {1: 'f'})\n    check('function', 'void f(void (*func(int, int))(int))', {1: 'f'})\n", "def test_build_domain_c_anon_dup_decl(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"anon-dup-decl\")\n    assert len(ws) == 2\n    assert \"WARNING: c:identifier reference target not found: @a\" in ws[0]\n    assert \"WARNING: c:identifier reference target not found: @b\" in ws[1]\n", "def test_build_c_expr_roles(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"c_expr_roles\")\n    assert len(ws) == 0\n\n    # Check that :c:expr: roles are rendered correctly\n    with open(app.outdir / \"c_expr_roles.html\") as f:\n        html = f.read()\n        assert '<span class=\"c c-expr\">int*</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">***</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">*</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">*</span><span class=\"c c-expr\">**</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">*</span><span class=\"c c-expr\">const</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">*</span><span class=\"c c-expr\">volatile</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">*</span><span class=\"c c-expr\">restrict</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">*</span><span class=\"c c-expr\">volatile</span><span class=\"c c-expr\">const</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">*</span><span class=\"c c-expr\">restrict</span><span class=\"c c-expr\">const</span>' in html\n        assert '<span class=\"c c-expr\">int</span><span class=\"c c-expr\">*</span><span class=\"c c-expr\">restrict</span><span class=\"c c"], "sample_874": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n", "def test_step_selector_with_feature_names():\n    sel = StepSelector(step=2)\n    sel.fit(X, y, feature_names=feature_names)\n    Xt_actual = sel.transform(X)\n    feature_names_t_actual = sel.transform(feature_names)\n\n    assert_array_equal(Xt, Xt_actual)\n    assert_array_equal(feature_names_t, feature_names_t_actual)\n", "def test_fit_transform_with_y():\n    sel = StepSelector()\n    Xt_actual = sel.fit(X, y).transform(X)\n    Xt_actual2 = sel.fit_transform(X, y)\n    assert_array_equal(Xt, Xt_actual)\n    assert_array_equal(Xt, Xt_actual2)\n\n    # Check if y is not None in fit\n    with pytest.raises(TypeError):\n        sel.fit(X)\n", "def test_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    feature_names_out = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out)\n\n    # Test without input_features\n    feature_names_out_default = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_default)\n\n    # Test with input_features not matching fit\n    with pytest.raises(ValueError, match=\"feature_names mismatch\"):\n        sel.get_feature_names_out(input_features=[\"A\", \"B\", \"C\"])\n"], "sample_1005": ["def test_MatrixProduct_printing():\n    from sympy.tensor.functions import MatrixProduct\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert latex(MatrixProduct(A, B)) == r\"A B\"\n", "def test_Quaternion_latex_printing_with_functions():\n    q = Quaternion(sin(x), cos(y), tan(z), cot(t))\n    assert latex(q) == r\"\\sin{\\left (x \\right )} + \\cos{\\left (y \\right )} i + \\tan{\\left (z \\right )} j + \\cot{\\left (t \\right )} k\"\n", "def test_Quaternion_division_printing():\n    q1 = Quaternion(x, y, z, t)\n    q2 = Quaternion(a, b, c, d)\n    assert latex(q1 / q2) == r\"\\frac{x a + y b + z c + t d}{a^2 + b^2 + c^2 + d^2} - \\frac{x b - y a - z d + t c}{a^2 + b^2 + c^2 + d^2} i - \\frac{x c + y d - z a + t b}{a^2 + b^2 + c^2 + d^2} j - \\frac{x d - y c + z b - t a}{a^2 + b^2 + c^2 + d^2} k\"\n", "def test_TensorProduct_printing_evaluate():\n    from sympy.tensor.functions import TensorProduct\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    assert latex(TensorProduct(A, B, evaluate=False)) == r\"A \\otimes B\"\n    assert latex(TensorProduct(A, B, evaluate=True)) == r\"\\left[\\begin{matrix}5 & 6\\\\7 & 8\\\\10 & 12\\\\14 & 16\\end{matrix}\\right]\"\n", "def test_issue_14344():\n    expr = Sum(1/k, (k, 1, oo))\n    assert latex(expr) == r\"\\sum_{k=1}^{\\infty} \\frac{1}{k}\"\n"], "sample_1153": ["def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', complex=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == DiracDelta(f(x))*f(x)/Abs(f(x))\n", "def test_polarify_with_lift():\n    x = Symbol('x', real=True)\n    eq = sqrt(1 - x**2)\n    polarified, subs = polarify(eq, lift=True)\n    assert unpolarify(polarified, subs) == eq\n", "def test_polar_lift():\n    from sympy import polar_lift, Symbol, I, pi, exp_polar, Abs\n    x = Symbol('x')\n    p = Symbol('p', polar=True)\n    neg = Symbol('x', negative=True)\n\n    assert polar_lift(2) == 2*exp_polar(0)\n    assert polar_lift(-2) == 2*exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(p) == p\n    assert polar_lift(2*x) == 2*polar_lift(x)\n    assert polar_lift(neg) == Abs(neg)*exp_polar(I*pi)\n", "def test_issue_15963():\n    x = Symbol('x', complex=True)\n    f = Function('f')\n    assert conjugate(f(x)).diff(x) == conjugate(f(x).diff(x))\n    assert conjugate(f(x)).diff(x, x) == conjugate(f(x).diff(x, x))\n    assert conjugate(f(x)).diff(x, 2) == conjugate(f(x).diff(x, 2))\n", "def test_Abs_derivative():\n    x = Symbol('x')\n    assert Abs(x).diff(x) == sign(x)\n    assert Abs(x**2).diff(x) == 2*x\n\n    z = Symbol('z', complex=True)\n    assert Abs(z).diff(z) == z / Abs(z)\n\n    p = Symbol('p', positive=True)\n    assert Abs(p).diff(p) == 1\n\n    n = Symbol('n', negative=True)\n    assert Abs(n).diff(n) == -1\n"], "sample_924": ["def test_template_specialization():\n    check('class', 'template<> {key}A<int>', {2: 'IE1AIiE'})\n    check('function', 'template<> void f<int>()', {2: 'IE1fIiEE'})\n    check('member', 'template<> A<int> a', {2: 'IE1aIiE'})\n    check('type', 'template<> {key}a<int>', {2: 'IE1aIiE'}, key='using')\n", "def test_xref_consistency_cpp_expr(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_template_arg_deduction():\n    check('type', 'template<typename T> {key}A = std::vector<T>',\n          {2: 'I0E1A'},\n          key='using')\n    check('type', 'template<typename T> {key}A = std::vector<T, std::allocator<T>>',\n          {2: 'I0E1A'},\n          key='using')\n    check('type', 'template<typename T, typename U> {key}A = std::pair<T, U>',\n          {2: 'I00E1A'},\n          key='using')\n", "def test_xref_invalid_types(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_invalid_types.html'\n    output = (app.outdir / test).read_text()\n\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_xref_resolution(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_resolution.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = r'{role}-role:.*?<a .*?href=[\"\\'](.*?)[\"\\'].*?>.*?</a>'.format(role=role)\n        return re.findall(pattern, output)\n\n    any_links = get_links('any')\n    cpp_any_links = get_links('cpp-any')\n    expr_links = get_links('cpp-expr')\n    texpr_links = get_links('cpp-texpr')\n\n    # any and cpp:any should resolve to the same link\n    expect = 'any and cpp:any resolve to the same link'\n    assert any_links == cpp_any_links, expect\n\n    # cpp:expr and cpp:texpr should resolve to the same link\n    expect = 'cpp:expr and cpp:texpr resolve to the same link'\n    assert expr_links == texpr_links, expect\n\n    # all links should resolve to the correct page\n    expect = 'all links resolve to the correct page'\n    for link in any_links + expr_links + texpr_links:\n        assert link.startswith(app.config.link_prefix), expect\n\n    # NYI: test that the links resolve to the correct part of the page\n"], "sample_308": ["    def test_twenty_four_hour_format(self):\n        tests = [\n            (0, '00'),\n            (1, '01'),\n            (11, '11'),\n            (12, '12'),\n            (13, '13'),\n            (23, '23'),\n        ]\n        for hour, expected in tests:\n            with self.subTest(hour=hour):\n                self.assertEqual(\n                    dateformat.format(datetime(2000, 1, 1, hour), 'G'),\n                    expected,\n                )\n", "def test_time_formats_with_timezone(self):\n    dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=utc)\n    self.assertEqual(dateformat.format(dt, 'H'), '05')\n    self.assertEqual(dateformat.format(dt, 'i'), '30')\n    self.assertEqual(dateformat.format(dt, 'O'), '+0000')\n    self.assertEqual(dateformat.format(dt, 'T'), 'UTC')\n", "def test_twenty_four_hour_format(self):\n    tests = [\n        (0, '00'),\n        (1, '01'),\n        (11, '11'),\n        (12, '12'),\n        (13, '13'),\n        (23, '23'),\n    ]\n    for hour, expected in tests:\n        with self.subTest(hour=hour):\n            self.assertEqual(\n                dateformat.format(datetime(2000, 1, 1, hour), 'G'),\n                expected,\n            )\n", "def test_iso8601_year_number(self):\n    tests = [\n        (datetime(2005, 1, 1), '2004'),  # New Year's Day is on a Friday\n        (datetime(2005, 1, 2), '2005'),  # Saturday\n        (datetime(2005, 1, 3), '2005'),  # Sunday\n        (datetime(2005, 1, 4), '2005'),  # Monday\n        (datetime(2005, 1, 5), '2005'),  # Tuesday\n        (datetime(2005, 1, 6), '2005'),  # Wednesday\n        (datetime(2005, 1, 7), '2005'),  # Thursday\n        (datetime(2005, 12, 31), '2005'),  # Last day of the year\n    ]\n    for dt, expected_year in tests:\n        with self.subTest(date=dt):\n            self.assertEqual(\n                dateformat.format(dt, 'o'),\n                expected_year,\n            )\n", "    def test_timezone_transition(self):\n        # Test the behavior around timezone transitions, which can cause ambiguity or non-existent times.\n        # This datetime is just before the DST transition in Europe/Copenhagen.\n        dt = datetime(2015, 10, 25, 1, 59, 59)\n\n        # Try all formatters that involve self.timezone.\n        self.assertEqual(format(dt, 'I'), '0')\n        self.assertEqual(format(dt, 'O'), '+0200')\n        self.assertEqual(format(dt, 'T'), 'CEST')\n        self.assertEqual(format(dt, 'Z'), '7200')\n\n        # This datetime doesn't exist in Europe/Copenhagen because it skips forward an hour.\n        dt = datetime(2015, 10, 25, 2, 30, 0)\n\n        # Try all formatters that involve self.timezone.\n        self.assertEqual(format(dt, 'I'), '')\n        self.assertEqual(format(dt, 'O'), '')\n        self.assertEqual(format(dt, 'T'), '')\n        self.assertEqual(format(dt, 'Z'), '')\n"], "sample_232": ["def test_key_lt_gt_lte_gte(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lt=15),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gt=13),\n        [self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lte=14),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__gte=14),\n        [self.objs[3], self.objs[4]],\n    )\n", "def test_key_transform_exact_query(self):\n    obj = NullableJSONModel.objects.create(value={'a': 1, 'b': 'test', 'c': True})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__exact=1),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__b__exact='test'),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__exact=True),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__exact=None),\n        [],\n    )\n", "def test_key_transform_lt_gt_gte_lte(self):\n    tests = [\n        ('value__c__lt', 15, self.objs[3:5]),\n        ('value__c__lt', 14, []),\n        ('value__c__gt', 13, [self.objs[3]]),\n        ('value__c__gt', 14, []),\n        ('value__c__gte', 14, self.objs[3:5]),\n        ('value__c__lte', 14, self.objs[3:5]),\n        ('value__c__lt', 14.5, self.objs[3:5]),\n        ('value__c__gt', 13.5, [self.objs[3]]),\n        ('value__c__gte', 14.5, []),\n        ('value__c__lte', 13.5, []),\n    ]\n    for lookup, value, expected in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(**{lookup: value}),\n                expected,\n            )\n", "def test_key_transform_with_integer_key(self):\n    obj = NullableJSONModel.objects.create(value={0: 'test'})\n    self.assertIs(NullableJSONModel.objects.filter(value__0='test').exists(), True)\n", "def test_key_transform_numeric_lookups(self):\n    tests = [\n        ('value__c__lt', 20),\n        ('value__c__lte', 20),\n        ('value__c__gt', 10),\n        ('value__c__gte', 10),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup, value=value):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(**{lookup: value}),\n                [self.objs[3], self.objs[4]],\n            )\n"], "sample_610": ["def test_cftimeindex_freq_property(calendar):\n    index = xr.cftime_range(start=\"2000\", periods=3, freq=\"D\", calendar=calendar)\n    assert index.freq == \"D\"\n", "def test_cftimeindex_constructor_from_list():\n    date_type = cftime.DatetimeNoLeap\n    dates = [date_type(1, 1, 1), date_type(1, 2, 1), date_type(2, 1, 1)]\n    index = CFTimeIndex(dates)\n    assert index.date_type == date_type\n    assert_array_equal(index, dates)\n", "def test_infer_freq_non_fixed_freq(freq):\n    indx = xr.cftime_range(\"2000-01-01\", periods=3, freq=freq)\n    with pytest.raises(ValueError, match=\"Cannot infer the frequency\"):\n        xr.infer_freq(indx)\n", "def test_cftimeindex_infer_freq():\n    index = xr.cftime_range(start=\"2000\", periods=3, freq=\"D\")\n    assert index.freq == \"D\"\n\n    index = xr.cftime_range(start=\"2000\", periods=3, freq=\"M\")\n    assert index.freq == \"M\"\n\n    index = xr.cftime_range(start=\"2000\", periods=2, freq=\"YS\")\n    assert index.freq == \"YS\"\n\n    index = xr.cftime_range(start=\"2000\", periods=2, freq=\"A-DEC\")\n    assert index.freq == \"A-DEC\"\n\n    index = xr.cftime_range(start=\"2000\", periods=2, freq=\"Q-NOV\")\n    assert index.freq == \"Q-NOV\"\n", "def test_cftimeindex_constructor():\n    import cftime\n\n    dates = [\n        cftime.DatetimeNoLeap(1, 1, 1),\n        cftime.DatetimeNoLeap(1, 2, 1),\n        cftime.DatetimeNoLeap(2, 1, 1),\n        cftime.DatetimeNoLeap(2, 2, 1),\n    ]\n    index = CFTimeIndex(dates)\n    assert isinstance(index, CFTimeIndex)\n    assert index.equals(CFTimeIndex(dates))\n\n    # Test with a non-datetime object\n    with pytest.raises(TypeError):\n        CFTimeIndex(np.array([1, 2, 3, 4]))\n"], "sample_455": ["def test_immediate_database_constraint_after_deferred(self):\n    obj_1 = UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"front\")\n    obj_2 = UniqueConstraintDeferrable.objects.create(name=\"p2\", shelf=\"back\")\n\n    # Initially deferred\n    with connection.cursor() as cursor:\n        constraint_name = connection.ops.quote_name(\"name_init_deferred_uniq\")\n        cursor.execute(\"SET CONSTRAINTS %s DEFERRED\" % constraint_name)\n        obj_1.name, obj_2.name = obj_2.name, obj_1.name\n        obj_1.save()\n        obj_2.save()\n\n    # Change to immediate\n    with connection.cursor() as cursor:\n        constraint_name = connection.ops.quote_name(\"name_init_deferred_uniq\")\n        cursor.execute(\"SET CONSTRAINTS %s IMMEDIATE\" % constraint_name)\n        with self.assertRaises(IntegrityError):\n            obj_1.save()\n", "def test_validate_include(self):\n    p1 = UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\", size=\"M\")\n    constraint = UniqueConstraintInclude._meta.constraints[0]\n    msg = \"Unique constraint include with this Name and Color already exists.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintInclude,\n            UniqueConstraintInclude(name=p1.name, color=p1.color, size=\"L\"),\n        )\n    # Values in 'include' fields are ignored.\n    constraint.validate(\n        UniqueConstraintInclude,\n        UniqueConstraintInclude(name=p1.name, color=p1.color, size=\"XL\"),\n    )\n", "def test_validate_expression_with_custom_error(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    constraint.violation_error_message = \"Custom message\"\n    constraint.violation_error_code = \"custom_code\"\n    msg = \"Custom message\"\n    with self.assertRaisesMessage(ValidationError, msg) as cm:\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    self.assertEqual(cm.exception.code, \"custom_code\")\n", "def test_contains_expressions(self):\n    constraint_with_expressions = models.UniqueConstraint(Lower(\"field\"), name=\"expr_uniq\")\n    self.assertTrue(constraint_with_expressions.contains_expressions)\n    constraint_without_expressions = models.UniqueConstraint(fields=[\"field\"], name=\"fields_uniq\")\n    self.assertFalse(constraint_without_expressions.contains_expressions)\n", "def test_validate_expression_custom_error(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\", violation_error_message=\"Custom message\", violation_error_code=\"custom_code\")\n    msg = \"Custom message\"\n    with self.assertRaisesMessage(ValidationError, msg) as cm:\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    self.assertEqual(cm.exception.code, \"custom_code\")\n"], "sample_576": ["def test_custom_legend(self, xy):\n\n    s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n    p = Plot(**xy, color=s).add(MockMark()).legend(title=\"Custom Title\").plot()\n    legend, = p._figure.legends\n    assert legend.get_title().get_text() == \"Custom Title\"\n", "def test_default_object(self):\n\n    d = Default()\n    assert d.compute(1) == 1\n    assert d.compute(2) == 2\n", "def test_stat_with_frame_and_frames(self, long_df):\n\n    class MockStat(Stat):\n            other = {\"x\": \"y\", \"y\": \"x\"}[orient]\n            return data.assign(**{other: data[orient]})\n\n    m = MockMark()\n    p = Plot(long_df, x=\"a\", y=\"z\").add(m, MockStat())\n    p._data.frames[(1, 2)] = long_df.assign(x=\"b\", y=\"c\")\n    p.plot()\n\n    expected = long_df.groupby(\"a\", sort=False)[\"z\"].mean().reset_index(drop=True)\n    expected = pd.concat([expected, pd.Series([1.0, 2.0], name=\"y\")])\n    assert_vector_equal(m.passed_data[0][\"y\"], expected)\n", "def test_pair_one_dimension_with_facet(self, long_df):\n\n    x = [\"y\", \"z\"]\n    facet_var = \"a\"\n    m = MockMark()\n    Plot(long_df).facet(facet_var).pair(x).add(m).plot()\n\n    facets = categorical_order(long_df[facet_var])\n    var_product = itertools.product(x, facets)\n\n    for data, (x_i, f_i) in zip(m.passed_data, var_product):\n        rows = long_df[facet_var] == f_i\n        assert_vector_equal(data[\"x\"], long_df.loc[rows, x_i])\n", "def test_stat_with_multiple_grouping_variables(self, long_df):\n\n    group_vars = [\"a\", \"b\"]\n    sem = dict(x=\"x\", y=\"z\", group=group_vars)\n    m = MockMark()\n    p = Plot(long_df, **sem).add(m, Agg()).plot()\n\n    grouped_data = long_df.groupby(group_vars, sort=False)[\"z\"].mean()\n    expected = grouped_data.reset_index().drop(columns=group_vars)\n    for data in m.passed_data:\n        assert_vector_equal(data[\"y\"], expected)\n\n    # Test that original data is not mutated\n    assert \"y\" not in long_df.columns\n"], "sample_724": ["def test_imputation_invalid_inputs():\n    # Test imputation with invalid inputs\n    X = np.array([[1, 2, 3], [4, 5, np.nan], [7, np.nan, 9]])\n\n    # Test invalid strategy\n    imputer = Imputer(strategy=\"invalid\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test invalid axis\n    imputer = Imputer(axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test mismatched shape during transform\n    imputer = Imputer()\n    imputer.fit(X)\n    X_mismatched = np.array([[1, 2]])\n    assert_raises(ValueError, imputer.transform, X_mismatched)\n", "def test_imputation_with_invalid_strategy():\n    # Test imputation with an invalid strategy\n    X = np.array([\n        [1, np.nan, 3],\n        [4, 5, np.nan],\n        [7, 8, 9]\n    ])\n    imputer = Imputer(missing_values=np.nan, strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_with_invalid_strategy():\n    # Test imputation with an invalid strategy.\n    X = np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ])\n\n    imputer = Imputer(missing_values=-1, strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_invalid_values():\n    # Test imputation with invalid strategy and axis values\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9],\n    ])\n\n    # Test with invalid strategy\n    imputer = Imputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test with invalid axis\n    imputer = Imputer(strategy=\"mean\", axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test with missing values not in X\n    imputer = Imputer(strategy=\"mean\", missing_values=10)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_equal(X_imputed, X)  # No imputation should occur\n", "def test_imputation_empty_array():\n    # Test imputation with empty array.\n    X = np.empty(shape=(0, 2))\n    X_true = X.copy()\n\n    # Mean\n    _check_statistics(X, X_true, \"mean\", [np.nan, np.nan], 'NaN')\n\n    # Median\n    _check_statistics(X, X_true, \"median\", [np.nan, np.nan], 'NaN')\n\n    # Most frequent\n    _check_statistics(X, X_true, \"most_frequent\", [np.nan, np.nan], 'NaN')\n"], "sample_242": ["def test_year_lookup_bounds(self):\n    connection = mock.Mock()\n    connection.ops.year_lookup_bounds_for_date_field.return_value = ('2010-01-01', '2010-12-31')\n    connection.ops.year_lookup_bounds_for_datetime_field.return_value = ('2010-01-01 00:00:00', '2010-12-31 23:59:59')\n\n    look_up = YearLookup(\n        lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n        rhs=Value(2010),\n    )\n\n    self.assertEqual(look_up.year_lookup_bounds(connection, 2010), ('2010-01-01 00:00:00', '2010-12-31 23:59:59'))\n\n    look_up = YearLookup(\n        lhs=Value(datetime(2010, 1, 1), output_field=DateTimeField()),\n        rhs=Value(2010),\n    )\n\n    self.assertEqual(look_up.year_lookup_bounds(connection, 2010), ('2010-01-01', '2010-12-31'))\n", "    def test_year_lookup_bounds(self):\n        lookup = YearLookup(lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()), rhs=Value(2010))\n        connection = mock.Mock()\n        connection.ops.year_lookup_bounds_for_datetime_field.return_value = (datetime(2010, 1, 1), datetime(2011, 1, 1))\n        self.assertEqual(lookup.year_lookup_bounds(connection, 2010), (datetime(2010, 1, 1), datetime(2011, 1, 1)))\n\n        connection.ops.year_lookup_bounds_for_date_field.return_value = (datetime(2010, 1, 1), datetime(2011, 1, 1))\n        lookup.lhs.output_field = mock.Mock()\n        lookup.lhs.output_field.get_internal_type.return_value = \"DateField\"\n        self.assertEqual(lookup.year_lookup_bounds(connection, 2010), (datetime(2010, 1, 1), datetime(2011, 1, 1)))\n", "def test_year_lookup_bounds(self):\n    lookup = YearLookup(lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()), rhs=Value(2010))\n    connection = mock.Mock()\n    connection.ops.year_lookup_bounds_for_datetime_field.return_value = (datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59))\n    bounds = lookup.year_lookup_bounds(connection, 2010)\n    self.assertEqual(bounds, (datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59)))\n", "def test_year_lookup_bounds(self):\n    lookup = YearLookup(\n        lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n        rhs=2010\n    )\n    connection = mock.MagicMock()\n    connection.ops.year_lookup_bounds_for_datetime_field.return_value = (\n        datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59)\n    )\n    self.assertEqual(\n        lookup.year_lookup_bounds(connection, 2010),\n        (datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59))\n    )\n", "def test_year_lookup_bounds(self):\n    from django.db.backends.base.operations import BaseDatabaseOperations\n\n    # Test with DateTimeField\n    lookup = YearLookup(lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()), rhs=Value(2010))\n    connection = mock.Mock()\n    connection.ops = BaseDatabaseOperations()\n    start, finish = lookup.year_lookup_bounds(connection, 2010)\n    self.assertEqual(start, datetime(2010, 1, 1, 0, 0, 0))\n    self.assertEqual(finish, datetime(2010, 12, 31, 23, 59, 59))\n\n    # Test with other field type (assuming it uses date_field version)\n    lookup = YearLookup(lhs=Value('2010-01-01'), rhs=Value(2010))\n    start, finish = lookup.year_lookup_bounds(connection, 2010)\n    self.assertEqual(start, '2010-01-01')\n    self.assertEqual(finish, '2010-12-31')\n"], "sample_842": ["def test_kernel_gradient_shape(kernel):\n    # Check that the shape of the kernel gradient is correct.\n    X_shape = (10, 5)\n    X = np.random.rand(*X_shape)\n    _, K_gradient = kernel(X, eval_gradient=True)\n\n    assert K_gradient.shape == (X_shape[0], X_shape[0], kernel.n_dims)\n", "def test_kernel_equality():\n    # Test the equality operator for kernels.\n    kernel1 = RBF(length_scale=2.0)\n    kernel2 = RBF(length_scale=2.0)\n    kernel3 = RBF(length_scale=3.0)\n\n    assert kernel1 == kernel2\n    assert kernel1 != kernel3\n", "def test_kernel_exponentiation():\n    # Test that exponentiation of kernels is correct\n    kernel = RBF(length_scale=2.0)\n    exp_kernel = kernel ** 2\n\n    K1 = exp_kernel(X)\n    K2 = kernel(X) ** 2\n\n    assert_almost_equal(K1, K2)\n", "def test_kernel_pickling():\n    # Test that kernels can be pickled and unpickled correctly.\n    import pickle\n\n    for kernel in kernels:\n        # pickle the kernel\n        dumped_kernel = pickle.dumps(kernel)\n        # unpickle the kernel\n        loaded_kernel = pickle.loads(dumped_kernel)\n\n        # check that the loaded kernel is identical to the original kernel\n        assert kernel == loaded_kernel\n        assert id(kernel) != id(loaded_kernel)\n\n        # check that the loaded kernel can be used correctly\n        K_original = kernel(X)\n        K_loaded = loaded_kernel(X)\n        assert_array_almost_equal(K_original, K_loaded)\n", "def test_kernel_hyperparameters_bounds():\n    # Test that kernel hyperparameters bounds are valid and used correctly.\n    kernel = RBF(length_scale=2.0, length_scale_bounds=(0.5, 2.0))\n    kernel.theta = [np.log(1.5)]  # Set length_scale to 1.5\n    assert np.exp(kernel.theta) == 1.5\n    assert np.exp(kernel.bounds)[0] == 0.5\n    assert np.exp(kernel.bounds)[1] == 2.0\n\n    # Test that bounds are enforced\n    with pytest.raises(ValueError):\n        kernel.theta = [np.log(0.2)]  # Set length_scale to 0.2, below the lower bound\n    with pytest.raises(ValueError):\n        kernel.theta = [np.log(3.0)]  # Set length_scale to 3.0, above the upper bound\n\n    # Test that bounds are used in the gradient\n    K, K_gradient = kernel(X, eval_gradient=True)\n    assert np.all(K_gradient >= 0.0)\n    kernel.theta = [np.log(0.5)]  # Set length_scale to the lower bound\n    K, K_gradient = kernel(X, eval_gradient=True)\n    assert np.all(K_gradient >= 0.0)\n    kernel.theta = [np.log(2.0)]  # Set length_scale to the upper bound\n    K, K_gradient = kernel(X, eval_gradient=True)\n    assert np.all(K_gradient <= 0.0)\n"], "sample_1026": ["def test_MatrixSymbol_issue_15578_transpose():\n    if not numpy:\n        skip(\"numpy not installed\")\n    A = MatrixSymbol('A', 2, 2)\n    A0 = numpy.array([[1, 2], [3, 4]])\n    f = lambdify(A, A.T)\n    assert numpy.allclose(f(A0), numpy.array([[1, 3], [2, 4]]))\n", "def test_complex_number():\n    f = lambdify(x, x + I*x)\n    assert f(1) == 1 + 1j\n", "def test_complex_functions():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    expr = I*x + 1j*y\n    func = lambdify((x, y), expr, modules=\"numpy\")\n    assert func(1, 2) == 1 + 2j\n    assert numpy.allclose(func(numpy.array([1, 2]), numpy.array([3, 4])), numpy.array([1 + 3j, 2 + 4j]))\n", "def test_NumPy_Matrix_issue_15578():\n    if not numpy:\n        skip(\"numpy not installed\")\n    A = numpy.matrix('[1, 2; 3, 4]')\n    f = lambdify([], A.I)\n    assert numpy.allclose(f(), numpy.matrix('[-2.0, 1.0; 1.5, -0.5]'))\n    g = lambdify([], A**3)\n    assert numpy.allclose(g(), numpy.matrix('[37, 54; 81, 118]'))\n", "def test_numpy_dotproduct_with_symbolic_dimensions():\n    if not numpy:\n        skip(\"numpy not installed\")\n    n = symbols('n')\n    A = MatrixSymbol('A', n, 1)\n    B = MatrixSymbol('B', 1, n)\n    f = lambdify((n, A, B), DotProduct(A, B), modules='numpy')\n    A_array = numpy.array([1, 2, 3])\n    B_array = numpy.array([[1, 2, 3]])\n    assert numpy.allclose(f(3, A_array, B_array), numpy.array([14]))\n"], "sample_153": ["def test_mysql_sql_mode_no_engines(self):\n    good_sql_modes = [\n        'NO_ENGINE_SUBSTITUTION',\n    ]\n    for response in good_sql_modes:\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=(response,)\n        ):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n    bad_sql_modes = ['', 'WHATEVER']\n    for response in bad_sql_modes:\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=(response,)\n        ):\n            # One warning for each database alias\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['mysql.W003', 'mysql.W003'])\n", "def test_postgresql_sql_identifiers(self):\n    # Test with a valid SQL identifier\n    with mock.patch(\n        'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n        return_value=('valid_identifier',)\n    ):\n        self.assertEqual(check_database_backends(databases=self.databases), [])\n\n    # Test with an invalid SQL identifier\n    with mock.patch(\n        'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n        return_value=('invalid-identifier',)\n    ):\n        result = check_database_backends(databases=self.databases)\n        self.assertEqual(len(result), 2)\n        self.assertEqual([r.id for r in result], ['postgresql.W003', 'postgresql.W003'])\n", "def test_postgresql_max_connections(self):\n    good_max_connections = ['100']\n    for response in good_max_connections:\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=(response,)\n        ):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n    bad_max_connections = ['10']\n    for response in bad_max_connections:\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=(response,)\n        ):\n            # One warning for each database alias\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n", "def test_mysql_sql_mode_not_set(self):\n    with mock.patch(\n        'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n        return_value=None\n    ):\n        result = check_database_backends(databases=self.databases)\n        self.assertEqual(len(result), 2)\n        self.assertEqual([r.id for r in result], ['mysql.W002', 'mysql.W002'])\n", "def test_postgresql_synchronous_commit(self):\n    good_synchronous_commit = ['on']\n    for response in good_synchronous_commit:\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=(response,)\n        ):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n    bad_synchronous_commit = ['off']\n    for response in bad_synchronous_commit:\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=(response,)\n        ):\n            # One warning for each database alias\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual([r.id for r in result], ['postgresql.W003', 'postgresql.W003'])\n"], "sample_1056": ["def test_not():\n    p = ~(x > 0)\n    l = lambdarepr(p)\n    eval(\"lambda x: \" + l)\n    assert l == \"(not (x > 0))\"\n", "def test_numexpr_functions():\n    # Test that NumExprPrinter supports all the functions it claims to\n    for sympy_func, numexpr_func in NumExprPrinter._numexpr_functions.items():\n        expr = globals()[sympy_func](x)\n        l = NumExprPrinter().doprint(expr)\n        assert numexpr_func in l\n", "def test_numexpr_printer():\n    prntr = NumExprPrinter()\n    assert prntr._print_Function(sin(x)) == 'sin(x)'\n    assert prntr._print_Function(cos(x)) == 'cos(x)'\n    assert prntr._print_Function(tan(x)) == 'tan(x)'\n    assert prntr._print_ImaginaryUnit(1j) == '1j'\n    A = Matrix([[x, y], [y*x, z**2]])\n    raises(TypeError, lambda: prntr._print_Matrix(A))\n    p = Piecewise((x, True), evaluate=False)\n    raises(TypeError, lambda: prntr._print_Piecewise(p))\n    s = Sum(i * x, (i, a, b))\n    l = prntr.doprint(s)\n    assert l == \"evaluate('sum(i*x for i in range(a, b+1))', truediv=True)\"\n", "def test_lambdarepr_functions():\n    # Test the printing of some functions\n    assert lambdarepr(sin(x)) == \"sin(x)\"\n    assert lambdarepr(sqrt(x)) == \"sqrt(x)\"\n    assert lambdarepr(abs(x)) == \"abs(x)\"\n    assert lambdarepr(Piecewise((abs(x), x > 0), (abs(-x), True))) == \"((abs(x)) if (x > 0) else (abs(-x)))\"\n", "def test_not():\n    # Test the _print_Not method\n    p = ~(x < y)\n    l = lambdarepr(p)\n    eval(f'lambda {x}, {y}: {l}')\n    assert l == \"(not (x < y))\"\n"], "sample_1076": ["def test_log2_log1p():\n    from sympy import log2, log1p\n\n    expr1 = log2(x)\n    expr2 = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(x)/numpy.log(2)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(x)/numpy.log(2)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log(x)/math.log(2)'\n    assert prntr.doprint(expr2) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log(x)/mpmath.log(2)'\n    assert prntr.doprint(expr2) == 'mpmath.log(x+1)'\n", "def test_log2_log1p():\n    from sympy import log, log2, log1p\n\n    expr1 = log(x, 2)\n    expr2 = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(x)/numpy.log(2)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log2(x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log(x)/math.log(2)'\n    assert prntr.doprint(expr2) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log(x)/mpmath.log(2)'\n    assert prntr.doprint(expr2) == 'mpmath.log(x+1)'\n", "def test_PythonCodePrinter_print_methods():\n    expr = CustomPrintedObject()\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'CustomPrintedObject()'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'CustomPrintedObject()'\n", "def test_SparseMatrix_printing():\n    p = NumPyPrinter()\n    smat = SparseMatrix(3, 3, {(0, 1): 2, (1, 2): 3})\n    assert p.doprint(smat) == 'scipy.sparse.coo_matrix([2, 3], ([0, 1], [1, 2]), shape=(3, 3))'\n    assert 'scipy.sparse' in p.module_imports\n", "def test_python3_print_function():\n    prntr = PythonCodePrinter({'standard': 'python3'})\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)), (2, x>6))) == 'print((1) if (x == 0) else (2) if (x > 6) else None)'\n"], "sample_1057": ["def test_fully_qualified_modules():\n    from sympy import symbols\n    x, y = symbols('x y')\n    ast = Print((x, y), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'fully_qualified_modules': True})\n    printer.doprint(ast)\n    assert 'import sympy.codegen.ast' in render_as_module(ast)\n    printer = PythonCodePrinter({'fully_qualified_modules': False})\n    printer.doprint(ast)\n    assert 'from sympy.codegen.ast import Print' in render_as_module(ast)\n", "def test_fully_qualified_modules():\n    from sympy import symbols\n    x, y = symbols('x y')\n    ast = Print((x, y), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules': True})\n    printer.doprint(ast)\n    module_imports_str = '\\n'.join('import %s' % k for k in printer.module_imports)\n    assert module_imports_str == 'import sympy.codegen.ast\\nimport sympy.codegen.pyutils'\n", "def test_fully_qualified_modules():\n    from sympy import sin, cos\n    ast = Print('sin(x) cos(x)'.split(), \"values: %12.5g %12.5g\")\n    code = render_as_module(ast, standard='python3')\n    assert 'import sin, cos' not in code\n    assert 'from sympy import sin, cos' in code\n", "def test_fully_qualified_modules():\n    ast = Print('cos(x)'.split(), \"cosine value: %12.5g\")\n    content = render_as_module(ast, standard='python3')\n    assert 'import math' in content\n    assert 'print(\"cosine value: %12.5g\" % (math.cos(x)))' in content\n", "def test_fully_qualified_modules():\n    from sympy import sin\n    ast = Print(sin(1))\n    result = render_as_module(ast, standard='python3')\n    assert 'import sympy' in result\n    assert 'print(sympy.sin(1))' in result\n"], "sample_196": ["def test_adapt_ipaddressfield_value(self):\n    self.assertIsNone(self.ops.adapt_ipaddressfield_value(''))\n    self.assertEqual(self.ops.adapt_ipaddressfield_value('192.168.0.1'), '192.168.0.1')\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('123.45')\n    max_digits = 8\n    decimal_places = 2\n    self.assertEqual(self.ops.adapt_decimalfield_value(value, max_digits, decimal_places), '123.45')\n", "def test_explain_query_prefix_not_supported_error(self):\n    msg = 'This backend does not support explaining query execution.'\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        self.ops.explain_query_prefix()\n", "def test_integer_field_range(self):\n    for field_type, (min_value, max_value) in self.ops.integer_field_ranges.items():\n        self.assertEqual(self.ops.integer_field_range(field_type), (min_value, max_value))\n", "def test_adapt_ipaddressfield_value(self):\n    self.assertIsNone(self.ops.adapt_ipaddressfield_value(None))\n    self.assertEqual(self.ops.adapt_ipaddressfield_value('192.168.0.1'), '192.168.0.1')\n"], "sample_1106": ["def test_merge_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    B = eye(2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n    Y = Matrix([[2, 2], [3, 5]])\n    assert merge_explicit(X) == Y\n", "def test_matadd_shape():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert MatAdd(A, B).shape == (3, 3)\n", "def test_matadd_construction():\n    from sympy import MatrixSymbol, MatAdd\n    A = MatrixSymbol('A', 5, 5)\n    B = MatrixSymbol('B', 5, 5)\n    C = MatrixSymbol('C', 5, 5)\n    assert MatAdd(A, B, C) == A + B + C\n", "def test_matadd_construction():\n    assert MatAdd(C, D).args == (C, D)\n    assert MatAdd(D, C).args == (D, C)\n    assert MatAdd(C, D).shape == C.shape\n    assert MatAdd(C, D).doit() == MatAdd(C, D)  # since C and D are not evaluated\n", "def test_matadd_properties():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    mat_add = MatAdd(X, Y, Z)\n    assert mat_add.shape == (2, 2)\n    assert mat_add._entry(1, 1) == X[1, 1] + Y[1, 1] + Z[1, 1]\n    assert mat_add._eval_transpose() == MatAdd(Transpose(X), Transpose(Y), Transpose(Z))\n    assert mat_add._eval_adjoint() == MatAdd(Adjoint(X), Adjoint(Y), Adjoint(Z))\n    assert mat_add._eval_trace() == Trace(X) + Trace(Y) + Trace(Z)\n    assert mat_add.doit() == MatAdd(X, Y, Z)\n"], "sample_1088": ["def test_symmetrize_multivariate():\n    assert symmetrize(x**2*y + x*y**2, x, y) == ((x + y)**2, 0)\n    assert symmetrize(x**2*y - x*y**2, x, y) == ((x + y)**2, -2*x*y)\n\n    assert symmetrize(x**2*y*z + x*y**2*z + x*y*z**2, x, y, z) == ((x + y + z)**2, -y*z - x*(y + z))\n", "def test_symmetrize_multivariate():\n    assert symmetrize(x**2 + y**2 + z**2) == (\n        (x + y + z)**2 - 2*(x*y + y*z + z*x), 0)\n    assert symmetrize(x**2 - y**2 + z**2) == (\n        (x + y + z)**2 - 2*(x*y + y*z + z*x), -2*y**2)\n    assert symmetrize(x**3 + y**2 + z, x, y, z) == (\n        (x + y + z)**3 - 3*(x*y + y*z + z*x)*(x + y + z) + 3*x*y*z, y**2)\n", "def test_symmetrize_multivariate():\n    F = x**2*y + x*y**2 + z**2\n    sym, non_sym = symmetrize(F)\n    assert sym == -2*x*y*z + (x + y)**2*z + x*z**2 + y*z**2\n    assert non_sym == 0\n\n    F = x**2*y + x*y**2\n    sym, non_sym = symmetrize(F)\n    assert sym == -x*y**2 - y*x**2 + (x + y)**2*z\n    assert non_sym == x*y\n\n    F = x**2*y + x*y**2 + z**2\n    sym, non_sym, polys = symmetrize(F, formal=True)\n    assert sym == -2*s1*s2 + s1**2*s3 + s3**2\n    assert non_sym == 0\n    assert polys == [(s1, x + y), (s2, x*y), (s3, z)]\n", "def test_symmetrize_multivariate():\n    assert symmetrize(x*y + y*z + z*x) == ((x + y + z)**2 - (x*y + x*z + y*z), 0)\n\n    X, Y, Z = symbols('X Y Z')\n    assert symmetrize(x*y + y*z + z*x, symbols=(X, Y, Z)) == (\n        (X + Y + Z)**2 - (X*Y + X*Z + Y*Z), 0, [(X, x + y + z)])\n\n    assert symmetrize(x*y + y*z + z*x, formal=True) == (\n        (s1**2 - s2 - s3, 0, [(s1, x + y + z), (s2, x*y), (s3, x*z)]))\n\n    X, Y = symbols('X Y')\n    assert symmetrize(x*y + x + y, symbols=(X, Y)) == (\n        (X + Y)**2 - (X*Y + X + Y), 0, [(X, x + y)])\n\n    assert symmetrize(x*y + x + y, formal=True) == (\n        (s1**2 - s2 - s1, 0, [(s1, x + y), (s2, x*y)]))\n", "def test_viete_more_roots():\n    r1, r2, r3, r4 = symbols('r1:5')\n\n    assert viete(\n        a*x**4 + b*x**3 + c*x**2 + d*x + e, [r1, r2, r3, r4], x) == [(r1 + r2 + r3 + r4, -b/a), (r1*r2 + r1*r3 + r1*r4 + r2*r3 + r2*r4 + r3*r4, c/a), (r1*r2*r3 + r1*r2*r4 + r1*r3*r4 + r2*r3*r4, -d/a), (r1*r2*r3*r4, e/a)]\n\n    raises(ValueError, lambda: viete(x**5 + 1, [r1, r2, r3]))\n    raises(ValueError, lambda: viete(x**4 + x**3 + x**2 + x + 1, [r1, r2, r3]))\n\n    raises(MultivariatePolynomialError, lambda: viete(x*y + x + y, [r1, r2]))\n"], "sample_1068": ["def test_octave_custom_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert mcode(f(x) + g(x) + g(mat), user_functions=custom_functions) == 'existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn([1 x])'\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert mcode(A[0:2, 1:3]) == \"A(1:2, 2:3)\"\n    assert mcode(A[:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[::2, ::2]) == \"A(1:2:end, 1:2:end)\"\n", "def test_MatrixSlice_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert mcode(A[0:2, 0:2]) == \"A(1:2, 1:2)\"\n    assert mcode(A[0:, 0:2]) == \"A(1:end, 1:2)\"\n    assert mcode(A[0:2, :]) == \"A(1:2, :)\"\n    assert mcode(A[0:3:2, 0:3:2]) == \"A(1:2:end, 1:2:end)\"\n    assert mcode(A[0, 0:3]) == \"A(1, 1:3)\"\n    assert mcode(A[0:3, 0]) == \"A(1:3, 1)\"\n", "def test_Indexed_printing():\n    from sympy import IndexedBase, Idx\n    i = Idx('i')\n    x = IndexedBase('x')\n    y = IndexedBase('y')\n    assert octave_code(x[i]) == 'x(i)'\n    assert octave_code(x[i+1]) == 'x(i + 1)'\n    assert octave_code(x[i]*y[i]) == 'x(i).*y(i)'\n    assert octave_code(x[i+1]*y[i+1]) == 'x(i + 1).*y(i + 1)'\n", "def test_issue_17499():\n    # Test case for issue #17499\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 1)\n    C = MatrixSymbol('C', 1, 3)\n\n    expr = A * B\n    assert mcode(expr) == \"A*B\"\n\n    expr = C * A * B\n    assert mcode(expr) == \"C*A*B\"\n\n    expr = A * B * C\n    assert mcode(expr, assign_to=\"result\") == \"result = A*B*C;\"\n\n    # Test case with HadamardProduct\n    D = HadamardProduct(A, B)\n    expr = D * C\n    assert mcode(expr) == \"(A.*B)*C\"\n"], "sample_973": ["def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @fun.register(int)\n        pass\n\n    assert inspect.is_singledispatch_function(fun) is True\n    assert inspect.is_singledispatch_function(fun.register) is False\n    assert inspect.is_singledispatch_function(fun.dispatch) is False\n    assert inspect.is_singledispatch_function(_) is False\n", "def test_isabstractmethod(app):\n    from target.methods import Base\n\n    assert inspect.isabstractmethod(Base.abstractmeth) is True  # abstractmethod\n    assert inspect.isabstractmethod(Base.meth) is False  # method\n", "def test_is_singledispatch_function(app):\n    from target.functions import func, singledispatch_func\n\n    assert inspect.is_singledispatch_function(func) is False\n    assert inspect.is_singledispatch_function(singledispatch_func) is True\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        pass\n\n    @fun.register(int)\n        pass\n\n    assert inspect.is_singledispatch_function(fun) is True\n    assert inspect.is_singledispatch_function(fun.register(int)) is False\n\n        pass\n\n    assert inspect.is_singledispatch_function(not_singledispatch) is False\n", "def test_isabstractmethod():\n    from abc import abstractmethod\n\n    class AbstractBase:\n        @abstractmethod\n            pass\n\n            pass\n\n    class Concrete(AbstractBase):\n            pass\n\n    assert inspect.isabstractmethod(AbstractBase.abstract_meth) is True\n    assert inspect.isabstractmethod(AbstractBase.concrete_meth) is False\n    assert inspect.isabstractmethod(Concrete.abstract_meth) is False  # overridden method\n    assert inspect.isabstractmethod(Concrete.concrete_meth) is False\n"], "sample_1154": ["def test__linsolve_complex():\n    eqs = [\n        x + I*y - 1 - I,\n        x - I*y - 2 + 3*I\n    ]\n    sol_exact = {x: 1.5, y: 2.5}\n    sol_linsolve = _linsolve(eqs, [x, y])\n    assert all_close(sol_exact, sol_linsolve)\n", "def test__linsolve_equations_with_coefficients():\n    eqs = [\n        2*x + 3*y - 5,\n        x - y + 1\n    ]\n    sol = {x: 2, y: 1}\n    assert _linsolve(eqs, (x, y)) == sol\n\n    eqs = [\n        4*x + 2*y - 8,\n        3*x - y + 3\n    ]\n    sol = {x: 1, y: 2}\n    assert _linsolve(eqs, (x, y)) == sol\n", "def test__linsolve_multiple_equations():\n    eqs = [\n        x + y + z - 6,\n        2*x + 3*y + z - 10,\n        x + y - z - 1\n    ]\n    sol = {x: 2, y: 2, z: 3}\n    assert _linsolve(eqs, (x, y, z)) == sol\n", "def test__linsolve_complex():\n    eqs = [\n        x + I*y - 1 - I,\n        x - I*y + 1 + I\n    ]\n    sol = {x: 1, y: 1}\n    assert _linsolve(eqs, (x, y)) == sol\n", "def test__linsolve_complex():\n    eqs = [\n        x + I*y - 1 - I,\n        I*x - y - 2 + I\n    ]\n    sol_exact = {x: 0.5 + 0.5*I, y: 2.5 - 0.5*I}\n    sol_linsolve = _linsolve(eqs, [x, y])\n    assert sol_exact == sol_linsolve\n"], "sample_1119": ["def test_inverse_matmul_canonicalization():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert Inverse(A*B).doit() == Inverse(B)*Inverse(A)\n", "def test_matpow_canonicalization():\n    A = MatrixSymbol('A', 3, 3)\n    assert MatPow(A, -1) == Inverse(A)\n    assert MatPow(A, 0) == Identity(3)\n    assert MatPow(A, 1) == A\n    assert MatPow(A, 2) == A*A\n    assert MatPow(A, 3) == A*A*A\n", "def test_matrix_multiplication():\n    assert (A*B).shape == (n, l)\n    assert (A*B)[0, 0] == sum(A[0, i]*B[i, 0] for i in range(m))\n    assert (A*B).transpose() == (B.transpose()*A.transpose())\n    assert (A*B*C).shape == (n, n)\n    assert A*(B*C) == (A*B)*C\n", "def test_inverse_simplification():\n    I = Identity(n)\n    assert Inverse(I) == I\n    assert Inverse(3*I) == I/3\n    assert Inverse(A*I*B) == Inverse(B)*Inverse(A)\n    assert Inverse(A*B).doit() == Inverse(B)*Inverse(A)\n", "def test_inverse_matmul_canonicalization():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert Inverse(A * B).doit() == Inverse(B) * Inverse(A)\n"], "sample_1036": ["def test_matmul_with_symbol():\n    x = Symbol('x')\n    A = MatrixSymbol('A', n, m)\n    assert MatMul(x, A) == MatMul(x, A, evaluate=False)\n    assert MatMul(A, x) == MatMul(A, x, evaluate=False)\n", "def test_matmul_order():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert MatMul(A, B, C).doit() == MatMul(A * B, C).doit()\n    assert MatMul(A, B, C).doit() != MatMul(B, A, C).doit()\n", "def test_matmul_different_dimensions():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', k, l)\n    assert MatMul(A, B).doit() == MatMul(A, B)  # should not raise error\n", "def test_matmul_construction_with_mul():\n    # Test that MatMul is constructed correctly when Mul is used\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert Mul(A, B) == MatMul(A, B)\n    assert Mul(B, A) == MatMul(B, A)\n\n    # Test that MatMul is not constructed when Mul is used with non-Matrix objects\n    x = Symbol('x')\n    assert Mul(x, A) == Mul(x, A)\n    assert not isinstance(Mul(x, A), MatMul)\n", "def test_matmul_evaluate_false():\n    assert MatMul(C, D, evaluate=False).doit() == MatMul(C, D)\n    assert MatMul(C, 2, D, evaluate=False).doit() == MatMul(2, C, D)\n"], "sample_927": ["def test_build_domain_cpp_warn_nested_template_param_qualified_name(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"warn-nested-template-param-qualified-name\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:type reference target not found: T::U::typeWarn\" in ws[0]\n    assert \"WARNING: cpp:type reference target not found: T::U::V::typeWarn\" in ws[1]\n", "def test_build_domain_cpp_noindexentry(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"noindexentry\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:noindexentry used on non-indexable object: 'g'\" in ws[0]\n", "def test_template_specialization_parsing():\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n        print(ast)\n\n    check('std::vector<int>')\n    check('std::vector<std::vector<int>>')\n    check('std::map<std::string, int>')\n    check('std::map<std::string, std::map<std::string, int>>')\n    check('std::function<int(int)>')\n    check('std::function<std::vector<int>(std::vector<int>)>')\n", "def test_alias_role_cpp(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    aliasPatterns = [\n        ('ref using alias ', 'using_alias'),\n        ('ref using alias with title ', 'using_alias_title'),\n        ('ref using alias with explicit title ', 'using_alias_explicit_title'),\n        ('ref using alias with title and target ', 'using_alias_title_and_target'),\n        ('ref using alias with explicit title and target ', 'using_alias_explicit_title_and_target'),\n        ('ref using alias with title and target ', 'using_alias_title_and_target2'),\n        ('ref using alias with explicit title and target ', 'using_alias_explicit_title_and_target2'),\n    ]\n\n    f = 'alias.html'\n    t = (app.outdir / f).read_text()\n    for s in aliasPatterns:\n        check(s, t, f)\n", "def test_xref_in_template_params():\n    check('type', 'template<typename T> {key}A = std::vector<T(*)()>',\n          {2: 'I0E1A'},\n          key='using')\n    check('type', 'template<typename T> {key}A = std::vector<std::function<T()>>',\n          {2: 'I0E1A'},\n          key='using')\n    check('type', 'template<typename T> {key}A = std::vector<std::function<void(T)>>',\n          {2: 'I0E1A'},\n          key='using')\n\n    check('type', 'template<typename T> {key}A = std::tuple<T(&)()>',\n          {2: 'I0E1A'},\n          key='using')\n    check('type', 'template<typename T> {key}A = std::tuple<T(&)(int)>',\n          {2: 'I0E1A'},\n          key='using')\n"], "sample_588": ["def test_auto_combine_with_string_coords(self):\n    objs = [Dataset({\"x\": \"a\"}), Dataset({\"x\": \"b\"})]\n    with pytest.warns(FutureWarning, match=\"supplied have global\"):\n        auto_combine(objs)\n", "def test_auto_combine_with_different_data_vars(self):\n    objs = [Dataset({\"x\": [0], \"y\": [0]}), Dataset({\"x\": [1], \"z\": [1]})]\n    with pytest.warns(FutureWarning, match=\"The datasets supplied require both\"):\n        auto_combine(objs)\n", "def test_auto_combine_with_merge_only(self):\n    objs = [Dataset({\"x\": 0}), Dataset({\"y\": 1})]\n    with pytest.warns(FutureWarning, match=\"requires the use of both\"):\n        auto_combine(objs)\n", "def test_auto_combine_with_existing_coords_and_merge(self):\n    objs = [\n        Dataset({\"foo\": (\"x\", [0])}, coords={\"x\": (\"x\", [0]), \"y\": ((), 1)}),\n        Dataset({\"foo\": (\"x\", [1])}, coords={\"x\": (\"x\", [1]), \"y\": ((), 2)}),\n    ]\n    with pytest.warns(FutureWarning, match=\"supplied have global\"):\n        auto_combine(objs)\n", "def test_auto_combine_with_dimension_coords_and_merge_and_concat(self):\n    objs = [\n        Dataset({\"x\": [0], \"y\": [0]}),\n        Dataset({\"x\": [1], \"z\": [1]}),\n        Dataset({\"y\": [2]}),\n    ]\n    with pytest.warns(FutureWarning, match=\"supplied have global\"):\n        with pytest.warns(FutureWarning, match=\"require both concatenation\"):\n            auto_combine(objs)\n"], "sample_430": ["def test_alter_field_to_fk_dependency_other_app_with_initial_true(self):\n    class Migration(migrations.Migration):\n        initial = True\n        operations = [\n            migrations.CreateModel(\"Author\", name=\"Author\", fields=[(\"id\", models.AutoField(primary_key=True))]),\n            migrations.CreateModel(\"Book\", name=\"Book\", fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n            ]),\n            migrations.AddField(\n                model_name=\"Book\",\n                name=\"author\",\n                field=models.ForeignKey(\"testapp.Author\", models.CASCADE),\n            ),\n        ]\n\n    migration = Migration(\"0001_initial\", \"test_app\")\n    suggest_name = migration.suggest_name()\n    self.assertEqual(suggest_name, \"initial\")\n", "def test_unique_constraint_change(self):\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, unique=True)),\n        ],\n    )\n    author_no_unique_constraint = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, unique=False)),\n        ],\n    )\n    changes = self.get_changes([initial_author], [author_no_unique_constraint])\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterField\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        model_name=\"author\",\n        name=\"name\",\n    )\n", "    def test_operation_with_verbose_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"LongModelNameThatDoesNotFitInTheSuggestion\",\n                    fields=[],\n                    options={\n                        'verbose_name': 'Short Name',\n                    },\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"short_name\")\n", "    def test_alter_index_together_with_dependencies(self):\n        before = self.make_project_state(\n            [self.author_empty, self.book_index_together]\n        )\n        after = self.make_project_state(\n            [self.author_empty, self.book_index_together_2]\n        )\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            name=\"book\",\n            index_together={(\"title\", \"author\")},\n        )\n        self.assertMigrationDependencies(\n            changes, \"otherapp\", 0, [(\"testapp\", \"__first__\")]\n        )\n", "def test_alter_field_before_model_deletion(self):\n    \"\"\"\n    Fields are altered _before_ the model they are part of are deleted.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_name, self.publisher_with_author],\n        [self.aardvark_testapp, self.publisher_with_aardvark_author],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"CreateModel\", \"AlterField\", \"DeleteModel\"]\n    )\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Aardvark\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"author\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 2, name=\"Author\")\n"], "sample_959": ["def test_domain_cpp_ast_template_alias():\n    check('type', 'template<typename T> {key}alias = A<T>', {2: 'I0E5alias'}, key='using')\n    check('type', 'template<typename T> {key}alias = A<T, T>', {2: 'I0E5alias'}, key='using')\n    check('type', 'template<typename T, typename U> {key}alias = A<T, U>', {2: 'I00E5alias'}, key='using')\n", "def test_domain_cpp_ast_concept_definitions():\n    # Test concept definition with template parameters and no requirements\n    check('concept', 'template<typename Param> {key}A::B::Concept',\n          {2: 'I0EN1A1B7ConceptE'})\n\n    # Test concept definition with multiple template parameters and no requirements\n    check('concept', 'template<typename A, typename B, typename ...C> {key}Foo',\n          {2: 'I00DpE3Foo'})\n\n    # Test concept definition without any template parameters\n    with pytest.raises(DefinitionError):\n        parse('concept', '{key}Foo')\n\n    # Test concept definition with nested template parameters\n    with pytest.raises(DefinitionError):\n        parse('concept', 'template<typename T> template<typename U> {key}Foo')\n", "def test_domain_cpp_ast_template_constraints():\n    check('class', 'template<typename T> requires A {key}B', {2: 'I_1CE1B'})\n    check('class', 'template<typename T> requires A && B {key}C', {2: 'I_1CE1C'})\n    check('class', 'template<typename T> requires A || B {key}D', {2: 'I_1CE1D'})\n    check('class', 'template<typename T> requires (A && B) || C {key}E', {2: 'I_1CE1E'})\n    check('class', 'template<typename T> requires requires A {key}F', {2: 'I_1CE1F'})\n    check('class', 'template<typename T> requires requires A && B {key}G', {2: 'I_1CE1G'})\n    check('class', 'template<typename T> requires requires (A && B) || C {key}H', {2: 'I_1CE1H'})\n    check('class', 'template<typename T> requires requires requires A {key}I', {2: 'I_1CE1I'})\n    check('class', 'template<typename T> requires A requires B {key}J', {2: 'I_1CE1J'})\n    check('class', 'template<typename T> requires A requires requires B {key}K', {2: 'I_1CE1K'})\n    check('class', 'template<typename T> requires requires A requires B {key}L', {2: 'I_1CE1L'})\n    check('class', 'template<typename T> requires requires requires A requires B {key}M', {2: 'I_1CE1M'})\n", "def test_domain_cpp_parse_duplicate_enumerator(app, warning):\n    # Issue 8270\n    text = (\".. cpp:enum:: E\\n\"\n            \"   .. cpp:enumerator:: a\\n\"\n            \"   .. cpp:enumerator:: a\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:2.\" in ws[0]\n    assert \"Declaration is '.. cpp:enumerator:: a'.\" in ws[1]\n    assert \"index.rst:4: WARNING: Duplicate C++ declaration, also defined at index:2.\" in ws[2]\n    assert \"Declaration is '.. cpp:enumerator:: a'.\" in ws[3]\n    assert ws[4] == \"\"\n", "def test_domain_cpp_ast_constructor_destructor_definitions():\n    check('function', 'MyClass::MyClass()', {1: \"MyClass::MyClass\", 2: \"N7MyClass7MyClassEv\"})\n    check('function', 'MyClass::MyClass(int i)', {1: \"MyClass::MyClass__i\", 2: \"N7MyClass7MyClassEi\"})\n    check('function', 'MyClass::MyClass(const MyClass& other)', {1: \"MyClass::MyClass__MyClassCR\", 2: \"N7MyClass7MyClassERK7MyClass\"})\n    check('function', 'MyClass::MyClass(MyClass&& other)', {1: \"MyClass::MyClass__MyClassRR\", 2: \"N7MyClass7MyClassEOR7MyClass\"})\n    check('function', 'MyClass::~MyClass()', {1: \"MyClass::~MyClass\", 2: \"N7MyClassD0Ev\"})\n"], "sample_1118": ["def test_matpow_non_square():\n    A = MatrixSymbol('A', 2, 3)\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n", "def test_matpow_entry():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatPow(A, 2)\n    assert B._entry(0, 0) == A._entry(0, 0)**2 + A._entry(0, 1)*A._entry(1, 0) + A._entry(0, 2)*A._entry(2, 0)\n", "def test_matpow_non_square():\n    A = MatrixSymbol('A', n, m)\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n", "def test_matpow_non_square_matrix():\n    A = MatrixSymbol('A', n, m)\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n", "def test_matpow_shape():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, -1).shape == (n, n)\n    assert MatPow(A, 3).shape == (n, m)\n"], "sample_969": ["def test_stringify_type_union_operator_with_none():\n    assert stringify(int | None, False) == \"Optional[int]\"  # type: ignore\n    assert stringify(int | None, True) == \"~typing.Optional[int]\"  # type: ignore\n\n    assert stringify(None | str, False) == \"Optional[str]\"  # type: ignore\n    assert stringify(None | str, True) == \"~typing.Optional[str]\"  # type: ignore\n\n    assert stringify(int | str | None, False) == \"Optional[Union[int, str]]\"  # type: ignore\n    assert stringify(int | str | None, True) == \"~typing.Optional[~typing.Union[int, str]]\"  # type: ignore\n", "def test_stringify_Annotated_with_arguments():\n    from typing import Annotated  # type: ignore\n    assert stringify(Annotated[str, \"foo\", \"bar\", 123], False) == \"str\"  # NOQA\n    assert stringify(Annotated[str, \"foo\", \"bar\", 123], True) == \"str\"  # NOQA\n", "def test_stringify_type_hints_type_alias():\n    from typing import TypeAlias  # type: ignore\n    IntOrStr: TypeAlias = Union[int, str]\n\n    assert stringify(IntOrStr, False) == \"IntOrStr\"\n    assert stringify(IntOrStr, True) == \"IntOrStr\"\n\n    assert stringify(List[IntOrStr], False) == \"List[IntOrStr]\"\n    assert stringify(List[IntOrStr], True) == \"~typing.List[IntOrStr]\"\n", "def test_restify_type_ForwardRef_evaluated():\n    from typing import _ForwardRef  # type: ignore\n    assert restify(_ForwardRef(\"MyClass1\")) == \":py:class:`tests.test_util_typing.MyClass1`\"\n", "def test_restify_type_ForwardRef_evaluated():\n    class MyClass:\n        field: \"MyClass\" = None\n\n    hints = get_type_hints(MyClass)\n    assert restify(hints['field']) == \":py:class:`tests.test_util_typing.test_restify_type_ForwardRef_evaluated.<locals>.MyClass`\"\n"], "sample_1141": ["def test_matrixsymbol_transpose():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.T == Transpose(A)\n    assert A.T.T == A\n", "def test_matrixsymbol_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert A.subs(A, B) == B\n    assert A.subs(A, A + B) == A + B\n    assert A.subs(A, A*B) == A*B\n    assert A.subs(A, C).free_symbols == {C}\n", "def test_matrix_transpose_simplify():\n    A = MatrixSymbol('A', 2, 2)\n    assert simplify(Transpose(A)) == Transpose(A)\n    assert simplify(Transpose(Transpose(A))) == A\n    assert simplify(Transpose(A + Transpose(A))) == Transpose(A) + A\n", "def test_matrix_symbol_hash():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('A', 2, 2)\n    assert hash(A) == hash(B)\n\n    C = MatrixSymbol('B', 2, 2)\n    assert hash(A) != hash(C)\n", "def test_matrixsymbol_solving_2():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    I = Identity(2)\n    assert (A*B - I).solve(A) == [I*B.inv()]\n    assert (A*B - I).solve(B) == [A.inv()]\n"], "sample_1174": ["def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', real=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == f(x) / Abs(f(x))\n", "def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', complex=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == f(x)/Abs(f(x))\n", "def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', real=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x)) * f(x) / Abs(f(x))\n", "def test_sign_real_imaginary():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n\n    assert sign(x) == 1\n    assert sign(-x) == -1\n    assert sign(y) == I\n    assert sign(-y) == -I\n", "def test_issue_15944():\n    # doesn't raise AssertionError\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    assert Abs(x - y) + Abs(y - x) == 2*Abs(y - x)\n"], "sample_133": ["def test_i18n_language_english_default_different_domain(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    if the default language is en-us, the selected language has a translation\n    available and a catalog composed by a domain different from djangojs of a\n    Python package is requested.\n    \"\"\"\n    trans_string = 'this app6 string is to be translated'\n    with override('fr'):\n        response = self.client.get('/jsi18n/app6/app6domain/')\n        self.assertContains(response, trans_string)\n", "def test_i18n_language_english_default_with_missing_translations(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a partial language catalog\n    if the default language is en-us, the selected language has a partial\n    translation available and a catalog composed by djangojs domain\n    translations of multiple Python packages is requested. See #13388,\n    #3594 and #13514 for more details.\n    \"\"\"\n    base_trans_string = 'this string is to be translated from '\n    app1_trans_string = base_trans_string + 'app1'\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n_multi_packages1/')\n        self.assertContains(response, app1_trans_string)\n        self.assertNotContains(response, 'app2')  # app2 translations are missing\n\n        response = self.client.get('/jsi18n/app1/')\n        self.assertContains(response, app1_trans_string)\n        self.assertNotContains(response, 'app2')  # app2 translations are missing\n\n        response = self.client.get('/jsi18n/app2/')\n        self.assertEqual(response.context['catalog'], {})  # app2 translations are missing\n", "def test_i18n_language_english_default_without_translation(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns an empty language catalog\n    if the default language is en-us, the selected language doesn't have a\n    translation available and a catalog composed by djangojs domain\n    translations of multiple Python packages is requested.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('es'):\n        response = self.client.get('/jsi18n_multi_packages1/')\n        self.assertNotContains(response, 'il faut traduire cette cha\u00eene de caract\u00e8res de app1')\n        self.assertNotContains(response, 'il faut traduire cette cha\u00eene de caract\u00e8res de app2')\n", "    def test_i18n_language_different_packages(self):\n        \"\"\"\n        Check if the JavaScript i18n view returns the correct language catalog\n        if the selected language has a translation available and a catalog\n        composed by djangojs domain translations of multiple Python packages\n        with different plural forms is requested.\n        \"\"\"\n        app1_plural_string = '(n != 1)'\n        app2_plural_string = '(n > 1)'\n        with self.settings(LANGUAGE_CODE='fr'), override('fr'):\n            response = self.client.get('/jsi18n_multi_packages1/')\n            self.assertContains(response, app1_plural_string)\n            self.assertContains(response, app2_plural_string)\n\n            response = self.client.get('/jsi18n/app1/')\n            self.assertContains(response, app1_plural_string)\n            self.assertNotContains(response, app2_plural_string)\n\n            response = self.client.get('/jsi18n/app2/')\n            self.assertNotContains(response, app1_plural_string)\n            self.assertContains(response, app2_plural_string)\n", "    def test_jsi18n_no_plurals(self):\n        \"\"\"\n        The javascript_catalog returns an empty plural expression for languages\n        with no plural forms.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='ja'), override('ja'):\n            response = self.client.get('/jsi18n/')\n            self.assertNotIn('plural', response.context)\n"], "sample_1058": ["def test_NumPyPrinter_print_MinMax():\n    p = NumPyPrinter()\n    assert p.doprint(Min(x, y)) == 'numpy.amin((x, y))'\n    assert p.doprint(Max(x, y)) == 'numpy.amax((x, y))'\n", "def test_NumPyPrinter_print_BlockMatrix():\n    p = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n    D = MatrixSymbol(\"D\", 2, 2)\n    expr = p.doprint(BlockMatrix([[A, B], [C, D]]))\n    assert expr == \"numpy.block([[A, B], [C, D]])\"\n", "def test_NumPyPrinter_print_arg():\n    p = NumPyPrinter()\n    expr = sqrt(-1)\n    assert p.doprint(expr.args[0]) == 'numpy.angle(-1)'\n", "def test_PythonCodePrinter_new_function():\n    prntr = PythonCodePrinter()\n    expr = sqrt(x) + 1\n    assert prntr.doprint(expr) == 'math.sqrt(x) + 1'\n    assert prntr.module_imports == {'math': {'sqrt'}}\n", "def test_PythonCodePrinter_issue_17578():\n    prntr = PythonCodePrinter()\n    expr = Mod(x, y)\n    assert prntr.doprint(expr) == 'x % y'\n"], "sample_828": ["def test_pairwise_distances_invalid_metric():\n    # Test that a value error is raised if the metric is unknown\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((2, 4))\n    assert_raises(ValueError, pairwise_distances, X, Y, metric=\"invalid_metric\")\n", "def test_pairwise_distances_data_derived_params_explicit():\n    # check that pairwise_distances give the same result in sequential and\n    # parallel, when metric has data-derived parameters explicitly provided.\n    with config_context(working_memory=1):  # to have more than 1 chunk\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((1000, 10))\n        Y = rng.random_sample((1000, 10))\n\n        if metric == \"seuclidean\":\n            params = {'V': np.var(X, axis=0, ddof=1)}\n        else:\n            params = {'VI': np.linalg.inv(np.cov(X.T)).T}\n\n        expected_dist = cdist(X, Y, metric=metric, **params)\n        dist = np.vstack(tuple(pairwise_distances_chunked(X, Y,\n                                                          metric=metric, n_jobs=n_jobs,\n                                                          **params)))\n\n        assert_allclose(dist, expected_dist)\n", "def test_pairwise_distances_data_derived_params_invalid():\n    # check that pairwise_distances raises error for invalid parameters\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 10))\n    Y = rng.random_sample((10, 10))\n    with pytest.raises(ValueError):\n        pairwise_distances(X, Y, metric=\"seuclidean\", V=np.zeros(9))\n    with pytest.raises(ValueError):\n        pairwise_distances(X, Y, metric=\"mahalanobis\", VI=np.zeros((9, 9)))\n", "def test_pairwise_distances_chunked_with_reduce_func():\n    # Test the pairwise_distance_chunked function with a custom reduce_func.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n\n        return np.mean(D_chunk, axis=1)\n\n    S = pairwise_distances(X, metric=\"euclidean\")\n    S_mean = np.mean(S, axis=1)\n    S_chunked_mean = np.hstack(list(pairwise_distances_chunked(X, reduce_func=reduce_func)))\n    assert_array_almost_equal(S_mean, S_chunked_mean)\n", "def test_pairwise_distances_argmin_with_Y_is_None():\n    # Test pairwise_distances_argmin_min when Y is None\n    X = np.array([[0], [1], [2]])\n    indices, distances = pairwise_distances_argmin_min(X, None, metric=\"euclidean\")\n    expected_indices = np.array([1, 0, 0])\n    expected_distances = np.array([1., 0., 1.])\n    np.testing.assert_array_equal(indices, expected_indices)\n    np.testing.assert_array_equal(distances, expected_distances)\n"], "sample_827": ["def test_inplace_swap_row_csr_negative_indices():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n\n    swap = linalg.get_blas_funcs(('swap',), (X,))\n    swap = swap[0]\n    X[-1], X[0] = swap(X[-1], X[0])\n    inplace_swap_row_csr(X_csr, -1, 0)\n    assert_array_equal(X, X_csr.toarray())\n\n    X[-3], X[-2] = swap(X[-3], X[-2])\n    inplace_swap_row_csr(X_csr, -3, -2)\n    assert_array_equal(X, X_csr.toarray())\n", "def test_inplace_normalize_l1_l2_consistency():\n    rs = RandomState(10)\n    X = rs.randn(10, 5)\n    X_csr_l1 = sp.csr_matrix(X.copy())\n    X_csr_l2 = sp.csr_matrix(X.copy())\n    inplace_csr_row_normalize_l1(X_csr_l1)\n    inplace_csr_row_normalize_l2(X_csr_l2)\n    assert_array_almost_equal(X_csr_l1.data, X_csr_l2.data, decimal=5)\n", "def test_inplace_csr_row_normalize_l2_zero_row():\n    # Test inplace_csr_row_normalize_l2 with a zero row\n    X = sp.csr_matrix([[0, 0, 0], [1, 2, 3]])\n    inplace_csr_row_normalize_l2(X)\n    assert_array_almost_equal(X.toarray(), [[0, 0, 0], [1/np.sqrt(14), 2/np.sqrt(14), 3/np.sqrt(14)]])\n", "def test_inplace_normalize_zero_rows():\n    for inplace_csr_row_normalize in (inplace_csr_row_normalize_l1,\n                                      inplace_csr_row_normalize_l2):\n        X = np.zeros((10, 5))\n        X_csr = sp.csr_matrix(X)\n        inplace_csr_row_normalize(X_csr)\n        assert_array_equal(X_csr.toarray(), X)\n\n    X = np.zeros((10, 5), dtype=np.float32)\n    X_csr = sp.csr_matrix(X)\n    inplace_csr_row_normalize_l1(X_csr)\n    assert_array_equal(X_csr.toarray(), X)\n\n    X = np.zeros((10, 5), dtype=np.float32)\n    X_csr = sp.csr_matrix(X)\n    inplace_csr_row_normalize_l2(X_csr)\n    assert_array_equal(X_csr.toarray(), X)\n", "def test_inplace_swap_row_csr_errors():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n\n    # Test with invalid m and n\n    assert_raises(TypeError, inplace_swap_row_csr, X_csr, np.array([0]), 1)\n    assert_raises(TypeError, inplace_swap_row_csr, X_csr, 0, np.array([1]))\n\n    # Test with out-of-bounds indices\n    assert_raises(IndexError, inplace_swap_row_csr, X_csr, -6, 1)\n    assert_raises(IndexError, inplace_swap_row_csr, X_csr, 0, 6)\n"], "sample_154": ["def test_check_database_backends_issues(self, mocked_connections):\n    mock_conn = mock.MagicMock()\n    mock_conn.validation.check.return_value = ['mock issue']\n    mocked_connections.__getitem__.return_value = mock_conn\n\n    issues = check_database_backends(databases=self.databases)\n    self.assertEqual(issues, ['mock issue', 'mock issue'])\n    self.assertEqual(mock_conn.validation.check.call_count, 2)\n", "def test_database_checks_with_errors(self, mocked_check):\n    issues = check_database_backends(databases=self.databases)\n    self.assertTrue(mocked_check.called)\n    self.assertEqual(len(issues), 1)\n    self.assertEqual(issues[0], 'Error')\n", "    def test_database_unusable(self, mocked_is_usable):\n        mocked_is_usable.return_value = False\n        with self.assertRaises(Exception) as context:\n            check_database_backends(databases=self.databases)\n        self.assertTrue('The connection %s is not usable' % connection.alias in str(context.exception))\n", "def test_no_database_connections(self):\n    with self.assertRaises(KeyError):\n        check_database_backends(databases={'nonexistent'})\n", "def test_database_checks_return_issues(self, mocked_check):\n    issues = check_database_backends(databases=self.databases)\n    self.assertTrue(mocked_check.called)\n    self.assertEqual(len(issues), 2)  # Assuming two databases are configured in 'databases'\n    self.assertEqual(issues[0].id, 'some_issue')  # Checking the first issue's id\n"], "sample_319": ["def test_alter_model_table(self):\n    \"\"\"Changing the model table adds a new operation.\"\"\"\n    changes = self.get_changes([self.other_pony], [self.other_pony_table])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"pony\", table=\"new_table\")\n", "    def test_add_constraint_suggest_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddConstraint(\n                    \"Person\",\n                    models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\"),\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"unique_name\")\n", "def test_add_constraint_suggest_name(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.AddConstraint(\n                \"Person\",\n                models.CheckConstraint(check=models.Q(age__gt=18), name=\"age_gt_18\"),\n            ),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    self.assertEqual(migration.suggest_name(), \"age_gt_18\")\n", "def test_alter_index_together_with_rename_index(self):\n    book_partial_index = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"author\"], name=\"book_author_idx\"),\n                models.Index(fields=[\"title\"], name=\"book_title_idx\"),\n            ],\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, self.book_index_together],\n        [AutodetectorTests.author_empty, book_partial_index],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\", \"RenameIndex\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        model_name=\"book\",\n        new_name=\"book_title_idx\",\n        old_fields=(\"title\",),\n    )\n", "    def test_alter_index_together(self):\n        before = [\n            ModelState(\n                \"app\",\n                \"Person\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"first_name\", models.CharField(max_length=200)),\n                    (\"last_name\", models.CharField(max_length=200)),\n                ],\n                options={\n                    \"index_together\": {(\"first_name\", \"last_name\")},\n                },\n            ),\n        ]\n        after = [\n            ModelState(\n                \"app\",\n                \"Person\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"first_name\", models.CharField(max_length=200)),\n                    (\"last_name\", models.CharField(max_length=200)),\n                ],\n                options={\n                    \"index_together\": {(\"last_name\", \"first_name\")},\n                },\n            ),\n        ]\n        changes = MigrationAutodetector(before, after)._detect_changes()\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"AlterIndexTogether\"])\n        self.assertOperationAttributes(\n            changes, \"app\", 0, 0, name=\"person\", index_together={(\"last_name\", \"first_name\")}\n        )\n"], "sample_415": ["def test_contains_expressions(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    self.assertTrue(constraint.contains_expressions)\n\n    constraint = models.UniqueConstraint(fields=[\"name\"], name=\"name_uniq\")\n    self.assertFalse(constraint.contains_expressions)\n", "def test_deconstruction_with_multiple_expressions(self):\n    name = \"unique_expressions\"\n    constraint = models.UniqueConstraint(Lower(\"title\"), F(\"author\"), name=name)\n    path, args, kwargs = constraint.deconstruct()\n    self.assertEqual(path, \"django.db.models.UniqueConstraint\")\n    self.assertEqual(args, (Lower(\"title\"), F(\"author\")))\n    self.assertEqual(kwargs, {\"name\": name})\n", "def test_deconstruction_with_condition_and_deferrable(self):\n    fields = [\"foo\", \"bar\"]\n    name = \"unique_fields\"\n    condition = models.Q(foo=models.F(\"bar\"))\n    deferrable = models.Deferrable.DEFERRED\n    constraint = models.UniqueConstraint(fields=fields, name=name, condition=condition, deferrable=deferrable)\n    path, args, kwargs = constraint.deconstruct()\n    self.assertEqual(path, \"django.db.models.UniqueConstraint\")\n    self.assertEqual(args, ())\n    self.assertEqual(\n        kwargs, {\"fields\": tuple(fields), \"name\": name, \"condition\": condition, \"deferrable\": deferrable}\n    )\n", "    def test_contains_expressions_with_fields(self):\n        constraint = models.UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n        self.assertIs(constraint.contains_expressions, False)\n", "def test_validate_expressions_exclusion(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"title\"),\n        F(\"author\"),\n        name=\"book_func_uq\",\n    )\n    book_1 = UniqueConstraintProduct(title=\"Same Title\", author=\"Author 1\")\n    book_2 = UniqueConstraintProduct(title=\"same title\", author=\"Author 2\")\n    msg = \"Constraint \u201cbook_func_uq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, book_1)\n        constraint.validate(UniqueConstraintProduct, book_2)\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, book_1, exclude={\"title\"})\n    constraint.validate(UniqueConstraintProduct, book_2, exclude={\"author\"})\n"], "sample_826": ["def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_specified_categories_mixed_columns_drop():\n    # multiple columns with drop\n    X = np.array([['a', 'b'], [0, 2]], dtype=object).T\n    enc = OneHotEncoder(categories=[['a', 'b', 'c'], [0, 1, 2]], drop='first')\n    exp = np.array([[0., 1.],\n                    [0., 0.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n    assert enc.categories_[0].tolist() == ['b', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n    assert enc.categories_[1].tolist() == [1, 2]\n    # integer categories but from object dtype data\n    assert np.issubdtype(enc.categories_[1].dtype, np.object_)\n", "def test_one_hot_encoder_drop_first_feature():\n    X = [['abc', 2, 55],\n         ['def', 1, 55],\n         ['abc', 3, 59]]\n    enc = OneHotEncoder(drop='first')\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 1, 1],\n           [1, 0, 1],\n           [0, 1, 0]]\n    assert_array_equal(trans, exp)\n    assert_array_equal(enc.drop_idx_, [0, 1, 1])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_ordinal_encoder_unsorted_categories(X):\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n\n    # test that passing categories as string raises the correct error\n    enc = OrdinalEncoder(categories='auto')\n    with pytest.raises(ValueError, match='Unsorted categories are not supported'):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_drop_auto(X):\n    enc = OneHotEncoder(drop='first')\n    enc.fit(X)\n    transformed = enc.transform(X).toarray()\n    expected = np.array([\n        [1, 0, 1],\n        [0, 1, 1],\n        [0, 0, 0]\n    ])\n    assert_array_equal(transformed, expected)\n"], "sample_781": ["def test_multi_target_single_column(name):\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(bootstrap=True)\n\n    X = iris.data\n    y = iris.target.astype(str)  # Single column target\n\n    # Try to fit and predict.\n    clf.fit(X, y)\n    clf.predict(X)\n", "def test_max_features_sqrt():\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n    all_estimators = [RandomForestClassifier, RandomForestRegressor,\n                      ExtraTreesClassifier, ExtraTreesRegressor]\n\n    for Estimator in all_estimators:\n        est = Estimator(max_features=\"sqrt\")\n        est.fit(X, y)\n        for tree in est.estimators_:\n            # Check that max_features is correctly set to sqrt(n_features)\n            n_features = X.shape[1]\n            assert_equal(tree.max_features, np.sqrt(n_features))\n", "def test_forest_feature_importances_shape():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                 n_estimators=200).fit(X, y)\n    assert clf.feature_importances_.shape == (X.shape[1],)\n", "def test_forest_max_features_auto():\n    # Test max_features='auto' for both classifiers and regressors\n    for ForestEstimator in [RandomForestClassifier, RandomForestRegressor]:\n        X, y = datasets.make_classification(n_samples=100, n_features=20, n_informative=10, random_state=42)\n        est = ForestEstimator(max_features='auto', random_state=42)\n        est.fit(X, y)\n        assert_greater(np.max(est.feature_importances_), 0)\n        assert_equal(np.count_nonzero(est.feature_importances_), 10)\n", "def test_max_features_auto(name):\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n    clf = ForestClassifier(n_estimators=10, max_features=\"auto\", random_state=1)\n    clf.fit(iris.data, iris.target)\n    n_features = iris.data.shape[1]\n    assert_equal(clf.max_features_, int(np.sqrt(n_features)))\n"], "sample_195": ["def test_bulk_insert_sql(self):\n    fields = ['field1', 'field2', 'field3']\n    placeholder_rows = [('value1', 'value2', 'value3'), ('value4', 'value5', 'value6')]\n    expected_sql = \"SELECT value1, value2, value3 UNION ALL SELECT value4, value5, value6\"\n    self.assertEqual(self.ops.bulk_insert_sql(fields, placeholder_rows), expected_sql)\n", "def test_datetime_cast_time_sql(self):\n    # Test the datetime_cast_time_sql method\n    field_name = 'some_field'\n    tzname = 'UTC'\n    expected_output = \"django_datetime_cast_time(some_field, 'UTC', 'UTC')\"\n    self.assertEqual(self.ops.datetime_cast_time_sql(field_name, tzname), expected_output)\n", "def test_combine_duration_expression(self):\n    with self.assertRaisesMessage(DatabaseError, 'Invalid connector for timedelta: +.'):\n        self.ops.combine_duration_expression('+', ['1 day', '2 hours'])\n", "    def test_insert_statement_ignore_conflicts(self):\n        sql = connection.ops.insert_statement(ignore_conflicts=True)\n        self.assertEqual(sql, 'INSERT OR IGNORE INTO')\n", "    def test_sql_flush_with_allow_cascade(self):\n        with transaction.atomic():\n            author1 = Author.objects.create(name='Author 1')\n            Book.objects.create(author=author1)\n            author2 = Author.objects.create(name='Author 2')\n            Book.objects.create(author=author2)\n            Book.objects.create(author=author2)\n\n        sql_list = connection.ops.sql_flush(\n            no_style(),\n            [Author._meta.db_table],\n            allow_cascade=True,\n        )\n        connection.ops.execute_sql_flush(sql_list)\n\n        with transaction.atomic():\n            self.assertIs(Author.objects.exists(), False)\n            self.assertIs(Book.objects.exists(), False)\n"], "sample_1152": ["def test_powsimp_with_float_base():\n    assert powsimp(2.0**x) == 2**x\n    assert powsimp(2.0**x * 2.0**y) == 2**(x + y)\n    assert powsimp(2.0**x * 4.0**y) == 2**(x + 2*y)\n    assert powsimp(0.5**x) == 2**(-x)\n    assert powsimp(0.5**x * 0.5**y) == 2**(-x - y)\n    assert powsimp(0.5**x * 0.25**y) == 2**(-x - 2*y)\n", "def test_powdenest_add_log():\n    x, y, z = symbols('x y z', positive=True)\n    assert powdenest(exp(3*(log(x) + log(y)))) == x**3*y**3\n    assert powdenest(exp(3*(log(x) + 2*log(y)))) == x**3*y**6\n    assert powdenest(exp(3*x*(log(a) + log(b)))) == (a*b)**(3*x)\n", "def test_issue_19700():\n    x, y = symbols('x y')\n    assert powdenest(exp(x * log(y) + x * log(x))) == y**x * x**x\n", "def test_powsimp_with_rational_base_and_exponent():\n    x, y, z = symbols('x y z', rational=True)\n    eq = (x**y)**z\n    assert powsimp(eq) == x**(y*z)\n    eq = (x**(2*y))**z\n    assert powsimp(eq) == x**(2*y*z)\n    eq = (x**(-y))**z\n    assert powsimp(eq) == x**(-y*z)\n", "def test_issue_20647():\n    assert powsimp((x**(2*a))**(-3*b)) == x**(-6*a*b)\n"], "sample_934": ["def test_duplicate_declaration():\n    # Test that duplicate declarations are handled correctly\n    check('function', 'void f(int x)', {1: 'f__i', 2: '1f1i'})\n    with pytest.raises(DefinitionError):\n        parse('function', 'void f(int y)')\n    with pytest.raises(DefinitionError):\n        parse('function', 'void f(int x)')\n", "def test_concept_definitions_with_requires_clause():\n    check('concept', 'template<typename T> requires std::is_integral_v<T> {key}Integral', {2: 'I0E7Integral'})\n    check('concept', 'template<typename T> requires std::is_integral_v<T> && requires(T a) { a > 0; } {key}PositiveIntegral', {2: 'I0E18PositiveIntegral'})\n", "def test_build_domain_cpp_cpp_index_common_prefix(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"cpp-index-common-prefix\")\n    assert len(ws) == 0\n\n    # Check if common prefixes are removed from the index\n    content = (app.outdir / 'index.html').read_text()\n    assert 'A::Bar' in content\n    assert 'A::Bar::baz' in content\n    assert 'B::Qux' in content\n    assert 'B::Qux::quux' in content\n    assert 'C::Foo' in content\n    assert 'C::Foo::bar' in content\n    assert 'D::Bar' not in content\n    assert 'D::Bar::baz' not in content\n", "def test_named_parameter_type():\n    check('function', 'void f(int i = g(42))', {1: \"f__i\", 2: \"1f42\"})\n    check('function', 'void f(int i = [](int j) { return j + 42; }(43))', {1: \"f__i\", 2: \"1f43\"})\n    check('function', 'void f(int i = [](int j) -> int { return j + 42; }(43))', {1: \"f__i\", 2: \"1f43\"})\n    check('function', 'void f(int i = [](int j) -> int const { return j + 42; }(43))', {1: \"f__i\", 2: \"1f43\"})\n    check('function', 'void f(int i = [](int j) const { return j + 42; }(43))', {1: \"f__i\", 2: \"1f43\"})\n    check('function', 'void f(int i = [](int j) { return j + 42; }(43), int k = 44)',\n          {1: \"f__i.i\", 2: \"1f4344\"})\n    check('function', 'void f(int i = [](int j) { return j + 42; }(i), int k = 44)',\n          {1: \"f__i.i\", 2: \"1f44\"})\n    check('function', 'void f(int i = [](int j) -> int { return j + 42; }(i), int k = 44)',\n          {1: \"f__i.i\", 2: \"1f44\"})\n    check('function', 'void f(int i = [](int j) -> int const { return j + 42; }(i), int k = 44)',\n          {1: \"f__i.i\", 2: \"1f44\"})\n    check('function', 'void f(int i =", "def test_more_requirements_clauses():\n    check('function', 'template<typename T> requires A && (B || C) && D void f()',\n          {4: 'I0EIQooaa1Aaa1Boo1Caa1DE1fvv'})\n    check('function', 'template<typename T> requires (A && B) || (C && D) void f()',\n          {4: 'I0EIQooaa1A1Boo1C1DE1fvv'})\n    check('function', 'template<typename T> requires A && B || C && D void f()',\n          {4: 'I0EIQooaa1A1Boo1C1DE1fvv'})\n    check('function', 'template<typename T> requires (A && B) || (C && D) && E void f()',\n          {4: 'I0EIQooaa1A1Boo1C1DEaa1EE1fvv'})\n"], "sample_132": ["    def test_debug_false_no_exception(self):\n        \"An exception report can be generated for just a request with DEBUG=False\"\n        request = self.rf.get('/test_view/')\n        reporter = ExceptionReporter(request, None, None, None)\n        html = reporter.get_traceback_html()\n        self.assertInHTML('<h1>Report at /test_view/</h1>', html)\n        self.assertIn('<pre class=\"exception_value\">No exception message supplied</pre>', html)\n        self.assertIn('<th>Request Method:</th>', html)\n        self.assertIn('<th>Request URL:</th>', html)\n        self.assertNotIn('<th>Exception Type:</th>', html)\n        self.assertNotIn('<th>Exception Value:</th>', html)\n        self.assertNotIn('<h2>Traceback ', html)\n        self.assertIn('<h2>Request information</h2>', html)\n        self.assertNotIn('<p>Request data not supplied</p>', html)\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n        self.request = RequestFactory().post('/', self.breakfast_data)\n", "    def test_traceback_hide(self):\n        \"\"\"\n        The __traceback_hide__ variable can be used to hide frames from the\n        traceback (#21604).\n        \"\"\"\n            raise Exception('Oops')\n\n            __traceback_hide__ = True\n            return func1()\n\n            return func2(request)\n\n        try:\n            view(self.rf.get('/'))\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('Exception: Oops', html)\n        self.assertNotIn('func2', html)\n", "    def test_template_source_loaded_via_loader(self):\n        \"\"\"\n        The ExceptionReporter should be able to load the source of a template\n        loaded via a template loader.\n        \"\"\"\n        with override_settings(TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [tempfile.gettempdir()],\n        }]):\n            template_name = \"template_source_loaded.html\"\n            template_path = os.path.join(tempfile.gettempdir(), template_name)\n            with open(template_path, 'w') as f:\n                f.write(\"This is a test template source.\")\n            try:\n                raise TemplateDoesNotExist(\"This is a test template exception.\")\n            except Exception:\n                exc_type, exc_value, tb = sys.exc_info()\n            request = self.rf.get('/')\n            reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n            frames = reporter.get_traceback_frames()\n            last_frame = frames[-1]\n            self.assertEqual(last_frame['context_line'], 'This is a test template source.')\n            self.assertEqual(last_frame['filename'], template_path)\n", "    def test_email_report_without_admin(self):\n        \"\"\"\n        Test that the email report is not sent if there are no ADMINS.\n        \"\"\"\n        with self.settings(ADMINS=[]):\n            mail.outbox = []  # Empty outbox\n            request = self.rf.post('/some_url/', self.breakfast_data)\n            with self.assertLogs('django.request', 'ERROR'):\n                sensitive_view(request)\n            self.assertEqual(len(mail.outbox), 0)\n"], "sample_731": ["def test_feature_names():\n    data = fetch()\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert(expected_feature_names == data.feature_names)\n", "def test_feature_names():\n    data = fetch()\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert data.feature_names == expected_feature_names, \"Feature names do not match expected values\"\n", "def test_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert(expected_feature_names == data.feature_names)\n", "def test_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert data.feature_names == expected_feature_names, \"Feature names do not match\"\n", "def test_feature_names():\n    try:\n        data = fetch()\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    expected_feature_names = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                              \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n    assert data.feature_names == expected_feature_names\n"], "sample_603": ["def test_summarize_variable():\n    var = xr.Variable([\"time\", \"x\"], [[1, 2, 3], [4, 5, 6]], {\"foo\": \"bar\"})\n    formatted = fh.summarize_variable(\"test_var\", var)\n    assert \"test_var\" in formatted\n    assert \"(time, x)\" in formatted\n    assert \"float64\" in formatted\n    assert \"<span>foo :</span>\" in formatted\n    assert \"<dd>bar</dd>\" in formatted\n", "def test_summarize_variable_with_unsafe_name_and_dtype():\n    var = xr.Variable([\"<x>\"], [1, 2, 3], dtype=\"<M8[ns]\")\n    formatted = fh.summarize_variable(\"<y>\", var, is_index=True)\n    assert \"&lt;y&gt;\" in formatted\n    assert \"&lt;M8[ns]&gt;\" in formatted\n    assert \"class='xr-has-index'\" in formatted\n", "def test_short_data_repr_html_no_repr_html(dataarray):\n    class NoReprHtmlObject:\n            return \"<NoReprHtmlObject>\"\n\n    dataarray.data = NoReprHtmlObject()\n    data_repr = fh.short_data_repr_html(dataarray)\n    assert data_repr == \"<pre>&lt;NoReprHtmlObject&gt;</pre>\"\n", "def test_summarize_variable_with_unsafe_name_and_dtype():\n    name = \"<variable>\"\n    dtype = \"<dtype>\"\n    var = xr.Variable([\"dim1\", \"dim2\"], np.random.randn(2, 3))\n    formatted = fh.summarize_variable(name, var, dtype=dtype)\n    assert \"&lt;variable&gt;\" in formatted\n    assert \"&lt;dtype&gt;\" in formatted\n", "def test_summarize_variable_with_unsafe_name():\n    name = \"<x>\"\n    var = xr.Variable([\"dim_0\", \"dim_1\"], np.random.randn(4, 6))\n    formatted = fh.summarize_variable(name, var)\n    assert \"&lt;x&gt;\" in formatted\n"], "sample_935": ["def test_return_type_mangling():\n    check('function', 'template<typename T> C()', {2: 'I0E1Cv'})\n    check('function', 'template<typename T> operator int()', {1: None, 2: 'I0Ecviv'})\n", "def test_concept_definitions():\n    check('concept', 'template<typename A, typename B, typename ...C> {key}Foo',\n          {2: 'I00DpE3Foo'})\n    with pytest.raises(DefinitionError):\n        parse('concept', '{key}Foo')\n    with pytest.raises(DefinitionError):\n        parse('concept', 'template<typename T> template<typename U> {key}Foo')\n", "def test_build_domain_cpp_multiple_class_declarations(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"multiple-class-declarations\")\n    assert len(ws) == 2\n    assert \"WARNING: Duplicate C++ declaration, also defined in 'multiple-class-declarations.rst'.\" in ws[0]\n    assert \"WARNING: Duplicate C++ declaration, also defined in 'multiple-class-declarations.rst'.\" in ws[1]\n", "def test_inline_literals():\n    check('function', 'void f(int* p, int i)', {1: 'f__iP.i', 2: '1fPi'},\n          output='void f(int *p, int i)')\n    check('function', 'void f(int *p, int i)', {1: 'f__iP.i', 2: '1fPi'},\n          output='void f(int *p, int i)')\n    check('function', 'void f(int * const p, int i)', {1: 'f__iPVC.i', 2: '1fPVCi'},\n          output='void f(int *const p, int i)')\n    check('function', 'void f(int * volatile p, int i)', {1: 'f__iPV.i', 2: '1fPVi'},\n          output='void f(int *volatile p, int i)')\n    check('function', 'void f(int * const volatile p, int i)', {1: 'f__iPVCV.i', 2: '1fPVCVi'},\n          output='void f(int *const volatile p, int i)')\n    check('function', 'void f(int * restrict p, int i)', {1: 'f__iPR.i', 2: '1fPRi'},\n          output='void f(int *restrict p, int i)')\n", "def test_function_definitions_with_type_alias():\n    check('function', 'using my_int = int; my_int f()', {1: 'f', 2: '1fv'})\n"], "sample_923": ["def test_anon_definitions_in_template():\n    check('class', 'template<typename T> {key}A<T::value>', {2: 'I0E1AI1T5valueEE'})\n    check('class', 'template<typename T> {key}A<T::value::inner>', {2: 'I0E1AI1T5value5innerEE'})\n", "def test_static_member_definitions():\n    check('member', 'static const int MyClass::static_member = 42', {1: \"MyClass::static_member__i\", 2: \"N7MyClass13static_memberE\"})\n", "def test_unary_expression():\n    exprCheck('++5', 'pp_L5E')\n    exprCheck('--5', 'mm_L5E')\n    exprCheck('*5', 'deL5E')\n    exprCheck('&5', 'adL5E')\n    exprCheck('+5', 'psL5E')\n    exprCheck('-5', 'ngL5E')\n    exprCheck('!5', 'ntL5E')\n    exprCheck('not 5', 'ntL5E')\n    exprCheck('~5', 'coL5E')\n    exprCheck('compl 5', 'coL5E')\n    exprCheck('sizeof...(a)', 'sZ1a')\n    exprCheck('sizeof(T)', 'st1T')\n    exprCheck('sizeof -42', 'szngL42E')\n    exprCheck('alignof(T)', 'at1T')\n    exprCheck('noexcept(-42)', 'nxngL42E')\n", "def test_pointer_to_member_type():\n    check('function', 'void f(int T::* p)', {2: '1fM1Ti'})\n    check('function', 'void f(int (T::* p)(float, double))', {2: '1fM1TFifdE'})\n    check('function', 'void f(int T::*const)', {2: '1fKM1Ti'})\n    check('function', 'void f(int T::*const&)', {2: '1fRKM1Ti'})\n    check('function', 'void f(int T::*volatile)', {2: '1fVM1Ti'})\n    check('function', 'void f(int T::*const volatile)', {2: '1fVKM1Ti'}, output='void f(int T::*volatile const)')\n    check('function', 'void f(int T::*volatile const)', {2: '1fVKM1Ti'})\n    check('function', 'void f(int T::**)', {2: '1fPM1Ti'})\n    check('function', 'void f(int T::*const *)', {2: '1fPKM1Ti'})\n", "def test_roles_and_directives():\n    # Test the usage of various roles and directives\n    check('.. c:function:: void f()', {1: 'f', 2: '1fv'})\n    check(':c:func:`f()`', {1: 'f', 2: '1fv'})\n    check(':c:func:`~f()`', {1: 'f', 2: '1fv'})\n    check(':c:func:`class::f()`', {1: 'class::f', 2: 'N6class1fEv'})\n    check(':c:func:`~class::f()`', {1: 'class::f', 2: 'N6class1fEv'})\n    check(':c:func:`::f()`', {1: 'f', 2: '1fv'})\n    check(':c:func:`::~f()`', {1: 'f', 2: '1fv'})\n\n    check('.. c:member:: int i', {1: 'i__i', 2: '1i'})\n    check(':c:var:`i`', {1: 'i', 2: '1i'})\n    check(':c:var:`~i`', {1: 'i', 2: '1i'})\n    check(':c:var:`class::i`', {1: 'class::i', 2: 'N6class1i'})\n    check(':c:var:`~class::i`', {1: 'class::i', 2: 'N6class1i'})\n    check(':c:var:`::i`', {1: 'i', 2: '1i'})\n    check(':c:var:`::~i`', {1: 'i', 2: '1i'})\n\n    check('.. c:type:: int', {2: 'i'})\n    check(':c:type:`int`', {2: 'i'})\n    check(':c:type:`~int`', {2: 'i'})\n    check(':c:type:`class::"], "sample_302": ["def test_runshell_signal_handling(self):\n    sigint_handler = object()\n    with mock.patch('signal.getsignal', return_value=sigint_handler):\n        with mock.patch('signal.signal') as signal_mock:\n            with mock.patch.object(super(BaseDatabaseClient, self.client), 'runshell') as super_runshell_mock:\n                self.client.runshell(None)\n                signal_mock.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n                signal_mock.assert_any_call(signal.SIGINT, sigint_handler)\n    super_runshell_mock.assert_called_once_with(None)\n", "def test_runshell_signal_handling(self):\n    sigint_handler = signal.SIG_IGN\n    with mock.patch('subprocess.run') as run:\n        with mock.patch('signal.signal') as signal_mock:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], None),\n            ):\n                self.client.runshell(None)\n                signal_mock.assert_has_calls([\n                    mock.call(signal.SIGINT, signal.SIG_IGN),\n                    mock.call(signal.SIGINT, sigint_handler),\n                ])\n", "def test_runshell_sigint_handler(self):\n    with mock.patch('subprocess.run') as run, mock.patch('signal.getsignal') as getsignal, mock.patch('signal.signal') as signal:\n        getsignal.return_value = 'original_handler'\n        self.client.runshell(None)\n        getsignal.assert_called_once_with(signal.SIGINT)\n        signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n        signal.assert_called_with(signal.SIGINT, 'original_handler')\n", "def test_runshell_sigint_handler(self):\n    sigint_handler = signal.getsignal(signal.SIGINT)\n    with mock.patch('subprocess.run') as run, \\\n         mock.patch.object(BaseDatabaseClient, 'settings_to_cmd_args_env', return_value=([], None)), \\\n         mock.patch('signal.signal') as mock_signal:\n        self.client.runshell(None)\n        mock_signal.assert_has_calls([mock.call(signal.SIGINT, signal.SIG_IGN),\n                                      mock.call(signal.SIGINT, sigint_handler)],\n                                     any_order=False)\n", "def test_runshell_signal_handling(self, mock_getsignal, mock_SIGINT, mock_signal, mock_run):\n    mock_getsignal.return_value = mock.sentinel.original_handler\n    mock_run.return_value = mock.sentinel.completed_process\n\n    self.client.runshell(None)\n\n    mock_getsignal.assert_called_once_with(mock_SIGINT)\n    mock_signal.assert_has_calls([\n        mock.call(mock_SIGINT, signal.SIG_IGN),\n        mock.call(mock_SIGINT, mock.sentinel.original_handler),\n    ])\n    mock_run.assert_called_once_with([], env=None, check=True)\n"], "sample_732": ["def test_return_X_y():\n    try:\n        X, y = fetch_kddcup99(download_if_missing=False, return_X_y=True)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(X.shape, (494021, 41))\n    assert_equal(y.shape, (494021,))\n\n    X_shuffled, y_shuffled = fetch_kddcup99(shuffle=True, random_state=0, return_X_y=True)\n    assert_equal(X.shape, X_shuffled.shape)\n    assert_equal(y.shape, y_shuffled.shape)\n", "def test_fetch_kddcup99_invalid_subset():\n    with assert_raises(ValueError):\n        fetch_kddcup99(subset='invalid_subset')\n", "def test_invalid_subset():\n    with assert_raises(ValueError):\n        fetch_kddcup99(subset='invalid')\n", "def test_invalid_subset():\n    with assert_raises(ValueError):\n        fetch_kddcup99(subset='invalid')\n", "def test_subset_check():\n    try:\n        fetch_kddcup99(subset='unknown')\n    except ValueError as ve:\n        assert_equal(str(ve), \"Unknown subset. Valid options are: None, 'SA', 'SF', 'http', 'smtp'\")\n"], "sample_575": ["def test_label_formatter_input_check(self, x):\n\n    err = \"Label formatter must be an instance of .*?, not <class 'str'>.\"\n    with pytest.raises(TypeError, match=err):\n        Temporal().label(\"{x}\")\n", "def test_label_with_params(self, t):\n    formatter = mpl.ticker.StrMethodFormatter(\"{x:%Y-%m-%d}\")\n    s = Temporal().label(formatter=formatter)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    a.set_view_interval(10, 1000)\n    labels = a.major.formatter.format_ticks([100])\n    assert labels == [\"1970-01-01\"]\n", "    def test_coordinate_with_transform(self, t, x):\n\n        # Test that applying a transform to a Temporal coordinate raises a ValueError\n        s = Temporal(trans=\"log\")\n        with pytest.raises(ValueError, match=\"Temporal scale does not support transforms\"):\n            s._setup(t, Coordinate())\n", "def test_temporal_coordinate_with_transform(self, t, x):\n    s = Temporal(trans=\"log\")._setup(t, Coordinate())\n    with pytest.raises(ValueError, match=\"Log transform not supported for Temporal scale\"):\n        s(t)\n", "def test_color_dict_palette_numeric(self, y):\n\n    cs = color_palette(\"crest\", 3)\n    pal = dict(zip([\"-1.5\", \"1\", \"3\"], cs))\n    s = Nominal(pal)._setup(y, Color())\n    assert_array_equal(s(y), [cs[0], cs[1], cs[2], cs[1]])\n"], "sample_926": ["def test_unhandled_exception():\n    with pytest.raises(DefinitionError):\n        parse('function', 'void f(B b=x(a')\n", "def test_build_domain_cpp_templates_in_using(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"templates-in-using\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:using declarations with templates are not supported\" in ws[0]\n", "def test_attributes_in_template():\n    check('class', '[[]] template<typename T> struct S', {2: 'I0E1S'})\n    check('class', '[ [ ] ] template<typename T> struct S', {2: 'I0E1S'})\n    check('class', '[[a]] template<typename T> struct S', {2: 'I0E1S'})\n    check('class', '__attribute__((a)) template<typename T> struct S', {2: 'I0E1S'})\n    check('class', '__attribute__((a, b)) template<typename T> struct S', {2: 'I0E1S'})\n    check('class', '__attribute__((optimize(3))) template<typename T> struct S', {2: 'I0E1S'})\n    check('class', 'id_attr template<typename T> struct S', {2: 'I0E1S'})\n    check('class', 'paren_attr() template<typename T> struct S', {2: 'I0E1S'})\n    check('class', 'paren_attr(a) template<typename T> struct S', {2: 'I0E1S'})\n", "def test_anon_names_in_expressions():\n    exprCheck('@1::value', 'N1Ut1_11E5valueE')\n    exprCheck('@a::value', 'N1Ut1_a1E5valueE')\n    exprCheck('@a<@b>::value', 'N1Ut1_aI1Ut1_bEE5valueE')\n", "def test_default_arguments():\n    check('function', 'void f(int i = 42)', {1: 'f__i', 2: '1fi'})\n    check('function', 'void f(int i = 42, int j = 23)', {1: 'f__i.i', 2: '1fii'})\n    check('function', 'void f(int i = 42, int j)', {1: 'f__i.i', 2: '1fii'})\n    check('function', 'void f(int i, int j = 23)', {1: 'f__i.i', 2: '1fii'})\n\n    # from breathe#441\n    check('function', 'void f(int i = 42, int j = i)', {1: 'f__i.i', 2: '1fii'})\n"], "sample_279": ["def test_no_fields(self):\n    msg = 'At least one field is required to define a unique constraint.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(name='uniq_no_fields', fields=[])\n", "    def test_empty_fields_constraint(self):\n        with self.assertRaisesMessage(ValueError, 'At least one field is required to define a unique constraint.'):\n            models.UniqueConstraint(name='uniq_empty', fields=[])\n", "    def test_check_constraint_sql(self):\n        constraint = models.CheckConstraint(\n            check=models.Q(price__gt=models.F('discounted_price')),\n            name='price_gt_discounted_price',\n        )\n        mock_schema_editor = mock.MagicMock()\n        mock_schema_editor.connection.ops.quote_name.side_effect = lambda x: '\"%s\"' % x\n        mock_schema_editor.quote_value.side_effect = lambda x: \"'%s'\" % x\n        sql = constraint.constraint_sql(Product, mock_schema_editor)\n        expected_sql = 'CHECK (\"price\" > \\'\"discounted_price\"\\')'\n        self.assertEqual(sql, expected_sql)\n", "    def test_clone(self):\n        constraint = models.UniqueConstraint(fields=['foo', 'bar'], name='unique')\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n", "def test_opclasses_database_constraint(self):\n    UniqueConstraintProduct.objects.create(name='p1', color='red')\n    with self.assertRaises(IntegrityError):\n        UniqueConstraintProduct.objects.create(name='p1', color='red')\n\n    constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\n    self.assertTrue(any(\n        'name_color_opclasses_uniq' in c and c['columns'] == ['name', 'color']\n        for c in constraints\n    ))\n    self.assertTrue(any(\n        'name_color_opclasses_uniq' in c and c['opclasses'] == ['varchar_pattern_ops', 'text_pattern_ops']\n        for c in constraints\n    ))\n"], "sample_611": ["def test_date_range_like_same_calendar_use_cftime():\n    src = date_range(\"2000-01-01\", periods=12, freq=\"6H\", use_cftime=True)\n    out = date_range_like(src, \"noleap\", use_cftime=True)\n    assert src is out\n\n    out = date_range_like(src, \"noleap\", use_cftime=False)\n    assert isinstance(out, pd.DatetimeIndex)\n", "def test_date_range_like_end_of_month(start, freq, cal_src, cal_tgt, use_cftime, exp0, exp_pd):\n    source = date_range(start, periods=12, freq=freq, calendar=cal_src)\n\n    out = date_range_like(source, cal_tgt, use_cftime=use_cftime)\n\n    assert len(out) == 12\n    assert infer_freq(out) == freq\n\n    assert out[0].isoformat().startswith(exp0)\n\n    if exp_pd:\n        assert isinstance(out, pd.DatetimeIndex)\n    else:\n        assert isinstance(out, CFTimeIndex)\n        assert out.calendar == cal_tgt\n", "def test_cftime_range_with_non_default_month(\n    start, end, periods, freq, closed, normalize, calendar, expected_date_args", "def test_date_range_like_end_of_month(start, freq, cal_src, cal_tgt, use_cftime, exp_last):\n    source = date_range(start, periods=3, freq=freq, calendar=cal_src)\n\n    out = date_range_like(source, cal_tgt, use_cftime=use_cftime)\n\n    assert out[-1].isoformat().startswith(exp_last)\n", "def test_date_range_annual_start_month(start, end, freq, calendar, use_cftime, expected_dates):\n    result = date_range(start, end, freq=freq, calendar=calendar, use_cftime=use_cftime)\n    assert [date.isoformat() for date in result] == expected_dates\n"], "sample_1064": ["def test_tensorflow_log():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = log(x)\n    assert tensorflow_code(expr) == \"tensorflow.math.log(x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(1, 10))\n", "def test_tensorflow_log():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = log(x)\n    assert tensorflow_code(expr) == \"tensorflow.math.log(x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(0.1, 1))\n", "def test_tensorflow_log():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = log(x)\n    assert tensorflow_code(expr) == \"tensorflow.math.log(x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(0, 1))\n", "def test_tensorflow_log():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = log(x)\n    assert tensorflow_code(expr) == \"tensorflow.math.log(x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(0.1, 10))\n", "def test_tensorflow_log():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = log(x)\n    assert tensorflow_code(expr) == \"tensorflow.math.log(x)\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.uniform(0.01, 1))\n"], "sample_948": ["def test_scope_control():\n    # test namespace directive\n    check('namespace', '{key}A', {1: \"A\", 2: \"1A\"})\n    check('namespace', '{key}A::B::C', {1: \"A::B::C\", 2: \"N1A1B1C\"})\n    # test namespace-push directive\n    check('namespace-push', '{key}A', {1: \"A\", 2: \"1A\"})\n    # test namespace-pop directive\n    check('namespace-pop', '', {}, output='')\n    # test scope control with a nested namespace-push\n    check('namespace-push', '{key}A', {1: \"A\", 2: \"1A\"})\n    check('namespace-push', '{key}B', {1: \"B\", 2: \"1B\"})\n    check('namespace-pop', '', {}, output='')\n    check('namespace-pop', '', {}, output='')\n", "def test_class_definitions_complex():\n    check('class', 'final class A : public B, private C, protected D',\n          {1: 'A', 2: '1A'}, output='final class A : public B, private C, protected D')\n    check('class', 'virtual class A : public B, private C, protected D',\n          {1: 'A', 2: '1A'}, output='virtual class A : public B, private C, protected D')\n    check('class', 'class A : public B, private C, protected D final',\n          {1: 'A', 2: '1A'}, output='class A : public B, private C, protected D final')\n    check('class', 'explicit class A : public B, private C, protected D',\n          {1: 'A', 2: '1A'}, output='explicit class A : public B, private C, protected D')\n", "def test_nested_classes():\n    check('class', '{key}::A::B', {1: \"A::B\", 2: \"N1A1BE\"})\n    check('function', 'void A::B::f()', {1: \"A::B::f\", 2: \"N1A1B1fv\"})\n", "def test_overloaded_functions():\n    app = sphinx.testing.util.SphinxTestApp()\n    app.build()\n\n    # Check that the cross-reference to the first version of the function is resolved correctly\n    node = addnodes.pending_xref(reftype='func', reftarget='overloaded_func', refdomain='cpp')\n    node.line = 1\n    node.source = 'test.rst'\n    resolved_node = app.env.domains['cpp'].resolve_xref(app.env, 'test.rst', app.builder, 'func', 'overloaded_func(int)', node, addnodes.Text('overloaded_func(int)'))\n    assert resolved_node['refdoc'] == 'test'\n    assert resolved_node['refuri'] == 'test.html#int-overloaded-func'\n\n    # Check that the cross-reference to the second version of the function is resolved correctly\n    node = addnodes.pending_xref(reftype='func', reftarget='overloaded_func', refdomain='cpp')\n    node.line = 1\n    node.source = 'test.rst'\n    resolved_node = app.env.domains['cpp'].resolve_xref(app.env, 'test.rst', app.builder, 'func', 'overloaded_func(double)', node, addnodes.Text('overloaded_func(double)'))\n    assert resolved_node['refdoc'] == 'test'\n    assert resolved_node['refuri'] == 'test.html#double-overloaded-func'\n\n    # Check that the cross-reference to the third version of the function is resolved correctly\n    node = addnodes.pending_xref(reftype='func', reftarget='overloaded_func', refdomain='cpp')\n    node", "def test_typedef_definitions():\n    check('type', '{key}typedef bool A::b', {1: \"A::b\", 2: \"N1A1bE\"}, key='typedef')\n    check('type', '{key}typedef bool *b', {1: \"b\", 2: \"1b\"}, key='typedef')\n    check('type', '{key}typedef bool *const b', {1: \"b\", 2: \"1b\"}, key='typedef')\n    check('type', '{key}typedef bool *volatile const *b', {1: \"b\", 2: \"1b\"}, key='typedef')\n    check('type', '{key}typedef bool &b', {1: \"b\", 2: \"1b\"}, key='typedef')\n    check('type', '{key}typedef bool b[]', {1: \"b\", 2: \"1b\"}, key='typedef')\n"], "sample_1069": ["def test_specfun_polylog():\n    assert octave_code(polylog(n, x)) == 'polylog(n, x)'\n", "def test_power_mod():\n    assert mcode(Pow(x, y, z)) == 'pow(x, y, z)'\n", "def test_log_abs():\n    assert octave_code(log(abs(x))) == 'log(abs(x))'\n    assert octave_code(log(abs(x), 10)) == 'log10(abs(x))'\n", "def test_Pow_with_float_exponent():\n    assert mcode(x**2.5) == 'x^(2.5)'\n    assert mcode(x**-2.5) == 'x^(-2.5)'\n    assert mcode(2**2.5) == '2^(2.5)'\n    assert mcode(2**-2.5) == '2^(-2.5)'\n", "def test_glsl_relational():\n    assert mcode(Eq(x, y)) == \"x == y\"\n    assert mcode(Ne(x, y)) == \"x != y\"\n    assert mcode(Le(x, y)) == \"x <= y\"\n    assert mcode(Lt(x, y)) == \"x < y\"\n    assert mcode(Gt(x, y)) == \"x > y\"\n    assert mcode(Ge(x, y)) == \"x >= y\"\n"], "sample_1125": ["def test_operator_identity():\n    O = Operator('O')\n    I = IdentityOperator()\n    assert O * I == O\n    assert I * O == O\n    assert I * I == I\n", "def test_differential_operator():\n    from sympy import Symbol, Function, Derivative\n    from sympy.physics.quantum.operator import DifferentialOperator\n    from sympy.physics.quantum.state import Wavefunction\n    from sympy.physics.quantum.qapply import qapply\n\n    x = Symbol('x')\n    f = Function('f')\n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n    w = Wavefunction(x**2, x)\n\n    assert d.variables == (x,)\n    assert d.function == f(x)\n    assert d.expr == Derivative(f(x), x)\n    assert d.free_symbols == {x}\n    assert qapply(d*w) == Wavefunction(2, x)\n", "def test_outer_product():\n    from sympy.physics.quantum import Ket, Bra, OuterProduct\n\n    k = Ket('k')\n    b = Bra('b')\n    op = OuterProduct(k, b)\n\n    assert op == OuterProduct(k, b)\n    assert Dagger(op) == OuterProduct(Dagger(b), Dagger(k))\n    assert op.ket == k\n    assert op.bra == b\n", "def test_operator_multiplication():\n    A = Operator('A')\n    B = Operator('B')\n    I = IdentityOperator()\n    assert A*B != B*A  # Non-commutative\n    assert A*I == A  # Identity operator property\n", "def test_outer_product():\n    from sympy.physics.quantum import Ket, Bra, OuterProduct\n\n    k1 = Ket('k1')\n    b1 = Bra('k1')\n    op1 = OuterProduct(k1, b1)\n    assert op1 == k1*b1\n    assert op1.ket == k1\n    assert op1.bra == b1\n    assert Dagger(op1) == OuterProduct(Dagger(b1), Dagger(k1))\n\n    k2 = Ket('k2')\n    b2 = Bra('k2')\n    op2 = OuterProduct(k2, b2)\n    assert op1*op2 == op1*k2*b2\n    assert op2*op1 == op2*k1*b1\n"], "sample_723": ["def test_imputation_with_invalid_strategy():\n    # Test imputation with invalid strategy\n    X = np.array([[np.nan, 1], [2, np.nan]])\n    imputer = SimpleImputer(strategy=\"invalid\")\n    with assert_raises(ValueError):\n        imputer.fit(X)\n", "def test_imputation_non_numeric():\n    # Test imputation with non-numeric values\n    X = np.array([\n        ['a', np.nan, 'c'],\n        ['a', 'b', 'c'],\n        [np.nan, 'b', 'c'],\n        ['a', 'b', np.nan],\n    ], dtype=object)\n\n    X_true = np.array([\n        ['a', 'b', 'c'],\n        ['a', 'b', 'c'],\n        ['a', 'b', 'c'],\n        ['a', 'b', 'c'],\n    ], dtype=object)\n\n    _check_statistics(X, X_true, \"most_frequent\", ['a', 'b', 'c'], np.nan)\n", "def test_imputation_error_handling():\n    # Test error handling for invalid strategy and axis\n    X = np.array([[1, np.nan], [2, np.nan]])\n    imputer = SimpleImputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    imputer = SimpleImputer(strategy=\"mean\", axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test error handling for invalid shape in transform\n    imputer = SimpleImputer(strategy=\"mean\")\n    imputer.fit(X)\n    X_invalid_shape = np.array([[1, 2, 3]])\n    assert_raises(ValueError, imputer.transform, X_invalid_shape)\n", "def test_imputation_invalid_input():\n    # Test imputation with invalid input\n    X = np.array([[1, 2, np.nan],\n                  [4, np.nan, 6],\n                  [7, 8, 9]])\n\n    # Invalid strategy\n    imputer = SimpleImputer(strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Invalid axis\n    imputer = SimpleImputer(axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Invalid shape for transform\n    imputer = SimpleImputer().fit(X)\n    X_invalid = np.array([[1, 2],\n                          [4, np.nan]])\n    assert_raises(ValueError, imputer.transform, X_invalid)\n\n    # All missing values in a row\n    imputer = SimpleImputer(axis=1)\n    X_invalid = np.array([[np.nan, np.nan, np.nan],\n                          [4, np.nan, 6]])\n    assert_raises(ValueError, imputer.fit_transform, X_invalid)\n", "def test_imputation_invalid_axis():\n    # Test imputation with invalid axis value\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n"], "sample_1142": ["def test_MatrixElement_str():\n    A = MatrixSymbol('A', n, m)\n    element = A[0, 0]\n    assert str(element) == \"A[0, 0]\"\n", "def test_MatrixElement_diff_symbol():\n    x = Symbol('x')\n    A = MatrixSymbol('A', 2, 2)\n    assert (A[0, 0]*x).diff(x) == A[0, 0]\n    assert (A[0, 0]*x).diff(A[0, 0]) == x\n", "def test_matrix_conjugate():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.conjugate() == Adjoint(A.T)\n    assert A.conjugate().conjugate() == A\n", "def test_matrixsymbol_equality():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('A', 2, 2)\n    D = MatrixSymbol('A', 3, 3)\n\n    assert A != B\n    assert A == C\n    assert A != D\n    assert A != 0\n    assert A != 1\n    assert A != 'A'\n", "def test_matrix_element_simplification():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, l)\n\n    assert simplify(A[i, j] * B[j, k]) == (A * B)[i, k]\n    assert simplify(A[i, j] + B[i, j]) == (A + B)[i, j]\n    assert simplify(A[i, j] - B[i, j]) == (A - B)[i, j]\n    assert simplify(A[i, j] * C[j, k] + A[i, j] * B[j, k]) == A[i, j] * (C + B)[j, k]\n"], "sample_309": ["    def test_parsing_rfc1123(self):\n        parsed = parse_http_date_safe('Sun, 06 Nov 1994 08:49:37 GMT')\n        self.assertEqual(\n            datetime.fromtimestamp(parsed, timezone.utc),\n            datetime(1994, 11, 6, 8, 49, 37, tzinfo=timezone.utc),\n        )\n", "    def test_parsing_invalid_date(self):\n        invalid_dates = (\n            'Mon, 01 Jan 2007 01:54:21 UTC',  # Invalid time zone\n            '01 Jan 2007 01:54:21 GMT',  # Missing day of the week\n            'Sun, 06 Nov 1994 08:49:37',  # Missing time zone\n            'Sun, 06 Nov 1994 08:49:37 GMTT',  # Invalid time zone\n            'Sun, 06 Nov 94 08:49:37 GMT',  # Two-digit year\n            'Sun, 06 Nov 1994 25:49:37 GMT',  # Invalid hour\n            'Sun, 06 Nov 1994 08:60:37 GMT',  # Invalid minute\n            'Sun, 06 Nov 1994 08:49:60 GMT',  # Invalid second\n            'Sun, 32 Nov 1994 08:49:37 GMT',  # Invalid day\n            'Sun, 06 Dec 1994 08:49:37 GMT',  # Invalid month\n            'Sun, 06 Nov 1994 08:49:37 G',  # Invalid time zone\n            'Sun, 06 Nov 1994 08:49:37 GMTT',  # Invalid time zone\n        )\n        for date in invalid_dates:\n            with self.subTest(date=date):\n                with self.assertRaises(ValueError):\n                    parse_http_date(date)\n", "    def test_parsing_invalid_date(self):\n        invalid_dates = (\n            'Invalid date',\n            'Mon, 01 Jan 2007 01:54:21',\n            'Mon, 01 Jan 2007 01:54:21 UTC',\n            'Mon, 01 Jan 2007 25:54:21 GMT',\n            'Mon, 01 Jan 2007 01:60:21 GMT',\n            'Mon, 32 Jan 2007 01:54:21 GMT',\n            'Mon, 01 Feb 2007 01:54:21 GMT',\n            'Mon, 01 Jan 2007 01:54:61 GMT',\n            'Mon, 01 Jan 0000 01:54:21 GMT',\n            'Mon, 01 Jan 2007 01:54:21 GMZ',\n        )\n        for date in invalid_dates:\n            with self.subTest(date=date):\n                with self.assertRaises(ValueError):\n                    parse_http_date(date)\n", "def test_parsing_invalid(self):\n    invalid_dates = [\n        '',\n        'Sun, 06 Nov 1994 08:49:37',  # No timezone\n        'Sun, 06 Nov 1994 08:49:37 GMTT',  # Invalid timezone\n        'Sun, 06 Nov 1994 25:49:37 GMT',  # Invalid hour\n        'Sun, 06 Nov 1994 08:60:37 GMT',  # Invalid minute\n        'Sun, 06 Nov 1994 08:49:61 GMT',  # Invalid second\n        'Sun, 06 Nov 1994 08:49:37 GM',  # Invalid timezone\n        'Sun, 35 Nov 1994 08:49:37 GMT',  # Invalid day\n        'Sun, 06 Dec 1994 08:49:37 GMT',  # Invalid month\n        'Sun, 06 Nov 94 08:49:37 GMT',  # Invalid year\n        'Sun, 06 Nov 1994 08:49 GMT',  # Missing seconds\n        'Sunday, 06-Nov-94 08:49:37 GMT',  # Invalid day format\n        'Sunday, 06-Nov-1994 08:49:37',  # No timezone\n        'Sunday Nov 6 08:49:37 1994',  # Invalid month format\n        'Sun Nov 6 08:49:37 1994',  # Single digit month\n    ]\n    for date in invalid_dates:\n        with self.subTest(date=date):\n            with self.assertRaises(ValueError):\n                parse_http_date(date)\n", "    def test_parsing_invalid_date(self):\n        invalid_dates = (\n            'invalid date',\n            'Sun, 06 Nov 1994 25:49:37 GMT',\n            'Tuesday, 31-Dec-69 99:49:37 GMT',\n            'Sun Nov  6 99:49:37 1994',\n            'Sun Nov 32 08:49:37 1994',\n        )\n        for date in invalid_dates:\n            with self.subTest(date=date):\n                with self.assertRaises(ValueError):\n                    parse_http_date(date)\n"], "sample_1038": ["def test_MatrixElement_diff_with_index_summation():\n    i, j, k = symbols('i j k')\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n\n    expr = Sum(A[i, j]*B[j, k], (j, 0, n-1))\n    result = MatrixExpr.from_index_summation(expr)\n    expected = A*B\n\n    assert result == expected\n", "def test_MatMul_simplify():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, k)\n\n    # Test simplification of MatMul with Identity\n    assert simplify(MatMul(A, Identity(m))) == A\n    assert simplify(MatMul(Identity(n), B)) == B\n\n    # Test simplification of MatMul with ZeroMatrix\n    assert simplify(MatMul(A, ZeroMatrix(m, l))) == ZeroMatrix(n, l)\n    assert simplify(MatMul(ZeroMatrix(m, n), B)) == ZeroMatrix(m, l)\n\n    # Test simplification of MatMul with scalars\n    assert simplify(MatMul(2, A)) == 2*A\n    assert simplify(MatMul(A, 2)) == 2*A\n\n    # Test simplification of MatMul with MatrixSymbols\n    assert simplify(MatMul(A, B, C)) == MatMul(A, MatMul(B, C))\n", "def test_MatMul_simplify_with_zeros():\n    A = MatrixSymbol(\"A\", 2, 2)\n    Z = ZeroMatrix(2, 2)\n    assert simplify(MatMul(A, Z)) == Z\n    assert simplify(MatMul(Z, A)) == Z\n", "def test_matrix_element_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    X = MatrixSymbol('X', n, n)\n    v = MatrixSymbol('v', n, 1)\n    w = MatrixSymbol('w', n, 1)\n\n    assert A[0, 0].diff(v[0, 0]) == 0\n    assert (A*B)[0, 0].diff(v[0, 0]) == 0\n    assert (A*B)[0, 0].diff(A[0, 0]) == B[0, 0]\n    assert (A*B)[0, 0].diff(B[0, 0]) == A[0, 0]\n    assert (Inverse(X)*v)[0, 0].diff(X[0, 0]) == -Inverse(X)[0, 0]*v[0, 0]*Inverse(X)[0, 0]\n    assert (Inverse(X)*v)[0, 0].diff(v[0, 0]) == 0\n", "def test_MatrixExpr_as_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    A_explicit = A.as_explicit()\n    assert A_explicit == Matrix([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]]])\n\n    # Test with a concrete matrix\n    B = Matrix([[1, 2], [3, 4]])\n    B_explicit = B.as_explicit()\n    assert B_explicit == B\n"], "sample_431": ["def test_refresh_fk_on_delete_cascade(self):\n    a = Article.objects.create(\n        headline=\"Parrot programs in Python\",\n        pub_date=datetime(2005, 7, 28),\n    )\n    s1 = SelfRef.objects.create(article=a)\n    a.delete()\n    with self.assertRaises(SelfRef.DoesNotExist):\n        s1.refresh_from_db()\n", "def test_refresh_no_fields_after_update(self):\n    a = Article.objects.create(pub_date=datetime.now())\n    a.headline = \"New headline\"\n    a.save(update_fields=[\"headline\"])\n    with self.assertNumQueries(0):\n        a.refresh_from_db(fields=[])\n    self.assertEqual(a.headline, \"New headline\")\n", "def test_refresh_clears_private_reverse_related(self):\n    \"\"\"refresh_from_db() clear cached private reverse relations.\"\"\"\n    article = Article.objects.create(\n        headline=\"Parrot programs in Python\",\n        pub_date=datetime(2005, 7, 28),\n    )\n    self.assertFalse(hasattr(article, \"featured_articles\"))\n    FeaturedArticle.objects.create(article_id=article.pk)\n    article.refresh_from_db()\n    self.assertTrue(hasattr(article, \"featured_articles\"))\n", "def test_refresh_clears_many_to_many_field(self):\n    article1 = Article.objects.create(\n        headline=\"Parrot programs in Python\",\n        pub_date=datetime(2005, 7, 28),\n    )\n    article2 = Article.objects.create(\n        headline=\"Django is awesome\",\n        pub_date=datetime(2022, 1, 1),\n    )\n    tag = Tag.objects.create(name=\"Python\")\n    article1.tags.add(tag)\n    article1.refresh_from_db()\n    self.assertCountEqual(article1.tags.all(), [tag])\n    article2.tags.add(tag)\n    article1.refresh_from_db(fields=[\"tags\"])\n    self.assertCountEqual(article1.tags.all(), [tag])\n    article2.tags.remove(tag)\n    article1.refresh_from_db(fields=[\"tags\"])\n    self.assertCountEqual(article1.tags.all(), [])\n", "def test_refresh_prefetched_cache_clear_on_specific_fields(self):\n    a = Article.objects.create(pub_date=datetime(2005, 7, 28))\n    s = SelfRef.objects.create(article=a)\n    # refresh_from_db() with fields=[...]\n    a_prefetched = Article.objects.prefetch_related(\"selfref_set\").first()\n    self.assertCountEqual(a_prefetched.selfref_set.all(), [s])\n    s.article = None\n    s.save()\n    # Relation is cleared and prefetch cache is stale.\n    self.assertCountEqual(a_prefetched.selfref_set.all(), [s])\n    a_prefetched.refresh_from_db(fields=[\"pub_date\"])\n    # Cache was not cleared since the refreshed fields did not include the prefetch relation.\n    self.assertCountEqual(a_prefetched.selfref_set.all(), [s])\n"], "sample_604": ["def test_short_data_repr():\n    cases = [\n        np.random.randn(500),\n        np.random.randn(20, 20),\n        np.random.randn(5, 10, 15),\n        np.random.randn(5, 10, 15, 3),\n        np.random.randn(100, 5, 1),\n    ]\n    for array in cases:\n        da = xr.DataArray(array)\n        result = formatting.short_data_repr(da)\n        assert len(result.splitlines()) == 1\n", "def test_inline_variable_array_repr_sparse():\n    import sparse\n\n    value = sparse.COO([[0, 1, 2], [0, 1, 2]], [1, 2, 3], shape=(3, 3))\n    variable = xr.Variable(\"x\", value)\n\n    max_width = 30\n    actual = formatting.inline_variable_array_repr(variable, max_width=max_width)\n\n    expected = \"<COO: nnz=3, fill_value=0>\"\n    assert actual == expected\n", "def test_unindexed_dims_repr():\n    ds = xr.Dataset(coords={\"foo\": [1, 2, 3]}, data_vars={\"bar\": ((\"x\",), [1, 2, 3])})\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    expected = \"Dimensions without coordinates: x\"\n    assert actual == expected\n\n    ds = xr.Dataset(coords={\"x\": [1, 2, 3]}, data_vars={\"bar\": ((\"x\",), [1, 2, 3])})\n    actual = formatting.unindexed_dims_repr(ds.dims, ds.coords)\n    assert actual is None\n", "def test_coords_repr():\n    coords = {\"x\": xr.DataArray([1, 2, 3], dims=\"x\"), \"y\": xr.DataArray([1, 2], dims=\"y\")}\n    col_width = 8\n    expected = \"Coordinates:\\n  * x        (x) int64 1 2 3\\n    y        (y) int64 1 2\"\n    actual = formatting.coords_repr(coords, col_width=col_width)\n    assert actual == expected\n", "def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days\", \"01:00:00\", \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days\", \"00:00:00\", \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days\", \"03:00:00\", \"0 days 03:00:00\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\", \"NaT\", \"NaT\"),\n    ]\n    for item, date_expected, time_expected, full_expected in cases:\n        date_actual = formatting.format_timedelta(item, timedelta_format=\"date\")\n        time_actual = formatting.format_timedelta(item, timedelta_format=\"time\")\n        full_actual = formatting.format_timedelta(item)\n        assert date_expected == date_actual\n        assert time_expected == time_actual\n        assert full_expected == full_actual\n"], "sample_917": ["def test_noexcept_expressions():\n    check('function', 'void f() noexcept', {1: 'f', 2: '1fv'})\n    check('function', 'void f() noexcept(true)', {1: 'f', 2: '1fv'})\n    check('function', 'void f() noexcept(false)', {1: 'f', 2: '1fv'})\n    check('function', 'void f() noexcept(42)', {1: 'f', 2: '1fv'})\n    check('function', 'void f() noexcept(g())', {1: 'f', 2: '1fv'})\n    check('function', 'void f() noexcept(noexcept(g()))', {1: 'f', 2: '1fv'})\n", "def test_xref_consistency(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?id=[\"\\'](?P<ids>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_pointer_to_member():\n    check('member', 'int A::*p;', {1: 'p__A::iP', 2: '1p'})\n    check('member', 'int A::*const p;', {1: 'p__A::iPK', 2: '1p'})\n    check('member', 'int A::*volatile p;', {1: 'p__A::iPV', 2: '1p'})\n    check('member', 'int A::*const volatile p;', {1: 'p__A::iPVC', 2: '1p'})\n", "def test_nested_namespace_definitions():\n    check('namespace', 'A::B::C', {3: \"N1A1B1CE\"})\n    check('class', 'A::B::C', {1: \"A::B::C\", 2: \"N1A1B1CE\"})\n    check('function', 'void A::B::C::f()', {1: \"A::B::C::f\", 2: \"N1A1B1C1Ev\"})\n", "def test_function_template_member():\n    check('function', 'template<class T> void A::f(T t)', {2: 'I0E1AN1A1fE1T'})\n    check('function', 'template<class T> void A::B::f(T t)', {2: 'I0E1AN1A1B1fE1T'})\n    check('function', 'template<class T> void A<T>::f()', {2: 'I0E1AI1T1fEv'})\n    check('function', 'template<class T> void A<T>::B::f()', {2: 'I0E1AI1T1B1fEv'})\n"], "sample_1159": ["def test_issue_16978():\n    c = Symbol('c', complex=True)\n    assert c.is_finite is True\n    raises(InconsistentAssumptions, lambda: Dummy(complex=True, finite=False))\n", "def test_symbol_extended_real_true():\n    # issue 3848\n    a = Symbol('a', extended_real=True)\n\n    assert a.is_real is True\n    assert a.is_integer is False\n    assert a.is_zero is False\n\n    assert a.is_negative is None\n    assert a.is_positive is None\n    assert a.is_nonnegative is None\n    assert a.is_nonpositive is None\n    assert a.is_nonzero is None\n\n    assert a.is_extended_negative is False\n    assert a.is_extended_positive is False\n    assert a.is_extended_nonnegative is None\n    assert a.is_extended_nonpositive is None\n    assert a.is_extended_nonzero is None\n", "def test_symbol_noninteger():\n    x = Symbol('x', noninteger=True)\n    assert x.is_integer is False\n    assert x.is_noninteger is True\n    assert x.is_positive is None\n    assert x.is_negative is None\n    assert x.is_zero is None\n", "def test_issue_16978():\n    # Test the assumption that complex numbers are finite\n    c = Symbol('c', complex=True)\n    assert c.is_finite is True\n    raises(InconsistentAssumptions, lambda: Dummy(complex=True, finite=False))\n", "def test_issue_18092():\n    x = Symbol('x', positive=True, integer=True)\n    y = Symbol('y', negative=True, integer=True)\n    assert (x + y).is_zero is None\n    assert (x - x).is_zero is True\n    assert (y - y).is_zero is True\n"], "sample_1173": ["def test_convert_xor():\n    transformations = standard_transformations + (convert_xor,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"2^3\", transformations=transformations) == 2**3\n    assert parse_expr(\"x^y\", transformations=transformations) == x**y\n", "def test_split_symbols_custom_predicate():\n        if symbol not in ('list', 'of', 'unsplittable', 'names'):\n                return _token_splittable(symbol)\n        return False\n\n    transformation = split_symbols_custom(can_split)\n    assert parse_expr('unsplittable', transformations=standard_transformations +\n        (transformation, implicit_multiplication)) == Symbol('unsplittable')\n", "def test_function_exponent_with_implicit_multiplication():\n    transformations = (standard_transformations +\n                       (implicit_multiplication_application, function_exponentiation))\n    x = Symbol('x')\n    assert parse_expr(\"sin x^2\", transformations=transformations) == (sin(x))**2\n    assert parse_expr(\"sin x^y\", transformations=transformations) == (sin(x))**y\n", "def test_split_symbols_underscore():\n    transformations = standard_transformations + \\\n                      (split_symbols, implicit_multiplication,)\n    x_y = Symbol('x_y')\n\n    assert parse_expr(\"x_y\") == x_y\n    assert parse_expr(\"x_y\", transformations=transformations) == x_y\n", "def test_parse_expr_with_custom_transformations():\n        result = []\n        for toknum, tokval in tokens:\n            if toknum == NAME and tokval == 'CUSTOM_FUNC':\n                result.append((NAME, 'Function'))\n                result.append((OP, '('))\n                result.append((NAME, \"'custom_func'\"))\n                result.append((OP, ')'))\n            else:\n                result.append((toknum, tokval))\n        return result\n\n    x = Symbol('x')\n    custom_func = Function('custom_func')\n    assert parse_expr('CUSTOM_FUNC(x)', local_dict={'x': x}, transformations=(custom_transform,)) == custom_func(x)\n"], "sample_1034": ["def test_grover_with_different_functions():\n    nqubits = 3\n        return qubits == IntQubit(3, nqubits=qubits.nqubits)\n    assert apply_grover(return_one_on_three, nqubits) == IntQubit(3, nqubits=nqubits)\n\n        return qubits == IntQubit(4, nqubits=qubits.nqubits)\n    assert apply_grover(return_one_on_four, nqubits) == IntQubit(4, nqubits=nqubits)\n", "def test_grover_oracle_function_complexity():\n    # Test that the OracleGate works correctly with a more complex function\n        return qubits.int_form % 2 == 0\n\n    nqubits = 3\n    v = OracleGate(nqubits, return_one_on_even)\n    for i in range(2**nqubits):\n        qubit = IntQubit(i, nqubits=nqubits)\n        if i % 2 == 0:\n            assert qapply(v*qubit) == -qubit\n        else:\n            assert qapply(v*qubit) == qubit\n", "def test_grover_measurement():\n    nqubits = 2\n    state = apply_grover(return_one_on_one, nqubits)\n    measurement = qapply(state, measurements=[(0, 1)])\n    assert measurement.measure(0) == 1\n", "def test_grover_with_custom_iterations():\n    nqubits = 3\n    iterations = 5\n    basis_states = superposition_basis(nqubits)\n    v = OracleGate(nqubits, return_one_on_one)\n    iterated = basis_states\n    for _ in range(iterations):\n        iterated = grover_iteration(iterated, v)\n        iterated = qapply(iterated)\n    expected = apply_grover(return_one_on_one, nqubits, iterations)\n    assert iterated == expected\n", "def test_grover_multiple_iterations():\n    nqubits = 2\n    v = OracleGate(nqubits, return_one_on_one)\n    iterated = superposition_basis(nqubits)\n    for _ in range(5):\n        iterated = grover_iteration(iterated, v)\n        iterated = qapply(iterated)\n    assert iterated == IntQubit(1, nqubits=nqubits)\n"], "sample_437": ["    def test_validate_thread_sharing_allowed(self):\n        connection.inc_thread_sharing()\n        self.addCleanup(connection.dec_thread_sharing)\n        connection.validate_thread_sharing()\n", "    def setUp(self):\n        self.conn = connections[DEFAULT_DB_ALIAS]\n        self.conn.close()\n        self.addCleanup(self.conn.close)\n", "def test_health_checks_disabled_errors_occurred(self):\n    self.patch_settings_dict(conn_health_checks=False)\n    self.assertIsNone(connection.connection)\n    # Newly created connections are considered healthy without performing\n    # the health check.\n    with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n        self.run_query()\n\n    old_connection = connection.connection\n    # Simulate errors_occurred.\n    connection.errors_occurred = True\n    # Simulate request_started (the connection is healthy).\n    connection.close_if_unusable_or_obsolete()\n    # Persistent connections are enabled.\n    self.assertIs(old_connection, connection.connection)\n    # Health checks are not performed.\n    with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n        self.run_query()\n        # The connection is unchanged after the next query during the current\n        # \"request\" when errors occurred.\n        self.assertIs(old_connection, connection.connection)\n", "def test_set_autocommit_health_checks_disabled(self):\n    self.patch_settings_dict(conn_health_checks=False)\n    self.assertIsNone(connection.connection)\n    # Newly created connections are considered healthy without performing\n    # the health check.\n    with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n        # Simulate outermost atomic block: changing autocommit for\n        # a connection.\n        connection.set_autocommit(False)\n        self.run_query()\n        connection.commit()\n        connection.set_autocommit(True)\n\n    old_connection = connection.connection\n    # Simulate request_finished.\n    connection.close_if_unusable_or_obsolete()\n    # Persistent connections are enabled.\n    self.assertIs(old_connection, connection.connection)\n\n    # Simulate outermost atomic block: changing autocommit for\n    # a connection.\n    connection.set_autocommit(False)\n    # Health checks are not performed.\n    with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n        self.run_query()\n        connection.commit()\n        connection.set_autocommit(True)\n        # The connection is unchanged.\n        self.assertIs(old_connection, connection.connection)\n\n    # Simulate request_finished.\n    connection.close_if_unusable_or_obsolete()\n    # The underlying connection is being reused further as health checks\n    # are disabled.\n    connection.set_autocommit(False)\n    self.run_query()\n    connection.commit()\n    connection.set_autocommit(True)\n    self.assertIs(old_connection, connection.connection)\n", "def test_close_if_unusable_or_obsolete_broken_autocommit(self):\n    self.patch_settings_dict(conn_health_checks=True)\n    self.assertIsNone(connection.connection)\n    connection.set_autocommit(False)\n    with patch.object(connection, \"is_usable\", return_value=True):\n        connection.close_if_unusable_or_obsolete()\n    self.assertIsNone(connection.connection)\n"], "sample_1155": ["def test_complex_numbers():\n    assert construct_domain([1 + 2*I, 3 - 4*I]) == (CC, [CC(1, 2), CC(3, -4)])\n", "def test_complex_numbers():\n    assert construct_domain([1 + I, 2 - 3*I, 4*I]) == (CC, [CC(1, 1), CC(2, -3), CC(0, 4)])\n", "def test_construct_domain_with_complex_float():\n    complex_float = 3.14 + 2.71j\n    result = construct_domain([complex_float])\n    assert isinstance(result[0], ComplexField)\n    assert result[1] == [CC(3.14, 2.71)]\n", "def test_construct_domain_with_algebraic_and_rational():\n    alg = QQ.algebraic_field(sqrt(2))\n    assert construct_domain([7, S.Half, sqrt(2)], extension=True) == \\\n        (alg, [alg.convert(7), alg.convert(S.Half), alg.convert(sqrt(2))])\n    assert construct_domain([7, sqrt(2), S.Half], extension=True) == \\\n        (alg, [alg.convert(7), alg.convert(sqrt(2)), alg.convert(S.Half)])\n", "def test_complex_numbers():\n    assert construct_domain([1 + I]) == (CC, [CC(1, 1)])\n    assert construct_domain([2 + 3*I]) == (CC, [CC(2, 3)])\n    assert construct_domain([1 + I/2]) == (QQ_I, [QQ_I(1, 1/2)])\n    assert construct_domain([I/2]) == (QQ_I, [QQ_I(0, 1/2)])\n    assert construct_domain([1, 2*I]) == (ZZ_I, [ZZ_I(1, 0), ZZ_I(0, 2)])\n    assert construct_domain([3.14 + 2.71j]) == (CC, [CC(3.14, 2.71)])\n"], "sample_1037": ["def test_MatMul_transpose():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert (A*B).T == B.T*A.T\n", "def test_matmul_determinant():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert (A*B).det() == A.det() * B.det()\n", "def test_matmul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    expr = A * B * C\n    result = expr.doit()\n    assert result == MatMul(MatMul(A, B).doit(), C).doit()\n", "def test_MatMul_simplify():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = A * B\n    simplified_C = simplify(C)\n    assert simplified_C == C, \"Simplify should not change the expression\"\n", "def test_Matrix_doit():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatMul(A, B)\n    D = C.doit()\n    assert D.shape == (n, l)\n    assert all(D[i, j] == Sum(A[i, k]*B[k, j], (k, 0, m-1)) for i in range(n) for j in range(l))\n"], "sample_1063": ["def test_abs_numpy():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    f = lambdify(x, Abs(x), modules=\"numpy\")\n    assert f(-1) == 1\n    assert f(1) == 1\n    assert f(3+4j) == 5\n", "def test_abs_math():\n    f = lambdify(x, Abs(x), modules=\"math\")\n    assert f(-1) == 1\n    assert f(1) == 1\n", "def test_lambdify_function_arg_with_underscore():\n    f = Function('f_')(x)\n    g = lambdify(f, f + 1)\n    assert g(2) == 3\n", "def test_abs_complex():\n    f = lambdify(x, Abs(x))\n    assert f(2+3j) == 5\n", "def test_issue_17211():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    x = symbols('x')\n    f = x**2\n    f_ = lambdify(x, f, modules='numpy')\n    assert f_(2) == 4\n"], "sample_586": ["def test_concat_positions(self):\n    ds1 = Dataset({\"x\": (\"y\", [1, 2]), \"y\": [0, 1]})\n    ds2 = Dataset({\"x\": (\"y\", [3, 4]), \"y\": [0, 1]})\n    expected = Dataset({\"x\": (\"z\", [3, 1, 2, 4]), \"y\": [0, 1], \"z\": [0, 1, 1, 2]})\n    actual = concat([ds1, ds2], dim=\"z\", positions=[[1, 3], [0, 2]])\n    assert_identical(actual, expected)\n", "def test_concat_with_indexer(self):\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[0]])}, coords={\"x\": [0], \"y\": [0]})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[0]])}, coords={\"x\": [1], \"y\": [1]})\n\n    expected = Dataset(\n        {\"a\": ((\"x\", \"y\"), [[0], [0]])},\n        {\"x\": [0, 1], \"y\": [0, 1]},\n    )\n\n    actual = concat([ds1, ds2], dim=\"z\", positions=[[0], [1]])\n    assert_equal(actual, expected)\n", "def test_concat_different_coords(self):\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2])}, {\"x\": [0, 1], \"y\": (\"x\", [1, 2])})\n    ds2 = Dataset({\"foo\": (\"x\", [3, 4])}, {\"x\": [0, 1], \"y\": (\"x\", [3, 4])})\n\n    result = concat([ds1, ds2], dim=\"x\", coords=\"different\")\n    expected = Dataset(\n        {\"foo\": (\"x\", [1, 2, 3, 4]), \"y\": (\"x\", [1, 2, 3, 4])}, {\"x\": [0, 1, 0, 1]}\n    )\n    assert_identical(result, expected)\n", "def test_concat_incompatible_datasets(self):\n    ds1 = Dataset({\"x\": (\"t\", np.random.randn(10))}, coords={\"t\": np.arange(10)})\n    ds2 = Dataset({\"x\": (\"t\", np.random.randn(15))}, coords={\"t\": np.arange(15)})\n\n    with raises_regex(ValueError, \"variables are not equal\"):\n        concat([ds1, ds2], dim=\"t\")\n", "def test_concat_position_kwarg(self):\n    data = Dataset({\"foo\": DataArray([1, 2, 3], coords=[(\"x\", [1, 2, 3])])})\n    split_data = [data.isel(x=slice(1)), data.isel(x=slice(1, None))]\n    positions = [[0, 2], [1]]\n    expected = Dataset({\"foo\": DataArray([1, 3, 2], coords=[(\"x\", [1, 3, 2])])})\n    actual = concat(split_data, dim=\"x\", positions=positions)\n    assert_identical(actual, expected)\n"], "sample_780": ["def test_lda_transform_input_format():\n    # Test LDA transform for sparse and dense input\n    # result should be the same for both dense and sparse input\n    rng = np.random.RandomState(0)\n    X = rng.randint(5, size=(20, 10))\n    n_components = 3\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=rng)\n    X_trans_dense = lda.fit_transform(X)\n    X_trans_sparse = lda.fit_transform(csr_matrix(X))\n    assert_array_almost_equal(X_trans_dense, X_trans_sparse)\n", "def test_lda_fit_transform_with_subsampling():\n    # Test LDA fit_transform & transform with sub-sampling\n    # fit_transform and transform result should be the same\n    rng = np.random.RandomState(0)\n    X = rng.randint(10, size=(50, 20))\n    lda = LatentDirichletAllocation(n_components=5, learning_method='batch',\n                                    random_state=rng)\n    X_fit = lda.fit_transform(X)\n    X_trans = lda.transform(X)\n    X_trans_subsampling = lda.transform(X, sub_sampling=True)\n    assert_array_almost_equal(X_fit, X_trans, 4)\n    assert X_trans_subsampling.shape == X_trans.shape\n", "def test_lda_fit_transform_online_vs_batch():\n    # Test LDA fit_transform & transform with online vs batch learning\n    # fit_transform and transform results should be similar for both methods\n    rng = np.random.RandomState(0)\n    X = rng.randint(10, size=(50, 20))\n    lda_online = LatentDirichletAllocation(n_components=5, learning_method='online', random_state=rng)\n    lda_batch = LatentDirichletAllocation(n_components=5, learning_method='batch', random_state=rng)\n    X_fit_online = lda_online.fit_transform(X)\n    X_fit_batch = lda_batch.fit_transform(X)\n    assert_array_almost_equal(X_fit_online, X_fit_batch, decimal=2)\n", "def test_lda_fit_transform_shape():\n    # Test LDA fit_transform shape\n    # The shape of the output of fit_transform should match the expected shape\n    rng = np.random.RandomState(0)\n    n_samples = rng.randint(6, 10)\n    n_features = rng.randint(6, 10)\n    n_components = rng.randint(3, 6)\n    X = rng.randint(4, size=(n_samples, n_features))\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=rng)\n    X_fit = lda.fit_transform(X)\n    assert_equal(X_fit.shape, (n_samples, n_components))\n", "def test_lda_input_validation():\n    # Test LDA input validation\n    # Test with non-positive n_components\n    assert_raises_regexp(ValueError, r\"^Invalid 'n_components' parameter: -5$\",\n                         LatentDirichletAllocation, n_components=-5)\n\n    # Test with non-positive total_samples\n    assert_raises_regexp(ValueError, r\"^Invalid 'total_samples' parameter: -10$\",\n                         LatentDirichletAllocation, total_samples=-10)\n\n    # Test with non-positive learning_offset\n    assert_raises_regexp(ValueError, r\"^Invalid 'learning_offset' parameter: -2$\",\n                         LatentDirichletAllocation, learning_offset=-2)\n\n    # Test with invalid learning_method\n    assert_raises_regexp(ValueError, r\"^Invalid 'learning_method' parameter: 'invalid'$\",\n                         LatentDirichletAllocation, learning_method='invalid')\n"], "sample_1075": ["def test_beta_real():\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n\n    assert beta(x, y).is_real is True\n    assert beta(x, -y).is_real is False\n    assert beta(-x, y).is_real is False\n    assert beta(-x, -y).is_real is True\n", "def test_beta_properties():\n    x, y = Symbol('x'), Symbol('y')\n\n    # Test the property B(a, 1) = 1/a\n    assert beta(x, 1) == 1/x\n\n    # Test the property B(a, b) = B(b, a)\n    assert beta(x, y) == beta(y, x)\n\n    # Test the property B(a, b) = \u0393(a)\u0393(b)/\u0393(a+b)\n    assert beta(x, y) == gamma(x)*gamma(y)/gamma(x + y)\n\n    # Test the property for integral values of a and b\n    a, b = Symbol('a', integer=True), Symbol('b', integer=True)\n    assert beta(a, b) == factorial(a - 1)*factorial(b - 1)/factorial(a + b - 1)\n", "def test_beta_is_real():\n    x, y = Symbol('x'), Symbol('y')\n    i = Symbol('I')\n\n    assert beta(x, y).is_real == (x.is_real and y.is_real)\n    assert beta(1 + i, 1 + i).is_real is False\n", "def test_beta_properties():\n    x, y = Symbol('x'), Symbol('y')\n\n    assert beta(x, 1) == 1/x  # Beta function property\n    assert beta(x, y) == beta(y, x)  # Beta function symmetry property\n    assert beta(x, y)._eval_is_real() is (x.is_real and y.is_real)  # Real property\n    assert beta(x, y)._eval_conjugate() == beta(x.conjugate(), y.conjugate())  # Conjugate property\n", "def test_beta_properties():\n    x, y = Symbol('x'), Symbol('y')\n\n    assert beta(x, 1) == 1/x  # Property 1: Beta(a, 1) = 1/a\n    assert beta(x, y) == beta(y, x)  # Property 2: Beta(a, b) = Beta(b, a)\n\n    # Property 3: For integral values of a and b\n    # Beta(a, b) = (a-1)! * (b-1)! / (a+b-1)!\n    a, b = 3, 4\n    expected = factorial(a - 1) * factorial(b - 1) / factorial(a + b - 1)\n    assert beta(a, b) == expected\n\n    # Property 4: Beta function is real for real arguments\n    assert beta(x, y).is_real == (x.is_real and y.is_real)\n"], "sample_906": ["def test_domain_cpp_parse_noindexentry_for_member(app):\n    text = (\".. cpp:class:: MyClass\\n\"\n            \"   :noindex:\\n\"\n            \"   .. cpp:member:: int myMember\\n\"\n            \"   .. cpp:member:: int myOtherMember\\n\"\n            \"      :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[])\n    assert_node(doctree[2], addnodes.index, entries=[])\n", "def test_domain_cpp_ast_parse_invalid_name():\n    with pytest.raises(DefinitionError):\n        parse('invalid_name', 'void f()')\n", "def test_domain_cpp_parse_template_param_qualified_name(app, warning):\n    text = (\".. cpp:type:: T::typeWarn\\n\"\n            \".. cpp:type:: T::U::typeWarn\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"WARNING: cpp:type reference target not found: T::typeWarn\" in ws[0]\n    assert \"WARNING: cpp:type reference target not found: T::U::typeWarn\" in ws[2]\n    assert ws[4] == \"\"\n", "def test_domain_cpp_ast_template_id():\n    # Check that the template ID is correctly generated for a function template\n    check('function', 'template<typename T> void f(T t)', {1: 'f__T', 2: 'I0E1f1T'})\n\n    # Check that the template ID is correctly generated for a class template\n    check('class', 'template<typename T> class A', {1: 'A', 2: 'I0E1A'})\n\n    # Check that the template ID is correctly generated for a member function template\n    check('member', 'template<typename T> void A::f(T t)', {1: 'A::f__T', 2: 'I0E1f1T'})\n\n    # Check that the template ID is correctly generated for a member variable template\n    check('member', 'template<typename T> T A::t', {1: 'A::t__T', 2: 'I0E1t'})\n\n    # Check that the template ID is correctly generated for a function template with default arguments\n    check('function', 'template<typename T = int> void f(T t)', {1: 'f__T', 2: 'I0E1f1T'})\n\n    # Check that the template ID is correctly generated for a nested template\n    check('function', 'template<typename T> void f(std::vector<T> v)', {1: 'f__std::vector:T:', 2: '1fNSt6vectorITEE'})\n", "def test_domain_cpp_parse_function_declaration_with_explicit(app, warning):\n    text = \".. cpp:function:: explicit void f()\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc,))\n    assert_node(doctree[0], desc, domain='cpp', objtype='function', desctype='function')\n    assert_node(doctree[0], desc, noindex=False)\n    assert_node(doctree[0], desc, addnodes.desc_signature, classes=['sig', 'sig-object', 'cpp', 'function'])\n    assert_node(doctree[0].children[1], addnodes.desc_content)\n    assert_node(doctree[0].children[1], desc, domain='cpp', objtype='functionParam', desctype='functionParam')\n    assert_node(doctree[0].children[1], desc, noindex=False)\n    assert_node(doctree[0].children[1], addnodes.desc_signature, classes=['sig', 'sig-object', 'cpp', 'functionParam'])\n    assert_node(doctree[0].children[1].children[0], addnodes.desc_name, 'f')\n    assert_node(doctree[0].children[1].children[1], addnodes.desc_parameterlist)\n    assert_node(doctree[0].children[1].children[1], addnodes.desc_parameter, 'explicit', classes=['sig-name', 'sig-name-keyword'])\n"], "sample_825": ["def test_pls_svd_algorithm():\n    # Test the svd algorithm for PLSCanonical\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    n_components = 2\n\n    pls_nipals = pls_.PLSCanonical(algorithm=\"nipals\", n_components=n_components)\n    pls_svd = pls_.PLSCanonical(algorithm=\"svd\", n_components=n_components)\n\n    pls_nipals.fit(X, Y)\n    pls_svd.fit(X, Y)\n\n    assert_array_almost_equal(pls_nipals.x_scores_, pls_svd.x_scores_, decimal=5)\n    assert_array_almost_equal(pls_nipals.y_scores_, pls_svd.y_scores_, decimal=5)\n    assert_array_almost_equal(pls_nipals.x_weights_, pls_svd.x_weights_, decimal=5)\n    assert_array_almost_equal(pls_nipals.y_weights_, pls_svd.y_weights_, decimal=5)\n    assert_array_almost_equal(pls_nipals.x_loadings_, pls_svd.x_loadings_, decimal=5)\n    assert_array_almost_equal(pls_nipals.y_loadings_, pls_svd.y_loadings_, decimal=5)\n", "def test_pls_svd_algorithm():\n    # Test the svd algorithm for PLSCanonical and PLSRegression\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(algorithm=\"svd\"), pls_.PLSRegression(algorithm=\"svd\")]:\n        clf.fit(X, Y)\n        assert_equal(clf.n_iter_, [])\n", "def test_pls_algorithm_choice():\n    # Test that the 'algorithm' parameter works as expected\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls_nipals = pls_.PLSCanonical(algorithm='nipals')\n    pls_svd = pls_.PLSCanonical(algorithm='svd')\n    pls_nipals.fit(X, Y)\n    pls_svd.fit(X, Y)\n    assert_array_almost_equal(pls_nipals.x_scores_, pls_svd.x_scores_, decimal=5,\n                              err_msg=\"nipals and svd implementations lead to different x scores\")\n    assert_array_almost_equal(pls_nipals.y_scores_, pls_svd.y_scores_, decimal=5,\n                              err_msg=\"nipals and svd implementations lead to different y scores\")\n", "def test_pls_regression_predict():\n    # Test the predict method of PLSRegression\n    X = np.array([[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls_2 = pls_.PLSRegression(n_components=X.shape[1])\n    pls_2.fit(X, Y)\n\n    # Predict on the same data and check that the output is close to the target\n    Y_pred = pls_2.predict(X)\n    assert_array_almost_equal(Y_pred, Y, decimal=2)\n\n    # Predict on new data and check that the output has the correct shape\n    X_new = np.array([[3., 3., 3.]])\n    Y_new_pred = pls_2.predict(X_new)\n    assert_equal(Y_new_pred.shape, (1, 2))\n", "def test_pls_mode_B():\n    # Test for mode B in PLSRegression\n    X = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls_b = pls_.PLSRegression(n_components=2, mode=\"B\")\n    pls_b.fit(X, Y)\n    # Assertions to check the results of mode B PLSRegression\n    # Add appropriate assertions here based on the expected results\n"], "sample_1004": ["def test_CondSet_union():\n    input_conditionset1 = ConditionSet(x, x > 0, Interval(1, 4, False, False))\n    input_conditionset2 = ConditionSet(x, x < 3, Interval(2, 5, False, False))\n    output_conditionset = ConditionSet(x, And(x > 0, x < 3), Interval(1, 4, False, False))\n    assert Union(input_conditionset1, input_conditionset2) == output_conditionset\n", "def test_CondSet_union():\n    input_conditionset1 = ConditionSet(x, x > 5, Interval(1, 7, False, False))\n    input_conditionset2 = ConditionSet(x, x < 2, Interval(0, 3, False, False))\n    output_conditionset = ConditionSet(x, Or(x > 5, x < 2), Interval(0, 7, False, False))\n    assert Union(input_conditionset1, input_conditionset2) == output_conditionset\n", "def test_CondSet_Union():\n    input_conditionset1 = ConditionSet(x, x**2 > 4, Interval(1, 4, False, False))\n    input_conditionset2 = ConditionSet(x, x < 2, Interval(2, 5, False, False))\n    output_conditionset = ConditionSet(x, (x**2 > 4) | (x < 2), Interval(1, 5, False, False))\n    assert Union(input_conditionset1, input_conditionset2) == output_conditionset\n", "def test_condition_set_base_set_subs():\n    c = ConditionSet(x, x < 2, Interval(0, 5))\n    assert c.subs(Interval(0, 5), Interval(1, 4)) == ConditionSet(x, x < 2, Interval(1, 4))\n    assert c.subs(Interval(0, 5), Interval(2, 3)) == EmptySet()\n", "def test_CondSet_union():\n    input_conditionset1 = ConditionSet(x, x**2 > 4, Interval(1, 4, False, False))\n    input_conditionset2 = ConditionSet(x, x < 0, Interval(-2, 2, False, False))\n    output_conditionset = Union(ConditionSet(x, x**2 > 4, Interval(1, 2, False, False)), ConditionSet(x, x < 0, Interval(-2, 0, False, False)))\n    assert Union(input_conditionset1, input_conditionset2) == output_conditionset\n"], "sample_958": ["def test_domain_cpp_parse_noindex(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :noindex:\\n\"\n            \".. cpp:function:: void g()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (desc, desc))\n    assert_node(doctree[0], desc, noindex=True)\n    assert_node(doctree[1], desc, noindex=None)\n", "def test_domain_cpp_parse_mix_decl_duplicate_with_option(app, warning):\n    # Issue 8270\n    text = (\".. cpp:struct:: A\\n\"\n            \"   :noindex:\\n\"\n            \".. cpp:function:: void A()\\n\"\n            \".. cpp:struct:: A\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[0]\n    assert \"Declaration is '.. cpp:struct:: A'.\" in ws[1]\n    assert \"index.rst:4: WARNING: Duplicate C++ declaration, also defined at index:1.\" in ws[2]\n    assert \"Declaration is '.. cpp:struct:: A'.\" in ws[3]\n    assert ws[4] == \"\"\n", "def test_domain_cpp_ast_namespaces():\n    check('class', 'namespace::A', {2: 'N9namespace1AE'})\n    check('class', 'namespace::A::B', {2: 'N9namespace1A1BE'})\n    check('function', 'void f(namespace::A a)', {2: '1fN9namespace1AE'})\n    check('function', 'void f(namespace::A::B a)', {2: '1fN9namespace1A1BE'})\n", "def test_domain_cpp_ast_function_definitions_nested_namespaces():\n    check('function', 'namespace ns { void f(); }', {1: \"ns::f\", 2: \"N2ns1fv\"})\n    check('function', 'namespace ns1 { namespace ns2 { void f(); } }', {1: \"ns1::ns2::f\", 2: \"N4ns13ns21fv\"})\n", "def test_domain_cpp_parse_noindex(app):\n    text = (\".. cpp:struct:: A\\n\"\n            \"   :noindex:\\n\"\n            \".. cpp:function:: void A::f()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[])\n"], "sample_303": ["    def test_runshell_extend_env(self):\n        env = {'TEST_VAR': 'test_value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], env),\n            ):\n                self.client.runshell(None)\n            expected_env = {**os.environ, **env}\n            run.assert_called_once_with([], env=expected_env, check=True)\n", "def test_runshell_use_provided_env(self):\n    provided_env = {'TEST_ENV': 'test_value'}\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=([], provided_env),\n        ):\n            self.client.runshell(None)\n        expected_env = {**os.environ, **provided_env}\n        run.assert_called_once_with([], env=expected_env, check=True)\n", "    def test_runshell_use_provided_env(self):\n        provided_env = {'TEST_ENV': 'test_value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], provided_env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with([], env={**os.environ, **provided_env}, check=True)\n", "def test_runshell_merge_env(self):\n    new_env = {'TEST_VAR': 'test_value'}\n    with mock.patch('subprocess.run') as run:\n        with mock.patch.object(\n            BaseDatabaseClient,\n            'settings_to_cmd_args_env',\n            return_value=([], new_env),\n        ):\n            self.client.runshell(None)\n        expected_env = {**os.environ, **new_env}\n        run.assert_called_once_with([], env=expected_env, check=True)\n", "    def test_runshell_combine_env(self):\n        test_env = {'TEST_VAR': 'test_value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], test_env),\n            ):\n                self.client.runshell(None)\n            expected_env = {**os.environ, **test_env}\n            run.assert_called_once_with([], env=expected_env, check=True)\n"], "sample_1126": ["def test_operator():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_operators():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_operator_dagger():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_operator_dagger():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_operator_dagger():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n\n"], "sample_1117": ["def test_positive_definite_implies_symmetric():\n    X = MatrixSymbol('X', 3, 3)\n    assert ask(Q.positive_definite(X), Q.symmetric(X)) is True\n", "def test_orthogonal_symmetric():\n    X = MatrixSymbol('X', 3, 3)\n    assert ask(Q.orthogonal(X), Q.symmetric(X))\n    assert ask(Q.symmetric(X), Q.orthogonal(X)) is None\n", "def test_matrix_element_sets_hadamard_product():\n    from sympy.matrices.expressions.hadamard import HadamardProduct\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.integer_elements(HadamardProduct(X, Y)), Q.integer_elements(X) & Q.integer_elements(Y))\n", "def test_matrix_element_sets_hadamard_product():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert ask(Q.integer_elements(HadamardProduct(X, Y)),\n                       Q.integer_elements(X) & Q.integer_elements(Y))\n", "def test_positive_elements():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert ask(Q.positive(X[0, 0]), Q.positive_definite(X))\n    assert ask(Q.positive(X + Y), Q.positive_elements(X) & Q.positive_elements(Y))\n    assert not ask(Q.positive(X - Y), Q.positive_elements(X) & Q.positive_elements(Y))\n    assert ask(Q.positive(X * Y), Q.positive_elements(X) & Q.positive_elements(Y))\n    assert not ask(Q.positive(X / Y), Q.positive_elements(X) & Q.positive_elements(Y))\n    assert ask(Q.positive(X.T), Q.positive_elements(X))\n    assert ask(Q.positive(X.I), Q.positive_elements(X) & Q.invertible(X))\n"], "sample_1035": ["def test_measure_all_oneshot():\n    nqubits = 3\n    state = IntQubit(3, nqubits=nqubits)\n    result = measure_all_oneshot(state)\n    assert result == state\n", "def test_measure_all():\n    nbits = 2\n    state = IntQubit(0, nqubits=nbits)/2 + IntQubit(1, nqubits=nbits)/2\n    measurement_result = measure_all(state)\n    assert len(measurement_result) == 2\n    assert measurement_result[0][0] in [IntQubit(0, nbits), IntQubit(1, nbits)]\n    assert measurement_result[1][0] in [IntQubit(0, nbits), IntQubit(1, nbits)]\n    assert measurement_result[0][1] == measurement_result[1][1] == 0.5\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import Qubit, measure_all\n    from sympy.physics.quantum.gate import H\n    from sympy.physics.quantum.qapply import qapply\n\n    c = H(0)*H(1)*Qubit('00')\n    q = qapply(c)\n    result = measure_all(q)\n    expected_result = [(Qubit('00'), 1/4), (Qubit('01'), 1/4), (Qubit('10'), 1/4), (Qubit('11'), 1/4)]\n    assert result == expected_result\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import measure_all, Qubit\n    from sympy.physics.quantum.gate import H, X\n    from sympy.physics.quantum.qapply import qapply\n\n    q = H(0) * Qubit('0')\n    q = qapply(q)\n    result = measure_all(q)\n    assert len(result) == 2\n    assert (Qubit('0'), 1/2) in result\n    assert (Qubit('1'), 1/2) in result\n", "def test_measure_partial():\n    nqubits = 2\n    basis_states = superposition_basis(nqubits)\n    measurement_result = measure_partial(basis_states, (0,))\n    possible_outcomes = [(sqrt(2)*IntQubit(0, nqubits)/2 + sqrt(2)*IntQubit(2, nqubits)/2, 1/2),\n                         (sqrt(2)*IntQubit(1, nqubits)/2 + sqrt(2)*IntQubit(3, nqubits)/2, 1/2)]\n    assert measurement_result == possible_outcomes\n"], "sample_1116": ["def test_inverse_determinant():\n    from sympy import det\n    assert Inverse(C)._eval_determinant() == 1/det(C)\n", "def test_inverse_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 3, 3, complex=True)\n    dA = A.diff(x)\n    assert Inverse(A).diff(x) == -Inverse(A)*dA*Inverse(A)\n", "def test_inverse_matpow_simplification():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert Inverse(MatPow(A*B, 2)).doit() == Inverse(B)*Inverse(A)*Inverse(B)*Inverse(A)\n", "def test_inverse_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2, real=True)\n    B = Matrix([[x, x], [x, x]])\n    assert Inverse(A).diff(x) == -A.I * A.diff(x) * A.I\n    assert Inverse(B).diff(x) == -B.I * B.diff(x) * B.I\n", "def test_inverse_determinant():\n    import sympy\n    A = MatrixSymbol('A', 2, 2)\n    a, b, c, d = symbols('a b c d')\n    A[0, 0] = a\n    A[0, 1] = b\n    A[1, 0] = c\n    A[1, 1] = d\n    assert Inverse(A)._eval_determinant() == 1 / sympy.det(A)\n"], "sample_779": ["def test_check_class_weight_balanced_linear_classifier_no_exception():\n    # check that well-computed balanced weights does not raise an exception\n    check_class_weight_balanced_linear_classifier('estimator_name', LinearRegression)\n", "def test_check_fit_score_takes_y_works_on_class_with_no_y_in_fit():\n    # Tests that check_fit_score_takes_y does not raise an error on a class\n    # with a fit method that does not take a y argument\n\n    class TestEstimatorWithoutYInFit(BaseEstimator):\n            return self\n\n    check_fit_score_takes_y(\"test\", TestEstimatorWithoutYInFit())\n", "def test_check_no_attributes_set_in_init_exclude_deprecated():\n    # check that check_no_attributes_set_in_init() does not fail on\n    # deprecated estimators\n    class DeprecatedEstimator:\n            self.old_param = old_param\n            self.new_param = new_param\n\n        @deprecated(\"Use new_param instead.\")\n            self.old_param = old_param\n\n    check_no_attributes_set_in_init(\"DeprecatedEstimator\", DeprecatedEstimator())\n", "def test_check_fit_score_takes_y_works_on_deprecated_fit():\n    # Tests that check_fit_score_takes_y works on a class with\n    # a deprecated fit method\n\n    class TestEstimatorWithDeprecatedFitMethod(BaseEstimator):\n        @deprecated(\"Deprecated for the purpose of testing check_fit_score_takes_y\")\n            return self\n\n    check_fit_score_takes_y(\"test\", TestEstimatorWithDeprecatedFitMethod())\n", "def test_check_estimator_changes_attribute_during_predict():\n    # check that check_estimator() raises an error when estimator changes\n    # an attribute during predict\n    msg = \"Estimator ChangesDict changes __dict__ during predict\"\n    assert_raises_regex(AssertionError, msg, check_estimator, ChangesDict)\n"], "sample_454": ["def test_validate_expression_with_opclass(self):\n    constraint = models.UniqueConstraint(\n        models.OpClass(Lower(\"name\"), \"varchar_pattern_ops\"),\n        name=\"name_lower_pattern_uniq\",\n    )\n    msg = \"Constraint \u201cname_lower_pattern_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper()),\n        exclude={\"name\"},\n    )\n", "def test_validate_expression_opclass(self):\n    constraint = models.UniqueConstraint(\n        models.Func(models.F(\"name\"), function=\"LOWER\"),\n        name=\"name_lower_opclass_uniq\",\n        opclasses=[\"varchar_pattern_ops\"],\n    )\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper())\n    msg = \"Constraint \u201cname_lower_opclass_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n", "def test_validate_expression_custom_error(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    constraint.violation_error_message = \"Custom message\"\n    constraint.violation_error_code = \"custom_code\"\n    msg = \"Custom message\"\n    with self.assertRaisesMessage(ValidationError, msg) as cm:\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    self.assertEqual(cm.exception.code, \"custom_code\")\n", "def test_unique_constraint_with_opclass_database_constraint(self):\n    from django.contrib.postgres.indexes import OpClass\n\n    UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n    constraint = models.UniqueConstraint(\n        OpClass(\"color\", \"varchar_pattern_ops\"),\n        name=\"color_opclass_uniq\",\n    )\n    # This should not raise an IntegrityError as the OpClass does not affect\n    # the unique constraint behavior.\n    UniqueConstraintInclude.objects.create(name=\"p1\", color=\"RED\")\n", "def test_validate_expression_opclass(self):\n    from django.contrib.postgres.indexes import OpClass\n\n    constraint = models.UniqueConstraint(\n        OpClass(\"name\", \"varchar_pattern_ops\"), name=\"name_opclass_uniq\"\n    )\n    msg = \"Constraint \u201cname_opclass_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"another-name\"),\n    )\n    # Existing instances have their existing row excluded.\n    constraint.validate(UniqueConstraintProduct, self.p1)\n    # Unique field is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=self.p1.name.upper()),\n        exclude={\"name\"},\n    )\n"], "sample_1087": ["def test_f_polys():\n    from sympy.polys.specialpolys import f_polys\n    from sympy.polys.rings import ring\n\n    R, x, y, z = ring(\"x,y,z\", ZZ)\n    f_0_expected = 2115*x**4*y + 45*x**3*z**3*y - 45*x**3*y - 423*x*y**4 - 47*x*y**3 + 141*x*y*z**3 + 94*x*y*z - 9*y**3*z**3 + 9*y**3 - y**2*z**3 + y**2 - 3*z**6 + 2*z**4*y - 3*z**3 + 2*z*y\n    f_1_expected = 3*x**5*y**2 + x**4*z**4 + x**4 + x**3*y**3*z + x**3*z + x**2*y**4 + x**2*y**3*z**3 + x**2*y*z**5 + x**2*y*z + x*y**2*z**4 + x*y**2 + x*y*z**7 + x*y*z**3 + x*y*z**2 + y**2*z + y*z**4\n    f_2_expected = -x**3 - 3*x**2*y + 3*x**2*z - 3*x*y**2 + 6*x*y*z - 3*x*z**2 - y**3 + 3*y**2*z - 3*y*z**2 + z**3\n    f_3_expected = 24*x**8*y**3 + 48*x**8*y**2 + 24*x**7*y**5 - 72*x**7*y**2 + 25*x**6*y**4 + 2*x**6*y**3 + 4*x**", "def test_f_polys():\n    from sympy.polys.specialpolys import f_polys\n\n    # Testing the polynomials f_0 to f_6\n    for i, f in enumerate(f_polys()):\n        # Check that the degree of each polynomial matches the expected value\n        assert f.degree() == [6, 5, 5, 5, 9, 3, 4][i]\n        # Check that the coefficients of each polynomial are integers\n        assert all(c.is_Integer for c in f.coeffs())\n", "def test_random_poly_domain():\n    poly = random_poly(x, 10, -100, 100, domain=QQ, polys=False)\n\n    assert Poly(poly).degree() == 10\n    assert all(-100 <= coeff <= 100 for coeff in Poly(poly).coeffs()) is True\n    assert all(coeff.is_Rational for coeff in Poly(poly).coeffs()) is True\n\n    poly = random_poly(x, 10, -100, 100, domain=QQ, polys=True)\n\n    assert poly.degree() == 10\n    assert all(-100 <= coeff <= 100 for coeff in poly.coeffs()) is True\n    assert all(coeff.is_Rational for coeff in poly.coeffs()) is True\n", "def test_f_polys():\n    from sympy.polys.specialpolys import f_polys\n\n    # Add your assertions here to test the output of f_polys()\n    # For example, you can check the degree, number of terms, coefficients, etc.\n    # Below is a placeholder assertion that always passes\n    assert f_polys() is not None\n", "def test_f_polys():\n    from sympy.polys.specialpolys import f_polys\n    from sympy.polys.rings import ring\n\n    # Define the expected results for the polynomials\n    expected_results = [\n        # Provide the expected results for _f_0(), _f_1(), ..., _f_6()\n        # in the same order as they are returned by f_polys()\n    ]\n\n    # Get the polynomials from the f_polys() function\n    polynomials = f_polys()\n\n    # Check if the number of polynomials is correct\n    assert len(polynomials) == len(expected_results)\n\n    # Check each polynomial against the expected result\n    for poly, expected_result in zip(polynomials, expected_results):\n        assert poly == expected_result\n\n    # Define a different ring for the polynomials\n    R, x, y, z = ring(\"x,y,z\", ZZ)\n\n    # Check each polynomial against the expected result using the new ring\n    for poly, expected_result in zip(polynomials, expected_results):\n        assert poly.set_ring(R) == expected_result.set_ring(R)\n"], "sample_243": ["def test_filter_with_f_expression(self):\n    query = Query(Item)\n    where = query.build_where(Q(name=F('creator__name')))\n    name_exact = where.children[0]\n    self.assertIsInstance(name_exact, Exact)\n    self.assertIsInstance(name_exact.lhs, Col)\n    self.assertEqual(name_exact.lhs.target, Item._meta.get_field('name'))\n    self.assertIsInstance(name_exact.rhs, Col)\n    self.assertEqual(name_exact.rhs.target, Author._meta.get_field('name'))\n", "def test_add_filtered_relation(self):\n    query = Query(Item)\n    filtered_relation = FilteredRelation('creator', condition=Q(creator__num__gt=2))\n    query.add_filtered_relation(filtered_relation, 'creator_alias')\n    self.assertEqual(query._filtered_relations['creator_alias'], filtered_relation)\n", "def test_filter_with_lookup_expression(self):\n    query = Query(Author)\n    where = query.build_where(Q(num__exact=F('id')))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Col)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n    self.assertIsInstance(lookup.rhs, Col)\n    self.assertEqual(lookup.rhs.target, Author._meta.get_field('id'))\n", "def test_complex_query_with_transform(self):\n    query = Query(Author)\n    with register_lookup(CharField, Lower):\n        where = query.build_where(Q(name__lower__contains='foo') | Q(num__lt=0))\n    self.assertEqual(where.connector, OR)\n\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, Contains)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup.lhs.lhs, Col)\n    self.assertEqual(lookup.lhs.lhs.target, Author._meta.get_field('name'))\n\n    lookup = where.children[1]\n    self.assertIsInstance(lookup, LessThan)\n    self.assertEqual(lookup.rhs, 0)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_filter_with_lookup_value_iterable(self):\n    query = Query(Item)\n    where = query.build_where(Q(name__in=['a', 'b']))\n    name_in = where.children[0]\n    self.assertIsInstance(name_in, In)\n    self.assertIsInstance(name_in.rhs, tuple)\n    self.assertEqual(name_in.rhs, ('a', 'b'))\n"], "sample_1025": ["def test_issue_14284():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(oo) == \"float('inf')\"\n", "def test_NumPyPrinter_Pow():\n    p = NumPyPrinter()\n    expr = x**0.5\n    assert p.doprint(expr) == 'numpy.sqrt(x)'\n    expr = x**2\n    assert p.doprint(expr) == 'x**2'\n", "def test_PythonCodePrinter_function_definition():\n    f = Function('f')(x)\n    fd = FunctionDefinition(f, x, x**2)\n    assert PythonCodePrinter().doprint(fd) == \"def f(x):\\n    x**2\"\n", "def test_Additional_Functionality():\n    p = PythonCodePrinter()\n    expr = x / y\n    assert p.doprint(expr) == 'x / y'\n    expr = x // y\n    assert p.doprint(expr) == 'x // y'\n    expr = x ** y\n    assert p.doprint(expr) == 'x ** y'\n    expr = -x\n    assert p.doprint(expr) == '-x'\n    expr = +x\n    assert p.doprint(expr) == '+x'\n", "def test_PythonCodePrinter_functions():\n    p = PythonCodePrinter()\n    # Testing custom function\n    p.known_functions['CustomFunc'] = 'custom_module.custom_func'\n    expr = CustomFunc(x, y)\n    assert p.doprint(expr) == 'custom_module.custom_func(x, y)'\n    assert 'custom_module' in p.module_imports\n    assert 'custom_func' in p.module_imports['custom_module']\n"], "sample_976": ["def test_Symbol_assumptions():\n    a = Symbol(\"a\", commutative=False)\n    b = Symbol(\"b\", commutative=False)\n    assert a * b != b * a\n    assert a * b * 2 == 2 * a * b\n\n    x = Symbol(\"x\", real=True)\n    assert x.is_real\n    assert not x.is_integer\n\n    y = Symbol(\"y\", integer=True)\n    assert y.is_integer\n    assert not y.is_real\n\n    z = Symbol(\"z\", positive=True)\n    assert z.is_positive\n    assert z.is_real\n    assert not z.is_integer\n\n    w = Symbol(\"w\", negative=True)\n    assert w.is_negative\n    assert w.is_real\n    assert not w.is_integer\n", "def test_symbol_assumptions():\n    # Test the assumption handling in Symbol.__new__\n    x = Symbol('x', real=True)\n    assert x.is_real is True\n    assert x.is_integer is False\n    assert x.is_complex is False\n\n    y = Symbol('y', integer=True)\n    assert y.is_real is True\n    assert y.is_integer is True\n    assert y.is_complex is False\n\n    z = Symbol('z', complex=True)\n    assert z.is_real is False\n    assert z.is_integer is False\n    assert z.is_complex is True\n\n    # Test invalid assumptions\n    raises(ValueError, lambda: Symbol('w', commutative=None))\n    raises(ValueError, lambda: Symbol('v', bounded=True))\n", "def test_symbols_as_function():\n    f = symbols('f', cls=Function)\n    x = Symbol('x')\n    assert f(x).is_Function\n    assert f(x).args == (x,)\n    assert f(x).func == f\n", "def test_constant():\n    x, y, z = symbols('x y z')\n    a = Symbol('a', constant=True)\n\n    assert x.is_constant() is False\n    assert a.is_constant(x, y, z) is True\n    assert a.is_constant(x) is False\n    assert a.is_constant(y) is True\n    assert a.is_constant() is False\n    assert a.is_constant(a) is True\n", "def test_Symbol_commutativity():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert x*y == y*x  # Commutativity by default\n\n    x = Symbol('x', commutative=False)\n    y = Symbol('y', commutative=False)\n\n    assert x*y != y*x  # Non-commutativity when specified\n\n    # Testing the assumption error\n    try:\n        Symbol('z', commutative=None)\n        raise AssertionError(\"Expected ValueError\")\n    except ValueError:\n        pass\n"], "sample_907": ["def test_domain_cpp_parse_mix_decl_noindexentry(app):\n    text = (\".. cpp:struct:: A\\n\"\n            \".. cpp:function:: void A()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. cpp:struct:: A\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'A (C++ struct)', '_CPPv41A', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[])\n", "def test_domain_cpp_parse_mix_decl_duplicate_in_class(app, warning):\n    text = (\".. cpp:class:: A\\n\"\n            \"   .. cpp:function:: void f()\\n\"\n            \"   .. cpp:function:: void f()\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"index.rst:3: WARNING: Duplicate C++ declaration, also defined at index:2.\" in ws[0]\n    assert \"Declaration is '.. cpp:function:: void f()'.\" in ws[1]\n    assert ws[2] == \"\"\n    assert ws[3] == \"\"\n    assert ws[4] == \"\"\n", "def test_domain_cpp_parse_duplicate_id(app, warning):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :id: myid\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \"   :id: myid\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 5\n    assert \"index.rst:2: WARNING: Duplicate ID myid, also defined at index:1.\" in ws[0]\n    assert \"Declaration is '.. cpp:function:: void f()'.\" in ws[1]\n    assert \"index.rst:4: WARNING: Duplicate ID myid, also defined at index:1.\" in ws[2]\n    assert \"Declaration is '.. cpp:function:: void g()'.\" in ws[3]\n    assert ws[4] == \"\"\n", "def test_domain_cpp_parse_duplicate_template_params(app, warning):\n    # Issue 8906\n    text = (\".. cpp:class:: template<typename T, typename T> A\\n\"\n            \".. cpp:class:: template<typename U, typename U> B\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 2\n    assert \"index.rst:1: WARNING: Duplicate template parameter 'T' in C++ declaration.\" in ws[0]\n    assert \"index.rst:2: WARNING: Duplicate template parameter 'U' in C++ declaration.\" in ws[1]\n", "def test_domain_cpp_ast_qualified_name_in_template():\n    check('class', 'template<typename T> {key}A<T::type>', {2: 'I0E1AI1T4typeEE'})\n    check('class', 'template<typename T> {key}A<T::U::type>', {2: 'I0E1AIN1T1U4typeEE'})\n"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(np.array(feature_names_t), feature_names_out)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n\n    # Check error if input_features does not match with feature_names_in_\n    with pytest.raises(ValueError, match=\"input_features is not equal to feature_names_in_\"):\n        sel.get_feature_names_out(input_features=feature_names[:-1])\n"]}, "raw_scored_generations": {"sample_0": [false, false, true, false, false], "sample_3": [true, true, false, false, false], "sample_1": [false, false, false, false, false], "sample_5": [false, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_2": [false, false, true, true, true], "sample_6": [true, false, false, false, false], "sample_7": [false, false, false, true, true], "sample_8": [false, false, false, false, false], "sample_9": [false, false, false, false, false], "sample_10": [true, false, false, true, false], "sample_11": [true, false, false, true, true], "sample_12": [true, false, false, true, false], "sample_13": [false, false, false, false, false], "sample_14": [false, false, false, false, false], "sample_15": [false, false, false, false, false], "sample_16": [false, false, false, true, true], "sample_17": [false, false, false, false, false], "sample_18": [false, false, false, false, false], "sample_19": [false, false, false, false, false], "sample_20": [false, true, false, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [false, false, false, false, false], "sample_24": [false, true, false, false, false], "sample_25": [false, false, false, false, false], "sample_26": [false, false, false, false, false], "sample_27": [false, false, true, false, false], "sample_28": [false, false, false, false, false], "sample_29": [false, false, false, false, false], "sample_30": [true, true, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [true, false, false, false, false], "sample_33": [true, false, false, false, true], "sample_34": [true, false, true, false, false], "sample_35": [false, false, false, false, false], "sample_36": [true, false, false, true, true], "sample_37": [false, false, false, false, false], "sample_38": [false, false, false, false, true], "sample_39": [false, false, false, true, false], "sample_40": [true, false, true, true, false], "sample_41": [true, true, true, true, false], "sample_42": [true, true, false, false, false], "sample_43": [true, false, true, false, false], "sample_44": [false, false, false, false, false], "sample_45": [false, true, true, true, true], "sample_46": [true, true, false, true, true], "sample_47": [true, true, true, false, true], "sample_48": [true, true, true, true, true], "sample_49": [false, false, false, false, false], "sample_50": [true, true, true, true, true], "sample_51": [true, false, true, true, false], "sample_52": [true, false, true, true, false], "sample_54": [true, true, true, true, true], "sample_53": [true, true, true, true, true], "sample_55": [false, true, true, true, true], "sample_58": [true, false, true, true, false], "sample_56": [true, true, true, true, true], "sample_57": [false, true, false, true, true], "sample_59": [true, true, true, true, true], "sample_60": [true, false, false, false, true], "sample_61": [true, true, true, false, true], "sample_62": [true, true, true, true, true], "sample_63": [false, false, false, false, false], "sample_64": [true, false, false, false, true], "sample_65": [true, true, true, true, true], "sample_67": [true, false, true, true, true], "sample_66": [true, true, true, true, true], "sample_68": [false, true, true, false, false], "sample_69": [true, true, true, true, true], "sample_70": [false, true, false, false, true], "sample_71": [false, true, false, true, false], "sample_72": [true, true, true, true, true], "sample_73": [false, false, false, false, false], "sample_75": [true, true, true, true, true], "sample_74": [true, true, true, true, true], "sample_76": [true, true, true, true, true], "sample_77": [true, true, true, true, true], "sample_78": [true, true, true, true, true], "sample_79": [false, false, false, false, false], "sample_80": [true, true, true, true, true], "sample_82": [true, true, true, true, true], "sample_81": [true, true, true, true, true], "sample_83": [false, false, false, false, false], "sample_85": [true, true, true, true, true], "sample_84": [false, false, true, true, true], "sample_86": [true, true, true, false, true], "sample_88": [true, false, false, true, true], "sample_87": [true, true, true, false, false], "sample_89": [true, true, true, false, true], "sample_90": [true, false, true, true, true], "sample_91": [true, true, false, true, true], "sample_92": [false, false, false, false, false], "sample_93": [true, true, true, true, true], "sample_94": [true, true, false, true, false], "sample_95": [false, false, true, false, false], "sample_98": [false, false, false, false, false], "sample_96": [true, true, true, true, true], "sample_99": [false, true, true, true, true], "sample_97": [true, true, true, true, true], "sample_100": [true, true, true, true, true], "sample_102": [true, true, true, true, true], "sample_101": [false, true, false, true, true], "sample_103": [true, true, true, true, true], "sample_104": [true, true, true, true, true], "sample_107": [false, true, true, false, true], "sample_106": [true, true, true, true, true], "sample_105": [true, false, false, true, true], "sample_108": [true, true, true, true, true], "sample_109": [true, true, true, true, false], "sample_111": [true, true, true, true, true], "sample_110": [true, true, true, true, true], "sample_112": [true, true, true, true, true], "sample_113": [true, true, false, false, false], "sample_114": [true, true, true, true, true], "sample_115": [true, true, false, false, false], "sample_116": [true, true, false, true, true], "sample_117": [false, false, true, false, false], "sample_118": [false, true, false, true, true], "sample_119": [true, true, true, true, true], "sample_120": [true, false, false, false, false], "sample_121": [true, true, false, true, false], "sample_122": [true, true, true, false, true], "sample_123": [false, true, false, false, true], "sample_124": [true, true, true, false, false], "sample_125": [true, true, true, true, true], "sample_126": [true, true, true, true, true], "sample_127": [true, true, true, true, true], "sample_128": [false, false, true, false, false], "sample_129": [false, true, true, true, true], "sample_130": [true, true, true, true, true], "sample_131": [false, false, false, false, false], "sample_132": [false, false, false, false, false], "sample_133": [true, true, true, true, true], "sample_135": [true, true, true, true, true], "sample_134": [true, true, true, true, true], "sample_136": [true, true, true, true, true], "sample_139": [true, false, true, false, false], "sample_137": [true, true, true, false, true], "sample_138": [true, false, true, true, true], "sample_140": [false, false, false, false, false], "sample_141": [true, true, true, true, true], "sample_142": [true, true, true, true, true], "sample_143": [false, true, true, true, true], "sample_144": [true, true, true, true, true], "sample_145": [true, true, true, true, true], "sample_146": [true, true, true, false, true], "sample_147": [true, true, true, true, true], "sample_148": [true, false, true, true, true], "sample_151": [true, true, false, true, true], "sample_149": [true, true, false, false, false], "sample_152": [true, true, false, false, false], "sample_150": [true, true, true, true, true], "sample_153": [true, true, true, true, true], "sample_154": [true, true, false, true, true], "sample_155": [true, true, true, true, true], "sample_156": [true, true, false, false, true], "sample_157": [false, false, false, false, false], "sample_158": [true, true, true, true, true], "sample_159": [true, true, false, false, true], "sample_160": [true, true, false, true, false], "sample_161": [true, true, true, true, true], "sample_162": [true, true, true, true, true], "sample_163": [true, true, false, true, false], "sample_164": [false, false, false, false, true], "sample_165": [true, true, true, true, true], "sample_166": [true, true, true, true, true], "sample_167": [false, false, false, false, false], "sample_168": [true, true, true, false, true], "sample_169": [true, true, true, true, true], "sample_171": [true, true, true, true, true], "sample_170": [false, true, false, false, false], "sample_172": [true, false, true, true, true], "sample_173": [true, true, true, true, true], "sample_174": [true, true, true, true, true], "sample_175": [true, false, false, false, true], "sample_176": [true, true, true, true, true], "sample_177": [true, true, true, true, false], "sample_178": [true, true, false, true, true], "sample_180": [true, true, false, true, true], "sample_179": [true, true, true, true, true], "sample_182": [true, true, true, true, true], "sample_181": [true, true, true, true, true], "sample_183": [true, true, true, true, true], "sample_184": [true, false, true, true, true], "sample_185": [true, false, true, false, false], "sample_186": [true, true, true, true, true], "sample_187": [true, true, true, true, true], "sample_188": [true, false, true, true, true], "sample_189": [true, true, true, true, true], "sample_190": [true, true, true, true, true], "sample_191": [true, false, true, true, true], "sample_192": [true, true, true, true, true], "sample_193": [true, true, true, true, true], "sample_194": [true, true, true, false, false], "sample_195": [true, true, true, true, true], "sample_196": [true, true, true, true, true], "sample_198": [true, true, true, true, true], "sample_197": [true, false, true, true, true], "sample_199": [true, true, true, true, true], "sample_200": [true, true, true, true, false], "sample_201": [true, true, true, true, true], "sample_202": [true, true, true, true, true], "sample_203": [true, false, false, true, true], "sample_204": [true, true, true, true, true], "sample_205": [true, false, true, true, true], "sample_206": [true, true, true, true, true], "sample_207": [true, true, true, true, true], "sample_208": [true, true, true, true, false], "sample_209": [true, true, true, true, true], "sample_210": [false, false, false, true, true], "sample_211": [false, true, false, true, true], "sample_213": [true, true, true, true, false], "sample_212": [true, false, false, false, true], "sample_214": [true, true, true, true, true], "sample_215": [false, false, false, false, false], "sample_216": [true, true, true, true, true], "sample_217": [true, false, false, false, false], "sample_218": [true, true, false, true, true], "sample_219": [true, true, true, true, true], "sample_220": [true, true, true, true, true], "sample_221": [true, true, true, true, true], "sample_222": [true, true, false, true, true], "sample_223": [true, true, true, false, true], "sample_224": [true, true, true, true, true], "sample_225": [false, false, false, true, false], "sample_226": [false, false, false, true, false], "sample_227": [true, true, true, true, false], "sample_228": [true, false, true, true, true], "sample_229": [true, true, true, true, true], "sample_230": [false, true, false, true, true], "sample_231": [true, false, false, true, true], "sample_232": [true, true, true, true, true], "sample_233": [true, true, true, true, true], "sample_234": [true, true, true, true, true], "sample_235": [true, true, true, false, false], "sample_236": [true, false, true, false, true], "sample_237": [true, false, true, true, true], "sample_238": [true, true, true, true, true], "sample_239": [true, false, false, true, true], "sample_240": [true, true, true, false, true], "sample_241": [true, true, false, false, true], "sample_242": [true, false, true, true, true], "sample_243": [true, true, true, true, true], "sample_244": [true, true, true, true, true], "sample_245": [true, true, true, true, false], "sample_246": [true, true, true, true, true], "sample_247": [true, true, true, true, true], "sample_248": [true, true, true, true, true], "sample_249": [true, false, true, false, false], "sample_250": [true, true, true, true, true], "sample_251": [true, true, true, true, true], "sample_252": [true, true, true, true, true], "sample_253": [true, true, true, true, false], "sample_254": [true, true, false, true, true], "sample_256": [true, true, false, false, true], "sample_255": [true, true, true, false, false], "sample_257": [true, true, true, true, true], "sample_258": [false, true, false, false, false], "sample_259": [true, true, true, true, true], "sample_260": [true, true, true, true, true], "sample_261": [true, false, false, true, true], "sample_262": [true, false, true, false, false], "sample_263": [false, true, true, true, true], "sample_264": [true, true, true, true, true], "sample_265": [true, true, true, true, true], "sample_266": [true, true, false, true, true], "sample_267": [true, false, true, true, false], "sample_268": [true, true, true, true, true], "sample_269": [true, true, true, true, true], "sample_270": [true, true, true, true, true], "sample_271": [true, true, true, true, true], "sample_272": [false, false, false, false, false], "sample_273": [true, true, false, false, false], "sample_274": [true, true, true, true, true], "sample_275": [true, true, true, true, false], "sample_276": [true, true, true, true, true], "sample_277": [true, true, false, true, true], "sample_278": [true, true, true, false, true], "sample_279": [true, true, false, true, true], "sample_280": [true, true, true, true, true], "sample_281": [true, true, true, true, true], "sample_282": [true, true, true, true, true], "sample_283": [true, true, true, true, true], "sample_284": [true, true, true, true, false], "sample_285": [true, true, true, true, true], "sample_286": [true, true, true, true, false], "sample_287": [true, true, true, false, false], "sample_288": [true, true, true, true, true], "sample_289": [true, true, true, true, true], "sample_290": [true, true, true, false, true], "sample_291": [false, true, true, false, false], "sample_292": [true, true, true, true, true], "sample_293": [false, false, true, false, false], "sample_294": [true, true, true, true, true], "sample_295": [true, true, true, true, true], "sample_296": [true, true, true, true, true], "sample_297": [false, false, false, false, false], "sample_298": [true, true, true, true, true], "sample_299": [true, false, true, true, true], "sample_300": [true, true, true, true, true], "sample_301": [true, true, true, false, false], "sample_302": [true, true, true, true, true], "sample_303": [false, true, false, true, false], "sample_304": [true, false, true, true, false], "sample_305": [true, true, true, true, true], "sample_306": [false, false, true, false, false], "sample_307": [true, true, true, true, true], "sample_308": [false, true, true, true, false], "sample_309": [false, false, false, true, true], "sample_310": [true, true, true, true, true], "sample_312": [true, true, true, true, true], "sample_311": [true, true, true, false, true], "sample_313": [true, true, true, true, false], "sample_314": [false, true, true, false, true], "sample_315": [false, false, false, false, false], "sample_316": [true, true, true, true, true], "sample_317": [true, true, true, true, true], "sample_318": [true, false, false, false, true], "sample_319": [true, false, true, true, false], "sample_320": [true, false, false, true, true], "sample_321": [true, true, true, true, true], "sample_322": [true, true, false, true, false], "sample_323": [false, true, false, true, false], "sample_324": [true, true, true, true, true], "sample_325": [true, true, false, false, false], "sample_326": [true, true, true, true, true], "sample_327": [false, true, false, false, true], "sample_328": [true, true, true, true, true], "sample_329": [true, false, true, true, true], "sample_330": [false, false, false, false, true], "sample_331": [true, false, false, true, true], "sample_332": [true, false, true, true, false], "sample_333": [true, true, true, true, true], "sample_334": [true, true, true, false, false], "sample_335": [true, true, true, true, true], "sample_336": [true, false, true, false, true], "sample_337": [true, true, true, true, true], "sample_338": [true, true, true, true, true], "sample_339": [true, true, true, true, true], "sample_340": [true, true, true, true, true], "sample_341": [true, true, true, true, false], "sample_342": [true, false, true, true, true], "sample_343": [true, true, true, true, true], "sample_344": [true, true, true, true, false], "sample_345": [true, false, true, true, true], "sample_346": [false, false, false, false, false], "sample_347": [true, true, true, true, true], "sample_348": [true, false, false, false, false], "sample_349": [false, true, true, true, false], "sample_350": [true, true, true, true, true], "sample_351": [true, true, true, true, true], "sample_352": [true, false, false, false, true], "sample_353": [true, true, true, true, true], "sample_354": [true, true, false, false, false], "sample_355": [false, false, false, false, false], "sample_356": [false, false, false, true, true], "sample_357": [true, false, false, true, false], "sample_358": [false, false, false, false, true], "sample_359": [true, true, false, true, false], "sample_360": [true, true, false, true, true], "sample_361": [true, true, true, true, true], "sample_362": [false, true, true, true, false], "sample_363": [true, true, true, true, true], "sample_364": [true, true, false, true, false], "sample_365": [false, false, false, true, false], "sample_366": [false, true, true, true, false], "sample_367": [false, false, false, false, false], "sample_368": [false, true, true, true, true], "sample_369": [false, false, false, false, true], "sample_370": [false, true, true, true, true], "sample_371": [true, true, true, false, false], "sample_372": [false, false, false, false, false], "sample_373": [true, true, false, false, false], "sample_374": [true, true, true, true, true], "sample_375": [true, false, true, true, false], "sample_376": [true, true, true, true, true], "sample_377": [true, false, false, false, true], "sample_378": [true, true, true, true, true], "sample_379": [false, false, true, true, true], "sample_380": [true, true, true, true, true], "sample_381": [true, true, false, false, false], "sample_382": [false, true, true, false, true], "sample_383": [true, false, false, true, false], "sample_384": [true, true, true, true, false], "sample_385": [true, true, true, true, false], "sample_386": [true, true, true, true, false], "sample_387": [true, true, true, true, true], "sample_388": [false, false, false, false, false], "sample_389": [false, false, false, true, false], "sample_390": [false, true, false, false, false], "sample_391": [true, true, true, true, true], "sample_392": [true, true, true, true, true], "sample_393": [true, true, true, true, true], "sample_394": [true, true, false, false, true], "sample_395": [true, false, false, true, true], "sample_396": [true, false, true, false, false], "sample_397": [true, true, true, true, true], "sample_398": [false, false, true, false, false], "sample_399": [true, true, true, true, true], "sample_400": [false, true, true, false, false], "sample_401": [true, false, true, true, false], "sample_402": [true, true, true, true, true], "sample_403": [true, true, false, true, true], "sample_404": [true, true, true, true, true], "sample_405": [true, true, true, true, true], "sample_406": [true, true, true, true, true], "sample_407": [true, true, true, true, true], "sample_408": [false, true, true, false, false], "sample_409": [true, true, true, true, true], "sample_410": [true, true, false, true, false], "sample_411": [true, true, true, true, true], "sample_412": [true, true, true, true, true], "sample_413": [true, true, false, false, false], "sample_414": [true, true, false, true, true], "sample_415": [true, true, true, true, true], "sample_416": [true, true, true, true, true], "sample_417": [true, true, true, true, true], "sample_418": [true, true, true, true, true], "sample_419": [true, true, true, true, true], "sample_420": [true, true, false, true, true], "sample_421": [true, true, true, true, true], "sample_422": [true, true, true, true, true], "sample_423": [true, true, true, true, true], "sample_424": [true, true, true, true, false], "sample_425": [true, false, true, false, true], "sample_426": [true, false, true, true, true], "sample_427": [true, true, true, true, true], "sample_428": [false, true, false, true, false], "sample_429": [true, false, false, true, false], "sample_430": [true, true, false, false, true], "sample_431": [true, true, true, true, true], "sample_432": [true, true, true, true, true], "sample_433": [true, false, true, true, true], "sample_434": [true, false, false, true, true], "sample_435": [false, true, false, false, false], "sample_436": [true, true, true, true, false], "sample_437": [false, false, true, true, true], "sample_438": [true, true, true, true, true], "sample_439": [true, true, true, false, false], "sample_440": [true, true, true, true, true], "sample_441": [true, false, true, true, true], "sample_442": [true, false, true, true, true], "sample_443": [false, false, true, true, true], "sample_444": [true, false, true, true, true], "sample_445": [false, true, true, true, true], "sample_446": [true, true, false, false, true], "sample_447": [true, true, true, true, true], "sample_448": [true, true, false, false, false], "sample_449": [false, true, true, true, false], "sample_450": [true, true, false, false, true], "sample_451": [true, true, true, true, true], "sample_453": [true, true, true, true, true], "sample_452": [true, true, true, false, false], "sample_454": [true, true, true, true, true], "sample_455": [true, true, true, true, true], "sample_456": [false, true, true, true, true], "sample_457": [true, true, true, false, true], "sample_458": [true, false, true, false, true], "sample_459": [true, true, true, true, true], "sample_460": [true, true, true, true, true], "sample_461": [true, true, true, true, true], "sample_462": [true, true, true, true, true], "sample_463": [true, true, true, false, false], "sample_464": [true, false, true, true, true], "sample_465": [false, true, true, true, true], "sample_466": [true, true, true, true, true], "sample_467": [true, false, false, true, true], "sample_469": [true, true, true, true, true], "sample_468": [true, true, true, true, true], "sample_470": [true, true, false, true, false], "sample_471": [true, true, true, true, true], "sample_472": [true, false, true, false, true], "sample_473": [false, false, false, false, false], "sample_474": [true, false, true, true, false], "sample_475": [true, true, true, true, false], "sample_476": [false, false, true, false, false], "sample_477": [true, true, true, true, true], "sample_478": [true, true, true, true, true], "sample_479": [true, true, true, true, true], "sample_480": [true, true, true, true, true], "sample_481": [true, true, true, true, true], "sample_482": [true, true, true, true, true], "sample_483": [true, true, true, true, true], "sample_484": [true, true, false, true, true], "sample_485": [true, true, true, true, true], "sample_486": [true, true, true, true, true], "sample_487": [true, true, true, false, true], "sample_488": [true, true, false, true, true], "sample_489": [true, true, true, true, true], "sample_490": [false, true, true, false, true], "sample_491": [true, false, true, true, true], "sample_492": [true, true, true, false, true], "sample_493": [true, true, true, true, true], "sample_494": [true, true, true, true, false], "sample_495": [true, true, false, true, true], "sample_496": [true, false, true, true, false], "sample_497": [true, false, false, false, false], "sample_498": [true, true, false, false, false], "sample_499": [true, false, false, true, false], "sample_500": [false, true, false, false, false], "sample_501": [true, true, false, false, false], "sample_502": [false, true, true, false, false], "sample_503": [true, false, false, false, true], "sample_504": [false, false, false, true, false], "sample_505": [false, true, false, false, false], "sample_506": [false, false, false, false, false], "sample_507": [false, false, false, true, false], "sample_508": [true, false, false, false, false], "sample_509": [false, false, false, false, false], "sample_510": [true, false, false, false, true], "sample_511": [true, true, true, false, false], "sample_512": [true, false, true, false, false], "sample_513": [true, false, true, true, false], "sample_514": [false, false, false, false, false], "sample_515": [true, false, false, true, false], "sample_516": [false, false, false, false, false], "sample_517": [false, true, false, false, false], "sample_518": [false, false, false, true, false], "sample_519": [false, true, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [true, true, false, false, false], "sample_523": [false, false, true, false, false], "sample_524": [true, false, false, false, false], "sample_525": [false, true, false, false, false], "sample_526": [false, true, false, false, false], "sample_527": [false, false, false, false, false], "sample_528": [false, false, true, false, true], "sample_529": [false, false, false, false, false], "sample_530": [false, false, false, false, false], "sample_531": [false, false, true, false, false], "sample_532": [true, false, false, false, false], "sample_533": [false, false, false, false, false], "sample_534": [false, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, true, false], "sample_537": [false, false, false, false, false], "sample_538": [true, false, false, false, true], "sample_539": [true, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [true, true, false, false, false], "sample_542": [false, true, false, false, false], "sample_543": [true, true, false, false, false], "sample_544": [true, true, true, true, false], "sample_545": [false, false, false, false, true], "sample_546": [false, true, false, true, true], "sample_547": [true, false, false, false, false], "sample_548": [false, true, true, false, false], "sample_549": [false, true, true, false, false], "sample_550": [false, false, false, false, false], "sample_551": [false, false, false, false, true], "sample_552": [false, false, false, false, false], "sample_553": [false, false, false, false, false], "sample_554": [true, true, true, true, true], "sample_555": [false, true, true, false, false], "sample_556": [false, true, false, true, false], "sample_557": [false, false, true, true, false], "sample_558": [false, false, false, false, true], "sample_559": [false, false, false, true, false], "sample_560": [false, false, false, false, true], "sample_561": [false, true, false, true, true], "sample_562": [false, false, false, true, false], "sample_563": [false, false, true, false, false], "sample_564": [true, false, false, true, false], "sample_565": [false, false, false, true, true], "sample_566": [true, false, false, false, false], "sample_567": [false, true, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, false], "sample_571": [false, false, false, false, false], "sample_572": [false, false, false, false, false], "sample_573": [false, false, false, true, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, false, false, false], "sample_577": [false, false, false, false, false], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [true, false, true, false, true], "sample_581": [false, false, false, false, false], "sample_582": [true, true, false, false, false], "sample_583": [true, false, true, false, true], "sample_584": [false, false, false, false, false], "sample_585": [false, true, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, false], "sample_590": [false, false, false, false, false], "sample_591": [false, false, false, false, false], "sample_592": [false, false, false, true, false], "sample_593": [true, false, true, false, true], "sample_594": [false, false, false, false, true], "sample_595": [true, false, true, false, false], "sample_596": [false, false, true, true, false], "sample_597": [false, false, false, false, false], "sample_598": [false, false, false, false, false], "sample_599": [false, false, false, false, true], "sample_600": [true, true, false, false, false], "sample_601": [false, false, false, false, true], "sample_602": [false, false, false, false, false], "sample_603": [false, false, false, false, true], "sample_604": [false, false, true, false, false], "sample_605": [false, false, false, false, true], "sample_606": [false, false, false, false, false], "sample_607": [false, false, false, false, false], "sample_608": [false, false, false, false, false], "sample_609": [false, true, false, false, false], "sample_610": [false, true, false, false, true], "sample_611": [false, false, false, false, false], "sample_612": [false, false, false, false, false], "sample_613": [true, false, false, false, false], "sample_614": [false, false, false, false, true], "sample_615": [false, false, false, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, false, false, false], "sample_618": [false, false, false, false, false], "sample_619": [true, false, true, false, false], "sample_620": [false, true, false, true, false], "sample_621": [false, false, false, false, false], "sample_622": [false, false, true, false, false], "sample_623": [false, false, false, false, false], "sample_624": [true, false, false, false, false], "sample_625": [false, false, false, false, false], "sample_626": [false, true, false, false, false], "sample_627": [false, false, false, false, false], "sample_628": [false, false, false, false, false], "sample_629": [true, true, false, true, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, false, false, false], "sample_639": [true, false, false, false, false], "sample_640": [false, false, false, false, false], "sample_641": [false, false, false, false, true], "sample_642": [false, false, false, false, false], "sample_643": [false, false, true, true, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, false, false, false], "sample_646": [false, false, false, false, false], "sample_647": [true, false, false, false, true], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, true, false, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, false, false, false, false], "sample_656": [false, false, false, false, false], "sample_657": [true, true, false, false, false], "sample_658": [false, false, false, false, false], "sample_659": [false, false, false, false, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, true, false, false, false], "sample_666": [false, false, false, false, false], "sample_667": [false, false, true, false, true], "sample_668": [false, false, false, false, false], "sample_669": [true, false, true, true, false], "sample_670": [false, false, false, false, false], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, true, false], "sample_673": [false, false, false, false, false], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [true, false, false, false, false], "sample_678": [false, true, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, true, false, false], "sample_683": [true, false, false, true, false], "sample_684": [false, false, false, false, false], "sample_685": [false, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [false, false, true, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [false, false, true, false, false], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, true, false, false, true], "sample_696": [false, false, false, false, false], "sample_697": [false, false, false, false, false], "sample_698": [true, false, false, true, false], "sample_699": [false, false, false, false, true], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, true, false, false], "sample_703": [true, false, true, false, true], "sample_704": [false, false, false, false, false], "sample_705": [false, true, false, false, false], "sample_706": [false, false, false, false, false], "sample_707": [false, false, false, false, false], "sample_708": [true, true, false, false, false], "sample_709": [true, false, true, false, false], "sample_710": [false, false, false, false, false], "sample_711": [false, true, false, false, false], "sample_712": [false, false, false, false, false], "sample_713": [false, false, true, false, false], "sample_714": [true, true, false, true, true], "sample_715": [false, false, false, false, false], "sample_716": [false, false, false, true, false], "sample_717": [true, true, true, true, true], "sample_718": [false, true, false, false, false], "sample_719": [true, false, false, true, true], "sample_720": [false, false, false, false, false], "sample_721": [true, false, true, false, false], "sample_722": [false, false, true, false, true], "sample_723": [true, false, true, true, true], "sample_724": [true, true, false, false, false], "sample_725": [false, false, false, false, true], "sample_726": [false, false, true, true, true], "sample_727": [true, true, true, false, false], "sample_728": [false, true, false, false, true], "sample_729": [true, true, false, false, false], "sample_730": [true, false, true, true, false], "sample_731": [false, false, true, true, true], "sample_732": [true, false, false, false, true], "sample_733": [true, true, false, true, false], "sample_734": [true, true, false, false, true], "sample_735": [false, false, true, false, false], "sample_736": [true, true, false, false, false], "sample_737": [true, false, false, false, false], "sample_738": [false, true, false, true, false], "sample_739": [false, true, false, true, false], "sample_740": [false, true, false, true, false], "sample_741": [true, false, false, false, false], "sample_742": [true, false, true, false, true], "sample_743": [false, false, false, true, false], "sample_744": [false, false, false, false, false], "sample_745": [true, false, true, true, true], "sample_746": [true, true, false, false, false], "sample_747": [false, false, false, false, false], "sample_748": [false, false, false, false, false], "sample_749": [false, false, false, false, false], "sample_750": [false, false, true, false, false], "sample_751": [false, true, false, true, false], "sample_752": [true, false, false, true, true], "sample_753": [false, false, true, false, false], "sample_754": [false, false, false, false, false], "sample_755": [false, false, false, false, true], "sample_756": [true, false, true, false, false], "sample_757": [false, false, false, false, true], "sample_758": [false, false, false, false, false], "sample_759": [true, true, false, true, false], "sample_760": [true, false, true, false, true], "sample_761": [false, true, true, false, false], "sample_762": [false, false, true, true, true], "sample_763": [false, false, true, true, false], "sample_764": [false, false, false, false, false], "sample_765": [false, false, false, false, false], "sample_766": [false, true, false, false, true], "sample_767": [false, false, false, true, false], "sample_768": [false, true, true, false, false], "sample_769": [false, true, false, false, true], "sample_770": [false, false, true, false, false], "sample_771": [false, false, false, false, false], "sample_772": [false, true, false, false, false], "sample_773": [false, false, false, false, false], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [false, false, true, true, false], "sample_777": [true, true, true, true, true], "sample_778": [true, false, false, false, true], "sample_779": [false, false, false, false, false], "sample_780": [false, false, false, true, false], "sample_781": [false, false, true, false, false], "sample_782": [false, true, false, true, true], "sample_783": [true, false, false, false, false], "sample_784": [true, false, true, false, false], "sample_785": [true, true, false, false, true], "sample_786": [false, false, true, false, false], "sample_787": [false, true, false, false, false], "sample_788": [false, true, false, true, true], "sample_789": [false, true, false, false, false], "sample_790": [false, true, false, true, false], "sample_791": [true, false, false, false, false], "sample_792": [false, true, false, false, false], "sample_793": [false, false, false, false, false], "sample_794": [false, false, false, false, false], "sample_795": [false, false, false, false, false], "sample_796": [true, true, false, true, false], "sample_797": [false, false, false, false, false], "sample_798": [true, false, false, true, false], "sample_799": [false, false, false, false, false], "sample_800": [false, false, false, false, false], "sample_801": [false, true, false, false, false], "sample_802": [false, false, false, true, false], "sample_803": [false, false, false, true, false], "sample_804": [false, false, false, false, true], "sample_805": [true, true, false, false, false], "sample_806": [true, true, true, false, false], "sample_807": [false, false, true, true, false], "sample_808": [true, false, true, true, true], "sample_809": [false, true, false, true, true], "sample_810": [false, true, true, false, false], "sample_811": [false, true, false, false, false], "sample_812": [false, false, false, false, false], "sample_813": [true, true, false, false, false], "sample_814": [true, true, true, false, true], "sample_815": [true, true, false, true, true], "sample_816": [false, false, true, true, false], "sample_817": [true, false, false, true, false], "sample_818": [false, false, false, true, false], "sample_819": [true, false, false, false, false], "sample_820": [false, true, true, false, true], "sample_821": [true, true, true, true, false], "sample_822": [false, false, true, true, true], "sample_823": [false, true, false, true, false], "sample_824": [false, false, true, false, false], "sample_825": [true, false, true, true, false], "sample_826": [true, false, false, false, false], "sample_827": [false, false, false, true, false], "sample_828": [true, false, false, false, false], "sample_829": [true, true, false, false, false], "sample_830": [false, false, false, false, false], "sample_831": [true, true, false, false, false], "sample_832": [false, false, false, false, true], "sample_833": [true, false, false, false, false], "sample_834": [false, true, true, false, true], "sample_835": [false, true, false, true, false], "sample_836": [false, false, true, false, false], "sample_837": [false, false, false, true, false], "sample_838": [false, false, false, false, false], "sample_839": [false, false, false, false, false], "sample_840": [true, false, false, false, false], "sample_841": [false, false, false, true, false], "sample_842": [false, true, true, true, false], "sample_843": [false, false, false, true, false], "sample_844": [false, true, false, false, false], "sample_845": [false, false, false, false, true], "sample_846": [false, false, true, false, false], "sample_847": [false, true, true, false, true], "sample_848": [true, true, false, true, false], "sample_849": [true, true, true, true, false], "sample_850": [false, false, true, true, false], "sample_851": [true, true, false, false, false], "sample_852": [true, false, true, true, false], "sample_853": [true, true, true, false, true], "sample_854": [true, true, false, true, false], "sample_855": [true, false, false, false, false], "sample_856": [false, false, false, true, false], "sample_857": [true, false, false, false, true], "sample_858": [false, false, false, false, true], "sample_859": [false, false, false, false, false], "sample_860": [true, false, false, true, true], "sample_861": [false, false, false, false, false], "sample_862": [false, true, false, false, false], "sample_863": [false, true, false, true, false], "sample_864": [false, true, true, false, true], "sample_865": [false, true, true, true, false], "sample_866": [true, true, false, true, false], "sample_867": [true, false, false, false, false], "sample_868": [false, false, false, false, false], "sample_869": [false, true, false, false, true], "sample_870": [false, true, false, true, false], "sample_871": [false, true, false, false, true], "sample_872": [false, false, false, false, false], "sample_873": [true, true, true, true, false], "sample_874": [true, false, false, true, false], "sample_875": [false, false, false, false, false], "sample_876": [false, false, true, false, false], "sample_877": [true, false, true, false, true], "sample_878": [false, false, false, false, false], "sample_879": [true, false, false, false, true], "sample_880": [false, false, true, false, false], "sample_881": [false, false, false, false, false], "sample_882": [false, false, false, false, true], "sample_883": [false, false, false, true, true], "sample_884": [false, false, false, false, true], "sample_885": [false, true, false, true, false], "sample_886": [true, true, true, false, false], "sample_887": [true, false, false, false, false], "sample_888": [false, false, false, true, false], "sample_889": [false, false, false, true, false], "sample_890": [false, false, false, false, true], "sample_891": [false, true, false, false, false], "sample_892": [false, false, false, false, false], "sample_893": [false, true, false, false, false], "sample_894": [false, false, false, false, true], "sample_895": [false, false, false, false, false], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [false, false, false, true, false], "sample_899": [false, false, false, false, false], "sample_900": [true, true, false, true, true], "sample_901": [true, true, false, false, true], "sample_902": [true, false, false, false, false], "sample_903": [true, false, true, true, false], "sample_904": [false, true, true, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, false, false, false, false], "sample_909": [false, false, false, false, false], "sample_910": [true, false, false, true, false], "sample_911": [false, false, false, false, false], "sample_912": [true, true, true, true, true], "sample_913": [true, false, true, true, true], "sample_914": [false, false, false, false, false], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [true, false, false, false, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [true, true, true, true, false], "sample_923": [false, false, false, false, false], "sample_924": [false, false, false, false, false], "sample_925": [false, true, false, false, true], "sample_926": [true, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [true, true, false, false, false], "sample_930": [false, false, false, false, false], "sample_931": [true, true, true, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, true, false, false, false], "sample_936": [false, false, false, false, true], "sample_937": [false, false, false, false, false], "sample_938": [false, false, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, false, false], "sample_941": [true, true, true, true, false], "sample_942": [false, false, false, true, false], "sample_943": [false, false, false, false, false], "sample_944": [true, true, false, false, false], "sample_945": [true, false, true, false, false], "sample_946": [true, true, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, true, false], "sample_951": [false, false, false, false, true], "sample_952": [false, false, false, false, false], "sample_953": [true, false, false, true, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, false, false], "sample_956": [true, true, false, false, true], "sample_957": [false, false, true, false, false], "sample_958": [false, false, false, false, false], "sample_959": [false, true, false, false, false], "sample_960": [false, true, false, false, false], "sample_961": [true, false, true, false, false], "sample_962": [true, true, false, false, false], "sample_963": [false, false, true, false, true], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [true, false, true, false, false], "sample_967": [false, false, false, false, false], "sample_968": [false, false, false, true, false], "sample_969": [false, true, false, false, false], "sample_970": [false, false, false, false, false], "sample_971": [false, true, false, false, true], "sample_972": [false, true, false, false, false], "sample_973": [false, false, false, false, false], "sample_974": [false, false, false, false, false], "sample_975": [false, true, false, false, false], "sample_976": [false, false, false, false, true], "sample_977": [false, true, false, true, false], "sample_978": [false, false, false, false, false], "sample_979": [true, false, false, false, false], "sample_980": [true, true, true, false, false], "sample_981": [true, true, false, false, true], "sample_982": [false, false, false, false, false], "sample_983": [true, true, true, true, true], "sample_984": [false, false, true, true, false], "sample_985": [true, false, false, false, false], "sample_986": [true, false, false, false, false], "sample_987": [false, false, false, false, false], "sample_988": [true, false, false, true, false], "sample_989": [false, false, true, false, false], "sample_990": [false, false, false, false, false], "sample_991": [false, false, false, false, false], "sample_992": [false, false, false, false, false], "sample_993": [false, true, false, false, true], "sample_994": [false, true, true, false, false], "sample_995": [true, false, false, false, false], "sample_996": [false, false, false, false, false], "sample_997": [true, false, false, true, false], "sample_998": [false, false, false, true, false], "sample_999": [true, true, false, false, false], "sample_1000": [true, true, false, true, false], "sample_1001": [true, false, false, false, false], "sample_1002": [false, false, true, false, false], "sample_1003": [false, false, true, false, false], "sample_1004": [false, false, false, false, false], "sample_1005": [false, true, false, false, true], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, false, true, false], "sample_1009": [false, false, false, false, false], "sample_1010": [false, true, false, true, false], "sample_1011": [false, true, false, false, false], "sample_1012": [false, true, true, true, false], "sample_1013": [false, true, true, true, true], "sample_1014": [false, false, false, true, false], "sample_1015": [false, false, false, false, false], "sample_1016": [false, true, false, false, false], "sample_1017": [true, false, false, false, true], "sample_1018": [true, true, true, false, false], "sample_1019": [false, false, true, false, false], "sample_1020": [false, false, false, false, true], "sample_1021": [true, false, false, false, false], "sample_1022": [false, false, false, false, false], "sample_1023": [true, false, true, false, false], "sample_1024": [true, false, false, false, false], "sample_1025": [true, true, false, false, false], "sample_1026": [true, true, true, true, true], "sample_1027": [true, true, false, false, false], "sample_1028": [false, true, true, false, false], "sample_1029": [false, false, false, false, false], "sample_1030": [true, true, true, false, true], "sample_1031": [false, false, false, false, false], "sample_1032": [true, false, false, true, false], "sample_1033": [false, true, true, false, false], "sample_1034": [false, false, false, true, false], "sample_1035": [false, false, true, true, false], "sample_1036": [true, true, false, false, true], "sample_1037": [true, false, true, true, false], "sample_1038": [true, false, false, false, false], "sample_1039": [false, false, false, false, false], "sample_1040": [true, false, false, false, false], "sample_1041": [false, false, true, false, false], "sample_1042": [false, true, false, false, false], "sample_1043": [false, false, false, false, false], "sample_1044": [false, false, false, false, false], "sample_1045": [false, true, false, true, false], "sample_1046": [false, false, false, false, false], "sample_1047": [true, true, false, false, true], "sample_1048": [false, true, false, true, false], "sample_1049": [false, false, false, false, false], "sample_1050": [true, false, true, false, false], "sample_1051": [false, false, false, false, false], "sample_1052": [true, false, false, false, false], "sample_1053": [true, true, false, false, false], "sample_1054": [true, true, true, false, false], "sample_1055": [true, true, true, false, false], "sample_1056": [false, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, true, true], "sample_1059": [true, true, true, false, false], "sample_1060": [true, false, false, false, true], "sample_1061": [false, true, true, true, true], "sample_1062": [false, false, false, false, false], "sample_1063": [true, true, true, false, true], "sample_1064": [true, true, true, true, true], "sample_1065": [false, false, false, true, false], "sample_1066": [false, false, false, false, false], "sample_1067": [true, false, false, false, false], "sample_1068": [true, false, false, true, false], "sample_1069": [false, false, false, false, true], "sample_1070": [false, false, false, false, false], "sample_1071": [true, true, true, false, false], "sample_1072": [false, false, false, false, false], "sample_1073": [false, false, true, false, true], "sample_1074": [false, false, true, false, false], "sample_1075": [false, false, false, false, false], "sample_1076": [false, false, false, false, false], "sample_1077": [false, true, true, false, true], "sample_1078": [false, false, true, true, false], "sample_1079": [true, false, true, false, false], "sample_1080": [false, false, false, false, false], "sample_1081": [false, false, false, false, true], "sample_1082": [false, false, false, false, false], "sample_1083": [false, false, false, true, false], "sample_1084": [true, true, true, false, true], "sample_1085": [true, true, true, false, true], "sample_1086": [false, false, true, false, false], "sample_1087": [false, false, false, true, false], "sample_1088": [false, false, false, false, true], "sample_1089": [true, true, true, false, false], "sample_1090": [true, true, false, false, false], "sample_1091": [false, false, false, false, true], "sample_1092": [true, true, false, false, true], "sample_1093": [true, false, false, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [false, false, false, false, false], "sample_1096": [false, true, false, false, false], "sample_1097": [false, false, false, false, false], "sample_1098": [true, true, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [true, true, true, true, false], "sample_1101": [true, false, false, false, false], "sample_1102": [false, false, true, false, false], "sample_1103": [false, true, true, true, false], "sample_1104": [true, false, false, false, false], "sample_1105": [false, true, false, false, false], "sample_1106": [false, false, true, false, false], "sample_1107": [false, false, false, true, false], "sample_1108": [true, false, false, false, false], "sample_1109": [true, false, false, false, false], "sample_1110": [false, false, true, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [true, false, false, true, false], "sample_1113": [false, false, false, false, false], "sample_1114": [false, false, false, false, false], "sample_1115": [false, false, false, false, false], "sample_1116": [true, false, false, false, false], "sample_1117": [false, false, true, false, false], "sample_1118": [false, true, false, false, true], "sample_1119": [true, false, false, false, true], "sample_1120": [true, false, false, false, false], "sample_1121": [true, true, true, false, true], "sample_1122": [false, true, false, false, false], "sample_1123": [true, false, false, false, false], "sample_1124": [false, false, false, false, false], "sample_1125": [true, false, true, true, false], "sample_1126": [true, true, true, true, true], "sample_1127": [false, false, false, false, false], "sample_1128": [true, false, false, true, false], "sample_1129": [false, false, false, false, false], "sample_1130": [false, false, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, true, false, false], "sample_1133": [false, false, true, false, false], "sample_1134": [true, true, false, false, false], "sample_1135": [true, true, false, true, false], "sample_1136": [true, false, false, false, false], "sample_1137": [false, false, false, false, false], "sample_1138": [true, true, false, false, false], "sample_1139": [false, false, true, true, false], "sample_1140": [true, false, false, false, false], "sample_1141": [true, true, false, true, false], "sample_1142": [true, true, true, true, false], "sample_1143": [true, true, false, true, false], "sample_1144": [true, true, true, false, true], "sample_1145": [false, true, true, false, false], "sample_1146": [true, true, false, true, false], "sample_1147": [false, false, false, true, false], "sample_1148": [true, false, false, false, false], "sample_1149": [true, false, true, true, false], "sample_1150": [false, true, true, false, false], "sample_1151": [false, false, false, false, false], "sample_1152": [false, true, false, false, false], "sample_1153": [false, false, false, false, false], "sample_1154": [false, false, false, false, false], "sample_1155": [false, false, true, true, false], "sample_1156": [true, true, false, true, true], "sample_1157": [true, false, false, false, false], "sample_1158": [false, true, false, true, true], "sample_1159": [true, false, false, true, true], "sample_1160": [false, false, false, false, false], "sample_1161": [true, false, true, false, false], "sample_1162": [true, false, true, false, false], "sample_1163": [false, false, false, false, false], "sample_1164": [true, false, false, false, false], "sample_1165": [false, false, false, true, false], "sample_1166": [false, false, false, false, false], "sample_1167": [true, false, false, false, true], "sample_1168": [false, false, false, false, false], "sample_1169": [false, false, true, true, false], "sample_1170": [true, false, true, false, false], "sample_1171": [true, false, false, false, false], "sample_1172": [true, true, false, false, false], "sample_1173": [true, false, false, true, false], "sample_1174": [false, false, false, false, true], "sample_1175": [false, false, false, false, false], "sample_1176": [true, false, false, true, true], "sample_1177": [false, false, false, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [true, true, true, false, true], "sample_1180": [false, false, false, true, false], "sample_1181": [false, true, false, true, true], "sample_1182": [false, true, false, true, false], "sample_1183": [true, true, false, true, false], "sample_1184": [true, false, false, false, false], "sample_1185": [true, true, true, false, false], "sample_1186": [true, true, true, true, true], "sample_1187": [true, false, false, false, false], "sample_1188": [true, true, true, true, false], "sample_1189": [true, true, true, true, true], "sample_1190": [true, false, false, false, false], "sample_1191": [true, false, false, true, false], "sample_1192": [false, false, true, false, true], "sample_1193": [false, false, false, false, false], "sample_1194": [true, true, false, false, true], "sample_1195": [false, false, false, false, false], "sample_1196": [false, false, true, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, true, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [false, true, false, false, false], "sample_1201": [false, false, true, false, false], "sample_1202": [true, false, true, false, false], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, false, false, false], "sample_1205": [false, false, false, false, false], "sample_1206": [false, true, false, true, false], "sample_1207": [true, false, false, true, false], "sample_1208": [true, false, false, false, true], "sample_1209": [false, false, false, false, false]}}