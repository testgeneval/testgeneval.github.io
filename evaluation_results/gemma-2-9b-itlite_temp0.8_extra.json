{"raw_generations": {"sample_96": ["def test_ridge_with_sparse_data():\n    from sklearn.datasets import make_regression\n    from sklearn.linear_model import Ridge\n    import numpy as np\n    from scipy.sparse import csr_matrix\n\n    # Create sparse data\n    X, y = make_regression(n_samples=100, n_features=100, random_state=42)\n    X_sparse = csr_matrix(X)\n\n    # Create Ridge model\n    ridge = Ridge(alpha=1.0)\n\n    # Fit the model to sparse data\n    ridge.fit(X_sparse, y)\n\n    # Test that the coefficients are calculated correctly\n    assert_array_almost_equal(ridge.coef_, ridge.fit(X, y).coef_)\n\n\n\n    \n", "def test_ridge_dtype_error_with_sparse_data():\n    # Test that a ValueError is raised when fitting Ridge with a sparse matrix and incompatible dtype\n    n_samples, n_features = 10, 5\n    X_csr = sp.csr_matrix((np.ones(n_samples), (np.arange(n_samples), np.arange(n_features))),\n                           shape=(n_samples, n_features)).astype(np.int32)\n    y = np.ones(n_samples)\n\n    reg = Ridge(alpha=1.0)\n\n    with pytest.raises(TypeError):\n        reg.fit(X_csr, y)\n", "def test_ridge_multiclass():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_clusters_per_class=1, random_state=0)\n    y = y.astype(int)\n    clf = RidgeClassifier(multi_class='multinomial')\n    clf.fit(X, y)\n    assert_equal(clf.classes_, np.unique(y))\n\n\n\n\n\n", "def test_ridge_class_weights_sparse():\n    rng = np.random.RandomState(0)\n    \n    n_samples, n_features = 100, 5\n    X = sp.csr_matrix(rng.randn(n_samples, n_features))\n    y = rng.randn(n_samples)\n\n    class_weights = {0: 1.0, 1: 10.0} \n\n    ridge = RidgeClassifier(class_weight=class_weights)\n    ridge.fit(X, y)\n\n    assert_array_almost_equal(ridge.coef_, ridge.coef_)\n\n    # Check if sample weights and class weights work together\n\n    sample_weights = rng.randn(n_samples)\n\n    ridge = RidgeClassifier(class_weight=class_weights, sample_weight=sample_weights)\n    ridge.fit(X, y)\n\n\n", "def test_ridge_classifier_class_weight_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, n_classes=3, random_state=0)\n    n_samples, n_features = X.shape\n    for solver in ['lbfgs', 'sag', 'saga']:\n        for class_weights in (None, {0: 1.0, 1: 2.0, 2: 0.5},\n                             {\"class_\" + str(i): 1.0 for i in range(3)}):\n            ridge = RidgeClassifier(solver=solver, class_weight=class_weights)\n            ridge.fit(X, y)\n            assert ridge.coef_.shape[0] == n_features\n            assert ridge.coef_.shape[1] == n_samples  \n"], "sample_62": ["    def test_get_cache_key_missing_template_variable(self):\n        request = self.factory.get(self.path)\n        template = engines[\"django\"].from_string(\"{% if variable exists %}This is a test{% endif %}\")\n        response = TemplateResponse(request, template)\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n        self.assertEqual(get_cache_key(request), None)\n", "    def test_cache_middleware_does_not_cache_status_code_404(self):\n        view = mock.Mock(return_value=HttpResponseNotFound())\n        cache_middleware = CacheMiddleware(view)\n        request = self.factory.get(\"/does/not/exist/\")\n        response = cache_middleware.process_view(\n            request, view, (), {}\n        )\n        view.assert_called_once()\n        self.assertEqual(response.status_code, 404)\n", "    def test_get_or_create_from_caches(self):\n        cache_handler = CacheHandler(\n            {\"cache_1\": {\"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\"}}\n        )\n        cache_1 = cache_handler[\"cache_1\"]\n        with self.subTest(cache_name=\"cache_1\"):\n            self.assertIsNotNone(\n                cache_handler._get_or_create_from_caches(\n                    \"cache_1\",\n                    {},\n                    None,\n                    None,\n                ),\n            )\n        with self.subTest(cache_name=\"nonexistent_cache\"):\n            with self.assertRaises(InvalidCacheBackendError):\n                cache_handler._get_or_create_from_caches(\n                    \"nonexistent_cache\", {}, None, None\n                )\n", "    def test_get_for_missing_cache_key(self):\n        cache_handler = CacheHandler({\n            \"default\": {\n                \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n            },\n        })\n        cache = cache_handler[\"default\"]\n        with self.subTest(\n            key=\"missing_key\",\n        ):\n            result = cache.get(\"missing_key\")\n            self.assertIsNone(result)\n", "    def test_cache_alias_order(self):\n        test_caches = CacheHandler(\n            {\n                \"cache_1\": {\n                    \"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\",\n                },\n                \"cache_2\": {\n                    \"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\",\n                },\n            }\n        )\n        test_caches.all()\n        self.assertEqual(test_caches.cache_order, [\"cache_1\", \"cache_2\"])\n\n\n\n"], "sample_26": ["    def test_serialization_with_object_references(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handles\n        # models with object references.\n        obj = Object.objects.create(name='Obj1')\n        obj_ref = ObjectReference.objects.create(obj=obj)\n        data = connection.creation.serialize_db_to_string()\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n        connection.creation.deserialize_db_from_string(data)\n        loaded_obj = Object.objects.get()\n        loaded_obj_ref = ObjectReference.objects.get()\n        self.assertEqual(loaded_obj.name, 'Obj1')\n        self.assertEqual(loaded_obj_ref.obj, loaded_obj)\n", "    def test_create_test_db_with_suffix(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        suffix = '_clone_1'\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.clone_test_db(suffix, verbosity=0, autoclobber=True, keepdb=False)\n            # Migrations run.\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n            # App is not synced.\n            mocked_sync_apps.assert_not_called()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0, suffix=suffix)\n", "    def test_serialization_empty_database(self):\n        # serialize_db_to_string() should return an empty string when the database is empty.\n        with self.subTest('Empty database'):\n            Object.objects.all().delete()\n            data = connection.creation.serialize_db_to_string()\n            self.assertEqual(data, '')\n\n        with self.subTest('No objects to serialize'):\n            data = connection.creation.serialize_db_to_string()\n            self.assertEqual(data, '')\n", "    def test_no_migrations_serialization(self):\n        # Test that `serialize_db_to_string()` works correctly even when \n        # migrations are disabled for all apps.\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = False\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n                loader_instance = loader.return_value\n                loader_instance.migrated_apps = {}\n                data = connection.creation.serialize_db_to_string()\n            Object.objects.all().delete()\n            connection.creation.deserialize_db_from_string(data)\n            self.assertEqual(Object.objects.count(), 1)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_object_reference_with_none(self):\n        # Ensure that deserialize_db_from_string() handles 'None' values \n        # for object references correctly.\n        obj_1 = Object.objects.create(pk=1)\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"obj_ref\": None, \"related_objects\": []}\n            }\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        obj = Object.objects.get(pk=1)\n        self.assertIsNone(obj.obj_ref)\n\n\n\n"], "sample_121": ["compilation error", "compilation error", "compilation error", "def test_inversion_vector():\n    p = Permutation([1, 3, 0, 2])\n    assert p.inversion_vector() == [2, 1, 1, 0]\n\n    p = Permutation([0, 1, 2, 3])\n    assert p.inversion_vector() == [0, 0, 0, 0]\n\n    p = Permutation([3, 2, 1, 0])\n    assert p.inversion_vector() == [3, 2, 1, 0]\n\n    p = Permutation([1, 5, 2, 0, 3, 6, 4])\n    assert p.inversion_vector() == [2, 1, 0, 1, 0, 0, 0]\n    p = Permutation([0])\n    assert p.inversion_vector() == [0]\n\n    p = Permutation([0, 1, 2, 3])\n    q = Permutation([3, 2, 1, 0])\n    assert p.inversion_vector() != q.inversion_vector()\n\n\n    q = Permutation([1, 5, 2, 0, 3, 6, 4])\n    assert Permutation.from_inversion_vector([2, 1, 0, 1, 0, 0, 0]) == q\n    raises(ValueError, lambda: Permutation.from_inversion_vector([0, 1, 2]))\n\n    a = [Permutation.unrank_nonlex(4, i).inversion_vector()\n        for i in range(24)]\n    assert a == [\n        [0, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 1, 0],\n        [0, 2, 0, 0], [0, 2, 1, 0], [0, 3, 0, 0], [0, 3, 1, 0],\n        [1, 0, 0, 0], [1, 0, 1, 0], [", "compilation error"], "sample_67": ["    def test_serialize_custom_migration_class(self):\n        class CustomMigration(migrations.Migration):\n                return \"my custom migration\"\n        migration = CustomMigration(\n            \"0001_initial\", \"testapp\"\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertEqual(output, \"my custom migration\")\n\n\n\n", "    def test_serialize_nested_fieldsets(self):\n        class NestedModel(models.Model):\n            name = models.CharField(max_length=255)\n            details = models.TextField()\n        \n        class ParentModel(models.Model):\n            name = models.CharField(max_length=255)\n            nested = NestedModel()\n\n        class ChildModel(models.Model):\n            name = models.CharField(max_length=255)\n            parent = models.ForeignKey(ParentModel, on_delete=models.CASCADE)\n\n        field = models.ForeignKey(ParentModel, on_delete=models.CASCADE, related_name=\"children\")\n        fieldset = models.Fieldset(\n            \"Parent Information\",\n            fields=[\n                (\"name\", field),\n                (\"nested\", models.OneToOneField(NestedModel, on_delete=models.CASCADE)),\n            ],\n        )\n        string = MigrationWriter.serialize(field, include_fields=True)[0]\n        self.assertIn(\n            \"models.ForeignKey(ParentModel, on_delete=models.CASCADE, related_name='children')\",\n            string,\n        )\n        string = MigrationWriter.serialize(fieldset)[0]\n        self.assertIn(\n            \"models.Fieldset('Parent Information', fields=[('name', ParentModel.name), ('nested', models.OneToOneField(NestedModel, on_delete=models.CASCADE))])\",\n            string,\n        )\n", "    def test_serialize_nested_class_with_custom_serializer(self):\n        class NestedSerializer(BaseSerializer):\n                return \"Nested(%r)\" % self.value, {}\n\n        class NestedClass:\n                self.value = value\n\n        class TestModel:\n            nested = NestedClass(1)\n\n        MigrationWriter.register_serializer(NestedClass, NestedSerializer)\n        string, imports = MigrationWriter.serialize(TestModel())\n        self.assertEqual(string, \"TestModel(nested=Nested(1))\")\n        self.assertEqual(imports, {\"from migrations.test_writer import NestedSerializer\"})\n        MigrationWriter.unregister_serializer(NestedClass)\n\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(**kwargs)\n\n                return 'MYCUSTOMTYPE'\n\n                return models.CharField(**kwargs)\n\n                super().contribute_to_class(model, name)\n\n        class MyModel(models.Model):\n            my_field = CustomField()\n        string = MigrationWriter.serialize(MyModel._meta)[0]\n        self.assertIn(\"my_field = CustomField()\", string)  \n", "    def test_serialize_custom_class(self):\n        class TestModel(models.Model):\n            field = models.CharField(max_length=100)\n\n        string, imports = MigrationWriter.serialize(TestModel)\n        self.assertEqual(\n            string,\n            \"class TestModel(migrations.Model):\\n    field = models.CharField(max_length=100)\",\n        )\n        self.assertEqual(imports, {\"from django.db import models\"})\n"], "sample_147": ["def test_nfloat_symbols_and_functions():\n    from sympy.core.function import nfloat\n    from sympy.abc import x, y\n    from sympy import cos, pi, sqrt\n\n    # Test that symbols are left as symbols\n    assert nfloat(x) == x\n    assert nfloat(y) == y\n\n    # Test that functions are left as functions\n    assert nfloat(cos(x)) == cos(x)\n    assert nfloat(pi) == pi\n\n    # Test function with arguments\n    assert nfloat(sqrt(y)) == sqrt(y)\n\n\n\n", "def test_nfloat_iterable():\n    from sympy.polys.polyutils import dict_from_args\n    from sympy import Eq, Symbol\n\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # list, tuple, set, dict\n    assert nfloat([(x, y), (2, 3)], n=2) == [(x, y), (2, 3)]\n    assert nfloat((x, y, 2, 3), n=2) == (x, y, 2, 3)\n    assert nfloat({x: y}, n=2) == {x: y}\n\n\n    # test nfloat on a dict with nested dictionaries\n    d = {\n        'a': {'b': x, 'c': y},\n        'd': 2,\n        'e': {\n            'f': 3,\n            'g': {\n                'h': 4\n            }\n        }\n    }\n    assert nfloat(d, n=2) == {\n        'a': {'b': x, 'c': y},\n        'd': 2,\n        'e': {\n            'f': 3,\n            'g': {\n                'h': 4\n            }\n        }\n    }\n\n    # test Eq\n    eq = Eq(x + y, 1)\n    assert nfloat(eq, n=2) == Eq(x + y, 1)\n\n\n\n", "def test_empty_iterable():\n    from sympy.functions.elementary.integers import floor\n\n    assert count_ops([], visual=False) == 0\n    assert count_ops([], visual=True) == S.Zero\n    assert count_ops(((),), visual=False) == 0\n    assert count_ops(((),), visual=True) == S.Zero\n    assert count_ops({ }, visual=False) == 0\n    assert count_ops({ }, visual=True) == S.Zero\n    assert count_ops([floor(1), floor(2)], visual=False) == 0\n\n\n\n", "def test_nfloat_dicts():\n    from sympy.core import sympify\n    from sympy.abc import x, y\n\n    expr = {x: sympify(x**2 + 2*x), y: sympify(sqrt(y))}\n\n    nfloat_expr = nfloat(expr)\n    assert isinstance(nfloat_expr, dict)\n    assert nfloat_expr[x] == x**2 + 2*x\n    assert isinstance(nfloat_expr[y], (float, AlgebraicNumber))\n\n\n", "def test_nfloat_dict():\n    from sympy.abc import x, y\n\n    expr = {'a': x**2, 'b': x/2 + y, 'c':  S(2)}\n    nfloat_expr = nfloat(expr)\n    assert isinstance(nfloat_expr, dict)\n    assert nfloat_expr['a'] ==  x**2\n    assert isinstance(nfloat_expr['b'], Add)\n    assert nfloat_expr['b'].args[0] == x/2\n    assert nfloat_expr['b'].args[1] == y\n    assert nfloat_expr['c'] == 2\n\n\n\n"], "sample_114": ["compilation error", "compilation error", "compilation error", "def test_class_distribution_multilabel_indicator():\n    y = np.array(\n        [[1, 0, 0], [1, 1, 0], [0, 1, 1], [1, 0, 1], [0, 0, 1]]\n    )\n    classes, n_classes, class_prior = class_distribution(y)\n    classes_expected = [[1, 0], [1], [1], [0], [1]]\n    n_classes_expected = [2, 1, 1, 1, 1]\n    assert_array_almost_equal(classes, classes_expected)\n    assert_array_almost_equal(n_classes, n_classes_expected)\n\n\n    # test sparse matrix input\n    data = np.array([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1])\n    indices = np.array([0, 1, 2, 3, 0, 1, 2, 3, 1, 2, 0, 3])\n    indptr = np.array([0, 4, 7, 7, 7])\n    y_sp = sp.csc_matrix((data, indices, indptr), shape=(5, 3))\n\n    classes_sp, n_classes_sp, class_prior_sp = class_distribution(y_sp)\n    assert_array_almost_equal(classes_sp, classes_expected)\n    assert_array_almost_equal(n_classes_sp, n_classes_expected)\n", "def test_class_distribution_multilabel_sparse():\n    # Test with sparse multilabel data\n    data = np.array([\n        [1, 0, 1],\n        [1, 1, 0],\n        [0, 1, 1],\n        [1, 1, 1],\n    ])\n    rows = [0, 1, 2, 3]\n    cols = [0, 1, 2, 0, 1, 2, 0, 1, 2]\n    data = [1] * 9\n    y_sparse = sp.csc_matrix((data, (rows, cols)), shape=(4, 3))\n    classes, n_classes, class_prior = class_distribution(y_sparse)\n    assert_array_equal(classes, [[1, 0, 1], [1, 1, 0], [0, 1, 1], [1, 1, 1]])\n    assert_array_equal(n_classes, [3, 3, 3, 3])  \n    assert_array_almost_equal(\n        class_prior, [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]\n    ) \n"], "sample_0": ["    def test_autocomplete_render(self):\n        rel = Album._meta.get_field('band').remote_field\n        form = AlbumForm(data={'band': Band.objects.create(name='The Beatles').pk})\n        output = form.as_p()\n        self.assertIn('<input type=\"hidden\" name=\"band\" value=\"%s\">' % Band.objects.create(name='The Beatles').pk, output)\n        self.assertIn('<span class=\"admin-autocomplete-input\">', output)\n        self.assertIn('data-ajax--url=\"/admin_widgets/band/autocomplete/\"', output)\n\n", "    def test_render_options_choices_attribute(self):\n        band1 = Band.objects.create(name='Band 1', style='rock')\n        band2 = Band.objects.create(name='Band 2', style='pop')\n        band3 = Band.objects.create(name='Band 3', style='jazz')\n        form = AlbumForm(initial={'band': band1.pk}, \n                        attr)\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>Band 1</option>' % band1.pk\n        option2 = '<option value=\"%s\">Band 2</option>' % band2.pk\n        option3 = '<option value=\"%s\">Band 3</option>' % band3.pk\n        self.assertIn(selected_option, output)\n        self.assertIn(option2, output)\n        self.assertIn(option3, output)\n        \n        form = AlbumForm(initial={'band': band1.pk, 'featuring': [band2.pk, band3.pk]}, \n                        attr)\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>Band 1</option>' % band1.pk\n        option2 = '<option value=\"%s\" selected>Band 2</option>' % band2.pk\n        option3 = '<option value=\"%s\" selected>Band 3</option>' % band3.pk\n        self.assertIn(selected_option, output)\n        self.assertIn(option2, output)\n        self.assertIn(option3, output)\n\n", "    def test_get_context_with_initial(self):\n        rel = Album._meta.get_field('band').remote_field\n        form = AlbumForm(initial={'band': Band.objects.first().pk})\n        context = form['band'].field.widget.get_context(name='my_field', value=form['band'].initial, attrs={})\n        self.assertIn('initial', context['widget'])\n        self.assertEqual(context['widget']['initial'], form['band'].initial)\n\n\n\n", "    def test_render_empty_option_with_custom_placeholder(self):\n        form = AlbumForm(initial={'band': None})\n        attrs = {'data-placeholder': 'Select a band'}\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n        self.assertIn('Select a band', output)\n\n", "    def test_render_empty_select(self):\n        form = AlbumForm(initial={'band': None})\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n\n\n"], "sample_42": ["    def test_serialize_custom_fields(self):\n        from django.db import models\n        from django.test.utils import override_settings\n\n        class CustomField(models.Field):\n            pass\n\n        class MyModel(models.Model):\n            custom_field = CustomField()\n\n        with override_settings(INSTALLED_APPS={'append': \"migrations.test_writer\"}):\n            migration = type(\"Migration\", (migrations.Migration,), {\n                \"operations\": [\n                    migrations.CreateModel(name=\"MyModel\", fields=[('custom_field', MyModel.custom_field)],)\n                ],\n            })\n            writer = MigrationWriter(migration)\n            output = writer.as_string()\n        self.assertIn(\n            'class CustomField(models.Field):\\n    pass', output\n        )\n", "    def test_serialize_lazy_objects(self):\n        class LazyObject:\n                self.value = value\n\n                return str(self.value)\n\n        lazy_object = LazyObject(123)\n        self.assertSerializedEqual(lazy_object)\n", "    def test_serialize_lazy_object_for_field(self):\n        class StringObject(object):\n                return isinstance(other, StringObject) and self.value == other.value\n\n                return self.value\n\n                return f\"StringObject('{self.value}')\"\n\n                self.value = value\n\n        class LazyStringObject(object):\n                self.value = value\n\n                return StringObject(self.value)\n\n        field = models.CharField(default=LazyStringObject(\"hello\"))\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, \"models.CharField(default=LazyStringObject('hello'))\")\n\n\n\n", "    def test_serialize_non_string_types(self):\n        for obj in [123, True, None, b'abc', 42.5, datetime.datetime.now(),\n                    set(), tuple(), frozenset(), dict({})]:\n            with self.subTest(obj=obj):\n                self.assertRaises(ValueError, self.assertSerializedEqual, obj)\n", "    def test_serialize_with_custom_serializer(self):\n        class CustomSerializer(BaseSerializer):\n                return f'CustomSerializer({value})', {'custom_serializer': 'from .custom_serializer import CustomSerializer'}\n\n        MigrationWriter.register_serializer(Money, CustomSerializer)\n        self.assertSerializedEqual(Money('1.3'))\n        self.assertSerializedResultEqual(\n            Money('1.3'),\n            ('CustomSerializer(1.3)', {'custom_serializer': 'from .custom_serializer import CustomSerializer'}),\n        )\n        MigrationWriter.unregister_serializer(Money)\n"], "sample_140": ["def test_point_vel_multiple_parents():\n    t = dynamicsymbols._t\n    q1, q2, u1 = dynamicsymbols('q1 q2 u1')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P1 = Point('P1')\n    P1.set_pos(O, q1 * N.x)\n    P1.set_vel(N, u1 * N.y)\n    P2 = Point('P2')\n    P2.set_pos(P1, q2 * B.z)\n    assert P2.vel(N) == u1 * N.y + q2.diff(t) * B.z\n", "def test_point_vel_multiple_velocities():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n    P = Point('P')\n    P.set_pos(O, q1 * N.y)\n    P.set_vel(B, u2 * B.x)\n    assert P.vel(N) == u1 * N.x + u2 * B.x + q1.diff(t) * N.y\n \n \n", "def test_point_vel_with_multiple_velocities():\n    q1, q2, q3, u1, u2 = dynamicsymbols('q1 q2 q3 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n    P = Point('P')\n    P.set_pos(O, q1 * B.x)\n    P.set_vel(B, u2 * B.y)\n    Q = Point('Q')\n    Q.set_pos(P, q2 * N.z)\n    Q.set_vel(B, q3 * B.x)\n    assert Q.vel(N) == (u1 + q1 * u2) * N.x + q2 * q3 * B.x - u2 * q1 * B.y  + q2 * q3 * B.z\n    assert P.vel(N) == u2 * B.y - u1 * q1 * B.x\n\n    B.set_ang_vel(N, q1 * N.y)\n    assert Q.vel(N) == (u1 + q1 * u2) * N.x + q2 * q3 * B.x - u2 * q1 * B.y  + q2 * q3 * B.z\n    assert P.vel(N) == u2 * B.y - u1 * q1 * B.x\n", "def test_point_vel_velocities_in_different_frames():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q1, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * N.x)\n    O.set_vel(N, 5 * N.x)\n    P.set_vel(B, 10 * B.y)\n    assert P.vel(N) == 10 * B.y + 5 * N.x\n\n", "def test_point_pos_multiple_points():\n    q, q1, q2 = dynamicsymbols('q q1 q2')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', q1 * B.x)\n    Q = P.locatenew('Q', q2 * B.y)\n    assert Q.pos_from(O) == q1 * B.x + q2 * B.y\n    R = O.locatenew('R', 10*N.x)\n    assert Q.pos_from(R) == q1 * B.x + q2 * B.y - 10 * N.x\n    assert O.pos_from(Q) == -q1 * B.x - q2 * B.y\n\n\n\n"], "sample_84": ["        def fix_method(self):\n            class Fixture:\n                    self.value = 1\n\n                    return self.value\n\n            return Fixture()\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_124": ["compilation error", "def test_tanh_asymptotes():\n    x = Symbol('x')\n    assert tanh(x).asymptotes() == [(-oo, 1), (oo, 1),]\n", "def test_atanh_int():\n    assert atanh(1).is_infinite is True\n    assert atanh(-1).is_infinite is True \n    assert atanh(0) == 0\n    assert atanh(0.5) == 0.5493061443340549\n    assert atanh(-0.5) == -0.5493061443340549\n\n\n\n", "compilation error", "compilation error"], "sample_117": ["    def test_stringify_other_type_hints(annotation: Any, expected: str):\n        assert stringify(annotation) == expected\n", "    def test_stringify_type_hints_ForwardRef():\n        from typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"myint\")) == \"myint\" \n\n\n\n", "    def test_stringify_type_hints_GenericMeta():\n        from typing import GenericMeta\n\n        class MyGenericMeta(GenericMeta):\n                return super().__new__(mcs, name, bases, attrs)\n\n        class MyGenericClass(metaclass=MyGenericMeta):\n            T = TypeVar('T')\n                self.arg = arg\n        assert stringify(MyGenericClass) == \"MyGenericClass[T]\" \n", "def test_stringify_type_hints_GenericMeta_forward_ref():\n    from typing import GenericMeta, ForwardRef  # type: ignore\n    class MyClass(GenericMeta, metaclass=type):\n        __args__ = [ForwardRef(\"MyType\")]\n    assert stringify(MyClass) == \"tests.test_util_typing.MyClass[MyType]\"\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n"], "sample_71": ["def test_reload_library():\n    with temp_style(\"test\", DUMMY_SETTINGS):\n        assert \"test\" in style.available\n        style.reload_library()\n        assert \"test\" not in style.available\n\n", "def test_use_empty_style():\n    with temp_style('empty_style', {}):\n        style.use('empty_style')\n        assert mpl.rcParams == mpl.rcParamsDefault\n", "def test_user_style_loads():\n    with TemporaryDirectory() as tmpdir:\n        # Create a style file in the user library directory\n        style_path = Path(tmpdir, 'my_style.mplstyle')\n        style_path.write_text(\n            f\"{PARAM}: {VALUE}\", encoding=\"utf-8\"\n        )\n\n        # Add the temporary directory to the user library paths\n        USER_LIBRARY_PATHS.append(tmpdir)\n\n        # Reload the style library to include the new style\n        style.reload_library()\n        \n        with style.context(\"my_style\"):\n            assert mpl.rcParams[PARAM] == VALUE\n\n        # Remove the temporary directory after use\n        USER_LIBRARY_PATHS.remove(tmpdir)\n\n", "def test_use_available():\n    with pytest.raises(ValueError):\n        style.use('nonexistent_style')\n", "def test_default_style():\n    mpl.rcParams.update({'figure.figsize': (10, 10)})\n    with style.context('default'):\n        assert mpl.rcParams['figure.figsize'] == (6.4, 4.8)\n\n"], "sample_145": ["def test_latex_printing_of_special_functions():\n    from sympy import exp, sin, cos, log, erf\n    assert latex(exp(x)) == r'e^{x}'\n    assert latex(sin(x)) == r'\\sin{\\left(x \\right)}'\n    assert latex(cos(x)) == r'\\cos{\\left(x \\right)}'\n    assert latex(log(x)) == r'\\log{\\left(x \\right)}'\n    assert latex(erf(x)) == r'\\operatorname{erf}{\\left(x \\right)}'\n", "def test_latex_printing_for_sympy_objects():\n    from sympy import symbols, sin, cos, tan, exp, log, factorial, pi\n    x, y, z = symbols('x y z')\n    assert latex(sin(x)) == r'\\sin{\\left(x \\right)}'\n    assert latex(cos(y)) == r'\\cos{\\left(y \\right)}'\n    assert latex(tan(z)) == r'\\tan{\\left(z \\right)}'\n    assert latex(exp(x)) == r'e^{x}'\n    assert latex(log(x)) == r'\\log{\\left(x \\right)}'\n    assert latex(factorial(x)) == r'\\Gamma{\\left(x + 1 \\right)}'\n    assert latex(pi) == r'\\pi'\n\n\n\n", "def test_latex_printing_for_special_functions():\n    from sympy import erfc, gamma, loggamma, sinh, cosh\n    assert latex(erfc(x)) == r\"\\operatorname{erfc}{\\left(x \\right)}\"\n    assert latex(gamma(x)) == r\"\\Gamma{\\left(x \\right)}\"\n    assert latex(loggamma(x)) == r\"\\log \\Gamma{\\left(x \\right)}\"\n    assert latex(sinh(x)) == r\"\\sinh{\\left(x \\right)}\"\n    assert latex(cosh(x)) == r\"\\cosh{\\left(x \\right)}\" \n", "def test_latex_symbol_with_font_style():\n    from sympy.abc import x\n    from sympy.core.symbol import Symbol\n    \n    assert latex(Symbol(\"alpha\", fontstyle=\"italic\")) == r'$\\mathit{alpha}$'\n    assert latex(Symbol(\"beta\", fontstyle=\"bold\")) == r'$\\mathbf{beta}$'\n    assert latex(Symbol(\"gamma\", fontstyle=\"script\")) == r'$\\mathcal{gamma}$'\n    assert latex(Symbol(\"delta\", fontstyle=\"fraktur\")) == r'$\\mathfrak{delta}$'\n\n\n\n", "def test_latex_matrix_symbols_with_indices():\n    from sympy import MatrixSymbol, Indexed\n    A = MatrixSymbol(\"A\", 2, 3)\n    i, j = symbols(\"i j\", integer=True)\n    b = Indexed(\"b\", (i, j))\n    assert latex(A[i, j]) == r\"A_{i j}\"\n    assert latex(A[i,j] + b[i,j]) == r\"\\operatorname{A}_{i j} + b_{i j}\"\n\n\n"], "sample_100": ["compilation error", "compilation error", "compilation error", "def test_one_hot_encoder_handle_unknown_multiple_categories():\n    X = np.array([['a', 2, 55], ['b', 1, 55], ['c', 3, 55]])\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n\n    X2 = np.array([['d', 4, 55]])\n    X2_tr = enc.transform(X2)\n    assert_array_equal(X2_tr.toarray(),\n                       [[0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n\n    for i in range(X2_tr.shape[1]):\n        assert_equal(X2_tr.getnnz(axis=0)[i], 1)  # single occurrence of unknown\n\n    enc = OneHotEncoder(handle_unknown='error')\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit_transform(X2)\n\n\n\n", "def test_one_hot_encoder_sparse_output():\n    X = [['Male', 1], ['Female', 3]]\n    enc = OneHotEncoder(sparse=True)\n    Xtr = enc.fit_transform(X)\n    assert isinstance(Xtr, sparse.csr_matrix)\n    assert Xtr.shape == (2, 4)\n"], "sample_144": ["def test_refine_non_constant_assumption():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n\n    assert refine(Abs(x), Q.less_than(x,y)) == Abs(x) \n\n\n\n", "def test_refine_with_multiplication():\n    from sympy.core.symbol import Symbol\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    assert refine(Abs(x*y), Q.positive(x) & Q.positive(y)) == x*y\n    assert refine(Abs(x*y), Q.positive(x) & Q.negative(y)) == -x*y\n    assert refine(Abs(x*y), Q.negative(x) & Q.positive(y)) == -x*y\n    assert refine(Abs(x*y), Q.negative(x) & Q.negative(y)) == x*y\n    assert refine(Abs(x*y*z), Q.positive(x) & Q.positive(y) & Q.positive(z)) == x*y*z\n    assert refine(Abs(x*y*z), Q.positive(x) & Q.positive(y) & Q.negative(z)) == -x*y*z\n    assert refine(Abs(x*y*z), Q.positive(x) & Q.negative(y) & Q.positive(z)) == -x*y*z\n    assert refine(Abs(x*y*z), Q.positive(x) & Q.negative(y) & Q.negative(z)) == x*y*z\n    assert refine(Abs(x*y*z), Q.negative(x) & Q.positive(y) & Q.positive(z)) == -x*y*z\n    assert refine(Abs(x*y*z), Q.negative(x) & Q.positive(y) & Q.negative(z)) == x*y*z\n    assert refine(Abs(x*y*z), Q.negative(x) & Q.negative(y) & Q.positive(z)) == x*y*z\n    assert refine(Abs(x*y*z), Q.negative(x) & Q.negative(y) & Q.negative(z)) == -x*y*z\n\n\n", "def test_refine_with_multiple_assumptions():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    assert refine(sqrt(x**2 + y**2), Q.positive(x) & Q.zero(y)) == x\n    assert refine(sqrt(x**2 + y**2), Q.zero(x) & Q.positive(y)) == y\n    assert refine(atan2(y, x), Q.positive(x) & Q.positive(y)) == atan(y/x)\n    assert refine(atan2(y, x), Q.positive(x) & Q.negative(y)) == atan(y/x) - pi\n    assert refine(atan2(y, x), Q.negative(x) & Q.positive(y)) == atan(y/x) + pi\n    assert refine(atan2(y, x), Q.negative(x) & Q.negative(y)) == atan(y/x) - pi\n", "def test_refine_mul():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    assert refine(x * y, Q.positive(x) & Q.positive(y)) == x * y\n    assert refine(x * y, Q.positive(x) & Q.negative(y)) == -x * y\n    assert refine(x * y, Q.negative(x) & Q.positive(y)) == -x * y\n    assert refine(x * y, Q.negative(x) & Q.negative(y)) == x * y\n\n    assert refine(x * y * z, Q.positive(x) & Q.positive(y) & Q.positive(z)) == x * y * z\n    assert refine(x * y * z, Q.positive(x) & Q.positive(y) & Q.negative(z)) == -x * y * z\n    assert refine(x * y * z, Q.positive(x) & Q.negative(y) & Q.positive(z)) == -x * y * z\n    assert refine(x * y * z, Q.positive(x) & Q.negative(y) & Q.negative(z)) == x * y * z\n    assert refine(x * y * z, Q.negative(x) & Q.positive(y) & Q.positive(z)) == -x * y * z\n    assert refine(x * y * z, Q.negative(x) & Q.positive(y) & Q.negative(z)) == x * y * z\n    assert refine(x * y * z, Q.negative(x) & Q.negative(y) & Q.positive(z)) == x * y * z\n    assert refine(x * y * z, Q.negative(x) & Q.negative(y) & Q.negative(z)) == -x * y * z\n", "    def test_refine_bool_ops():\n        x = Symbol('x')\n        y = Symbol('y')\n        z = Symbol('z')\n\n        # Test Bool operations with Q.even\n        assert refine(x & Q.even(x), Q.even(x)) == x\n        assert refine(x & Q.even(x), Q.odd(x)) == False\n        assert refine(x | Q.even(x), Q.even(x)) == x\n        assert refine(x | Q.even(x), Q.odd(x)) == Q.even(x)\n        assert refine(x ^ Q.even(x), Q.even(x)) == ~x\n        assert refine(x ^ Q.even(x), Q.odd(x)) == x\n\n        # Test Bool operations with Q.real\n        assert refine(x & Q.real(x), Q.real(x)) == x\n        assert refine(x & Q.real(x), Q.imaginary(x)) == False\n        assert refine(x | Q.real(x), Q.real(x)) == x\n        assert refine(x | Q.real(x), Q.imaginary(x)) == Q.real(x)\n        assert refine(x ^ Q.real(x), Q.real(x)) == ~x\n        assert refine(x ^ Q.real(x), Q.imaginary(x)) == x\n\n        # Test Bool operations with multiple assumptions\n        assert refine(x & Q.even(x) & Q.positive(x), And(Q.even(x), Q.positive(x))) == x\n        assert refine(x | Q.even(x) | Q.positive(x), Or(Q.even(x), Q.positive(x))) == x\n        assert refine(x & Q.even(x) | Q.positive(x), And(Q.even(x), Or(Q.positive(x)))) == x\n        assert refine(x | Q.even(x) & Q.positive(x), Or(Q.even(x), And(Q.positive(x)))) == x\n\n\n\n"], "sample_90": ["def test_parametrize_markers_order(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.one\n        @pytest.mark.two\n            assert 1 == 1\n    \"\"\"\n    )\n\n    reprec = testdir.inline_run(\"-v\")\n    outcomes = reprec.getoutcomes()\n    for outcome in outcomes:\n        if outcome.passed:\n            markers = outcome.nodeid.split(\"::\")[-1].split(\",\")\n            assert sorted(markers) == [\"one\", \"two\"]\n", "    def test_mark_expr_simple_match(testdir):\n        foo = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.internal_err\n                pass\n            @pytest.mark.passing\n                pass\n\n        \"\"\"\n        )\n        result = testdir.runpytest(foo, \"-m\", \"passing\")\n        result.assert_outcomes(passed=1, skipped=1)\n\n\n\n", "    def test_getitem_markers_from_parent(testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parent_marker\n            class Parent(object):\n                pass\n\n\n            class Child(Parent):\n                pass\n\n            @pytest.mark.child_marker\n                pass\n\n\n            @pytest.mark.parent_and_child_marker\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run(p.dirpath(), \"-k\", \"parent_marker\")\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 1\n        assert not failed\n        dlist = reprec.getcalls(\"pytest_deselected\")\n        assert len(dlist) == 2\n\n        reprec = testdir.inline_run(p.dirpath(), \"-k\", \"child_marker\")\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 1\n        assert not failed\n        dlist = reprec.getcalls(\"pytest_deselected\")\n        assert len(dlist) == 1\n", "    def test_internal_err_marker_expr_eval_success(testdir):\n        foo = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.internal_err\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(foo, \"-m\", \"internal_err\")\n        result.assert_outcomes(passed=1)\n", "    def test_parametrize_fixtures_and_markers(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture(scope=\"session\", autouse=True)\n                pass\n\n            @pytest.mark.parametrize(\"arg\", [1, 2], ids=['one', 'two'])\n                assert arg == 1\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n\n\n"], "sample_54": ["    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize(\"This is a long URL: http://www.example.com/this/is/a/very/long/url-with-lots-of-parts/\",\n                   trim_url_limit=20),\n            'This is a long URL: http://www.example.com/this/is/a/very/...',\n        )\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize(\n                \"This is a long URL: https://www.example.com/this-is-a-really-long-url-that-will-be-truncated\",\n                trim_url_limit=20,\n            ),\n            \"This is a long URL: https://www.example.com/this-is-a-really-long-url-tha...\"\n        )\n\n\n\n", "    def test_urlize_options(self):\n        text = \"Search for google.com/?q=foo+bar and see wikipedia.org/wiki/Search\"\n        \n        # Test trim_url_limit\n        self.assertEqual(urlize(text, trim_url_limit=10), 'Search for <a href=\"http://goo...> and see <a href=\"http://wikipedia.org/wiki/Search\">...</a>')\n        \n        # Test nofollow\n        self.assertEqual(urlize(text, nofollow=True), 'Search for <a href=\"http://google.com/?q=foo+bar\" rel=\"nofollow\">google.com/?q=foo+bar</a> and see <a href=\"http://wikipedia.org/wiki/Search\" rel=\"nofollow\">wikipedia.org/wiki/Search</a>')\n        \n        # Test both\n        self.assertEqual(urlize(text, trim_url_limit=10, nofollow=True), 'Search for <a href=\"http://goo...\" rel=\"nofollow\">...</a> and see <a href=\"http://wikipedia.org/wiki/Search\" rel=\"nofollow\">...</a>')\n\n\n\n", "    def test_urlize_no_follow_link(self):\n        self.assertEqual(\n            urlize(\n                \"follow <a href='https://www.example.com'>https://www.example.com</a>\",\n                nofollow=True,\n            ),\n            \"follow <a href='https://www.example.com' rel='nofollow'>https://www.example.com</a>\",\n        )\n\n\n\n\n", "    def test_urlize_with_trim_url_limit(self):\n        tests = (\n            (\n                \"Search for this very long url: http://www.example.com/this-is-a-very-long-url-that-exceeds-the-limit-of-30-characters\"\n                \" and see.\",\n                \"Search for this very long url: http://www.exam...\"\n                \" and see.\",\n            ),\n            (\n                \"Visit http://www.example.com/some/path/to/a/really/long/url\"\n                \" and see.\",\n                \"Visit http://www.example.com/some/path/to/a/re...\"\n                \" and see.\",\n            ),\n            (\"Visit http://longurl.com for more info and see.\",\n             \"Visit http://longurl.com for more info and see.\"),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=30), output)\n\n\n\n"], "sample_5": ["    def test_fast_delete_m2m_no_related_objects(self):\n        t = M2MTo.objects.create()\n        f = M2MFrom.objects.create()\n        # Simulate a scenario where f has no related objects in m2m\n        with self.assertRaises(IntegrityError):\n            f.delete()\n\n\n\n", "    def test_fast_delete_nullable_fk(self):\n        a = A.objects.create()\n        b = B.objects.create()\n        a.nullabel_fk = b\n        a.save()\n        a.delete()\n        self.assertIsNone(a.nullabel_fk)\n\n\n", "    def test_fast_delete_with_m2m_and_fk(self):\n        t = M2MTo.objects.create()\n        f = M2MFrom.objects.create()\n        f.m2m.add(t)\n        u = User.objects.create(avatar=Avatar.objects.create())\n        u.m2m_set.add(f)\n        # 1 query for deleting user, 1 for deleting f's m2m\n        self.assertNumQueries(2, u.delete)\n        self.assertFalse(M2MFrom.objects.exists())\n        self.assertFalse(M2MTo.objects.exists())\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n\n\n\n", "    def test_fast_delete_non_existent_related(self):\n        a = Avatar.objects.create(desc='a')\n        u = User.objects.create()\n        u.avatar = a\n        u.save()\n        a.delete()\n        self.assertEqual(User.objects.count(), 1)\n        self.assertIsNone(User.objects.get(pk=u.pk).avatar_id)\n\n\n\n", "    def test_fast_delete_dependent_on_deleted_object(self):\n        r = R.objects.create()\n        s = S.objects.create(r=r)\n        # First delete s, which depends on r.\n        self.assertNumQueries(2, s.delete)  \n        self.assertFalse(S.objects.exists())\n        self.assertFalse(R.objects.exists())\n\n"], "sample_48": ["    def test_rename_field(self):\n        operation = migrations.RenameField('Model', 'old_field', 'new_field')\n        self.assertEqual(operation.from_field, 'old_field')\n        self.assertEqual(operation.to_field, 'new_field')\n        self.assertEqual(operation.model, 'Model')\n\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            'Model', 'field', models.ForeignKey('Other', models.CASCADE, related_name='model_related' )\n        )\n        self.assertIs(operation.references_field('Other', 'model_related', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)", "    def test_references_field_by_related_fields(self):\n        operation = FieldOperation(\n            'Model', 'field', models.fields.related.OneToOneField('Other', models.CASCADE, related_name='related_field')\n        )\n        self.assertIs(operation.references_field('Other', 'related_field', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n\n", "    def test_references_field_by_m2m_fields(self):\n        operation = FieldOperation(\n            'Model', 'field', models.ManyToManyField('Other', related_name='other_models')\n        )\n        self.assertIs(operation.references_field('Other', 'other_models', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            'Model', 'field', models.ForeignKey('Other', models.CASCADE, related_name='+')\n        )\n        self.assertIs(operation.references_field('Other', '+', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n\n\n"], "sample_43": ["    def test_no_results_message(self):\n        class NoResultsAdmin(QuestionAdmin):\n            search_fields = ['question']\n\n        with model_admin(Question, NoResultsAdmin):\n            self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_question_add'))\n            elem = self.selenium.find_element_by_css_selector('.select2-selection')\n            elem.click()  # Open the autocomplete dropdown.\n            results = self.selenium.find_element_by_css_selector('.select2-results')\n            self.assertTrue(results.is_displayed())\n            option = self.selenium.find_element_by_css_selector('.select2-results__option')\n            self.assertEqual(option.text, 'No results found')\n\n\n", "    def test_search_results_with_related_model_sorting(self):\n        qs = Question.objects.all()\n        qs.update(question='question_with_to_field')\n        qs.create(question='Another question with to_field')\n        with self.subTest(sorting_field='question'):\n            class SortingQuestionAdmin(QuestionAdmin):\n                ordering = ['question']\n            with model_admin(Question, SortingQuestionAdmin):\n                response = AutocompleteJsonView.as_view(**self.as_view_args)(self.factory.get(self.url, {'term': '', **self.opts}))\n                self.assertEqual(response.status_code, 200)\n                data = json.loads(response.content.decode('utf-8'))\n                self.assertEqual([q.question for q in data['results']], ['Another question with to_field', 'question_with_to_field'])\n        with self.subTest(sorting_field='pk'):\n            with model_admin(Question, QuestionAdmin):\n                response = AutocompleteJsonView.as_view(**self.as_view_args)(self.factory.get(self.url, {'term': '', **self.opts}))\n                self.assertEqual(response.status_code, 200)\n                data = json.loads(response.content.decode('utf-8'))\n                self.assertEqual([q.pk for q in data['results']], list(qs.order_by('pk').values_list('pk', flat=True)))\n\n\n\n", "    def test_non_admin_user_permission_denied_on_autocomplete_view(self):\n        user = User.objects.create_user(\n            username='non_admin', password='secret', email='non_admin@example.com'\n        )\n        self.admin_login(username='non_admin', password='secret', login_url=reverse('autocomplete_admin:index'))\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n        elem.click()  # Open the autocomplete dropdown.\n        results = self.selenium.find_element_by_css_selector('.select2-results')\n        self.assertFalse(results.is_displayed())\n        with self.assertRaises(self.selenium.common.exceptions.NoSuchElementException):\n            self.selenium.find_element_by_css_selector('.select2-results__option') \n", "    def test_permissions_denied_on_redirect(self):\n        request = self.factory.get(self.url, {'term': 'is', **self.opts})\n        request.user = self.user\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, reverse('admin:index'))\n", "    def test_complex_filter(self):\n        class ComplexFilterQuestionAdmin(QuestionAdmin):\n            ordering = ['pk']\n            complex_filter = 'complex_filter_field'\n\n        with model_admin(Question, ComplexFilterQuestionAdmin):\n            # Create questions\n            q1 = Question.objects.create(question='question 1', complex_filter_field='a')\n            q2 = Question.objects.create(question='question 2', complex_filter_field='b')\n            q3 = Question.objects.create(question='question 3', complex_filter_field='c')\n\n        request = self.factory.get(self.url, {'term': 'question', 'complex_filter': 'a', **self.opts})\n        request.user = self.superuser\n        with self.select2_ajax_wait():\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q1.pk), 'text': q1.question} ],\n            'pagination': {'more': False},\n        })\n\n\n\n"], "sample_39": ["    def test_reordering_lookbehind_lookahead(self):\n        with self.subTest(url='/lookbehind+/a-city/'):\n            with self.assertRaises(TypeError):\n                resolve('/lookbehind+/a-city/')\n        \n", "    def test_include_app_name_namespace_without_urlconf(self):\n        with self.assertRaises(ImproperlyConfigured):\n            include((self.url_patterns, 'app_name'), namespace='namespace', urlconf=None)\n", "    def test_lookahead_regex(self):\n        test_urls = [\n            ('/lookahead-/a-city/', '(?<city>[^/]+)',),\n            ('/lookbehind-/a-city/', '(?!.*a-city)--(?!.*a-city-).+',),\n            ('/lookahead+/a-city/', '(?<city>[^/]+)',),\n            ('/lookbehind+/a-city/', '(?!.*a-city)--(?!.*a-city-).+',),\n        ]\n        for url, regex, in test_urls:\n            with self.subTest(url=url):\n                match = resolve(url)\n                self.assertEqual(match.url_name, 'lookahead-regex') or self.assertEqual(match.url_name, 'lookbehind-regex')\n                self.assertEqual(match.regex, regex)\n\n\n\n\n", "    def test_lookahead_after_optional_group(self):\n        test_urls = [\n            ('/optional/lookahead+/a-city/', {'path': 'optional', 'city': 'a-city'}),\n            ('/optional/lookahead+/not-a-city/', {'path': 'optional', 'city': 'not-a-city'}),\n            ('/optional/lookahead-/a-city/', {'path': 'optional', 'city': 'a-city'}),\n            ('/optional/lookahead-/not-a-city/', {'path': 'optional', 'city': 'not-a-city'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url, kwargs=kwargs):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n\n\n\n        test_urls = [\n            ('optional/lookahead-positive', {'path': 'optional', 'city': 'a-city'}, '/optional/lookahead+/a-city/'),\n            ('optional/lookahead-negative', {'path': 'optional', 'city': 'a-city'}, '/optional/lookahead-/a-city/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n", "    def test_precedence_in_conflict(self):\n        with self.subTest(precedence_conflict=True):\n            test_urls = [\n                ('/lookbehind-/a-city/', {'city': 'a-city'}),\n                ('/lookahead-/a-city/', {'city': 'a-city'}),\n            ]\n            for test_url, expected_kwargs in test_urls:\n                with self.subTest(url=test_url):\n                    self.assertEqual(resolve(test_url).kwargs, expected_kwargs)\n\n            for name, kwargs, expected in [\n                ('lookahead-positive', {'city': 'a-city'}, '/lookahead+/a-city/'),\n                ('lookahead-negative', {'city': 'a-city'}, '/lookahead-/a-city/'),\n            ]:\n                with self.subTest(name=name, kwargs=kwargs):\n                    self.assertEqual(reverse(name, kwargs=kwargs), expected)\n\n"], "sample_61": ["    def test_locale_grouping(self):\n        with self.settings(USE_THOUSAND_SEPARATOR=True, LANGUAGE_CODE=\"de_DE\"):\n            self.assertEqual(\n                nformat(1234567890, \".\", grouping=3, thousand_sep=\".\"), \"1.234.567.890\"\n            )\n", "    def test_grouping_with_non_uniform_digits(self):\n        self.assertEqual(\n            nformat(1234567890, \".\", grouping=(3, 2, 0), thousand_sep=\",\"),\n            \"123,45,678,90\",\n        )\n        self.assertEqual(\n            nformat(12345678901234567890, \".\", grouping=(3, 2, 0), thousand_sep=\",\"),\n            \"123,456,789,0123,456,789,0\",\n        )\n", "    def test_grouping_with_zeros(self):\n        with self.settings(USE_THOUSAND_SEPARATOR=True):\n            self.assertEqual(nformat(1234567890, \".\", grouping=3, thousand_sep=\",\"), \"1,234,567,890\")\n            self.assertEqual(nformat(1234567890123456, \".\", grouping=3, thousand_sep=\",\"), \"1,234,567,890,123,456\")\n            self.assertEqual(nformat(0, \".\", grouping=3, thousand_sep=\",\"), \"\")\n            self.assertEqual(nformat(10000, \".\", grouping=3, thousand_sep=\",\", force_grouping=True), \"10,000\")\n            self.assertEqual(nformat(100000, \".\", grouping=3, thousand_sep=\",\", force_grouping=True), \"100,000\")\n", "    def test_non_numeric_input(self):\n        self.assertEqual(nformat(\"abc\", \".\"), \"abc\")\n        self.assertEqual(nformat(True, \".\"), \"True\")\n        self.assertEqual(nformat(False, \".\"), \"False\")\n        self.assertEqual(nformat({}, \".\"), \"{}\")\n", "    def test_negative_grouping(self):\n        self.assertEqual(\n            nformat(-1234, \".\", grouping=2, thousand_sep=\",\", force_grouping=True),\n            \"-12,34\",\n        )\n"], "sample_85": ["compilation error", "def test_log_file_handler_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_handler.level == logging.WARNING\n            logging.getLogger('catchlog').info(\"This log message won't be shown\")\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level=DEBUG\n        \"\"\".format(\n            log_file\n        )\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"test_log_file_handler_level.py PASSED\"])\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"This log message will be shown\" in contents\n        assert \"This log message won't be shown\" in contents\n\n", "        def test_custom_log_handler(request):\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_handler.level == logging.WARNING\n            logging.getLogger('catchlog').info(\"This log message won't be shown\")\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            print('PASSED')", "    def test_filtered_log(testdir):\n        log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file={}\n            log_file_level = WARNING\n            log_cli=true\n            \"\"\" .format(log_file)\n        )\n        testdir.makeconftest(\n            \"\"\"\n            import logging\n\n                config.pluginmanager.getplugin(\"logging-plugin\").log_level = logging.INFO\n\n                logging.info(\"INFO message\")\n                logging.warning(\"WARNING message\")\n                logging.error(\"ERROR message\")\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\"*-- live log start --*\", \"*WARNING message*\", \"*-- live log finish --*\"]\n        )\n        with open(log_file, encoding=\"utf-8\") as rfh:\n            contents = rfh.read()\n            assert \"WARNING message\" in contents\n            assert \"INFO message\" not in contents\n            assert \"ERROR message\" not in contents\n", "compilation error"], "sample_60": ["    def test_serialize_custom_types(self):\n        class MyCustomType:\n                self.value = value\n\n            return f\"MyCustomType({value})\"\n\n        MigrationWriter.register_serializer(MyCustomType, serialize_custom_type)\n        self.assertSerializedEqual(MyCustomType(42))\n\n        MigrationWriter.unregister_serializer(MyCustomType)\n\n\n\n", "    def test_serialize_subclass_with_args(self):\n        class MyCustomField(models.CharField):\n                super().__init__(**kwargs)\n                self.extra_stuff = kwargs.pop(\"extra_stuff\", None)\n\n        # Verify custom field serialization with arguments.\n        field = MyCustomField(max_length=100, extra_stuff=\"some_value\")\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(\n            string,\n            (\n                \"migrations.test_writer.MyCustomField(max_length=100, extra_stuff='some_value')\"\n            ),\n        )\n        self.assertEqual(\n            imports,\n            {\"from migrations.test_writer import MyCustomField\"},\n        )\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n                return (\n                    \"MyCustomField\",\n                    [],\n                    {\"db_type\": self.db_type},\n                )\n\n                return \"my_custom_type\"\n\n        field = MyCustomField()\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(\n            string, \"MyCustomField(db_type='my_custom_type')\"\n        )\n        self.assertEqual(imports, {\"from myapp.models import MyCustomField\"})\n", "    def test_serialize_custom_fields(self):\n        class MyCustomField(models.Field):\n                super().__init__(**kwargs)\n\n                return \"MyCustomField\"\n\n                return \"custom_field\"\n                return \"custom_field_string\"\n        \n        field = MyCustomField()\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, \"models.MyCustomField\")\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.CharField):\n                return (self.__class__.__name__, self.max_length, (), {})\n\n        with self.subTest(CustomField):\n            string, imports = MigrationWriter.serialize(CustomField(max_length=100))\n            self.assertEqual(string, \"models.CharField(max_length=100)\")\n            self.assertEqual(imports, set())\n"], "sample_28": ["    def test_unregister(self):\n        self.site.register(User)\n        self.site.register(Article)\n        model_class_user = apps.get_model('auth', 'User').admin_class\n        self.assertIn(model_class_user, self.site._registry)\n        self.site.unregister(User)\n        self.assertNotIn(model_class_user, self.site._registry)\n", "    def test_is_registered(self):\n        self.assertTrue(site.is_registered(User))\n        self.assertTrue(site.is_registered(Article))\n        self.assertFalse(site.is_registered(Article))  \n", "    def test_is_registered(self):\n        self.assertTrue(self.site.is_registered(Article))\n        self.assertFalse(self.site.is_registered(User))\n", "    def test_register_unregister_models(self):\n        with self.assertRaises(AlreadyRegistered):\n            site.register(Article)\n            site.register(Article)\n        site.unregister(Article)\n        site.register(Article)\n", "    def test_unregister(self):\n        self.site.register(Article)\n\n        with self.assertRaises(TypeError):\n            self.site.unregister(Article)\n\n        with self.assertRaises(ValueError):\n            self.site.unregister(User)\n\n\n\n\n        self.site.unregister(Article)\n\n\n\n        with self.assertRaises(ValueError):\n            self.site.unregister(Article)\n"], "sample_86": ["compilation error", "compilation error", "compilation error", "def test_record_testsuite_property_name_conflict(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"name\", \"test_func1\")\n            record_testsuite_property(\"name\", \"test_func2\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p1_node.assert_attr(name=\"name\", value=\"test_func1\")\n    p2_node.assert_attr(name=\"name\", value=\"test_func2\")\n", "def test_junit_property_ordering(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"a\", 1)\n            record_property(\"b\", 2)\n\n            record_property(\"c\", 3)\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p_nodes = properties_node.find_by_tag(\"property\")\n    assert p_nodes[0].get(\"name\") == \"a\"\n    assert p_nodes[1].get(\"name\") == \"b\"\n    assert p_nodes[2].get(\"name\") == \"c\"\n"], "sample_101": ["def test_pipeline_inverse_transform():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    class InverseTransformer:\n            self.fitted = True\n            return self\n            return X * 2\n            return X * 2\n\n    transf = InverseTransformer()\n    pipe = Pipeline([('transf', transf), ('clf', DummyClassifier())])\n\n    X_transformed = pipe.fit_transform(X)\n    X_back = pipe.inverse_transform(X_transformed)\n    assert_array_almost_equal(X_back, X)\n\n    # Check attributes are set correctly\n    assert transf.fitted is True\n\n\n\n", "def test_pipeline_fit_transform_with_memory():\n    import joblib\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    memory = Memory(location=cachedir, verbose=0)\n    pipe = Pipeline([('transf', DummyTransf()), ('clf', SVC())], memory=memory)\n\n    # Fit and transform without caching\n    pipe.fit(X, y)\n    X_transformed = pipe.transform(X)\n    # Check the cache directory for empty state\n    assert not any(f.endswith('.npy') for f in os.listdir(cachedir))\n\n    # Fit and transform with caching enabled\n    pipe.fit(X, y)\n    X_transformed_cached = pipe.transform(X)\n\n    # Check if the cache directory is populated with .npy files\n    assert any(f.endswith('.npy') for f in os.listdir(cachedir))\n    # Check that the transformed arrays are the same\n    assert_array_equal(X_transformed, X_transformed_cached)\n\n    # Check that the cache is still populated even when refitting the pipeline\n    pipe.fit(X, y)\n    X_transformed_cached_refit = pipe.transform(X)\n    assert_array_equal(X_transformed_cached, X_transformed_cached_refit)\n\n    shutil.rmtree(cachedir)\n", "def test_pipeline_set_params_nested_steps():\n    pipeline = Pipeline([\n        ('a', Pipeline([\n            ('b', DummyRegressor()),\n            ('c', DummyRegressor())\n        ])),\n        ('d', DummyRegressor())\n    ])\n\n    pipeline.set_params(a__b__alpha=0.001, a__c__reg=LinearRegression())\n    \n    # Check if the set_params was correctly applied to the nested steps\n    assert pipeline.named_steps['a'].named_steps['b'].alpha == 0.001\n    assert isinstance(pipeline.named_steps['a'].named_steps['c'], LinearRegression)\n\n    # Check if calling set_params with a nested step that doesn't exist \n    # raises a ValueError\n    assert_raises_regex(ValueError, \"Estimator 'z' not found in nested steps\",\n                        pipeline.set_params, a__z__reg=LinearRegression())\n", "def test_pipeline_memory_with_fit_transform():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(location=cachedir, verbose=10)\n        pipe = Pipeline([('transf', DummyTransf()), ('clf', SVC())],\n                        memory=memory)\n\n        # Test fit_transform with memoization\n        pipe.fit_transform(X, y)\n        # Get the time stamp of the transformer in the cached pipeline\n        ts = pipe.named_steps['transf'].timestamp_\n        # Check that cached_pipe and pipe yield identical results\n        pipe.fit_transform(X, y)\n        assert_array_equal(pipe.predict(X), pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           pipe.named_steps['transf'].means_)\n        assert_equal(ts, pipe.named_steps['transf'].timestamp_)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_invalid_cache_argument():\n    X = np.array([[1, 2]])\n    y = np.array([1, 2])\n\n    # Test with invalid cache arguments\n    with assert_raises(ValueError):\n        Pipeline([('transf', Transf()), ('clf', Mult())],\n                 memory={'invalid': 'key'})\n    with assert_raises(ValueError):\n        Pipeline([('transf', Transf()), ('clf', Mult())],\n                 memory=({'invalid': 'key'}, 'path'))\n\n\n"], "sample_2": ["def test_sip_with_no_crpix():\n    \"\"\"\n    Test handling of SIP with missing CRPIX information in the header.\n    \"\"\"\n    hdr = get_pkg_data_contents(\"data/sip-no_crpix.hdr\")\n    w = wcs.WCS(hdr)\n    assert w.sip is not None\n    assert w.sip.crpix is None\n\n\n", "def test_invalid_sip_missing_coeffs():\n    hdr = get_pkg_data_contents(\"data/sip_missing_coeffs.hdr\")\n    with pytest.raises(ValueError):\n        w = wcs.WCS(hdr)\n", "compilation error", "def test_wcs_errors():\n    \"\"\"\n    Test that some of the functions raise errors as expected.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    with pytest.raises(ValueError):\n        w.wcs_pix2world([1, 2, 3], 0)\n    with pytest.raises(ValueError):\n        w.wcs_world2pix([1, 2], 0)\n    with pytest.raises(NotImplementedError):\n        w.wcs_transform([[1, 2], [3, 4]], 0)\n    with pytest.raises(TypeError):\n        w.wcs_pix2world(\"a\", 0)\n", "def test_world_to_pixel_non_standard_crval():\n    \"\"\"\n    Test wcs.wcs.world2pix with non-standard CRVAL values\n    \"\"\"\n    # this header has some unusual CRVAL values\n    hdr = get_pkg_data_contents(\"data/nonstandard_units.hdr\")\n    w = wcs.WCS(hdr)\n\n    # Test that it handles the non-standard units correctly\n    ra, dec = w.wcs_world2pix(0.0, 0.0, 0)\n    assert_allclose(ra, 0.0)\n    assert_allclose(dec, 0.0)\n"], "sample_155": ["def test_issue_27910():\n    from sympy.physics.units import Quantity\n    from sympy.physics.units.definitions import (\n        Plank_action, Planck_constant, Planck_length\n    )\n\n    # Check that quantities with symbolic expressions in the dimensional formula\n    # can be converted\n    p = Quantity('p')\n    SI.set_quantity_dimension(p, Planck_action)\n    p.set_global_relative_scale_factor(1, Planck_action)\n    assert convert_to(p, Planck_constant).simplify() == 1\n    assert convert_to(p, Planck_length).simplify() == Planck_length\n\n\n\n", "def test_issue_24062():\n    from sympy.core.numbers import E\n    from sympy.physics.units import impedance, capacitance, time, ohm, farad, second\n\n    R = Quantity('R')\n    C = Quantity('C')\n    T = Quantity('T')\n    SI.set_quantity_dimension(R, impedance)\n    SI.set_quantity_dimension(C, capacitance)\n    SI.set_quantity_dimension(T, time)\n    R.set_global_relative_scale_factor(1, ohm)\n    C.set_global_relative_scale_factor(1, farad)\n    T.set_global_relative_scale_factor(1, second)\n    expr = T / (R * C)\n    dim = SI._collect_factor_and_dimension(expr)[1]\n    assert SI.get_dimension_system().is_dimensionless(dim)\n\n    exp_expr = 1 + exp(expr)\n    assert SI._collect_factor_and_dimension(exp_expr) == (1 + E, Dimension(1))\n\n", "def test_get_dimensional_expr_with_units():\n    u = Quantity(\"u\", abbrev=\"u\", dimension=length)\n    v = Quantity(\"v\", abbrev=\"v\", dimension=time)\n    w = Quantity(\"w\", abbrev=\"w\", dimension=mass)\n    assert SI.get_dimensional_expr(u) == length\n    assert SI.get_dimensional_expr(v) == time\n    assert SI.get_dimensional_expr(w) == mass\n    assert SI.get_dimensional_expr(u * v) == length * time\n    assert SI.get_dimensional_expr(u / v) == length / time\n    assert SI.get_dimensional_expr(u ** 2) == length**2\n    assert SI.get_dimensional_expr(u ** 3 / v) == length**3 / time\n    assert SI.get_dimensional_expr(Quantity('1') * u) == length\n    assert SI.get_dimensional_expr(Quantity('1') / u) == 1/length\n", "def test_issue_13768():\n    from sympy.physics.units import gram, kg, meter, second\n    SI.set_quantity_dimension(gram, mass)\n    SI.set_quantity_dimension(kg, mass)\n    SI.set_quantity_dimension(meter, length)\n    SI.set_quantity_dimension(second, time)\n    g = Quantity('g', gram)\n    kgm = Quantity('kgm', kg * meter)\n    assert convert_to(g.convert_to(kgm), kg) == 1\n\n\n\n", "def test_issue_8897():\n    from sympy.physics.units import Quantity\n    from sympy.physics.units.systems.si import SI\n\n    u = Quantity('u', abbrev='U', dim=length*length)\n    SI.set_quantity_dimension(u, u.dimension)\n    v = Quantity('v', abbrev='V', dim=length)\n    SI.set_quantity_dimension(v, v.dimension)\n    \n    assert str(u) == 'U'\n    assert str(v) == 'V'\n    assert u.convert_to(meter) == 1 * u.dimension.base_units[0] ** 2\n    assert v.convert_to(meter) == 1 * v.dimension.base_units[0]\n\n\n"], "sample_92": ["    def test_importorskip_failure_message(testdir):\n        with pytest.raises(\n            pytest.ImportError,\n            match=\"^could not import 'missing_package'.*\",\n        ):\n            pytest.importorskip(\"missing_package\")\n\n", "compilation error", "compilation error", "def test_importorskip_conditionals(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif('False')\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*1 passed*\"]\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif('True')\n            assert 0\n        \"\"\"\n    )\n    result.stdout.fnmatch_lines(\n        [\"*1 skipped*\"]\n    )\n", "        def pytest_xfail_raise(item):\n            raise Exception(\"custom xfail\")"], "sample_44": ["    def test_empty_queryset(self):\n        class EmptyChoicesForm(forms.Form):\n            category = forms.ModelChoiceField(Category.objects.none(), required=False)\n\n        form = EmptyChoicesForm()\n        self.assertEqual(form.fields['category'].choices, [('', '---------')])\n        self.assertTrue(form.is_valid())\n\n\n\n", "    def test_empty_label_with_disabled_field(self):\n        class ArticleForm(forms.ModelForm):\n            categories = forms.ModelMultipleChoiceField(\n                Category.objects.all(),\n                required=False,\n                widget=forms.CheckboxSelectMultiple,\n                disabled=True,\n            )\n\n            class Meta:\n                model = Article\n                fields = ['categories']\n\n        form = ArticleForm()\n        self.assertEqual(form.fields['categories'].empty_label, '---------')\n        self.assertEqual(form.fields['categories'].choices, [])\n", "    def test_choices_with_empty_label_and_to_field_name(self):\n        f = forms.ModelChoiceField(Category.objects.all(),\n                                   empty_label='',\n                                   to_field_name='slug')\n        self.assertEqual(list(f.choices), [\n            ('', ''),\n            (self.c1.slug, 'Entertainment'),\n            (self.c2.slug, 'test'),\n            (self.c3.slug, 'third-test'),\n        ])\n", "    def test_limit_choices_to(self):\n        limit = lambda obj: obj.name == 'A test'\n        f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=limit)\n        self.assertEqual(len(f.choices), 1)\n        f.limit_choices_to = lambda obj: obj.name == 'Entertainment'\n        self.assertEqual(len(f.choices), 1)\n        with self.assertRaises(ValueError):\n            f.clean(self.c3.pk)\n        self.assertEqual(f.clean(self.c2.pk).name, 'A test')\n\n\n\n", "    def test_modelchoicefield_with_empty_label_and_to_field(self):\n        f = forms.ModelChoiceField(Category.objects.all(), to_field_name='slug', empty_label=None)\n        self.assertEqual(list(f.choices), [\n            (self.c1.pk, 'entertainment'),\n            (self.c2.pk, 'test'),\n            (self.c3.pk, 'third-test'),\n        ])\n\n        self.assertEqual(f.clean(self.c1.slug), self.c1)\n\n\n"], "sample_104": ["def test_n_max_elements_to_show_nested():\n    n_max_elements_to_show = 5\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', SVC())\n    ])\n    expected = \"\"\"", "def test_complex_pipeline():\n    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n    from sklearn.linear_model import LogisticRegression\n\n    # A multi-stage pipeline with multiple transformers and different\n    # estimators\n    numeric_features = ['feature1', 'feature2']\n    categorical_features = ['feature3', 'feature4']\n\n    preprocessor = ColumnTransformer([\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n\n    pipeline = Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', LogisticRegression())\n    ])\n\n    expected = \"\"\"", "def test_multi_line_params():\n    lr = LogisticRegression(class_weight='balanced', \n                            C=0.1, \n                            penalty='l2', \n                            random_state=42, \n                            tol=1e-5,\n                            max_iter=1000,\n                            solver='liblinear')\n    expected = \"\"\"", "def test_custom_max_nesting_level():\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True,\n                                  max_nesting_level=2)\n\n    # Render a deeply nested estimator\n    rfe = RFE(RFE(RFE(RFE(LogisticRegression()))))\n    expected = \"\"\"", "def test_special_chars():\n    # Test that special characters are handled correctly in the repr\n    text = \"Test with special characters: \\\"\\' & < > !\"\n    vectorizer = CountVectorizer(input='content', lowercase=False,\n                                  strip_accents=False)\n    vectorizer.fit([text])\n    expected = r\"\"\""], "sample_78": ["def test_cli_override_command(app):\n        click.echo(\"Custom app command\")\n\n    @app.cli.command(\"my_command\")\n        click.echo(\"Default my_command\")\n\n    @app.cli.command(\"my_command\")\n        click.echo(\"Overridden my_command\")\n\n    # Run the default command\n    result = app.test_cli_runner().invoke(args=[\"my_command\"])\n    assert \"Default my_command\" in result.output\n\n    # Run the overridden command\n    result = app.test_cli_runner().invoke(args=[\"my_command\"])\n    assert \"Overridden my_command\" in result.output\n\n    # Run the custom command from the application level\n    result = app.test_cli_runner().invoke(args=[\"my_command\"])\n    assert \"Custom app command\" in result.output\n\n", "    def empty_command(self):\n        pass\n", "def test_cli_commands_with_arguments(app):\n    custom = Blueprint(\"custom\", __name__, cli_group=\"customized\")\n    @custom.cli.command(\"custom_arg\")\n    @click.argument(\"name\")\n        click.echo(f\"Hello, {name}!\")\n\n    app.register_blueprint(custom)\n\n    app_runner = app.test_cli_runner()\n    result = app_runner.invoke(args=[\"customized\", \"custom_arg\", \"World\"])\n    assert \"Hello, World!\" in result.output\n", "def test_cli_context_manager(app, runner):\n    @app.cli.command()\n    @with_appcontext\n        test_var = current_app._get_current_object().test_var\n        assert test_var == \"test value\"\n\n    app.test_var = \"test value\"\n    context = app.test_cli_runner().invoke(args=[\"test_command\"])\n    assert context.exit_code == 0\n    assert \"test value\" in context.output\n\n", "    def test_cli_nested_group(app):\n        \"\"\"Test nested CLI groups created from blueprints.\"\"\"\n        outer = Blueprint(\"outer\", __name__)\n        inner = Blueprint(\"inner\", __name__, cli_group=\"inner\")\n        inner_command = Blueprint(\"inner_command\", __name__)\n\n        @inner_command.cli.command(\"inner_command_func\")\n            click.echo(\"Inner command function\")\n\n        inner.cli.add_command(inner_command.cmd)\n        outer.cli.add_command(inner.cli.command)\n\n        app.register_blueprint(outer)\n\n        app_runner = app.test_cli_runner()\n\n        result = app_runner.invoke(args=[\"outer\", \"inner\", \"inner_command_func\"])\n        assert \"Inner command function\" in result.output\n\n"], "sample_3": ["    def model_b(x, y):\n        return x * y\n    ", "    def model_d(x):\n        return x**2", "    def model_d(x, y):\n        return x * y\n", "    def model_d(x, y):\n        return x + x * y\n", "    def model_d(x, y):\n        return x * y\n"], "sample_148": ["compilation error", "def test_issue_16728():\n    from sympy import (sqrt, cos, sin, exp, Abs,\n                       conjugate, integrate, Piecewise)\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n\n    assert integrate(sqrt(x**2 + y**2)).subs(y, 0).doit() == (x**3/3)**Rational(1, 2)\n    assert integrate(sqrt(x**2 + y**2), (x, 0, 1)).subs(y, 0).doit() == (sqrt(2)/3)\n    assert integrate(sqrt(x**2 + y**2), (x, 0, 1), (y, 0, 1)).subs(y, 0).doit() == (sqrt(2)/3)\n\n    assert integrate(sqrt(exp(x)**2 + y**2), (y, 0, exp(x))).doit() == (exp(x)**2/2)\n    assert integrate(sqrt(Abs(x)**2 + y**2), (y, -Abs(x), Abs(x))).doit() == (x**2)\n\n\n", "compilation error", "def test_issue_17769():\n    from sympy import Abs, arg, re, imag\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Abs(x + I*y).is_real is False\n    assert arg(x + I*y).is_real is False\n    assert re(x + I*y).is_real is True\n    assert imag(x + I*y).is_real is False\n", "def test_issue_17474():\n    from sympy import (re, im, polar_lift,\n                       unpolarify,\n                       Abs,\n                       Matrix)\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    z = Symbol('z', complex=True)\n\n    assert re(x + I*y).conjugate() == x - I*y\n    assert im(x + I*y).conjugate() == -y\n    assert (x + I*y).conjugate() == x - I*y\n    assert (x + I*y).conjugate().conjugate() == x + I*y\n\n    assert re(polar_lift(x + I*y)).conjugate() == re(x + I*y)\n    assert im(polar_lift(x + I*y)).conjugate() == -im(x + I*y)\n    assert (polar_lift(x + I*y)).conjugate() == polar_lift(x - I*y)\n\n    assert unpolarify(polar_lift(x + I*y)).conjugate() == x - I*y\n\n    a = Matrix([[x, y], [0, x]])\n    assert a.conjugate().transpose() == a.transpose().conjugate()\n    assert (a.conjugate().transpose()**2).subs(x, I*y) == Matrix([[I*y, -x], [x, I*y]])\n\n"], "sample_11": ["    def test_serialize_custom_model_fields(self):\n        class MyCharField(models.CharField):\n                self.custom_arg = kwargs.pop('custom_arg', None)\n                super().__init__(*args, **kwargs)\n\n                return ('migrations.test_writer.MyCharField', [], {'custom_arg': self.custom_arg})\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.AddField(\"mymodel\", \"myfield\", MyCharField(max_length=10, custom_arg='value'))\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertEqual(\n            output,\n            \"models.AddField(name='myfield', field=migrations.test_writer.MyCharField(max_length=10, custom_arg='value'))\"\n        )\n", "    def test_serialize_lazy_object(self):\n        class LazyString(LazyStringMixin):\n                return self.string\n\n        string = LazyString(\"hello\")\n        self.assertSerializedEqual(string)\n", "    def test_serialize_custom_models(self):\n        class CustomField(models.Field):\n                return 'custom_type'\n\n                return forms.CharField(**kwargs)\n\n        class CustomModel(models.Model):\n            my_field = CustomField()\n\n        with mock.patch('django.db.models.get_field', side_effect=CustomField):\n            migration = type(\"Migration\", (migrations.Migration,), {\n                \"operations\": [\n                    migrations.CreateModel(\"CustomModel\", (CustomModel,), {})\n                ],\n            })\n            writer = MigrationWriter(migration)\n            output = writer.as_string()\n            self.assertEqual(output.count('CustomField'), 1)", "    def test_serialize_deferred_references(self):\n        class DeferredModel(models.Model):\n            name = models.CharField(max_length=255)\n\n        class DeferredManager(models.Manager):\n                raise NotImplementedError\n\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n            deferred_model = models.ForeignKey(DeferredModel, on_delete=models.CASCADE, related_name='my_models')\n            deferred_manager = models.Manager.from_queryset(DeferredModel.objects.all())\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"MyModel\", fields=[\n                    (\"name\", models.CharField(max_length=255)), \n                    (\"deferred_model\", models.ForeignKey(DeferredModel, on_delete=models.CASCADE, related_name='my_models')),\n                    (\"deferred_manager\", models.Manager(DeferredManager)),\n                ]),\n            ],\n            \"dependencies\": [],\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"from . import DeferredModel\", output)\n\n\n\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.CharField):\n                return ('MyCustomField', [], {})\n\n        field = MyCustomField(max_length=100)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"models.CharField(max_length=100)\")\n        self.assertEqual(imports, {'from django.db import models'})\n\n"], "sample_152": ["def test_array_shape_mismatch():\n    for ArrayType in mutable_array_types:\n        A = ArrayType([[1, 2, 3], [4, 5, 6]])\n        B = ArrayType([[1, 2], [3, 4], [5, 6]])\n        raises(ValueError, lambda: A + B)\n        raises(ValueError, lambda: A - B)\n        raises(ValueError, lambda: A * B)\n        raises(ValueError, lambda: A / B)\n\n", "def test_reshape():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2, 3], [4, 5, 6]])\n\n        B = A.reshape((3, 2))\n        assert B.shape == (3, 2)\n        assert list(B) == [[1, 2], [3, 4], [5, 6]]\n\n        raises(ValueError, lambda: A.reshape((4, 2)))\n        raises(ValueError, lambda: A.reshape((3, 3)))\n\n        C = A.reshape((1, 6))\n        assert C.shape == (1, 6)\n        assert list(C) == [[1, 2, 3, 4, 5, 6]]\n\n        D = A.reshape((6, 1))\n        assert D.shape == (6, 1)\n        assert list(D) == [[1], [4], [7], [2], [5], [8]]\n\n\n\n", "def test_issue_20859():\n    a = MutableDenseNDimArray([[1, 2], [3, 4]])\n    b = MutableDenseNDimArray([[5, 6], [7, 8]])\n    c = a * b\n    assert isinstance(c, MutableDenseNDimArray)\n    assert c.shape == a.shape\n    assert list(c) == [5, 12, 21, 32]\n", "def test_array_arithmetic_with_scalar():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2], [3, 4]])\n        assert A * 2 == ArrayType([[2, 4], [6, 8]])\n        assert 2 * A == ArrayType([[2, 4], [6, 8]])\n        assert A / 2 == ArrayType([[0.5, 1], [1.5, 2]])\n        assert A + 5 == ArrayType([[6, 7], [8, 9]])\n        assert 5 + A == ArrayType([[6, 7], [8, 9]])\n        assert A - 2 == ArrayType([[ -1, 0], [1, 2]])\n        assert 2 - A == ArrayType([[-3, -4], [-2, -2]])\n\n\n\n", "def test_tensor_product():\n    from sympy.tensor.array.dense_ndim_array import MutableDenseNDimArray\n    a = MutableDenseNDimArray([[1, 2], [3, 4]])\n    b = MutableDenseNDimArray([[5, 6], [7, 8]])\n    c = a.tensorproduct(b)\n    expected = MutableDenseNDimArray([[5, 6, 10, 12],\n                                     [7, 8, 15, 16]])\n    assert c == expected\n"], "sample_106": ["    def test_precomputed_init(init):\n        # Test that precomputed initialization works correctly.\n\n        X = np.random.rand(10, 5)\n        y = np.random.randint(0, 2, size=10)\n        n_components = 3\n\n        # Generate some random transformation matrix.\n        transformation = np.random.rand(n_components, X.shape[1])\n        nca = NeighborhoodComponentsAnalysis(init=transformation,\n                                            n_components=n_components)\n\n        # Fit the model\n        nca.fit(X, y)\n\n\n", "def test_random_state_consistency():\n    rng = np.random.RandomState(42)\n    X_train, X_test, y_train, y_test = train_test_split(iris_data,\n                                                        iris_target,\n                                                        test_size=0.2,\n                                                        random_state=42)\n    nca_1 = NeighborhoodComponentsAnalysis(n_components=2, random_state=rng)\n    nca_2 = NeighborhoodComponentsAnalysis(n_components=2, random_state=rng)\n    nca_1.fit(X_train, y_train)\n    nca_2.fit(X_train, y_train)\n\n    assert_array_almost_equal(nca_1.components_, nca_2.components_)\n\n    nca_3 = NeighborhoodComponentsAnalysis(n_components=2, random_state=43)\n    nca_3.fit(X_train, y_train)\n    assert_array_not_equal(nca_1.components_, nca_3.components_)\n", "def test_transform_init_shape(init_type, shape):\n    # check that the dimension of transform is correct.\n\n    rng = np.random.RandomState(42)\n    X = iris_data\n    y = iris_target\n    nca = NeighborhoodComponentsAnalysis(init=init_type, random_state=42)\n    nca.fit(X, y)\n    # n_components must be set to something else than init.shape[0] or\n    # nca.transform will throw an error.\n    nca = NeighborhoodComponentsAnalysis(init=init_type,\n                                        n_components=X.shape[1],\n                                        random_state=42)\n    nca.fit(X, y)\n    nca_trans = nca.transform(X)\n    assert nca_trans.shape[1] == shape[0]\n\n\n", "def test_random_state_reproducibility():\n    rng = np.random.RandomState(42)\n\n    # Test that random initialization reproduces the same results\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n    nca_1 = NeighborhoodComponentsAnalysis(\n        init='random', random_state=rng, max_iter=10)\n    nca_1.fit(X, y)\n\n    nca_2 = NeighborhoodComponentsAnalysis(\n        init='random', random_state=rng, max_iter=10)\n    nca_2.fit(X, y)\n\n    assert_array_almost_equal(nca_1.components_, nca_2.components_)\n\n\n\n", "def test_n_components_invalid(n_components):\n    # Test that ValueError is raised for invalid n_components\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components)\n    assert_raise_message(ValueError,\n                         'The number of components ({}) must be '\n                         'greater than 0.'.format(n_components),\n                         nca.fit, X, y)\n\n"], "sample_59": ["    def test_custom_widget_rendering(self):\n        class MyTextInput(forms.TextInput):\n                attrs = attrs or {}\n                attrs['class'] = 'custom-widget'\n                return super().render(name, value, attrs)\n\n        class MyForm(forms.Form):\n            my_field = forms.CharField(widget=MyTextInput)\n\n        ChoiceFormSet = formset_factory(MyForm)\n        formset = ChoiceFormSet()\n        rendered_html = str(formset)\n        self.assertIn('class=\"custom-widget\"', rendered_html)\n", "    def test_extra_form_rendering(self):\n        ChoiceFormSet = formset_factory(Choice, extra=2)\n        formset = ChoiceFormSet()\n        self.assertEqual(len(formset), 2)\n        self.assertHTMLEqual(\n            str(formset),\n            f\"<ul class='errorlist nonfield'>\"\n            \"</ul>\\n<div>\"\n            f\"<input type='hidden' name='form-TOTAL_FORMS' value='2'>\"\n            f\"<input type='hidden' name='form-INITIAL_FORMS' value='0'>\"\n            f\"<input type='hidden' name='form-MIN_NUM_FORMS' value='0'>\"\n            f\"<input type='hidden' name='form-MAX_NUM_FORMS' value='0'>\"\n            f\"<div><div><input type='text' name='form-0-choice' value='' \"\n            f\"id='id_form-0-choice'></div><div><input type='number' name='form-0-votes' value='' \"\n            f\"id='id_form-0-votes'></div></div>\"\n            f\"<div><div><input type='text' name='form-1-choice' value='' \"\n            f\"id='id_form-1-choice'></div><div><input type='number' name='form-1-votes' value='' \"\n            f\"id='id_form-1-votes'></div></div>\\n\",\n        )\n", "    def test_hidden_management_form_fields(self):\n        from django.forms.renderers import DjangoTemplates\n\n        with isolate_lru_cache(get_default_renderer), self.settings(\n            FORM_RENDERER=\"django.forms.renderers.DjangoTemplates\"\n        ):\n            ChoiceFormSet = formset_factory(Choice, formset=BaseFormSet)\n            formset = ChoiceFormSet()\n            rendered_html = str(formset.management_form)\n            self.assertIn('<input type=\"hidden\" name=\"form-TOTAL_FORMS\"', rendered_html)\n            self.assertIn('<input type=\"hidden\" name=\"form-INITIAL_FORMS\"', rendered_html)\n            self.assertIn('<input type=\"hidden\" name=\"form-MAX_NUM_FORMS\"', rendered_html)\n            self.assertIn('<input type=\"hidden\" name=\"form-MIN_NUM_FORMS\"', rendered_html)\n", "    def test_formset_initial_data_without_initial_form_count(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertEqual(formset.initial_form_count(), 0)\n\n", "    def test_extra_forms_added_to_management_form(self):\n        with isolate_lru_cache(get_default_renderer), self.settings(\n            FORM_RENDERER=\"django.forms.renderers.DjangoTemplates\"\n        ):\n            ChoiceFormSet = formset_factory(Choice, extra=2)\n            formset = ChoiceFormSet()\n            management_form_html = str(formset.management_form)\n            self.assertIn(\n                '<input type=\"hidden\" name=\"form-TOTAL_FORMS\" value=\"2\">', management_form_html\n            )\n            self.assertIn(\n                '<input type=\"hidden\" name=\"form-INITIAL_FORMS\" value=\"0\">', management_form_html\n            )\n            self.assertIn(\n                '<input type=\"hidden\" name=\"form-MAX_NUM_FORMS\" value=\"2\">', management_form_html\n            )\n"], "sample_119": ["def test_user_functions():\n    mcode.known_functions.update({'my_func': [(lambda x: True, 'MyFunc')]})\n    assert mcode(my_func(x)) == 'MyFunc[x]'\n\n\n", "def test_user_functions():\n    settings = {'user_functions': {'my_func': [(lambda x: True, 'MyFunc[x]') ]}}\n    assert mcode(my_func(x), **settings) == \"MyFunc[x]\" \n", "def test_user_defined_functions():\n    assert mcode(user_func(x)) == \"user_func[x]\"\n    \n    class user_func(Function):\n            super(user_func, self).__init__(\"user_func\", *args)\n\n    user_func = user_func(x)\n    \n    settings = {'user_functions': {'user_func': [(lambda x: True, 'my_func')]}}\n    assert mcode(user_func(x), **settings) == \"my_func[x]\"\n", "compilation error", "def test_user_function():\n        return x**2 + 1\n    MCodePrinter.known_functions['myfunc'] = [(lambda x: True, 'myfunc')]\n    assert mcode(myfunc(x)) == \"myfunc[x]\"\n"], "sample_95": ["def test_invalid_skip_condition(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"abc\", reason=\"skip_if\")\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n\n\n", "def test_relpath_with_parent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        **{\n            \"dir1/test_1.py\": \"\"\"\n        import pytest\n        @pytest.mark.skip()\n            pass\n            \"\"\",\n        }\n    )\n    result = pytester.runpytest(\"-rs\", \"dir1/test_1.py\", \"--rootdir=.\")\n    result.stdout.fnmatch_lines(\n        [\"SKIPPED [[]1[]] dir1/test_1.py:2: unconditional skip\"]\n    )\n", "def test_importorskip_with_submodule(pytester: Pytester) -> None:\n    pytester.mkdir(\"sub\")\n    pytester.set_file(\"sub/module.py\", \"\"\"\n            pass\n    \"\"\")\n    with pytest.raises(\n        pytest.skip.Exception,\n        match=\"^could not import 'sub.module': No module named 'sub'\\.\",\n    ):\n        pytest.importorskip(\"sub.module\")\n\n\n\n", "def test_importorskip_with_reason(pytester: Pytester) -> None:\n    with pytest.raises(\n        pytest.skip.Exception,\n        match=\"^Skipping 'doesnotexist' due to reason: this is a reason$\",\n    ):\n        pytest.importorskip(\"doesnotexist\", reason=\"this is a reason\")\n", "def test_importorskip_in_conftest(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        pytest.importorskip(\"doesnotexist\")\n            parser.addoption(\"--fake-import\", action='store_true')\n\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.parametrize(\"fake_import\", [True, False])\n            pass\n    \"\"\"\n    )\n    result = pytester\n    result.runpytest(\"--fake-import\")\n    result.stdout.fnmatch_lines(\n        [\"*test_import_with_param*\"]\n    )\n\n"], "sample_25": ["    def test_add_field_after_model_deletion(self):\n        \"\"\"\n        Adding a field to a model that no longer exists should produce a meaningful\n        error message.\n        \"\"\"\n        with self.assertRaises(migrations.MigrationError) as context:\n            changes = self.get_changes(\n                [], [ModelState('testapp', 'Author', [\n                    ('id', models.AutoField(primary_key=True)),\n                    ('name', models.CharField(max_length=200)),\n                ])]\n            )\n        self.assertIn(\"Cannot add field to model 'A' because it has been deleted.\", str(context.exception))\n", "    def test_remove_model_with_fk_to_pk_model(self):\n        \"\"\"\n        #23100 - Deleting a model with a foreign key to another model's primary key\n        should trigger the correct dependency.\n        \"\"\"\n        A = ModelState(\"a\", \"A\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        B = ModelState(\"a\", \"B\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"a_id\", models.ForeignKey(\"a.A\", models.CASCADE)),\n        ])\n        changes = self.get_changes([A, B], [A])\n        self.assertNumberMigrations(changes, 'a', 1)\n        self.assertOperationTypes(changes, 'a', 0, [\"DeleteModel\"])\n        self.assertMigrationDependencies(changes, 'a', 0, [('a', '__first__')])\n\n\n\n", "    def test_add_model_with_field_removed_from_base_model(self):\n        \"\"\"\n        Removing a base field takes place before adding a new inherited model\n        that has a field with the same name.\n        \"\"\"\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'book', [\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "    def test_rename_field_in_mti_base_model(self):\n        \"\"\"\n        Renaming a field in a base model correctly propagates to inherited models.\n        \"\"\"\n        before = [\n            ModelState('app', 'animal', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ]),\n            ModelState('app', 'dog', [], bases=('app.animal',)),\n        ]\n        after = [\n            ModelState('app', 'animal', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),  # Renamed\n            ]),\n            ModelState('app', 'dog', [], bases=('app.animal',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RenameField', 'null', \n                                                     'null'])\n\n\n\n", "    def test_multiple_operations_different_types(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RenameField('Person', 'id', 'user_id'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_person_id')\n\n"], "sample_47": ["    def test_migrate_restores_state_after_error(self):\n        \"\"\"\n        Test that migrate gracefully handles errors during app\n        migration.\n\n        - Migrate to a state where a second migration fails.\n        - Check the existing state should be preserved.\n        \"\"\"\n        class FakeError(Exception):\n            pass\n\n        class Migration(migrations.Migration):\n                if self.name == '0002_second':\n                    raise FakeError(\n                        \"This migration is intentionally failing\"\n                    )\n                super().apply(project_state, schema_editor, collect_sql)\n\n        executor = MigrationExecutor(None)\n        graph = MigrationGraph()\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = Migration(\n            '0002_second',\n            migrations.CreateModel('failed_model', [\n                migrations.AutoField(primary_key=True),\n            ]),\n        )\n        a2 = ('a', '2')\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_dependency(None, a2, a1)\n\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n        })\n\n        with self.assertRaises(FakeError):\n            executor.migrate(['a'], [a1, a2])\n        # Assert the schema is as expected after the error.\n        self.assertIs(\n            executor.loader.graph.nodes[a1].applied,\n            True\n        )\n        self.assertIs(\n            executor.loader.graph.nodes[a2].applied,\n            False\n        )\n\n\n\n", "    def test_minimize_rollbacks_circular(self):\n        \"\"\"\n        Minimize rollbacks when there's a circular dependency.\n\n        a: 1 <--- 2\n        b:    \\- 1          <--- 3\n        c:     \\- 1\n\n        If a1 is applied already, and we're asked to migrate to a1,\n        we shouldn't rollback b1 or c1, even though b1 depends on a1\n        and c1 depends on b1.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, b1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n\n\n", "    def test_minimize_rollbacks_circular_dependency(self):\n        \"\"\"\n        Minimize rollbacks when dealing with cyclic dependencies in migrations.\n\n        This test ensures that even in the presence of cyclic dependencies\n        between migrations, the executor minimizes the number of migrations\n        rolled back.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a1, b1)\n        graph.add_dependency(None, b1, a2)\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n        plan = executor.migration_plan({a1})\n        self.assertEqual(plan, [(a2_impl, True)])\n", "    def test_migrate_with_no_migrations_applied(self):\n        executor = MigrationExecutor(None)\n        # When there are no applied migrations, migration plan should be empty\n        self.assertEqual(executor.migration_plan([]), [])\n", "    def test_minimize_rollbacks_circular_dependency(self):\n        \"\"\"Minimize rollbacks when target has circular dependency.\n        \n        a1 <---- a2\n         \\     /\n          \\--- b1\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a1, a2)\n        graph.add_dependency(None, b1, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n"], "sample_130": ["def test_issue_16993():\n    if not mpmath:\n        skip(\"mpmath not installed\")\n    f = lambdify(x, sqrt(x), 'mpmath')\n    assert f(16) == 4\n", "def test_issue_14941_2():\n    x, y = symbols('x y')\n    f = lambdify([x, y], {x: x + y, y: x * y}, 'sympy')\n    assert f(1, 2) == {x: 3, y: 2}\n", "def test_issue_15147():\n    x, y = symbols('x y')\n    f = lambdify((x, y), x**y)\n    assert f(2, 2) == 4\n\n\n", "def test_issue_15607():\n    # Test lambdify with matrices as arguments\n    if not numpy:\n        skip(\"numpy not installed\")\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    f = lambdify((A, B), A + B, 'numpy')\n    assert f(A, B) == Matrix([[6, 8], [10, 12]])\n", "def test_issue_10992():\n    if not numpy:\n        skip(\"numpy not installed\")\n    f = lambdify(x, 2*x**2 + 1, \"numpy\")\n    assert f([[1, 2]]).shape == (2,)\n    assert f([[1, 2]]).dtype == numpy.float64\n\n\n\n"], "sample_17": ["    def test_deserialize_large_data_string(self):\n        # deserialize_db_from_string() handles large data strings.\n        from io import StringIO\n\n        # Create a large data string with many objects.\n        data_string = StringIO()\n        for i in range(1000):\n            data_string.write(f'{{\"model\": \"backends.object\", \"pk\": {i}, \"fields\": {{}}}')\n            data_string.write(\"\\n\")\n        data_string.seek(0)\n\n        \n        connection.creation.deserialize_db_from_string(data_string.read())\n        \n        objects = Object.objects.all()\n        self.assertEqual(len(objects), 1000)\n\n", "    def test_large_data(self):\n        # deserialize_db_from_string() handles large amounts of data.\n        # This test creates a large test dataset and deserializes it.\n        # You will need to adjust the number of objects based on your system's resources\n        # and database engine capabilities.\n        from django.db import transaction\n        from io import StringIO\n\n        with transaction.atomic():\n            for i in range(1000):\n                Object.objects.create(obj_ref=None)\n            with open('large_data.json', 'w') as f:\n                serialized_data = serializers.serialize('json', Object.objects.all(), indent=None, stream=f)\n\n        with transaction.atomic():\n            Object.objects.all().delete()\n            data = StringIO(serialized_data.getvalue())\n            connection.creation.deserialize_db_from_string(data.getvalue())\n            self.assertEqual(Object.objects.count(), 1000)\n", "    def test_deserialize_db_from_string_with_constraints(self):\n        # deserialize_db_from_string() handles constraint checks after deserialization.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"obj_ref\": 1, \"related_objects\": []}\n            },\n            {\n                \"model\": \"backends.objectreference\",\n                \"pk\": 1,\n                \"fields\": {\"obj\": 1}\n            }\n        ]\n        \"\"\"\n        Object.objects.create(obj_ref=None)  # Create an object with a foreign key violating a constraint to test constraint check\n        connection.creation.deserialize_db_from_string(data)\n        with self.assertRaises(Exception):\n            Object.objects.get(pk=1)  # Should raise an exception due to constraint violation\n\n\n\n", "    def test_large_serialized_data(self):\n        # deserialize_db_from_string() can handle large amounts of data.\n        from io import StringIO\n\n        # Generate some large serialized data\n        large_data = \"\\n\".join([\"{\"\n                    f'model: \"backends.object\", pk: {i}, fields: {{ \"field1\": \"value{i}\" }}'\n                    \"}\" for i in range(1000)])\n\n        serialized_data = StringIO(large_data)\n        connection.creation.deserialize_db_from_string(serialized_data.getvalue())\n\n        # Verify that all objects are present\n        objects = Object.objects.all()\n        self.assertEqual(len(objects), 1000)\n", "    def test_deserialize_db_from_string_with_large_data(self):\n        # deserialize_db_from_string() handles large data sets.\n        data = \"\"\"\n        [\n            {\"model\": \"backends.object\", \"pk\": 1, \"fields\": {\"large_data\": \"A\" * 1000000}}\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        obj = Object.objects.get()\n        self.assertEqual(len(obj.large_data), 1000000)\n"], "sample_4": ["    def test_set_cookie(self):\n        response = HttpResponse()\n        response.set_cookie('key', 'value')\n        self.assertIn('Set-Cookie: key=value', response.headers)  \n\n\n\n", "    def test_cookie_sorting(self):\n        \"\"\"\n        Test that cookies are sorted alphabetically by name.\n        \"\"\"\n        c = SimpleCookie()\n        c['c'] = '3'\n        c['a'] = '1'\n        c['b'] = '2'\n        self.assertEqual(c.output().splitlines(), [\n            'a=1',\n            'b=2',\n            'c=3'\n        ])\n\n", "    def test_cookie_with_domain(self):\n        request = HttpRequest(\n            method=\"GET\",\n            path=\"/\",\n            protocol=\"HTTP/1.1\",\n            headers={'cookie': 'name=value; Domain=example.com'},\n        )\n        response = HttpResponse()\n        response.set_cookie('name', 'new_value', domain='example.com')\n        response._set_cookie_response_headers(request)\n        self.assertContains(response, 'Set-Cookie: name=new_value; Domain=example.com')\n\n\n", "    def test_cookie_creation(self):\n        c = SimpleCookie()\n        c['name'] = 'value'\n        self.assertEqual(c['name'].value, 'value')\n        self.assertEqual(c['name']['expires'], None)\n\n        c['name'] = 'value', {'expires': 'sun, 19 aug 2024 12:00:00 gmt'}\n        self.assertEqual(c['name'].value, 'value')\n        self.assertEqual(c['name']['expires'].strftime('%a, %d %b %Y %H:%M:%S GMT'), 'sun, 19 aug 2024 12:00:00 GMT')\n\n        c['name'] = 'value', {'max-age': 3600}\n        self.assertEqual(c['name'].value, 'value')\n        self.assertEqual(c['name']['max_age'], 3600)\n", "    def test_cookie_with_spaces(self):\n        # Spaces in cookie values are allowed and should be preserved.\n        self.assertEqual(parse_cookie('name=\"value with spaces\"'), {'name': 'value with spaces'})\n"], "sample_131": ["def test_Complex():\n    assert mcode(complex(3,4)) == \"3 + 4 I\"\n", "def test_complex_expressions():\n    assert mcode(sin(x) * exp(2*x) + cos(x) / (x**2 + 1)) == \\\n        \"Sin[x]*Exp[2*x] + Cos[x]/(x^2 + 1)\"\n    assert mcode(Integral(x**2 * sin(x), (x, 0, pi))) == \\\n        \"Hold[Integrate[x^2*Sin[x], {x, 0, Pi}]]\"\n    assert mcode((x + 1)**(2/3) * (x**2 - 1)**(-1/4)) == \\\n        \"(x + 1)^(2/3)*(x^2 - 1)^(-1/4)\"\n", "def test_user_functions():\n    assert mcode(myfunc(x)) == \"myfunc[x]\"\n    assert mcode(myfunc(x, y)) == \"myfunc[x, y]\"\n    assert mcode(myfunc(x, y, z)) == \"myfunc[x, y, z]\" \n\n        return x*y + z\n    from sympy import symbols\n    x, y, z = symbols('x y z')\n\n\n", "def test_print_conditional_function():\n    assert mcode(f(x) if x > 0 else 5) == \"If[x > 0, f[x], 5]\"\n", "def test_complex():\n    assert mcode(x + 1j) == \"x + 1 I\"\n    assert mcode(2*x + 3*1j) == \"2 x + 3 I\" \n    assert mcode(x*y*1j) == \"x*y I\"\n    assert mcode(complex(2,3)) == \"2 + 3 I\" \n"], "sample_123": ["compilation error", "def test_issue_10409():\n    assert Float('1.23456789012345678901234567890123', 20) == Float(1.23456789012345678901234567890, 20)\n", "def test_issue_10557():\n    assert Float('1e5') == Float('100000')\n    assert Float('-1e5') == Float('-100000')\n\n", "def test_issue_11001():\n    from sympy.core.numbers import Number\n    class MyNumber(Number):\n            self.value = value\n            return self.value\n            return int(self.value)\n    \n    x = MyNumber(1.23)\n    assert Float(x) == Float(1.23)\n\n\n", "def test_Float_from_sympy_integer():\n    x = Integer(123)\n    f = Float(x)\n    assert f == 123.0\n    assert f.is_real == True\n    assert f.is_integer == False\n    assert f._prec == 53 \n"], "sample_32": ["    def test_complex_key_transform_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                key=KeyTransform('f', KeyTransform('1', KeyTransform('d', 'value'))),\n                chain=KeyTransform('f', KeyTransform('1', 'key')),\n                expr=KeyTransform('f', KeyTransform('1', 'key')),\n            ).filter(chain='g'),\n            [self.objs[4]],\n        )\n\n\n\n", "    def test_key_transform_with_case_sensitive_lookup(self):\n        obj = NullableJSONModel.objects.create(value={'Foo': 'bar'})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__Foo='bar'),\n            [obj],\n        )\n", "    def test_key_transform_with_subquery(self):\n        obj = NullableJSONModel.objects.create(\n            value={'f': {'bar': 1, 'baz': 2}, 'g': 3}\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__f__contains=Subquery(\n                    NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value__f__bar')\n                )\n            ),\n            [obj],\n        )\n\n\n\n\n", "    def test_key_transform_with_nested_objects(self):\n        obj = NullableJSONModel.objects.create(\n            value={'d': [{'e': 'f', 'g': {'h': 'i'}}]}),\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__0__g__h=F('value__d__0__e')\n            ),\n            [obj],\n        )\n\n\n", "    def test_nested_key_transform_annotation_expression_with_subquery(self):\n        related_obj = RelatedJSONModel.objects.create(\n            value={'d': ['f', 'e']},\n            json_model=self.objs[4],\n            related_value={'key': 'value'},\n        )\n        RelatedJSONModel.objects.create(\n            value={'d': ['e', 'f']},\n            json_model=self.objs[4],\n            related_value={'key': 'value1'},\n        )\n        self.assertSequenceEqual(\n            RelatedJSONModel.objects.annotate(\n                key=F('value__d'),\n                related_key=F('json_model__value__d'),\n                chain=F('key__1'),\n                expr=Cast('key', models.JSONField()),\n            ).filter(chain=F('related_key__0'), related_value__key='value').exists(),\n            [True],\n        )\n"], "sample_94": ["def test_getstatementrange_issue_107() -> None:\n    source = Source(\n        \"\"\"\n            if x == 1:\n                return x + 1\n            else:\n                return x ** 2\n        \"\"\"\n    )\n    assert str(getstatement(0, source)) == \"def f(x):\"\n    assert str(getstatement(1, source)) == \"    if x == 1:\"\n    assert str(getstatement(2, source)) == \"        return x + 1\"\n    assert str(getstatement(3, source)) == \"    else:\"\n    assert str(getstatement(4, source)) == \"        return x ** 2\"\n", "def test_multiline_with_indentation_and_comments() -> None:\n    source = \"\"\"\\\n        \"\"\" Docstring\n     \\\n        \"\"\"\n        if 1:\n            # Comment 1\n            x = 3\n            \"\"\"\n            Nested comment 1\n            \"\"\"\n            y = 4\n\n    print(x)\n    \"\"\"\n    Docstring\n\n\n    \"\"\"", "def test_getstartingblock_complex() -> None:\n    class A:\n            frame = sys._getframe(1)\n            self.source = Frame(frame).statement\n\n    # fmt: off\n    x = A(\n        'x',\n        'y',\n        'z',\n        # comment\n        {'a': 1},\n    )\n    # fmt: on\n    values = [i for i in x.source.lines if i.strip()]\n    assert len(values) == 5\n\n\n", "def test_indentation_in_string() -> None:\n    source = Source(\n        \"\"\"\n            x = \"\"\"\\\n            a = 1\n            b = 2\n            \"\"\"\n    \"\"\"\n    )\n    assert str(source.getstatement(2)).strip() == \"    x = \"\"\"\\n        a = 1\\n        b = 2\\n    \"\"\"\n", "def test_multiline_string_in_if() -> None:\n    source = \"\"\""], "sample_115": ["def test_set_output_dataframe_index():\n    \"\"\"Check that set_output with a DataFrame index works.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=[\"a\", \"b\"])\n    est = EstimatorWithSetOutputIndex().set_output(transform=\"pandas\")\n    est.fit(X)\n\n    X_trans = est.transform(X)\n    assert_array_equal(X_trans.index, [\"s0\", \"s1\"])\n", "def test_set_output_dataframe_index_override():\n    \"\"\"Check that set_output allows overriding DataFrame index.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=[\"a\", \"b\"])\n    est = EstimatorWithSetOutputIndex().set_output(transform=\"pandas\")\n    est.fit(X)\n\n    new_index = [\"x\", \"y\"]\n    X_trans = est.transform(X, index=new_index)\n    assert_array_equal(X_trans.index, new_index)\n", "def test_set_output_with_sparse_data():\n    \"\"\"Check that set_output is handled correctly for sparse data.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_sparse = csr_matrix([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X_sparse)\n    with pytest.raises(ValueError, match=\"Pandas output does not support sparse data\"):\n        est.set_output(transform=\"pandas\")\n        est.transform(X_sparse)\n\n    # Default output should still work\n    est.set_output(transform=\"default\")\n    assert isinstance(est.transform(X_sparse), np.ndarray)\n", "def test_set_output_with_feature_names_out():\n    \"\"\"Check that set_output respects feature_names_out.\n\n    Non-regression test for gh-28659.\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    class EstimatorWithFeatureNames(_SetOutputMixin):\n            return [\"feature_1\", \"feature_2\"]\n\n            return pd.DataFrame(X, columns=[\"feature_1\", \"feature_2\"])\n\n    est = EstimatorWithFeatureNames().set_output(transform=\"pandas\")\n    X_trans = est.transform(np.random.randn(5, 2))\n    assert_array_equal(X_trans.columns, [\"feature_1\", \"feature_2\"])\n", "def test_set_output_with_sparse_data():\n    \"\"\"Check handling of sparse data with set_output.\"\"\"\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X_sparse)\n    with pytest.raises(ValueError, match=\"Pandas output does not support sparse data.\"):\n        est.set_output(transform=\"pandas\")\n"], "sample_157": ["def test_tensor_product_commutativity():\n    A, B = symbols('A B', commutative=False)\n    C, D = symbols('C D')\n    assert (TP(A,B) - TP(B,A)).expand(tensorproduct=True) != 0\n    assert (TP(A,B)*TP(C,D) - TP(C,D)*TP(A,B)).expand(tensorproduct=True) != 0\n    assert (TP(A,B)*TP(C,D) + TP(C,D)*TP(A,B)).expand(tensorproduct=True) == \\\n        TP(A*C,B*D) + TP(A*D,B*C)\n", "def test_tensor_product_ordering():\n    a, b, c, d = symbols('a b c d', commutative=False)\n    tp1 = TensorProduct(a, b)\n    tp2 = TensorProduct(c, d)\n    assert tp1 * tp2 == TensorProduct(a * c, b * d)\n    assert tp2 * tp1 == TensorProduct(a * c, b * d)\n", "def test_tensor_product_simplify_commutative():\n    A, B, C = symbols('A B C', commutative=True)\n    assert tensor_product_simp(TP(A,B)*TP(C,B)) == TP(A*C, B*B)\n    assert tensor_product_simp(TP(A*B,C)) == TP(A*B,C)\n", "def test_tensor_product_combined_printing():\n    combined_tensor_product = (\n        TensorProduct(Qubit('1'), Qubit('2'))\n        + TensorProduct(Qubit('3'), Qubit('4'))\n    )\n\n    combined_printing_on = TensorProduct.combined_tensor_printing(True)\n    assert combined_tensor_product._sympystr(printer, *args) == '|12> + |34>'\n    combined_printing_off = TensorProduct.combined_tensor_printing(False)\n    assert combined_tensor_product._sympystr(printer, *args) == '( |1>|2> ) + ( |3>|4> )'\n\n\n", "def test_combined_tensor_printing():\n    combined_tensor_printing(True)\n    assert str(TensorProduct(Qubit('1'), Qubit('2'))) == '(|1> \\\\otimes |2>)'\n    combined_tensor_printing(False)\n    assert str(TensorProduct(Qubit('1'), Qubit('2'))) == '|1>x|2>'\n"], "sample_15": ["    def test_language_code_not_in_languages(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        for tag in ['es', 'de', 'zh-Hans']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(msg, id='translation.E004'),\n                ])\n", "    def test_invalid_variant_inconsistent_language_settings(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        for tag in ['fr-US', 'es-invalid', 'de-xyz', 'ca-ES-invalid']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(msg, id='translation.E004'),\n                ])\n", "    def test_invalid_variant_consistent_language_settings(self):\n        tests = [\n            'fr-US',\n            'de-fr',\n            'es-GB',\n            'en-invalid',\n        ]\n        for tag in tests:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004'),\n                ])\n", "    def test_invalid_language_code_in_languages(self):\n        msg = 'You have provided a value for the LANGUAGE_CODE setting that is '\n        msg += 'not in the LANGUAGES setting.'\n        with self.subTest('invalid'), self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')]):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "    def test_invalid_script_language_code(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        for tag in ['zh-Hant', 'sr@latin']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(msg, id='translation.E004'),\n                ])\n"], "sample_149": ["compilation error", "def test_monomial_mulpow():\n    assert monomial_mulpow((3, 4, 1), (2, 2, 2)) == (6, 8, 4)\n", "compilation error", "def test_monomial_mulpow():\n    assert monomial_mulpow((3, 4, 1), (2, 3)) == (6, 12, 2)\n", "def test_monomial_mulpow():\n    assert monomial_mulpow((3, 4, 1), (1, 2, 0), 2) == (3**2, 4**2, 1**2)\n    assert monomial_mulpow((3, 4, 1), (1, 2, 0), 0) == (3**0, 4**0, 1**0)\n"], "sample_102": ["def test_iforest_sample_weight():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    y_train = [1, 1, 1]\n\n    sample_weight = [0.5, 1.0, 0.25]\n    clf = IsolationForest(contamination=0.1, sample_weight=sample_weight).fit(\n        X_train, y_train\n    )\n    assert_array_equal(clf.sample_weights_, sample_weight)\n    \n    # Check that sample weights are used in decision function\n    score_with_weights = clf.decision_function([[2., 2.]])\n    \n    clf_no_weights = IsolationForest(contamination=0.1).fit(\n        X_train, y_train\n    )\n    score_no_weights = clf_no_weights.decision_function([[2., 2.]])\n    \n    assert_array_almost_equal(score_with_weights, score_no_weights * np.array(sample_weight)) \n   \n", "def test_iforest_with_sparse_matrix():\n    \"\"\"Test that Isolation Forest works properly with sparse matrices.\"\"\"\n    from scipy.sparse import csr_matrix\n\n    rng = check_random_state(0)\n    X = rng.randn(100, 20)\n    X_sparse = csr_matrix(X)\n\n    clf = IsolationForest().fit(X_sparse)\n    y_pred = clf.predict(X_sparse)\n    assert_array_equal(y_pred, clf.predict(X))\n", "def test_iforest_predict_proba():\n    X = iris.data\n    clf = IsolationForest(random_state=0).fit(X)\n    \n    # predict_proba should return probabilities for each sample\n    probabilities = clf.predict_proba(X)\n    assert_array_equal(probabilities.shape, (X.shape[0], 2))\n\n    # probabilities should sum to 1\n    assert_array_almost_equal(np.sum(probabilities, axis=1), 1.0,\n                             rtol=1e-05)\n\n    # probabilities should be between 0 and 1\n    assert_array_almost_equal(np.min(probabilities), 0.0,\n                             rtol=1e-05)\n    assert_array_almost_equal(np.max(probabilities), 1.0,\n                             rtol=1e-05)\n\n\n\n", "def test_iforest_predict_proba():\n    # Test that predict_proba works as expected\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest(contamination=0.1).fit(X_train)\n\n    probas = clf.predict_proba([[2., 2.]])\n    assert np.shape(probas) == (1, 2)\n    assert np.all(probas >= 0)\n    assert np.isclose(np.sum(probas, axis=1), 1.0)\n\n    # Check that predict_proba returns the same result for outliers and\n    # inliers (as they are 1 and 0)\n    probas_inlier = clf.predict_proba([[1., 1.]])\n    assert np.all(probas_inlier == [0.0, 1.0])\n    probas_outlier = clf.predict_proba([[2., 2.]])\n    assert np.all(probas_outlier == [1.0, 0.0])\n", "def test_iforest_feature_importances():\n    X = iris.data\n    y = iris.target\n    clf = IsolationForest(n_estimators=100, random_state=42).fit(X, y)\n\n    # check feature importances\n    sorted_idx = np.argsort(clf.feature_importances_)\n    assert_array_almost_equal(clf.feature_importances_[sorted_idx],\n                             np.sort(clf.feature_importances_))\n\n    # check behavior with different estimators\n    clf2 = IsolationForest(n_estimators=50, random_state=42).fit(X, y)\n    assert_array_almost_equal(clf.feature_importances_, clf2.feature_importances_)\n    \n\n"], "sample_103": ["def test_mutual_info_sparse_matrix():\n    # Test mutual information estimation with sparse matrices\n    # Ensure it behaves as expected for both classification and regression\n\n    X_sparse_class = csr_matrix([[1, 0, 0],\n                                [0, 1, 1],\n                                [1, 1, 0],\n                                [0, 0, 1]])\n    y_class = np.array([0, 1, 1, 0])\n\n    X_sparse_reg = csr_matrix([[1, 0, 0],\n                                [0, 1, 1],\n                                [1, 1, 0],\n                                [0, 0, 1]])\n    y_reg = np.array([0.5, -0.2, 0.8, 0.1])\n\n    # Test classification\n    mi_sparse_class = mutual_info_classif(X_sparse_class, y_class,\n                                         discrete_features='auto')\n    mi_dense_class = mutual_info_classif(X_sparse_class.toarray(), y_class,\n                                         discrete_features='auto')\n\n    assert_array_almost_equal(mi_sparse_class, mi_dense_class)\n\n    # Test regression\n    mi_sparse_reg = mutual_info_regression(X_sparse_reg, y_reg,\n                                          discrete_features='auto')\n    mi_dense_reg = mutual_info_regression(X_sparse_reg.toarray(), y_reg,\n                                          discrete_features='auto')\n\n    assert_array_almost_equal(mi_sparse_reg, mi_dense_reg)\n\n\n\n", "def test_mutual_info_regression_sparse():\n    n_samples = 100\n    rng = check_random_state(0)\n    X = rng.rand(n_samples, 3)\n    X[:, 1:] = 0.5 * X[:, 1:]\n    y = X[:, 0]\n    X = csr_matrix(X)\n    mi = mutual_info_regression(X, y, random_state=0)\n\n\n", "def test_mutual_info_classif_sparse():\n    n_samples = 100\n    n_features = 5\n\n    rng = check_random_state(0)\n\n    # Create sparse matrix with random values\n    X = csr_matrix((rng.rand(n_samples, n_features),\n                    (np.arange(n_samples),\n                     rng.randint(0, n_features, size=n_samples))))\n\n    y = rng.randint(0, 3, size=n_samples)\n\n    mi_dense = mutual_info_classif(X.toarray(), y)\n    mi_sparse = mutual_info_classif(X, y)\n\n    assert_array_almost_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse_data():\n    n_samples = 100\n    n_features = 5\n    rng = check_random_state(0)\n    X = rng.rand(n_samples, n_features)\n    y = (X[:, 0] + X[:, 1]) > 0.5\n\n    X_sparse = csr_matrix(X)\n    mi_dense = mutual_info_classif(X, y, discrete_features=False, random_state=0)\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features=False, random_state=0)\n    assert_array_almost_equal(mi_dense, mi_sparse) \n", "def test_mutual_info_classif_sparse():\n    # Test mutual_info_classif with sparse input matrices.\n\n    n_samples = 500\n    rng = check_random_state(0)\n    X = rng.rand(n_samples, 3)\n    y = rng.randint(0, 3, size=n_samples)\n\n    # Create a sparse matrix with the same values as X\n    X_csr = csr_matrix(X)\n\n    mi_dense = mutual_info_classif(X, y, discrete_features=True)\n    mi_sparse = mutual_info_classif(X_csr, y, discrete_features=True)\n\n    assert_array_almost_equal(mi_dense, mi_sparse)\n"], "sample_118": ["def test_ccode_complex_numbers():\n    from sympy import I, re, im, Abs\n\n    assert ccode(I) == \"I\"\n    assert ccode(x + 2 * I) == \"x + 2*I\"\n    assert ccode(re(x + 2 * I)) == \"creal(x + 2*I)\"\n    assert ccode(im(x + 2 * I)) == \" cimag(x + 2*I)\"\n    assert ccode(Abs(x + 2 * I)) == \"cabs(x + 2*I)\"\n", "def test_ccode_complex():\n    from sympy import I, sympify\n    x, y = symbols('x y', complex=True)\n    assert ccode(x + I*y) == \"(x + %s*y)\" % (sympify(\"I\").n())\n    assert ccode(x*I + y*I) == \"(x + y)*%s\" % (sympify(\"I\").n())\n    assert ccode(I**2) == \"-1\"\n\n\n", "def test_ccode_Indexed_with_Sum():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols\n    n, m = symbols('n m', integer=True)\n    A = IndexedBase('A')\n    x = IndexedBase('x')\n    y = IndexedBase('y')\n    i = Idx('i', m)\n    j = Idx('j', n)\n\n    s = (\n        'y = 0;\\n'\n        'for (int i=0; i<m; i++){\\n'\n        '   for (int j=0; j<n; j++){\\n'\n        '      y = y + A[%s];\\n' % (i*n + j) +\\\n        '   }\\n'\n        '}'\n    )\n    c = ccode(sum(A[i, j] for j in range(n) for i in range(m)), assign_to=y)\n    assert c == s \n\n", "def test_ccode_Piecewise_lambda_expressions():\n\n    expr = Piecewise((lambda x: x**2, x > 0), (lambda x: -x, True))\n    assert ccode(expr) == (\n            \"((x > 0) ? (\\n\"\n            \"   pow(x, 2)\\n\"\n            \")\\n\"\n            \": (\\n\"\n            \"   - x\\n\"\n            \"))\")\n\n\n\n", "def test_ccode_user_functions_with_args():\n    from sympy.functions import sin\n    x, y = symbols('x y', real=True)\n    custom_functions = {\n        'sin': ['sin_custom', [lambda f, arg: f(arg)*2]]\n    }\n    assert ccode(sin(x), user_functions=custom_functions) == \\\n        'sin_custom(x)*2'\n"], "sample_49": ["    def test_get_template_directories_with_duplicate_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates',\n                ROOT / 'templates_extra',\n            }\n        )\n", "    def test_get_template_directories_with_relative_app_dir(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                Path(__file__).parent / 'templates_extra',\n            }\n        )\n", "    def test_nested_directories(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates' / 'subdirectory',\n                ROOT / 'templates_extra' / 'subdirectory',\n            }\n        )\n", "    def test_nested_template_directories(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'template_tests/nested_dir/templates',\n            }\n        )\n", "    def test_get_template_directories_multiple_django_template_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n                ROOT / 'django_template_dir',\n            }\n        )\n"], "sample_68": ["    def test_update_conflicts_non_existent_fields(self):\n        msg = \"FieldsWithDbColumns has no field named 'nonexistent'\"\n        with self.assertRaisesMessage(FieldDoesNotExist, msg):\n            FieldsWithDbColumns.objects.bulk_create(\n                [FieldsWithDbColumns(rank=1, name=\"a\")],\n                update_conflicts=True,\n                update_fields=[\"nonexistent\"],\n                unique_fields=[\"rank\"],\n            )\n", "    def test_update_conflicts_with_target(self):\n        with self.assertNumQueries(1):\n            Country.objects.bulk_create(self.data)\n        self.assertEqual(Country.objects.count(), 4)\n\n        new_data = [\n            Country(\n                name=\"Germany\",\n                iso_two_letter=\"DE\",\n                description=(\"Germany is a country in Central Europe.\"),\n            ),\n            Country(\n                name=\"Czech Republic\",\n                iso_two_letter=\"CZ\",\n                description=(\n                    \"The Czech Republic is a landlocked country in Central Europe.\"\n                ),\n            ),\n        ]\n        results = Country.objects.bulk_create(\n            new_data,\n            update_conflicts=True,\n            update_fields=[\"description\"],\n            unique_fields=[\"iso_two_letter\"],\n        )\n        self.assertEqual(len(results), len(new_data))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(Country.objects.count(), 6)\n        self.assertCountEqual(\n            Country.objects.values(\"iso_two_letter\", \"description\"),\n            [\n                {\"iso_two_letter\": \"US\", \"description\": \"\"},\n                {\"iso_two_letter\": \"NL\", \"description\": \"\"},\n                {\"iso_two_letter\": \"DE\", \"description\": (\"Germany is a country in Central Europe.\")},\n                {\"iso_two_letter\": \"CZ\", \"description\": (\"The Czech Republic is a landlocked country in Central Europe.\")},\n                {\"iso_two_letter\": \"AU\", \"description\": \"\"},\n                {\"iso_two_letter\": \"JP\", \"description\": \"\"},\n            ],\n        )\n\n", "    def test_update_conflicts_unique_fields_update_fields_custom_db_column(self):\n        CustomDbColumnModel.objects.bulk_create(\n            [\n                CustomDbColumnModel(rank=1, name=\"a\"),\n                CustomDbColumnModel(rank=2, name=\"b\"),\n            ]\n        )\n        self.assertEqual(CustomDbColumnModel.objects.count(), 2)\n\n        conflicting_objects = [\n            CustomDbColumnModel(pk=1, rank=1, name=\"c\"),\n            CustomDbColumnModel(pk=2, rank=2, name=\"d\"),\n        ]\n        results = CustomDbColumnModel.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            unique_fields=[\"rank\"],\n            update_fields=[\"name\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(CustomDbColumnModel.objects.count(), 2)\n        self.assertCountEqual(\n            CustomDbColumnModel.objects.values(\"rank\", \"name\"),\n            [\n                {\"rank\": 1, \"name\": \"c\"},\n                {\"rank\": 2, \"name\": \"d\"},\n            ],\n        )\n", "    def test_update_conflicts_unique_fields_with_target(self):\n        Country.objects.bulk_create(self.data)\n        self.assertEqual(Country.objects.count(), 4)\n\n        new_data = [\n            Country(\n                name=\"Germany\",\n                iso_two_letter=\"DE\",\n                description=(\"Germany is a country in Central Europe.\"),\n            ),\n            Country(\n                name=\"Czech Republic\",\n                iso_two_letter=\"CZ\",\n                description=(\n                    \"The Czech Republic is a landlocked country in Central Europe.\"\n                ),\n            ),\n        ]\n        results = Country.objects.bulk_create(\n            new_data,\n            update_conflicts=True,\n            update_fields=[\"description\"],\n            unique_fields=[\"iso_two_letter\"],\n        )\n        self.assertEqual(len(results), len(new_data))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(Country.objects.count(), 6)\n        self.assertCountEqual(\n            Country.objects.values(\"iso_two_letter\", \"description\"),\n            [\n                {\"iso_two_letter\": \"US\", \"description\": \"\"},\n                {\"iso_two_letter\": \"NL\", \"description\": \"\"},\n                {\"iso_two_letter\": \"DE\", \"description\": (\"Germany is a country in Central Europe.\")},\n                {\"iso_two_letter\": \"CZ\", \"description\": (\"The Czech Republic is a landlocked country in Central Europe.\")},\n                {\"iso_two_letter\": \"AU\", \"description\": \"\"},\n                {\"iso_two_letter\": \"JP\", \"description\": \"\"},\n            ],\n        )\n\n\n\n", "    def test_update_conflicts_no_target_field(self):\n        msg = (\n            \"bulk_create() cannot be used with update_conflicts=True \"\n            \"without specifying unique_fields.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n\n            TwoFields.objects.bulk_create(\n                [TwoFields(f1=1, f2=1), TwoFields(f1=2, f2=2)],\n                update_conflicts=True,\n                update_fields=[\"f1\"],\n            )\n"], "sample_82": ["def test_groupby_with_empty_groups():\n\n    array = xr.DataArray([1, 2, 3], [(\"x\", [1, 2, 3])])\n\n    # test for empty groups\n    with pytest.raises(ValueError):\n        array.groupby(\"x\").sum()\n\n    # test for empty groups in a groupby chained with map\n    with pytest.raises(ValueError):\n        array.groupby(\"x\").map(lambda x: x.mean())\n\n\n", "def test_groupby_invalid_arg():\n    with pytest.raises(ValueError):\n        xr.DataArray([1, 2, 3]).groupby(None)\n\n    with pytest.raises(ValueError):\n        xr.DataArray([1, 2, 3]).groupby(\"invalid_dimension\")\n", "def test_groupby_datetimeindex_index_mismatch():\n    times = pd.date_range(\"2020-01-01\", periods=10)\n    data = np.arange(10)\n    da = xr.DataArray(data, dims=\"time\", coords={\"time\": times})\n\n    with pytest.raises(ValueError):\n        da.groupby(da.time.dt.month).mean()\n    \n", "def test_groupby_multiindex():\n    x = xr.DataArray(\n        np.arange(12).reshape((3, 4)),\n        dims=(\"x\", \"y\"),\n        coords={\"x\": [1, 1, 2, 2], \"y\": [\"a\", \"b\", \"a\", \"b\"]},\n    )\n    g = x.groupby([\"x\", \"y\"])\n\n    assert_identical(g.mean(), x.groupby([\"x\", \"y\"]).mean())\n    assert_identical(g.sum(), x.groupby([\"x\", \"y\"]).sum())\n    assert_identical(g.idxmax(), x.groupby([\"x\", \"y\"]).idxmax())\n\n\n    \n", "def test_groupby_preserve_attrs():\n    ds = xr.Dataset({\"foo\": (\"x\", [1, 2, 3]), \"bar\": (\"x\", [4, 5, 6])},\n                     attrs={\"test_attr\": \"test_value\"})\n    grouped = ds.groupby(\"x\")\n\n    # Check that attrs are preserved in the result\n    assert grouped.foo.attrs == ds.foo.attrs\n\n"], "sample_142": ["    def test_nchoosek():\n        assert nchoosek(0, 0) == 1\n        assert nchoosek(5, 0) == 1\n        assert nchoosek(5, 5) == 1\n        assert nchoosek(5, 1) == 5\n        assert nchoosek(5, 2) == 10\n        assert nchoosek(5, 3) == 10\n        assert nchoosek(5, 4) == 5\n\n\n", "def test_multiset_partitions():\n    assert list(multiset_partitions('mississippi')) == [\n        ['m', 'i', 's', 's', 'i', 's', 's', 'i', 'p', 'p', 'i'],\n        ['m', 'i', 's', 's', 'i', 's', 's', 'i', 'p', 'i', 'p'],\n        ['m', 'i', 's', 's', 'i', 's', 's', 'i', 'p', 'p', 'i'],\n        ['m', 'i', 's', 's', 'i', 's', 'i', 's', 'p', 'p', 'i'],\n        # ... more permutations\n    ]\n    assert list(multiset_partitions(multiset('aaabbbccc'))) == [\n        ['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c'],\n        # ... more permutations\n    ]\n    assert list(multiset_partitions('')) == [[]]\n", "compilation error", "def test_partitions_recursive():\n    assert partitions_recursive(4) == {4: 1, 1: 1, 2: 2, 3: 3, 1: 4}\n    assert partitions_recursive(5) == {5: 1, 1: 1, 2: 2, 3: 3, 4: 4, \n                                       1: 5, 2: 5, 3: 5}\n    assert partitions_recursive(8, 2) == {2: 3, 1: 3, 4: 4, 3: 4, 6: 4,\n                                         5: 4, 7: 4, 8: 4}\n\n\n\n", "compilation error"], "sample_154": ["    def test_issue_23881():\n        if not cupy:\n            skip(\"CuPy not installed\")\n        x = symbols('x')\n        f = lambdify(x, sin(x), modules='cupy')\n        assert f(1.0).dtype == cupy.float32\n        assert f(1.0).type == cupy.float32\n\n\n\n", "def test_issue_24197():\n    from sympy import Symbol, sin, cos\n    x = Symbol('x')\n\n    f = lambdify(x, sin(x) ** 2 + cos(x) ** 2)\n    assert f(1.0) == 1.0\n\n\n", "def test_issue_23467():\n    if not scipy:\n        skip(\"scipy not installed\")\n    x = symbols('x')\n    y = symbols('y')\n    f = lambdify((x, y), S.exp(x**2 + y**2), modules='scipy')\n    assert abs(f(1, 2) - scipy.exp(1**2 + 2**2)) < 1e-10\n", "    def test_issue_19397():\n        if not mkl:\n            skip(\"Intel MKL not available\")\n        f = lambdify(x, cos(x), modules='mkl')\n        assert abs(f(1.2) - cos(1.2)) < 1e-12\n", "    def test_issue_24694():\n        if not scipy:\n            skip(\"scipy not installed\")\n\n        x = symbols(\"x\")\n        f = lambdify(x, erf(x), modules='scipy')\n        assert abs(f(0) - 0) < 1e-10\n        assert abs(f(1) - 0.8427007937) < 1e-10\n\n"], "sample_81": ["    def test_issue_2321_should_trigger_comments_with_spaces(self) -> None:\n        code = \"# TODO this should trigger a fixme\"\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"fixme\",\n                line=1,\n                args=\"TODO this should trigger a fixme\",\n                col_offset=1,\n            )\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_issue_2524_comment_before_first_line(self) -> None:\n        code = \"\"\"\n        # This is a comment before the first line\n        a = 1\n        # FIXME this is a fixme\n        \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"fixme\",\n                line=3,\n                args=\"FIXME this is a fixme\",\n                col_offset=17,\n            )\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_notes(self) -> None:\n        code = \"\"\"a = 1\n                # TODO this\n                # TODO that\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO this\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"TODO that\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_issue_2890_should_not_trigger_on_multiline_comments(self) -> None:\n        code = \"\"\"\n            \"\"\"\n            This is a multiline comment that should not trigger a fixme.\n            It has multiple lines and is not prefixed with #TODO, #FIXME or #XXX\n            \"\"\"\n            pass\n        \"\"\"\n        with self.assertNoMessages():\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_codetag(self) -> None:\n        code = \"\"\"\n        # TODO API: What is this task about?\n        # TODO API: Look up a task's due date\n        # TODO API: Look up a Project/Label/Task ID\n        \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=1, args=\"TODO API: What is this task about?\", col_offset=1),\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO API: Look up a task's due date\", col_offset=1),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"TODO API: Look up a Project/Label/Task ID\", col_offset=1),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n"], "sample_58": ["    def test_missing_required_parameters(self):\n        with self.assertRaises(ValueError) as ctx:\n            DatabaseClient.settings_to_cmd_args_env({\"NAME\": \"dbname\"})\n        self.assertIn(\"Missing required parameters: USER\", str(ctx.exception))\n\n", "    def test_empty_options(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\"NAME\": \"dbname\",}),\n            ([\"psql\", \"dbname\"], None),\n        )\n", "    def test_empty_settings(self):\n        self.assertEqual(self.settings_to_cmd_args_env({}), ([\"psql\"], None))\n", "    def test_default_database(self):\n        args, env = self.settings_to_cmd_args_env({})\n        self.assertEqual(args, [\"psql\", \"postgres\"])\n        self.assertEqual(env, None)\n", "    def test_default_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            (\n                [\"psql\", \"postgres\"],\n                None,\n            ),\n        )\n"], "sample_111": ["def test_empty_input(metric_name, y_true, y_pred):\n    metric = SUPERVISED_METRICS[metric_name]\n    for i in range(y_true.size):\n        with pytest.raises(ValueError, match='Input array is empty'):\n            metric(y_true[:i], y_pred)\n\n", "def test_empty_array(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match='Input arrays cannot be empty'):\n        metric([], [])\n\n    with pytest.raises(ValueError, match='Input arrays cannot be empty'):\n        metric([], [0])\n", "def test_empty_labels(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match=\"Input arrays must have an equal length\"):\n        metric([], [])\n    with pytest.raises(ValueError, match=\"Input arrays must have an equal length\"):\n        metric([1], [])\n    with pytest.raises(ValueError, match=\"Input arrays must have an equal length\"):\n        metric([], [1])\n\n\n", "def test_fowlkes_mallows_score_empty_set(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match=\"`labels_true` and `labels_pred` must have at least 1 sample\"):\n        metric([], [])\n", "def test_empty_array_input(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match='y_true and y_pred must'):\n        metric([], [])\n    with pytest.raises(ValueError, match='y_true and y_pred must'):\n        metric([0], [])\n    with pytest.raises(ValueError, match='y_true and y_pred must'):\n        metric([], [0])\n"], "sample_105": ["def test_invalid_voting_str():\n    with pytest.raises(ValueError,\n                       match=\"Invalid voting strategy: 'invalid'\"):\n        VotingClassifier(estimators=[('lr', LogisticRegression())],\n                         voting='invalid')\n", "def test_partial_fit():\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n                             voting='soft')\n\n    X_train = X[:len(X)//2]\n    y_train = y[:len(y)//2]\n    X_test = X[len(X)//2:]\n    y_test = y[len(y)//2:]\n\n    eclf.partial_fit(X_train, y_train)\n    assert_array_almost_equal(eclf.predict_proba(X_test),\n                              eclf.predict_proba(X_test), decimal=2)\n\n    eclf.partial_fit(X_train, y_train, classes=[0, 1])\n    assert_array_almost_equal(eclf.predict_proba(X_test),\n                              eclf.predict_proba(X_test), decimal=2)\n\n    eclf.partial_fit(X_test, y_test)\n    assert_array_almost_equal(eclf.predict_proba(X_test),\n                              eclf.predict_proba(X_test), decimal=2)\n\n\n", "def test_custom_proba_for_hard_voting():\n    \"\"\"Check custom probabilities for hard voting.\"\"\"\n    clf1 = LogisticRegression(random_state=123, probability=True)\n    clf2 = RandomForestClassifier(random_state=123, probability=True)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n                            voting='hard',\n                            weights=[1, 1])\n    X = np.array([[1], [2]])\n    y = np.array([1, 2])\n\n    # Custom probabilities for hard voting using a lambda function\n        if isinstance(estimator, LogisticRegression):\n            return estimator.predict_proba(X)\n        elif isinstance(estimator, RandomForestClassifier):\n            return np.zeros((X.shape[0], 2))\n\n    eclf.set_params(proba=custom_proba)\n\n    y_pred = eclf.predict(X)\n    assert_array_equal(y_pred, np.array([1, 2]))\n", "compilation error", "def test_voting_with_different_classes():\n    \"\"\"Check VotingClassifier with estimators having different number of classes.\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y1 = np.array([1, 2, 3, 4])\n    y2 = np.array([1, 1, 2, 2])  \n\n    clf1 = LogisticRegression(random_state=123, classes=range(1, 6))\n    clf2 = Perceptron(random_state=123, max_iter=5)\n    eclf = VotingClassifier(\n        estimators=[('lr', clf1), ('perceptron', clf2)], voting='hard')\n\n    eclf.fit(X, y1)\n    assert_array_equal(eclf.predict(X), clf1.predict(X))\n    assert_array_equal(eclf.predict_proba(X), clf1.predict_proba(X))\n\n    eclf = VotingClassifier(\n        estimators=[('lr', clf1), ('perceptron', clf2)], voting='soft')\n    eclf.fit(X, y1)\n    assert_array_almost_equal(eclf.predict_proba(X),\n                              eclf.predict_proba(X).mean(axis=1),\n                              decimal=2) \n\n\n"], "sample_113": ["def test_transformer_names_with_lambda_features(\n    transformers, remainder, expected_names", "def test_empty_remainder_selection(remainder, input_features, expected_names):\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame(\n        [[1, 2, 3, 4], [5, 6, 7, 8]], columns=input_features\n    )\n    ct = ColumnTransformer(\n        [\n            (\"first\", TransWithNames(), [\"a\", \"b\"]),\n            (\"second\", TransWithNames(), [\"c\", \"d\"]),\n        ],\n        remainder=remainder,\n        verbose_feature_names_out=False,\n    )\n    ct.set_output(transform=\"pandas\")\n\n    # Simulate empty selection by passing a list like this\n    X_trans = ct._transform(X)\n    assert_array_equal(X_trans.columns, expected_names) \n", "compilation error", "def test_column_transformer_with_invalid_transformers(remainder):\n    \"\"\"Check that ColumnTransformer raises an error when provided with invalid transformers.\"\"\"\n    with pytest.raises(ValueError, match=\"Transformer must have a 'fit' method\"):\n        ct = ColumnTransformer(\n            [(\"invalid\", object(), [\"a\"])], remainder=remainder\n        )\n        ct.fit(pd.DataFrame({\"a\": [1, 2]}))\n\n\n\n", "def test_column_transformer_set_output_with_multiple_transformers(transformers, remainder):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        transformers,\n        remainder=remainder,\n        verbose_feature_names_out=False,\n    )\n    ct.set_output(transform=\"pandas\")\n\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, pd.DataFrame)\n    assert isinstance(ct.get_feature_names_out(), np.ndarray)\n    assert len(ct.get_feature_names_out()) != 0\n\n\n\n"], "sample_53": ["    def test_order_of_dependencies(self):\n        \"\"\"\n        #23873 - The dependency detector correctly sorts migrations\n        based on their dependencies.\n        \"\"\"\n        before = [\n            ModelState(\n                \"app1\",\n                \"Author\",\n                [(\"id\", models.AutoField(primary_key=True))],\n            ),\n            ModelState(\n                \"app2\",\n                \"Book\",\n                [],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"app1\",\n                \"Author\",\n                [(\"id\", models.AutoField(primary_key=True)), (\"book\", models.ForeignKey(\"app2.Book\", on_delete=models.CASCADE))],\n            ),\n            ModelState(\n                \"app2\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"app1.Author\", on_delete=models.CASCADE)),\n                ],\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"app1\", 1)\n        self.assertNumberMigrations(changes, \"app2\", 1)\n        self.assertOperationTypes(changes, \"app1\", 0, [\"MakeField\"])\n        self.assertOperationTypes(changes, \"app2\", 0, [\"CreateModel\"])\n        self.assertMigrationDependencies(changes, \"app1\", 0, [(\"__setting__\", \"AUTH_USER_MODEL\")])\n        self.assertMigrationDependencies(changes, \"app2\", 0, [(\"app1\", \"__first__\")])\n", "    def test_complex_operation_names(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField(\n                    model_name=\"Person\", old_name=\"first_name\", new_name=\"given_name\"\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"rename_person_first_name_to_given_name\")\n", "    def test_add_fk_to_model_already_with_field(self):\n        \"\"\"\n        Adding a FK to a model that already has a field with the same name should\n        result in a suggested migration name reflecting the relationship.\n        \"\"\"\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[(\"id\", models.AutoField(primary_key=True)),]\n                ),\n                migrations.CreateModel(\n                    \"Author\", fields=[(\"id\", models.AutoField(primary_key=True)),]\n                ),\n                migrations.AddField(\n                    model_name=\"Person\",\n                    name=\"author\",\n                    field=models.ForeignKey(\n                        \"test_app.Author\",\n                        on_delete=models.CASCADE,\n                    ),\n                ),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_author\")\n", "def test_operation_with_custom_name(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.RenameField(model_name=\"Person\", old_name=\"Person\", new_name=\"Name\"),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    self.assertEqual(migration.suggest_name(), \"person_rename_field_person_to_name\")\n\n", "    def test_multiple_fields_in_create_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"MultipleFields\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                )\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"multiple_fields\")\n"], "sample_36": ["    def test_resolve_expression(self):\n        q = Q(pk__in=F('ids'))\n        clause, joins = q.resolve_expression()\n        self.assertEqual(clause, 'pk__in')\n        self.assertEqual(joins, ())\n\n\n", "    def test_resolve_expression(self):\n        q = Q(id=1)\n        clause = q.resolve_expression()\n        self.assertIn('id=1', clause)\n\n\n\n", "    def test_resolve_expression(self):\n        q = Q(id__gt=1)\n        clause, _ = q.resolve_expression(allow_joins=False)\n        self.assertEqual(clause, 'id__gt=1')\n\n", "    def test_resolve_expression(self):\n        q = Q(name__contains='test')\n        clause, _ = q.resolve_expression()\n        self.assertEqual(clause, 'name__contains=\"test\"') \n", "    def test_select_related_descend(self):\n        from django.db import models\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n            books = models.ManyToManyField('Book')\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        self.assertTrue(select_related_descend(Book.author, Author._meta, Book.objects.get(title='The Lord of the Rings').author, set(), reverse=True))\n        self.assertFalse(select_related_descend(Book.author, Author._meta, Book.objects.get(title='The Lord of the Rings').author, {'title':}, reverse=True))\n        self.assertFalse(select_related_descend(Book.author, Author._meta, Book.objects.get(title='The Lord of the Rings').author, set(), reverse=False))\n        self.assertTrue(select_related_descend(Book.author, Author._meta, Book.objects.get(title='The Lord of the Rings').author, {'author':}, reverse=False))\n\n\n"], "sample_50": ["    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = [Message(constants.INFO, 'message 1'),\n                    Message(constants.INFO, 'message 2')]\n\n        # Add a message and store it.\n        storage.add(*messages)\n        storage._store(messages, response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n\n        # Retrieve the cookie and check if we have all messages.\n        messages_retrieved = list(storage)\n        self.assertEqual(messages_retrieved, messages)\n\n        # Add more messages and store them, this time we reach the max_cookie_size\n        additional_messages = [Message(constants.INFO, 'message 3'),\n                              Message(constants.INFO, 'message 4')]\n        storage.add(*additional_messages)\n        storage._store(messages + additional_messages, response, remove_oldest=False)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 4)\n\n        self.assertEqual(messages, messages_retrieved)\n\n        # Iterate all messages, they should all be retrieved on next call\n        messages_retrieved = list(storage)\n        self.assertEqual(messages_retrieved, messages + additional_messages)\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add a small amount of messages to ensure data is stored\n        for i in range(3):\n            storage.add(constants.INFO, f\"Message {i}\")\n\n        # Add more messages to fill up the cookie. This should trigger the\n        # not_finished sentinel.\n        for i in range(3, 10):\n            storage.add(constants.INFO, f\"Message {i}\")\n\n        storage.update(response)\n        messages = storage._decode(response.cookies['messages'].value)\n        self.assertEqual(len(messages), 3)\n        self.assertEqual(messages[-1], storage.not_finished)\n\n        # The remaining messages should be retrieved once the cookie is cleared\n        storage.clear_cookie(response)\n        messages = stored_cookie_messages_count(storage, response)\n        self.assertEqual(messages, 7)\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = ['test1', 'test2', 'test3']\n        storage._store(messages, response, remove_oldest=False)\n\n        # Retrieve messages and check if the sentinel value is present\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertEqual(retrieved_messages, ['test1', 'test2'])\n        self.assertFalse(all_retrieved)\n\n        # Enqueue more messages, this should finish the group\n        storage._store(['test4', 'test5'], response, remove_oldest=False)\n\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertEqual(retrieved_messages, ['test1', 'test2', 'test3', 'test4', 'test5'])\n        self.assertTrue(all_retrieved)\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Test that the sentinel value is added correctly\n        messages = []\n        for i in range(5):\n            storage.add(constants.INFO, f\"message {i}\")\n\n        storage.update(response)\n\n        # Retrieve messages from the cookie.\n        messages_from_cookie = list(storage)\n        self.assertEqual(len(messages_from_cookie), 1)\n        self.assertEqual(messages_from_cookie[0].message, 'messagesnotfinished')\n\n\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store a set of messages, but don't consume them all.\n        messages = ['message 1', 'message 2', 'message 3']\n        storage.add_messages(messages[:2])\n        storage.update(response)\n\n        # Retrieve the messages. We should have 2 stored, and a not_finished message.\n        retrieved_messages = storage._decode(response.cookies['messages'].value)\n        self.assertEqual(len(retrieved_messages), 2)\n        self.assertEqual(retrieved_messages[-1], CookieStorage.not_finished)\n\n        # Retrieve the remaining messages.\n        retrieved_messages = list(storage)\n        self.assertEqual(len(retrieved_messages), 1)\n        self.assertEqual(retrieved_messages[0], messages[2])\n\n        # The cookie should now be empty.\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n\n"], "sample_99": ["def test_sparse_precomputed_distances():\n    X = csr_matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    distances = np.array([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\n    nbrs = neighbors.NearestNeighbors(algorithm='auto')\n    nbrs.fit(X, dist_matrix=distances)\n\n    distances_pred, indices_pred = nbrs.kneighbors(X)\n    assert_array_almost_equal(distances_pred, distances)\n    assert_array_equal(indices_pred, np.array([[1, 2, 0], [0, 2, 1], [0, 1, 2]]))\n\n    # Test radius neighbors\n    distances_pred_radius, indices_pred_radius = nbrs.radius_neighbors(\n        X, radius=2)\n    assert_array_almost_equal(distances_pred_radius,\n                             np.array([[0, 1, 2], [1, 0, 3], [2, 3, 0]]))\n    assert_array_equal(indices_pred_radius, np.array(\n        [[1, 2, 0], [0, 2, 1], [0, 1, 2]]))\n\n    nbrs_sparse = neighbors.NearestNeighbors(\n        algorithm='kd_tree', metric='euclidean').fit(X, dist_matrix=distances)\n    distances_pred_sparse, indices_pred_sparse = nbrs_sparse.kneighbors(X)\n    assert_array_almost_equal(distances_pred_sparse, distances)\n    assert_array_equal(indices_pred_sparse, np.array(\n        [[1, 2, 0], [0, 2, 1], [0, 1, 2]]))\n", "def test_precomputed_distances_knn():\n    X = np.array([[0, 1], [1, 1], [1, 0]])\n    D = metrics.pairwise_distances(X, metric='euclidean')\n    knn = neighbors.KNeighborsClassifier(n_neighbors=2).fit(X, [0, 1, 0])\n    y_pred = knn.predict(X)\n    assert_array_equal(y_pred, [0, 1, 0])\n\n    knn_pre = neighbors.KNeighborsClassifier(n_neighbors=2,\n                                         algorithm='precomputed').fit(D,\n                                                                       [0, 1, 0])\n    y_pred_pre = knn_pre.predict(D)\n    assert_array_equal(y_pred_pre, y_pred)\n\n    knn_pre.kneighbors(D)\n\n", "def test_non_euclidean_radius_neighbors_graph():\n    rng = np.random.RandomState(0)\n    X = rng.rand(5, 5)\n\n    # Find a reasonable radius.\n    dist_array = pairwise_distances(X).flatten()\n    np.sort(dist_array)\n    radius = dist_array[15]\n\n    # Test kneighbors_graph\n    for metric in ['manhattan', 'chebyshev']:\n        nbrs_graph = neighbors.radius_neighbors_graph(\n            X, radius, metric=metric, mode='connectivity',\n            include_self=True).toarray()\n        nbrs1 = neighbors.NearestNeighbors(metric=metric, radius=radius).fit(X)\n        assert_array_equal(nbrs_graph, nbrs1.radius_neighbors_graph(X).A)\n\n\n\n", "compilation error", "compilation error"], "sample_138": ["def test_block_collapse_matrix_addition():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = BlockMatrix([[A + C, B + D], [C, D]])\n    assert block_collapse(X + X) == Y\n", "def test_block_collapse_with_MatrixSlices():\n    A = MatrixSymbol('A', 5, 5)\n    B = MatrixSymbol('B', 3, 3)\n    M = BlockMatrix([[A, MatrixSlice(A, 1, 4, 1, 4)],[MatrixSlice(A, 0, 3, 1, 4), B]])\n\n    assert block_collapse(M * M).blocks[0, 0].shape == (5, 5)\n    assert block_collapse(M * M).blocks[0, 1].shape == (5, 3)\n\n    assert isinstance(block_collapse(M.T), BlockMatrix)\n    assert isinstance(block_collapse(M.I), BlockMatrix)\n", "def test_block_collapse_with_matrices():\n    M = Matrix([[1, 2], [3, 4]])\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n\n    X = BlockMatrix([[A, M], [M, B]])\n\n    assert block_collapse(X + X).blocks[0, 0] == 2*A\n    assert block_collapse(X * X).blocks[0, 0] == A*A + M*B\n", "def test_block_collapse_block_shape():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 2, 3)\n\n    X = BlockMatrix([[A, B], [C, A]])\n    assert X.blockshape == (2, 2)\n\n    assert block_collapse(X).blockshape == (2, 2)\n", "def test_block_collapse_multiple_blockmatrices():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = BlockMatrix([[2*A, 2*B], [2*C, 2*D]])\n    Z = BlockMatrix([[MatrixSymbol('Z', 2, 2), MatrixSymbol('W', 2, 2)],\n                     [MatrixSymbol('V', 2, 2), MatrixSymbol('U', 2, 2)]])\n\n    assert block_collapse(X + Y) == BlockMatrix([[A + 2*A, B + 2*B],\n                                                 [C + 2*C, D + 2*D]])\n    assert block_collapse(X * Y) == BlockMatrix([[A*2*A + B*2*C, A*2*B + B*2*D],\n                                                 [C*2*A + D*2*C, C*2*B + D*2*D]])\n    assert block_collapse(X * Z) == BlockMatrix([[A*Z[0, 0] + B*Z[0, 1],\n                                                 A*Z[1, 0] + B*Z[1, 1]],\n                                                [C*Z[0, 0] + D*Z[0, 1],\n                                                 C*Z[1, 0] + D*Z[1, 1]]])\n"], "sample_56": ["    def test_template_tags_with_same_library_name_with_multiple_paths(self):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [\n                Error(\n                    E003.msg.format(\n                        \"'same_tags'\",\n                        \"'check_framework.template_test_apps.same_tags_app_1.\"\n                        \"templatetags.same_tags', \"\n                        \"'check_framework.template_test_apps.same_tags_app_2.\"\n                        \"templatetags.same_tags'\",\n                    ),\n                    id=E003.id,\n                )\n            ],\n        )\n", "    def test_template_tags_with_same_library_name_and_different_module_path(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.same_tags_app_2.other_module.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [\n                    Error(\n                        E003.msg.format(\n                            \"'same_tags'\",\n                            \"'check_framework.template_test_apps.same_tags_app_1.templatetags.same_tags', \"\n                            \"'check_framework.template_test_apps.same_tags_app_2.other_module.templatetags.same_tags'\",\n                        ),\n                        id=E003.id,\n                    )\n                ],\n            )\n", "    def test_template_tags_with_same_library_name_and_module_path(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.same_tags_app_2.templatetags.same_tags\"  \n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [self.error_same_tags],\n            )\n", "    def test_template_tags_with_different_name_in_separate_templates(self):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_template_tags_with_different_library_name_and_module_name(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"different_tags\",\n                    \"different_tags_app.templatetags.different_tags\",\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [],\n            )\n"], "sample_55": ["    def test_normalize_path_patterns_handles_relative_paths(self):\n        expected = [os.path.normcase(p) for p in [\"../foo/bar\", \"bar/*/\"]]\n        self.assertEqual(\n            normalize_path_patterns([\"../foo/bar/*\", \"bar/*\"]), expected\n        )\n", "    def test_normalize_path_patterns_handles_empty_patterns(self):\n        self.assertEqual(normalize_path_patterns([]), [])\n        self.assertEqual(normalize_path_patterns([\" \"]), [])\n\n\n\n", "    def test_normalize_path_patterns_with_duplicate_patterns(self):\n        expected = [os.path.normcase(p) for p in [\"foo/bar\", \"bar/baz\", \"baz\"]]\n        self.assertEqual(\n            normalize_path_patterns(\n                [\"foo/bar\", \"bar/baz\", \"bar/baz\", \"baz\"]\n            ),\n            expected,\n        )\n", "    def test_normalize_path_patterns_handles_absolute_paths(self):\n        expected = [\n            os.path.normcase(\"/foo/bar\"),\n            os.path.normcase(\"/bar/*/\"),\n        ]\n        self.assertEqual(\n            normalize_path_patterns([\"/foo/bar/*\", \"/bar/*/\"]), expected\n        )\n", "    def test_normalize_path_patterns_handles_trailing_slashes(self):\n        expected = [os.path.normcase(p) for p in [\"foo/bar/\", \"bar/baz/\"]]\n        self.assertEqual(\n            normalize_path_patterns([\"foo/bar/*\", \"bar/baz/\"] ), expected\n        )\n"], "sample_91": ["compilation error", " def test_importorskip_relative_import(testdir):\n    testdir.makepyfile(\n        \"my_module.py\",\n        \"\"\"\n            return \"Hello\"\n    \"\"\",\n    )\n    with pytest.raises(\n        pytest.skip.Exception,\n        match=\"^could not import 'my_module.my_other_module': .*No module named.*\",\n    ):\n        pytest.importorskip(\"my_module.my_other_module\")\n\n\n", "compilation error", "    def test_relpath_rootdir_no_file(testdir):\n        testdir.makepyfile(\n            **{\n                \"tests/test_1.py\": \"\"\"\n        import pytest\n        @pytest.mark.skip()\n            pass\n            \"\"\",\n            }\n        )\n        result = testdir.runpytest(\"-rs\", \"tests/nonexistent.py\", \"--rootdir=tests\")\n        result.stdout.fnmatch_lines([\"ERROR*test_one.py:1: no such file or directory*\"])\n", "    def test_relpath_rootdir_with_setup_teardown(testdir):\n        testdir.makepyfile(\n            **{\n                \"tests/test_1.py\": \"\"\"\n                import pytest\n                    print(\"setup_module\")\n                    print(\"teardown_module\")\n                @pytest.mark.skip()\n                    pass\n                \"\"\"\n            }\n        )\n        result = testdir.runpytest(\"-rs\", \"tests/test_1.py\", \"--rootdir=tests\")\n        result.stdout.fnmatch_lines([\"setup_module\", \"teardown_module\", \"SKIPPED [[]1[]] tests/test_1.py:5: unconditional skip\"])\n"], "sample_37": ["    def test_expression_output_field(self):\n        expr = ExpressionWrapper(F('field'), output_field=CharField())\n        self.assertIsInstance(expr.output_field, CharField)\n", "    def test_output_field_not_None_in_aggregate(self):\n        with self.assertRaises(TypeError):\n            ExpressionWrapper(Avg('a'), output_field=IntegerField()).aggregate(\n                value=Avg('a'),\n            )\n", "    def test_output_field_override(self):\n        value = Value('f')\n        value.output_field = None\n        expr = ExpressionWrapper(Lower(value), output_field=IntegerField())\n        self.assertEqual(expr.output_field, IntegerField())\n", "    def test_output_field_from_expression(self):\n        expr = ExpressionWrapper(\n            CombinedExpression(F('a'), Combinable.ADD, F('b')),\n            output_field=IntegerField(),\n        )\n        self.assertEqual(expr.output_field, IntegerField())\n", "    def test_get_group_by_cols_with_alias(self):\n        value = Value('f')\n        value.output_field = None\n        expr = ExpressionWrapper(Lower(value), output_field=IntegerField(), alias='aliased_value')\n        group_by_cols = expr.get_group_by_cols(alias='aliased_value')\n        self.assertEqual(group_by_cols, [expr.expression])\n        self.assertEqual(group_by_cols[0].alias, 'aliased_value')\n        self.assertEqual(group_by_cols[0].output_field, expr.output_field)\n"], "sample_21": ["    def test_fast_delete_multiple_parents(self):\n        # Test deletion of child instances with multiple parents.\n        p1 = Parent.objects.create()\n        p2 = Parent.objects.create()\n        c = Child.objects.create(parent_ptr=p1)\n        c.parent_ptr = p2\n        c.save()\n        self.assertEqual(c.parent_ptr, p2)\n        self.assertNumQueries(2, c.delete)\n        self.assertFalse(Child.objects.exists())\n        self.assertEqual(Parent.objects.count(), 2)\n", "    def test_fast_delete_with_signal_handling(self):\n        class SignalCounter:\n                self.count = 0\n\n                self.count += 1\n\n        counter = SignalCounter()\n        models.signals.pre_delete.connect(counter.receiver, sender=R)\n        r = R.objects.create()\n        r.delete()\n        self.assertEqual(counter.count, 1)\n        models.signals.pre_delete.disconnect(counter.receiver, sender=R)\n", "    def test_fast_delete_with_related_manager(self):\n        manager = RelatedManager.objects.create(related_model=RelatedModel.objects.create())\n        self.assertNumQueries(1, manager.delete())\n        self.assertFalse(RelatedManager.objects.exists())\n        self.assertFalse(RelatedModel.objects.exists())\n\n\n\n", "    def test_fast_delete_signal_handling(self):\n        \"\"\"\n        test that signal handlers are called properly during fast deletion\n        \"\"\"\n        delete_calls = 0\n\n            nonlocal delete_calls\n            delete_calls += 1\n\n        models.signals.post_delete.connect(delete_handler, sender=R)\n        r = R.objects.create()\n        r.delete()\n        self.assertEqual(delete_calls, 1)\n\n", "    def test_fast_delete_with_signals(self):\n            pass\n\n        origin = Origin.objects.create()\n        referer = Referrer.objects.create(\n            origin=origin, unique_field=42\n        )\n        signal = models.signals.post_delete\n        signal.connect(receiver, sender=Referrer)\n        with self.assertNumQueries(2):\n            referer.delete()\n        signal.disconnect(receiver, sender=Referrer)\n\n"], "sample_41": ["    def test_clean_for_empty_form(self):\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data={}, prefix='choices')\n        self.assertEqual(formset.cleaned_data, [])\n\n\n\n\n", "    def test_all_valid_with_mixed_form_validity(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset3 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2, formset3)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {'votes': ['This field is required.']},  {}]\n        self.assertEqual(formset1._errors, expected_errors[0])\n        self.assertEqual(formset2._errors, expected_errors[1])\n        self.assertEqual(formset3._errors, expected_errors[2])\n", "    def test_formset_is_bound_with_extra_forms(self):\n        data = {\n            'form-TOTAL_FORMS': '3',\n            'form-INITIAL_FORMS': '0',\n            'form-0-choice': 'Zero',\n            'form-0-votes': '0',\n            'form-1-choice': 'One',\n            'form-1-votes': '1',\n            'form-2-choice': 'Two',\n            'form-2-votes': '2',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='form')\n        self.assertTrue(formset.is_bound)\n        self.assertEqual(len(formset.forms), 3)\n\n\n", "    def test_can_delete_extra_formset_forms(self):\n        ChoiceFormFormset = formset_factory(form=Choice, can_delete=True, extra=2)\n        formset = ChoiceFormFormset()\n        self.assertEqual(len(formset), 2)\n        self.assertIn('DELETE', formset.forms[0].fields)\n        self.assertIn('DELETE', formset.forms[1].fields)\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, validate_on_bind=True)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset1.is_valid())\n        self.assertEqual(formset1.non_form_errors(), [\n            'This is a non-form error!'\n        ])\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n"], "sample_133": ["compilation error", "def test_fcode_complex_mixed():\n    x = Symbol('x', real=True)\n    y = Symbol('y')\n    result = codegen(('test', x + y), 'f95', 'test', header=False, empty=False)\n    source = (result[0][1])\n    expected = (\n        \"COMPLEX*16 function test(x, y)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"COMPLEX*16, intent(in) :: y\\n\"\n        \"test = x + y\\n\"\n        \"end function\\n\"\n    )\n    assert source == expected\n", "def test_fcode_integer_overflow():\n    x, y = symbols('x y', integer=True)\n    result = codegen(('test', x*y), 'f95', 'test', header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        \"INTEGER*4 function test(x, y)\\n\"\n        \"implicit none\\n\"\n        \"INTEGER*4, intent(in) :: x\\n\"\n        \"INTEGER*4, intent(in) :: y\\n\"\n        \"test = x*y\\n\"\n        \"end function\\n\"\n    )\n    assert source == expected\n\n\n", "compilation error", "compilation error"], "sample_13": ["    def test_absolute_url(self):\n        tests = (\n            ('//example.com', '//%2Fexample.com'),\n            ('https://example.com', 'https://%2Fexample.com'),\n            ('http://example.com', 'http://%2Fexample.com'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n", "    def test_already_escaped(self):\n        self.assertEqual(escape_leading_slashes('/%2Fexample.com'), '/%2Fexample.com')\n", "    def test_single_percent(self):\n        self.assertEqual(urlquote('Paris%Orl\\xe9ans'), 'Paris%25Orl%C3%A9ans')\n        self.assertEqual(urlquote('Paris%Orl\\xe9ans', safe=\"%\"), 'Paris%Orl%C3%A9ans')\n", "    def test_no_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n\n\n\n", "    def test_no_effect_if_no_double_slashes(self):\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n        self.assertEqual(escape_leading_slashes('example.com'), 'example.com')\n"], "sample_79": ["    def test_concat_empty_datasets(self):\n        with raises_regex(ValueError, \"Input arrays must have the same dimensions\"):\n            concat([], dim=\"x\")\n        with raises_regex(ValueError, \"Input arrays must have the same dimensions\"):\n            concat([Dataset({\"x\": [0]}), Dataset({\"y\": [1]}), Dataset({\"z\": [2]}),], dim=\"x\")\n", "    def test_concat_multiple_arrays(self):\n        arrays = [\n            DataArray([1, 2, 3], coords=[(\"x\", [0, 1, 2])]),\n            DataArray([4, 5, 6], coords=[(\"x\", [3, 4, 5])]),\n            DataArray([7, 8, 9], coords=[(\"x\", [6, 7, 8])]),\n        ]\n        result = concat(arrays, dim=\"x\")\n        expected = DataArray(\n            np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n            dims=[\"x\"],\n            coords={\"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8]},\n        )\n        assert_identical(result, expected)\n", "    def test_concat_preserve_attrs(self):\n        ds1 = Dataset({\"x\": ([\"y\"], [1]), \"y\": ([\"y\"], [2])}, attrs={\"foo\": \"bar\"})\n        ds2 = Dataset({\"x\": ([\"y\"], [3]), \"y\": ([\"y\"], [4])}, attrs={\"foo\": \"baz\"})\n\n        actual = concat([ds1, ds2], dim=\"y\")\n        expected = Dataset(\n            {\"x\": ((\"y\", \"x\"), [[1, 3], [2, 4]]), \"y\": ((\"y\", \"x\"), [[1, 2], [3, 4]])},\n            attrs={\"foo\": \"bar\"},\n        )\n\n        assert_identical(actual, expected)\n", "    def test_concat_shared_dims(self):\n        ds1 = Dataset({\"x\": [0, 1], \"y\": [4, 5]})\n        ds2 = Dataset({\"x\": [2, 3], \"y\": [6, 7]})\n\n        with raises_regex(ValueError, \"must have the same dimensions\"):\n            concat([ds1, ds2], dim=\"y\")\n\n\n", "    def test_concat_non_identical_data_variables(self):\n        ds1 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.random.random((2, 3))), \"bar\": ((\"x\", \"y\"), [1, 2])},\n            {\"x\": [0, 1]},\n        )\n        ds2 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.random.random((2, 3))), \"bar\": ((\"x\", \"y\"), [3, 4])},\n            {\"x\": [0, 1]},\n        )\n\n        with raises_regex(ValueError, \"Cannot concatenate\"):\n            concat([ds1, ds2], dim=\"x\", data_vars=[\"foo\"])\n\n        with raises_regex(ValueError, \"Cannot concatenate\"):\n            concat([ds1, ds2], dim=\"x\", data_vars=[\"bar\"])\n\n\n\n"], "sample_38": ["    def test_invalid_password_message(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'test', 'password2': 'test123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [form.error_messages['password_mismatch']])\n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'test',\n            'password2': 'test',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        with self.subTest('password1 is too short'):\n            data['password1'] = 'te'\n            form = AdminPasswordChangeForm(user, data)\n            self.assertFalse(form.is_valid())\n            self.assertIn('password1', form.errors)\n            self.assertIn('password_too_short', form.errors['password1'])\n        with self.subTest('password2 is too short'):\n            data['password2'] = 'te'\n            form = AdminPasswordChangeForm(user, data)\n            self.assertFalse(form.is_valid())\n            self.assertIn('password2', form.errors)\n            self.assertIn('password_too_short', form.errors['password2'])\n\n", "    def test_password_length_validation(self):\n        user = User.objects.get(username='testclient')\n        short_password = 'too'\n        data = {'password1': short_password, 'password2': short_password}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            'This password is too short. It must contain at least 8 characters.',\n        ])\n        self.assertEqual(form.errors['password2'], [\n            'This password is too short. It must contain at least 8 characters.',\n        ])\n", "    def test_invalid_password_input(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'short', 'password2': 'short'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('Incorrect password format.', form.errors['password1'])\n        self.assertIn('Incorrect password format.', form.errors['password2'])\n", "    def test_password_validators(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'password',\n            'password2': 'password'\n        }\n\n        with self.assertRaises(ValidationError) as validation_error:\n            form = AdminPasswordChangeForm(user, data,\n                                          validators=[django.contrib.auth.validators.MinimumLengthValidator(10)])\n            form.validate()\n        self.assertIn(\n            'Password must be at least 10 characters long.',\n            list(validation_error.exception.args[0].items())\n        )\n\n"], "sample_158": ["def test_quantity_dimension_with_subscripts():\n    speed_of_light_squared = Quantity('speed_of_light_squared')\n    SI.set_quantity_dimension(speed_of_light_squared, (meter/second)**2)\n    assert SI.get_dimensional_expr(speed_of_light_squared) == (meter/second)**2\n    assert speed_of_light_squared.dimension == (length/time)**2\n\n\n\n", "compilation error", "def test_quantity_multiplication():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    SI.set_quantity_dimension(u, length)\n    SI.set_quantity_dimension(v, length)\n    u.set_global_relative_scale_factor(1, meter)\n    v.set_global_relative_scale_factor(2, meter)\n\n    assert (u * v).convert_to(meter**2) == 2*meter**2\n\n    assert (u * v * u).convert_to(meter**3) == 2*meter**3\n    \n    assert (u / v).convert_to(1/meter) == 0.5/meter\n    assert (u / v * u).convert_to(1) == 0.5 \n\n\n", "def test_quantity_symbol_name():\n    q1 = Quantity('q1')\n    q2 = Quantity('q2')\n    assert q1.name == 'q1'\n    assert q2.name == 'q2'\n    assert str(q1) == 'q1'\n    assert str(q2) == 'q2'\n    assert q1.abbrev is None\n    assert q2.abbrev is None\n", "def test_issue_24417():\n    from sympy.physics.units import  Quantity, Symbol, kilogram, meter, second\n    from sympy.physics.units.systems.si import dimsys_SI\n\n    mass = Quantity(\"mass\")\n    velocity = Quantity(\"velocity\")\n    time = Quantity(\"time\")\n    SI.set_quantity_dimension(mass, kilogram)\n    SI.set_quantity_dimension(velocity, meter/second)\n    SI.set_quantity_dimension(time, second)\n    x = Symbol('x')\n    expr = velocity**2 / (2 * mass * x)\n    exp_dim = dimsys_SI.get_dimensional_dependencies(expr)\n    assert exp_dim == {length: 1, mass: -1, time: -2}\n\n"], "sample_22": ["    def test_camel_case_to_spaces(self):\n        cases = (\n            ('ThisIsACamelCaseString', 'This Is A Camel Case String'),\n            ('test_string_name', 'test string name'),\n            ('this_is_a_test_string', 'this is a test string'),\n            ('anotherTest', 'another Test'),\n            ('camelCaseWithNumbers', 'camel Case With Numbers'),\n            ('camelCaseWITH_underscores', 'camel Case With Underscores'),\n        )\n        for input_string, expected_output in cases:\n            self.assertEqual(text.camel_case_to_spaces(input_string), expected_output)\n\n\n\n\n\n\n\n", "    def test_camel_case_to_spaces(self):\n        unittest.TestCase.assertEqual(text.camel_case_to_spaces('camelCase'), 'camel Case')\n        unittest.TestCase.assertEqual(text.camel_case_to_spaces('some_mixed_case'), 'some Mixed Case')\n        unittest.TestCase.assertEqual(text.camel_case_to_spaces('ALL_CAPS'), 'ALL CAPS')\n        unittest.TestCase.assertEqual(text.camel_case_to_spaces('underscore_case'), 'underscore Case')\n        unittest.TestCase.assertEqual(text.camel_case_to_spaces('mixed_Case_Words'), 'mixed Case Words')\n        unittest.TestCase.assertEqual(text.camel_case_to_spaces('First'), 'First')\n        unittest.TestCase.assertEqual(text.camel_case_to_spaces('some'), 'some')\n\n\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('thisIsACamelCaseString'), 'this is a camel case string')\n        self.assertEqual(text.camel_case_to_spaces('snake_case'), 'snake case')\n        self.assertEqual(text.camel_case_to_spaces('PascalCase'), 'Pascal Case')\n        self.assertEqual(text.camel_case_to_spaces('mixedCase'), 'mixed case')\n", "    def test_camel_case_to_spaces(self):\n        items = (\n            ('thisIsCamelCase', 'this is camel case'),\n            ('this_is_snake_case', 'this is snake case'),\n            ('A_Class_Name', 'A Class Name'),\n            ('FooBar', 'Foo Bar'),\n            ('camelCaseWithNumbers', 'camel case with numbers'),\n            ('MixedCase', 'Mixed Case'),\n            ('allCaps', 'all caps'),\n            ('all_lowercase', 'all lowercase'),\n            ('empty', ''),\n            ('123', '123'),\n        )\n        for value, output in items:\n            self.assertEqual(text.camel_case_to_spaces(value), output)\n            self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "    def test_lazy_str_interning(self):\n        lazy_str_1 = lazystr(\"hello\")\n        lazy_str_2 = lazystr(\"hello\")\n        self.assertIs(lazy_str_1, lazy_str_2)\n        lazy_str_3 = lazystr(\"world\")\n        self.assertNotIs(lazy_str_1, lazy_str_3)\n        \n        # Test interning in a larger string\n        large_string = \"This is a test string with {} and {}\".format(lazy_str_1, lazy_str_3)\n        self.assertEqual(large_string, \"This is a test string with hello and world\")\n\n\n\n        \n"], "sample_27": ["    def test_token_generation_with_email_field_override(self):\n        \"\"\"\n        Ensure the token generator correctly uses a custom email field\n        when provided.\n        \"\"\"\n        class CustomUser(User):\n            email = CustomEmailField()\n\n        CustomUser.objects.create_user(\n            'custom_email_user', 'user@example.com', 'testpw'\n        )\n        p0 = PasswordResetTokenGenerator()\n        user = CustomUser.objects.get(username='custom_email_user')\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n", "    def test_token_with_user_change(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        \n        # Change a user's field that should invalidate the token\n        user.is_staff = True\n        user.save()\n        \n        self.assertIs(p0.check_token(user, tk1), False)\n\n\n", "    def test_token_with_timestamp_overflow(self):\n        user = User.objects.create_user('overflowuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        # Generate a large timestamp to test for overflow.\n        timestamp = 2**64 - 1\n        token = p0._make_token_with_timestamp(user, timestamp)\n        self.assertIs(p0.check_token(user, token), False)\n", "    def test_token_with_custom_hashing_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        custom_algorithm = 'blake2b'\n        generator = PasswordResetTokenGenerator()\n        generator.algorithm = custom_algorithm\n        token = generator.make_token(user)\n        self.assertIs(generator.check_token(user, token), True)\n", "    def test_token_re_generation(self):\n        user = User.objects.create_user('re_gen_user', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n\n        # Try to check it with the old token\n        self.assertIs(p0.check_token(user, tk1), True)\n\n        # Generate a new token\n        tk2 = p0.make_token(user)\n\n        # Check it's invalid if the old token is used\n        self.assertIs(p0.check_token(user, tk1), False)\n\n        # Check if the newly generated token works\n        self.assertIs(p0.check_token(user, tk2), True)\n"], "sample_153": ["def test_pretty_print_nested_quantities():\n    from sympy.vector import CoordSys3D, Vector, simplify\n    from sympy.abc import a\n\n    N = CoordSys3D('N')\n\n    q = N.x**2\n    v = N.i * q + N.j * q**2\n\n    assert pretty(v) == 'x_N**2 i_N + x_N**4 j_N'\n    assert upretty(v) == 'x_N^{2} i_N + x_N^{4} j_N'\n\n\n", "def test_issue_23872():\n    from sympy.vector import CoordSys3D,  Vec, dot, cross, dyadic\n    from sympy import Symbol\n\n    N = CoordSys3D('N')\n    x = Symbol('x')\n\n    v = Vec(N, [x, x, 0])\n    u = v.unit\n    v_parallel = dot(v, u)*u\n    v_perp = v - v_parallel\n\n    assert upretty(v_parallel) == \"u_N\"\n    assert upretty(v_perp) == \"x i_N + x j_N\"\n\n    v_dyadic = v | v\n    v_dyadic_str = \"\"\"\\", "def test_pretty_print_dot_product():\n    from sympy.vector import CoordSys3D, Vector\n\n    N = CoordSys3D('N')\n    v1 = N.i + N.j\n    v2 = N.j + N.k\n    dot_product = v1 | v2 \n\n    assert pretty(dot_product) == 'N.i \u22c5 N.j + N.i \u22c5 N.k + N.j \u22c5 N.j + N.j \u22c5 N.k'\n    \n    assert upretty(dot_product) == 'i_N \u22c5 j_N + i_N \u22c5 k_N + j_N \u22c5 j_N + j_N \u22c5 k_N'\n", "def test_pretty_pretty_unicode_roundtrip():\n    expr = upretty_v_8\n    assert pretty(xpretty(expr)) == upretty(expr)\n    assert pretty(xpretty(v[8])) == upretty_v_8\n    assert pretty(xpretty(v[11])) == upretty_v_11\n    assert pretty(xpretty(s)) == upretty_s\n\n\n\n", "def test_partial_derivatives():\n    from sympy import symbols, diff, CoordSys3D\n    N = CoordSys3D('N')\n    x, y, z = N.x, N.y, N.z\n\n    f = x**2 + y**2 + z**2\n    assert pretty(diff(f, x)) == '2\u22c5x'\n    assert pretty(diff(f, y)) == '2\u22c5y'\n    assert pretty(diff(f, z)) == '2\u22c5z'\n\n"], "sample_110": ["def test_affinity_propagation_preference_range():\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    S = -euclidean_distances(X, squared=True)\n    \n    # Test different preference values\n    for pref in [-10, 0, 10, np.mean(S)]:\n        cluster_center_indices, labels = affinity_propagation(\n            S, preference=pref)\n    \n        # Ensure labels make sense based on preference value \n        if pref < np.min(S):\n            assert_array_equal(cluster_center_indices, [0])\n            assert_array_equal(labels, [0, 0, 0])\n        elif pref > np.max(S):\n            assert_array_equal(cluster_center_indices, [0, 1, 2])\n            assert_array_equal(labels, [0, 1, 2])\n        else :\n            assert len(cluster_center_indices) > 1\n            assert len(np.unique(labels)) > 1\n", "def test_affinity_propagation_preference_limits():\n    # Test the behavior of affinity_propagation() when preference is extreme\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    S = -euclidean_distances(X, squared=True)\n\n    # preference is much larger than all similarities\n    preference = np.max(S) * 10\n    with assert_warns(UserWarning,\n                      match=\"Preference is much larger than all \"\n                            \"similarities\"):\n        affinity_propagation(S, preference=preference)\n\n    # preference is much smaller than all similarities\n    preference = np.min(S) / 10\n    with assert_warns(UserWarning,\n                      match=\"Preference is much smaller than all \"\n                            \"similarities\"):\n        affinity_propagation(S, preference=preference)\n", "def test_affinity_propagation_preference_impact():\n    centers = np.array([[1, 1], [-1, -1], [1, -1]]) + 10\n    X, _ = make_blobs(n_samples=60, n_features=2, centers=centers,\n                      cluster_std=0.4, shuffle=True, random_state=0)\n    \n    # Test impact of preference on number of clusters\n    preferences = [0, 1, 10, 100]\n    for preference in preferences:\n        af = AffinityPropagation(preference=preference, max_iter=100)\n        cluster_centers_indices = af.fit(X).cluster_centers_indices_\n        n_clusters = len(cluster_centers_indices)\n\n        # If preference is high enough, expect to see more clusters\n        if preference >= 10:\n            assert n_clusters > 1\n        else:\n            assert n_clusters == 1 \n", "def test_affinity_propagation_preference_nan():\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n    with pytest.raises(ValueError):\n        affinity_propagation(\n            -euclidean_distances(X, squared=True), preference=np.nan)\n", "def test_affinity_propagation_preference_handling():\n    # Test handling of preference parameter. \n    X = np.array([[1, 1], [-1, -1], [1, -1]])\n    S = -euclidean_distances(X, squared=True)\n    n_samples = X.shape[0]\n    # Test preference is less than all similarities\n    preference = np.min(S) * 0.5\n    cluster_center_indices, labels = affinity_propagation(\n        S, preference=preference)\n    assert np.all(cluster_center_indices == np.arange(n_samples))\n    assert np.all(labels == np.arange(n_samples))\n\n    # Test preference equals all similarities, should be similar \n    preference = np.median(S)\n    cluster_center_indices, labels = affinity_propagation(\n        S, preference=preference)\n    assert np.all(cluster_center_indices == np.arange(n_samples))\n    assert np.all(labels == np.arange(n_samples))\n\n    # Test preference is greater than all similarities, should pick one \n    preference = np.max(S) * 1.1\n    cluster_center_indices, labels = affinity_propagation(\n        S, preference=preference)\n    assert len(np.unique(cluster_center_indices)) == 1\n\n\n    "], "sample_134": ["def test_ImmutableSparseMatrix():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    from sympy.sparse.matrices import ImmutableDenseMatrix\n\n    data = [[1, 2, 3],\n            [4, 5, 6]]\n    i, j = [0, 1, 2], [0, 1, 2]\n    sparse_matrix = ImmutableDenseMatrix(data, shape=(3, 3))\n    sparse_matrix_repr = sparse_matrix._smat\n\n    # This should match the NumPyPrinter output for a sparse matrix\n    assert NumPyPrinter().doprint(sparse_matrix) == \\\n        \"{name}({data}, ({i}, {j}), shape={shape})\".format(\n            name=self._module_format('scipy.sparse.coo_matrix'),\n            data=data, i=i, j=j, shape=sparse_matrix.shape\n        )\n\n", "def test_issue_17534():\n    if not np:\n        skip(\"NumPy not installed\")\n    from sympy import MatrixSymbol\n    M = MatrixSymbol(\"M\", 2, 2)\n    N = MatrixSymbol(\"N\", 2, 2)\n    expr = M @ N\n    f = lambdify((M, N), expr, \"numpy\")\n    ma = np.array([[1, 2], [3, 4]])\n    mb = np.array([[5, 6], [7, 8]])\n    assert (f(ma, mb) == ma @ mb).all()\n\n", "def test_issue_17016():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    from sympy.codegen.array_utils import CodegenArrayDiagonal\n    from sympy.tensor import tensorproduct\n\n    x = symbols('x')\n\n    A = CodegenArrayDiagonal(tensorproduct(x, x), (0,1))\n    assert A.args[0].shape == (2,)\n    assert printer.doprint(A) == 'numpy.diagonal(numpy.einsum(x, [0], x, [1]), axis1=0, axis2=1)'\n\n    \n\n", "def test_issue_17118():\n    if not np:\n        skip(\"NumPy not installed\")\n    a = symbols('a')\n    b = symbols('b')\n    expr = Matrix([[a, b], [a*b, b]])\n    f = lambdify((a, b), expr, 'numpy')\n    assert np.array_equal(f(1, 2), [[1, 2], [2, 2]])\n\n\n", "def test_Issue_17855():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    i = symbols('i', integer=True)\n    M = MatrixSymbol('M', 3, 3)\n    N = MatrixSymbol('N', 3, 3)\n    expr = (M @ N).transpose()\n    f = lambdify((M, N), expr, 'numpy')\n    ma = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    mb = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n    mr = f(ma, mb)\n    assert np.array_equal(mr, np.transpose(ma @ mb))\n\n"], "sample_40": ["    def test_render_field_with_required_true(self):\n        class MyForm(Form):\n            use_required_attribute = True\n            f1 = CharField(max_length=30)\n            \n                super().__init__(*args, **kwargs)\n                self.renderer = CustomRenderer()\n\n        form = MyForm()\n        self.assertHTMLEqual(\n            form.render('f1'),\n            '<p><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\" required></p>',\n        )\n\n", "    def test_renderer_property(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n", "    def test_custom_renderer_for_form_subclass(self):\n        class MyForm(Form):\n            field = CharField()\n\n            renderer = 'custom_renderer'  \n\n                return 'My custom render output'\n\n        class CustomRendererClass(object):\n                return 'Rendered by custom renderer'\n\n        custom_renderer = CustomRendererClass()\n\n        with patch('django.forms.Form.renderer', new=custom_renderer):\n            form = MyForm()\n            self.assertEqual(form.render(), 'Rendered by custom renderer')\n", "    def test_renderer_override_parent_class(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        class SubForm(CustomForm):\n            pass\n\n        custom = CustomRenderer()\n        form = SubForm(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n", "    def test_renderer_override_for_field(self):\n        class CustomRenderer(DjangoTemplates):\n            pass\n\n        class MyForm(Form):\n            name = CharField()\n\n        form = MyForm(renderer=CustomRenderer())\n        self.assertEqual(form.renderer, CustomRenderer)\n        field = form.fields['name']\n        field_renderer = field.renderer\n        self.assertEqual(field_renderer, form.renderer)\n        \n\n        class MyField(CharField):\n                kwargs['renderer'] = CustomRenderer()\n                super().__init__(**kwargs)\n        \n        form = MyForm(renderer=DjangoTemplates(), fields={'name':MyField()})\n        self.assertEqual(form.renderer, DjangoTemplates)\n        field = form.fields['name']\n        field_renderer = field.renderer\n        self.assertEqual(field_renderer, CustomRenderer)\n"], "sample_126": ["compilation error", "def test_issue_10070():\n    assert Float('1.0', 15) == Float('1.0000000000000000000')\n\n", "def test_issue_10926():\n    assert Float(S(0.25).n(3)).n(3) == Float(0.25).n(3)\n\n\n", "compilation error", "    def test_issue_12246():\n        x = Float('1.2345e+10', precision=30)\n        assert x._prec == 30\n        assert x == Float('12345000000.')\n"], "sample_20": ["    def test_check_unique_constraint_with_invalid_field(self):\n        class Model(models.Model):\n            field = models.IntegerField(unique=True)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['invalid_field'], name='name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field \"\n                \"'invalid_field'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n", "    def test_unique_constraint_with_null_field(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=100)\n            field2 = models.CharField(max_length=100, null=True)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['field1', 'field2'], name='name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"You cannot include null fields in unique constraints.\",\n                obj=Model,\n                id='models.E017',\n            ),\n        ])\n", "    def test_unique_constraint_with_doubled_fields(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['field', 'field'], name='name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the same field 'field' twice \"\n                \"in a UniqueConstraint. Each field must be unique.\",\n                obj=Model,\n                id='models.E014',\n            ),\n        ])\n\n", "    def test_unique_constraint_pointing_to_fk_nonunique_related_name(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE, related_name='related_model')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['fk'], name='name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' attempts to create a unique constraint on a \"\n                \"ForeignKey with a non-unique related_name, which would result in \"\n                \"a conflict.\",\n                obj=Model,\n                id='models.E039',\n            )\n        ])\n", "    def test_unique_constraint_pointing_to_self_fk(self):\n        class Model(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='children')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['parent_id'], name='unique_parent'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to a ForeignKey 'parent' that points to \"\n                \"the same model in a self-referential constraint. \"\n                \"This can lead to unexpected behavior.\",\n                obj=Model,\n                id='models.E039',\n            ),\n        ])\n\n\n\n"], "sample_31": ["    def test_no_startup_option(self):\n        with captured_stdout() as stdout:\n            call_command('shell', no_startup=True)\n        self.assertNotIn(\"running the interactive shell\", stdout.getvalue())\n", "    def test_no_startup_option(self):\n        with self.assertLogs('test', 'INFO') as cm:\n            call_command('shell', no_startup=True, command='print(\"test\")')\n        self.assertEqual(len(cm.records), 0)  \n", "    def test_stdin_read_empty(self, select):\n        with captured_stdin() as stdin, captured_stdout() as stdout:\n            stdin.seek(0)  # Simulate an empty input stream\n            call_command('shell')\n        self.assertEqual(stdout.getvalue().strip(), '')\n", "    def test_no_startup_option(self):\n        with self.assertLogs('test', 'INFO') as cm:\n            call_command(\n                'shell',\n                '--no-startup',\n                command=(\n                    'import django; from logging import getLogger; '\n                    'getLogger(\"test\").info(django.__version__)'\n                ),\n            )\n        self.assertEqual(cm.records[0].getMessage(), __version__)\n", "    def test_no_startup_option(self):\n        with captured_stdout() as stdout:\n            call_command('shell', no_startup=True, command=self.script_globals)\n        self.assertEqual(stdout.getvalue().strip(), 'True')\n"], "sample_88": ["def test_cyclic_references():\n    class A:\n            self.b = b\n\n    class B:\n            self.a = a\n\n    a = A(B(a))  \n    assert saferepr(a)\n", "def test_cyclic_references():\n    class A:\n            self.b = b\n\n    class B:\n            self.a = a\n\n    a = A(B(a))\n    b = B(a)  # Cycle!\n    try:\n        saferepr(a)\n    except ValueError as e:\n        assert \"cyclic references\" in str(e)\n\n\n\n", "def test_circular_references():\n    class A:\n            self.b = b\n\n    class B:\n            self.a = a\n\n    a = A(B(a))\n    try:\n        saferepr(a)\n    except RuntimeError as e:\n        assert \"circular reference\" in str(e)\n", "def test_circular_references():\n    class A:\n            self.b = b\n\n    class B:\n            self.a = a\n\n    a = A(B(a))\n    b = B(a)\n    assert saferepr(a) != saferepr(b)  # Different objects, should have different reprs\n    assert \"<[unpresentable exception\" in saferepr(a)\n", "def test_cyclic_references():\n    class A:\n            self.b = B(self)\n\n    class B:\n            self.a = a\n\n            return \"<B>\"\n\n    a = A()\n    s = saferepr(a)\n    assert \"B\" in s\n    assert \"A\" in s\n    assert \"self.b\" in s\n    assert \"self.a\" in s\n\n\n\n"], "sample_45": ["    def test_cache_control_decorator_with_kwargs(self):\n        @cache_control(max_age=3600, private=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'private'}\n        )\n\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, must_revalidate=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'must-revalidate', 'private'},\n        )\n\n", "    def test_cache_control_decorator_multiple_args(self):\n        @cache_control(max_age=3600, no_cache=True, private=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'no-cache', 'private'},\n        )\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, public=True, must_revalidate=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'public', 'must-revalidate'},\n        )\n\n\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, must_revalidate=True, private=True, no_cache=True, no_store=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'must-revalidate', 'private', 'no-cache', 'no-store'},\n        )\n\n\n"], "sample_122": ["def test_sparse_power():\n    a = SparseMatrix([\n        [0, 1],\n        [1, 0]\n    ])\n    assert a**3 == SparseMatrix([\n        [1, 0],\n        [0, 1]\n    ])\n    a = SparseMatrix([\n        [1, 2],\n        [0, 1]\n    ])\n    assert a**3 == SparseMatrix([\n        [1, 6],\n        [0, 1]\n    ])\n    a = SparseMatrix([\n        [2, 0],\n        [0, 3]\n    ])\n    assert a**3 == SparseMatrix([\n        [8, 0],\n        [0, 27]\n    ])\n\n\n\n\n\n", "def test_sparse_permute_rows():\n    A = SparseMatrix([[1, 2],\n                      [3, 4],\n                      [5, 6]])\n    B = A.permute_rows([2, 0, 1])\n    assert B == SparseMatrix([[5, 6],\n                             [1, 2],\n                             [3, 4]])\n    B = A.permute_rows([1, 2, 0])\n    assert B == SparseMatrix([[3, 4],\n                             [5, 6],\n                             [1, 2]])\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_23": ["    def test_union_with_subquery(self):\n        qs1 = Number.objects.filter(num__lte=5)\n        subquery = Number.objects.filter(num__gt=5).values('num')\n        self.assertNumbersEqual(qs1.union(subquery).order_by('-num'), [5, 6, 7, 8, 9])\n", "    def test_union_with_distinct_and_then_filter(self):\n        qs1 = Number.objects.filter(num__lte=3).distinct()\n        qs2 = Number.objects.filter(num__gte=4)\n        self.assertNumbersEqual(qs1.union(qs2).filter(num__gt=3), [4, 5, 6, 7, 8, 9])\n", "    def test_union_with_distinct_and_count(self):\n        Number.objects.bulk_create([\n            Number(num=0, other_num=10),\n            Number(num=1, other_num=9),\n            Number(num=2, other_num=8),\n            Number(num=0, other_num=10),\n        ])\n        qs1 = Number.objects.filter(num__lte=1).distinct()\n        qs2 = Number.objects.filter(num__gte=2).distinct()\n        self.assertEqual(qs1.union(qs2).count(), 3)\n        self.assertEqual(list(qs1.union(qs2)), [0, 1, 2])\n\n", "    def test_union_with_subquery(self):\n        qs1 = Number.objects.filter(num__gt=1)\n        qs2 = Number.objects.filter(num__lt=4).filter(num__gt=2)\n        self.assertNumbersEqual(qs1.union(qs2).order_by('num'), [3, 2, 1])\n", "    def test_prefetch_related_after_union(self):\n        ReservedName.objects.create(name='a', order=2)\n\n        class Person(models.Model):\n            name = models.CharField(max_length=100)\n            reserved_name = models.OneToOneField(ReservedName, on_delete=models.CASCADE)\n\n        persons = [\n            Person(name='person1'),\n            Person(name='person2'),\n        ]\n        Person.objects.bulk_create(persons)\n\n        qs1 = Number.objects.filter(num=1).values('num')\n        qs2 = Person.objects.values('name')\n\n        combined_qs = qs1.union(qs2)\n        # Ensure prefetch_related works as expected\n        combined_qs = combined_qs.prefetch_related('reserved_name')\n\n        for obj in combined_qs:\n            if isinstance(obj, Number):\n                pass  # Number doesn't have a reserved_name\n            else:\n                self.assertIsNotNone(obj.reserved_name)\n\n\n\n"], "sample_107": ["def test_LogisticRegression_get_params():\n    # Make sure get_params correctly handles the\n    #  l1_ratio and Cs attributes for elasticnet penalty\n\n    logistic = LogisticRegression(penalty='elasticnet', random_state=0,\n                                  l1_ratio=0.5, Cs=[0.1, 1, 10])\n\n    params = logistic.get_params()\n    assert params['penalty'] == 'elasticnet'\n    assert params['l1_ratio'] == 0.5\n    assert params['Cs'] == [0.1, 1, 10]\n", "def test_logistic_regression_fit_intercept(penalty):\n    # Test that fit_intercept is properly applied depending on the penalty\n    X, y = make_classification(random_state=0)\n    for fit_intercept in [True, False]:\n        lr = LogisticRegression(penalty=penalty, solver='saga',\n                                fit_intercept=fit_intercept,\n                                random_state=0)\n        lr.fit(X, y)\n        if fit_intercept:\n            assert lr.intercept_ is not None\n            assert len(lr.intercept_) == 1 if penalty == 'l2' else len(unique_labels(y))\n        else:\n            assert lr.intercept_ is None\n", "def test_logistic_regression_predict_proba_threshold():\n    # Test predict_proba with probability threshold\n\n    X, y = make_classification(n_samples=100, random_state=0)\n    lr = LogisticRegression(random_state=0, solver='saga')\n    lr.fit(X, y)\n    y_proba = lr.predict_proba(X)\n\n    thresholds = [0.1, 0.5, 0.9]\n\n    for threshold in thresholds:\n        y_pred = lr.predict_proba(X)[:, 1] > threshold\n        assert y_pred.shape == lr.predict(X).shape\n        assert lr.predict(X) == y_pred.astype(int).reshape(-1, 1)\n\n\n", "def test_penalty_none_with_intercept(solver):\n    # - Make sure that penalty='none' doesn't raise error when\n    #   fit_intercept=True.\n\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    lr_none = LogisticRegression(penalty='none', solver=solver,\n                                 random_state=0, fit_intercept=True)\n    lr_none.fit(X, y)\n\n\n", "def test_logistic_regression_with_intercept(penalty, solver):\n    n_samples = 500\n    X, y = make_classification(n_samples=n_samples, random_state=0)\n\n    lr = LogisticRegression(penalty=penalty, solver=solver,\n                            fit_intercept=True, random_state=0)\n    lr_no_intercept = LogisticRegression(penalty=penalty, solver=solver,\n                                        fit_intercept=False, random_state=0)\n\n    lr.fit(X, y)\n    lr_no_intercept.fit(X, y)\n\n    # Check that coefficients differ due to the intercept term\n    assert not np.allclose(lr.coef_, lr_no_intercept.coef_)\n\n    # Check that intercepts differ\n    assert not np.isclose(lr.intercept_, lr_no_intercept.intercept_)\n"], "sample_97": ["def test_multilabel_binarizer_empty_classes():\n    mlb = MultiLabelBinarizer()\n    with assert_raises(ValueError):\n        mlb.fit_transform([])\n", "def test_label_binarizer_errors():\n    # Check that invalid arguments yield ValueError\n    mlb = MultiLabelBinarizer()\n    assert_raises(ValueError, mlb.transform, [])\n    assert_raises(ValueError, mlb.inverse_transform, [])\n\n    # Fail on unseen labels\n    mlb = MultiLabelBinarizer()\n    mlb.fit([{'a', 'b'}, {'a', 'c'}, {'b', 'c'}])\n    msg = \"contains previously unseen labels\"\n    assert_raise_message(ValueError, msg, mlb.inverse_transform, {'d'})\n    assert_raise_message(ValueError, msg, mlb.inverse_transform, {'d', 'e'})\n\n    # Fail on inverse_transform(\"\")\n    msg = \"bad input shape ()\"\n    assert_raise_message(ValueError, msg, mlb.inverse_transform, \"\")\n\n\n\n\n", "compilation error", "def test_label_binarize_sparse_output_with_multiclass_data():\n    y = [0, 1, 2]\n    classes = [0, 1, 2]\n    expected = np.array([[1, 0, 0],\n                         [0, 1, 0],\n                         [0, 0, 1]])\n\n    binarized = label_binarize(y, classes, sparse_output=True)\n    assert_true(issparse(binarized))\n    assert_array_equal(toarray(binarized), expected)\n", "compilation error"], "sample_129": ["def test_Issue_14324():\n    from sympy.matrices import MatrixSymbol, Matrix\n    from sympy.stats import Normal, E\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    X = Matrix( [[x,1], [2,3]])\n    assert latex(E(A*X)) == r\"\\mathbb{E}\\left(A X\\right)\"\n    assert latex(Normal(mu=x, sigma=1).pdf(x)) == r\"\\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu}{1}\\right)^{2}}\"\n\n", "def test_gradient_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import gradient\n    f = Function('f')(x, y)\n    grad_f = gradient(f)\n    assert latex(grad_f) == r'\\frac{\\partial f}{\\partial x} \\mathbf{i} + \\frac{\\partial f}{\\partial y} \\mathbf{j}'\n", "def test_LieBracket_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import LieBracket\n    X = R2.X\n    Y = R2.Y\n    bracket = LieBracket(X, Y)\n    assert latex(bracket) == r\"\\left[X, Y\\right]\"\n", "def test_commutator():\n    from sympy.tensor.tensor import Commutator\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert latex(Commutator(A, B)) == r\"[A,B]\"\n    assert latex(Commutator(A, B, evaluate=False)) == r\"[A, B]\"  \n", "def test_CovariantDerivative_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import CovariantDerivative\n    wp = WedgeProduct(R2.dx, R2.dy)\n    A = MatrixSymbol(\"A\", 2, 2)\n    D = CovariantDerivative(A, R2.dx)\n    assert latex(D(wp)) == r\"D_{\\mathrm{d}x}\\left(\\mathrm{d}x \\wedge \\mathrm{d}y\\right)\"\n"], "sample_127": ["def test_issue_16902():\n    from sympy.physics import QED\n    A = QED.Field(\"A\")\n    assert latex(A.dual) == r\"A^{\\ast}\"\n", "def test_issue_15114():\n    from sympy.physics.units import hour\n    assert latex(2*hour) == r'2 \\cdot \\detokenize {hour}'\n\n\n\n", "compilation error", "def test_issue_15301():\n    from sympy.matrices import Matrix\n    a = Matrix([[1, 2], [3, 4]])\n    assert latex(a.col_at(0)) == r\"\\begin{bmatrix}1 \\\\ 3\\end{bmatrix}\"\n\n\n", "def test_issue_14472():\n    from sympy.physics.units import kilogram, meter, second\n    x = Symbol('x', unit=meter)\n    v = Symbol('v', unit=meter/second)\n    a = Symbol('a', unit=meter/second**2)\n    assert latex(v*a) == r'v \\cdot a' \n"], "sample_135": ["def test_rewrite_as():\n    from sympy.functions import cos, sin, exp, sqrt\n\n    x = symbols('x')\n    assert (sin(x) + cos(x)).rewrite_as(exp).subs(exp(I * x), 2*I).subs(exp(-I * x), I * 2) == 2*I + I * 2 \n\n    assert (sin(x) + cos(x)).rewrite_as(sqrt).subs(sqrt(1), 1).subs(sqrt(-1), I) == sqrt(1 + 2 * x)\n    \n    assert (3 * sin(x) + 2 * cos(x)).rewrite_as(exp).subs(exp(I * x), 2*I).subs(exp(-I * x), I * 2) == 3*(2*I) + 2*(I * 2)\n\n", "def test_rewrite_and_order_of_arguments():\n    from sympy.functions import cos, sin, tan\n\n    x, y = symbols('x y')\n    expr = cos(x) * exp(y)\n    assert expr.rewrite(tan, (sin, cos)).expand() == \\\n        (sin(x)/(cos(x))) * exp(y)\n    assert expr.rewrite([cos, sin], [exp, tan]).expand() == \\\n       exp(y) * (exp(I*x) - exp(-I*x))/(2*I)\n", "def test_class_key():\n    x = symbols('x')\n    assert Basic().class_key() == (1, 0, '')\n    assert (x + 1).class_key() == (1, 0, 'Add')\n    assert (x**2).class_key() == (1, 0, 'Pow')\n", "def test_repr():\n    assert repr(Basic(1, 2, 3)) == 'Basic(1, 2, 3)'\n", "compilation error"], "sample_83": ["def test_multi_reporter_custom_output_format(tmp_path):\n    output_file = tmp_path / \"output.txt\"\n    json_data = {\n        \"module\": \"test_module\",\n        \"messages\": [\n            {\"msg_id\": \"C0301\", \"message\": \"Line too long\", \"line\": 1},\n        ],\n    }\n\n    class CustomReporter(BaseReporter):\n            super().__init__()\n            self.output_file = output_file\n\n            pass\n\n            with open(self.output_file, \"w\") as f:\n                f.write(json.dumps(json_data))\n\n    class TextReporter(TextReporter):\n            super().__init__(*args, **kwargs)\n            self.output_file = None\n\n            pass\n\n    multi_reporter = MultiReporter(\n        [CustomReporter(output_file), TextReporter()], lambda: None\n    )\n\n    linter = PyLinter()\n    linter.set_option(\"output-format\", \"json:output.txt\")\n    linter.reporter = multi_reporter\n\n    linter.add_message(\"line-too-long\", line=1, args=(1, 2))\n    linter.generate_reports()\n\n\n\n    with open(output_file, \"r\") as f:\n        assert f.read() == json.dumps(json_data)\n\n\n\n", "def test_message_location_tuple_formatting(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = \"{path}:{line}:{column}:{end_line}:{end_column}: {msg_id}: {msg} ({symbol})\"\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2), end_lineno=2, end_col_offset=4)\n    linter.generate_reports()\n    assert output.getvalue() == (\n        \"my_mod:1:0:2:4: C0301: Line too long (1/2) (line-too-long)\\n\"\n    )\n\n\n\n", "def test_template_option_with_invalid_option(linter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = \"{path}:{line}:{categ}:({a_second_new_option:03d})\"\n    linter.open()\n    with pytest.warns(UserWarning) as records:\n        linter.set_current_module(\"my_mod\")\n        assert len(records) == 2\n        assert \"Don't recognize the argument 'categ'\" in records[0].message.args[0]\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\n        \"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4)\n    )\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1] == \"my_mod:1::()\"\n    assert out_lines[2] == \"my_mod:2::()\"\n\n\n\n", "def test_set_output_deprecated():\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\", category=DeprecationWarning)\n        reporter = TextReporter()\n        # noinspection PyDeprecation\n        reporter.set_output(sys.stdout)\n\n    assert len(cm) == 1\n    assert \"set_output' will be removed in 3.0\" in str(cm[0].message)\n", "def test_show_source_disabled(tmp_path):\n    source_file = tmp_path / \"somemodule.py\"\n    source_file.write_text('NOT_EMPTY = \"This module is not empty\"\\n')\n\n    with redirect_stdout(StringIO()):\n        linter = PyLinter()\n        linter.load_default_plugins()\n        linter.set_option(\"reports\", True)\n        linter.set_option(\"show-source\", False)\n        linter.set_option(\"output-format\", \"json\")\n        linter.open()\n        linter.check_single_file_item(FileItem(\"somemodule\", source_file, \"somemodule\"))\n        linter.generate_reports()\n\n    with open(tmp_path / \"somemodule.py\", encoding=\"utf-8\") as f:\n        assert f.read() == text.getvalue()\n"], "sample_74": ["def test_colorbar_label_spacing():\n\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    # check default spacing works\n    cb.ax.set_title(\"My Colorbar\")\n    fig.draw_without_rendering()\n    assert cb.ax.title.get_position()[1] > cb.ax.get_position()[3]\n\n    # set spacing to 0\n    cb.ax.set_title(\"My Colorbar\")\n    cb.ax.set_title(\"My Colorbar\", loc=\"left\", pad=0)\n\n    fig.draw_without_rendering()\n    assert cb.ax.title.get_position()[1] == cb.ax.get_position()[3]\n\n\n\n    cb.ax.set_title(\"My Colorbar\", loc=\"left\", pad=0.2)\n    fig.draw_without_rendering()\n    assert cb.ax.title.get_position()[1] < cb.ax.get_position()[3] # check that\n\n    # manually set spacing to large value\n    cb.ax.set_title(\"My Colorbar\")\n    cb.ax.set_title(\"My Colorbar\", loc=\"left\", pad=1.5)\n    fig.draw_without_rendering()\n    assert cb.ax.title.get_position()[1] < cb.ax.get_position()[1]\n", "def test_colorbar_position_with_subplots():\n    fig = plt.figure(figsize=(8, 4))\n    grid = fig.add_gridspec(2, 2, width_ratios=[1, 1], height_ratios=[1, 2])\n\n    ax1 = grid[0, 0]\n    ax2 = grid[0, 1]\n    ax3 = grid[1, 0]\n    ax4 = grid[1, 1]\n\n    im1 = ax1.imshow([[1, 2], [3, 4]], cmap='viridis')\n    im2 = ax2.imshow([[5, 6], [7, 8]], cmap='inferno')\n\n    # Test colorbar position with the same figure\n    cb1 = fig.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n    cb2 = fig.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n    assert cb1.ax.get_position().bounds != cb2.ax.get_position().bounds\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_76": ["    def test_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n        res_order1 = PolyFit(order=1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        res_order3 = PolyFit(order=3)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        for group in res_order1.groupby(\"group\"):\n            assert group[1][\"y\"].nunique() > res_order3.loc[group[0]][\n                \"y\"\n            ].nunique()\n", "    def test_order(self, df):\n        groupby = GroupBy([\"group\"])\n        res1 = PolyFit(order=1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        res2 = PolyFit(order=2)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert_array_almost_equal(res1[\"y\"].values, res2[\"y\"].values, decimal=2)\n\n\n", "    def test_order_too_large(self, df):\n\n        groupby = GroupBy([\"group\"])\n        with pytest.raises(ValueError):\n            PolyFit(order=df[\"x\"].nunique() + 1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n", "    def test_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n        for order in range(1, 5):\n            res1 = PolyFit(order=order)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n            res2 = PolyFit(order=order + 1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n            # Check that the number of points in the outputs is as expected\n            assert len(res1) == len(res2)\n            # Check that the y values are different for different orders, meaning the fit is changing\n            assert not np.array_equal(res1[\"y\"], res2[\"y\"])\n", "    def test_order_too_high(self, df):\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=df[\"x\"].nunique() + 1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        assert res.empty\n\n"], "sample_150": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_18": ["    def test_invalid_field_ordering(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('invitee', 'invalid_field'),\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"The intermediary model 'invalid_models_tests.Invitation' has no field 'invalid_field'.\",\n                hint=\"Did you mean one of the following foreign keys to 'Event': event?\",\n                obj=field,\n                id='fields.E338',\n            ),\n        ])\n\n\n\n\n", "    def test_overlapping_fields(self):\n        class Person(models.Model):\n            name = models.CharField(max_length=255)\n            age = models.IntegerField()\n\n        class Group(models.Model):\n            name = models.CharField(max_length=255)\n            members = models.ManyToManyField(Person, through='GroupMembership', through_fields=('group', 'person'))\n\n        class GroupMembership(models.Model):\n            group = models.ForeignKey(Group, on_delete=models.CASCADE)\n            person = models.ForeignKey(Person, on_delete=models.CASCADE)\n\n        field = Group._meta.get_field('members')\n        self.assertEqual(field.check(from_model=Group), [\n            Error(\n                \"The intermediary model 'invalid_models_tests.GroupMembership' has conflicting fields 'group' and 'person'.\",\n                hint=(\n                    \"Ensure that the intermediary model's fields are distinct from any fields in the related model(s).\"\n                ),\n                obj=field,\n                id='fields.E338',\n            ),\n        ])\n", "    def test_duplicate_field_names(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(Fan, through='Invitation', through_fields=('invitee', 'event'))\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            # Duplicate field name\n            event = models.ForeignKey(Event, models.CASCADE) \n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"Intermediary model 'invalid_models_tests.Invitation' has duplicate field name 'event'.\",\n                hint=\"Rename one of the fields to avoid duplication.\",\n                obj=field,\n                id='fields.E340',\n            ),\n        ])\n\n\n\n", "    def test_non_existent_through_model(self):\n        with self.assertRaises(ImproperlyConfigured):\n            class Fan(models.Model):\n                pass\n\n            class Event(models.Model):\n                invitees = models.ManyToManyField(\n                    Fan,\n                    through='NonExistentThroughModel',\n                )\n", "    def test_unique_foreign_object_with_intersection(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['a', 'b'], name='unique_ab'),\n                    models.UniqueConstraint(fields=['a', 'c'], name='unique_ac'),\n                ]\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [])\n\n\n\n\n"], "sample_9": ["    def test_notify_file_changed_does_not_notify_twice(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            with mock.patch('django.utils.autoreload.StatReloader.send_signal') as mock_send_signal:\n                self.reloader.notify_file_changed(self.existing_file)\n                self.reloader.notify_file_changed(self.existing_file)\n                self.assertEqual(mock_send_signal.call_count, 1)\n", "    def test_snapshot_files_with_multiple_globs(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.tempdir / 'does_not_exist.py']):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.assertNotIn(self.tempdir / 'does_not_exist.py', snapshot)\n\n\n\n", "    def test_snapshot_files_with_non_existing_file(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.nonexistent_file, snapshot)\n\n", "    def test_snapshot_files_with_globs(self):\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.tempdir / 'nonexistent.py']):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.assertNotIn(self.tempdir / 'nonexistent.py', snapshot)\n\n", "    def test_snapshot_files_with_relative_file_paths(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.tempdir / 'file.py']):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.tempdir / 'file.py', snapshot)\n            self.assertEqual(snapshot[self.tempdir / 'file.py'], os.stat(self.tempdir / 'file.py').st_mtime)\n"], "sample_132": ["def test_intersection_polygon_circle():\n    circ = Circle((0, 0), 1)\n    p1 = Polygon((0, 1), (1, 1), (1, 0), (0, 0))\n    p2 = Polygon((-1, 0), (0, 1), (1, 0), (0, -1))\n    assert intersection(circ, p1) == [Point2D(0, 0)]\n    assert intersection(circ, p2) == [Point2D(0, 0)]\n    assert intersection(p1, p2) == []\n    assert intersection(circ, p1, pairwise=True) == [Point2D(0, 0)]\n    assert intersection(circ, p2, pairwise=True) == [Point2D(0, 0)]\n    assert intersection(p1, p2, pairwise=True) == []\n", "    def test_intersection_line_segment():\n        p1 = Point2D(0, 0)\n        p2 = Point2D(5, 0)\n        l = Line(p1, p2)\n        s = Segment(p1, p2)\n        assert intersection(l, s) == s\n        assert intersection(s, l) == s\n\n        p3 = Point2D(2, 1)\n        l = Line(p1, p3)\n        s = Segment(p1, p2)\n        assert intersection(l, s) == [Point2D(2, 0)]\n        assert intersection(s, l) == [Point2D(2, 0)]\n        assert intersection(l, s, pairwise=True) == [Point2D(2, 0)]\n\n        l = Line(p1, p3)\n        s = Segment(p3, p2)\n        assert intersection(l, s) == [Point2D(3, 0)]\n        assert intersection(s, l) == [Point2D(3, 0)]\n        assert intersection(l, s, pairwise=True) == [Point2D(3, 0)]\n\n        l = Line(p1, p2)\n        s = Segment(p2, p3)\n        assert intersection(l, s) == [Point2D(5, 0)]\n        assert intersection(s, l) == [Point2D(5, 0)]\n        assert intersection(l, s, pairwise=True) == [Point2D(5, 0)]\n\n        # Non-intersecting cases\n        l = Line(p1, p3)\n        s = Segment(p2, p3)\n        assert intersection(l, s) == []\n        assert intersection(s, l) == []\n        assert intersection(l, s, pairwise=True) == []\n", "def test_intersection_with_special_cases():\n    p = Point(0, 0)\n    c = Circle(p, 1)\n    assert intersection(c, c) == [c]\n    assert intersection(c, p) == [p]\n    l1 = Line((0, 0), (1, 1))\n    l2 = line = Line((0, 1), (1, 0))\n    assert intersection(l1, l2, pairwise=True) == [Point2D(1/2, 1/2)]\n    l3 = Line((0, 0), (1, 0))\n    assert intersection(l1, l3, pairwise=True) == [\n        Point2D(1/2, 0)]\n    assert intersection(l1, l3) == []\n    assert intersection(c, l1) == [Point2D(0, 0), Point2D(1, 1)]\n\n    from sympy.geometry import Ray\n    r1 = Ray((0, 0), (1, 0))\n    r2 = Ray((0, 0), (0, 1))\n\n    assert intersection(r1, r2) == [Point2D(0, 0)]\n\n\n\n", "compilation error", "def test_intersection_multi_args():\n    p = Point(0, 0)\n    circ = Circle(p, 1)\n    l = Line(p, (1, 1))\n    s = Segment(p, (2, 0))\n    raise_assertion = lambda: raises(TypeError, lambda: intersection(\n        circ, l, s=1))\n    assert raise_assertion()\n    assert intersection(circ, l) == [Point2D(0, 0)]\n    assert intersection(circ, l, s) == [Point2D(0, 0)]\n    assert intersection(circ, s) == [Point2D(0, 0), Point2D(1, 0)]\n    assert intersection(circ, l, s, pairwise=True) == \\\n        [Point2D(0, 0), Point2D(1, 0), Point2D(0, 0)]\n    assert intersection(l, s) == [Point2D(0, 0), Point2D(1, 0)]\n"], "sample_77": ["    def test_label_format(self, t):\n\n        fmt = \"%b %Y\"\n        s = Temporal().label(format=fmt)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"Sep 1972\"\n\n", "    def test_label_concise_with_format(self, t):\n\n        ax = mpl.figure.Figure().subplots()\n        Temporal().label(concise=\"%Y-%m\")._setup(t, Coordinate(), ax.xaxis)\n        formatter = ax.xaxis.get_major_formatter()\n        label, = formatter.format_ticks([100])\n        assert label == \"1970-01\"\n", "    def test_tick_every(self, t, x):\n\n        s = Temporal().tick(every=5)._setup(t, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        ticks = a.major.locator()\n        assert_allclose(ticks, x[::5])\n\n", "    def test_tick_count(self, t, x):\n\n        n = 5\n        s = Temporal().tick(count=n)._setup(t, Coordinate())\n        assert len(s(t)) == n\n\n\n", "    def test_label_custom_format(self, t):\n\n        fmt = \"%b %Y\"\n        s = Temporal().label(fmt)._setup(t, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"Sep 1972\" \n"], "sample_87": ["def test_collect_python_files_with_exclude(testdir):\n    testdir.makepyfile(\n        \"def test_1(): pass\",\n        \" def test_2(): pass\",\n        \"__init__.py\",\n    )\n    result = testdir.runpytest(\"-k\", \"test_1\", \"--exclude=test_2.py\")\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n\n", "compilation error", "compilation error", "def test_collect_skip_if_modified(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import os\n        import time\n            time.sleep(0.1)\n            os.system('touch %s' % testdir.tmpdir.join(\"ignored_file\").strpath)\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    \n    testdir.tmpdir.join(\"ignored_file\").write(\"test\")\n    p = testdir.runpytest(\"--collect-only\", \"--skipif=os.path.exists('{ignore_file}')\"\n                        .format(ignore_file=str(testdir.tmpdir.join(\"ignored_file\"))))\n    p.stdout.fnmatch_lines([\"collected 1 item\"])\n    \n\n\n\n", "compilation error"], "sample_128": ["def test_allowed_flags():\n    allowed_flags({'domain': ZZ}, ['frac'])\n    allowed_flags({'domain': ZZ, 'frac': True}, ['frac'])\n    raises(FlagError, lambda: allowed_flags({'domain': ZZ}, ['gaussian']))\n", "def test_allowed_flags():\n    flags = ['domain', 'field', 'gaussian', 'extension', 'modulus',\n             'symmetric', 'strict', 'auto', 'frac', 'formal', 'polys',\n             'include', 'all', 'gen', 'symbols', 'method']\n    allowed_flags = {'domain': ZZ, 'field': True}\n\n    allowed_flags = set(allowed_flags.keys())\n    for flag in flags:\n        if flag in allowed_flags:\n            allowed_flags.remove(flag)\n    assert len(allowed_flags) == 0\n    \n    raises(FlagError, lambda: allowed_flags({'domain': ZZ, 'frac': True}))\n", "def test_allowed_flags():\n    options = {'domain': ZZ, 'field': True}\n\n    allowed_flags = ['frac']\n    allowed_flags_ = ['frac', 'gaussian']\n\n    with raises(FlagError):\n        allowed_flags(options, allowed_flags)\n\n    allowed_flags(options, allowed_flags_)\n", "def test_allowed_flags():\n    allowed_flags(\n        {'domain': ZZ},\n        ['frac'])\n    allowed_flags(\n        {'domain': ZZ, 'frac': True},\n        ['frac'])\n    raises(FlagError, lambda: allowed_flags(\n        {'domain': ZZ, 'frac': True}, ['split']))\n\n\n", "def test_allowed_flags():\n    args = {'domain': ZZ, 'field': True}\n\n    allowed_flags(args, [])\n    \n    \n    args = {'domain': ZZ, 'frac': True}\n    raises(FlagError, allowed_flags, args, []) \n"], "sample_125": ["compilation error", "def test_issue_10868():\n    assert Float('1.23456789e10', 10) == Float(1.23456789e10, 10)\n    assert Float('1.23456789e-10', 10) == Float(1.23456789e-10, 10) \n", "def test_issue_10309():\n    assert Float(1.23456789012345678901234567890123).n(100) == \"1.23456789012345678901234567890123\"\n\n\n\n", "def test_issue_10836():\n    assert Float(np.e).n(5) == 2.71828\n\n\n", "compilation error"], "sample_65": ["    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"b\": \"<b>1</b>\"})\n        self.assertEqual(output, '<span><b>1</b></span><span><b>1</b></span>')\n\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alphabeta & me\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [1, 2, 3], \"var\": \" + \"})\n        self.assertEqual(output, \"1 + 2 + 3\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": mark_safe(\" & \")}\n        )\n        self.assertEqual(output, \"alpha & beta & me\")\n", "    def test_join_empty_list(self):\n        output = self.engine.render_to_string(\"join_empty_list\", {\"a\": []})\n        self.assertEqual(output, \"\")\n"], "sample_89": ["def test_prunetraceback():\n    class FakeCollector(nodes.Collector):\n            super().__init__(*args, **kwargs)\n            self.prune_called = False\n\n            self.prune_called = True\n\n    # Test the 'prune_traceback' method\n    collector = FakeCollector(fspath=py.path.local(\"foo\"))\n    excinfo = nodes.ExceptionInfo(Exception(\"Test\"))\n    collector._prunetraceback(excinfo)\n    assert collector.prune_called is True\n", "def test_node_location_to_relpath():\n    from _pytest.pathlib import Path\n\n    class FakeSession:\n            return str(path.relto(self._initialpaths[0]))\n\n    session = FakeSession()\n    session._initialpaths = [py.path.local(\"/home/user/project\")]\n    assert session._node_location_to_relpath(py.path.local(\"/home/user/project/test.py\")) == \"test.py\"\n    assert session._node_location_to_relpath(py.path.local(\"/home/user/other_project/test.py\")) is None\n", "def test_prunetraceback_from_different_nodes(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import os\n\n            x = 1 / 0\n\n            x = 1 / 0\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\n        \"*1 passed*\",\n        \"*1 failed*\",\n    ])\n    items = result.getcollected()\n\n    # Ensure that we can prune tracebacks correctly across different nodes.\n    for item in items:\n      if item.name == \"test_my_other_test\":\n        excinfo = item.excinfo\n        assert excinfo.traceback.first().path != nodes.tracebackcutdir\n\n\n\n", "def test_repr_failure_fulltrace_short_tbstyle(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            raise ValueError(\"Test Failure\")\n    \"\"\"\n    )\n    p.runpytest()\n    # Assert that the full traceback is included\n    result = p.runpytest_stdout(\n        \"--tbstyle=short\", \"--fulltrace\"\n    )\n    assert result.stdout.fnmatch_lines(\n        [\n            \"*1 failed*\",\n            \"*Test Failure*\",\n            \"*short traceback\",\n        ]\n    )\n", "def test_node_from_parent_fspath_handling():\n    class FakeParent(nodes.Node):\n            super().__init__(\"parent\", None, None, None, fspath=fspath)\n\n    with pytest.raises(TypeError, match=\"config is\"):\n        nodes.Node.from_parent(FakeParent(py.path.local(\"parent\")), config=None)\n\n    with pytest.raises(TypeError, match=\"session is\"):\n        nodes.Node.from_parent(FakeParent(py.path.local(\"parent\")), session=None)\n\n"], "sample_120": ["def test_matrix_dot_commutative():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    assert (A@B).doit() == (B@A).doit() \n", "def test_transpose_adjoint():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    assert (A.T).T == A\n    assert (A.T).shape == (m, n)\n    assert (B.T).shape == (n, m)\n    assert (A + B).T == A.T + B.T\n    assert (A*B).T == B.T*A.T\n    assert (A.T).conjugate() == A.conjugate().T\n    assert (A.T).adjoint() == A.adjoint().T\n    assert (A.conj()).T == (A.T).conj()\n\n\n\n", "def test_MatrixSymbol_subs_mul():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', l, k)\n    assert (A*B*C).subs(B, C).shape == (n, k)\n    assert (A*B*C).subs(A, Identity(n)).shape == (n, k)\n    assert (A*B*C).subs(C, Identity(k)).shape == (n, l)\n", "def test_simplify_matrix_element():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A[0, 0] * B[0, 1] + A[1, 0] * B[0, 0]\n    assert simplify(expr).doit() == A[0, 0] * B[0, 1] + A[1, 0] * B[0, 0]\n", "def test_matrix_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    E = MatrixSymbol('E', m, n)\n\n    # Test substitution of MatrixSymbols\n    assert (A*B).subs({A: C, B: D}).shape == (n, l)\n    assert (A + B).subs({A: C, B: D}) == C + D\n\n    # Test substitution of values\n    assert (A*B).subs({A: [[1, 2], [3, 4]], B: [[5, 6], [7, 8]]}).shape == (n, l)\n    assert (A + B).subs({A: [[1, 2], [3, 4]], B: [[5, 6], [7, 8]]}) == [[6, 8], [10, 12]]\n\n    # Test substitution with complex expressions\n    expr = (A*B + C)**2\n    assert expr.subs({A: C, B: D}) == (C*D + C)**2\n\n    # Test substitution with conditional expressions\n    expr = Piecewise((A*B, A.rows == B.cols), (C*D, A.rows != B.cols))\n    assert expr.subs({A: C, B: D}) == Piecewise((C*D, C.rows == D.cols), (C*D, C.rows != D.cols))\n"], "sample_73": ["def test_offsetbox_clip_children_issue10476():\n    fig, ax = plt.subplots()\n\n    h, w = 100, 100\n    bg = mpatches.Rectangle((0, 0), w, h, facecolor='lightgray')\n    child = DrawingArea(h, w, clip=True)\n    child.add_artist(bg)\n    \n\n    ab = AnchoredOffsetbox(loc='center', child=child,\n                          pad=0., frameon=False,\n                          bbox_to_anchor=(0.5, 0.5),\n                          bbox_transform=ax.transAxes,\n                          borderpad=0.)\n\n    ax.add_artist(ab)\n    fig.canvas.draw()\n\n    # Add a black line extending beyond the clip bounds\n    line = mlines.Line2D([0, 110], [h/2, h/2], color='black', linewidth=2)\n    ab.add_artist(line)\n    fig.canvas.draw()\n\n    \n    # Make sure the line is clipped\n    bb = ab.get_window_extent(fig.canvas.get_renderer())\n    assert bb.width < w + 10  \n\n", "def test_annotationbbox_rotation():\n    fig, ax = plt.subplots()\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0.2, 0.3),\n                        xycoords='data', boxcoords='axes fraction',\n                        rotation=45,\n                        arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n", "def test_picking_after_remove(child_type, boxcoords):\n    # These all take up approximately the same area.\n    if child_type == 'draw':\n        picking_child = DrawingArea(5, 5)\n        picking_child.add_artist(mpatches.Rectangle((0, 0), 5, 5, linewidth=0))\n    elif child_type == 'image':\n        im = np.ones((5, 5))\n        im[2, 2] = 0\n        picking_child = OffsetImage(im)\n    elif child_type == 'text':\n        picking_child = TextArea('\\N{Black Square}', textprops={'fontsize': 5})\n    else:\n        assert False, f'Unknown picking child type {child_type}'\n\n    fig, ax = plt.subplots()\n    ab = AnnotationBbox(picking_child, (0.5, 0.5), boxcoords=boxcoords)\n    ab.set_picker(True)\n    ax.add_artist(ab)\n\n    calls = []\n    fig.canvas.mpl_connect('pick_event', lambda event: calls.append(event))\n\n    # Annotation should be picked by an event occurring at its center.\n    if boxcoords == 'axes points':\n        x, y = ax.transAxes.transform_point((0, 0))\n        x += 0.5 * fig.dpi / 72\n        y += 0.5 * fig.dpi / 72\n    elif boxcoords == 'axes pixels':\n        x, y = ax.transAxes.transform_point((0, 0))\n        x += 0.5\n        y += 0.5\n    else:\n        x, y = ax.transAxes.transform_point((0.5, 0.5))\n    fig.canvas.draw()\n    calls.clear()\n    MouseEvent(\n        \"button_press_event\", fig.canvas, x, y, MouseButton.LEFT)._", "def test_annotationbbox_align():\n    fig, ax = plt.subplots()\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0.2, 0.3),\n                        xycoords='data',\n                        boxcoords=\"axes fraction\",\n                        bbox_transform=ax.transAxes,\n                        alignment=(0.5, 0.5))\n    ax.add_artist(ab)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bbox = ab.get_bbox(renderer)\n    assert bbox.min[0] > 0 and bbox.max[1] < 1  # bounds clip to axes\n", "def test_textarea_set_multiline(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    text0 = AnchoredText(\"Foo\\nBar\", \"upper left\")\n    ax_ref.add_artist(text0)\n\n    ax_test = fig_test.add_subplot()\n    text1 = AnchoredText(\"Foo\\nBar\", \"upper left\")\n    ax_test.add_artist(text1)\n    text1.set_multilinebaseline(True)\n\n\n\n"], "sample_30": ["    def test_inlines_verbose_name_plural(self):\n        \"\"\"\n        The item added by the \"Add another XXX\" link must use the correct\n        verbose_name_plural in the inline form.\n        \"\"\"\n        self.admin_login(username='super', password='secret')\n        # Hide sidebar.\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_course_add'))\n        toggle_button = self.selenium.find_element_by_css_selector('#toggle-nav-sidebar')\n        toggle_button.click()\n        # Each combination of horizontal/vertical filter with stacked/tabular\n        # inlines.\n        tests = [\n            'admin:admin_inlines_course_add',\n            'admin:admin_inlines_courseproxy_add',\n            'admin:admin_inlines_courseproxy1_add',\n            'admin:admin_inlines_courseproxy2_add',\n        ]\n        css_selector = '.dynamic-class_set#class_set-%s h2'\n\n        for url_name in tests:\n            with self.subTest(url=url_name):\n                self.selenium.get(self.live_server_url + reverse(url_name))\n                # First inline shows the verbose_name_plural.\n                available, chosen = self.selenium.find_elements_by_css_selector(css_selector % 0)\n                self.assertEqual(available.text, 'AVAILABLE ATTENDANTS')\n                self.assertEqual(chosen.text, 'CHOSEN ATTENDANTS')\n                # Added inline should also have the correct verbose_name_plural.\n                self.selenium.find_element_by_link_text('Add another Class').click()\n                available, chosen = self.selenium.find_elements_by_css_selector(css_selector % 1)\n                self.assertEqual(available.text, 'AVAILABLE ATTENDANTS')\n                self.assertEqual(chosen.text, 'CHOSEN ATTENDANTS')\n                # Third inline should also have the correct verbose_name_plural.\n                self.selenium.find_element_by_link_text('Add another Class').click()\n                available, chosen = self.selenium.find_elements_", "    def test_inline_field_order(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder6_add'))\n        # Verify that fields in the inlines are returned in\n        # the same order as defined in the model.\n        stacked_inline_fields = [\n            'id_inner6stacked_set-0-name', 'id_inner6stacked_set-0-email', 'id_inner6stacked_set-0-nonfield'\n        ]\n        tabular_inline_fields = [\n            'id_inner6tabular_set-0-name', 'id_inner6tabular_set-0-email', 'id_inner6tabular_set-0-nonfield'\n        ]\n        stacked_field_order = self.selenium.find_elements_by_css_selector(\n            '#inner6stacked_set-group input'\n        )\n        tabular_field_order = self.selenium.find_elements_by_css_selector(\n            '#inner6tabular_set-group input'\n        )\n        self.assertEqual(\n            [element.get_attribute('name') for element in stacked_field_order],\n            stacked_inline_fields\n        )\n        self.assertEqual(\n            [element.get_attribute('name') for element in tabular_field_order],\n            tabular_inline_fields\n        )\n\n\n\n", "    def test_inline_formset_validation_errors(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        stacked_inline_formset_selector = 'div#inner5stacked_set-group fieldset.module.collapse'\n        tabular_inline_formset_selector = 'div#inner5tabular_set-group fieldset.module.collapse'    \n        # Inlines without errors.\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector + '.error')), 0\n        )\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(tabular_inline_formset_selector + '.error')), 0\n        )\n\n        test_fields = ['#id_inner5stacked_set-0-dummy', '#id_inner5tabular_set-0-dummy']\n        for show_index, field_name in enumerate(test_fields):\n            show_links = self.selenium.find_elements_by_link_text('SHOW')\n            show_links[show_index].click()\n            self.wait_until_visible(field_name)\n            self.selenium.find_element_by_id(field_name[1:]).send_keys('error')\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector + '.error')), 1\n        )\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(tabular_inline_formset_selector + '.error')), 1\n        )\n        # Click on an error message to scroll to the form field\n        error_message = self.selenium.find_elements_by_css_selector(\n            '.errorlist li').text\n        error_message_element = self.selenium.find_element_by_xpath(\n            f'//button[contains(", "    def test_inline_formset_error_unique_together(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_author_add'))\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        self.assertEqual(len(show_links), 2)\n        show_links[0].click()\n        self.wait_until_visible('#id_nonautopkbook_set-0-title')\n        self.selenium.find_element_by_id('id_nonautopkbook_set-0-title').send_keys('Test Book')\n\n        show_links[1].click()\n        self.wait_until_visible('#id_nonautopkbook_set-1-title')\n        self.selenium.find_element_by_id('id_nonautopkbook_set-1-title').send_keys('Test Book')\n\n        with self.wait_page_loaded():\n            self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n\n        # assert error message present\n        error_message = self.selenium.find_element_by_css_selector(\n            '.errorlist li:nth-child(1)'\n        ).text\n        self.assertEqual(\n            error_message,\n            'Please fix the following errors:\\n- The \"Test Book\" value is already in use.',\n        )\n", "    def test_inlines_formset_ordering(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder6_add'))\n        # Ensure the inlines are in the expected order.\n        inlines = self.selenium.find_elements_by_css_selector('.dynamic-inner6stacked_set')\n        self.assertEqual([inline.get_attribute('id') for inline in inlines],\n                         ['inner6stacked_set-0', 'inner6stacked_set-1', 'inner6stacked_set-2'])\n\n\n\n"], "sample_51": ["    def test_was_modified_since_unicode_header(self):\n        \"\"\"\n        was_modified_since handles unicode HTTP headers.\n        See #19464 for details.\n        \"\"\"\n        import unicodedata\n        header = unicodedata.normalize(\n            'NFKD', 'Mon, 18 Jan 2038 05:14:07 GMT'\n        ).encode('ascii', 'ignore')\n        self.assertFalse(was_modified_since(header, 1343416141))\n\n", "    def test_was_modified_since_content_length(self):\n        \"\"\"Content length header overrides If-Modified-Since if present.\"\"\"\n        mtime = 1343416141\n        header = http_date(mtime)\n        self.assertFalse(\n            was_modified_since(\n                header,\n                mtime,\n                size=1234,\n            )\n        )\n", "    def test_was_modified_since_empty_header(self):\n        self.assertTrue(was_modified_since(header=None, mtime=1))\n", "    def test_was_modified_since_empty_header(self):\n        self.assertTrue(was_modified_since(header=None, mtime=1))\n", "    def test_was_modified_since_None_Header(self):\n        self.assertTrue(was_modified_since(header=None, mtime=1))\n"], "sample_52": ["    def test_references_field_by_through_fields_reverse(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", through_fields=(\"second\", \"first\")\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"first\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"second\", \"migrations\"), True\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_to_models\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_to_models\", \"migrations\"),\n            True,\n        )\n\n\n", "    def test_references_field_by_through_back_fields(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", through_back_fields=(\"back_first\", \"back_second\")\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"back_first\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"back_second\", \"migrations\"), True\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\", models.CASCADE, related_name=\"related_to_model\"\n            ),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_to_model\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n\n\n\n", "    def test_references_field_by_reverse_relationship(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_models\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_models\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n\n"], "sample_14": ["    def test_serialize_custom_operation_classes(self):\n        class CustomOperation(migrations.MigrationOperation):\n                super().__init__()\n                self.value = value\n\n                return (\n                    'custom_migration_operations.operations.CustomOperation',\n                    ('value', self.value),\n                )\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                CustomOperation(42),\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\n            'CustomOperation(value=42)',\n            output\n        )\n", "    def test_serialization_with_custom_operations(self):\n        class CustomOperation(migrations.MigrationOperation):\n                super().__init__(name=name)\n                self.value = value\n\n                return (\n                    \"custom_migration_operations.operations.CustomOperation\",\n                    (self.value,),\n                    {}\n                )\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                CustomOperation(\"test_operation\", \"foobar\"),\n            ]\n        })\n        writer = MigrationWriter(migration)\n        buffer, imports = writer.serialize()\n        self.assertEqual(buffer, \"custom_migration_operations.operations.CustomOperation('foobar')\\n\")\n        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n", "    def test_serialize_custom_model_fields(self):\n        class MyCustomModelField(models.CharField):\n                super().__init__(*args, **kwargs)\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.AddField(\"MyModel\", \"myfield\", MyCustomModelField(max_length=255)),\n            ]\n        })\n        string = MigrationWriter.serialize(migration.operations[0])[0]\n        self.assertEqual(string, \"migrations.MyCustomModelField(max_length=255)\")\n", "    def test_serialize_custom_migrations_operations(self):\n        class CustomOperation(custom_migration_operations.Operation):\n                return (\n                    'custom_migrations_test.operations.CustomOperation',\n                    [],\n                    {},\n                    None,\n                )\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                CustomOperation(),\n            ],\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn('custom_migrations_test.operations.CustomOperation()', output)\n\n", "    def test_serialize_deconstructible_fields(self):\n        from django.db import models\n        class MyCharField(models.CharField):\n                return (\n                    \"migrations.test_writer.MyCharField\",\n                    (\n                        self.max_length,   \n                        self.null,\n                        self.blank,\n                        self.default,\n                        self.verbose_name,\n                        self.verbose_name_plural,\n                    ), \n                    self.attrs\n                )\n\n        class MyModel(models.Model):\n            my_field = MyCharField(max_length=10)\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"MyModel\", (\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"my_field\", MyCharField(max_length=10)),\n                ), {})\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"MyCharField(max_length=10)\", output)\n        self.assertIn(\"migrations.test_writer.MyCharField\", output)\n"], "sample_112": ["def test_isotonic_regression_sample_weight_zero_value():\n    # Test for the case where sample_weight has zero values, ensuring it's handled correctly\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([2, 4, 1, 3, 5])\n\n    # Create sample weights with a zero value\n    sample_weight = np.array([1, 0, 1, 1, 1])\n\n    # Fit the IsotonicRegression model with the sample weights\n    ir = IsotonicRegression()\n    ir.fit(X, y, sample_weight=sample_weight)\n\n    # Check if the model behaves as expected with the zero-weighted data point\n    # by ensuring the prediction for the zero-weighted data point is not considered\n    # while fitting the model.\n    assert ir.predict(X)[1] != y[1] \n", "def test_isotonic_regression_invalid_parameters():\n    with pytest.raises(ValueError, match=\"y_min must be less than or equal to y_max\"):\n        IsotonicRegression(y_min=5, y_max=1)\n\n    with pytest.raises(ValueError, match=\"`out_of_bounds` must be one of\"):\n        IsotonicRegression(out_of_bounds=\"invalid\")\n\n\n\n", "def test_isotonic_regression_sample_weight_nan():\n    # https://github.com/scikit-learn/scikit-learn/issues/30331\n\n    # Check that NaN values in sample_weight are properly handled\n    rng = np.random.RandomState(42)\n    n_samples = 10\n    X = rng.rand(n_samples)\n    y = rng.rand(n_samples)\n    sample_weight = np.ones(n_samples)\n    sample_weight[3] = np.nan\n\n    # Fit the model\n    iso = IsotonicRegression()\n    iso.fit(X, y, sample_weight=sample_weight)\n\n    # Check if NaN is propagated to the output\n    pred = iso.predict(X)\n    assert np.isnan(pred[3])\n\n\n\n", "def test_isotonic_regression_increasing_decreasing_extremes():\n    # Test behavior when y_min and y_max are set to extremes\n    # Regression test for: https://github.com/scikit-learn/scikit-learn/issues/8854\n\n    for increasing in [True, False]:\n        X = np.array([0, 1, 2, 3, 4])\n        y = np.array([0, 1, 2, 3, 4])\n        \n        # Test with y_min and y_max set to min and max of y\n        reg = IsotonicRegression(increasing=increasing)\n        reg.fit(X, y)\n        assert_allclose(reg.predict(X), y)\n\n        if increasing:\n            y_min = np.min(y) - 1\n            y_max = np.max(y) + 1\n        else:\n            y_min = np.max(y) + 1\n            y_max = np.min(y) - 1\n\n        reg = IsotonicRegression(increasing=increasing, y_min=y_min, y_max=y_max)\n        reg.fit(X, y)\n        y_pred = reg.predict(X)\n        if increasing:\n            assert_allclose(y_pred, y)\n        else:\n            assert_allclose(y_pred, y[::-1])\n\n", "def test_isotonic_regression_with_y_min_and_y_max():\n    # Test both min and max values are respected\n    X = np.array([0, 1, 2, 3, 4])\n    y = np.array([0, 1, 2, 3, 4])\n    low = 0.2\n    high = 3.8\n\n    ir = IsotonicRegression(y_min=low, y_max=high)\n    ir.fit(X, y)\n\n    y_pred = ir.predict(X)\n    for i in range(len(y)):\n        assert low <= y_pred[i] <= high\n"], "sample_7": ["    def test_notify_file_created(self):\n        new_file = self.ensure_file(self.tempdir / 'new_file.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[new_file] ) as mock_watched_files:\n            self.reloader._notify_file_changed(new_file)\n            mock_watched_files.assert_called_once()\n\n\n\n", "    def test_stat_reloader_handles_modification_events_correctly(self):\n        with mock.patch.object(os, 'stat', side_effect=[mock.sentinel.stat1, mock.sentinel.stat2]) as mocked_stat:\n            self.reloader.watch_file(self.existing_file)\n            self.reloader._handle_modification_event(self.existing_file)\n            mocked_stat.assert_called_once_with(str(self.existing_file))\n\n            self.increment_mtime(self.existing_file)\n            self.reloader._handle_modification_event(self.existing_file)\n            mocked_stat.assert_called_with(str(self.existing_file))\n\n        self.assertEqual(self.reloader.last_mtime, mock.sentinel.stat2.st_mtime)\n", "    def test_file_modified_twice_in_quick_succession(self):\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n            self.increment_mtime(self.existing_file)\n        self.assertEqual(self.reloader._reload_times[self.existing_file], 1)\n", "    def test_on_change(self):\n        with mock.patch.object(self.reloader, '_get_modified_files', return_value=[self.existing_file]):\n            self.reloader._on_change()\n            self.assertTrue(self.reloader.should_restart)\n\n\n\n", "    def test_stat_reloader_handles_changing_files(self):\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n            self.reloader.run_loop()\n        self.assertEqual(self.reloader.should_notify, True)\n"], "sample_66": ["    def test_actions_with_permission(self):\n        @admin.action(permissions=[\"some_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = [my_action]\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"BandAdmin must define a has_some_permission() method for \"\n            \"the my_action action.\",\n            id=\"admin.E129\",\n        ) \n", "    def test_actions_function_type(self):\n        class BandAdmin(ModelAdmin):\n            actions = (\"not_a_function\",)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions[0]' must be a callable.\",\n            id=\"admin.E131\",\n        )\n\n\n\n", "    def test_actions_require_custom_permission_method(self):\n        @admin.action(permissions=(\"my_permission\",))\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (my_action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"BandAdmin must define a function named 'has_{}' to \"\n            \"check if user has permission 'my_permission' for the action \"\n            \"'my_action'.\",\n            id=\"admin.E131\",\n            ),\n            expected_method_name=\"has_my_permission\",\n            invalid_obj=BandAdmin,\n", "    def test_actions_with_optional_permissions(self):\n        @admin.action(permissions=[])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action_without_permissions,)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_with_invalid_decorator(self):\n        @non_admin_decorator\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions[0]' must be a callable decorated with \"\n            \"'@admin.action'.\",\n            id=\"admin.E131\",\n        )\n"], "sample_16": ["    def test_quote_special_characters(self):\n        self.assertEqual(quote(\"something\\\"or'other\"), \"something_25or_27other\")\n\n\n", "    def test_quote_with_special_chars(self):\n        self.assertEqual(quote('something <>&\" \\' \\\"'), 'something_0A_0A_0A_0A')\n", "    def test_quote_for_different_characters(self):\n        self.assertEqual(quote('Some\\'thing\" \"with\" & < >'), \"Some_27thing_22_20with_26_20_3C_20_3E\") \n        self.assertEqual(quote('Special: /\\\\?#[]{}|'), 'Special_3A_2F_5C_3F_23_5B_5D_7B_7D_7C')\n", "    def test_display_for_field_null_list_choice(self):\n        \"\"\"\n        Regression test for #14574: display_for_field should handle\n        None values for field choices.\n        \"\"\"\n        with self.subTest(\n            field=models.CharField(choices=(\n                (None, '-----'),\n            ))\n        ):\n            self.assertEqual(display_for_field(None, models.CharField(choices=(\n                (None, '-----'),\n            )), self.empty_value), '-----')\n\n\n\n", "    def test_quote_with_single_quote(self):\n        self.assertEqual(quote('something\\'or\\nother'), 'something_27or_0Aother')\n\n"], "sample_63": ["    def test_password_validation_errors(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\n            \"password1\": \"short\",\n            \"password2\": \"short\",\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            \"This password is too short. It must contain at least 8 characters.\",\n            form.errors[\"password1\"],\n        )\n        self.assertIn(\n            \"This password is too short. It must contain at least 8 characters.\",\n            form.errors[\"password2\"],\n        )\n        self.assertEqual(form.changed_data, [])\n", "    def test_password_too_short(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"123\", \"password2\": \"123\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            \"This password is too short. It must contain at least 12 characters.\",\n            form.errors[\"password1\"],\n        )\n        self.assertEqual(form.changed_data, [\"password\"])\n", "    def test_password_reset_sent(self):\n        user = User.objects.get(username=\"testclient\")\n        form = AdminPasswordChangeForm(user, {\"password1\": \"newpassword\", \"password2\": \"newpassword\"})\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, [user.email])\n        self.assertIn(user.email, mail.outbox[0].message().get_payload())\n        self.assertIn(\"newpassword\", mail.outbox[0].message().get_payload())\n\n", "    def test_invalid_password_reason(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"password\", \"password2\": \"password\"}\n        form = AdminPasswordChangeForm(user, data)\n        form.is_valid()\n        with self.assertRaises(Field.ValidationError):\n            form.save()\n        expected_error = form.errors[\"password1\"]\n        self.assertIn(\n            \"Password does not meet requirements.\", expected_error\n        )\n\n\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"abc\", \"password2\": \"abc\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [\n                \"This password is too short. It must contain at least 12 characters.\"\n            ],\n        )\n        self.assertEqual(form.errors[\"password2\"], form.errors[\"password1\"])\n        self.assertEqual(form.changed_data, [])\n"], "sample_69": ["compilation error", "compilation error", "compilation error", "def test_set_alpha_for_array_negative():\n    art = martist.Artist()\n    with pytest.raises(TypeError, match='^alpha must be numeric or None'):\n        art._set_alpha_for_array('string')\n    with pytest.raises(ValueError, match=\"outside 0-1 range\"):\n        art._set_alpha_for_array([1.1])\n    with pytest.raises(ValueError, match=\"outside 0-1 range\"):\n        art._set_alpha_for_array([np.nan])\n    with pytest.raises(ValueError, match=\"alpha must be between 0 and 1\"):\n        art._set_alpha_for_array([0.5, -0.1])\n    with pytest.raises(ValueError, match=\"alpha must be between 0 and 1\"):\n        art._set_alpha_for_array([0.5, np.inf])\n\n", "compilation error"], "sample_46": ["    def test_index_names_with_suffix(self):\n        table = Person._meta.db_table\n        self.expressions.rename_table_references(table, 'other')\n        suffix = '_modified'\n        index_name = IndexName('other', ['first_name', 'last_name'], suffix, lambda table, columns, s: ', '.join(\"%s_%s_%s\" % (table, column, s) for column in columns))\n        self.expressions = Expressions(\n            table=table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n                IndexExpression(Upper('last_name')),\n            ).resolve_expression(self.expressions.compiler.query),\n            compiler=self.expressions.compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertEqual(str(index_name), 'other_first_name_modified, other_last_name_modified')\n", "    def test_renaming_nested_references(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.editor = connection.schema_editor()\n        expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()).subquery(),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n\n        old_column = 'first_name'\n        new_column = 'other'\n\n        expressions.rename_column_references(Person._meta.db_table, old_column, new_column)\n        self.assertIs(expressions.references_column(Person._meta.db_table, old_column), False)\n        self.assertIs(expressions.references_column(Person._meta.db_table, new_column), True)\n        self.assertIn(\n            '%s.%s' % (\n                self.editor.quote_name(Person._meta.db_table),\n                self.editor.quote_name(new_column),\n            ),\n            str(expressions),\n        )\n\n        \n", "    def test_rename_column_references_with_expressions(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        table = Person._meta.db_table\n        expressions = Expressions(\n            table=table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name') + F('last_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        expressions.rename_column_references(table, 'first_name', 'other')\n        self.assertIs(expressions.references_column(table, 'first_name'), False)\n        self.assertIs(expressions.references_column(table, 'other'), True)\n        self.assertIn(\n            '%s.%s' % (self.editor.quote_name(table), self.editor.quote_name('other')),\n            str(expressions),\n        )\n", "    def test_rename_column_references_with_alias(self):\n        compiler = Query(Person, alias_cols=True).get_compiler(connection=connection)\n        table = Person._meta.db_table\n        expressions = Expressions(\n            table=table,\n            expressions=ExpressionList(\n                IndexExpression(Upper('last_name')),\n                IndexExpression(F('first_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        expressions.rename_column_references(table, 'last_name', 'other_last_name')\n        self.assertIs(expressions.references_column(table, 'other_last_name'), True)\n        self.assertIs(expressions.references_column(table, 'last_name'), False)\n        expected_str = '(UPPER(%s.other_last_name)), %s.first_name' % (\n            self.editor.quote_name(table),\n            self.editor.quote_name(table),\n        )\n        self.assertEqual(str(expressions), expected_str)\n", "    def test_rename_column_references_multiple(self):\n        table = Person._meta.db_table\n        self.expressions.rename_column_references(table, ('first_name', 'last_name'), ('other_first_name', 'other_last_name'))\n        self.assertIs(self.expressions.references_column(table, 'other_first_name'), True)\n        self.assertIs(self.expressions.references_column(table, 'other_last_name'), True)\n        self.assertIs(self.expressions.references_column(table, 'first_name'), False)\n        self.assertIs(self.expressions.references_column(table, 'last_name'), False)\n        self.assertIn(\n            '%s.%s' % (self.editor.quote_name(table), self.editor.quote_name('other_first_name')),\n            str(self.expressions),\n        )\n        self.assertIn(\n            '%s.%s' % (self.editor.quote_name(table), self.editor.quote_name('other_last_name')),\n            str(self.expressions),\n        )\n"], "sample_35": ["    def test_modelchoicefield_empty_dataset(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.none(), error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '4')\n", "    def test_modelchoicefield_with_empty_label(self):\n        class CustomChoiceModel(ChoiceModel):\n            pass\n\n        CustomChoiceModel.objects.create(pk=1, name='a')\n\n        # ModelChoiceField\n        f = ModelChoiceField(\n            queryset=CustomChoiceModel.objects.all(),\n            empty_label='Select an option',\n            error_messages={\n                'required': 'REQUIRED',\n                'invalid_choice': 'INVALID CHOICE',\n            },\n        )\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '4')\n", "    def test_modelchoicefield_invalid_pk_value(self):\n        e = {\n            'invalid_pk_value': '\"%(pk)s\" is not a valid value.',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['\"invalidpk\" is not a valid value.'], f.clean, 'invalidpk')\n\n\n\n", "    def test_modelchoicefield_empty_choice(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, None) \n\n\n\n", "    def test_modelchoicefield_with_empty_label(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(\n            queryset=ChoiceModel.objects.all(),\n            empty_label=\"---------\",\n            error_messages=e,\n        )\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '4')\n\n"], "sample_136": ["def test_block_inverse_1x1():\n    A = MatrixSymbol('A', n, n)\n    B = BlockMatrix([[A]])\n    assert blockinverse_1x1(Inverse(B)) == B\n    assert blockinverse_1x1(B).blocks == A.I", "def test_block_identity_addition():\n    A = MatrixSymbol('A', n, n)\n    B = BlockMatrix([[A, ZeroMatrix(n, n)]])\n    assert bc_block_plus_ident(B + Identity(2*n)) == \\\n        BlockDiagMatrix(A + Identity(n), Identity(n))\n\n", "def test_block_collapse_nested():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    X = BlockMatrix([[A, B], [C, BlockMatrix([[A, B]])]])\n    assert block_collapse(X).blocks.shape == (2, 2)\n    assert block_collapse(X).blocks[0, 0] == A\n    assert block_collapse(X).blocks[0, 1] == B\n    assert block_collapse(X).blocks[1, 0] == C\n    assert block_collapse(X).blocks[1, 1] == BlockMatrix([[A, B]])\n\n\n", "def test_block_transpose_identity():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n    assert block_collapse(transpose(X + Identity(n+m))) == \\\n        transpose(block_collapse(X + Identity(n+m)))\n    assert block_collapse(transpose(X * Identity(n))) == \\\n        transpose(block_collapse(X * Identity(n)))\n\n\n\n", "def test_block_collapse_with_symbolic_matrices():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = BlockMatrix([[A, 2*B], [C, D]])\n\n    assert block_collapse(X + Y) == BlockMatrix([[2*A, 3*B], [C, 2*D]])\n"], "sample_75": ["def test_grid_axes_label_positioning(direction):\n    \"\"\"Test positioning of axes labels in Grid.\"\"\"\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), direction=direction)\n    for ax in np.ravel(grid.axes_row):\n        ax.set_xlabel(\"X\")\n        ax.set_ylabel(\"Y\")\n    fig.canvas.draw()\n    # Test nx.\n    if direction == \"row\":\n        assert grid[0].get_xlabel_position()[0] == 0.5\n        assert grid[1].get_xlabel_position()[0] == 0.5\n    else:\n        assert grid[0].get_ylabel_position()[1] == 0.5\n        assert grid[1].get_ylabel_position()[1] == 0.5\n    # Test ny.\n    if direction == \"row\":\n        assert grid[0].get_ylabel_position()[1] == 0.5\n        assert grid[2].get_ylabel_position()[1] == 0.5\n    else:\n        assert grid[0].get_xlabel_position()[0] == 0.5\n        assert grid[2].get_xlabel_position()[0] == 0.5\n\n\n\n", "def test_grid_axes_class_custom():\n    class CustomAxes(mpl.axes.Axes):\n            super().__init__(*args, **kwargs)\n            self.set_aspect('equal')\n\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), axes_class=CustomAxes)\n    for ax in grid:\n        ax.plot([0, 1], [0, 1])\n", "def test_grid_with_twinx_and_twiny():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (1, 2), direction=\"row\")\n    ax1, ax2 = grid\n\n    ax1.plot([1, 2, 3], [1, 4, 9])\n    ax2 = ax1.twinx()\n    ax2.plot([1, 2, 3], [2, 8, 27], color=\"red\")\n\n\n    ax3 = ax1.twiny()\n    ax3.plot([1, 2, 3], [3, 6, 9], color=\"green\")\n", "def test_imagegrid_cbar_mode_single():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(18, 9))\n\n    grid = ImageGrid(fig, 111, nrows_ncols=(2, 2), cbar_mode='single',\n                     cbar_location='right', cbar_size='20%',\n                     axes_pad=0.1)\n\n    ax1, ax2, ax3, ax4 = grid.axes\n\n    ax1.imshow(arr, cmap='nipy_spectral')\n    ax2.imshow(arr.T, cmap='hot')\n    ax3.imshow(np.hypot(arr, arr.T), cmap='jet')\n    ax4.imshow(np.arctan2(arr, arr.T), cmap='hsv')\n\n\n", "def test_grid_with_auto_adjust_pad():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), direction=\"row\", auto_adjust_pad=True)\n\n    # Check that the padding is adjusted automatically.\n    assert isinstance(grid.axes[0].get_pad(), (tuple, list))\n\n    # Manually set the padding and check that it is respected.\n    grid.axes[0].set_pad(0.1)\n    assert grid.axes[0].get_pad() == (0.1, 0.1, 0.1, 0.1)\n\n    # Check that the automatic adjustment doesn't modify the padding.\n    grid.auto_adjust_pad = False\n    assert grid.axes[0].get_pad() == (0.1, 0.1, 0.1, 0.1)\n"], "sample_19": ["    def test_sensitive_variables_decorator_with_callable_func(self):\n        @sensitive_variables\n            return data['password']\n\n        with self.assertRaises(ValueError):\n            get_sensitive_data({'password': 'secret'})\n", "    def test_sensitive_variables_decorator_with_existing_settings(self):\n        @sensitive_variables\n            return 'super_secret'\n\n        with self.settings(DEBUG=True, SECRET_KEY='some_secret'):\n            response = self.client.post('/some_url/', {'password': 'test'})\n            self.assertNotContains(response, 'test', status_code=500)\n\n\n\n", "    def test_sensitive_variables_decorator_with_custom_substitute(self):\n        @sensitive_variables(cleansed_substitute=\"***REDACTED***\")\n            return password\n\n        with self.assertRaises(ValueError):\n            test_func(\"MY_PASSWORD\")\n", "    def test_sensitive_variables_decorator_with_args(self):\n        @sensitive_variables\n            return sensitive_string\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/sensitive_view/?sensitive_string=secret&not_sensitive_string=public')\n            self.assertEqual(response.status_code, 500)\n            self.assertNotIn('secret', response.content.decode())\n", "    def test_sensitive_variables_decorates_functions(self):\n        @sensitive_variables\n            return {'secret': secret_variable, 'data': sensitive_data}\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/my_function/',\n                                      {'secret_variable': 'example_secret',\n                                       'sensitive_data': 'top_secret'})\n            self.assertIn('secret_variable', response.json())\n            self.assertNotIn('sensitive_data', response.json())\n"], "sample_139": ["def test_limit_as_real_approaches_imaginary():\n    from sympy import limit, Symbol, I, oo\n\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test approaching a complex number with a real argument\n    assert limit(Abs(x + I*y), x, oo) == oo\n\n    assert limit(Abs(x + I*y), y, oo) == oo\n    \n    # Test approaching a complex number with a real argument\n    assert limit(Abs(x + I*y), x, -oo) == oo\n\n    assert limit(Abs(x + I*y), y, -oo) == oo    \n\n    # Special case where real and imaginary parts tend to infinity\n    assert limit(Abs(x + I*y), (x, y), (oo, oo)) == oo \n\n\n", "def test_issue_16882():\n    from sympy import Abs, symbols, re, im\n    x, y = symbols('x y', complex=True)\n    assert Abs(x*y).simplify() == Abs(x)*Abs(y)\n    assert re(Abs(x*y)).simplify() == re(Abs(x))*re(Abs(y))\n    assert im(Abs(x*y)).simplify() == im(Abs(x))*im(Abs(y))\n\n", "def test_issue_16091():\n    from sympy import (im, re, conjugate, Abs, Symbol,\n                       polar_lift, exp, pi)\n    x = Symbol('x')\n    y = Symbol('y', imaginary=True)\n    assert im(conjugate(x + I*y)) == -re(y)\n    assert re(conjugate(x + I*y)) == x\n    assert Abs(conjugate(x + I*y)) == Abs(x + I*y)\n    assert polar_lift(conjugate(exp(I*x))) == exp(-I*x)\n    assert conjugate(polar_lift(exp(I*x))) == exp(-I*x)\n    assert conjugate(exp(I*x * polar_lift(pi/2))) == exp(-I * x * polar_lift(pi/2))\n\n\n", "def test_abs_zero():\n    x = Symbol('x')\n    assert Abs(0) == 0\n    assert Abs(-0) == 0\n\n\n\n", "def test_issue_17271():\n    from sympy.functions import sign\n    x = Symbol('x')\n    assert sign(x**2) == 1\n    assert sign(x**2 + 1) == 1 \n\n\n"], "sample_141": ["def test_issue_16277():\n    from sympy.physics.units.dimensions import (\n        Dimension, energy, length, time, mass, temperature\n    )\n    u = Quantity(\"u\")\n    u.set_dimensional_expr(energy * length / time)\n    assert u.dimensional_expr == energy * length / time\n    assert Dimension(u) == energy * length / time\n    assert SI.get_dimensional_expr(u) == energy * length / time\n    u = Quantity(\"u\", dimension=energy * length / time)\n    assert u.dimensional_expr == energy * length / time\n    assert Dimension(u) == energy * length / time\n\n", "def test_issue_17768():\n    from sympy.physics.units import Quantity\n    from sympy import Symbol\n\n    x = Symbol(\"x\")\n\n    q1 = Quantity(\"q1\")\n    q2 = Quantity(\"q2\")\n    q3 = Quantity(\"q3\")\n\n    q1.set_global_relative_scale_factor(1, meter)\n    q2.set_global_relative_scale_factor(2, meter)\n    q3.set_global_relative_scale_factor(3, meter**2)\n\n    expr1 = (q1 + q2)**x\n    expr2 = (q1 * q2)**x\n    expr3 = (q1 / q2)**x\n    expr4 = q1**(x * q2)\n    expr5 = q1**(q2 * x)\n\n    assert expr1.args == (q1 + Symbol(\"q2\"),) and expr1.func == \"**\" and expr1.as_coefficients_dict() == {q1: 1, q2: 1}\n    assert expr2.args == (q1 * q2,) and expr2.func == \"**\" and expr2.as_coefficients_dict() == {q1: 1, q2: 1}\n    assert expr3.args == (q1 / q2,) and expr3.func == \"**\" and expr3.as_coefficients_dict() == {q1: 1, q2: -1}\n    assert expr4.args == (q1,) and expr4.func == \"**\" and expr4.as_coefficients_dict() == {q1: 1}\n    assert expr5.args == (q1,) and expr5.func == \"**\" and expr5.as_coefficients_dict() == {q1: 1}\n\n", "def test_issue_18447():\n    from sympy.physics.units import centimeter, meter\n    q = Quantity('q')\n    q.set_global_relative_scale_factor(1, centimeter)\n    assert q.convert_to(meter) == 0.01*meter\n\n    # Test the case when a quantity is multiplied by a number\n    assert (5*q).convert_to(meter) == 0.05*meter\n\n\n\n", "def test_issue_16328():\n    a = Quantity(\"a\", 2*meter)\n    b = Quantity(\"b\", 3*meter)\n    assert a.is_Quantity is True\n    assert b.is_Quantity is True\n    assert a.has_units is True\n    assert b.has_units is True\n    assert a.unit is meter\n    assert b.unit is meter \n    assert a.scale_factor == 2\n    assert b.scale_factor == 3\n    assert (a + b).unit is meter\n    assert (a * b).unit is meter**2\n    assert (a / b).unit is meter**-1\n    \n    assert (a + b).scale_factor == 5\n    assert (a * b).scale_factor == 6*meter**2\n    assert (a / b).scale_factor == 2/3\n", "def test_issue_16020():\n    from sympy.physics.units import Quantity, find_unit\n    u1 = Quantity(\"u1\")\n    u2 = Quantity(\"u2\")\n    u1.set_global_relative_scale_factor(1, meter)\n    u2.set_global_relative_scale_factor(1, meter/second)\n    assert find_unit(u1+u2) == ['m', 'meter', 'meters']\n"], "sample_109": ["def test_group_kfold_error_check():\n    with pytest.raises(ValueError, match=r\"groups must have at least one element\"):\n        GroupKFold(n_splits=3).split([], [])\n\n\n\n", "def test_RepeatedStratifiedKFold_random_state():\n    rs = np.random.RandomState(42)\n    skf = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=rs)\n    for train, test in skf.split(X, y):\n        assert train.shape[0] == len(y) // 3\n        assert len(np.unique(y[train])) == len(np.unique(y[:len(train)]))\n\n", "def test_time_series_split_with_empty_dataset():\n    with pytest.raises(ValueError, match='empty dataset'):\n        TimeSeriesSplit(n_splits=2).split([])\n", "compilation error", "def test_check_cv_classifier_errors():\n    with pytest.raises(ValueError, match=r'classifier=.*must be bool'):\n        check_cv(3, classifier='string')\n\n    with pytest.raises(ValueError, match=r'classifier=.*must be bool'):\n        check_cv(3, classifier=3.14)\n\n    with pytest.raises(ValueError, match=r'classifier=.*must be bool'):\n        check_cv(3, classifier=[True, False])\n\n\n\n"], "sample_108": ["compilation error", "def test_kernel_callable_with_custom_data_structure():\n    # Regression test for #10095\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 0, 0])\n\n    class CustomData:\n            self.data = data\n\n            return np.array(self.data)\n\n    X_custom = CustomData(X)\n    clf = svm.SVC(kernel=lambda x, y: np.dot(x, y.T))\n    clf.fit(X_custom, y)\n\n\n", "def test_oneclass_svr_fit_intercept():\n    # One-class SVM should always fit intercept.\n    X = np.array([[0.], [0.44], [0.45], [0.46], [1.]])\n    clf = svm.OneClassSVM(fit_intercept=True)\n    clf.fit(X)\n    assert clf.fit_intercept\n\n\n\n", "def test_svm_coef_linear():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n\n    clf = svm.SVC(kernel='linear')\n    clf.fit(X, y)\n\n    assert clf.coef_.shape == (2, 3)\n\n    # check if the coefficients are meaningful\n    assert np.allclose(clf.coef_[0], np.array([1.0, -1.0]))\n    assert np.allclose(clf.coef_[1], np.array([-1.0, 1.0]))\n", "def test_linear_svc_dual_and_penalty():\n    # Test that dual=False for linear SVC works as expected\n    for penalty in ['l1', 'l2']:\n        lsvc = svm.LinearSVC(penalty=penalty, dual=False, random_state=0)\n        lsvc.fit(X, Y)\n        assert lsvc.dual_coef_.size == 0\n    \n        # check that weights are correctly set\n        assert_array_almost_equal(lsvc.coef_[0], lsvc.coef_)\n        assert_array_almost_equal(lsvc.intercept_[0], lsvc.intercept_) \n\n    # Check that penalty='l1', dual=False leads to l1 regularization\n\n\n"], "sample_116": ["def test_create_index_with_subentry(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Sphinx; documentation tool\\n\"\n            \".. index:: pair: Sphinx; :+1:\\n\"\n            \".. index:: pair: Sphinx; \u0415\u043b\u044c\\n\"\n            \".. index:: pair: Sphinx; \u0451\u043b\u043a\u0430\\n\"\n            \".. index:: pair: docutils; subentry\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 8\n", "def test_create_index_with_subentries(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Sphinx; :+1:\\n\"\n            \".. index:: pair: Sphinx; documentation tool\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 5\n    assert index[0] == ('D',\n                        [('documentation tool', [[], [('Sphinx', [('', '#index-3')])], None]),\n                         ('docutils', [[], [('reStructuredText', [('', '#index-0')])], None])])\n    assert index[1] == ('I', [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[2] == ('P', [('Python', [[], [('interpreter', [('', '#index-1')])], None])])\n    assert index[3] == ('R', [('reStructuredText', [[], [('docutils', [('', '#index-0')])], None])])\n    assert index[4] == ('S', [('Sphinx', [[],\n                                     [(':+1:', [('', '#index-3')]),\n                                      ('documentation tool', [('', '#index-2')])],\n                                     None])])\n", "def test_index_entry_sorting(app):\n    text = (\".. index:: pair: foo; bar\\n\"\n            \".. index:: pair: bar; foo\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 1\n    assert index[0] == ('B', [('bar', [[], [('foo', [('', '#index-0')])], None])])\n", "def test_create_index_with_subentries(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: docutils; reStructuredText\\n\"\n            '.. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: Sphinx; documentation tool\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 5\n    assert index[0] == ('D', [('docutils', [[], [('reStructuredText', [('', '#index-0')])], None])])\n    assert index[1] == ('D', [('docutils', [[], [('reStructuredText', [('', '#index-0')])], None])])\n    assert index[2] == ('I', [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[3] == ('I', [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[4] == ('S', [('Sphinx', [[], [('documentation tool', [('', '#index-2')])], None])])\n\n\n\n", "def test_create_index_with_subitems(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Sphinx; documentation tool\\n\"\n            \".. index:: pair: Sphinx; :+1:\\n\"\n            \".. index:: single: foo\\n\"\n            \"   :subentry: bar\\n\"\n            \".. index:: single: Python; sub-entry: hello\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 6\n    assert index[0] == ('D',\n                        [('documentation tool', [[], [('Sphinx', [('', '#index-2')])], None]),\n                         ('docutils', [[], [('reStructuredText', [('', '#index-0')])], None])])\n    assert index[1] == ('I', [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[2] == ('P', [('Python', [[], [('interpreter', [('', '#index-1')])], None])])\n    assert index[3] == ('S', [('Sphinx', [[],\n                                     [(':+1:', [('', '#index-3')]),\n                                      ('documentation tool', [('', '#index-2')])],\n                                     None])])\n    assert index[4] == ('F', [('foo', [[('bar', [('', '#index-4')])], [], None])])\n    assert index[5] == ('P', [('Python', [[('hello', [('', '#index-5')])], [], None])])\n"], "sample_24": ["    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, exception.update_error_dict(error_dict))\n\n        error_dict = {'field1': []}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, exception.update_error_dict(error_dict))\n\n        error_dict = {'field1': ['E1']}\n        exception = ValidationError({'field2': ['E2']})\n        self.assertEqual(\n            {'field1': ['E1'], 'field2': ['E2']},\n            exception.update_error_dict(error_dict),\n        )\n\n        exception = ValidationError({'field1': ['E1', 'E2']})\n        self.assertEqual(\n            {'field1': ['E1', 'E2']},\n            exception.update_error_dict(error_dict),\n            \"Error: update_error_dict should not modify the existing error dict\"\n        )\n\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n\n        error_dict = {'field1': ['E1', 'E2']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n        error_dict = {'field2': ['E3', 'E4']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        error_dict = {\n            'field3': ['E5', 'E6'],\n            'field4': ValidationError('error %(parm1)s %(parm2)s',\n                                       code='my_code',\n                                       params={'parm1': 'val1', 'parm2': 'val2'}),\n        }\n        exception = ValidationError({'field1': ['E1', 'E2']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {\n            'field3': ['E5', 'E6'],\n            'field4': ValidationError('error %(parm1)s %(parm2)s',\n                                       code='my_code',\n                                       params={'parm1': 'val1', 'parm2': 'val2'})\n        })\n\n        self.assertEqual(exception.error_dict, {\n            'field1': ['E1', 'E2'],\n            'field3': ['E5', 'E6'],\n            'field4': ValidationError('error %(parm1)s %(parm2)s',\n                                       code='my_code',\n                                       params={'parm1': 'val1', 'parm2': 'val2'})\n        })\n\n        error_dict = {}\n        exception = ValidationError({'field1': ['E1', 'E2']})\n        exception.update_", "    def test_update_error_dict(self):\n        error_dict = {}\n        error1 = ValidationError('error1')\n        error2 = ValidationError('error2')\n        error3 = ValidationError({'field1': 'error3'})\n        error1.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'message': [error1]})\n        error2.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'message': [error1, error2]})\n        error3.update_error_dict(error_dict)\n        self.assertEqual(\n            error_dict,\n            {'message': [error1, error2], 'field1': [error3['field1']]}\n        )\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n\n        error_dict = {'field1': 'E1', 'field2': 'E2'}\n        exception = ValidationError(error_dict)\n        self.assertEqual(exception.update_error_dict(error_dict), error_dict)\n\n        error_dict = {'field1': 'E1', 'field2': 'E2'}\n        exception = ValidationError(error_dict)\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1'], 'field2': ['E2']})\n\n        error_dict = {'field1': 'E1', 'field2': 'E2'}\n        exception = ValidationError(error_dict)\n        exception.update_error_dict({'field3': 'E3'})\n        self.assertEqual(error_dict, {'field1': ['E1'], 'field2': ['E2'], 'field3': ['E3']})\n\n        error_dict = {'field1': 'E1', 'field2': 'E2'}\n        exception = ValidationError(error_dict)\n        exception.update_error_dict({'field1': ['E3'], 'field4': 'E4'})\n        self.assertEqual(error_dict, {'field1': ['E1', 'E3'], 'field2': ['E2'], 'field4': ['E4']})\n\n        error_dict = {\n            'field1': ValidationError({'field1': 'error'}),\n            'field2': 'E2',\n        }\n        exception = ValidationError(error_dict)\n        self.assertEqual(exception.update_error_dict(error_dict), error_dict)\n\n        \n\n\n\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {})\n        error_dict = {'field1': ['E1', 'E2']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n        error_dict = {'field2': ['E3', 'E4']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {'field2': ['E3', 'E4']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        error_dict = {'field3': ['E5', 'E6']}\n        exception = ValidationError({'field1': ['E7', 'E8']})\n        self.assertEqual(error_dict, {'field3': ['E5', 'E6']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E7', 'E8'], 'field3': ['E5', 'E6']})\n\n        exception = ValidationError(NON_FIELD_ERRORS)\n        error_dict = {'field1': ['E1', 'E2']}\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'__all__': ['E1', 'E2']})\n\n        error_dict = {'__all__': ['E1', 'E2']}\n        exception = ValidationError(error_dict)\n        error_dict = {'field1': ['E3', 'E4']}\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'__all__': ['E1', 'E2"], "sample_93": ["def test_make_numbered_dir_on_existing_dir(tmp_path):\n    d = tmp_path / \"test\"\n    d.mkdir()\n    with pytest.raises(FileExistsError):\n        make_numbered_dir(root=d, prefix=TestNumberedDir.PREFIX)\n", "compilation error", "compilation error", "compilation error", "    def test_tmp_path_factory_handles_invalid_dir_characters(\n        tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch"], "sample_156": ["def test_parser_mathematica_complex_expressions():\n    parser = MathematicaParser()\n\n    full_form1 = \"Simplify[(x + y)*(x - y)]\"\n    full_form2 = \"Expand[(x + y)^2]\"\n    full_form3 = \"Log[Sin[x], 2]\"\n    full_form4 = \"Limit[x^2 / x, x -> 0]\"\n    full_form5 = \"Derivative[1][x^2]\"\n    full_form6 = \"N[Pi, 10]\"\n    full_form7 = \"Sum[i, {i, 1, 10}]\"\n\n    convert_chain2 = lambda expr: parser._from_fullformlist_to_fullformsympy(parser._from_fullform_to_fullformlist(expr))\n    convert_chain3 = lambda expr: parser._from_fullformsympy_to_sympy(convert_chain2(expr))\n\n    assert parser._from_fullform_to_fullformlist(full_form1) == [\"Simplify\", [\"Times\", [\"Plus\", \"x\", \"y\"], [\"Minus\", \"x\", \"y\"]]]\n    assert parser._from_fullform_to_fullformlist(full_form2) == [\"Expand\", [\"Power\", [\"Plus\", \"x\", \"y\"], \"2\"]]\n    assert parser._from_fullform_to_fullformlist(full_form3) == [\"Log\", \"Sin[x]\", \"2\"]\n    assert parser._from_fullform_to_fullformlist(full_form4) == [\"Limit\", \"x**2/x\", \"x\", \"0\"]\n    assert parser._from_fullform_to_fullformlist(full_form5) == [\"Derivative\", 1, \"x**2\"]\n    assert parser._from_fullform_to_fullformlist(full_form6) == [\"N\", \"Pi\", \"10\"]\n    assert parser._from_fullform_to_fullformlist(full_form7) == [\"Sum\", \"i\", \"i\", \"1\", \"10\"]\n\n    assert convert_chain2(full_form1) == simplify((x + y)*(x - y))\n    assert convert_", "compilation error", "compilation error", "def test_parser_mathematica_special_cases():\n    parser = MathematicaParser()\n\n    # Test cases for special symbols and functions\n    test_cases = [\n        (\"Pi\", \"Pi\"),\n        (\"e\", \"E\"),\n        (\"I\", \"I\"),\n        (\"Infinity\", \"oo\"),\n        (\"ComplexExpand[x^2]\", \"x**2\"),\n        (\"Simplify[x^2+y^2]\", \"x**2 + y**2\"),\n        (\"Expand[ (x + y)^2 ]\", \"x**2 + 2*x*y + y**2\"),\n        (\"Function[{x, y}, x^2 + y^2]\", \"Lambda(x, y, x**2 + y**2)\"),\n        (\"x[[1]]\", \"x[1]\"),\n        (\"x[ [1]  , 2 ]\", \"x[[1, 2]]\"),\n        (\"x[ {1, 2} ]\", \"x[[1, 2]]\"),\n        (\"x[[ All ]]\", \"x\"),\n        (\"x[ {All} ]\", \"x\"),\n        (\"x[ {1,2,3} ]\", \"x[[1, 2, 3]]\"),\n        (\"x[ {All, 2} ]\", \"x[[All, 2]]\"),\n        (\"x[ {1, All} ]\", \"x[[1, All]]\"),\n    ]\n    for expr, expected in test_cases:\n        assert parser._from_mathematica_to_text(expr) == expected\n\n\n\n", "def test_parser_mathematica_special_cases():\n    parser = MathematicaParser()\n\n    d = {\n        '1 + 2 + 3': '1 + 2 + 3',\n        '1 + 2 + 3 + 4': '1 + 2 + 3 + 4',\n        '1 + 2 + 3 + 4 + 5': '1 + 2 + 3 + 4 + 5',\n        '1 + 2 + 3 + 4 + 5 + 6': '1 + 2 + 3 + 4 + 5 + 6',\n        'a + b + c': 'a + b + c',\n        'a + b + c + d': 'a + b + c + d',\n        'a + b + c + d + e': 'a + b + c + d + e',\n        'a + (-b) + c': 'a - b + c',\n        'a + 1/b + c': 'a + 1/b + c',\n        'a + b/(c + d)': 'a + b/(c + d)',\n        'a + b*c + d': 'a + b*c + d',\n        'a + b*c*(d + e)': 'a + b*c*(d + e)',\n        'a + b**c': 'a + b**c',\n        'a + b^c': 'a + b**c',\n        'a + Sin[x] + Cos[y] + Tan[z]': 'a + sin(x) + cos(y) + tan(z)',\n        'a + Sin[2*x] + Cos[y*2]': 'a + sin(2*x) + cos(2*y)',\n        'a + Sin[x+y] + Cos[x-y]': 'a + sin(x + y) + cos(x - y)',\n        'a + Sin[x]^(y + z) + Cos[x]/(y - z)': 'a + sin(x)**(y + z) + cos(x)/(y - z)',\n        'a + (b + c)*(d + e)': 'a + (b +"], "sample_70": ["def test_legend_title_fontsize_kwargs():\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend(title=\"Aardvark\", title_fontsize=12,\n                    title_fontproperties={'family': 'serif'})\n    assert leg.get_title().get_fontsize() == 12\n    assert leg.get_title().get_fontproperties()['family'] == 'serif'\n\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend(title=\"Aardvark\", title_fontsize=12,\n                    title_fontproperties={'size': 12})\n    assert leg.get_title().get_fontsize() == 12\n    assert leg.get_title().get_fontproperties()['size'] == 12\n\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend(title=\"Aardvark\", title_fontsize=12,\n                    title_fontproperties={'family': 'sans-serif', 'size': 12})\n    assert leg.get_title().get_fontsize() == 12\n    assert leg.get_title().get_fontproperties()['family'] == 'sans-serif'\n    assert leg.get_title().get_fontproperties()['size'] == 12\n", "def test_legend_with_multiple_linecollections():\n    fig, ax = plt.subplots()\n    lc1 = mcollections.LineCollection([[1, 2, 3], [4, 5, 6]], colors=['r', 'g'])\n    lc2 = mcollections.LineCollection([[1, 2, 3], [4, 5, 6]], colors=['b', 'y'], linestyles=['-', '--'])\n    ax.add_collection(lc1)\n    ax.add_collection(lc2)\n    ax.legend()\n\n\n", "def test_legend_title_rotation():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='Aardvark')\n    leg = ax.legend(title=\"My Title\", title_fontsize=14,\n                    title_pad=0.5)\n    leg.set_title_position('right')\n    leg.get_title().set_rotation(90)\n\n    # Compare to a reference figure where the rotation is explicitly set by\n    # setting `title_loc` to \"right\" and then rotating the title.\n    fig_ref, ax_ref = plt.subplots()\n    ax_ref.plot(range(10), label='Aardvark')\n    leg_ref = ax_ref.legend(title=\"My Title\", title_fontsize=14,\n                           title_pad=0.5)\n    leg_ref.set_title_position('right')\n    leg_ref.get_title().set_rotation(90)\n\n\n\n", "def test_legend_title_position():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(title=\"My Legend\", title_position='right')\n    assert leg.get_title().get_position() == 'right' \n", "def test_legend_title_alignment():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(title=\"Aardvark\", title_align='left')\n    assert leg.get_title().get_horizontalalignment() == 'left'\n\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(title=\"Aardvark\", title_align='center')\n    assert leg.get_title().get_horizontalalignment() == 'center'\n\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(title=\"Aardvark\", title_align='right')\n    assert leg.get_title().get_horizontalalignment() == 'right'\n"], "sample_12": ["    def test_mti_inheritance_model_removal_no_parents(self):\n        # Test that removing a model with no children doesn't cause issues\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([Animal], [])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n", "    def test_mti_circular_dependency_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"dog\", models.OneToOneField('app.Dog', on_delete=models.CASCADE)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='Animal', model_name='Animal')\n", "    def test_mti_inheritance_field_change(self):\n        Animal = ModelState('app', 'Animal', [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=50))])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, model_name='Animal', name='name', max_length=100)\n", "    def test_mti_inheritance_model_removal_with_fk(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [\n            (\"animal\", models.ForeignKey(Animal, on_delete=models.CASCADE))\n        ], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "    def test_mti_order_with_respect_to(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',),\n                        order_with_respect_to='name')\n        Cat = ModelState('app', 'Cat', [], bases=('app.Animal',),\n                        order_with_respect_to='name')\n        changes = self.get_changes([Animal, Dog, Cat], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'DeleteModel', 'DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='Cat')\n        self.assertOperationAttributes(changes, 'app', 0, 2, name='Animal')\n\n"], "sample_1": ["def test_read_write_masked_values_with_comments(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1[\"a\"] = Column(data=[1, 2, np.nan, 4], name=\"a\")\n    t1[\"b\"] = Column(data=[5, np.nan, 7, 8], name=\"b\", mask=[False, True, False, False])\n\n    # Initial comment line 1\n    # Initial comment line 2\n    # READ TERR 1\n    # READ SERR 3\n    # ! Table 0 comment\n    # a,b\n    # 1,5\n    # 2,NO\n    # NaN,7\n    # 4,8\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [3]})\n    t2 = Table.read(test_file, format=\"ascii.qdp\", table_id=0)\n\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n\n    for col_name in t1.colnames:\n        assert col_name in t2.colnames\n\n    assert set(t1.meta.keys()) == set(t2.meta.keys())\n", "def test_read_write_error_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3, 4]))\n    t1.add_column(\n        MaskedColumn(\n            data=[4.0, np.nan, 3.0, 1.0], name=\"b\", mask=[False, False, False, True]\n        )\n    )\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [3]})\n    t2 = Table.read(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [3]})\n    assert np.allclose(t2[\"col1\"], t1[\"a\"])\n    assert np.all(t2[\"col1\"] == t1[\"a\"])\n    assert np.allclose(t2[\"col2_err\"], np.array([0.0, 0.0, 0.0, 0.0]))\n    assert np.allclose(t2[\"col2_perr\"], np.array([0.0, 0.0, 0.0, 0.0]))\n\n", "def test_read_with_err_specs(tmp_path):\n    example_qdp = \"\"\"\n    ! Initial comment line 1\n    ! Initial comment line 2\n    READ TERR 1\n    READ SERR 3\n    ! Table 0 comment\n    !a a(pos) a(neg) b c ce d\n    53000.5   0.25  -0.5   1  1.5  3.5 2\n    54000.5   1.25  -1.5   2  2.5  4.5 3\n    NO NO NO NO NO\n    \"\"\"\n    path = str(tmp_path / \"test.qdp\")\n    with open(path, \"w\") as fp:\n        print(example_qdp, file=fp)\n    \n    t = _read_table_qdp(path, names=[\"a\", \"b\", \"c\", \"d\"], err_specs={\"terr\": [1], \"serr\": [3]})\n\n    assert len(t.colnames) == 7\n    assert t.colnames[0:3] == [\"a\", \"a_err\", \"a_perr\"]\n    assert t.colnames[3:6] == [\"b\", \"b_err\", \"b_perr\"]\n    assert t.colnames[6] == \"c\"\n    \n    for col in [\"a_err\", \"a_perr\", \"b_err\", \"b_perr\"]:\n        assert np.all(np.isnan(t[col]))\n\n", "def test_read_write_masked_err_cols(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3, 4]))\n    t1.add_column(\n        MaskedColumn(\n            data=[4.0, np.nan, 3.0, 1.0],\n            name=\"b\",\n            mask=[False, False, False, True],\n        )\n    )\n    t1.add_column(\n        MaskedColumn(\n            data=[0.1, 0.2, 0.3, np.nan],\n            name=\"c\",\n            mask=[False, True, False, True],\n        )\n    )\n    t1.write(test_file, format=\"ascii.qdp\")\n    t2 = Table.read(test_file, format=\"ascii.qdp\")\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.all(t2[\"col1\"] == t1[\"a\"])\n    assert np.allclose(t2[\"col2\"][~np.isnan(t2[\"col2\"])], t1[\"b\"][~np.isnan(t1[\"b\"])])\n    assert np.allclose(\n        t2[\"col3\"][~np.isnan(t2[\"col3\"])], t1[\"c\"][~np.isnan(t1[\"c\"])]\n    )\n\n\n\n", "def test_read_write_masked_values_errors(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3, np.nan]))\n    t1.add_column(\n        MaskedColumn(\n            data=[np.nan, 2.0, 3.0, 4.0],\n            name=\"b\",\n            mask=[True, False, False, False],\n        )\n    )\n    t1.write(test_file, format=\"ascii.qdp\")\n    with pytest.warns(UserWarning) as record:\n        t2 = Table.read(test_file, format=\"ascii.qdp\")\n    assert np.any(\n        [\n            \"table_id not specified. Reading the first available table\"\n            in r.message.args[0]\n            for r in record\n        ]\n    )\n\n    assert np.allclose(t2[\"col1\"], t1[\"a\"])\n    assert np.ma.is_masked(t2[\"col2\"][0])\n    assert np.allclose(t2[\"col2\"][1], t1[\"b\"][1])\n    assert not np.ma.is_masked(t2[\"col2\"][2])\n    assert np.allclose(t2[\"col2\"][3], t1[\"b\"][3])\n\n"], "sample_137": ["def test_to_binary():\n    assert to_binary(0) == []\n    assert to_binary(1) == [1]\n    assert to_binary(3) == [1, 1]\n    assert to_binary(5) == [1, 0, 1]\n\n    assert to_binary(1234, scale=3) == [1, 2, 0, 1, 1]\n    assert to_binary(1234, scale=10) == [1, 2, 3, 4]\n\n    assert to_binary(0, scale=2, base=10) == []\n    assert to_binary(1, scale=2, base=10) == [1]\n    assert to_binary(3, scale=2, base=10) == [1, 1]\n    assert to_binary(5, scale=2, base=10) == [1, 0, 1]\n\n    raises(ValueError, lambda: to_binary(1234, scale=-1))\n\n\n", "def test_combinations_with_replacement():\n    assert list(combinations_with_replacement('abc', 3)) == [\n        ('a', 'a', 'a'), ('a', 'a', 'b'), ('a', 'a', 'c'),\n        ('a', 'b', 'a'), ('a', 'b', 'b'), ('a', 'b', 'c'),\n        ('a', 'c', 'a'), ('a', 'c', 'b'), ('a', 'c', 'c'),\n        ('b', 'a', 'a'), ('b', 'a', 'b'), ('b', 'a', 'c'),\n        ('b', 'b', 'a'), ('b', 'b', 'b'), ('b', 'b', 'c'),\n        ('b', 'c', 'a'), ('b', 'c', 'b'), ('b', 'c', 'c'),\n        ('c', 'a', 'a'), ('c', 'a', 'b'), ('c', 'a', 'c'),\n        ('c', 'b', 'a'), ('c', 'b', 'b'), ('c', 'b', 'c'),\n        ('c', 'c', 'a'), ('c', 'c', 'b'), ('c', 'c', 'c')]\n\n\n", "def test_is_partition():\n    assert is_partition([1, 2, 3])\n    assert is_partition({1, 2, 3})\n    assert is_partition([1, 1, 2])\n    assert is_partition({1, 1, 2})\n    assert not is_partition([1, 2, 3, 4])\n    assert not is_partition([1, 2, 0])\n    assert not is_partition([1, 2, 2, 3])\n    assert not is_partition([1, 1, 2, 3, 4])\n\n\n\n", "def test_cartesian_product():\n    assert list(cartesian_product([], [])) == [()]\n    assert list(cartesian_product([1], [2, 3])) == [(1, 2), (1, 3)]\n    assert list(cartesian_product([1, 2], [3, 4, 5])) == \\\n        [(1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5)]\n    assert list(cartesian_product(range(3), range(2, 5))) == \\\n        [(0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4)]\n    assert list(cartesian_product([1, 2], [3, 4], [5, 6])) == \\\n        [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6),\n         (2, 4, 5), (2, 4, 6)]\n", "compilation error"], "sample_98": ["def test_check_array_sparse_dtype():\n    # Test that sparse matrices with object dtype are correctly converted\n    # to float64 dtype.\n\n    X_obj = sp.csr_matrix([[1, 2, 3], [4, 5, 6]], dtype=object)\n    X_checked = check_array(X_obj, dtype=np.float64)\n    assert X_checked.dtype == np.float64\n\n    # Test that sparse matrices with a numeric dtype are not converted\n    X_float = sp.csr_matrix([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    X_checked = check_array(X_float, dtype=np.float64)\n    assert X_checked.dtype == np.float32\n\n\n", "compilation error", "compilation error", "compilation error", "    def test_check_array_dtype_deprecation(dtype):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            check_array(np.ones((4, 4)).astype(dtype), dtype=np.float32)\n        assert len(w) == 1\n        assert str(w[0].message) == (\n                f\"The dtype '{dtype}' is deprecated in favor of np.float32.\"\n                f\" Please use np.float32 instead.\"\n        )\n\n"], "sample_64": ["    def test_cell_count(self):\n        inline_admin_form = self.article_admin.get_inline_admin_formsets(None, request=self.request)[0]\n        count = self.template_tag_filter('cell_count')(inline_admin_form)\n        self.assertEqual(count, 8)\n", "    def test_datehierarchy_no_hierarchy(self):\n        modeladmin = ModelAdmin(Question, site)\n        modeladmin.date_hierarchy = None\n\n        request = self.factory.get(\"/\")\n        request.user = self.superuser\n        changelist = modeladmin.get_changelist_instance(request)\n        spec = date_hierarchy(changelist)\n        self.assertDictEqual(spec, {})\n", "    def test_cell_count_with_hidden_fields(self):\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_changelist\")\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        admin.date_hierarchy = \"date\"\n        formset = admin.get_formset_class()(instance=None)\n        formset = formset.bound_forms[0]\n        for field in formset.fields:\n            if field.name == \"hidden_field\":\n                field.is_hidden = True\n        count = cell_count(formset)\n        self.assertEqual(count, 2)\n", "    def test_date_hierarchy_ordering(self):\n        modeladmin = ModelAdmin(Question, site)\n        modeladmin.date_hierarchy = \"posted\"\n        Question.objects.bulk_create(\n            Question(question=\"q3\", posted=datetime.date(2017, 12, 15)),\n            Question(question=\"q1\", posted=datetime.date(2017, 10, 1)),\n            Question(question=\"q2\", posted=datetime.date(2017, 10, 1)),\n            Question(question=\"q4\", posted=datetime.date(2017, 12, 15)),\n            Question(question=\"q5\", posted=datetime.date(2017, 12, 31)),\n            Question(question=\"q6\", posted=datetime.date(2018, 2, 1)),\n        )\n        tests = (\n            ({}, [\"year=2017\", \"year=2018\"]),\n            ({\"year\": 2017}, [\"month=10\", \"month=12\"]),\n            ({\"year\": 2018}, [\"month=2\"]),\n        )\n        for query, expected_choices in tests:\n            with self.subTest(query=query):\n                query = {\"posted__%s\" % q: val for q, val in query.items()}\n                request = self.factory.get(\"/\", query)\n                request.user = self.superuser\n                changelist = modeladmin.get_changelist_instance(request)\n                spec = date_hierarchy(changelist)\n                choices = [choice[\"link\"] for choice in spec[\"choices\"]]\n                self.assertEqual(choices, expected_choices)\n\n", "    def test_override_change_list_template_tags_extra_context(self):\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_changelist\")\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        extra_context = {\"show_publish\": True, \"extra\": True}\n        response = admin.changelist_view(request, extra_context=extra_context)\n        response.render()\n        self.assertIs(response.context_data[\"show_publish\"], True)\n        self.assertIs(response.context_data[\"extra\"], True)\n"], "sample_159": ["def test_prefix_combinations():\n    prefix1 = PREFIXES['k']\n    prefix2 = PREFIXES['m']\n    assert prefix1 * prefix2 == PREFIXES['milli']\n\n    prefix3 = PREFIXES['m'] * prefix1\n    assert prefix3 == PREFIXES['milli']\n\n    prefix4 = PREFIXES['d'] * PREFIXES['c']\n    assert prefix4 == PREFIXES['milli']\n", "compilation error", "compilation error", "def test_latex_repr():\n    assert kilo._latex(None) == r'\\text{k}'\n    assert kibi._latex(None) == r'\\text{k}'\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['p']._latex(None) == r'\\text{p}'\n    assert PREFIXES['mu']._latex(None) == r\"\\mu\"\n", "def test_latex():\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['k']._latex(None) == r'\\text{k}'\n    assert PREFIXES['k']._latex(None) != r'\\text{K}'\n    assert BIN_PREFIXES['Ki']._latex(None) == r'\\text{Ki}'\n    assert BIN_PREFIXES['Ki']._latex(None) != r'\\text{k}'\n\n\n\n"], "sample_57": ["    def test_extra_form_with_initial_data(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"1\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertEqual(formset.total_form_count(), 3)\n        self.assertEqual(formset.initial_form_count(), 1)\n        for form in formset.forms:\n            self.assertTrue(form.is_bound)\n\n\n", "    def test_template_engine_override(self):\n        with isolate_lru_cache(get_default_renderer):\n            ChoiceFormSet = formset_factory(Choice, renderer=\"my_custom_renderer\")\n            formset = ChoiceFormSet()\n            self.assertEqual(formset.renderer, \"my_custom_renderer\")\n", "    def test_formset_invalid_with_non_form_errors(self):\n        class ChoiceWithNonFormErrorForm(Choice):\n                raise ValidationError(\"Custom non-form error\")\n\n        class ChoiceWithNonFormErrorFormSet(BaseFormSet):\n                raise ValidationError(\"Custom non-form error\")\n\n        ChoiceFormSet = formset_factory(ChoiceWithNonFormErrorForm)\n        formset = ChoiceFormSet(data={\"form-TOTAL_FORMS\": \"2\", \"form-INITIAL_FORMS\": \"0\"}, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [\"Custom non-form error\"])\n        self.assertEqual(formset.cleaned_data, [])\n\n        ChoiceFormSet = formset_factory(ChoiceWithNonFormErrorForm, formset=ChoiceWithNonFormErrorFormSet)\n        formset = ChoiceFormSet(data={\"form-TOTAL_FORMS\": \"2\", \"form-INITIAL_FORMS\": \"0\"}, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [\"Custom non-form error\"])\n        self.assertEqual(formset.cleaned_data, [])\n\n\n\n", "    def test_extra_forms_are_bound_after_management_form_removal(self):\n        \"\"\"\n        Regression test for #37579.\n        When extra forms exist, they should remain bound even if management\n        form is removed.\n        \"\"\"\n\n        data = {\n            \"choices-TOTAL_FORMS\": \"3\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n            \"choices-2-choice\": \"Two\",\n            \"choices-2-votes\": \"2\",\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        # Manually remove management form to simulate a different rendering\n        formset.management_form = None\n\n        self.assertEqual(len(formset.forms), 3)\n        self.assertTrue(formset.forms[0].is_bound)\n        self.assertTrue(formset.forms[1].is_bound)\n        self.assertTrue(formset.forms[2].is_bound)\n\n\n\n", "    def test_formset_can_add_extra_forms_after_initialisation(self):\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet()\n\n        #  Add one extra form\n        formset.add_form() \n        self.assertEqual(len(formset.forms), 1)\n\n        #  Add two extra forms\n        formset.add_form()\n        formset.add_form()\n        self.assertEqual(len(formset.forms), 3)\n\n\n\n\n"], "sample_29": ["    def test_output_field_override(self):\n        value = Value('f')\n        expr = ExpressionWrapper(Lower(value), output_field=CharField())\n        self.assertEqual(expr.output_field, CharField())\n", "    def test_no_output_field_raises(self):\n        with self.assertRaises(TypeError):\n            ExpressionWrapper(Value('foo'))\n", "    def test_resolve_output_field_from_expression(self):\n        expr = ExpressionWrapper(F('a') + F('b'), output_field=CharField())\n        self.assertIsInstance(expr.output_field, CharField)\n\n", "    def test_nested_expression_wrapper(self):\n        value = Value('f')\n        inner_expr = ExpressionWrapper(Lower(value), output_field=IntegerField())\n        outer_expr = ExpressionWrapper(inner_expr + Value(1), output_field=IntegerField())\n        self.assertEqual(outer_expr.get_group_by_cols(alias=None), [inner_expr.expression])\n", "    def test_output_field_inheritance(self):\n        outer_expr = ExpressionWrapper(\n            Expression(Value(1)), output_field=CharField())\n        inner_expr = ExpressionWrapper(outer_expr, output_field=IntegerField())\n        self.assertEqual(inner_expr.output_field, IntegerField())\n"], "sample_8": ["    def test_cleanse_setting_nested_dictionaries(self):\n        initial = {'users': {'john': {'password': 'secure'}}}\n        expected = {'users': {'john': {'password': CLEANSED_SUBSTITUTE}}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_with_nested_lists_and_dictionaries(self):\n        initial = {'complex_data': {'nested_dict': {'sensitive_key': 'secret1'}, 'list_with_dict': [{'another_secret': 'secret2'}]}}\n        expected = {'complex_data': {'nested_dict': {'sensitive_key': CLEANSED_SUBSTITUTE}, 'list_with_dict': [{'another_secret': CLEANSED_SUBSTITUTE}]}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_nested_dictionaries(self):\n        initial = {'nested': {'user': 'test', 'password': 'secret'}}\n        expected = {'nested': {'user': 'test', 'password': CLEANSED_SUBSTITUTE}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_with_nested_dictionaries(self):\n        initial = {'key1': {'pass1': 'secret', 'key2': 'value'}, 'key3': 'value'}\n        expected = {'key1': {'pass1': CLEANSED_SUBSTITUTE, 'key2': 'value'}, 'key3': 'value'}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_with_nested_dictionaries(self):\n        initial = {\n            'users': [\n                {'username': 'test1', 'password': 'secret1'},\n                {'username': 'test2', 'password': 'secret2'}\n            ],\n            'OTHER_SETTING': 'TEST'\n        }\n        expected = {\n            'users': [\n                {'username': 'test1', 'password': CLEANSED_SUBSTITUTE},\n                {'username': 'test2', 'password': CLEANSED_SUBSTITUTE}\n            ],\n            'OTHER_SETTING': 'TEST'\n        }\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n\n"], "sample_146": ["def test_printing_of_sympy_numbers():\n    assert str(S(10)) == '10'\n    assert str(S(10.5)) == '10.5'\n    assert str(S(10.5, rational=True)) == '21/2'\n    assert str(S(10, complex=True)) == '10.0'\n\n", "def test_printing_zeros():\n    from sympy import zeros, Point\n    assert str(zeros(2)) == 'zeros(2)'\n    assert str(Point(0, 0)) == '(0, 0)'\n", "compilation error", "def test_MatrixSlice_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 1, 3)\n    B = MatrixSymbol(\"B\", 1, 3)\n    C = MatrixSymbol(\"C\", 1, 3)\n\n    assert(str(A[0, 0]) == \"A[0, 0]\")\n    assert(str(3 * A[0, 0]) == \"3*A[0, 0]\")\n\n    F = C[0, 0].subs(C, A - B)\n    assert str(F) == \"(A - B)[0, 0]\"\n\n\n", "def test_printing_MatrixExpression_matrix_elements():\n    n = Symbol('n', integer=True)\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', n, n)    \n\n    assert str(X[0:n, 0:n]) == 'X[0:n, 0:n]'\n    assert str(X[0:n, 1:n+1]) == 'X[0:n, 1:n + 1]'\n    assert str(X[0, 0:n]) == 'X[0, 0:n]'\n    assert str(X[1:n, 0:n]) == 'X[1:n, 0:n]'\n    assert str(X[1:n, 1:n+1]) == 'X[1:n, 1:n + 1]'\n\n    assert str(X[0:n, 0:n] * Y[0:n, 0:n]) == 'X[0:n, 0:n]*Y[0:n, 0:n]'\n    assert str((X + Y)[0:n, 0:n]) == '(X + Y)[0:n, 0:n]'\n"], "sample_6": ["    def test_unicode_validator_special_chars(self):\n        valid_usernames = [\n            'joe.doe', 'john+smith', 'jane.doe@example.com',\n            'jean-marc_test', 'johndoe123',\n        ]\n        invalid_usernames = ['@username', 'username@', 'username..',\n                             'username..@example.com']\n        v = validators.UnicodeUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_username_validators_help_texts(self):\n        help_texts = validators.get_username_validators_help_texts()\n        self.assertEqual(len(help_texts), 2)\n        self.assertIn('ASCII characters', help_texts[0])\n        self.assertIn('Unicode characters', help_texts[1])\n\n", "    def test_unicode_validator_with_special_characters(self):\n        valid_usernames = ['joe.+-_', 'Ren\u00e9@.+-_']\n        invalid_usernames = ['joe.+_-@', 'Ren\u00e9@..+-_']\n        v = validators.UnicodeUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n\n", "    def test_username_validators_help_texts(self):\n        help_texts = validators.get_username_validators()\n        self.assertEqual(len(help_texts), 2)\n        self.assertIn('English letters', help_texts[0])\n        self.assertIn('letters', help_texts[1])\n", "    def test_custom_username_validator(self):\n        class MyUsernameValidator(validators.RegexValidator):\n            regex = r'^[a-zA-Z0-9_]{3,}$'\n            flags = 0\n            message = _('Must be at least 3 alphanumeric characters long.')\n\n        with self.assertRaises(ValidationError) as cm:\n            MyUsernameValidator().validate(\"too_short\")\n        self.assertEqual(cm.exception.messages, [_(\n            'Must be at least 3 alphanumeric characters long.'\n        )])\n\n\n"], "sample_10": ["    def test_ordering_with_custom_fields(self):\n        author = Author.objects.create(name='John Doe')\n        author2 = Author.objects.create(name='Jane Doe')\n        author3 = Author.objects.create(name='Bob Smith')\n        Article.objects.bulk_create([\n            Article(headline='Article 1', author=author, pub_date=datetime(2023, 11, 1)),\n            Article(headline='Article 2', author=author, pub_date=datetime(2023, 11, 5)),\n            Article(headline='Article 3', author=author2, pub_date=datetime(2023, 11, 3)),\n            Article(headline='Article 4', author=author3, pub_date=datetime(2023, 11, 2)),\n        ])\n\n        # Test ordering by a custom field\n        self.assertQuerysetEqual(\n            Article.objects.order_by('author__name'),\n            ['<Article: Article 1>', '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 4>']\n        )\n        \n        # Test ordering by a custom field in reverse order\n        self.assertQuerysetEqual(\n            Article.objects.order_by('-author__name'),\n            ['<Article: Article 4>', '<Article: Article 3>', '<Article: Article 2>', '<Article: Article 1>']\n        )\n", "    def test_exact_query_rhs_with_subquery_ordering(self):\n        newest_author = Author.objects.create(name='Author 2')\n        authors = Author.objects.filter(\n            pk__in=Article.objects.filter(pub_date=datetime(2023, 11, 11)).values_list('author_id', flat=True)\n        ).annotate(\n            max_id=Max('id'),\n        ).order_by('-max_id')[:1]\n        self.assertEqual(authors.get(), newest_author)\n\n\n\n", "    def test_exact_query_rhs_with_aggregate_column(self):\n        a1, a2, a3, a4, a5, a6, a7 = Article.objects.all()\n        # Test with an aggregate column\n        max_headline_length = Article.objects.values('headline').annotate(\n            headline_length=Length('headline')\n        ).order_by('-headline_length').values('headline_length')[0]['headline_length']\n        articles = Article.objects.filter(headline__length__exact=max_headline_length)\n        self.assertCountEqual(articles, [a1, a2, a3])\n", "    def test_lookup_with_subquery_ordering(self):\n        for i in range(10):\n            Article.objects.create(headline=f'headline {i}', pub_date=datetime(2023, 11, i))\n        articles = Article.objects.filter(\n            pub_date__year__in=Article.objects.values('pub_date__year').order_by('pub_date__year').distinct()\n        ).distinct()\n        self.assertEqual(articles.count(), 10)\n", "    def test_slice_queryset_with_date_time_field(self):\n        now = datetime.now()\n        Article.objects.bulk_create([\n            Article(pub_date=now - timedelta(days=1), headline=\"Article 1\"),\n            Article(pub_date=now - timedelta(days=2), headline=\"Article 2\"),\n            Article(pub_date=now - timedelta(days=3), headline=\"Article 3\"),\n            Article(pub_date=now - timedelta(days=4), headline=\"Article 4\"),\n            Article(pub_date=now - timedelta(days=5), headline=\"Article 5\"),\n        ])\n        # slicing with DateTimeField\n        self.assertCountEqual(\n            Article.objects.filter(pub_date__slice=[now - timedelta(days=3), now]),\n            [\n                '<Article: Article 3>',\n                '<Article: Article 2>',\n                '<Article: Article 1>',\n            ],\n        )\n\n\n\n"], "sample_33": ["    def test_receiver_dispatch_uid_collision(self):\n        @receiver(a_signal, dispatch_uid=\"uid\")\n            self.state.append(1)\n        @receiver(a_signal, dispatch_uid=\"uid\")\n            self.state.append(2)\n        self.state = []\n        a_signal.send(sender=self, val='test')\n        self.assertEqual(self.state, [1, 2])\n", "    def test_receiver_dispatch_uid(self):\n        @receiver(a_signal, dispatch_uid=\"uid\")\n            self.state = val\n        self.state = False\n        a_signal.send(sender=self, val=True)\n        self.assertTrue(self.state)\n        @receiver(a_signal, dispatch_uid=\"uid\")\n            self.state = val\n        g(\"b\")\n        a_signal.send(sender=self, val=\"c\")\n        self.assertEqual(self.state, \"b\")\n\n", "    def test_receiver_multiple_dispatch_uids(self):\n        @receiver(a_signal, dispatch_uid=\"uid_1\")\n            self.state.append(\"uid_1\")\n        @receiver(a_signal, dispatch_uid=\"uid_2\")\n            self.state.append(\"uid_2\")\n        self.state = []\n        a_signal.send(sender=self, val='test')\n        self.assertEqual(self.state, [\"uid_1\", \"uid_2\"])\n", "    def test_receiver_dispatch_uid(self):\n        @receiver(a_signal, dispatch_uid=\"uid1\")\n            self.state.append(\"f1\")\n        @receiver(a_signal, dispatch_uid=\"uid1\")\n            self.state.append(\"f2\")\n        @receiver(a_signal, dispatch_uid=\"uid2\")\n            self.state.append(\"f3\")\n        self.state = []\n        a_signal.send(sender=self, val='test')\n        self.assertIn(\"f1\", self.state)\n        self.assertIn(\"f2\", self.state)\n        a_signal.disconnect(dispatch_uid=\"uid2\")\n        a_signal.send(sender=self, val='test')\n        self.assertIn(\"f1\", self.state)\n        self.assertIn(\"f2\", self.state)\n        self.assertNotIn(\"f3\", self.state)\n", "    def test_receiver_dispatch_uid(self):\n        @receiver(a_signal, dispatch_uid='uid')\n            self.state1 = val\n        @receiver(a_signal, dispatch_uid='uid_2')\n            self.state2 = val\n        self.state1 = False\n        self.state2 = False\n        a_signal.send(sender=self, val='a')\n        a_signal.send(sender=self, val='b')\n        self.assertTrue(self.state1)\n        self.assertFalse(self.state2)\n        self.assertTestIsClean(a_signal)\n\n\n\n"], "sample_80": ["    def test_limit_lines_repr_length(self):\n        da = xr.DataArray(np.random.randn(1000, 1000))\n        limited_repr = formatting.limit_lines(repr(da), limit=10)\n        assert len(limited_repr.splitlines()) <= 10\n\n        # Should not modify the original repr if limit is greater than actual lines\n        original_lines = len(repr(da).splitlines())\n        formatted_repr = formatting.limit_lines(repr(da), limit=original_lines + 10)\n        assert len(formatted_repr.splitlines()) == original_lines\n\n\n\n", "    def test_format_array_flat_large_array(self):\n        da = xr.DataArray(np.random.randn(10000), dims=\"x\")\n        actual = formatting.format_array_flat(da.values, 10)\n        expected = \"0 ... 9999\"\n        assert actual == expected\n", "compilation error", "    def test_wrap_indent(self):\n        text = \"This is a very long string that needs to be wrapped and indented.\\nThis is the second line.\"\n        indented_text = formatting._wrap_indent(text, width=30)\n        expected = (\n            \"This is a very long string that needs\"\n            \" to be wrapped and indented.\\n\"\n            \"    This is the second line.\"\n        )\n        assert indented_text == expected\n", "    def test_diff_array_repr_array_equiv_allclose(self, compat, expected_output):\n        da_a = xr.DataArray(\n            np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"), dims=(\"x\", \"y\")\n        )\n        \n        da_b = xr.DataArray(np.array([5, 6, 7], dtype=\"int64\"), dims=(\"y\"))\n\n        actual = formatting.diff_array_repr(da_a, da_b, compat)\n        assert actual == expected_output\n\n"], "sample_143": ["def test_issue_18402():\n    from sympy.physics.quantum import Qubit, Dagger\n\n    qubit = Qubit('q')\n    dag = Dagger(qubit)\n    assert pretty(qubit) == 'q'\n    assert pretty(dag) == 'q\u2020'\n    assert pretty(qubit * dagger) == 'q \u22c5 q\u2020'\n\n", "def test_pretty_sympy_latex():\n    from sympy import sin, cos, pi\n    assert pretty(sin(pi / 2), mode='latex') == '\\\\sin\\\\left(\\\\frac{\\pi}{2}\\\\right)'  \n    assert pretty(cos(pi), mode='latex') == '\\\\cos\\\\left(\\\\pi\\\\right)'\n", "compilation error", "def test_issue_18435():\n    from sympy.tensor.tensor import TensorSymbol, tensor_indices, TensorHead\n    i, j, k = tensor_indices(\"i j k\")\n    A = TensorHead(\"A\", [i, j])\n    B = TensorSymbol(\"B\", [i, j])\n    expr = A[i, j] * B[i, j]\n    assert pretty(expr) == 'A[i, j] * B[i, j]'\n    assert upretty(expr) == 'A_{i j} \u22c5 B_{i j}'\n", "compilation error"], "sample_72": ["def test_rubberband_selection():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3, 4], [5, 6, 7, 8])\n\n    rb = fig.canvas.manager.toolmanager.get_tool('rubberband')\n    rb.button_press_event(\n        fig.canvas.mouse_event,\n        xdata=1.5, ydata=6.5, button=MouseButton.LEFT) \n\n    rb.button_release_event(\n        fig.canvas.mouse_event,\n        xdata=3.5, ydata=7.5, button=MouseButton.LEFT)\n    \n    artist = ax.select_data([1, 1.5, 2, 2.5, 3, 3.5, 4])\n\n    assert len(artist) == 4 \n    assert all([isinstance(x, mpl.lines.Line2D) for x in artist])\n\n", "def test_toolmanager_disabled_tools():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    disabled_tools = ['zoom', 'pan']\n    fig.canvas.manager.toolmanager.set_enabled_tools(disabled_tools)\n    for tool in disabled_tools:\n        assert not fig.canvas.manager.toolmanager.get_tool(tool).active\n", "    def test_toolmanager_update_keymap_tool_exist(tool, key, expected_key):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        fig.canvas.manager.toolmanager.update_keymap(tool, key)\n        assert fig.canvas.manager.toolmanager.get_tool_keymap(tool) == [expected_key]\n", "def test_interactive_zoom_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10))\n    assert ax.get_navigate()\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n\n    # Mouse move starts from 0.5, 0.5\n    mousestart = (0.5, 0.5)\n    # Convert to screen coordinates (\"s\").  Events are defined only with pixel\n    # precision, so round the pixel values, and below, check against the\n    # corresponding xdata/ydata, which are close but not equal to d0/d1.\n    sstart = ax.transData.transform(mousestart).astype(int)\n    send = ax.transData.transform(mousestart).astype(int)\n\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, *sstart, button=MouseButton.LEFT)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas, *send, button=MouseButton.LEFT)\n\n    tb.press_zoom(start_event)\n    tb.drag_zoom(stop_event)\n    tb.release_zoom(stop_event)\n    \n\n", "def test_toolmanager_undo_redo():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3], [4, 5, 6])\n\n    rubberband = fig.canvas.manager.toolmanager.get_tool('rubberband')\n    ax.set_navigate(True)  # Enable navigation for the axes\n    rubberband.set_active(ax)\n    fig.canvas.manager.toolmanager.trigger_tool('rubberband')\n    # Simulate first click and drag\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, 0, 0, MouseButton.LEFT)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas, 500, 500, MouseButton.LEFT)\n\n    fig.canvas.callbacks.process(start_event.name, start_event)\n    fig.canvas.callbacks.process(stop_event.name, stop_event)\n\n    # Undo\n    fig.canvas.manager.toolmanager.undo()\n    # Should be back to original plot\n\n    # Redo\n    fig.canvas.manager.toolmanager.redo()\n\n\n\n"], "sample_151": ["compilation error", "def test_issue_14106():\n    p1 = Point3D(1, 0, 0)\n    p2 = Point3D(0, 1, 0)\n    p3 = Point3D(0, 0, 1)\n    assert p1.projection(p2) == Point3D(0, 0, 0)\n    assert p1.projection(p3) == Point3D(0, 0, 0)\n    assert p2.projection(p1) == Point3D(0, 0, 0)\n    assert p2.projection(p3) == Point3D(0, 0, 0)\n    assert p3.projection(p1) == Point3D(0, 0, 0)\n    assert p3.projection(p2) == Point3D(0, 0, 0)\n", "compilation error", "compilation error", "compilation error"], "sample_34": ["    def test_explicit_pk_with_explicit_auto_field_conflicts(self):\n        class Model(models.Model):\n            id = models.BigAutoField(primary_key=True)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(\n                \"Primary key field 'id' is defined both explicitly and as a \"\n                \"default auto-field. The explicit definition will be used.\",\n                hint=(\n                    \"Consider removing the primary key definition for 'id' \"\n                    \"as it is already being managed by the \"\n                    \"DEFAULT_AUTO_FIELD setting.\"\n                ),\n                obj=Model,\n                id='models.W043',\n            ),\n        ])\n", "    def test_unique_together_ordering_not_allowed(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = [('field1', 'field2')]\n                ordering = ['field1']\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"You cannot define both unique_together and ordering for model \"\n                \"check_framework.Model\",\n                id='models.E043',\n            )\n        ])\n", "    def test_model_ordering_with_router(self, apps):\n        class Model1(models.Model):\n            class Meta:\n                db_table = 'test_table'\n                ordering = ['id']\n\n        class Model2(models.Model):\n            class Meta:\n                db_table = 'test_table'\n                ordering = ['id']\n\n        self.assertEqual(len(checks.run_checks(app_configs=apps.get_app_configs())), 0)\n\n\n\n", "    def test_auto_created_pk_across_apps(self, apps):\n        class BasicModel(models.Model):\n            class Meta:\n                app_label = 'basic'\n\n        class CheckFrameworkModel(models.Model):\n            class Meta:\n                app_label = 'check_framework'\n\n        self.assertEqual(checks.run_checks(app_configs=apps.get_app_configs()), [\n            Warning(self.msg, hint=self.hint, obj=BasicModel, id='models.W042'),\n        ])\n\n\n", "    def test_auto_created_pk_custom_primary_key_attribute(self, mock_primary_key):\n        class Model(models.Model):\n            id = models.AutoField()  \n            \n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(\n                \"Auto-created primary key used when not defining a primary key type, \"\n                \"by default 'django.db.models.AutoField'.\"\n            , hint=self.hint, obj=Model, id='models.W042'),\n        ])\n        mock_primary_key.assert_called_once_with()\n"]}, "raw_scored_generations": {"sample_0": [false, false, false, false, false], "sample_1": [false, false, false, false, false], "sample_2": [false, false, false, false, false], "sample_3": [false, false, false, false, false], "sample_6": [false, false, false, false, false], "sample_4": [false, false, false, false, true], "sample_5": [false, false, false, false, false], "sample_7": [false, false, false, false, false], "sample_9": [false, true, true, true, false], "sample_8": [true, false, true, false, false], "sample_10": [false, false, false, false, false], "sample_11": [false, false, false, false, false], "sample_13": [false, true, false, true, true], "sample_12": [true, false, false, false, false], "sample_14": [false, false, false, false, false], "sample_15": [false, false, false, true, false], "sample_16": [false, false, false, true, false], "sample_17": [false, false, false, false, false], "sample_18": [false, false, false, false, true], "sample_19": [false, false, false, false, false], "sample_21": [false, false, false, false, false], "sample_20": [true, false, false, false, false], "sample_23": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_24": [false, false, false, false, false], "sample_25": [false, false, false, false, false], "sample_28": [false, false, false, false, false], "sample_26": [false, false, false, false, false], "sample_27": [false, false, false, true, false], "sample_29": [false, true, true, false, false], "sample_31": [true, false, false, true, true], "sample_30": [true, true, false, true, true], "sample_32": [false, true, false, false, false], "sample_33": [false, false, false, false, false], "sample_34": [false, false, false, false, false], "sample_35": [true, false, false, false, true], "sample_36": [false, false, false, false, false], "sample_37": [true, false, false, false, false], "sample_39": [false, false, false, false, true], "sample_38": [true, false, false, false, false], "sample_40": [false, true, false, true, false], "sample_42": [false, false, false, false, false], "sample_41": [false, false, true, true, false], "sample_43": [true, true, true, true, true], "sample_45": [false, false, false, false, false], "sample_44": [false, false, false, false, false], "sample_46": [false, false, true, false, false], "sample_48": [false, false, false, false, false], "sample_47": [false, true, false, true, false], "sample_50": [false, false, false, false, false], "sample_49": [false, false, false, false, false], "sample_51": [false, true, true, true, true], "sample_52": [true, true, false, false, false], "sample_53": [false, false, false, true, false], "sample_55": [false, false, false, true, false], "sample_54": [false, false, false, false, false], "sample_56": [false, false, false, true, true], "sample_58": [false, true, false, true, true], "sample_57": [false, true, false, false, false], "sample_60": [false, false, false, false, false], "sample_59": [false, false, true, true, false], "sample_61": [true, false, false, true, true], "sample_63": [false, false, false, false, false], "sample_62": [false, false, false, true, false], "sample_64": [false, false, false, false, false], "sample_66": [false, false, false, false, false], "sample_65": [false, false, false, false, false], "sample_68": [true, false, false, false, false], "sample_67": [false, false, false, false, false], "sample_69": [false, false, false, false, false], "sample_70": [false, false, false, false, false], "sample_71": [false, false, true, false, false], "sample_72": [false, false, true, true, false], "sample_73": [false, false, false, false, false], "sample_74": [false, false, false, false, false], "sample_75": [false, false, false, false, false], "sample_76": [false, false, false, true, true], "sample_77": [false, false, false, false, false], "sample_78": [false, false, false, false, false], "sample_79": [false, true, false, false, false], "sample_80": [true, true, false, true, true], "sample_82": [false, false, false, false, false], "sample_81": [true, false, true, false, false], "sample_83": [false, false, true, true, false], "sample_84": [false, false, false, false, false], "sample_85": [false, false, true, true, false], "sample_86": [false, false, false, false, false], "sample_87": [false, false, false, false, false], "sample_88": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [false, true, true, true, true], "sample_91": [false, false, false, false, false], "sample_92": [true, false, false, false, false], "sample_93": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_94": [false, false, false, false, false], "sample_96": [false, false, false, false, false], "sample_97": [false, false, false, true, false], "sample_98": [false, false, false, false, true], "sample_99": [false, false, true, false, false], "sample_100": [false, false, false, false, true], "sample_101": [false, false, false, false, false], "sample_102": [false, true, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [false, false, false, false, false], "sample_105": [false, false, false, false, false], "sample_107": [false, false, false, false, false], "sample_106": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [false, false, false, false, false], "sample_110": [false, false, false, false, false], "sample_111": [false, false, false, false, false], "sample_112": [true, false, false, false, true], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, false], "sample_115": [true, false, false, false, false], "sample_116": [false, false, false, false, false], "sample_117": [true, true, false, false, false], "sample_118": [false, false, false, false, false], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, true, false], "sample_121": [false, false, false, false, false], "sample_122": [false, true, false, false, false], "sample_123": [false, false, true, false, true], "sample_124": [false, false, true, false, false], "sample_125": [false, true, false, false, false], "sample_126": [false, true, true, false, true], "sample_127": [false, false, false, false, false], "sample_128": [false, false, false, false, false], "sample_129": [false, false, false, false, false], "sample_130": [true, false, true, true, true], "sample_131": [false, false, false, false, false], "sample_132": [false, true, false, false, false], "sample_133": [false, false, true, false, false], "sample_134": [true, true, true, true, true], "sample_135": [false, false, false, true, false], "sample_136": [false, false, false, false, false], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, false], "sample_139": [false, false, false, true, false], "sample_140": [false, false, false, false, true], "sample_142": [true, false, false, false, false], "sample_141": [false, false, false, false, false], "sample_143": [false, false, false, false, false], "sample_144": [false, true, false, false, true], "sample_145": [true, false, false, false, false], "sample_146": [false, false, false, true, false], "sample_147": [false, false, false, false, false], "sample_148": [false, false, false, false, false], "sample_149": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_151": [false, false, false, false, false], "sample_152": [true, false, false, false, false], "sample_153": [false, false, false, true, false], "sample_154": [true, true, true, true, true], "sample_155": [false, true, false, false, false], "sample_156": [false, false, false, false, false], "sample_157": [false, false, true, false, false], "sample_158": [false, false, false, false, false], "sample_159": [false, false, false, false, false]}}