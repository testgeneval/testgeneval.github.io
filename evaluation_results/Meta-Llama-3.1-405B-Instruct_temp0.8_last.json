{"raw_generations": {"sample_0": ["def test_conversion_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        start_uncert.represent_as(VarianceUncertainty)\n", "def test_self_conversion_via_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        start_uncert.represent_as(UncertClass)\n", "def test_self_conversion_via_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        start_uncert.represent_as(UncertClass)\n", "def test_conversion_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        start_uncert.represent_as(VarianceUncertainty)\n", "def test_conversion_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        start_uncert.represent_as(VarianceUncertainty)\n"], "sample_3": ["def test_write_with_invalid_dtype():\n    \"\"\"Test writing a table with an invalid dtype.\"\"\"\n    t = Table()\n    t['a'] = np.array([1, 2], dtype=np.complex128)\n    out = StringIO()\n    with pytest.raises(ValueError, match=\"datatype 'complex128' of column 'a' is not in allowed values\"):\n        t.write(out, format='ascii.ecsv')\n", "def test_ecsv_header_missing_metadata():\n    \"\"\"Test ECSV header with missing metadata\"\"\"\n    txt = \"\"\"\\", "def test_ecsv_round_trip_quantity_with_units(tmpdir):\n    \"\"\"Ensure that we can read-back Quantity with units.\"\"\"\n    filename = str(tmpdir.join('test.ecsv'))\n    t = Table()\n    t['q'] = np.arange(5) * u.m\n    t.write(filename)\n    t2 = Table.read(filename)\n    assert isinstance(t2['q'], Column)\n    assert t2['q'].unit == u.m\n    assert np.all(t2['q'] == t['q'])\n", "def test_read_ecsv_with_missing_meta():\n    \"\"\"Test reading an ECSV file with a missing meta section.\"\"\"\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    # datatype:\n    # - {name: a, datatype: int64}\n    # schema: astropy-2.0\n    a\n    1\n    2\n    3\n    \"\"\"\n    t = Table.read(txt, format='ascii.ecsv')\n    assert len(t) == 3\n    assert np.all(t['a'] == [1, 2, 3])\n", "def test_ecsv_write_read_masked_column():\n    \"\"\"Test writing and reading a masked column with ECSV format\"\"\"\n    t = Table()\n    t['a'] = MaskedColumn([1, 2, 3], mask=[True, False, True])\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert isinstance(t2['a'], MaskedColumn)\n    assert np.all(t2['a'].mask == t['a'].mask)\n    assert np.all(t2['a'].data == t['a'].data)\n"], "sample_1": ["def test_separable_single_input_multi_output():\n    model = models.Shift(1)\n    model.n_inputs = 1\n    model.n_outputs = 2\n    result = is_separable(model)\n    assert_allclose(result, np.array([False, False]))\n", "def test_separable_single_input_multi_output():\n    model = models.Polynomial1D(1, n_outputs=2)\n    assert_allclose(is_separable(model), np.array([False, False]))\n    assert_allclose(separability_matrix(model), np.ones((2, 1)))\n", "def test_separable_single_input_model():\n    model = models.Polynomial1D(1)\n    assert_allclose(is_separable(model), np.array([False]))\n    assert_allclose(separability_matrix(model), np.array([[True]]))\n", "def test_custom_model_separable():\n    @custom_model\n        return x\n\n    @custom_model(separable=True)\n        return x + y\n\n    @custom_model(separable=False)\n        return x * y\n\n    assert_allclose(is_separable(model_a()), np.array([True]))\n    assert_allclose(separability_matrix(model_a()), np.array([[True]]))\n    assert_allclose(is_separable(model_b()), np.array([False, False]))\n    assert_allclose(separability_matrix(model_b()), np.array([[True, True], [True, True]]))\n    assert_allclose(is_separable(model_c()), np.array([False, False]))\n    assert_allclose(separability_matrix(model_c()), np.array([[True, True], [True, True]]))\n", "def test_separable_single_input_multi_output():\n    model = models.Polynomial1D(1, n_outputs=2)\n    assert_allclose(is_separable(model), np.array([False, False]))\n    assert_allclose(separability_matrix(model), np.ones((2, 1)))\n"], "sample_2": ["def test_ccddata_with_quantity_mask():\n    ccd_data = create_ccd_data()\n    mask = np.ones_like(ccd_data.data) * u.dimensionless_unscaled\n    with pytest.raises(TypeError):\n        CCDData(ccd_data.data, mask=mask, unit='adu')\n", "def test_ccddata_with_wcs(tmpdir):\n    ccd_data = create_ccd_data()\n    wcs = WCS(naxis=2)\n    wcs.wcs.crpix = np.array(ccd_data.shape) / 2\n    wcs.wcs.cdelt = np.array([-0.066667, 0.066667])\n    wcs.wcs.crval = [0, -90]\n    wcs.wcs.ctype = [\"RA---AIR\", \"DEC--AIR\"]\n    ccd_data.wcs = wcs\n\n    filename = tmpdir.join('test.fits').strpath\n    ccd_data.write(filename)\n\n    ccd_disk = CCDData.read(filename, unit=ccd_data.unit)\n    assert ccd_disk.wcs is not None\n    assert ccd_disk.wcs.wcs == wcs.wcs\n", "def test_ccddata_arithmetic_with_invalid_operand():\n    ccd_data = create_ccd_data()\n    with pytest.raises(TypeError):\n        ccd_data.add(object())\n\n    with pytest.raises(TypeError):\n        ccd_data.subtract(object())\n\n    with pytest.raises(TypeError):\n        ccd_data.multiply(object())\n\n    with pytest.raises(TypeError):\n        ccd_data.divide(object())\n", "def test_ccddata_mask_setter():\n    ccd_data = create_ccd_data()\n    mask = np.ones_like(ccd_data.data, dtype=np.bool_)\n    ccd_data.mask = mask\n    np.testing.assert_array_equal(ccd_data.mask, mask)\n\n    # Test that setting mask to None works\n    ccd_data.mask = None\n    assert ccd_data.mask is None\n\n    # Test that setting mask to a non-ndarray raises an error\n    with pytest.raises(TypeError):\n        ccd_data.mask = \"not a mask\"\n", "def test_ccddata_read_raises_type_error_if_keyword_is_unsupported(tmpdir):\n    ccd_data = create_ccd_data()\n    filename = tmpdir.join('test.fits').strpath\n    ccd_data.write(filename)\n\n    with pytest.raises(TypeError):\n        CCDData.read(filename, unit=ccd_data.unit, scale_back=True)\n    with pytest.raises(TypeError):\n        CCDData.read(filename, unit=ccd_data.unit, do_not_scale_image_data=True)\n"], "sample_4": ["def test_html_identify():\n    \"\"\"Test html_identify function\"\"\"\n    assert html_identify(None, \"test.html\", None) is True\n    assert html_identify(None, \"test.txt\", None) is False\n    assert html_identify(None, b\"test.html\", None) is False  # filepath should be str\n", "def test_readwrite_html_format_argument(self, cosmo, read, write, tmp_path):\n    \"\"\"Test format argument in read/write functions\"\"\"\n    fp = tmp_path / \"test_readwrite_html_format_argument.html\"\n\n    # Test format argument in write function\n    write(fp, format=\"ascii.html\")\n\n    # Test format argument in read function\n    got = read(fp, format=\"ascii.html\")\n    assert got == cosmo\n\n    # Test incorrect format argument in write function\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not 'ascii.csv'\"):\n        write(fp, format=\"ascii.csv\")\n\n    # Test incorrect format argument in read function\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not 'ascii.csv'\"):\n        read(fp, format=\"ascii.csv\")\n", "def test_read_html_table_format_error(self, tmp_path):\n    \"\"\"Test if the format is not 'ascii.html'.\"\"\"\n    fp = tmp_path / \"test_read_html_table_format_error.html\"\n\n    with pytest.raises(ValueError, match=\"format must be 'ascii.html', not 'csv'\"):\n        read_html_table(fp, format=\"csv\")\n", "    def test_readwrite_html_format_argument(self, cosmo, read, write, tmp_path):\n        \"\"\"Test that the format argument is checked correctly\"\"\"\n        fp = tmp_path / \"test_readwrite_html_format_argument.html\"\n\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            write(fp, format=\"wrong\")\n\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            read(fp, format=\"wrong\")\n", "    def test_readwrite_html_table_instance_with_format(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test cosmology -> ascii.html -> cosmology with explicit format.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_instance_with_format.html\"\n\n        # ------------\n        # To Table\n\n        write(fp, format=\"ascii.html\")\n\n        # ------------\n        # From Table\n\n        got = read(fp, format=\"ascii.html\")\n        assert got.__class__ is cosmo_cls\n        assert got.name == cosmo.name\n\n        # Check that the format argument is checked\n        with pytest.raises(ValueError, match=\"format must be 'ascii.html'\"):\n            read(fp, format=\"other\")\n"], "sample_5": ["def test_parameter_setters(model):\n    m = model['class'](**model['parameters'])\n\n    # Test setting a parameter to a value with the same unit as the default\n    for param_name, param_value in model['parameters'].items():\n        if param_value is not None:\n            param = getattr(m, param_name)\n            new_value = param_value * 2\n            setattr(m, param_name, new_value)\n            assert getattr(m, param_name).value == new_value.value\n\n    # Test setting a parameter to a value with a different unit than the default\n    for param_name, param_value in model['parameters'].items():\n        if param_value is not None and param_value.unit is not None:\n            param = getattr(m, param_name)\n            new_value = param_value.value * 2 * param_value.unit.to(param.unit)\n            setattr(m, param_name, new_value)\n            assert getattr(m, param_name).value == new_value.value\n\n    # Test setting a parameter to a Quantity object\n    for param_name, param_value in model['parameters'].items():\n        if param_value is not None:\n            param = getattr(m, param_name)\n            new_value = u.Quantity(param_value.value * 2, param_value.unit)\n            setattr(m, param_name, new_value)\n            assert getattr(m, param_name).value == new_value.value\n", "def test_models_parameter_validator(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n\n    for param_name in m.param_names:\n        param = getattr(m, param_name)\n\n        # Test that the validator is called when setting a value\n            raise ValueError(\"Validator was called\")\n        param.validator(validator)\n        with pytest.raises(ValueError, match=\"Validator was called\"):\n            setattr(m, param_name, 1 * param.unit)\n\n        # Test that the validator is not called when setting a quantity with equivalent units\n            raise ValueError(\"Validator should not be called\")\n        param.validator(validator)\n        original_unit = param.unit\n        setattr(m, param_name, param.value * original_unit)\n", "def test_models_parameter_fixed(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n    m = model['class'](**model['parameters'])\n    for param_name in m.param_names:\n        par = getattr(m, param_name)\n        par.fixed = True\n        assert par.fixed\n        par.fixed = False\n        assert not par.fixed\n", "def test_parameter_unit_setter(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    for param_name, param_value in model['parameters'].items():\n        if hasattr(param_value, 'unit'):\n            param = getattr(m, param_name)\n            new_value = 2 * param_value\n            param.value = new_value.value\n            param.unit = new_value.unit\n            assert param.quantity == new_value\n", "def test_models_parameter_value_update(model):\n    m = model['class'](**model['parameters'])\n    for param_name, value in model['parameters'].items():\n        if isinstance(value, Quantity):\n            new_value = 2 * value\n        else:\n            new_value = 2 * np.array(value)\n        setattr(m, param_name, new_value)\n        assert_quantity_allclose(getattr(m, param_name).value, new_value.value)\n        assert getattr(m, param_name).unit == new_value.unit\n"], "sample_6": ["def test_angle_string_parsing():\n    \"\"\"\n    Test parsing of angle strings in various formats.\n    \"\"\"\n    # Test parsing with units specified\n    a1 = Angle(\"10.2345 degrees\")\n    npt.assert_almost_equal(a1.value, 10.2345)\n\n    a2 = Angle(\"1:2:30.43 hours\")\n    npt.assert_almost_equal(a2.hour, 1.04178611)\n\n    # Test parsing without units specified (default to degrees)\n    a3 = Angle(\"10.2345\")\n    npt.assert_almost_equal(a3.value, 10.2345)\n\n    # Test parsing with HMS format and no unit specified (default to hours)\n    a4 = Angle(\"1:2:30.43\")\n    npt.assert_almost_equal(a4.hour, 1.04178611)\n\n    # Test parsing with DMS format and unit specified\n    a5 = Angle(\"1\u00b02\u20323\u2033\")\n    npt.assert_almost_equal(a5.degree, 1.03416667)\n\n    # Test parsing with HMS format and unit specified\n    a6 = Angle(\"1h2m3.4s\")\n    npt.assert_almost_equal(a6.hour, 1.03427778)\n\n    # Test parsing with sign and unit specified\n    a7 = Angle(\"-1d2m3.4s\")\n    npt.assert_almost_equal(a7.degree, -1.03427778)\n", "def test_angle_signed_dms():\n    \"\"\"\n    Test signed dms tuple.\n    \"\"\"\n    a1 = Angle(45.5, unit=u.degree)\n    sign, d, m, s = a1.signed_dms\n    npt.assert_almost_equal(sign, 1)\n    npt.assert_almost_equal(d, 45)\n    npt.assert_almost_equal(m, 30)\n    npt.assert_almost_equal(s, 0)\n\n    a2 = Angle(-45.5, unit=u.degree)\n    sign, d, m, s = a2.signed_dms\n    npt.assert_almost_equal(sign, -1)\n    npt.assert_almost_equal(d, 45)\n    npt.assert_almost_equal(m, 30)\n    npt.assert_almost_equal(s, 0)\n", "def test_angle_string_parsing():\n    \"\"\"\n    Test parsing of string input for Angle objects.\n    \"\"\"\n    # Test basic string parsing\n    a1 = Angle('10.5d')\n    assert_allclose(a1.value, 10.5)\n    assert a1.unit == u.degree\n\n    # Test sexagesimal string parsing\n    a2 = Angle('1:2:3', unit=u.degree)\n    assert_allclose(a2.value, 1 + 2/60 + 3/3600)\n\n    # Test hour angle string parsing\n    a3 = Angle('1h2m3s')\n    assert_allclose(a3.value, 1 + 2/60 + 3/3600)\n    assert a3.unit == u.hourangle\n\n    # Test parsing with different separators\n    a4 = Angle('1\u00b02\u20323\u2033')\n    assert_allclose(a4.value, 1 + 2/60 + 3/3600)\n    assert a4.unit == u.degree\n\n    # Test parsing with direction suffix\n    a5 = Angle('1\u00b02\u20323\u2033N')\n    assert_allclose(a5.value, 1 + 2/60 + 3/3600)\n    assert a5.unit == u.degree\n\n    # Test parsing with decimal degrees\n    a6 = Angle('12.3456d')\n    assert_allclose(a6.value, 12.3456)\n    assert a6.unit == u.degree\n\n    # Test parsing with degree, minute, second format\n    a7 = Angle('1d2m3.4s')\n    assert_allclose(a7.value, 1 + 2/60 + 3.4/3600)\n    assert a7.unit == u.degree\n", "def test_angle_signed_dms():\n    \"\"\"\n    Test signed_dms property of Angle objects.\n    \"\"\"\n    a1 = Angle([-90, -45.5, 45.5], unit=u.degree)\n    sign, d, m, s = a1.signed_dms\n    npt.assert_almost_equal(sign, [-1, -1, 1])\n    npt.assert_almost_equal(d, [90, 45, 45])\n    npt.assert_almost_equal(m, [0, 30, 30])\n    npt.assert_almost_equal(s, [0, 0, 0])\n\n    a2 = Angle([-12, 34.5], unit=u.degree)\n    sign, d, m, s = a2.signed_dms\n    npt.assert_almost_equal(sign, [-1, 1])\n    npt.assert_almost_equal(d, [12, 34])\n    npt.assert_almost_equal(m, [0, 30])\n    npt.assert_almost_equal(s, [0, 0])\n", "def test_angle_edge_cases():\n    \"\"\"\n    Test Angle edge cases.\n    \"\"\"\n    # Test creation with NaN and inf values\n    a = Angle([np.nan, np.inf, -np.inf], unit=u.deg)\n    assert np.isnan(a.value[0])\n    assert np.isinf(a.value[1])\n    assert np.isinf(a.value[2])\n\n    # Test wrapping at 360 degrees\n    a = Angle(360 * u.deg)\n    assert a.wrap_at(360 * u.deg).value == 0\n\n    # Test wrapping at 180 degrees\n    a = Angle(180 * u.deg)\n    assert a.wrap_at(180 * u.deg).value == -180\n\n    # Test signed dms\n    a = Angle(-30 * u.deg)\n    assert a.signed_dms.sign == -1\n    assert a.signed_dms.d == 30\n    assert a.signed_dms.m == 0\n    assert a.signed_dms.s == 0\n\n    # Test to_string with precision\n    a = Angle(12.3456 * u.deg)\n    assert a.to_string(precision=2) == '12.35\u00b0'\n\n    # Test to_string with decimal=True\n    a = Angle(12.3456 * u.deg)\n    assert a.to_string(decimal=True) == '12.3456\u00b0'\n\n    # Test to_string with format='latex'\n    a = Angle(12.3456 * u.deg)\n    assert a.to_string(format='latex') == r'$12.3456^{\\circ}$'\n"], "sample_7": ["def test_column_init_quantity_maskedarray():\n    \"\"\"Test initializing a Column with a Quantity and MaskedArray.\"\"\"\n    data = np.ma.array([1, 2, 3], mask=[True, False, True])\n    quantity = u.Quantity(data, unit='m')\n    column = table.Column(quantity)\n    assert np.all(column.data == data)\n    assert column.unit == 'm'\n", "def test_column_info_merge():\n    \"\"\"Test ColumnInfo.merge()\"\"\"\n    c1 = table.Column([1, 2], name='a', unit='m', format='%i',\n                      description='test column', meta={'c': 8, 'd': 12})\n    c2 = table.Column([3, 4], name='b', unit='cm', format='%g',\n                      description='another test column', meta={'e': 9, 'f': 13})\n\n    # Merge with another column\n    attrs = c1.info.merge(c2.info)\n    assert attrs['name'] == 'a'  # name is not merged\n    assert attrs['unit'] == 'm'  # unit is not merged\n    assert attrs['format'] == '%i'  # format is not merged\n    assert attrs['description'] == 'test column'  # description is not merged\n    assert attrs['meta'] == {'c': 8, 'd': 12}  # meta is not merged\n\n    # Merge with a dictionary of attributes\n    attrs = c1.info.merge({'unit': 'km', 'format': '%f', 'description': 'merged',\n                           'meta': {'x': 1, 'y': 2}})\n    assert attrs['name'] == 'a'  # name is not merged\n    assert attrs['unit'] == 'km'\n    assert attrs['format'] == '%f'\n    assert attrs['description'] == 'merged'\n    assert attrs['meta'] == {'c': 8, 'd': 12, 'x': 1, 'y': 2}\n\n    # Merge with another column and a dictionary of attributes\n    attrs = c1.info.merge(c2.info, {'unit': 'km', 'format': '%f', 'description': 'merged',\n                                    'meta': {'x': 1, 'y': 2}})\n    assert attrs['name'] == 'a'  # name is not merged\n    assert attrs['unit'] == 'km'\n    assert attrs['format'] == '%f'\n    assert attrs['description'] == 'merged'\n    assert attrs['meta'] == {'c': 8, 'd': 12, 'x': 1, 'y': 2}\n", "def test_col_copy_indices():\n    \"\"\"Test that Column copy method correctly handles the 'copy_indices' argument\"\"\"\n    c = table.Column([1, 2, 3], name='a', dtype='i4')\n    c.indices = ['index1', 'index2']\n\n    c_copy = c.copy()\n    assert c_copy.indices == ['index1', 'index2']\n\n    c_copy_no_indices = c.copy(copy_indices=False)\n    assert c_copy_no_indices.indices == []\n", "def test_column_info_new_like():\n    col = table.Column([1, 2, 3], name='a', unit='m', dtype='f8')\n    new_col = col.info.new_like([col], length=5)\n    assert isinstance(new_col, table.Column)\n    assert len(new_col) == 5\n    assert new_col.name == 'a'\n    assert new_col.unit == u.m\n    assert new_col.dtype == np.float64\n", "def test_column_init_from_quantity_with_fill_value():\n    \"\"\"Test that creating a Column from a Quantity with a fill value works as expected.\"\"\"\n    q = u.Quantity([1, 2, 3], unit='m', dtype='float64')\n    c = table.Column(q, fill_value=np.nan)\n    assert np.all(c.data == q.value)\n    assert c.unit == q.unit\n    assert c.fill_value == np.nan\n"], "sample_8": ["    def test_masked_array_with_numpy_median(self):\n        \"\"\"Check that numpy's median function works correctly with MaskedArray.\"\"\"\n        np_ma = self.ma.filled(np.nan)\n        result = np.nanmedian(np_ma)\n        expected = np.median(self.a[~self.mask_a])\n        assert result == expected\n", "def test_masked_array_to_numpy_ma(self):\n    \"\"\"Check that we can convert a MaskedArray to a np.ma.MaskedArray.\"\"\"\n    np_ma = self.ma.to_numpy_ma()\n    assert type(np_ma) is np.ma.MaskedArray\n    assert type(np_ma.data) is np.ndarray\n    assert type(np_ma.mask) is np.ndarray\n    assert_array_equal(np_ma.data, self.a)\n    assert_array_equal(np_ma.mask, self.mask_a)\n", "    def test_asarray(self):\n        np_ma = np.asarray(self.ma)\n        assert type(np_ma) is MaskedNDArray\n        assert_array_equal(np_ma.unmasked, self.a)\n        assert_array_equal(np_ma.mask, self.mask_a)\n", "    def test_masked_array_from_ndarray(self):\n        \"\"\"Check that we can initialize a MaskedArray from an NDArray.\"\"\"\n        from astropy.nddata import NDArray\n        nda = NDArray(self.a)\n        ma = Masked(nda, mask=self.mask_a)\n        assert type(ma) is MaskedNDArray\n        assert type(ma.unmasked) is NDArray\n        assert_array_equal(ma.unmasked.data, self.a)\n        assert_array_equal(ma.mask, self.mask_a)\n", "    def test_numpy_sum(self):\n        \"\"\"Check that numpy functions work with masked arrays.\"\"\"\n        result = np.sum(self.ma)\n        expected_data = np.sum(self.a)\n        expected_mask = self.ma.mask.any()\n        assert_array_equal(result.unmasked, expected_data)\n        assert_array_equal(result.mask, expected_mask)\n"], "sample_9": ["def test_write_table_html_js_css_files():\n    \"\"\"\n    Test that passing jsfiles and cssfiles in htmldict should include\n    corresponding tags in the output HTML.\n    \"\"\"\n\n    col1 = [1, 2]\n    col2 = [1.234567e-11, -9.876543e11]\n    table = Table([col1, col2], names=('C1', 'C2'))\n\n    expected = \"\"\"\\", "def test_write_table_html_css():\n    \"\"\"\n    Test to make sure that the HTML writer includes CSS styling.\n    \"\"\"\n\n    col1 = [1, 2]\n    col2 = [1.234567e-11, -9.876543e11]\n    table = Table([col1, col2], names=('C1', 'C2'))\n\n    expected = \"\"\"\\", "def test_write_table_html_js_files():\n    \"\"\"\n    Test to make sure that the HTML writer writes out using the\n    supplied jsfiles.\n    \"\"\"\n\n    col1 = [1, 2]\n    col2 = [1.234567e-11, -9.876543e11]\n    table = Table([col1, col2], names=(\"C1\", \"C2\"))\n\n    expected = \"\"\"\\", "def test_write_table_js_and_css():\n    \"\"\"\n    Test that passing js and css files or code should include them in the output HTML.\n    \"\"\"\n    buffer_output = StringIO()\n    t = Table([[1], [2]], names=('a', 'b'))\n\n    ascii.write(t, buffer_output, format='html',\n                htmldict={'jsfiles': ['file1.js', 'file2.js'],\n                          'cssfiles': ['file1.css', 'file2.css'],\n                          'js': 'console.log(\"Hello World!\");',\n                          'css': 'body {background-color: #f2f2f2;}'})\n    \n    expected = \"\"\"\\", "def test_write_table_html_css_files():\n    \"\"\"\n    Test to make sure that the HTML writer writes out using the\n    supplied CSS files.\n    \"\"\"\n\n    col1 = [1, 2]\n    col2 = [1.234567e-11, -9.876543e11]\n    table = Table([col1, col2], names=('C1', 'C2'))\n\n    expected = \"\"\"\\"], "sample_11": ["def test_combine_slices_edge_cases():\n    with pytest.raises(ValueError):\n        combine_slices(slice(1, 10, 2), slice(None))\n\n    with pytest.raises(ValueError):\n        combine_slices(slice(None), slice(1, 10, 2))\n\n    assert combine_slices(slice(5, 10), 3) == 8\n\n    assert combine_slices(slice(5, 10), slice(None)) == slice(5, 10)\n\n    assert combine_slices(slice(None), slice(5, 10)) == slice(5, 10)\n", "def test_sanitize_slices_invalid_input():\n    with pytest.raises(ValueError):\n        sanitize_slices((1, 2, 3), 2)\n\n    with pytest.raises(IndexError):\n        sanitize_slices((1, [2, 3]), 3)\n\n    with pytest.raises(IndexError):\n        sanitize_slices((slice(1, 2, 3),), 1)\n\n    with pytest.raises(IndexError):\n        sanitize_slices((Ellipsis, Ellipsis), 3)\n", "def test_dropped_dimensions_with_time(cube_4d_fitswcs):\n\n    sub = SlicedLowLevelWCS(cube_4d_fitswcs, np.s_[12, :, 5, 5])\n\n    dwd = sub.dropped_world_dimensions\n    wao_classes = dwd.pop(\"world_axis_object_classes\")\n    wao_components = dwd.pop(\"world_axis_object_components\")\n\n    validate_info_dict(dwd, {\n        \"value\": [1.e+10, 5.e+00],\n        \"world_axis_physical_types\": [\"em.freq\", \"time\"],\n        \"world_axis_names\": [\"Frequency\", \"Time\"],\n        \"world_axis_units\": [\"Hz\", \"s\"],\n        \"serialized_classes\": False,\n        })\n\n    assert wao_components[0][0:2] == ('spectral', 0)\n    assert wao_components[1][0] == 'time'\n    assert wao_components[1][1] == 0\n\n    assert wao_classes['spectral'][0:3] == (u.Quantity, (), {})\n    assert wao_classes['time'][0:3] == (Time, (), {})\n", "def test_sliced_low_level_wcs_repr():\n    wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n    assert repr(wcs) == str(wcs)\n", "def test_combine_slices_with_integer_slice():\n    # Test combine_slices with integer slice as the second argument\n    assert combine_slices(slice(1, 10), 5) == 6\n    assert combine_slices(slice(1, 10, 2), 5) == ValueError('Only slices with steps of 1 are supported')\n    assert combine_slices(slice(None, 10), 5) == 5\n    assert combine_slices(slice(1, None), 5) == 6\n"], "sample_12": ["def test_longitude_wrap_angle():\n    # Test that Longitude's wrap_angle is properly set when creating a new Longitude object\n    lon = Longitude(10, u.deg)\n    assert lon.wrap_angle == 360 * u.deg\n\n    lon = Longitude(10, u.deg, wrap_angle=180 * u.deg)\n    assert lon.wrap_angle == 180 * u.deg\n\n    # Test that Longitude's wrap_angle can be changed after creation\n    lon = Longitude(10, u.deg)\n    lon.wrap_angle = 180 * u.deg\n    assert lon.wrap_angle == 180 * u.deg\n\n    # Test that Longitude's wrap_angle is propagated when creating a new Longitude object from an existing one\n    lon1 = Longitude(10, u.deg, wrap_angle=180 * u.deg)\n    lon2 = Longitude(lon1)\n    assert lon2.wrap_angle == 180 * u.deg\n\n    # Test that Longitude's wrap_angle is not propagated when creating a new Angle object from a Longitude object\n    lon = Longitude(10, u.deg, wrap_angle=180 * u.deg)\n    angle = Angle(lon)\n    assert not hasattr(angle, 'wrap_angle')\n", "def test_angle_to_string_precision():\n    # Test that the precision argument in to_string works correctly\n    angle = Angle(1.23456789, unit=u.degree)\n    assert angle.to_string(precision=0) == '1d14m04s'\n    assert angle.to_string(precision=1) == '1d14m04.6s'\n    assert angle.to_string(precision=2) == '1d14m04.57s'\n    assert angle.to_string(precision=8) == '1d14m04.56789000s'\n    # Test that it also works for arrays\n    angles = Angle([1.23456789, 9.87654321], unit=u.degree)\n    assert np.all(angles.to_string(precision=2) == ['1d14m04.57s', '9d52m35.55s'])\n", "def test_angle_to_string_format():\n    \"\"\"\n    Test that the to_string method works correctly with different formats.\n    \"\"\"\n    a = Angle(1.2345, unit=u.deg)\n\n    # Test latex format\n    assert a.to_string(format='latex') == r'$1^\\circ14{}^\\prime04.2{}^{\\prime\\prime}$'\n\n    # Test unicode format\n    assert a.to_string(format='unicode') == '1\u00b014\u203204.2\u2033'\n\n    # Test default format\n    assert a.to_string() == '1d14m04.2s'\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the to_string method of Angle correctly handles precision\n    for different units.\n    \"\"\"\n    a = Angle('1d2m3.456s')\n\n    # Test degree, arcminute, and arcsecond\n    assert a.to_string(unit=u.deg, precision=2) == '1d02m03.46s'\n    assert a.to_string(unit=u.arcmin, precision=2) == '62.056arcmin'\n    assert a.to_string(unit=u.arcsec, precision=2) == '3723.36arcsec'\n\n    # Test hour, minute, and second\n    a = Angle('1h2m3.456s')\n    assert a.to_string(unit=u.hour, precision=2) == '1h02m03.46s'\n    assert a.to_string(unit=u.minute, precision=2) == '62.056min'\n    assert a.to_string(unit=u.second, precision=2) == '3723.36s'\n\n    # Test radian\n    a = Angle(np.pi/4, u.rad)\n    assert a.to_string(unit=u.rad, precision=4) == '0.7854rad'\n", "def test_angle_formatting_with_unicode():\n    \"\"\"\n    Tests string formatting for Angle objects with unicode\n    \"\"\"\n\n    angle = Angle(\"54.12412\", unit=u.degree)\n\n    # __str__ is the default `format`\n    assert str(angle) == angle.to_string(format='unicode')\n\n    res = '54\u00b007\u203226.832\u2033'\n    assert f\"{angle.to_string(unit=u.degree, format='unicode')}\" == res\n\n    res = '-54\u00b007\u203226.832\u2033'\n    assert f\"{Angle(-54.12412, unit=u.degree).to_string(unit=u.degree, format='unicode')}\" == res\n\n    res = '-0\u00b000\u203210\u2033'\n    assert f\"{Angle('-00:00:10', u.deg).to_string(unit=u.degree, format='unicode')}\" == res\n\n    res = '-0\u00b010\u203200\u2033'\n    assert f\"{Angle('-00:10', u.deg).to_string(unit=u.degree, format='unicode')}\" == res\n\n    res = '3h36m29.7888s'\n    assert f\"{angle.to_string(unit=u.hour, format='unicode')}\" == res\n"], "sample_13": ["def test_angle_to_string_format():\n    # Test to_string with format parameter\n    angle = Angle(1.2345, unit=u.deg)\n    assert angle.to_string(format='latex') == r'$1^\\circ14{}^\\prime04.2{}^{\\prime\\prime}$'\n    assert angle.to_string(format='unicode') == '1\u00b014\u203204.2\u2033'\n\n    angle = Angle(1.2345, unit=u.hour)\n    assert angle.to_string(format='latex') == r'$1^{\\mathrm{h}}14^{\\mathrm{m}}04.2^{\\mathrm{s}}$'\n    assert angle.to_string(format='unicode') == '1\u02b014\u1d5004.2\u02e2'\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the precision argument in to_string works as expected\n    \"\"\"\n    a = Angle(1.23456789, unit=u.deg)\n    assert a.to_string(precision=0) == '1d14m04s'\n    assert a.to_string(precision=1) == '1d14m04.6s'\n    assert a.to_string(precision=2) == '1d14m04.57s'\n    assert a.to_string(precision=3) == '1d14m04.568s'\n    assert a.to_string(precision=4) == '1d14m04.5679s'\n    assert a.to_string(precision=5) == '1d14m04.56789s'\n", "def test_angle_to_string_with_format():\n    a = Angle(1.234, u.deg)\n    assert a.to_string(format='latex') == r'$1^\\circ14{}^\\prime02{}^{\\prime\\prime}$'\n    assert a.to_string(format='unicode') == '1\u00b014\u203202\u2033'\n\n    a = Angle(-1.234, u.deg)\n    assert a.to_string(format='latex') == r'$-1^\\circ14{}^\\prime02{}^{\\prime\\prime}$'\n    assert a.to_string(format='unicode') == '-1\u00b014\u203202\u2033'\n\n    a = Angle(1.234, u.hour)\n    assert a.to_string(format='latex') == r'$1^{\\mathrm{h}}14^{\\mathrm{m}}02^{\\mathrm{s}}$'\n    assert a.to_string(format='unicode') == '1\u02b014\u1d5002\u02e2'\n\n    a = Angle(-1.234, u.hour)\n    assert a.to_string(format='latex') == r'$-1^{\\mathrm{h}}14^{\\mathrm{m}}02^{\\mathrm{s}}$'\n    assert a.to_string(format='unicode') == '-1\u02b014\u1d5002\u02e2'\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the precision argument in Angle.to_string works as expected.\n    \"\"\"\n    a = Angle('10:20:30.12345678d')\n    assert a.to_string(precision=8) == '10d20m30.12345678s'\n    assert a.to_string(precision=4) == '10d20m30.1235s'\n    assert a.to_string(precision=0) == '10d20m30s'\n\n    # Test with an array of angles\n    a = Angle(['10:20:30.12345678d', '11:21:31.12345678d'])\n    assert np.all(a.to_string(precision=8) == ['10d20m30.12345678s', '11d21m31.12345678s'])\n    assert np.all(a.to_string(precision=4) == ['10d20m30.1235s', '11d21m31.1235s'])\n    assert np.all(a.to_string(precision=0) == ['10d20m30s', '11d21m31s'])\n", "def test_angle_format_roundtripping_with_precision():\n    \"\"\"\n    Ensures that the string representation of an angle can be used to create a\n    new valid Angle, with various precision levels.\n    \"\"\"\n\n    a1 = Angle(0.54321, unit=u.degree)\n    for precision in range(10):\n        a2 = Angle(a1.to_string(precision=precision))\n        assert_allclose(a1.degree, a2.degree)\n\n    # also check Longitude/Latitude\n    ra = Longitude('1h2m3.4567s')\n    dec = Latitude('1d2m3.4567s')\n\n    for precision in range(10):\n        assert_allclose(Longitude(ra.to_string(precision=precision)).degree, ra.degree)\n        assert_allclose(Latitude(dec.to_string(precision=precision)).degree, dec.degree)\n"], "sample_14": ["def test_angle_pickle():\n    \"\"\"\n    Test that Angle objects can be pickled and unpickled correctly.\n    \"\"\"\n    a = Angle('10:20:30', unit=u.degree)\n    b = pickle.loads(pickle.dumps(a))\n    assert a == b\n    assert type(b) is Angle\n\n    lon = Longitude('10:20:30', unit=u.degree)\n    lon_unpickled = pickle.loads(pickle.dumps(lon))\n    assert lon == lon_unpickled\n    assert type(lon_unpickled) is Longitude\n\n    lat = Latitude('40:30:20', unit=u.degree)\n    lat_unpickled = pickle.loads(pickle.dumps(lat))\n    assert lat == lat_unpickled\n    assert type(lat_unpickled) is Latitude\n", "def test_angle_to_string_precision():\n    \"\"\"\n    Test that the to_string method of Angle correctly handles precision.\n    \"\"\"\n    a = Angle('1d2m3.456789s')\n    assert a.to_string(precision=0) == '1d02m03s'\n    assert a.to_string(precision=1) == '1d02m03.5s'\n    assert a.to_string(precision=2) == '1d02m03.46s'\n    assert a.to_string(precision=3) == '1d02m03.457s'\n    assert a.to_string(precision=4) == '1d02m03.4568s'\n    assert a.to_string(precision=5) == '1d02m03.45679s'\n", "def test_pickle_longitude_latitude():\n    \"\"\"\n    Test pickling of Longitude and Latitude objects\n    \"\"\"\n    lon = Longitude('10d')\n    lat = Latitude('45d')\n\n    # Test pickle protocol 0\n    lon_pickled = pickle.dumps(lon, protocol=0)\n    lat_pickled = pickle.dumps(lat, protocol=0)\n\n    lon_unpickled = pickle.loads(lon_pickled)\n    lat_unpickled = pickle.loads(lat_pickled)\n\n    assert isinstance(lon_unpickled, Longitude)\n    assert isinstance(lat_unpickled, Latitude)\n\n    assert lon_unpickled == lon\n    assert lat_unpickled == lat\n\n    # Test pickle protocol 4 (latest available in Python 3.8+)\n    lon_pickled = pickle.dumps(lon, protocol=4)\n    lat_pickled = pickle.dumps(lat, protocol=4)\n\n    lon_unpickled = pickle.loads(lon_pickled)\n    lat_unpickled = pickle.loads(lat_pickled)\n\n    assert isinstance(lon_unpickled, Longitude)\n    assert isinstance(lat_unpickled, Latitude)\n\n    assert lon_unpickled == lon\n    assert lat_unpickled == lat\n", "def test_angle_info():\n    # Test LongitudeInfo class.\n    lon = Longitude(10, 'deg')\n    info = lon.info\n    assert isinstance(info, LongitudeInfo)\n    assert isinstance(info, u.QuantityInfo)\n\n    # Check wrap_angle is included in _represent_as_dict_attrs\n    assert 'wrap_angle' in info._represent_as_dict_attrs\n\n    # Check wrap_angle is represented as a dictionary\n    rep = info._represent_as_dict()\n    assert 'wrap_angle' in rep\n    assert rep['wrap_angle'] == 360.0\n\n    # Check default wrap_angle is used if not set.\n    lon2 = Longitude(10, 'deg')\n    del lon2._wrap_angle\n    rep = lon2.info._represent_as_dict()\n    assert 'wrap_angle' in rep\n    assert rep['wrap_angle'] == 360.0\n", "def test_angle_to_string_decimal():\n    a = Angle(10.2345 * u.deg)\n    assert a.to_string(decimal=True) == \"10.2345\"\n    a = Angle(10.2345 * u.hour)\n    assert a.to_string(decimal=True) == \"10.2345\"\n    a = Angle(10.2345 * u.rad)\n    assert a.to_string(decimal=True) == \"10.2345\"\n\n    # Test with precision argument\n    a = Angle(10.23456789 * u.deg)\n    assert a.to_string(decimal=True, precision=4) == \"10.2346\"\n    a = Angle(10.23456789 * u.hour)\n    assert a.to_string(decimal=True, precision=4) == \"10.2346\"\n    a = Angle(10.23456789 * u.rad)\n    assert a.to_string(decimal=True, precision=4) == \"10.2346\"\n"], "sample_15": ["    def test_quantity_info(self):\n        q = u.Quantity(10, u.m)\n        info = q.info\n\n        assert isinstance(info, u.QuantityInfo)\n\n        assert info.dtype == q.dtype\n        assert info.unit == q.unit\n\n        new_dtype = np.float32\n        info.dtype = new_dtype\n        assert q.dtype == new_dtype\n\n        new_unit = u.km\n        info.unit = new_unit\n        assert q.unit == new_unit\n\n        assert info.name is None\n        info.name = \"length\"\n        assert info.name == \"length\"\n\n        assert info.description is None\n        info.description = \"some description\"\n        assert info.description == \"some description\"\n\n        assert info.format is None\n        info.format = \"%0.2f\"\n        assert info.format == \"%0.2f\"\n", "    def test_indexing_keeps_class(self):\n        q = np.arange(10.0) * u.m\n        assert isinstance(q[0], u.Quantity)\n        assert q[0].unit == u.m\n", "    def test_get_item_with_quantity(self):\n        q = np.array([1.0, 2.0, 3.0]) * u.m\n        assert q[0].unit == u.m\n        assert np.all(q[[0, 1]].unit == u.m)\n", "    def test_quantity_structured(self):\n        # regression test for #6053\n        dt = np.dtype([('x', float), ('y', float)])\n        q = np.array([(1.0, 2.0), (3.0, 4.0)], dtype=dt) * u.m\n        assert q.unit == u.m\n", "    def test_array_equal(self):\n        q1 = np.array([1.0, 2.0, 3.0]) * u.m\n        q2 = np.array([1.0, 2.0, 4.0]) * u.m\n        assert np.array_equal(q1, q1)\n        assert not np.array_equal(q1, q2)\n"], "sample_16": ["    def test_structured_to_unstructured(self):\n        structured = u.Quantity([(1, 2), (3, 4)], dtype=[(\"a\", \"f8\"), (\"b\", \"f8\")])\n        unstructured = rfn.structured_to_unstructured(structured)\n        assert_array_equal(unstructured, [1, 2, 3, 4] * u.one)\n", "    def setup_class(cls):\n        cls.pv_dtype = np.dtype([(\"p\", \"f8\"), (\"v\", \"f8\")])\n        cls.pv_t_dtype = np.dtype(\n            [(\"pv\", np.dtype([(\"pp\", \"f8\"), (\"vv\", \"f8\")])), (\"t\", \"f8\")]\n        )\n\n        cls.pv = np.array([(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)], cls.pv_dtype)\n        cls.pv_t = np.array(\n            [((4.0, 2.5), 0.0), ((5.0, 5.0), 1.0), ((6.0, 7.5), 2.0)], cls.pv_t_dtype\n        )\n\n        cls.pv_unit = u.StructuredUnit((u.km, u.km / u.s), (\"p\", \"v\"))\n        cls.pv_t_unit = u.StructuredUnit((cls.pv_unit, u.s), (\"pv\", \"t\"))\n\n        cls.q_pv = cls.pv << cls.pv_unit\n        cls.q_pv_t = cls.pv_t << cls.pv_t_unit\n", "    def test_structured_to_unstructured(self):\n        # Create a structured Quantity with units of km and km/s\n        pv_dtype = np.dtype([(\"p\", \"f8\"), (\"v\", \"f8\")])\n        pv_unit = u.StructuredUnit((u.km, u.km / u.s), (\"p\", \"v\"))\n        q_pv = np.array([(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)], pv_dtype) << pv_unit\n\n        # Convert the structured Quantity to an unstructured one\n        unstruct = rfn.structured_to_unstructured(q_pv)\n\n        # Check that the resulting Quantity has the correct units and values\n        assert_array_equal(unstruct, [[1, 0.25], [2, 0.5], [3, 0.75]] * u.km)\n", "    def setup_method(self):\n        self.q1 = np.arange(6.0).reshape(2, 3) * u.m\n        self.q2 = np.array([4.0, 5.0, 6.0]) / u.s\n", "    def test_merge_arrays_empty(self):\n        assert rfn.merge_arrays([]).shape == (0,)\n"], "sample_17": ["    def setup_method(self):\n        self.pv_dtype = np.dtype([(\"p\", \"f8\"), (\"v\", \"f8\")])\n        self.pv_t_dtype = np.dtype(\n            [(\"pv\", np.dtype([(\"pp\", \"f8\"), (\"vv\", \"f8\")])), (\"t\", \"f8\")]\n        )\n\n        self.pv = np.array([(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)], self.pv_dtype)\n        self.pv_t = np.array(\n            [((4.0, 2.5), 0.0), ((5.0, 5.0), 1.0), ((6.0, 7.5), 2.0)], self.pv_t_dtype\n        )\n\n        self.pv_unit = u.StructuredUnit((u.km, u.km / u.s), (\"p\", \"v\"))\n        self.pv_t_unit = u.StructuredUnit((self.pv_unit, u.s), (\"pv\", \"t\"))\n\n        self.q_pv = self.pv << self.pv_unit\n        self.q_pv_t = self.pv_t << self.pv_t_unit\n", "    def test_structured_to_unstructured(self):\n        pv_dtype = np.dtype([(\"p\", \"f8\"), (\"v\", \"f8\")])\n        pv_unit = u.StructuredUnit((u.km, u.km / u.s), (\"p\", \"v\"))\n        q_pv = np.array([(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)], pv_dtype) << pv_unit\n\n        unstruct = rfn.structured_to_unstructured(q_pv)\n        assert_array_equal(unstruct, [[1.0, 0.25], [2.0, 0.5], [3.0, 0.75]] * u.km)\n\n        # Ensure units are applied to sub-arrays when necessary\n        pv_t_dtype = np.dtype(\n            [(\"pv\", np.dtype([(\"pp\", \"f8\"), (\"vv\", \"f8\")])), (\"t\", \"f8\")]\n        )\n        pv_t_unit = u.StructuredUnit((pv_unit, u.s), (\"pv\", \"t\"))\n        q_pv_t = np.array(\n            [((4.0, 2.5), 0.0), ((5.0, 5.0), 1.0), ((6.0, 7.5), 2.0)], pv_t_dtype\n        ) << pv_t_unit\n\n        unstruct = rfn.structured_to_unstructured(q_pv_t)\n        assert_array_equal(\n            unstruct,\n            [[[4.0, 2.5], [0.0]], [[5.0, 5.0], [1.0]], [[6.0, 7.5], [2.0]]] * u.km,\n        )\n", "    def test_einsum_path(self):\n        q1 = np.arange(9.0).reshape(3, 3) * u.m\n        p1 = np.einsum_path(\"...i\", q1)\n        assert len(p1[0]) == 1\n        assert p1[1] == {}\n        \n        p2 = np.einsum_path(\"ii\", q1)\n        assert len(p2[0]) == 1\n        assert p2[1] == {}\n        \n        q2 = np.eye(3) / u.s\n        p3 = np.einsum_path(\"ij,jk\", q1, q2)\n        assert len(p3[0]) == 2\n        assert p3[1] == {}\n", "    def test_structured_to_unstructured(self):\n        # Create a structured quantity with compatible units\n        struct = u.Quantity([(1, 2, 3)], u.Unit(\"(m, m, m)\"))\n        unstruct = rfn.structured_to_unstructured(struct)\n        assert_array_equal(unstruct, [1, 2, 3] * u.m)\n\n        # Create a structured quantity with incompatible units\n        struct = u.Quantity([(1, 2, 3)], u.Unit(\"(m, s, m)\"))\n        with pytest.raises(u.UnitConversionError, match=\"'m'\"):\n            rfn.structured_to_unstructured(struct)\n", "    def setup_class(self):\n        self.pv_dtype = np.dtype([(\"p\", \"f8\"), (\"v\", \"f8\")])\n        self.pv_t_dtype = np.dtype(\n            [(\"pv\", np.dtype([(\"pp\", \"f8\"), (\"vv\", \"f8\")])), (\"t\", \"f8\")]\n        )\n\n        self.pv = np.array([(1.0, 0.25), (2.0, 0.5), (3.0, 0.75)], self.pv_dtype)\n        self.pv_t = np.array(\n            [((4.0, 2.5), 0.0), ((5.0, 5.0), 1.0), ((6.0, 7.5), 2.0)], self.pv_t_dtype\n        )\n\n        self.pv_unit = u.StructuredUnit((u.km, u.km / u.s), (\"p\", \"v\"))\n        self.pv_t_unit = u.StructuredUnit((self.pv_unit, u.s), (\"pv\", \"t\"))\n\n        self.q_pv = self.pv << self.pv_unit\n        self.q_pv_t = self.pv_t << self.pv_t_unit\n"], "sample_18": ["    def test_np_sum(self):\n        q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        q.info.name = \"v\"\n        q.info.description = \"air speed of a african swallow\"\n        sum_q = np.sum(q)\n        assert_info_equal(sum_q, q)\n", "    def setup_class(self):\n        self.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        self.q.info.name = \"v\"\n        self.q.info.description = \"air speed of a african swallow\"\n", "    def setup_class(self):\n        self.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n", "    def setup_class(self):\n        self.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        self.q.info.name = \"v\"\n        self.q.info.description = \"air speed of a african swallow\"\n", "    def test_broadcast_to(self):\n        q = u.Quantity(np.arange(3), \"m\")\n        q2 = q.broadcast_to((3, 3))\n        assert q2.shape == (3, 3)\n        assert np.all(q2.value == np.broadcast_to(q.value, (3, 3)))\n        assert q2.unit == q.unit\n"], "sample_20": ["def test_fits_with_astropy_time(tmp_path):\n    from astropy.time import Time\n\n    filename = tmp_path / \"test.fits\"\n    t = Table()\n    t[\"time\"] = Time([\"2022-01-01T00:00:00\", \"2022-01-02T00:00:00\"])\n    t.write(filename, format=\"fits\")\n\n    t2 = Table.read(filename, format=\"fits\", astropy_native=True)\n    assert isinstance(t2[\"time\"], Time)\n    assert np.all(t2[\"time\"].jd == t[\"time\"].jd)\n", "def test_encode_mixins_empty(tmp_path):\n    \"\"\"Test encoding an empty table with mixin columns\"\"\"\n    filename = tmp_path / \"test.fits\"\n    t = Table()\n    t[\"a\"] = Time([], format=\"jd\")\n    t.write(filename)\n    t2 = Table.read(filename, astropy_native=True)\n    assert len(t2) == 0\n    assert isinstance(t2[\"a\"], Time)\n", "def test_read_table_fits_invalid_unit_parse_strict(tmp_path):\n    filename = tmp_path / \"test_invalid_unit.fits\"\n    t1 = Table([[1, 2], [3, 4]], names=[\"a\", \"b\"])\n    t1[\"a\"].unit = \"invalid_unit\"\n    t1.write(filename, overwrite=True)\n\n    with pytest.raises(u.UnitScaleError, match=\"Invalid unit 'invalid_unit'\"):\n        Table.read(filename, unit_parse_strict=\"raise\")\n\n    with pytest.warns(\n        u.UnitsWarning, match=\"'invalid_unit' did not parse as fits unit\"\n    ) as w:\n        t2 = Table.read(filename, unit_parse_strict=\"warn\")\n    assert len(w) == 1\n\n    t3 = Table.read(filename, unit_parse_strict=\"silent\")\n    assert np.all(t2 == t3)\n", "def test_is_fits_identifier(tmp_path):\n    filename = tmp_path / \"test.fits\"\n    assert connect.is_fits(filename, None, None) is True\n\n    filename = tmp_path / \"test.fit\"\n    assert connect.is_fits(filename, None, None) is True\n\n    filename = tmp_path / \"test.fts\"\n    assert connect.is_fits(filename, None, None) is True\n\n    filename = tmp_path / \"test.txt\"\n    assert connect.is_fits(filename, None, None) is False\n\n    with open(filename, \"wb\") as f:\n        f.write(b\"SIMPLE  =                    T / file does conform to FITS standard\")\n    assert connect.is_fits(None, filename, open(filename, \"rb\")) is True\n", "def test_read_fits_with_invalid_unit(tmp_path):\n    filename = tmp_path / \"test.fits\"\n    data = np.array([1, 2, 3])\n    hdu = fits.TableHDU(data)\n    hdu.columns[0].unit = \" invalid unit\"\n    hdu.writeto(filename)\n\n    with pytest.warns(u.UnitsWarning, match=\"did not parse as fits unit\"):\n        Table.read(filename)\n"], "sample_21": ["def test_line_type():\n    assert _line_type(\"READ SERR 3\") == \"command\"\n    assert _line_type(\" \\\\n    !some gibberish\") == \"comment\"\n    assert _line_type(\"   \") == \"comment\"\n    assert _line_type(\" 21345.45\") == \"data,1\"\n    assert _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\") == \"data,6\"\n    assert _line_type(\" 21345.45 ! a comment to disturb\") == \"data,1\"\n    assert _line_type(\"NO NO NO NO NO\") == \"new\"\n\n    with pytest.raises(ValueError):\n        _line_type(\"N O N NOON OON O\")\n\n    with pytest.raises(ValueError):\n        _line_type(\" some non-comment gibberish\")\n", "def test_get_lines_from_file(tmp_path):\n    example_qdp = \"\"\"\n    ! Initial comment line 1\n    ! Initial comment line 2\n    READ TERR 1\n    READ SERR 3\n    ! Table 0 comment\n    !a a(pos) a(neg) b c ce d\n    53000.5   0.25  -0.5   1  1.5  3.5 2\n    54000.5   1.25  -1.5   2  2.5  4.5 3\n    NO NO NO NO NO\n    \"\"\"\n\n    path = tmp_path / \"test.qdp\"\n\n    with open(path, \"w\") as fp:\n        print(example_qdp, file=fp)\n\n    lines1 = _get_lines_from_file(fp.name)\n    lines2 = _get_lines_from_file(str(path))\n    lines3 = _get_lines_from_file(example_qdp)\n\n    assert lines1 == lines2 == lines3\n\n    with pytest.raises(ValueError):\n        _get_lines_from_file(12345)\n", "def test_get_lines_from_file(tmp_path):\n    example_qdp = \"\"\"\n    ! Initial comment line 1\n    ! Initial comment line 2\n    READ TERR 1\n    READ SERR 3\n    ! Table 0 comment\n    !a a(pos) a(neg) b c ce d\n    53000.5   0.25  -0.5   1  1.5  3.5 2\n    54000.5   1.25  -1.5   2  2.5  4.5 3\n    NO NO NO NO NO\n    ! Table 1 comment\n    !a a(pos) a(neg) b c ce d\n    54000.5   2.25  -2.5   NO  3.5  5.5 5\n    55000.5   3.25  -3.5   4  4.5  6.5 nan\n    \"\"\"\n    path = tmp_path / \"test.qdp\"\n\n    with open(path, \"w\") as fp:\n        print(example_qdp, file=fp)\n\n    lines = _get_lines_from_file(path)\n    assert lines[0].startswith(\"! Initial comment line 1\")\n\n    lines = _get_lines_from_file(str(path))\n    assert lines[0].startswith(\"! Initial comment line 1\")\n\n    lines = _get_lines_from_file(example_qdp.split(\"\\n\"))\n    assert lines[0].startswith(\"! Initial comment line 1\")\n\n    with pytest.raises(ValueError):\n        _get_lines_from_file(123)\n", "def test_get_lines_from_file(tmp_path):\n    example_qdp = \"\"\"\n    ! Swift/XRT hardness ratio of trigger: XXXX, name: BUBU X-2\n    ! Columns are as labelled\n    READ TERR 1\n    READ SERR 2\n    ! WT -- hard data\n    !MJD            Err (pos)       Err(neg)        Rate            Error\n    53000.123456 2.37847222222222e-05    -2.37847222222222e-05   NO       0.212439\n    \"\"\"\n\n    path = tmp_path / \"test.qdp\"\n\n    with open(path, \"w\") as fp:\n        print(example_qdp, file=fp)\n\n    lines = _get_lines_from_file(fp.name)\n    assert len(lines) == 7\n\n    lines = _get_lines_from_file(str(example_qdp))\n    assert len(lines) == 7\n\n    lines = _get_lines_from_file(example_qdp.split(\"\\n\"))\n    assert len(lines) == 7\n\n    with pytest.raises(ValueError):\n        _get_lines_from_file(123)\n", "def test_get_lines_from_file(tmp_path):\n    example_qdp = \"\"\"\n    ! Swift/XRT hardness ratio of trigger: XXXX, name: BUBU X-2\n    ! Columns are as labelled\n    READ TERR 1\n    READ SERR 2\n    ! WT -- hard data\n    !MJD            Err (pos)       Err(neg)        Rate            Error\n    53000.123456 2.37847222222222e-05    -2.37847222222222e-05   -0.212439       0.212439\n    \"\"\"\n\n    path = tmp_path / \"test.qdp\"\n\n    with open(path, \"w\") as fp:\n        print(example_qdp, file=fp)\n\n    lines = _get_lines_from_file(fp.name)\n    assert lines[0].startswith(\"! Swift\")\n\n    # Test reading from a string\n    lines = _get_lines_from_file(example_qdp)\n    assert lines[0].startswith(\"! Swift\")\n\n    # Test reading from an iterable\n    lines = _get_lines_from_file(example_qdp.splitlines())\n    assert lines[0].startswith(\"! Swift\")\n"], "sample_22": ["def test_matrix_product():\n    \"\"\"Test the matrix product function.\"\"\"\n    # Test with two 3x3 matrices\n    m1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    m2 = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n    result = matrix_product(m1, m2)\n    expected = np.dot(m1, m2)\n    assert_array_equal(result, expected)\n\n    # Test with three 3x3 matrices\n    m3 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    result = matrix_product(m1, m2, m3)\n    expected = np.dot(np.dot(m1, m2), m3)\n    assert_array_equal(result, expected)\n\n    # Test with non-square matrices\n    m4 = np.array([[1, 2, 3], [4, 5, 6]])\n    m5 = np.array([[7, 8], [9, 10], [11, 12]])\n    result = matrix_product(m4, m5)\n    expected = np.dot(m4, m5)\n    assert_array_equal(result, expected)\n\n    # Test with a single matrix (should return the original matrix)\n    result = matrix_product(m1)\n    assert_array_equal(result, m1)\n", "def test_matrix_product():\n    \"\"\"Test the matrix product function.\"\"\"\n    # Simple case: two 3x3 matrices\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    m2 = rotation_matrix(45 * u.deg, \"y\")\n    result = matrix_product(m1, m2)\n    assert_allclose(result, np.dot(m1, m2))\n\n    # More complex case: three 3x3 matrices\n    m3 = rotation_matrix(90 * u.deg, \"z\")\n    result = matrix_product(m1, m2, m3)\n    assert_allclose(result, np.dot(np.dot(m1, m2), m3))\n\n    # Broadcasting case: (M, 3, 3) and (3, 3)\n    n1 = np.tile(m1, (2, 1, 1))\n    result = matrix_product(n1, m2)\n    assert_allclose(result, np.dot(n1, m2))\n\n    # Broadcasting case: (M, 3, 3) and (M, 3, 3)\n    n2 = np.tile(m2, (2, 1, 1))\n    result = matrix_product(n1, n2)\n    assert_allclose(result, np.dot(n1, n2))\n", "def test_matrix_transpose():\n    \"\"\"Test the matrix transpose function.\"\"\"\n    # Create a 3x3 matrix\n    m1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    # Transpose the matrix\n    m1_t = matrix_transpose(m1)\n\n    # Check that the transpose is correct\n    assert_array_equal(m1_t, m1.T)\n\n    # Create a stack of 2 3x3 matrices\n    n1 = np.stack((m1, m1))\n\n    # Transpose the stack of matrices\n    n1_t = matrix_transpose(n1)\n\n    # Check that the transpose is correct for each matrix in the stack\n    assert_array_equal(n1_t[0], m1.T)\n    assert_array_equal(n1_t[1], m1.T)\n", "def test_matrix_product():\n    # Simple test of matrix product\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    m2 = rotation_matrix(45 * u.deg, \"y\")\n    assert_allclose(matrix_product(m1, m2), np.dot(m1, m2))\n\n    # Test with multiple matrices\n    m3 = rotation_matrix(90 * u.deg, \"z\")\n    assert_allclose(matrix_product(m1, m2, m3), np.dot(np.dot(m1, m2), m3))\n\n    # Test with non-square matrices\n    m4 = np.random.rand(3, 4)\n    m5 = np.random.rand(4, 5)\n    assert_allclose(matrix_product(m4, m5), np.dot(m4, m5))\n\n    # Test with stacks of matrices\n    m6 = np.random.rand(2, 3, 3)\n    m7 = np.random.rand(2, 3, 3)\n    assert_allclose(matrix_product(m6, m7), np.einsum(\"ijk,ikl->ijl\", m6, m7))\n", "def test_matrix_product():\n    \"\"\"Test the matrix product function.\"\"\"\n    # Simple 2x2 matrix multiplication\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    result = matrix_product(m1, m2)\n    assert_allclose(result, np.dot(m1, m2))\n\n    # Matrix multiplication with more than two matrices\n    m3 = np.array([[9, 10], [11, 12]])\n    result = matrix_product(m1, m2, m3)\n    assert_allclose(result, np.dot(np.dot(m1, m2), m3))\n\n    # Matrix multiplication with non-square matrices\n    m4 = np.array([[13, 14, 15], [16, 17, 18]])\n    m5 = np.array([[19, 20], [21, 22], [23, 24]])\n    result = matrix_product(m4, m5)\n    assert_allclose(result, np.dot(m4, m5))\n\n    # Test deprecation warning\n    with pytest.warns(AstropyDeprecationWarning):\n        matrix_product(m1, m2)\n"], "sample_23": ["def test_angle_roundtrip_pickle():\n    \"\"\"\n    Test round-trip pickling of Angle objects\n    \"\"\"\n    a1 = Angle(3.60827466667, unit=u.hour)\n    a2 = Angle(\"54:07:26.832\", unit=u.degree)\n\n    # Pickle and unpickle\n    a1_pickled = pickle.dumps(a1)\n    a1_unpickled = pickle.loads(a1_pickled)\n\n    a2_pickled = pickle.dumps(a2)\n    a2_unpickled = pickle.loads(a2_pickled)\n\n    assert a1 == a1_unpickled\n    assert a2 == a2_unpickled\n\n    # Also test for array angles\n    a3 = Angle([1, 2, 3], unit=u.deg)\n    a3_pickled = pickle.dumps(a3)\n    a3_unpickled = pickle.loads(a3_pickled)\n\n    assert np.all(a3 == a3_unpickled)\n", "def test_angle_to_string_edge_cases():\n    \"\"\"\n    Tests edge cases for Angle.to_string method\n    \"\"\"\n    # Test that a single angle is returned as a string, not an array of strings\n    angle = Angle(1.0 * u.deg)\n    assert isinstance(angle.to_string(), str)\n\n    # Test that an array of angles is returned as an array of strings\n    angles = Angle([1.0, 2.0] * u.deg)\n    assert isinstance(angles.to_string(), np.ndarray)\n    assert all(isinstance(s, str) for s in angles.to_string())\n\n    # Test that an empty array of angles returns an empty array of strings\n    angles = Angle([] * u.deg)\n    assert isinstance(angles.to_string(), np.ndarray)\n    assert len(angles.to_string()) == 0\n\n    # Test that the sep argument works with arrays of angles\n    angles = Angle([1.0, 2.0] * u.deg)\n    assert all(\":\" in s for s in angles.to_string(sep=\":\"))\n\n    # Test that the precision argument works with arrays of angles\n    angles = Angle([1.0, 2.0] * u.deg)\n    assert all(len(s.split(\":\")[2].split(\".\")[1]) == 4 for s in angles.to_string(precision=4))\n", "def test_angle_pickle():\n    \"\"\"\n    Test pickling Angles and their subclasses.\n    \"\"\"\n    a = Angle(1, u.deg)\n    assert pickle.loads(pickle.dumps(a)) == a\n\n    lon = Longitude(1, u.deg)\n    assert pickle.loads(pickle.dumps(lon)) == lon\n\n    lat = Latitude(1, u.deg)\n    assert pickle.loads(pickle.dumps(lat)) == lat\n", "def test_angle_from_string_with_whitespace():\n    \"\"\"\n    Tests creating an Angle object from a string with whitespace\n    \"\"\"\n    angle = Angle(\" 54.12412 \", unit=u.degree)\n    assert_allclose(angle.degree, 54.12412)\n\n    angle = Angle(\" 54:07:26.832 \", unit=u.degree)\n    assert_allclose(angle.degree, 54.12412)\n\n    angle = Angle(\" 3h 36m 29.7888000120s \")\n    assert_allclose(angle.hour, 3.60827466667)\n", "def test_angle_to_string_decimal():\n    \"\"\"\n    Test that the to_string method with decimal=True returns a string\n    representation of the angle in decimal degrees.\n    \"\"\"\n    a = Angle(\"1h2m3.4s\")\n    assert a.to_string(decimal=True) == \"15.6\"\n    a = Angle(\"1d2m3.4s\")\n    assert a.to_string(decimal=True) == \"1.034722\"\n    a = Angle(\"1h2m3.4s\", unit=u.hour)\n    assert a.to_string(decimal=True, unit=u.deg) == \"15.6\"\n"], "sample_24": ["    def test_eig(self):\n        out = np.linalg.eig(self.ma)\n        expected_w, expected_vl = np.linalg.eig(self.a)\n        assert_array_equal(out[0].unmasked, expected_w)\n        assert_array_equal(out[1].unmasked, expected_vl)\n", "    def setup_class(self):\n        self.a = np.array([1.0, 2.0, 3.0, 4.0])\n        self.mask_a = np.array([False, True, False, False])\n        self.ma = Masked(self.a, mask=self.mask_a)\n        self.b = np.array([0.5, 1.5, 2.5, 3.5])\n        self.mask_b = np.array([False, False, False, True])\n        self.mb = Masked(self.b, mask=self.mask_b)\n", "    def test_eig(self):\n        out = np.linalg.eig(self.ma)\n        expected_w, expected_vl = np.linalg.eig(self.a)\n        expected_mask = np.isnan(expected_w)\n        assert_array_equal(out[0].unmasked, expected_w)\n        assert_array_equal(out[1].mask, expected_mask[:, None])\n        assert_array_equal(out[1].unmasked, expected_vl)\n", "    def test_empty_like(self):\n        # Test creating an empty MaskedArray with a structured dtype.\n        dtype = np.dtype([('a', float), ('b', int)])\n        ma = Masked(np.empty((2, 3), dtype=dtype))\n        o = np.empty_like(ma)\n        assert isinstance(o, Masked)\n        assert o.dtype == dtype\n", "    def setup_class(self):\n        self.a = np.array(\n            [\n                (1.0, 2.0, 3.0),\n                (4.0, 5.0, 6.0),\n            ],\n            dtype=[(\"x\", float), (\"y\", float), (\"z\", float)],\n        )\n        self.mask_a = np.array(\n            [\n                (True, False, True),\n                (False, True, False),\n            ],\n            dtype=[(\"x\", bool), (\"y\", bool), (\"z\", bool)],\n        )\n        self.ma = Masked(self.a, mask=self.mask_a)\n"], "sample_25": ["def test_header_strip_with_commentary_cards(self):\n    \"\"\"\n    Test that stripping a header with commentary cards removes all non-commentary cards.\n    \"\"\"\n\n    header = fits.Header()\n    header[\"FOO\"] = \"BAR\"\n    header.add_comment(\"Comment 1\")\n    header.add_history(\"History 1\")\n    header.add_blank(\"Blank 1\")\n\n    header.strip()\n\n    assert len(header) == 3\n    assert header[0].keyword == \"COMMENT\"\n    assert header[1].keyword == \"HISTORY\"\n    assert header[2].keyword == \"\"\n", "def test_header_strip_with_undefined_value():\n    header = fits.Header([(\"A\", \"B\"), (\"C\", None)])\n    header.strip()\n    assert set(header) == {\"A\", \"C\"}\n    assert header[\"A\"] == \"B\"\n    assert header[\"C\"] is None\n\n    header = fits.Header([(\"A\", \"B\"), (\"C\", fits.card.UNDEFINED)])\n    header.strip()\n    assert set(header) == {\"A\", \"C\"}\n    assert header[\"A\"] == \"B\"\n    assert header[\"C\"] is None\n", "def test_header_fromstring_with_trailing_whitespace(self):\n    \"\"\"Test reading a Header from a string with trailing whitespace.\"\"\"\n\n    header_str = \"SIMPLE  =                    T / file does conform to FITS standard\\n\"\n    header_str += \"BITPIX  =                   16 / number of bits per data pixel\\n\"\n    header_str += \"NAXIS   =                    0 / number of data axes\\n\"\n    header_str += \"EXTEND  =                    T / FITS dataset may contain extensions\\n\"\n    header_str += \"COMMENT   FITS (Flexible Image Transport System) format is defined in 'Astronomy\\n\"\n    header_str += \"COMMENT   and Astrophysics', volume 376, page 359; bibcode: 2001A&A...376..359H\\n\"\n    header_str += \"END\" + \" \" * 77\n\n    header = fits.Header.fromstring(header_str)\n    assert len(header) == 5\n    assert header[\"SIMPLE\"] is True\n    assert header[\"BITPIX\"] == 16\n    assert header[\"NAXIS\"] == 0\n    assert header[\"EXTEND\"] is True\n", "def test_card_repr():\n    \"\"\"Test that the __repr__ method of a Card object returns the expected string.\"\"\"\n    c = fits.Card(\"KEYWORD\", \"value\", \"comment\")\n    expected_repr = repr((c.keyword, c.value, c.comment))\n    assert repr(c) == expected_repr\n\n    c = fits.Card(\"KEYWORD\", 1.0)\n    expected_repr = repr((c.keyword, c.value, c.comment))\n    assert repr(c) == expected_repr\n\n    c = fits.Card(\"KEYWORD\", 1.0, \"comment\")\n    expected_repr = repr((c.keyword, c.value, c.comment))\n    assert repr(c) == expected_repr\n\n    c = fits.Card.fromstring(\"HIERARCH KEYWORD = 1.0 / comment\")\n    expected_repr = repr((c.keyword, c.value, c.comment))\n    assert repr(c) == expected_repr\n", "def test_header_fromstring_bytes_with_continuation_cards(self):\n    \"\"\"\n    Test reading a Header from a `bytes` string with continuation cards.\n\n    See https://github.com/astropy/astropy/issues/8706\n    \"\"\"\n\n    with open(self.data(\"test0.fits\"), \"rb\") as fobj:\n        pri_hdr_from_bytes = fits.Header.fromstring(fobj.read())\n\n    # Create a header with a long string value that will be split into multiple cards\n    hdr = fits.Header()\n    hdr[\"LONGKEY\"] = \"a\" * 70\n\n    # Convert the header to a bytes string\n    hdr_bytes = hdr.tostring().encode(\"ascii\")\n\n    # Read the header back in from the bytes string\n    hdr_from_bytes = fits.Header.fromstring(hdr_bytes)\n\n    assert hdr == hdr_from_bytes\n    assert hdr.tostring() == hdr_from_bytes.tostring()\n"], "sample_26": ["def test_image_hdu_scale_back_uint(tmp_path):\n    \"\"\"\n    Test that when using scale_back=True with an ImageHDU, unsigned integer\n    data is properly scaled back to its original values.\n    \"\"\"\n\n    # Create a sample unsigned integer array\n    data = np.array([0, 1, 2, 3, 4, 5], dtype=np.uint8)\n\n    # Create an ImageHDU from the data and set BSCALE/BZERO\n    hdu = fits.ImageHDU(data=data)\n    hdu.header[\"BSCALE\"] = 2.0\n    hdu.header[\"BZERO\"] = 10\n\n    # Write the HDU to a file\n    filename = tmp_path / \"test.fits\"\n    hdu.writeto(filename)\n\n    # Read the HDU back in with scale_back=True\n    with fits.open(filename, mode=\"update\", scale_back=True) as hdul:\n        hdu_read = hdul[0]\n\n    # Check that the data was scaled back correctly\n    assert_equal(hdu_read.data, data)\n", "def test_image_with_zero_size(tmp_path):\n    # Test that an image with zero size can be written and read correctly.\n\n    hdu = fits.PrimaryHDU(np.empty((0, 0)))\n    filename = tmp_path / \"test.fits\"\n    hdu.writeto(filename)\n\n    with fits.open(filename) as hdul:\n        assert hdul[0].data.shape == (0, 0)\n", "def test_image_hdu_update_header_with_new_data(tmp_path):\n    # Regression test for an issue where updating the header of an ImageHDU\n    # after replacing its data with a new array of a different shape would not\n    # update the NAXIS keywords correctly.\n\n    filename = tmp_path / \"test.fits\"\n\n    hdu = fits.ImageHDU(np.zeros((10, 10)))\n    hdu.writeto(filename)\n\n    with fits.open(filename, mode=\"update\") as hdul:\n        hdul[0].data = np.zeros((20, 20))\n        hdul[0].header[\"TEST\"] = \"test\"\n        hdul.flush()\n\n    with fits.open(filename) as hdul:\n        assert hdul[0].header[\"NAXIS1\"] == 20\n        assert hdul[0].header[\"NAXIS2\"] == 20\n", "def test_image_update_header_uint_scale_back(tmp_path):\n    # Regression test for https://github.com/astropy/astropy/issues/2305\n\n    arr = np.arange(100, dtype=np.uint16)\n    hdu = fits.PrimaryHDU(data=arr)\n    hdu.writeto(tmp_path / \"test.fits\")\n\n    with fits.open(tmp_path / \"test.fits\", mode=\"update\", scale_back=True) as hdul:\n        hdul[0].header[\"BSCALE\"] = 2\n        hdul[0].header[\"BZERO\"] = 32768\n        hdul.flush()\n\n    with fits.open(tmp_path / \"test.fits\", do_not_scale_image_data=True) as hdul:\n        assert hdul[0].header[\"BITPIX\"] == 16\n        assert hdul[0].header[\"BSCALE\"] == 2\n        assert hdul[0].header[\"BZERO\"] == 32768\n        assert hdul[0].data.dtype == np.dtype(\"uint16\")\n", "def test_image_hdu_with_scaled_data(tmp_path):\n    # Regression test for an issue where ImageHDU with scaled data would not be\n    # written correctly to a file.\n\n    data = np.arange(100, dtype=np.uint16)\n    hdu = fits.ImageHDU(data=data)\n\n    filename = tmp_path / \"test.fits\"\n    hdu.writeto(filename)\n\n    with fits.open(filename) as hdul:\n        assert hdul[0].data.dtype == np.uint16\n        assert np.all(hdul[0].data == data)\n\n    # Repeat the test with CompImageHDU\n    hdu = fits.CompImageHDU(data=data)\n    hdu.writeto(filename, overwrite=True)\n\n    with fits.open(filename) as hdul:\n        assert hdul[1].data.dtype == np.uint16\n        assert np.all(hdul[1].data == data)\n"], "sample_27": ["def test_fitsdiff_with_ignore_keywords(tmp_path):\n    \"\"\"Test FITSDiff with ignore_keywords\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(header=Header(cards=[(\"TELESCOP\", \"HST\")]))])\n    hdulist.writeto(path1)\n    hdulist[0].header[\"TELESCOP\"] = \"JWST\"\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2, ignore_keywords=[\"TELESCOP\"])\n    assert diff.identical\n\n    report = diff.report()\n    assert \"No differences found.\" in report\n", "def test_rawdatadiff_unlimited_diffs(self):\n    a = np.arange(100, dtype=\"uint8\") + 10\n    b = a.copy()\n    for i in range(len(a)):\n        b[i] = (b[i] + 1) % 256\n\n    hdu_a = DummyNonstandardExtHDU(data=a)\n    hdu_b = DummyNonstandardExtHDU(data=b)\n    diff = HDUDiff(hdu_a, hdu_b, numdiffs=-1)\n\n    assert not diff.identical\n\n    report = diff.report()\n    assert \"Data contains differences:\" in report\n    for i in range(len(a)):\n        assert f\"Data differs at byte {i}:\" in report\n    assert \"100 different bytes found (100.00% different).\" in report\n", "def test_fitsdiff_extension_level(tmp_path):\n    \"\"\"Make sure diff report reports HDU XTENSION and EXTLEVEL if same in files\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\")])\n    hdulist.writeto(path1)\n    hdulist[1].header[\"EXTLEVEL\"] = 2\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Extension levels differ:\" in diff.report()\n    assert \"a: 1\" in diff.report()\n    assert \"b: 2\" in diff.report()\n", "def test_fitsdiff_ignore_keywords(tmp_path):\n    \"\"\"Test FITSDiff with ignore_keywords\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdu1 = PrimaryHDU(header=Header([(\"A\", 1), (\"B\", 2)]))\n    hdu2 = PrimaryHDU(header=Header([(\"A\", 1), (\"B\", 3)]))\n\n    hdu1.writeto(path1)\n    hdu2.writeto(path2)\n\n    diff = FITSDiff(path1, path2, ignore_keywords=[\"B\"])\n    assert diff.identical\n\n    report = diff.report()\n    assert \"No differences found.\" in report\n", "def test_fitsdiff_with_header_diff(tmp_path):\n    \"\"\"Make sure diff report reports header differences\"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist = HDUList([PrimaryHDU(header=Header(cards=[(\"TELESCOP\", \"HST\")]))])\n    hdulist.writeto(path1)\n    hdulist[0].header[\"TELESCOP\"] = \"JWST\"\n    hdulist.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert \"Headers contain differences:\" in diff.report()\n    assert \"TELESCOP\" in diff.report()\n"], "sample_28": ["def test_header_fromstring_trailing_whitespace():\n    \"\"\"\n    Test reading a Header from a string with trailing whitespace.\n\n    Regression test for https://github.com/astropy/astropy/issues/12419\n    \"\"\"\n\n    header_string = \"SIMPLE  =                    T / file does conform to FITS standard\\n\"\n    header_string += \"BITPIX  =                  -32 / number of bits per data pixel\\n\"\n    header_string += \"NAXIS   =                    0 / number of data axes\\n\"\n    header_string += \"EXTEND  =                    T / FITS dataset may contain extensions\\n\"\n    header_string += \"COMMENT   FITS (Flexible Image Transport System) format is defined in 'Astronomy\\n\"\n    header_string += \"COMMENT   and Astrophysics', volume 376, page 359; bibcode: 2001A&A...376..359H\\n\"\n    header_string += \"END\" + \"   \"\n\n    header = fits.Header.fromstring(header_string)\n    assert len(header) == 5\n    assert header[\"SIMPLE\"]\n    assert header[\"BITPIX\"] == -32\n    assert header[\"NAXIS\"] == 0\n    assert header[\"EXTEND\"]\n", "def test_header_fromstring_bytes_with_trailing_newline(self):\n    \"\"\"\n    Test reading a Header from a `bytes` string with trailing newline.\n\n    See https://github.com/astropy/astropy/issues/8706\n    \"\"\"\n\n    with open(self.data(\"test0.fits\"), \"rb\") as fobj:\n        pri_hdr_from_bytes = fits.Header.fromstring(fobj.read() + b\"\\n\")\n\n    pri_hdr = fits.getheader(self.data(\"test0.fits\"))\n    assert pri_hdr[\"NAXIS\"] == pri_hdr_from_bytes[\"NAXIS\"]\n    assert pri_hdr == pri_hdr_from_bytes\n    assert pri_hdr.tostring() == pri_hdr_from_bytes.tostring()\n", "def test_card_strip_whitespace():\n    \"\"\"\n    Test Card.strip() method which strips whitespace from the value of a card.\n    \"\"\"\n\n    c = fits.Card(\"DP1\", \"NAXIS: 2\")\n    c.strip()\n    assert str(c).rstrip() == \"DP1     = 'NAXIS: 2'\"\n\n    c = fits.Card(\"DP1\", \" NAXIS: 2 \")\n    c.strip()\n    assert str(c).rstrip() == \"DP1     = 'NAXIS: 2'\"\n\n    c = fits.Card.fromstring(\"DP1     = ' NAXIS: 2 ' / A comment\")\n    c.strip()\n    assert str(c).rstrip() == \"DP1     = 'NAXIS: 2' / A comment\"\n", "def test_header_set_value_to_nan(self):\n    \"\"\"\n    Setting the value of a card to NaN should raise a ValueError.\n    \"\"\"\n\n    header = fits.Header()\n    header[\"FOO\"] = \"BAR\"\n    pytest.raises(ValueError, header.set, \"FOO\", np.nan)\n    pytest.raises(ValueError, header.__setitem__, \"FOO\", np.nan)\n\n    # Create a header that contains an undefined value and a defined value.\n    hstr = \"UNDEF   = \\nDEFINED = 42\"\n    header = fits.Header.fromstring(hstr, sep=\"\\n\")\n\n    # Explicitly add a card with an UNDEFINED value\n    c = fits.Card(\"UNDEF2\", fits.card.UNDEFINED)\n    header.extend([c])\n\n    # And now assign an undefined value to the header through setitem\n    pytest.raises(ValueError, header.set, \"UNDEF3\", np.nan)\n    pytest.raises(ValueError, header.__setitem__, \"UNDEF3\", np.nan)\n", "def test_header_update_with_undefined(self):\n    \"\"\"Test updating a header with undefined values\"\"\"\n    header = fits.Header([(\"A\", 1), (\"B\", 2)])\n    new_header = fits.Header([(\"A\", fits.card.UNDEFINED), (\"C\", 3)])\n    header.update(new_header)\n    assert \"A\" not in header\n    assert header[\"B\"] == 2\n    assert header[\"C\"] == 3\n"], "sample_29": ["def test_write_latex_format(self, write, tmp_path):\n    \"\"\"Test that 'format' argument must be 'latex'\"\"\"\n    fp = tmp_path / \"test_write_latex_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex'\"):\n        write(fp, format=\"csv\")\n", "def test_write_latex_invalid_format(self, write, tmp_path):\n    \"\"\"Test passing an invalid format\"\"\"\n    fp = tmp_path / \"test_write_latex_invalid_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex'\"):\n        write(fp, format=\"invalid\")\n", "def test_write_latex_format_mismatch(self, write, tmp_path):\n    \"\"\"Test passing a format that doesn't match 'latex'\"\"\"\n    fp = tmp_path / \"test_write_latex_format_mismatch.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex', not\"):\n        write(fp, format=\"ascii\")\n", "def test_write_latex_invalid_format(self, write, tmp_path):\n    \"\"\"Test passing an invalid format\"\"\"\n    fp = tmp_path / \"test_write_latex_invalid_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex'\"):\n        write(fp, format=\"invalid\")\n", "def test_write_latex_invalid_format(self, write, tmp_path):\n    \"\"\"Test passing an invalid format\"\"\"\n    fp = tmp_path / \"test_write_latex_invalid_format.tex\"\n    with pytest.raises(ValueError, match=\"format must be 'latex'\"):\n        write(fp, format=\"invalid\")\n"], "sample_30": ["def test_invalid_values():\n    votable = parse(get_pkg_data_filename(\"data/invalid_values.xml\"))\n    table = votable.get_first_table()\n    assert table.array.dtype[\"int\"].kind == \"i\"\n    assert np.isnan(table.array[\"float\"][0])\n    assert table.array[\"float\"][1] == np.inf\n    assert table.array[\"float\"][2] == -np.inf\n    assert table.array.mask[\"char\"][0]\n    assert not table.array.mask[\"char\"][1]\n    assert table.array.mask[\"bit\"][0]\n    assert not table.array.mask[\"bit\"][1]\n    assert table.array.dtype[\"boolean\"].kind == \"b\"\n    assert table.array.mask[\"boolean\"][0]\n    assert not table.array.mask[\"boolean\"][1]\n    assert table.array.mask[\"complex\"][0]\n    assert not table.array.mask[\"complex\"][1]\n    assert table.array[\"complex\"][1] == np.inf + 1j * np.inf\n", "def test_votable_from_table(tmp_path):\n    from astropy.table import Table\n\n    table = Table(\n        {\n            \"a\": [1, 2, 3],\n            \"b\": [4.0, 5.0, 6.0],\n            \"c\": [\"x\", \"y\", \"z\"],\n        }\n    )\n\n    votable = tree.VOTableFile.from_table(table)\n\n    assert len(votable.resources) == 1\n    assert len(votable.resources[0].tables) == 1\n\n    output_file = tmp_path / \"output.xml\"\n    votable.to_xml(str(output_file))\n\n    votable_roundtrip = parse(str(output_file))\n    assert len(votable_roundtrip.resources) == 1\n    assert len(votable_roundtrip.resources[0].tables) == 1\n\n    roundtrip_table = votable_roundtrip.get_first_table().to_table()\n    assert np.all(roundtrip_table[\"a\"] == table[\"a\"])\n    assert np.all(roundtrip_table[\"b\"] == table[\"b\"])\n    assert np.all(roundtrip_table[\"c\"] == table[\"c\"])\n", "def test_table_from_scratch():\n    from astropy.table import Table\n    from astropy.io.votable.tree import VOTableFile\n\n    table = Table({\n        'ra': [1.0, 2.0, 3.0],\n        'dec': [4.0, 5.0, 6.0]\n    })\n\n    votable = VOTableFile.from_table(table)\n\n    assert len(votable.resources) == 1\n    assert len(votable.resources[0].tables) == 1\n    assert votable.resources[0].tables[0].array.shape == (3,)\n", "def test_get_table_by_id():\n    votable = parse(get_pkg_data_filename(\"data/timesys.xml\"))\n    table = votable.get_table_by_id(\"table1\")\n    assert table is not None\n    with pytest.raises(KeyError):\n        votable.get_table_by_id(\"nonexistent_table\")\n", "def test_invalid_version(tmp_path):\n    votable = tree.VOTableFile()\n    votable.version = \"2.0\"\n    with pytest.raises(ValueError) as excinfo:\n        votable.to_xml(str(tmp_path / \"invalid_version.xml\"))\n    assert \"Invalid VOTable version '2.0'\" in str(excinfo.value)\n"], "sample_31": ["    def test_write_latex_parameters_unit(self, write, tmp_path, format):\n        \"\"\"Test that parameters with units are correctly written to LaTeX\"\"\"\n        fp = tmp_path / \"test_write_latex_parameters_unit.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        # asserts each column has correct unit\n        for column_name in tbl.colnames[2:]:\n            param_name = [k for k, v in _FORMAT_TABLE.items() if v == column_name]\n            if param_name:\n                param_name = param_name[0]\n                param = getattr(self.cosmo.__class__, param_name, None)\n                if param and param.unit not in (None, u.one):\n                    assert tbl[column_name].unit == param.unit\n", "def test_write_latex_unit_conversion(self, write, tmp_path):\n    \"\"\"Test that units are correctly converted when writing to LaTeX\"\"\"\n    fp = tmp_path / \"test_write_latex_unit_conversion.tex\"\n    write(fp, format=\"latex\")\n    tbl = QTable.read(fp)\n    # Check that columns with units are correctly converted\n    for column_name in tbl.colnames[2:]:\n        if tbl[column_name].unit is not None:\n            assert tbl[column_name].unit == _FORMAT_TABLE.get(column_name, column_name)\n", "def test_write_latex_unit_conversion(self, write, tmp_path, format):\n    \"\"\"Test that units are correctly converted\"\"\"\n    fp = tmp_path / \"test_write_latex_unit_conversion.tex\"\n    # Assuming the cosmology instance has a parameter with unit\n    write(fp, format=format)\n    tbl = QTable.read(fp)\n    # Get the first column with a unit (assuming there is one)\n    for col in tbl.columns:\n        if col.unit is not None:\n            assert col.unit == u.one or isinstance(col.unit, u.UnitBase)\n            break\n", "    def test_write_latex_kwargs(self, write, tmp_path, format):\n        \"\"\"Test that kwargs are passed through to QTable.write\"\"\"\n        fp = tmp_path / \"test_write_latex_kwargs.tex\"\n        write(fp, format=format, latex_names=True, caption=\"Cosmology Parameters\")\n        with open(fp, \"r\") as f:\n            contents = f.read()\n            assert \"Cosmology Parameters\" in contents\n", "def test_write_latex_kwargs_passed_to_table_write(self, write, tmp_path):\n    \"\"\"Test that kwargs are passed to QTable.write\"\"\"\n    fp = tmp_path / \"test_write_latex_kwargs_passed_to_table_write.tex\"\n    write(fp, format=\"latex\", latex_names=True, caption=\"Test Caption\")\n    with open(fp, 'r') as f:\n        content = f.read()\n        assert \"Test Caption\" in content\n"], "sample_32": ["def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    z = np.array([0.0, 0.5, 1.0, 1.5, 2.3])\n    assert u.allclose(\n        cosmo.de_density_scale(z),\n        [1.0, 1.56014481, 2.29755984, 3.33818145, 5.25130642],\n    )\n", "def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    z = np.array([0.0, 0.5, 1.0, 1.5, 2.3])\n    expected = (z + 1) ** (3 * (1 + cosmo.w0 - cosmo.wz)) * np.exp(3 * cosmo.wz * z)\n    assert u.allclose(cosmo.de_density_scale(z), expected)\n", "def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    z = np.array([0.0, 0.5, 1.0, 1.5, 2.3])\n    expected = np.array([1.0, 1.67665946, 2.71828183, 4.48168907, 9.54992558])\n    assert u.allclose(cosmo.de_density_scale(z), expected)\n", "def test_de_density_scale(self, cosmo):\n    \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n    z = np.array([0.0, 0.5, 1.0, 1.5, 2.3])\n    expected = np.array([1.0, 1.75949175, 3.08021697, 5.39985595, 10.16615144])\n    assert u.allclose(cosmo.de_density_scale(z), expected)\n", "    def setup_class(self):\n        \"\"\"Setup for testing.\"\"\"\n        self.cosmo = w0wzCDM(H0=70, Om0=0.3, Ode0=0.7, w0=-0.9, wz=0.2)\n"], "sample_33": ["def test_indent():\n    s = \"Hello\\nWorld\"\n    indented = misc.indent(s, shift=1, width=4)\n    assert indented == \"    Hello\\n    World\"\n\n    s = \"Hello\\nWorld\\n\"\n    indented = misc.indent(s, shift=1, width=4)\n    assert indented == \"    Hello\\n    World\\n\"\n\n    s = \"\"\n    indented = misc.indent(s, shift=1, width=4)\n    assert indented == \"\"\n", "def test_indent():\n    s = \"Hello\\nWorld\"\n    assert misc.indent(s) == '    Hello\\n    World'\n    assert misc.indent(s, shift=2, width=4) == '        Hello\\n        World'\n    assert misc.indent(s, shift=1, width=2) == '  Hello\\n  World'\n", "def test_did_you_mean():\n        return [s + '1', s + '2']\n\n    assert misc.did_you_mean('foo', ['bar', 'baz']) == ''\n    assert misc.did_you_mean('bar', ['bar', 'baz']) == ''\n    assert misc.did_you_mean('bax', ['bar', 'baz']) == \"Did you mean bar?\"\n    assert misc.did_you_mean('bax', ['bar', 'baz'], fix=fix) == \"Did you mean bar1 or bar2?\"\n", "def test_indent():\n    assert misc.indent(\"Hello\\nWorld\", 4, 4) == \"    Hello\\n    World\\n\"\n    assert misc.indent(\"Hello\\nWorld\", 2, 8) == \"        Hello\\n        World\\n\"\n    assert misc.indent(\"Hello World\", 1, 4) == \"    Hello World\"\n", "def test_did_you_mean():\n    # Test that the did_you_mean function is working correctly\n    candidates = ['apple', 'banana', 'orange']\n    result = misc.did_you_mean('aple', candidates)\n    assert result == \"Did you mean apple?\"\n    \n    # Test that the function returns an empty string if no match is found\n    result = misc.did_you_mean('grape', candidates)\n    assert result == \"\"\n    \n    # Test that the function works with multiple close matches\n    candidates = ['apple', 'application', 'appetite']\n    result = misc.did_you_mean('aple', candidates)\n    assert \"apple\" in result and \"application\" in result\n    \n    # Test that the function works with a fix callback\n        return [s + 's']\n    candidates = ['apple', 'banana', 'orange']\n    result = misc.did_you_mean('aple', candidates, fix=fix)\n    assert result == \"Did you mean apples?\"\n"], "sample_34": ["def test_get_current_unit_registry():\n    # Test that get_current_unit_registry returns the current registry\n    registry = u.get_current_unit_registry()\n    assert isinstance(registry, u._UnitRegistry)\n    assert registry is u.get_current_unit_registry()\n", "def test_dimensionless_compose():\n    x = u.dimensionless_unscaled.compose()\n    assert len(x) == 1\n    assert x[0] is u.dimensionless_unscaled\n", "def test_find_equivalent_units():\n    # Issue #4359\n    assert len(u.m.find_equivalent_units(equivalencies=u.dimensionless_angles())) > 0\n    assert len(u.deg.find_equivalent_units(equivalencies=u.dimensionless_angles())) > 0\n\n    assert len(u.m.find_equivalent_units(units=[u.km, u.cm])) == 2\n    assert len(u.m.find_equivalent_units(units=[u.kg, u.s])) == 0\n", "def test_unit_get_format_name():\n    \"\"\"Test that Unit.get_format_name() returns the correct name for a unit in a specific format.\"\"\"\n    unit = u.Unit('m')\n    assert unit.get_format_name('latex') == 'm'\n    assert unit.get_format_name('unicode') == 'm'\n\n    unit = u.Unit('km/s')\n    assert unit.get_format_name('latex') == '\\\\mathrm{km} \\\\, \\\\mathrm{s}^{-1}'\n    assert unit.get_format_name('unicode') == 'km s\u207b\u00b9'\n", "def test_get_units_with_same_physical_type():\n    # Test that the function returns a list of units with the same physical type\n    unit_registry = u.get_current_unit_registry()\n    units = unit_registry.get_units_with_physical_type(u.m)\n    assert all(unit.is_equivalent(u.m) for unit in units)\n\n    # Test that the function works with composite units\n    units = unit_registry.get_units_with_physical_type(u.m/u.s)\n    assert all(unit.is_equivalent(u.m/u.s) for unit in units)\n\n    # Test that the function works with units that have equivalencies\n    with u.add_enabled_equivalencies(u.spectral()):\n        units = unit_registry.get_units_with_physical_type(u.Hz)\n        assert any(unit.is_equivalent(u.m) for unit in units)\n"], "sample_35": ["def test_isinstancemethod():\n    class TestClass:\n            pass\n\n        @classmethod\n            pass\n\n        @staticmethod\n            pass\n\n    assert isinstancemethod(TestClass, TestClass.instance_method)\n    assert not isinstancemethod(TestClass, TestClass.class_method)\n    assert not isinstancemethod(TestClass, TestClass.static_method)\n", "def test_minversion():\n    assert minversion('astropy', '0.1') is True\n    assert minversion('astropy', '100.0') is False\n\n    # Test inclusive=False\n    assert minversion('astropy', '0.1', inclusive=False) is True\n    assert minversion('astropy', '100.0', inclusive=False) is False\n\n    # Test passing a module object instead of its name\n    import astropy\n    assert minversion(astropy, '0.1') is True\n    assert minversion(astropy, '100.0') is False\n\n    # Test passing a version that is not in the standard format\n    assert minversion('astropy', '1.dev') is True\n\n    # Test passing a non-existent module\n    assert minversion('non_existent_module', '1.0') is False\n", "def test_minversion():\n    import astropy\n    assert minversion(astropy, '0.4.4') is True\n    assert minversion('astropy', '0.4.4') is True\n    assert minversion('nonexistentmodule', '0.4.4') is False\n    with pytest.raises(ValueError):\n        minversion(12345, '0.4.4')\n", "def test_minversion():\n    # Test with module object\n    assert minversion(introspection, '1.0') is True\n\n    # Test with module name\n    assert minversion('astropy.utils.introspection', '1.0') is True\n\n    # Test with non-existent module\n    assert minversion('non_existent_module', '1.0') is False\n\n    # Test with invalid version string\n    with pytest.raises(TypeError):\n        minversion(introspection, 'invalid_version')\n\n    # Test with inclusive=False\n    assert minversion(introspection, introspection.__version__, inclusive=False) is False\n", "def test_minversion():\n    assert minversion('astropy', '0.1') is True\n    assert minversion('astropy', '10.0') is False\n    assert minversion(introspection, '0.1') is True\n    with pytest.raises(ValueError):\n        minversion('not a module or string', '0.1')\n    with pytest.raises(ImportError):\n        minversion('nonexistentmodule', '0.1')\n"], "sample_36": ["def test_biweight_location_M():\n    \"\"\"\n    Test that biweight_location raises no error when M is an array.\n    \"\"\"\n\n    data = np.random.normal(0, 1, size=(100, 3))\n    M = np.median(data, axis=0)\n    cbl = biweight_location(data, M=M, axis=0)\n    assert_allclose(cbl, M)\n\n    with pytest.raises(ValueError) as e:\n        biweight_location(data, M=np.median(data, axis=0)[:, np.newaxis], axis=0)\n    assert 'M must be a scalar or 1D array.' in str(e.value)\n", "def test_biweight_location_M_keyword():\n    \"\"\"\n    Test that the biweight_location function behaves correctly when\n    given a location estimate M.\n    \"\"\"\n\n    with NumpyRNGContext(12345):\n        randvar = randn(10000)\n        cbl_no_m = biweight_location(randvar)\n        cbl_with_m = biweight_location(randvar, M=np.median(randvar))\n        assert_allclose(cbl_no_m, cbl_with_m)\n\n    # test with 2D array\n    ny = 100\n    nx = 200\n    data = normal(5, 2, (ny, nx))\n    cbl_no_m = biweight_location(data, axis=0)\n    cbl_with_m = biweight_location(data, axis=0, M=np.median(data, axis=0))\n    assert_allclose(cbl_no_m, cbl_with_m)\n", "def test_biweight_midcorrelation_constant():\n    x = np.ones(10)\n    y = np.ones(10)\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n\n    x = np.ones(10)\n    y = np.zeros(10)\n    assert_allclose(biweight_midcorrelation(x, y), 0.0)\n", "def test_biweight_location_non_finite_values():\n    data = np.array([1, 2, 3, np.nan, np.inf])\n    with catch_warnings() as w:\n        cbl = biweight_location(data)\n        assert len(w) == 0\n    assert_allclose(cbl, 2.)\n\n    data = np.array([np.nan, np.nan, np.nan])\n    with catch_warnings() as w:\n        cbl = biweight_location(data)\n        assert len(w) == 0\n    assert np.isnan(cbl)\n\n    data = np.array([np.inf, np.inf, np.inf])\n    with catch_warnings() as w:\n        cbl = biweight_location(data)\n        assert len(w) == 0\n    assert np.isinf(cbl)\n", "def test_biweight_midcorrelation_constant():\n    x = [1, 1, 1]\n    y = [2, 2, 2]\n    assert_allclose(biweight_midcorrelation(x, y), np.nan)\n"], "sample_37": ["def test_axis_type_names():\n    \"\"\"\n    Test axis type names\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cname = [\"RA\", \"DEC\"]\n\n    assert w.axis_type_names == ['RA', 'DEC']\n\n    w.wcs.cname = ['', '']\n    assert w.axis_type_names == ['RA', 'DEC']\n", "def test_sub():\n    w = wcs.WCS(naxis=3)\n    w.wcs.crval = [1, 2, 3]\n    w.wcs.cdelt = [0.1, 0.2, 0.3]\n    w.wcs.crpix = [1, 2, 3]\n    w._naxis = [100, 200, 300]\n\n    new_w = w.sub([1, 2])\n\n    assert new_w.naxis == 2\n    assert_array_equal(new_w.wcs.crval, [1, 2])\n    assert_array_equal(new_w.wcs.cdelt, [0.1, 0.2])\n    assert_array_equal(new_w.wcs.crpix, [1, 2])\n    assert new_w._naxis == [100, 200]\n", "def test_dropaxis():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w2 = w.dropaxis(2)\n    assert w2.naxis == 2\n    assert w2.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\"]\n\n    with pytest.raises(ValueError):\n        w.dropaxis(3)\n", "def test_wcs_to_header_precision():\n    w = wcs.WCS(naxis=2)\n    w.wcs.crval = [1.234567890123456, 2.345678901234567]\n    w.wcs.cdelt = [0.1234567890123456, 0.2345678901234567]\n    header = w.to_header()\n    assert header['CRVAL1'] == 1.234567890123456\n    assert header['CDELT1'] == 0.1234567890123456\n", "def test_sip_foc2pix():\n    \"\"\"\n    Test sip_foc2pix method.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.sip = wcs.Sip(np.array([[1, 0], [0, 1]]), np.array([[0, 0], [0, 0]]),\n                    np.array([[1, 0], [0, 1]]), np.array([[0, 0], [0, 0]]),\n                    [1, 1])\n    pix = w.sip_foc2pix([1, 2], [3, 4], 0)\n    assert_array_almost_equal(pix[0], [1, 2])\n    assert_array_almost_equal(pix[1], [3, 4])\n"], "sample_38": ["def test_wcs_deepcopy():\n    \"\"\"\n    Test for #5643, ensures that WCS can be deepcopied correctly.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.crval = [1, 1]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crpix = [1, 1]\n    w._naxis = [1000, 500]\n\n    w2 = copy.deepcopy(w)\n\n    assert w2.wcs.crval == w.wcs.crval\n    assert w2.wcs.cdelt == w.wcs.cdelt\n    assert w2.wcs.crpix == w.wcs.crpix\n    assert w2._naxis == w._naxis\n\n    w2.wcs.crval = [2, 2]\n    assert w2.wcs.crval != w.wcs.crval\n\n    w2.wcs.cdelt = [0.2, 0.2]\n    assert w2.wcs.cdelt != w.wcs.cdelt\n\n    w2.wcs.crpix = [2, 2]\n    assert w2.wcs.crpix != w.wcs.crpix\n\n    w2._naxis = [2000, 1000]\n    assert w2._naxis != w._naxis\n", "def test_dropaxis():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"STOKES\"]\n    w2 = w.dropaxis(2)\n    assert w2.naxis == 2\n    assert_array_equal(w2.wcs.ctype, [\"RA---TAN\", \"DEC--TAN\"])\n\n    w2 = w.dropaxis(0)\n    assert w2.naxis == 2\n    assert_array_equal(w2.wcs.ctype, [\"DEC--TAN\", \"STOKES\"])\n", "def test_invalid_units():\n    w = wcs.WCS(naxis=2)\n    w.wcs.crval = [1, 1]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crpix = [1, 1]\n    w.wcs.cunit = ['deg', 'invalid_unit']\n\n    with pytest.raises(ValueError):\n        w.to_header()\n", "def test_wcs_noaxes():\n    \"\"\"\n    Test WCS with no axes.\n    \"\"\"\n    w = wcs.WCS(naxis=0)\n    assert w.wcs.naxis == 0\n    assert w.naxis == 0\n    assert w.wcs.crval == []\n    assert w.wcs.crpix == []\n    assert w.wcs.cdelt == []\n    assert w.wcs.ctype == []\n    assert w.wcs.cunit == []\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n    w.wcs.cname = ['', '']\n\n    axis_type_names = w.axis_type_names\n    assert axis_type_names == ['RA', 'DEC']\n"], "sample_39": ["def test_celestial_reorient():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"STOKES\"]\n    w2 = w.reorient_celestial_first()\n    assert w2.wcs.naxis == 3\n    assert w2.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\", \"STOKES\"]\n\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"STOKES\", \"RA---TAN\", \"DEC--TAN\"]\n    w2 = w.reorient_celestial_first()\n    assert w2.wcs.naxis == 3\n    assert w2.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\", \"STOKES\"]\n", "def test_sip_foc2pix():\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n        w = wcs.WCS(f[0].header)\n\n    inp = np.array([[1, 2], [3, 4]])\n    result = w.sip_foc2pix(inp, 0)\n    assert_array_equal(inp, result)\n", "def test_p4_pix2foc():\n    \"\"\"\n    Test p4_pix2foc method.\n    \"\"\"\n    with fits.open(get_pkg_data_filename('data/dist.fits')) as f:\n        w = wcs.WCS(f[0].header, f)\n    result = w.p4_pix2foc([1, 2], [3, 4], 0)\n    assert_array_almost_equal(result, [[1.00000188, 3.00000738],\n                                       [2.00000375, 4.00001476]])\n", "def test_celestial(self):\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    assert w.is_celestial\n    assert w.has_celestial\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    assert not w.is_celestial\n    assert w.has_celestial\n", "def test_celestial_methods():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---CAR\", \"DEC--CAR\"]\n    w.wcs.cdelt = [10, 10]\n    w.wcs.crval = [1, 1]\n    w.wcs.crpix = [1, 1]\n\n    assert w.has_celestial\n    assert not w.is_celestial\n\n    assert isinstance(w.celestial, wcs.WCS)\n\n    assert w.axis_type_names == ['RA', 'DEC']\n\n    assert isinstance(w.get_axis_types(), list)\n"], "sample_40": ["def test_with_H0():\n    from ..cosmology import default_cosmology\n    H0 = default_cosmology.get().H0\n    littleh = u.with_H0(H0)\n    assert (u.km/u.s/u.Mpc).to_value(1, equivalencies=littleh) == 1. / 100.\n", "def test_with_H0():\n    from ..cosmology import default_cosmology\n    H0 = default_cosmology.get().H0\n\n    h100_val_unit = u.Unit(H0.to((u.km/u.s)/u.Mpc).value/100 * u.littleh)\n\n    assert h100_val_unit.is_equivalent(u.dimensionless_unscaled, u.with_H0())\n    assert (1*h100_val_unit).to_value(u.dimensionless_unscaled, u.with_H0()) == 1.\n", "def test_with_H0():\n    from astropy.cosmology import Planck15\n    with u.set_enabled_equivalencies(u.with_H0(Planck15.H0)):\n        assert (1 * u.littleh).to_value(u.dimensionless_unscaled) == Planck15.H0.value / 100.\n        assert (1 * u.Mpc / u.littleh).to_value(u.Mpc) == Planck15.H0.value / 100.\n\n    with u.set_enabled_equivalencies(u.with_H0()):\n        assert (1 * u.littleh).to_value(u.dimensionless_unscaled) == Planck15.H0.value / 100.\n        assert (1 * u.Mpc / u.littleh).to_value(u.Mpc) == Planck15.H0.value / 100.\n\n    h100 = 0.7 * u.dimensionless_unscaled\n    with u.set_enabled_equivalencies(u.with_H0(h100 * 100 * u.km / (u.s * u.Mpc))):\n        assert (1 * u.littleh).to_value(u.dimensionless_unscaled) == h100.value\n        assert (1 * u.Mpc / u.littleh).to_value(u.Mpc) == h100.value\n", "def test_with_H0():\n    from astropy.cosmology import Planck15\n    H0 = Planck15.H0\n    littleh = u.with_H0(H0)\n    assert (1 * u.littleh).to(u.dimensionless_unscaled, equivalencies=littleh) == 0.6736\n", "def test_with_H0():\n    from astropy.cosmology import Planck15\n\n    # Test that the Hubble constant is correctly applied\n    h100 = 0.7\n    H0_default = Planck15.H0.value / 100\n    H0_custom = 65 / 100\n\n    # Default H0\n    equiv = u.with_H0()\n    assert (1 * u.littleh).to_value(1, equivalencies=equiv) == H0_default\n    assert (H0_default * u.dimensionless_unscaled).to_value(u.littleh, equivalencies=equiv) == 1\n\n    # Custom H0\n    equiv = u.with_H0(H0=65 * u.km / (u.s * u.Mpc))\n    assert (1 * u.littleh).to_value(1, equivalencies=equiv) == H0_custom\n    assert (H0_custom * u.dimensionless_unscaled).to_value(u.littleh, equivalencies=equiv) == 1\n"], "sample_41": ["def test_unit_summary_prefixes_custom_units():\n    \"\"\"\n    Test that the unit summary table correctly reports whether or not a custom\n    unit supports prefixes.\n\n    Regression test for https://github.com/astropy/astropy/issues/3835\n    \"\"\"\n\n    custom_unit_with_prefixes = u.def_unit('custom_unit_with_prefixes', u.m, prefixes=True)\n    custom_unit_without_prefixes = u.def_unit('custom_unit_without_prefixes', u.m)\n\n    custom_units = {\n        'custom_unit_with_prefixes': custom_unit_with_prefixes,\n        'custom_unit_without_prefixes': custom_unit_without_prefixes,\n    }\n\n    for summary in utils._iter_unit_summary(custom_units):\n        unit, _, _, _, prefixes = summary\n\n        if unit.name == 'custom_unit_with_prefixes':\n            assert prefixes\n        elif unit.name == 'custom_unit_without_prefixes':\n            assert not prefixes\n", "def test_units_with_physical_type():\n    from astropy import units as u\n    from astropy.units import physical\n\n    # Test that units with the same physical type are correctly identified\n    assert u.m.physical_type == 'length'\n    assert u.cm.physical_type == 'length'\n    assert u.m.physical_type == u.cm.physical_type\n\n    # Test that units with different physical types are correctly distinguished\n    assert u.m.physical_type != u.s.physical_type\n\n    # Test that the physical type of a composite unit is correctly determined\n    assert (u.m / u.s).physical_type == 'speed'\n\n    # Test that the physical type of a unit can be retrieved using the physical module\n    assert physical.get_physical_type(u.m) == 'length'\n", "def test_unit_format():\n    \"\"\"\n    Test the formatting of units using different formatters.\n    \"\"\"\n    unit = u.Unit(\"10 m/s\")\n    assert unit.to_string() == \"10 m / s\"\n    assert unit.to_string(\"latex\") == r\"10 \\, \\mathrm{\\frac{m}{s}}\"\n    assert unit.to_string(\"unicode\") == \"10 m\u2044s\"\n\n    unit = u.Unit(\"10 km/s^2\")\n    assert unit.to_string() == \"10 km / s2\"\n    assert unit.to_string(\"latex\") == r\"10 \\, \\mathrm{\\frac{km}{s^{2}}}\"\n    assert unit.to_string(\"unicode\") == \"10 km\u2044s\u00b2\"\n", "def test_unit_scale_error():\n    \"\"\"\n    Test UnitScaleError when scaled units are not recognized by FITS format.\n    \"\"\"\n    with pytest.raises(u.UnitScaleError):\n        u.m.to_string('fits')\n", "def test_add_enabled_units_with_equivalencies():\n    from astropy.units import imperial\n    with u.add_enabled_units(imperial):\n        assert imperial.mile in u.m.find_equivalent_units(equivalencies=[(u.m, u.pc, lambda x: x*206264.806, lambda x: x/206264.806)])\n"], "sample_42": ["def test_with_H0():\n    cosmo = cosmology.default_cosmology.get()\n    H0 = 70 * u.km / (u.Mpc * u.s)\n    h100 = H0 / (100 * u.km / (u.Mpc * u.s))\n\n    with u.set_enabled_equivalencies(u.with_H0(H0=H0)):\n        assert_quantity_allclose(1*u.littleh, h100)\n\n    with u.set_enabled_equivalencies(u.with_H0(cosmo)):\n        assert_quantity_allclose(1*u.littleh, cosmo.h)\n\n    with pytest.raises(ValueError):\n        u.with_H0(H0=5)\n", "def test_with_H0():\n    from astropy.cosmology import Planck15\n\n    # Test that H0 is correctly used if passed explicitly\n    h100 = 0.7 * u.littleh\n    equiv = u.with_H0(H0=70*u.km/u.s/u.Mpc)\n    assert h100.to_value(1, equivalencies=equiv) == 1\n\n    # Test that H0 from default cosmology is used if not passed explicitly\n    equiv = u.with_H0()\n    assert h100.to_value(1, equivalencies=equiv) == (100/Planck15.H0.value)\n", "def test_with_H0():\n    from astropy.cosmology import Planck15\n    h100 = 0.7\n    H0 = Planck15.H0\n\n    # Test that equivalency can be created with or without specifying H0\n    equiv1 = u.with_H0(H0)\n    equiv2 = u.with_H0()\n\n    assert len(equiv1) == 1\n    assert len(equiv2) == 1\n\n    # Test the forward and backward conversions for each equivalency\n    for equiv in [equiv1, equiv2]:\n        unit = equiv[0][0]\n        value = 1.0\n        converted_value = value * unit.to(1, equivalencies=equiv)\n\n        assert_allclose(converted_value, h100, rtol=1e-5)\n\n        roundtrip_value = converted_value * (1).to(unit, equivalencies=equiv)\n        assert_allclose(roundtrip_value, value, rtol=1e-10)\n", "def test_with_H0():\n    from astropy.cosmology import default_cosmology\n\n    # Test with no H0 provided (should use default cosmology)\n    h100 = u.with_H0()\n    assert h100[0][0].value == 100 / default_cosmology.get().H0.value\n\n    # Test with custom H0 provided\n    h_custom = 50 * u.km / (u.s * u.Mpc)\n    h100 = u.with_H0(H0=h_custom)\n    assert h100[0][0].value == 100 / h_custom.value\n\n    # Test with invalid H0 (should raise UnitsError)\n    with pytest.raises(u.UnitsError):\n        u.with_H0(H0=5*u.Jy)\n", "def test_with_H0():\n    H0 = 70 * u.km / (u.s * u.Mpc)\n    equiv = u.with_H0(H0)\n    h100 = 0.7 * u.littleh\n    assert h100.to_value(1, equivalencies=equiv) == 1\n"], "sample_43": ["def test_bayesian_blocks_gamma_prior():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n\n    # specify gamma directly\n    bins1 = bayesian_blocks(t, fitness='events', gamma=0.1)\n\n    # specify ncp_prior, which should give the same result\n    bins2 = bayesian_blocks(t, fitness='events', ncp_prior=-np.log(0.1))\n\n    assert_allclose(bins1, bins2)\n\n    # specifying p0 should give a different result\n    bins3 = bayesian_blocks(t, fitness='events', p0=0.05)\n    assert not np.allclose(bins1, bins3)\n", "def test_bayesian_blocks_gamma():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n\n    # specify gamma directly\n    bins1 = bayesian_blocks(t, fitness='events', gamma=0.01)\n\n    # specify ncp_prior directly\n    bins2 = bayesian_blocks(t, fitness='events', ncp_prior=-np.log(0.01))\n\n    # check that results are the same\n    assert_allclose(bins1, bins2)\n", "def test_bayesian_blocks_gamma():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n\n    # Test that specifying gamma overrides p0\n    bins1 = bayesian_blocks(t, fitness='events', p0=0.01, gamma=0.5)\n    bins2 = bayesian_blocks(t, fitness='events', gamma=0.5)\n    assert_allclose(bins1, bins2)\n\n    # Test that specifying ncp_prior overrides gamma and p0\n    bins3 = bayesian_blocks(t, fitness='events', p0=0.01, gamma=0.5, ncp_prior=1.0)\n    bins4 = bayesian_blocks(t, fitness='events', ncp_prior=1.0)\n    assert_allclose(bins3, bins4)\n", "def test_bayesian_blocks_gamma():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n\n    # Test that specifying gamma overrides p0\n    bins1 = bayesian_blocks(t, fitness='events', p0=0.01, gamma=0.5)\n    bins2 = bayesian_blocks(t, fitness='events', gamma=0.5)\n    assert_allclose(bins1, bins2)\n\n    # Test that specifying ncp_prior overrides gamma and p0\n    bins3 = bayesian_blocks(t, fitness='events', p0=0.01, gamma=0.5, ncp_prior=2.0)\n    bins4 = bayesian_blocks(t, fitness='events', ncp_prior=2.0)\n    assert_allclose(bins3, bins4)\n", "def test_bayesian_blocks_gamma_prior():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n\n    # test that specifying gamma overrides p0\n    bins1 = bayesian_blocks(t, fitness='events', p0=0.1, gamma=0.5)\n    bins2 = bayesian_blocks(t, fitness='events', gamma=0.5)\n    assert_allclose(bins1, bins2)\n\n    # test that specifying ncp_prior overrides gamma and p0\n    bins3 = bayesian_blocks(t, fitness='events', p0=0.1, gamma=0.5, ncp_prior=1.0)\n    bins4 = bayesian_blocks(t, fitness='events', ncp_prior=1.0)\n    assert_allclose(bins3, bins4)\n"], "sample_44": ["    def setup(self):\n        self.mJy = np.arange(1., 5.).reshape(2, 2) * u.mag(u.Jy)\n        self.m1 = np.arange(1., 5.5, 0.5).reshape(3, 3) * u.mag()\n", "def test_log_quantity_repr_latex(self):\n    \"\"\"Test that LogQuantity has a nice LaTeX representation.\"\"\"\n    lq = u.Magnitude(10. * u.Jy)\n    assert lq._repr_latex_() == r'$\\mathrm{10 \\, mag(Jy)}$'\n", "    def test_pickle(self):\n        lq = u.Magnitude(np.arange(1., 10.)*u.Jy)\n        s = pickle.dumps(lq)\n        lq2 = pickle.loads(s)\n        assert np.all(lq.value == lq2.value)\n        assert lq.unit == lq2.unit\n", "    def test_quantity_pickle(self):\n        \"\"\"Check that LogQuantity objects can be pickled and unpickled.\"\"\"\n        lq = u.Magnitude(1.23 * u.Jy)\n        s = pickle.dumps(lq)\n        lq2 = pickle.loads(s)\n        assert lq == lq2\n        assert type(lq) is type(lq2)\n        assert type(lq.unit) is type(lq2.unit)\n", "    def setup(self):\n        self.lq1 = u.Magnitude(np.array([1., 2., 3., np.nan, np.inf])*u.Jy)\n        self.lq2 = u.Dex(np.array([1., 2., 3., np.nan, np.inf]))\n"], "sample_45": ["def test_trunc_func_with_timezone_transition(self):\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = datetime(2016, 2, 20, 23, 30)\n    end_datetime = datetime(2016, 10, 16, 0, 30)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    # Test transition from standard to daylight saving time.\n    qs = DTModel.objects.annotate(\n        truncated=TruncHour('start_datetime', tzinfo=sao)\n    )\n    with self.assertRaisesMessage(pytz.AmbiguousTimeError, '2016-02-20 23:00:00'):\n        qs.get()\n\n    # Test transition from daylight saving to standard time.\n    qs = DTModel.objects.annotate(\n        truncated=TruncDay('end_datetime', tzinfo=sao)\n    )\n    with self.assertRaisesMessage(pytz.NonExistentTimeError, '2016-10-16 00:00:00'):\n        qs.get()\n\n    # Test with is_dst parameter.\n    qs = DTModel.objects.annotate(\n        truncated_start=TruncHour('start_datetime', tzinfo=sao, is_dst=True),\n        truncated_end=TruncDay('end_datetime', tzinfo=sao, is_dst=False),\n    )\n    model = qs.get()\n    self.assertEqual(model.truncated_start.dst(), timedelta(0, 3600))\n    self.assertEqual(model.truncated_end.dst(), timedelta(0))\n", "def test_trunc_func_with_timezone_transition(self):\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = datetime(2016, 10, 16, 1)\n    end_datetime = datetime(2017, 2, 19, 23)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    model = DTModel.objects.annotate(\n        truncated_start=TruncDay('start_datetime', tzinfo=sao),\n        truncated_end=TruncDay('end_datetime', tzinfo=sao),\n    ).get()\n\n    self.assertEqual(model.truncated_start.dst(), timedelta(0, 3600))\n    self.assertEqual(model.truncated_end.dst(), timedelta(0))\n", "def test_trunc_func_with_timezone_transition(self):\n    \"\"\"\n    If the truncated datetime transitions to a different offset (daylight\n    saving) then the returned value will have that new timezone/offset.\n    \"\"\"\n    start_datetime = datetime(2015, 3, 29, 1, 30, 50, 321)\n    end_datetime = datetime(2016, 11, 6, 1, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('hour')\n    test_datetime_kind('day')\n", "def test_trunc_func_with_timezone_transition_crossing(self):\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = datetime(2016, 10, 16, 0, 30)\n    end_datetime = datetime(2016, 2, 21, 1, 30)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=sao)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(sao), kind, sao)),\n                (end_datetime, truncate_to(end_datetime.astimezone(sao), kind, sao))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n", "def test_trunc_func_with_timezone_transition(self):\n    melb = pytz.timezone('Australia/Melbourne')\n    start_datetime = datetime(2022, 10, 1, 23, 59, 59)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    self.create_model(start_datetime, None)\n\n    qs = DTModel.objects.annotate(\n        truncated=Trunc('start_datetime', 'hour', output_field=DateTimeField(), tzinfo=melb)\n    )\n    model = qs.get()\n    self.assertEqual(model.truncated.dst(), timedelta(0, 3600))\n"], "sample_46": ["    def setUpTestData(cls):\n        cls.objs = [\n            UUIDModel.objects.create(field=uuid.uuid4()),\n            UUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n            UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440000'),\n        ]\n", "    def setUpTestData(cls):\n        cls.objs = [\n            UUIDModel.objects.create(field=uuid.uuid4()),\n            UUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n        ]\n", "    def setUpTestData(cls):\n        cls.objs = [\n            UUIDModel.objects.create(field=uuid.uuid4()),\n            UUIDModel.objects.create(field='550e8400e29b41d4a716446655440000'),\n            UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440000'),\n        ]\n", "    def test_lookup_aliases(self):\n        instance1 = UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440000')\n        instance2 = UUIDModel.objects.create(field='550e8400-e29b-41d4-a716-446655440001')\n\n        # Using iexact lookup type should match even though the case is different.\n        self.assertSequenceEqual(\n            UUIDModel.objects.filter(field__iexact='550E8400-E29B-41D4-A716-446655440000'),\n            [instance1],\n        )\n\n        # Using exact lookup type should not match since the case is different.\n        self.assertSequenceEqual(\n            UUIDModel.objects.filter(field__exact='550E8400-E29B-41D4-A716-446655440000'),\n            [],\n        )\n\n        # Using iexact lookup type with exists should match even though the case is different.\n        self.assertTrue(\n            UUIDModel.objects.filter(\n                field__iexact='550E8400-E29B-41D4-A716-446655440000',\n            ).exists(),\n        )\n\n        # Using in lookup type with a single value should match.\n        self.assertSequenceEqual(\n            UUIDModel.objects.filter(field__in=['550e8400-e29b-41d4-a716-446655440000']),\n            [instance1],\n        )\n\n        # Using in lookup type with multiple values should match.\n        self.assertSequenceEqual(\n            UUIDModel.objects.filter(\n                field__in=[\n                    '550e8400-e29b-41d4-a716-446655440000',\n                    '550e8400-e29b-41d4-a716-446655440001',\n                ],\n            ),\n            [instance1, instance2],\n        )\n", "    def test_uuid_field_with_default(self):\n        model = UUIDModel.objects.create()\n        self.assertIsInstance(model.field, uuid.UUID)\n        self.assertIsNotNone(model.field)\n"], "sample_47": ["    def test_cleanse_setting_dictionary(self):\n        setting = {'foo': 'bar', 'PASSWORD': 'super_secret'}\n        cleansed_setting = cleanse_setting('TEST', setting)\n        self.assertEqual(cleansed_setting, {'foo': 'bar', 'PASSWORD': CLEANSED_SUBSTITUTE})\n", "    def test_get_traceback_data(self):\n        request = self.rf.get('/test_view/')\n        try:\n            raise ValueError(\"Can't find my keys\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIn('request', data)\n        self.assertIn('user_str', data)\n        self.assertIn('filtered_POST_items', data)\n        self.assertIn('settings', data)\n        self.assertIn('sys_executable', data)\n        self.assertIn('sys_version_info', data)\n        self.assertIn('server_time', data)\n        self.assertIn('django_version_info', data)\n        self.assertIn('sys_path', data)\n        self.assertIn('template_info', data)\n        self.assertIn('template_does_not_exist', data)\n        self.assertIn('postmortem', data)\n", "    def test_cleanse_setting_dictionary(self):\n        self.assertEqual(\n            cleanse_setting('TEST', {'a': 'b', 'c': 'd'}),\n            {'a': 'b', 'c': 'd'}\n        )\n        self.assertEqual(\n            cleanse_setting('PASSWORD', {'a': 'b', 'c': 'd'}),\n            CLEANSED_SUBSTITUTE\n        )\n        self.assertEqual(\n            cleanse_setting('TEST', {'password': 'super_secret', 'other': 'value'}),\n            {'password': CLEANSED_SUBSTITUTE, 'other': 'value'}\n        )\n", "    def test_get_safe_settings(self):\n        sensitive_settings = {\n            'SECRET_KEY': 'secret',\n            'PASSWORD': 'password',\n            'API_KEY': 'api_key',\n        }\n        with self.settings(**sensitive_settings):\n            safe_settings = get_safe_settings()\n            for setting in sensitive_settings:\n                self.assertEqual(safe_settings[setting], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_dictionary(self):\n        setting = {'key1': 'value1', 'key2': 'value2'}\n        cleansed_setting = cleanse_setting('SETTING', setting)\n        self.assertEqual(cleansed_setting, setting)\n"], "sample_48": ["def test_stddev(self):\n    vals = Book.objects.aggregate(stddev=StdDev('price'))\n    self.assertAlmostEqual(vals['stddev'], 31.080232, places=6)\n\n    vals = Book.objects.aggregate(stddev=StdDev('price', sample=False))\n    self.assertAlmostEqual(vals['stddev'], 28.284271, places=6)\n", "def test_stddev(self):\n    vals = Book.objects.aggregate(std_dev=StdDev('price'))\n    self.assertAlmostEqual(vals['std_dev'], 28.284, places=2)\n\n    vals = Book.objects.aggregate(std_dev=StdDev('price', sample=False))\n    self.assertAlmostEqual(vals['std_dev'], 26.331, places=2)\n", "def test_aggregate_with_filter(self):\n    publishers = Publisher.objects.annotate(\n        num_books=Count(\"book__id\", filter=Q(book__rating__gt=4.0))\n    ).filter(num_books__gt=1).order_by(\"pk\")\n    self.assertQuerysetEqual(publishers, ['Prentice Hall'], lambda p: p.name)\n\n    publishers = (\n        Publisher.objects\n        .annotate(num_books=Count(\"book__id\"))\n        .filter(Q(num_books__gt=1) & Q(book__rating__gt=4.0))\n        .order_by(\"pk\")\n    )\n    self.assertQuerysetEqual(publishers, ['Prentice Hall', 'Morgan Kaufmann'], lambda p: p.name)\n", "def test_aggregate_filter_clause(self):\n    # Create a filter clause to use in an aggregate function.\n    filter_clause = Q(rating__gt=3)\n\n    # Use the filter clause in an aggregate function.\n    max_rating = Book.objects.aggregate(Max('rating', filter=filter_clause))\n\n    # Check that the maximum rating is as expected.\n    self.assertEqual(max_rating['rating__max'], 5)\n", "def test_stddev(self):\n    # Create some sample data\n    Book.objects.all().delete()\n    for i in range(1, 11):\n        Book.objects.create(\n            isbn=f\"abcde{i}\", name=f\"none{i}\", pages=i*10, rating=i,\n            price=Decimal(f\"{i}.99\"), contact=self.a1, publisher=self.p1,\n            pubdate=timezone.now())\n\n    # Test StdDev\n    qs = Book.objects.aggregate(std_dev=StdDev('rating'))\n    self.assertAlmostEqual(qs['std_dev'], 2.8722813232691962)\n\n    # Test StdDev with sample\n    qs = Book.objects.aggregate(std_dev=StdDev('rating', sample=True))\n    self.assertAlmostEqual(qs['std_dev'], 3.0276503540974917)\n"], "sample_49": ["def test_media_absolute_url(self):\n    m = Media(css={'all': ('/path/to/css',)}, js=('http://example.com/path/to/js',))\n    self.assertEqual(str(m), \"\"\"<link href=\"/path/to/css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "def test_absolute_path(self):\n    media = Media(css={'all': ['relative/path.css']}, js=['relative/path.js'])\n    self.assertEqual(media.absolute_path('relative/path.css'), '/static/relative/path.css')\n    self.assertEqual(media.absolute_path('/absolute/path.css'), '/absolute/path.css')\n    self.assertEqual(media.absolute_path('http://example.com/remote/path.css'), 'http://example.com/remote/path.css')\n    self.assertEqual(media.absolute_path('https://example.com/remote/path.css'), 'https://example.com/remote/path.css')\n", "def test_absolute_path(self):\n    media = Media(css={'all': ['relative/path.css', '/absolute/path.css']}, js=['relative/script.js', '/absolute/script.js'])\n    self.assertEqual(media.absolute_path('relative/path.css'), 'http://media.example.com/static/relative/path.css')\n    self.assertEqual(media.absolute_path('/absolute/path.css'), '/absolute/path.css')\n    self.assertEqual(media.absolute_path('relative/script.js'), 'http://media.example.com/static/relative/script.js')\n    self.assertEqual(media.absolute_path('/absolute/script.js'), '/absolute/script.js')\n", "def test_absolute_path(self):\n    m = Media(css={'all': ('path/to/css1', '/path/to/css2')}, js=('http://media.other.com/path/to/js2',))\n    self.assertEqual(m.absolute_path('path/to/css1'), 'http://media.example.com/static/path/to/css1')\n    self.assertEqual(m.absolute_path('/path/to/css2'), '/path/to/css2')\n    self.assertEqual(m.absolute_path('http://media.other.com/path/to/js2'), 'http://media.other.com/path/to/js2')\n", "def test_media_absolute_path(self):\n    media = Media(css={'all': ['path/to/css']}, js=['path/to/js'])\n    self.assertEqual(media.absolute_path('path/to/css'), 'http://media.example.com/static/path/to/css')\n    self.assertEqual(media.absolute_path('/path/to/css'), '/path/to/css')\n    self.assertEqual(media.absolute_path('http://example.com/path/to/css'), 'http://example.com/path/to/css')\n    self.assertEqual(media.absolute_path('https://example.com/path/to/css'), 'https://example.com/path/to/css')\n"], "sample_50": ["def test_sigint_handler(self):\n    original_sigint_handler = signal.getsignal(signal.SIGINT)\n\n        pass\n\n    signal.signal(signal.SIGINT, mock_sigint_handler)\n    try:\n        DatabaseClient.runshell_db({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        })\n    finally:\n        current_sigint_handler = signal.getsignal(signal.SIGINT)\n        self.assertEqual(current_sigint_handler, original_sigint_handler)\n", "def test_sigint_handler(self):\n    @mock.patch('signal.signal')\n    @mock.patch('subprocess.run')\n        DatabaseClient.runshell_db({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        })\n    _run_it()\n    with mock.patch('signal.getsignal', return_value='original_handler'):\n        _run_it()\n        signal.signal.assert_called_with(signal.SIGINT, 'original_handler')\n", "def test_sigint_handler_restored(self):\n        # Simulate a SIGINT exception.\n        raise KeyboardInterrupt()\n    with mock.patch('subprocess.run', new=_mock_subprocess_run):\n        original_sigint_handler = signal.getsignal(signal.SIGINT)\n        try:\n            DatabaseClient.runshell_db({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n                'host': 'somehost',\n                'port': '444',\n            })\n        except KeyboardInterrupt:\n            pass\n        self.assertEqual(signal.getsignal(signal.SIGINT), original_sigint_handler)\n", "def test_signal_handling(self):\n    # Test that the original SIGINT handler is restored after running the subprocess.\n    original_sigint_handler = signal.getsignal(signal.SIGINT)\n\n        # Simulate a SIGINT while the subprocess is running.\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        return subprocess.CompletedProcess(list(*args), 0)\n\n    with mock.patch('subprocess.run', new=_mock_subprocess_run):\n        DatabaseClient.runshell_db({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        })\n\n    # Check that the original SIGINT handler is restored.\n    self.assertEqual(signal.getsignal(signal.SIGINT), original_sigint_handler)\n", "def test_sigint_handler(self):\n    dbinfo = {\n        'database': 'dbname',\n        'user': 'someuser',\n        'password': 'somepassword',\n        'host': 'somehost',\n        'port': '444',\n    }\n\n    original_sigint_handler = signal.getsignal(signal.SIGINT)\n\n        self.subprocess_args = list(*args)\n        self.pgpassword = env.get('PGPASSWORD')\n        return subprocess.CompletedProcess(self.subprocess_args, 0)\n\n    with mock.patch('subprocess.run', new=_mock_subprocess_run):\n        try:\n            DatabaseClient.runshell_db(dbinfo)\n        finally:\n            current_sigint_handler = signal.getsignal(signal.SIGINT)\n            self.assertEqual(current_sigint_handler, original_sigint_handler)\n"], "sample_51": ["def test_iso8601_duration(self):\n    test_values = (\n        ('P3DT12H30M5S', timedelta(days=3, hours=12, minutes=30, seconds=5)),\n        ('PT3H4M5S', timedelta(hours=3, minutes=4, seconds=5)),\n        ('P2D', timedelta(days=2)),\n        ('PT15M', timedelta(minutes=15)),\n        ('-P3DT12H30M5S', timedelta(days=-3, hours=-12, minutes=-30, seconds=-5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3Y6M4DT12H30M5S', timedelta(days=1267, hours=12, minutes=30, seconds=5)),\n        ('P23DT23H', timedelta(days=23, hours=23)),\n        ('PT0S', timedelta()),\n        ('PT5S', timedelta(seconds=5)),\n        ('-P3Y6M4DT12H30M5S', timedelta(days=-1267, hours=-12, minutes=-30, seconds=-5)),\n        ('P0D', timedelta(days=0)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration(self):\n    test_values = (\n        ('P3DT12H30M5S', timedelta(days=3, hours=12, minutes=30, seconds=5)),\n        ('PT12H30M5S', timedelta(hours=12, minutes=30, seconds=5)),\n        ('P3D', timedelta(days=3)),\n        ('PT5S', timedelta(seconds=5)),\n        ('P0D', timedelta(days=0)),\n        ('-P3DT12H30M5S', timedelta(days=-3, hours=-12, minutes=-30, seconds=-5)),\n        ('P3DT-12H30M5S', timedelta(days=3, hours=-12, minutes=30, seconds=5)),\n        ('P-3DT12H30M5S', timedelta(days=-3, hours=12, minutes=30, seconds=5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('P1DT12H', timedelta(days=1, hours=12)),\n        ('PT23H', timedelta(hours=23)),\n        ('P3Y6M4DT12H30M5S', None),  # This one is not supported by the current implementation\n        ('P1W', None),  # This one is not supported by the current implementation\n        ('PT1H2M3.4S', timedelta(hours=1, minutes=2, seconds=3, milliseconds=400)),\n        ('-P3D', timedelta(days=-3)),\n        ('-PT23H', timedelta(hours=-23)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('PT4H', timedelta(hours=4)),\n        ('PT5M', timedelta(minutes=5)),\n        ('PT6S', timedelta(seconds=6)),\n        ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n        ('-P1DT2H3M4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n        ('P0D', timedelta()),\n        ('PT0S', timedelta()),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n"], "sample_52": ["def test_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name__startswith': 'A'})\n    self.assertEqual(len(f.choices), 2)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=lambda: {'name__startswith': 'T'})\n    self.assertEqual(len(f.choices), 2)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_modelchoicefield_limit_choices_to(self):\n    limited_choices = Category.objects.filter(slug__contains='test')\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'slug__contains': 'test'})\n    self.assertQuerysetEqual(f.queryset, limited_choices, ordered=False)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name__startswith': 'A'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n\n    # Update the limit_choices_to parameter of an existing ModelChoiceField.\n    f.limit_choices_to = {'name__startswith': 'T'}\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_limit_choices_to(self):\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'slug': 'test'})\n    self.assertEqual(len(f.choices), 2)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n    ])\n\n    # Ensure limit_choices_to can be changed after the field is created.\n    f.limit_choices_to = {'url': 'entertainment'}\n    self.assertEqual(len(f.choices), 2)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n    ])\n\n    # Ensure a callable limit_choices_to works.\n        return {'name': 'Third'}\n\n    f.limit_choices_to = limit_func\n    self.assertEqual(len(f.choices), 2)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_model_choice_field_get_limit_choices_to(self):\n        return {'name': 'A test'}\n\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=limit_choices_to)\n    self.assertEqual(f.get_limit_choices_to(), {'name': 'A test'})\n\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name': 'Entertainment'})\n    self.assertEqual(f.get_limit_choices_to(), {'name': 'Entertainment'})\n\n    f = forms.ModelChoiceField(Category.objects.all())\n    self.assertIsNone(f.get_limit_choices_to())\n"], "sample_54": ["def test_file_from_disk_with_non_ascii_filename(self):\n    filename = 'test_\u00e9nonc\u00e9_fileresponse.py'\n    with tempfile.NamedTemporaryFile(suffix=filename) as tmp:\n        response = FileResponse(tmp)\n        self.assertEqual(response['Content-Disposition'], f'inline; filename*=utf-8\\'\\'{filename}')\n", "def test_file_response_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt')\n    self.assertEqual(response['Content-Length'], '14')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_disk_with_non_ascii_filename(self):\n    filename = 'test_fileresponse_\u00e7.py'\n    with tempfile.NamedTemporaryFile(suffix=filename) as tmp:\n        response = FileResponse(tmp)\n        self.assertEqual(response['Content-Disposition'], f'inline; filename*=utf-8\\'\\'{filename}')\n        response.close()\n", "def test_file_response_with_non_ascii_filename(self):\n    filename = 'test_\u00e5\u00e4\u00f6.txt'\n    response = FileResponse(io.BytesIO(b'binary content'), as_attachment=True, filename=filename)\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertIn('filename*=utf-8\\'\\'', response['Content-Disposition'])\n    self.assertIn(quote(filename), response['Content-Disposition'])\n    self.assertEqual(list(response), [b'binary content'])\n", "def test_file_from_disk_with_filename(self):\n    response = FileResponse(open(__file__, 'rb'), filename='custom_filename.py')\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_filename.py\"')\n    response.close()\n"], "sample_53": ["def test_render_options_with_empty_label(self):\n    form = AlbumForm()\n    output = form.as_table()\n    self.assertIn('<option value=\"\">---------</option>', output)\n\n    class AlbumFormWithEmptyLabel(forms.ModelForm):\n        class Meta:\n            model = Album\n            fields = ['band']\n            widgets = {\n                'band': AutocompleteSelect(\n                    Album._meta.get_field('band').remote_field,\n                    admin.site,\n                    attrs={'class': 'my-class'},\n                )\n            }\n\n            super().__init__(*args, **kwargs)\n            self.fields['band'].empty_label = None\n\n    form = AlbumFormWithEmptyLabel()\n    output = form.as_table()\n    self.assertNotIn('<option value=\"\">---------</option>', output)\n", "def test_format_value(self):\n    widget = forms.SelectDateWidget()\n    value = datetime.date(2017, 5, 10)\n    self.assertEqual(widget.format_value(value), {'year': 2017, 'month': 5, 'day': 10})\n    self.assertEqual(widget.format_value('2017-05-10'), {'year': 2017, 'month': 5, 'day': 10})\n    self.assertEqual(widget.format_value(None), {'year': None, 'month': None, 'day': None})\n", "def test_optgroups(self):\n    widget = forms.Select()\n    choices = [\n        ('group1', [\n            ('value1', 'Label 1'),\n            ('value2', 'Label 2'),\n        ]),\n        ('group2', [\n            ('value3', 'Label 3'),\n            ('value4', 'Label 4'),\n        ]),\n        ('value5', 'Label 5'),\n    ]\n    expected_html = \"\"\"\n        <select>\n            <optgroup label=\"group1\">\n                <option value=\"value1\">Label 1</option>\n                <option value=\"value2\">Label 2</option>\n            </optgroup>\n            <optgroup label=\"group2\">\n                <option value=\"value3\">Label 3</option>\n                <option value=\"value4\">Label 4</option>\n            </optgroup>\n            <option value=\"value5\">Label 5</option>\n        </select>\n    \"\"\"\n    self.assertHTMLEqual(widget.render('my_field', 'value1', choices=choices), expected_html)\n", "def test_use_required_attribute(self):\n    form = AlbumForm()\n    widget = form['band'].field.widget\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertTrue(widget.use_required_attribute(''))\n    self.assertFalse(widget.use_required_attribute('some_value'))\n", "def test_format_value(self):\n    widget = forms.TextInput()\n    self.assertEqual(widget.format_value('hello'), 'hello')\n    self.assertIsNone(widget.format_value(None))\n    self.assertIsNone(widget.format_value(''))\n\n    class TestWidget(forms.TextInput):\n        is_localized = True\n\n    localized_widget = TestWidget()\n    with translation.override('en'):\n        self.assertEqual(localized_widget.format_value(12345), '12,345')\n"], "sample_58": ["def test_media(self):\n    class MediaForm(Form):\n        pass\n\n    form = MediaForm()\n    self.assertEqual(form.media, Media())\n\n    class MediaForm(Form):\n        class Media:\n            css = {'all': ('path/to/css.css',)}\n            js = ('path/to/js.js',)\n\n    form = MediaForm()\n    self.assertEqual(form.media, Media(css={'all': ('path/to/css.css',)}, js=('path/to/js.js',)))\n\n    class MediaForm(Form):\n        media_css = Media(css={'all': ('path/to/css.css',)})\n        media_js = Media(js=('path/to/js.js',))\n\n    form = MediaForm()\n    self.assertEqual(form.media, Media(css={'all': ('path/to/css.css',)}, js=('path/to/js.js',)))\n", "def test_boundfield_widget_attrs(self):\n    class SomeForm(Form):\n        field = CharField(widget=TextInput(attrs={'class': 'my-class'}))\n\n    boundfield = SomeForm()['field']\n    self.assertEqual(boundfield.widget.attrs, {'class': 'my-class'})\n    self.assertEqual(boundfield.widget.attrs.get('required'), None)\n    self.assertEqual(boundfield.widget.attrs.get('type'), None)\n", "def test_empty_fields_required(self):\n    class Person(Form):\n        name = CharField(required=True)\n        age = IntegerField(required=True)\n\n    p = Person({'name': '', 'age': ''})\n    self.assertFalse(p.is_valid())\n    self.assertEqual(p.errors, {\n        'name': ['This field is required.'],\n        'age': ['This field is required.'],\n    })\n", "def test_render_with_custom_renderer(self):\n    class CustomRenderer(forms.Renderer):\n            return 'Custom renderer output'\n\n    class MyForm(Form):\n        field = CharField()\n\n    form = MyForm(renderer=CustomRenderer())\n    self.assertEqual(form.as_p(), 'Custom renderer output')\n    self.assertEqual(form.as_table(), 'Custom renderer output')\n    self.assertEqual(form.as_ul(), 'Custom renderer output')\n", "def test_renderer_override(self):\n    class CustomForm(Form):\n        default_renderer = DjangoTemplates()\n\n    custom_renderer = CustomRenderer()\n    form = CustomForm(renderer=custom_renderer)\n    self.assertEqual(form.renderer, custom_renderer)\n"], "sample_56": ["def test_check_list_filter_item(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [\"title\", \"album\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [\"nonexistent\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent', which does not refer to a Field.\",\n            obj=SongAdmin,\n            id='admin.E116',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_display_links_not_a_list_or_tuple(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = [\"pk\", \"title\"]\n        list_display_links = 'test'\n\n    self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'list_display_links' must be a list, a tuple, or None.\",\n            obj=SongAdmin,\n            id='admin.E110',\n        )\n    ])\n", "def test_check_list_filter_item(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = ['title', 'album']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [('title', admin.AllValuesFieldListFilter)]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class InvalidListFilter:\n        pass\n\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = [('title', InvalidListFilter)]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0][1]' must inherit from 'FieldListFilter'.\",\n            obj=SongAdmin,\n            id='admin.E115',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_item(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = ['title', 'album']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class InvalidListFilterAdmin(admin.ModelAdmin):\n        list_filter = ['nonexistent']\n\n    errors = InvalidListFilterAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent', which does not refer to a Field.\",\n            obj=InvalidListFilterAdmin,\n            id='admin.E116',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_display_links_none(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_display = [\"pk\", \"title\"]\n        list_display_links = None\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_57": ["    def test_normalize_username(self):\n        field = UsernameField()\n        self.assertEqual(field.to_python('test\u2126'), 'test\u03a9')\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_widget_attrs(self):\n        field = UsernameField()\n        widget = field.widget\n        self.assertEqual(widget.attrs['autocapitalize'], 'none')\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'unusable'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'unusable_hash'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n"], "sample_59": ["def test_check_constraints(self):\n    \"\"\"\n    Check that model validation raises a Warning when a database does not \n    support check constraints.\n    \"\"\"\n\n    class TestModel(models.Model):\n        foo = models.IntegerField()\n        bar = models.IntegerField()\n\n        class Meta:\n            constraints = [\n                models.CheckConstraint(check=models.Q(foo__gte=0), name='foo_gte_0'),\n            ]\n\n    from django.core import checks\n    errors = TestModel.check()\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], checks.Warning)\n    self.assertEqual(errors[0].id, 'models.W027')\n", "def test_model_refresh_from_db(self):\n    # Regression test for #18305: Model.refresh_from_db() with no fields.\n    m1 = Model1.objects.create(pkey=1000)\n    m2 = Model2.objects.create(model1=m1)\n\n    # Change the underlying data in the database.\n    Model1.objects.filter(pk=m1.pk).update(pkey=2000)\n\n    # The refresh_from_db call should update the in-memory model instance.\n    m2.model1.refresh_from_db()\n    self.assertEqual(m2.model1.pkey, 2000)\n", "def test_full_clean(self):\n    # Ensure full_clean() doesn't raise an exception when called on an object\n    # with a custom primary key.\n    instance = NonAutoPK(name=\"one\")\n    instance.full_clean()\n\n    # Ensure full_clean() raises an exception when a field value exceeds the\n    # maximum allowed length.\n    too_long = \"x\" * 51\n    instance = Article(headline=too_long)\n    with self.assertRaises(ValidationError):\n        instance.full_clean()\n", "def test_model_clean_fields(self):\n    # Test clean_fields() method to ensure it clears the cached related objects.\n    article = Article.objects.create(headline=\"Test Article\")\n    article.reporter = None\n\n    # The reporter should not be a cached attribute on the article yet.\n    self.assertNotIn('reporter', article.__dict__)\n\n    # Accessing article.reporter caches the attribute.\n    article.reporter\n    self.assertIsNone(article.reporter)\n    self.assertIn('reporter', article.__dict__)\n\n    # Clean fields should clear the cached attribute.\n    article.clean_fields()\n    self.assertNotIn('reporter', article.__dict__)\n", "def test_deferred_fields(self):\n    # Test that deferred fields work correctly with models\n    obj = Model1.objects.create(pkey=1000)\n    obj_deferred = Model1.objects.defer('pkey').get(pk=obj.pk)\n\n    # Accessing a deferred field should load it from the database\n    self.assertEqual(obj_deferred.pkey, 1000)\n\n    # Accessing a non-deferred field should not trigger a database query\n    with self.assertNumQueries(0):\n        _ = obj_deferred.model2_set\n\n    # Deferred fields should not be included in the object's __dict__\n    self.assertNotIn('pkey', obj_deferred.__dict__)\n\n    # Deferred fields should be included in the object's __dict__ after they are accessed\n    _ = obj_deferred.pkey\n    self.assertIn('pkey', obj_deferred.__dict__)\n"], "sample_60": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n        e = Episode.objects.create(name='This Week in Django')\n        self.episode_pk = e.pk\n", "def test_get_readonly_fields(self):\n    class MediaInline(GenericTabularInline):\n        model = Media\n        readonly_fields = ['description']\n\n    ma = MediaInline(Media, self.site)\n    fields = ma.get_readonly_fields(request)\n    self.assertEqual(fields, ['description'])\n\n    class MediaInline(GenericTabularInline):\n        model = Media\n\n    ma = MediaInline(Media, self.site)\n    fields = ma.get_readonly_fields(request)\n    self.assertEqual(fields, ())\n", "    def setUp(self):\n        self.site = AdminSite()\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "def test_get_exclude(self):\n    class MediaInline(GenericTabularInline):\n        model = Media\n        exclude = ('url',)\n\n    ma = MediaInline(Media, self.site)\n    self.assertEqual(ma.get_exclude(request), ('url',))\n\n    class MediaInline(GenericTabularInline):\n        model = Media\n\n    ma = MediaInline(Media, self.site)\n    self.assertIsNone(ma.get_exclude(request))\n"], "sample_61": ["    def test_validator(self):\n        valid_usernames = ['joe', 'joe123', 'joe.123', 'joe@123']\n        invalid_usernames = ['Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', '\u0623\u062d\u0645\u062f']\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_validate(self):\n        valid_usernames = ['joe', 'username123', 'user.name']\n        invalid_usernames = ['Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', '\u0623\u062d\u0645\u062f']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', 'BigBird', 'user123']\n        invalid_usernames = [\n            \"Ren\u00e9\", \"\u0623\u062d\u0645\u062f\", \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_validator(self):\n        valid_usernames = ['joe', 'jane_doe', 'jane.doe', 'jane-doe', 'jane+doe']\n        invalid_usernames = ['Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', '\u0623\u062d\u0645\u062f', \"o'connell\"]\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_validator(self):\n        valid_usernames = ['joe', 'rene', 'bigbird']\n        invalid_usernames = [\n            \"Ren\u00e9\", \"\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30\", \"\u0623\u062d\u0645\u062f\",\n            \"o'connell\", \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_62": ["    def test_admin_site_name(self):\n        site = admin.AdminSite(name='myadmin')\n        self.assertEqual(site.name, 'myadmin')\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n"], "sample_63": ["    def setUp(self):\n        self.engine = Engine(dirs=[TEMPLATE_DIR])\n", "    def test_find_template(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        template, origin = engine.find_template('index.html')\n        self.assertEqual(template.name, 'index.html')\n", "    def test_template_found(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        template, origin = engine.find_template('index.html')\n        self.assertIsNotNone(template)\n        self.assertIsNotNone(origin)\n", "    def test_find_template(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        template, origin = engine.find_template('index.html')\n        self.assertIsNotNone(template)\n        self.assertEqual(origin.template_name, 'index.html')\n", "    def setUp(self):\n        self.engine = Engine(dirs=[TEMPLATE_DIR])\n"], "sample_64": ["    def test_file_response(self):\n        response = FileResponse(open(__file__, 'rb'))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('Content-Type', response)\n        self.assertEqual(response['Content-Type'], 'text/x-python; charset=utf-8')\n", "    def test_file_response(self):\n        with open(__file__, 'rb') as f:\n            response = FileResponse(f)\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response['Content-Type'], 'text/x-python')\n            self.assertTrue(response['Content-Length'])\n", "    def test_file_response(self):\n        file_path = os.path.join(os.path.dirname(__file__), 'abc.txt')\n        with open(file_path, 'rb') as file:\n            response = FileResponse(file)\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response['Content-Type'], 'text/plain')\n            self.assertEqual(response['Content-Length'], str(os.path.getsize(file_path)))\n", "    def test_set_cookie(self):\n        response = HttpResponse()\n        response.set_cookie('test', 'value')\n        self.assertIn('test=value', response.cookies.output(header=''))\n", "    def test_file_response(self):\n        file_path = os.path.join(os.path.dirname(__file__), 'abc.txt')\n        response = FileResponse(open(file_path, 'rb'))\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/plain')\n"], "sample_65": ["def test_i18n_language_non_english_default_plural(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    with correct plural forms if the default language is non-English.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='fr'), override('ru'):\n        response = self.client.get('/jsi18n/app6/')\n        self.assertEqual(\n            response.context['catalog']['{count} plural'],\n            ['{count} plural', '{count} plurals', '{count} pluralt']\n        )\n", "def test_i18n_language_non_english_default_with_locale_paths(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    if the default language is non-English, the selected language has a\n    translation available and a catalog composed by djangojs domain\n    translations of multiple Python packages is requested with extended\n    LOCALE_PATHS.\n    \"\"\"\n    extended_locale_paths = settings.LOCALE_PATHS + [\n        path.join(\n            path.dirname(path.dirname(path.abspath(__file__))),\n            'app6',\n            'locale',\n        ),\n    ]\n    with self.settings(LANGUAGE_CODE='fr', LOCALE_PATHS=extended_locale_paths):\n        with override('es-ar'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, 'este texto de app6 debe ser traducido')\n", "    def test_get_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = DjangoTranslation('en', domain='djangojs')\n        catalog._num_plurals = 2\n        catalog._plural_string = 'n != 1'\n        result = catalog.get_catalog()\n        self.assertIsInstance(result, dict)\n        self.assertIn('this is to be translated', result)\n", "    def test_json_catalog_content_type(self):\n        response = self.client.get('/jsoni18n/')\n        self.assertEqual(response['Content-Type'], 'application/json')\n", "def test_i18n_language_english_default_plural(self):\n    \"\"\"\n    Check if the JavaScript i18n view returns a complete language catalog\n    with plural forms if the default language is en-us, the selected language\n    has a translation available and a catalog composed by djangojs domain\n    translations of multiple Python packages is requested.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n        response = self.client.get('/jsi18n/app1/')\n        catalog_str = response.context['catalog_str']\n        self.assertIn('\"django.pluralidx\": function(n) {', catalog_str)\n        self.assertIn('var newcatalog = {', catalog_str)\n        self.assertIn('\"{count} plural2\": [\"{count} plural2\", \"{count} plural2s\"],', catalog_str)\n        self.assertIn('\"{count} plural3\": [\"{count} plural3\", \"{count} plural3s\", \"{count} plural3t\"]', catalog_str)\n"], "sample_67": ["    def test_model_form_localized_fields(self):\n        class LocalizedTripleForm(forms.ModelForm):\n            class Meta:\n                model = Triple\n                localized_fields = '__all__'\n                fields = '__all__'\n\n        f = LocalizedTripleForm({'left': 10, 'middle': 10, 'right': 10})\n        self.assertTrue(f.is_valid())\n        self.assertTrue(f.fields['left'].localize)\n        self.assertTrue(f.fields['middle'].localize)\n        self.assertTrue(f.fields['right'].localize)\n", "    def test_get_queryset(self):\n        class BaseModelFormSetTestModel(models.Model):\n            pass\n\n        BaseModelFormSetTestModel.objects.create()\n        BaseModelFormSetTestModel.objects.create()\n\n        formset = modelformset_factory(BaseModelFormSetTestModel, fields='__all__')\n        self.assertEqual(formset.get_queryset().count(), 2)\n", "    def test_model_form_with_f_expressions(self):\n        class PublicationDefaultsForm(forms.ModelForm):\n            class Meta:\n                model = PublicationDefaults\n                fields = ('title', 'date_published', 'mode', 'category')\n\n        form = PublicationDefaultsForm()\n        self.maxDiff = 2000\n        today_str = str(datetime.date.today())\n        self.assertHTMLEqual(\n            form.as_p(),\n            \"\"\"\n            <p><label for=\"id_title\">Title:</label>\n                <input id=\"id_title\" maxlength=\"30\" name=\"title\" type=\"text\" required></p>\n            <p><label for=\"id_date_published\">Date published:</label>\n                <input id=\"id_date_published\" name=\"date_published\" type=\"text\" value=\"{0}\" required>\n                <input id=\"initial-id_date_published\" name=\"initial-date_published\" type=\"hidden\" value=\"{0}\"></p>\n            <p><label for=\"id_mode\">Mode:</label> <select id=\"id_mode\" name=\"mode\">\n                <option value=\"di\" selected>direct</option>\n                <option value=\"de\">delayed</option></select>\n                <input id=\"initial-id_mode\" name=\"initial-mode\" type=\"hidden\" value=\"di\"></p>\n           <p><label for=\"id_category\">Category:</label> <select id=\"id_category\" name=\"category\">\n                <option value=\"1\">Games</option>\n                <option value=\"2\">Comics</option>\n                <option value=\"3\" selected>Novel</option></select>\n                <input id=\"initial-id_category\" name=\"initial-category\" type=\"hidden\" value=\"3\">\n            \"\"\".format(today_str)\n        )\n        empty_data = {\n            'title': '',\n            'date_published': today_str,\n            'initial-date_published': today_str,\n            'mode': 'di',\n            'initial-mode': 'di',\n            'category': '3',\n            'initial-category': '3',\n        }\n        bound_form = PublicationDefaultsForm(empty_data)\n        self.assertFalse(bound_form.has_changed())\n", "    def test_fields_for_model_with_parent_link(self):\n        fields = fields_for_model(ImprovedArticle, fields='__all__')\n        self.assertEqual(list(fields), ['article'])\n", "    def test_model_choice_field_with_none_return_value(self):\n        class ModelChoiceForm(forms.Form):\n            categories = forms.ModelChoiceField(Category.objects.all())\n\n        form = ModelChoiceForm(data={'categories': None})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['categories'], ['This field is required.'])\n\n        class ModelChoiceForm(forms.Form):\n            categories = forms.ModelChoiceField(Category.objects.all(), required=False)\n\n        form = ModelChoiceForm(data={'categories': None})\n        self.assertTrue(form.is_valid())\n        self.assertIsNone(form.cleaned_data['categories'])\n"], "sample_68": ["    def test_technical_404_response(self):\n        request = self.rf.get('/test_view/')\n        exception = Exception('Test exception')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, 'technical_404.html')\n", "    def test_cleanse_setting_dictionary(self):\n        setting = {'foo': 'bar', 'password': 'super_secret'}\n        cleansed = cleanse_setting('SETTING', setting)\n        self.assertEqual(cleansed['foo'], 'bar')\n        self.assertEqual(cleansed['password'], CLEANSED_SUBSTITUTE)\n", "def test_get_traceback_frames(self):\n    try:\n        raise ValueError(\"Test exception\")\n    except ValueError:\n        exc_type, exc_value, tb = sys.exc_info()\n    reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n    frames = reporter.get_traceback_frames()\n    self.assertIsInstance(frames, list)\n    self.assertGreater(len(frames), 0)\n    for frame in frames:\n        self.assertIn('filename', frame)\n        self.assertIn('lineno', frame)\n        self.assertIn('function', frame)\n        self.assertIn('code', frame)\n", "    def test_cleanse_setting_with_sensitive_keys(self):\n        sensitive_settings = [\n            'SECRET_KEY',\n            'PASSWORD',\n            'API_KEY',\n            'AUTH_TOKEN',\n        ]\n        for setting in sensitive_settings:\n            value = {setting: \"should not be displayed\"}\n            self.assertEqual(cleanse_setting(setting, value), {setting: CLEANSED_SUBSTITUTE})\n", "    def test_cleanse_setting_dictionary(self):\n        setting = {'foo': 'bar', 'password': 'super_secret'}\n        cleansed_setting = cleanse_setting('SETTING', setting)\n        self.assertEqual(cleansed_setting, {'foo': 'bar', 'password': CLEANSED_SUBSTITUTE})\n"], "sample_69": ["    def test_get_child_arguments_with_django_main(self):\n        # Mock the __main__ module of django\n        import django\n        original_main = django.__main__\n        django.__main__ = mock.MagicMock(__file__='/path/to/django/__main__.py')\n        \n        sys.argv = ['manage.py', 'runserver']\n        args = autoreload.get_child_arguments()\n        self.assertEqual(args, [sys.executable, '-m', 'django'] + sys.argv[1:])\n        \n        django.__main__ = original_main\n", "    def test_trigger_reload(self):\n        with mock.patch('sys.exit') as mocked_exit:\n            autoreload.trigger_reload('test_file.py')\n        self.assertEqual(mocked_exit.call_count, 1)\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "def test_wait_for_apps_ready_times_out(self):\n    app_reg = Apps()\n    thread = mock.MagicMock()\n    thread.is_alive.return_value = True\n    with mock.patch('time.sleep') as mocked_sleep:\n        with mock.patch.object(app_reg.ready_event, 'wait', return_value=False):\n            self.assertFalse(self.reloader.wait_for_apps_ready(app_reg, thread))\n    self.assertEqual(mocked_sleep.call_count, 1)\n    self.assertLessEqual(mocked_sleep.call_args[0][0], 0.1)\n", "    def test_trigger_reload(self):\n        with mock.patch('sys.exit') as mocked_exit:\n            autoreload.trigger_reload('test.py')\n        self.assertEqual(mocked_exit.call_count, 1)\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n"], "sample_70": ["def testCollector_sort(self):\n    # Create a collector with some data\n    collector = Collector(using='default')\n    model1 = A\n    model2 = R\n    model3 = M\n    collector.data = {\n        model1: [A.objects.create(name='a1'), A.objects.create(name='a2')],\n        model2: [R.objects.create(), R.objects.create()],\n        model3: [M.objects.create(), M.objects.create()],\n    }\n\n    # Add some dependencies between models\n    collector.dependencies = {\n        model1._meta.concrete_model: {model2._meta.concrete_model},\n        model2._meta.concrete_model: {model3._meta.concrete_model},\n    }\n\n    # Sort the collector\n    collector.sort()\n\n    # Check that the models are in the correct order\n    self.assertEqual(list(collector.data.keys()), [model3, model2, model1])\n", "def test_delete_with_fast_delete_on_related(self):\n    a = Avatar.objects.create()\n    u1 = User.objects.create(avatar=a)\n    u2 = User.objects.create(avatar=a)\n    # 1 for delete avatar, 1 for fast-delete users\n    self.assertNumQueries(2, a.delete)\n    self.assertFalse(Avatar.objects.exists())\n    self.assertFalse(User.objects.exists())\n", "def testCollector_sort(self):\n    # Test the Collector's sort method to ensure it properly sorts models in an order suitable for databases that\n    # don't support transactions or cannot defer constraint checks until the end of a transaction.\n    \n    # Create a collector with some data\n    collector = Collector(using='default')\n    model1 = R.objects.create()\n    model2 = S.objects.create(r=model1)\n    collector.add([model1], source=None, nullable=False)\n    collector.add([model2], source=None, nullable=False)\n\n    # Manually set up dependencies for the test\n    collector.dependencies[model1._meta.concrete_model] = {model2._meta.concrete_model}\n\n    # Call the sort method\n    collector.sort()\n\n    # Verify that model1 comes before model2 in the sorted data\n    self.assertEqual(list(collector.data.keys())[0], model1.__class__)\n    self.assertEqual(list(collector.data.keys())[1], model2.__class__)\n", "def test_can_fast_delete_with_from_field(self):\n    # Testing can_fast_delete with from_field.\n    a = A.objects.create()\n    r = R.objects.create()\n    a.auto = r\n    self.assertTrue(Collector(using='default').can_fast_delete(a, from_field=A._meta.get_field('auto')))\n    self.assertFalse(Collector(using='default').can_fast_delete(r, from_field=R._meta.get_field('a_set')))\n", "def test_delete_related_objects_with_protect(self):\n    a = create_a('protect')\n    msg = (\n        \"Cannot delete some instances of model 'R' because they are \"\n        \"referenced through a protected foreign key: 'A.protect'\"\n    )\n    with self.assertRaisesMessage(IntegrityError, msg):\n        a.protect.delete()\n    # Check that the related objects are still present\n    self.assertTrue(R.objects.filter(pk=a.protect_id).exists())\n    self.assertTrue(A.objects.filter(pk=a.pk).exists())\n"], "sample_71": ["def test_non_uniform_digit_grouping(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=','), '1,23,45,678,90')\n    self.assertEqual(nformat(1234567, '.', grouping=(2, 3), thousand_sep=','), '12,345,67')\n    self.assertEqual(nformat(1234567890, '.', grouping=(2, 3, 0), thousand_sep=','), '12,345,678,90')\n", "def test_decimal_pos_with_zero(self):\n    self.assertEqual(nformat(1234, '.', decimal_pos=0), '1234')\n    self.assertEqual(nformat(1234.2, '.', decimal_pos=0), '1234')\n    self.assertEqual(nformat(Decimal('1234'), '.', decimal_pos=0), '1234')\n    self.assertEqual(nformat(Decimal('1234.2'), '.', decimal_pos=0), '1234')\n    self.assertEqual(nformat(1234.99, '.', decimal_pos=0), '1235')\n    self.assertEqual(nformat(Decimal('1234.99'), '.', decimal_pos=0), '1235')\n", "def test_format_non_uniform_grouping(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n    self.assertEqual(nformat(-1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '-1,23,45,67')\n    self.assertEqual(nformat(1234.56, '.', decimal_pos=2, grouping=(3, 2, 0), thousand_sep=','), '1,23,4.56')\n", "def test_non_uniform_digit_grouping(self):\n    # Test non-uniform digit grouping.\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,567')\n    self.assertEqual(nformat(12345, '.', grouping=(3, 2, 0), thousand_sep=','), '12,345')\n    self.assertEqual(nformat(123, '.', grouping=(3, 2, 0), thousand_sep=','), '123')\n    self.assertEqual(nformat(123456789, '.', grouping=(2, 3, 0), thousand_sep=','), '12,345,6789')\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2), thousand_sep=','), '123,45,6789')\n    self.assertEqual(nformat(123456789, '.', grouping=(2, 3, 2), thousand_sep=','), '12,345,67,89')\n    self.assertEqual(nformat(123456789, '.', grouping=(1, 2, 3), thousand_sep=','), '1,23,456,789')\n    self.assertEqual(nformat(-123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '-12,34,56,789')\n    self.assertEqual(nformat(Decimal('123456789'), '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n"], "sample_72": ["def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    string, imports = MigrationWriter.serialize(complex(1, 2))\n    self.assertEqual(string, \"complex((1+2j))\")\n\n    Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize'):\n        MigrationWriter.serialize(complex(1, 2))\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    Serializer.unregister(complex)\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize:'):\n        MigrationWriter.serialize(complex(1 + 2j))\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    try:\n        string, imports = MigrationWriter.serialize(complex(1, 2))\n        self.assertEqual(string, \"complex((1+2j))\")\n        self.assertEqual(imports, set())\n\n        Serializer.unregister(complex)\n\n        with self.assertRaisesMessage(ValueError, 'Cannot serialize: (1+2j)'):\n            MigrationWriter.serialize(complex(1, 2))\n    finally:\n        Serializer.unregister(complex)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize:'):\n        MigrationWriter.serialize(complex(1))\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    try:\n        Serializer.unregister(complex)\n    except KeyError:\n        self.fail(\"unregister() raised a KeyError unexpectedly\")\n"], "sample_73": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._original_max_post_process_passes = storage.staticfiles_storage.max_post_process_passes\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n\n        # Create a file that will be referenced by another file.\n        self.referenced_file_path = os.path.join(temp_dir, 'test', 'referenced.css')\n        with open(self.referenced_file_path, 'w') as f:\n            f.write('/* This file will be referenced. */')\n\n        # Update settings to include the temporary directory in the static files dirs.\n        self.settings(STATICFILES_DIRS=[temp_dir]).enable()\n", "    def setUp(self):\n        super().setUp()\n        self._max_post_process_passes = storage.staticfiles_storage.max_post_process_passes\n"], "sample_75": ["    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='The Great Gatsby')\n        cls.author1 = Author.objects.create(name='F. Scott Fitzgerald', first_book=cls.book1)\n        cls.book1.authors.add(cls.author1)\n", "    def test_col_of_non_cached_related_field(self):\n        # Create an instance of Book and Author\n        book = Book.objects.create(title='Test Book')\n        author = Author.objects.create(name='Test Author')\n\n        # Get the column of 'book' attribute on AuthorWithAge\n        output_field = Book._meta.get_field('title')\n        col = AuthorWithAge._meta.get_field('first_book').get_col(AuthorWithAge, output_field)\n\n        self.assertEqual(col.target.column, 'id')\n        self.assertEqual(col.target.model, Book)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n        cls.author3 = Author.objects.create(name='Emily', first_book=cls.book2)\n        cls.book1.authors.add(cls.author1, cls.author2)\n        cls.book2.authors.add(cls.author3)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Les confessions Volume I')\n        cls.book2 = Book.objects.create(title='Candide')\n        cls.author1 = Author.objects.create(name='Rousseau', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Voltaire', first_book=cls.book2)\n        FavoriteAuthors.objects.create(author=cls.author1, likes_author=cls.author2)\n", "    def setUpTestData(cls):\n        cls.teacher1 = Teacher.objects.create(name=\"Mr Cleese\")\n        cls.teacher2 = Teacher.objects.create(name=\"Mr Idle\")\n"], "sample_74": ["def test_sigint_handler(self):\n    @mock.patch('signal.signal')\n    @mock.patch('subprocess.run')\n        DatabaseClient.runshell_db({\n            'database': 'dbname',\n            'user': 'someuser',\n            'password': 'somepassword',\n            'host': 'somehost',\n            'port': '444',\n        })\n\n    with mock.patch('signal.getsignal', return_value='original_handler'):\n        _run_it()\n        signal.signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n        signal.signal.assert_called_with(signal.SIGINT, 'original_handler')\n", "def test_empty_params(self):\n    self.assertEqual(\n        self._run_it({\n            'database': '',\n            'user': '',\n            'password': '',\n            'host': '',\n            'port': '',\n        }), (\n            ['psql', ''],\n            {},\n        )\n    )\n", "def test_sigint_handler_restored(self):\n        # Raise an exception to simulate the subprocess.run() call being interrupted.\n        raise KeyboardInterrupt()\n    with mock.patch('subprocess.run', new=_mock_subprocess_run):\n        original_sigint_handler = signal.getsignal(signal.SIGINT)\n        try:\n            DatabaseClient.runshell_db({\n                'database': 'dbname',\n                'user': 'someuser',\n                'host': 'somehost',\n                'port': '444',\n            })\n        except KeyboardInterrupt:\n            pass\n        self.assertEqual(signal.getsignal(signal.SIGINT), original_sigint_handler)\n", "def test_sigint_handler(self):\n    original_sigint_handler = signal.getsignal(signal.SIGINT)\n    try:\n        DatabaseClient.runshell_db({\n            'database': 'dbname',\n            'user': 'someuser',\n            'host': 'somehost',\n            'port': '444',\n        })\n    finally:\n        self.assertEqual(signal.getsignal(signal.SIGINT), original_sigint_handler)\n", "def test_sigint_handler_restored(self):\n        # Simulate a KeyboardInterrupt during subprocess.run()\n        raise KeyboardInterrupt\n    with mock.patch('subprocess.run', new=_mock_subprocess_run):\n        original_sigint_handler = signal.getsignal(signal.SIGINT)\n        try:\n            DatabaseClient.runshell_db({\n                'database': 'dbname',\n                'user': 'someuser',\n                'host': 'somehost',\n                'port': '444',\n            })\n        except KeyboardInterrupt:\n            pass\n        self.assertEqual(signal.getsignal(signal.SIGINT), original_sigint_handler)\n"], "sample_76": ["def test_language_settings_consistent(self):\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')]):\n        msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.'\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(msg, id='translation.E004'),\n        ])\n", "def test_language_settings_consistent(self):\n    with self.subTest('LANGUAGE_CODE in LANGUAGES'), self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.subTest('LANGUAGE_CODE not in LANGUAGES'), self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')]):\n        msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.'\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(msg, id='translation.E004'),\n        ])\n\n    with self.subTest('LANGUAGE_CODE not in LANGUAGES but is en-us'), self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_settings_consistent(self):\n    # Test that LANGUAGE_CODE is in LANGUAGES or 'en-us'\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')]):\n        msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.'\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(msg, id='translation.E004'),\n        ])\n\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.settings(LANGUAGE_CODE=None, LANGUAGES=[('en', 'English')]):\n        msg = 'You have provided an invalid value for the LANGUAGE_CODE setting: None.'\n        self.assertEqual(check_setting_language_code(None), [\n            Error(msg, id='translation.E001'),\n        ])\n", "def test_language_settings_consistent(self):\n    with self.subTest('valid'), self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    with self.subTest('invalid'), self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')]):\n        msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.'\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error(msg, id='translation.E004'),\n        ])\n\n    with self.subTest('default language code'), self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_settings_consistent(self):\n    # Test that check_language_settings_consistent returns an error when LANGUAGE_CODE is not in LANGUAGES.\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('fr', 'French')]):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004'),\n        ])\n\n    # Test that check_language_settings_consistent returns no errors when LANGUAGE_CODE is in LANGUAGES.\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n\n    # Test that check_language_settings_consistent returns no errors when LANGUAGE_CODE is 'en-us' and it's not in LANGUAGES.\n    with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_77": ["def test_urlize_nofollow(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n        ),\n        (\n            lazystr('Search for google.com/?q=!'),\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!'\n        ),\n        ('foo@example.com', '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        (\n            'Check out google.com.',\n            'Check out <a href=\"http://google.com/\">google.com</a>.'\n        ),\n        (\n            'Check out google.com,',\n            'Check out <a href=\"http://google.com/\">google.com</a>,'\n        ),\n        (\n            'Check out google.com?',\n            'Check out <a href=\"http://google.com/\">google.com</a>?'\n        ),\n        (\n            'Check out google.com!',\n            'Check out <a href=\"http://google.com/\">google.com</a>!'\n        ),\n        (\n            'Check out google.com:',\n            'Check out <a href=\"http://google.com/\">google.com</a>:'\n        ),\n        (\n            'Check out google.com;',\n            'Check out <a href=\"http://google.com/\">google.com</a>;'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('Try www.django.rocks!', 'Try <a href=\"http://www.django.rocks\">www.django.rocks</a>!'),\n        ('It\\'s http://example.com.', 'It\\'s <a href=\"http://example.com\">http://example.com</a>.'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Check out www.djangoproject.com.', 'Check out <a href=\"http://www.djangoproject.com\">www.djangoproject.com</a>.'),\n        ('Welcome to djangoproject.com!', 'Welcome to <a href=\"http://djangoproject.com\">djangoproject.com</a>!'),\n        ('Search for google.com/?q=, and see.', 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>, and see.'),\n        ('Try example.net...', 'Try <a href=\"http://example.net\">example.net</a>...'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_nofollow(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n        ),\n        (\n            lazystr('Search for google.com/?q=!'),\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!'\n        ),\n        ('foo@example.com', '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n"], "sample_78": ["def test_find_commands_empty_path(self):\n    \"\"\"\n    find_commands() returns an empty list when the path doesn't exist or is empty.\n    \"\"\"\n    self.assertEqual(find_commands('/non/existent/path'), [])\n    self.assertEqual(find_commands(''), [])\n", "def test_normalize_path_patterns(self):\n    patterns = ['foo/bar', 'baz', 'qux/*']\n    normalized = normalize_path_patterns(patterns)\n    self.assertEqual(normalized, [os.path.normpath(p) for p in patterns])\n", "def test_output_wrapper(self):\n    wrapper = OutputWrapper(StringIO())\n    wrapper.write(\"Hello, world!\")\n    self.assertEqual(wrapper._out.getvalue(), \"Hello, world!\\n\")\n    wrapper.write(\"Hello, world!\", ending=\"\")\n    self.assertEqual(wrapper._out.getvalue(), \"Hello, world!\\nHello, world!\")\n    wrapper.style_func = lambda x: x.upper()\n    wrapper.write(\"Hello, world!\")\n    self.assertEqual(wrapper._out.getvalue(), \"Hello, world!\\nHello, world!HELLO, WORLD!\\n\")\n", "def test_find_commands_empty_dir(self):\n    \"\"\"\n    find_commands should return an empty list if there are no commands in the directory.\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        self.assertEqual(find_commands(temp_dir), [])\n", "def test_command_parser_called_from_command_line(self):\n    parser = BaseCommand().create_parser('prog_name', 'subcommand')\n    self.assertIsNone(parser.called_from_command_line)\n    parser = BaseCommand()._called_from_command_line = True\n    parser = BaseCommand().create_parser('prog_name', 'subcommand')\n    self.assertTrue(parser.called_from_command_line)\n"], "sample_79": ["    def test_add(self):\n        self.check_values(('0', '5'), ('1', '6'), ('2', '7'))\n", "    def test_filesizeformat(self):\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1024}), 'file1.0 KB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1025}), 'file1.0 KB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 10240}), 'file10.0 KB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 10241}), 'file10.0 KB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1048576}), 'file1.0 MB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1048577}), 'file1.0 MB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': -1048577}), '-file1.0 MB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1073741824}), 'file1.0 GB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1099511627776}), 'file1.0 TB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1125899906842624}), 'file1.0 PB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1152921504606846976}), 'file1.0 PB')\n", "    def test_add(self):\n        self.assertEqual(self.engine.render_to_string('t', {'value': 5}), '10')\n", "    def test_string_format_float(self):\n        output = self.engine.render_to_string('t', {'value': 3.1415926535})\n        self.assertEqual(output, '3.14')\n", "    def test_add(self):\n        self.check_values(('0', '5'), ('1', '6'), ('2', '7'))\n"], "sample_80": ["def test_related_isnull(self):\n    query = Query(ObjectC)\n    where = query.build_where(Q(objecta__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.target, ObjectC._meta.get_field('objecta'))\n", "def test_related_isnull(self):\n    query = Query(ObjectC)\n    where = query.build_where(Q(objecta__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.target.model, ObjectC._meta.get_field('objecta').related_model)\n", "def test_foreign_key_isnull(self):\n    query = Query(Item)\n    where = query.build_where(Q(creator__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.target, Item._meta.get_field('creator'))\n", "def test_related_isnull(self):\n    query = Query(ObjectC)\n    where = query.build_where(Q(objecta__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.target, ObjectC._meta.get_field('objecta'))\n", "def test_related_is_null(self):\n    query = Query(ObjectC)\n    where = query.build_where(Q(objecta__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, RelatedIsNull)\n    self.assertEqual(lookup.lhs.target, ObjectC._meta.get_field('objecta'))\n"], "sample_82": ["def test_value_from_datadict_invalid_date(self):\n    data = {'field_day': '31', 'field_month': '2', 'field_year': '2000'}\n    self.assertEqual(self.widget.value_from_datadict(data, {}, 'field'), '2000-02-31')\n\n    data = {'field_day': '29', 'field_month': '2', 'field_year': '2001'}\n    self.assertEqual(self.widget.value_from_datadict(data, {}, 'field'), '2001-02-29')\n", "def test_id_for_label(self):\n    self.assertEqual(self.widget.id_for_label('id_mydate'), 'id_mydate_month')\n", "def test_value_from_datadict_custom_format(self):\n    with override_settings(DATE_INPUT_FORMATS=['%d-%m-%Y']):\n        widget = SelectDateWidget(years=('2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016'))\n        data = {'date_year': '2010', 'date_month': '8', 'date_day': '13'}\n        self.assertEqual(widget.value_from_datadict(data, {}, 'date'), '13-08-2010')\n", "def test_decompress(self):\n    widget = SelectDateWidget(years=('2014',))\n    self.assertEqual(widget.decompress(date(2014, 6, 15)), [15, 6, 2014])\n    self.assertEqual(widget.decompress('2014-06-15'), [15, 6, 2014])\n    self.assertEqual(widget.decompress(None), [None, None, None])\n    self.assertEqual(widget.decompress(''), [None, None, None])\n    with self.assertRaises(ValueError):\n        widget.decompress('invalid')\n", "def test_use_required_attribute(self):\n    widget = SelectDateWidget()\n    self.assertTrue(widget.use_required_attribute(True))\n    self.assertFalse(widget.use_required_attribute(False))\n\n    # If the first option has a value, 'required' should not be added to the\n    # widget's attributes.\n    widget = SelectDateWidget(empty_label=None)\n    self.assertFalse(widget.use_required_attribute(True))\n"], "sample_81": ["    def test_str(self):\n        pattern = RegexPattern('^test/$')\n        url_pattern = URLPattern(pattern, lambda x: None)\n        self.assertEqual(str(url_pattern), '<URLPattern ^test/$>')\n", "    def test_lookup_str_function_based_view(self):\n        resolver = get_resolver(None)\n        pattern = RegexPattern(r'^test/$')\n        url_pattern = resolver.urlconf_module.urlpatterns[0].__class__(pattern, lambda x: None)\n        self.assertEqual(url_pattern.lookup_str, 'django.urls.resolvers.<lambda>')\n", "    def test_urlpattern_check(self):\n        resolver = get_resolver()\n        # A valid pattern should return no warnings or errors.\n        self.assertEqual(resolver.url_patterns[0].check(), [])\n", "    def test_resolve_no_trailing_slash(self):\n        resolver = get_resolver()\n        with self.settings(APPEND_SLASH=False):\n            match = resolver.resolve('/example/')\n            self.assertIsNone(match)\n", "    def test_url_pattern_str(self):\n        resolver = get_resolver()\n        pattern = RegexPattern(r'^test/')\n        url_pattern = resolver.url_patterns[0].__class__(pattern, lambda x: x)\n        self.assertEqual(str(url_pattern), '<URLPattern ^test/>')\n"], "sample_83": ["    def test_get_resolved_arguments(self):\n        class DummyNode(TagHelperNode):\n                super().__init__(func, takes_context, args, kwargs)\n\n            return ''\n\n        node = DummyNode(dummy_func, True, ['arg1', 'arg2'], {'kwarg1': 'value1'})\n        context = {'arg1': 'resolved_arg1', 'arg2': 'resolved_arg2'}\n        resolved_args, resolved_kwargs = node.get_resolved_arguments(context)\n        self.assertEqual(resolved_args, [context, 'resolved_arg1', 'resolved_arg2'])\n        self.assertEqual(resolved_kwargs, {'kwarg1': 'value1'})\n", "    def test_get_resolved_arguments(self):\n        node = TagHelperNode(func=lambda: '', takes_context=False, args=[], kwargs={})\n        context = {'key': 'value'}\n        resolved_args, resolved_kwargs = node.get_resolved_arguments(context)\n        self.assertEqual(resolved_args, [])\n        self.assertEqual(resolved_kwargs, {})\n", "    def setUp(self):\n        self.library = Library()\n", "    def test_get_resolved_arguments(self):\n        class DummyNode(TagHelperNode):\n                super().__init__(func, takes_context, args, kwargs)\n\n            return ''\n\n        node = DummyNode(dummy_func, True, ['arg1', 'arg2'], {'kwarg1': 'value1'})\n        context = {'arg1': 'resolved_arg1', 'arg2': 'resolved_arg2', 'value1': 'resolved_value1'}\n        resolved_args, resolved_kwargs = node.get_resolved_arguments(context)\n        self.assertEqual(resolved_args, [context, 'resolved_arg1', 'resolved_arg2'])\n        self.assertEqual(resolved_kwargs, {'kwarg1': 'resolved_value1'})\n\n        node = DummyNode(dummy_func, False, ['arg1', 'arg2'], {'kwarg1': 'value1'})\n        resolved_args, resolved_kwargs = node.get_resolved_arguments(context)\n        self.assertEqual(resolved_args, ['resolved_arg1', 'resolved_arg2'])\n        self.assertEqual(resolved_kwargs, {'kwarg1': 'resolved_value1'})\n", "    def test_get_resolved_arguments(self):\n        class DummyNode(TagHelperNode):\n                super().__init__(func, takes_context, args, kwargs)\n\n            return ''\n\n        context = {'key': 'value'}\n        node = DummyNode(dummy_func, True, ['arg1', 'arg2'], {'key': 'value'})\n\n        resolved_args, resolved_kwargs = node.get_resolved_arguments(context)\n        self.assertEqual(resolved_args, [context, 'arg1', 'arg2'])\n        self.assertEqual(resolved_kwargs, {'key': 'value'})\n"], "sample_85": ["    def test_related_field_get_cache_name(self):\n        field = ForeignKey('self', on_delete=models.CASCADE)\n        self.assertEqual(field.get_cache_name(), field.name)\n", "    def test_get_accessor_name(self):\n        field = models.ForeignKey('self', on_delete=models.CASCADE)\n        self.assertEqual(field.remote_field.get_accessor_name(), 'from_testrelatedobject')\n        field = models.ForeignKey('self', related_name='self_fk', on_delete=models.CASCADE)\n        self.assertEqual(field.remote_field.get_accessor_name(), 'self_fk')\n", "    def test_reverse_related_field(self):\n        a = A.objects.create()\n        r = R.objects.create()\n        a.m2m.add(r)\n        self.assertEqual(a.m2m.through.reverse_related_fields[0][0].name, 'r')\n", "    def test_foreign_object_reverse_one_to_one(self):\n        r = R.objects.create()\n        a = A.objects.create(o2o=r)\n        self.assertEqual(a.o2o, r)\n        self.assertEqual(r.a, a)\n", "    def test_reverse_many_to_many(self):\n        m = M.objects.create()\n        r1 = R.objects.create()\n        r2 = R.objects.create()\n        m.m2m.add(r1, r2)\n        self.assertEqual(list(r1.m_set.all()), [m])\n        self.assertEqual(list(r2.m_set.all()), [m])\n"], "sample_84": ["    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('///example.com'), '/%2F/%2Fexample.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('example.com'), 'example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('example.com'), 'example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('example.com'), 'example.com')\n"], "sample_86": ["def test_lazy_object_bool(self):\n    \"\"\"\n    Test that __bool__() works correctly on lazy objects.\n    \"\"\"\n    lazy_true = lazy(lambda: True, bool)\n    lazy_false = lazy(lambda: False, bool)\n\n    self.assertTrue(lazy_true())\n    self.assertFalse(lazy_false())\n", "def test_lazy_hash(self):\n    \"\"\"\n    hash() works correctly for Promises.\n    \"\"\"\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 4, int)\n    lazy_c = lazy(lambda: 5, int)\n\n    self.assertEqual(hash(lazy_a()), hash(lazy_b()))\n    self.assertNotEqual(hash(lazy_b()), hash(lazy_c()))\n", "def test_lazy_hash(self):\n    \"\"\"\n    hash() works correctly for Promises.\n    \"\"\"\n    lazy_obj = lazy(lambda: 42, int)\n    self.assertEqual(hash(lazy_obj()), hash(42))\n", "def test_lazy_hash(self):\n    \"\"\"\n    hash() works correctly for Promises.\n    \"\"\"\n    lazy_obj = lazy(lambda: 15, int)\n    self.assertEqual(hash(lazy_obj()), hash(15))\n", "def test_keep_lazy(self):\n    \"\"\"\n    keep_lazy marks a function as keeping its arguments lazy.\n    \"\"\"\n    @keep_lazy(int, str)\n        return a, b\n\n    lazy_a = lazy(lambda: 4, int)\n    lazy_b = lazy(lambda: 'hello', str)\n\n    result = my_func(lazy_a, lazy_b)\n    self.assertIsInstance(result, tuple)\n    self.assertIsInstance(result[0], Promise)\n    self.assertIsInstance(result[1], Promise)\n    self.assertEqual(result[0](), 4)\n    self.assertEqual(result[1](), 'hello')\n"], "sample_88": ["def test_sanitize_address_with_idna(self):\n    email_address = ('Firstname S\u00fcrname', 't\u00f3@\u00f6\u00e4\u00fc.com')\n    expected_result = '=?utf-8?q?Firstname_S=C3=BCrname?= <to@xn--4ca9at.com>'\n    self.assertEqual(sanitize_address(email_address, encoding='utf-8'), expected_result)\n", "def test_send_messages_after_close(self):\n    \"\"\"\n    send_messages() shouldn't try to send messages if close() has been called.\n    \"\"\"\n    backend = smtp.EmailBackend()\n    email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    backend.close()\n    self.assertEqual(backend.send_messages([email]), 0)\n", "def test_sanitize_address_non_ascii_real_name(self):\n    \"\"\"Email addresses with non-ASCII real names are properly sanitized.\"\"\"\n    email_address = ('Firstname S\u00fcrname', 'to@example.com')\n    sanitized_email = sanitize_address(email_address, encoding='utf-8')\n    self.assertEqual(sanitized_email, '=?utf-8?q?Firstname_S=C3=BCrname?= <to@example.com>')\n", "def test_sanitize_address_with_non_ascii_display_name_and_idna_domain(self):\n    email_address = ('Firstname S\u00fcrname', 't\u00f3@\u00e9xample.com')\n    sanitized = sanitize_address(email_address, encoding='utf-8')\n    self.assertEqual(sanitized, '=?utf-8?q?Firstname_S=C3=BCrname?= <t=C3=B3@xn--xample-9ua.com>')\n", "def test_sanitize_address_header(self):\n    \"\"\"Email addresses in headers are properly sanitized.\"\"\"\n    email = EmailMessage(\n        'Subject', 'Content', 'bounce@example.com', ['to@example.com'],\n        headers={'From': 'Firstname S\u00fcrname <from@example.com>'},\n    )\n    message = email.message()\n    self.assertEqual(message['From'], '=?utf-8?q?Firstname_S=C3=BCrname?= <from@example.com>')\n"], "sample_87": ["def test_ensure_echo_on(self):\n    with mock.patch('django.utils.autoreload.termios') as mocked_termios:\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n            self.assertTrue(mocked_termios.tcgetattr.called)\n            self.assertTrue(mocked_termios.tcsetattr.called)\n\n    with mock.patch('sys.stdin.isatty', return_value=False):\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcgetattr.called)\n        self.assertFalse(mocked_termios.tcsetattr.called)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_concurrent_access(self, mocked_exception):\n        exception = (RuntimeError, RuntimeError(), None)\n        mocked_exception.side_effect = [exception, None]\n        with self.assertRaises(RuntimeError):\n            autoreload.raise_last_exception()\n        # If _exception was not properly cleared, this would raise another RuntimeError.\n        autoreload.raise_last_exception()\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n"], "sample_89": ["    def test_trigger_reload(self):\n        with mock.patch('django.utils.autoreload.logger.info') as mocked_logger:\n            with mock.patch('django.utils.autoreload.sys.exit') as mocked_exit:\n                autoreload.trigger_reload('test.py')\n                self.assertEqual(mocked_logger.call_count, 1)\n                self.assertSequenceEqual(mocked_logger.call_args[0], ['%s changed, reloading.', 'test.py'])\n                self.assertEqual(mocked_exit.call_count, 1)\n                self.assertSequenceEqual(mocked_exit.call_args[0], [3])\n", "    def test_trigger_reload(self):\n        with mock.patch('django.utils.autoreload.sys.exit') as mocked_exit:\n            autoreload.trigger_reload('test_file.py')\n            self.assertEqual(mocked_exit.call_count, 1)\n            self.assertSequenceEqual(mocked_exit.call_args[0], [3])\n", "    def test_not_a_tty(self, mocked_isatty):\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_isatty.called)\n", "def test_get_child_arguments_with_main_module(self):\n    import django.__main__\n    original_argv = sys.argv\n    sys.argv = ['manage.py', 'runserver']\n    try:\n        args = autoreload.get_child_arguments()\n        self.assertEqual(args, [sys.executable, '-W%s' % o for o in sys.warnoptions] + ['-m', 'django'] + sys.argv[1:])\n    finally:\n        sys.argv = original_argv\n", "    def test_ensure_echo_on_when_termios_available(self, mocked_termios):\n        # Mocking termios to be available\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n            self.assertTrue(mocked_termios.tcsetattr.called)\n"], "sample_90": ["    def test_modelformset_factory_with_inheritance(self):\n        class BaseForm(forms.ModelForm):\n            class Meta:\n                model = Category\n                fields = '__all__'\n\n        FormSet = modelformset_factory(Category, form=BaseForm)\n        self.assertEqual(len(FormSet.form.base_fields), 3)\n\n        class ChildForm(BaseForm):\n            class Meta(BaseForm.Meta):\n                model = Category\n                fields = ('name', 'slug')\n\n        ChildFormSet = modelformset_factory(Category, form=ChildForm)\n        self.assertEqual(len(ChildFormSet.form.base_fields), 2)\n", "    def test_modelformset_factory_error_messages(self):\n        formset = modelformset_factory(\n            Article,\n            fields=('headline',),\n            error_messages={\n                NON_FIELD_ERRORS: {\n                    'unique_together': '%(model_name)s with this %(field_labels)s already exists.',\n                }\n            },\n        )\n        form1 = formset.forms[0]\n        form2 = formset.forms[1]\n        form1.cleaned_data = {'headline': 'Test headline'}\n        form2.cleaned_data = {'headline': 'Test headline'}\n        with self.assertRaisesMessage(ValidationError, 'Article with this Headline already exists.'):\n            formset.validate_unique()\n", "    def test_custom_field_with_choices(self):\n        class ColourfulItemForm(forms.ModelForm):\n            class Meta:\n                model = ColourfulItem\n                fields = '__all__'\n\n        colourful_item_form = ColourfulItemForm()\n        self.assertEqual(\n            colourful_item_form.fields['colour'].widget.choices,\n            [\n                ('', '---------'),\n                (1, 'Red'),\n                (2, 'Blue'),\n            ],\n        )\n", "    def test_modelformset_save_m2m_with_commit_false(self):\n        class ArticleForm(forms.ModelForm):\n            class Meta:\n                model = Article\n                fields = '__all__'\n\n        article1 = Article.objects.create(headline='Article 1')\n        article2 = Article.objects.create(headline='Article 2')\n\n        form_data = {\n            'form-0-headline': 'Article 1',\n            'form-0-pub_date': '2022-01-01',\n            'form-0-writer': '1',\n            'form-0-article': 'Hello.',\n            'form-0-categories': '1',\n            'form-1-headline': 'Article 2',\n            'form-1-pub_date': '2022-01-02',\n            'form-1-writer': '1',\n            'form-1-article': 'World.',\n            'form-1-categories': '2',\n        }\n\n        formset = forms.modelformset_factory(\n            Article, form=ArticleForm, extra=0,\n        )(data=form_data, queryset=Article.objects.all())\n\n        self.assertTrue(formset.is_valid())\n        instances = formset.save(commit=False)\n        for instance in instances:\n            instance.save()\n        formset.save_m2m()\n\n        article1.refresh_from_db()\n        article2.refresh_from_db()\n\n        self.assertEqual(article1.categories.count(), 1)\n        self.assertEqual(article2.categories.count(), 1)\n", "    def test_modelformset_factory(self):\n        # Create a formset for the Author model\n        AuthorFormSet = modelformset_factory(Author, fields=('name',))\n        formset = AuthorFormSet()\n        self.assertEqual(len(formset.forms), 1)\n\n        # Create a formset with an instance of the model\n        author = Author.objects.create(name='John Doe')\n        formset = AuthorFormSet(queryset=Author.objects.all())\n        self.assertEqual(len(formset.forms), 1)\n\n        # Create a formset with multiple instances of the model\n        author2 = Author.objects.create(name='Jane Doe')\n        formset = AuthorFormSet(queryset=Author.objects.all())\n        self.assertEqual(len(formset.forms), 2)\n"], "sample_91": ["def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n", "def test_permission_denied(self):\n    request = self.request_factory.get('/')\n    response = permission_denied(request, Exception())\n    self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n"], "sample_92": ["    def test_authenticate(self):\n        user = User.objects.create_user('test', 'test@example.com')\n        self.assertEqual(authenticate(remote_user='test'), user)\n", "    def test_no_remote_user(self):\n        request = HttpRequest()\n        self.assertIsNone(authenticate(request))\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com')\n", "    def test_remote_user_creation(self):\n        \"\"\"\n        Test that a new user is created when the remote user is unknown.\n        \"\"\"\n        backend = RemoteUserBackend()\n        user = backend.authenticate(request=None, remote_user='newuser')\n        self.assertIsNotNone(user)\n        self.assertEqual(user.username, 'newuser')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com')\n"], "sample_93": ["def test_expressionlist_iter(self):\n    expr_list = ExpressionList(F('field1'), F('field2'))\n    self.assertEqual(len(list(expr_list)), 2)\n    self.assertIsInstance(list(expr_list)[0], F)\n    self.assertIsInstance(list(expr_list)[1], F)\n", "def test_aggregation_with_custom_database_functions(self):\n    class CustomSum(Func):\n        function = 'SUM'\n        template = '%(function)s(%(expressions)s)'\n\n    vals = Book.objects.aggregate(total_price=CustomSum('price'))\n    self.assertEqual(vals, {\"total_price\": Decimal(\"270.27\")})\n", "def test_subquery_aggregation(self):\n    \"\"\"Subqueries can be used in aggregations.\"\"\"\n    publisher_ids = Publisher.objects.values_list('pk', flat=True)\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).values('publisher')\n    count = Count(Subquery(subquery, output_field=IntegerField()))\n    publishers = Publisher.objects.values('pk').annotate(count=count).order_by('pk')\n    self.assertEqual(\n        list(publishers),\n        [(p_id, len(Book.objects.filter(publisher=p_id))) for p_id in publisher_ids]\n    )\n", "def test_exists_subquery_annotation(self):\n    \"\"\"Exists subquery annotations are excluded from the GROUP BY if they are\n    not explicitly grouped against.\"\"\"\n    book_exists_qs = Book.objects.filter(\n        publisher=OuterRef('pk'), rating__gt=3\n    ).values('publisher')\n    publisher_qs = Publisher.objects.annotate(\n        has_good_books=Exists(book_exists_qs),\n    ).annotate(count=Count('book'))\n    with self.assertNumQueries(1) as ctx:\n        list(publisher_qs)\n    self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n", "def test_aggregate_over_subquery_annotation(self):\n    \"\"\"Aggregates can be used over subquery annotations.\"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).aggregate(Max('latest_book_pubdate'))\n    self.assertEqual(\n        publisher_qs,\n        {'latest_book_pubdate__max': datetime.date(2008, 11, 3)}\n    )\n"], "sample_94": ["    def test_required_fields(self):\n        \"A superuser can be created when a custom user model is in use\"\n        # We can use the management command to create a superuser\n        new_io = StringIO()\n        call_command(\n            \"createsuperuser\",\n            interactive=False,\n            email=\"joe@somewhere.org\",\n            date_of_birth=\"1976-04-01\",\n            first_name='Joe',\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = CustomUser._default_manager.get(email=\"joe@somewhere.org\")\n        self.assertEqual(u.date_of_birth, date(1976, 4, 1))\n\n        # created password should be unusable\n        self.assertFalse(u.has_usable_password())\n", "    def setUp(self):\n        self.command = createsuperuser.Command()\n        self.field = User._meta.get_field('username')\n", "    def setUp(self):\n        self.old_get_system_username = management.get_system_username\n", "    def test_createsuperuser_with_proxy_model(self):\n        new_io = StringIO()\n        call_command(\n            \"createsuperuser\",\n            interactive=False,\n            username=\"joe\",\n            email=\"joe@somewhere.org\",\n            stdout=new_io\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = User.objects.get(username=\"joe\")\n        self.assertEqual(u.email, 'joe@somewhere.org')\n\n        # created password should be unusable\n        self.assertFalse(u.has_usable_password())\n", "    def test_no_password_argument(self):\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'You must use --password with --noinput.'):\n            call_command('createsuperuser', interactive=False, username='joe', email='joe@somewhere.org', stdout=new_io)\n"], "sample_98": ["    def test_wsgi_application(self):\n        \"\"\"\n        The live server uses the WSGI application configured by the user in\n        ``settings.WSGI_APPLICATION``.\n        \"\"\"\n        with self.urlopen('/wsgi_app_test/') as f:\n            self.assertEqual(f.read(), b'WSGI application loaded successfully')\n", "    def test_get_internal_wsgi_application(self):\n        # Test that get_internal_wsgi_application returns the correct WSGI application\n        wsgi_app = get_internal_wsgi_application()\n        self.assertEqual(wsgi_app.__module__, 'django.core.wsgi')\n", "    def test_handle_error(self):\n        \"\"\"Test that handle_error logs the error and continues running.\"\"\"\n        with self.assertRaises(HTTPError) as err:\n            self.urlopen('/error_view/')\n        err.exception.close()\n        self.assertEqual(err.exception.code, 500, 'Expected 500 response')\n", "    def test_wsgi_application(self):\n        \"\"\"\n        Test that the WSGI application is correctly loaded and used by the live server.\n        \"\"\"\n        with self.urlopen('/wsgi_app_test/') as f:\n            self.assertEqual(f.read(), b'WSGI application works')\n", "    def test_wsgi_application_loading(self):\n        \"\"\"\n        Test that the WSGI application is loaded correctly.\n        \"\"\"\n        with self.urlopen('/example_view/') as f:\n            self.assertEqual(f.read(), b'example view')\n"], "sample_96": ["def test_actions_unique_with_custom_name(self):\n        pass\n\n        pass\n\n    action1.__name__ = 'action'\n    action2.__name__ = 'action'\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsInvalid(\n        BandAdmin, Band,\n        \"__name__ attributes of actions defined in \"\n        \"<class 'modeladmin.test_checks.ActionsCheckTests.\"\n        \"test_actions_unique_with_custom_name.<locals>.BandAdmin'> must be unique.\",\n        id='admin.E130',\n    )\n", "def test_actions_unique(self):\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsValid(BandAdmin, Band)\n", "    def test_readonly_fields_validation(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_view_on_site_is_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = lambda self, obj: '/'\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n", "def test_actions_unique_with_name(self):\n        pass\n\n    class BandAdmin(ModelAdmin):\n        actions = (action,)\n\n    self.assertIsValid(BandAdmin, Band)\n"], "sample_99": ["def test_trunc_func_with_timezone_transition_crossing(self):\n    \"\"\"\n    If the truncated datetime transitions to a different offset (daylight saving)\n    then the returned value will have that new timezone/offset.\n    \"\"\"\n    start_datetime = datetime(2018, 11, 4, 1, 30, 50, 321)\n    end_datetime = datetime(2019, 3, 10, 2, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=True)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=True)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    nyc = pytz.timezone('America/New_York')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=nyc)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(nyc), kind, nyc)),\n                (end_datetime, truncate_to(end_datetime.astimezone(nyc), kind, nyc))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('hour')\n    test_datetime_kind('minute')\n    test_datetime_kind('second')\n", "def test_trunc_func_with_timezone_transition_crossing(self):\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = datetime(2016, 10, 16, 0, 30)\n    end_datetime = datetime(2017, 2, 19, 23, 30)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=sao)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(sao), kind, sao)),\n                (end_datetime, truncate_to(end_datetime.astimezone(sao), kind, sao))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n", "def test_trunc_func_with_timezone_transition_crossing(self):\n    \"\"\"\n    If the truncated datetime transitions to a different offset (daylight saving)\n    then the returned value will have that new timezone/offset.\n    \"\"\"\n    start_datetime = datetime(2018, 3, 25, 1, 30, 50, 321)\n    end_datetime = datetime(2018, 11, 4, 1, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('day')\n", "def test_trunc_func_with_timezone_transition(self):\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = datetime(2016, 10, 16, 0, 30)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    self.create_model(start_datetime, None)\n\n        # Convert to target timezone before truncation\n        if tzinfo is not None:\n            value = value.astimezone(tzinfo)\n        return truncate_to(value, kind, tzinfo=tzinfo)\n\n    self.assertQuerysetEqual(\n        DTModel.objects.annotate(\n            truncated=Trunc('start_datetime', 'day', output_field=DateTimeField(), tzinfo=sao)\n        ).order_by('start_datetime'),\n        [\n            (start_datetime, truncate_to_transition(start_datetime, 'day', sao)),\n        ],\n        lambda m: (m.start_datetime, m.truncated)\n    )\n", "def test_trunc_func_with_timezone_transition(self):\n    sao = pytz.timezone('America/Sao_Paulo')\n    utc = pytz.timezone('UTC')\n    start_datetime = utc.localize(datetime(2016, 10, 16, 1))\n    end_datetime = utc.localize(datetime(2016, 2, 21, 13))\n    self.create_model(start_datetime, end_datetime)\n    with timezone.override(sao):\n        model = DTModel.objects.annotate(\n            truncated_start=TruncHour('start_datetime'),\n            truncated_end=TruncDay('end_datetime'),\n        ).get()\n        self.assertEqual(model.truncated_start.dst(), timedelta(0))\n        self.assertEqual(model.truncated_end.dst(), timedelta(0, 3600))\n"], "sample_97": ["    def test_no_exception_custom_message(self):\n        # Should raise no exception if _exception is None with custom message\n        try:\n            raise Exception('Custom Message')\n        except Exception:\n            exc_info = sys.exc_info()\n        with mock.patch('django.utils.autoreload._exception', exc_info):\n            with self.assertRaisesMessage(Exception, 'Custom Message'):\n                autoreload.raise_last_exception()\n", "    def test_trigger_reload(self, mocked_logger, mocked_exit):\n        autoreload.trigger_reload('test_file.py')\n        self.assertEqual(mocked_logger.call_count, 1)\n        self.assertSequenceEqual(mocked_logger.call_args[0], ['%s changed, reloading.', 'test_file.py'])\n        self.assertEqual(mocked_exit.call_count, 1)\n        self.assertSequenceEqual(mocked_exit.call_args[0], [3])\n", "    def test_autoreload_started_signal_sent(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.utils.autoreload.autoreload_started.send') as mocked_send:\n            reloader.run(mock.MagicMock())\n        self.assertEqual(mocked_send.call_count, 1)\n        self.assertSequenceEqual(mocked_send.call_args[0], [reloader])\n", "    def test_get_child_arguments(self):\n        original_argv = sys.argv\n        sys.argv = ['manage.py', 'runserver']\n        try:\n            args = autoreload.get_child_arguments()\n            self.assertEqual(args, [sys.executable] + ['-W%s' % o for o in sys.warnoptions] + sys.argv)\n        finally:\n            sys.argv = original_argv\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n"], "sample_100": ["    def test_sets_echo_when_stdin_is_tty(self, mocked_termios):\n        with mock.patch.object(sys.stdin, 'isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n", "    def test_raises_custom_exception(self):\n        class CustomException(Exception):\n                super().__init__(message)\n                self.code = code\n\n        # Create an exception\n        try:\n            raise CustomException('Test Message', 123)\n        except CustomException:\n            exc_info = sys.exc_info()\n\n        with mock.patch('django.utils.autoreload._exception', exc_info):\n            with self.assertRaisesMessage(CustomException, 'Test Message'):\n                autoreload.raise_last_exception()\n            exception = sys.exc_info()[1]\n            self.assertEqual(exception.code, 123)\n", "    def test_ensure_echo_on(self, mocked_termios):\n        # Make sure no exception is raised.\n        autoreload.ensure_echo_on()\n", "def test_extra_files_watched(self, mocked_modules, notify_mock):\n    extra_file = self.ensure_file(self.tempdir / 'extra.txt')\n    self.reloader.extra_files.add(extra_file)\n    with self.tick_twice():\n        self.increment_mtime(extra_file)\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [extra_file])\n", "    def test_does_nothing_if_termios_not_available(self):\n        with mock.patch('django.utils.autoreload.termios', None):\n            autoreload.ensure_echo_on()\n"], "sample_102": ["def test_union_with_foreign_key(self):\n    ReservedName.objects.create(name='rn1', order=7)\n    qs1 = Number.objects.annotate(\n        has_reserved_name=Exists(ReservedName.objects.filter(order=OuterRef('num')))\n    ).filter(has_reserved_name=True)\n    qs2 = Number.objects.filter(num=9)\n    self.assertCountEqual(qs1.union(qs2).values_list('num', flat=True), [7, 9])\n", "def test_union_with_values_list_and_distinct(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=5),\n        ReservedName(name='rn0', order=6),\n        ReservedName(name='rn9', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=6)\n    qs2 = ReservedName.objects.filter(order__lte=5)\n    union_qs = qs1.union(qs2).values_list('order', flat=True)\n    self.assertEqual(len(union_qs), 4)\n    self.assertEqual(len(union_qs.distinct()), 4)\n", "def test_union_with_empty_values_list(self):\n    qs1 = Number.objects.filter(num=1).values_list('num', flat=True)\n    qs2 = Number.objects.none().values_list('num', flat=True)\n    self.assertEqual(list(qs1.union(qs2)), [1])\n", "def test_union_with_distinct(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=4, num__lte=6)\n    self.assertEqual(len(qs1.union(qs2)), 6)\n    self.assertEqual(len(qs1.union(qs2, all=True)), 11)\n", "def test_union_with_distinct(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=1),\n        ReservedName(name='rn2', order=1),\n        ReservedName(name='rn3', order=2),\n    ])\n    qs1 = ReservedName.objects.filter(order=1)\n    qs2 = ReservedName.objects.filter(order=2)\n    self.assertEqual(qs1.union(qs2).count(), 3)\n    self.assertEqual(qs1.union(qs2).distinct().count(), 3)\n    self.assertEqual(qs1.union(qs2, all=True).count(), 3)\n    self.assertEqual(qs1.union(qs2, all=True).distinct().count(), 3)\n"], "sample_101": ["def test_limited_stream(self):\n    \"\"\"\n    LimitedStream doesn't allow reading past its limit.\n    \"\"\"\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(), b'')\n", "def test_limited_stream(self):\n    \"\"\"\n    Test the LimitedStream class.\n    \"\"\"\n    stream = BytesIO(b\"Hello World!\")\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(3), b\"Hel\")\n    self.assertEqual(limited_stream.read(2), b\"lo\")\n    self.assertEqual(limited_stream.read(), b\"\")\n\n    stream = BytesIO(b\"Hello World!\")\n    limited_stream = LimitedStream(stream, 10)\n\n    self.assertEqual(limited_stream.readline(), b\"Hello World!\")\n\n    stream = BytesIO(b\"Hello\\nWorld!\")\n    limited_stream = LimitedStream(stream, 10)\n\n    self.assertEqual(limited_stream.readline(), b\"Hello\\n\")\n    self.assertEqual(limited_stream.readline(), b\"World!\")\n", "def test_limited_stream(self):\n    \"\"\"\n    LimitedStream does not read past its limit.\n    \"\"\"\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(), b'')\n", "def test_limited_stream(self):\n    \"\"\"\n    Test the LimitedStream class.\n    \"\"\"\n    stream = BytesIO(b'Hello World!')\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(3), b'Hel')\n    self.assertEqual(limited_stream.read(2), b'lo')\n    self.assertEqual(limited_stream.read(), b'')\n\n    stream = BytesIO(b'Hello World!')\n    limited_stream = LimitedStream(stream, 15)\n\n    self.assertEqual(limited_stream.read(), b'Hello World!')\n\n    stream = BytesIO(b'Hello\\nWorld!')\n    limited_stream = LimitedStream(stream, 10)\n\n    self.assertEqual(limited_stream.readline(), b'Hello\\n')\n    self.assertEqual(limited_stream.readline(), b'World!')\n", "def test_limited_stream(self):\n    \"\"\"\n    Test LimitedStream's ability to limit the amount of data read.\n    \"\"\"\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 5)\n\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(), b'')\n"], "sample_103": ["def test_aggregate_over_subquery_annotation(self):\n    \"\"\"Aggregates can be computed over subquery annotations.\"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).aggregate(max_latest_book_pubdate=Max('latest_book_pubdate'))\n    self.assertIsInstance(publisher_qs['max_latest_book_pubdate'], datetime.date)\n", "def test_aggregation_with_filter(self):\n    \"\"\"Aggregations with filters are applied correctly.\"\"\"\n    expensive_books = Book.objects.filter(price__gt=Decimal(\"40.0\"))\n    aggregation = expensive_books.aggregate(Avg('price'))\n    self.assertEqual(aggregation['price__avg'], Decimal('75.90'))\n\n    publishers = Publisher.objects.annotate(avg_price=Avg('book__price', filter=Q(book__price__gt=Decimal(\"40.0\"))))\n    self.assertQuerysetEqual(\n        publishers.order_by('name'), [\n            ('Apress', None),\n            (\"Jonno's House of Books\", None),\n            ('Morgan Kaufmann', Decimal('75.00')),\n            ('Prentice Hall', Decimal('82.80')),\n            ('Sams', None)\n        ],\n        lambda p: (p.name, p.avg_price)\n    )\n", "def test_aggregate_over_subquery_annotation(self):\n    \"\"\"Aggregate can be used over a subquery annotation.\"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).aggregate(max_latest_book_pubdate=Max('latest_book_pubdate'))\n    self.assertEqual(\n        publisher_qs['max_latest_book_pubdate'],\n        datetime.date(2008, 11, 3)\n    )\n", "def test_aggregate_filter(self):\n    books = Book.objects.annotate(\n        num_authors=Count('authors', filter=Q(authors__name='Adrian Holovaty'))\n    ).filter(num_authors__gt=0)\n    self.assertEqual(list(books), [self.b1])\n", "def test_aggregation_with_exists(self):\n    \"\"\"Exists can be used in an aggregate expression.\"\"\"\n    authors_with_books = Author.objects.filter(\n        book__in=Book.objects.values('id')\n    ).values('id')\n    qs = Book.objects.annotate(\n        author_has_book=Exists(authors_with_books.filter(id=OuterRef('contact_id')))\n    ).aggregate(\n        num_books_by_authors_with_books=Count(Case(When(author_has_book=True, then='pk'))),\n    )\n    self.assertEqual(qs['num_books_by_authors_with_books'], 6)\n"], "sample_104": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def test_non_existent_file(self):\n        relpath = \"non_existent_file.css\"\n        err_msg = \"The file '%s' could not be found with %r.\" % (relpath, storage.staticfiles_storage._wrapped)\n        with self.assertRaisesMessage(ValueError, err_msg):\n            storage.staticfiles_storage.hashed_name(relpath)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def test_hashed_files_ordering(self):\n        self.hashed_file_path(\"cached/ordering.css\")\n        self.hashed_file_path(\"cached/ordering2.css\")\n\n        # Check if the hashed files for ordering.css and ordering2.css are different\n        self.assertNotEqual(\n            storage.staticfiles_storage.hashed_files[\"cached/ordering.css\"],\n            storage.staticfiles_storage.hashed_files[\"cached/ordering2.css\"]\n        )\n\n        # Check if the content of ordering.css and ordering2.css has been updated correctly\n        with storage.staticfiles_storage.open(storage.staticfiles_storage.hashed_files[\"cached/ordering.css\"]) as f:\n            self.assertIn(b\"other.d41d8cd98f00.css\", f.read())\n\n        with storage.staticfiles_storage.open(storage.staticfiles_storage.hashed_files[\"cached/ordering2.css\"]) as f:\n            self.assertIn(b\"other.6fbb49b9c8ac.css\", f.read())\n", "    def test_css_import_and_url_consistency(self):\n        relpath_import = self.hashed_file_path(\"cached/import.css\")\n        relpath_url = self.hashed_file_path(\"cached/url.css\")\n        with storage.staticfiles_storage.open(relpath_import) as import_file:\n            import_content = import_file.read()\n        with storage.staticfiles_storage.open(relpath_url) as url_file:\n            url_content = url_file.read()\n        self.assertIn(b\"styles.5e0040571e1a.css\", import_content)\n        self.assertIn(b\"styles.5e0040571e1a.css\", url_content)\n"], "sample_107": ["    def test_sensitive_function_caller_with_args_and_kwargs(self):\n        \"\"\"\n        Sensitive variables don't leak in the sensitive_variables decorator's\n        frame, when those variables are passed as arguments and keyword arguments\n        to the decorated function.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_args_and_kwargs_function_caller)\n            self.verify_unsafe_email(sensitive_args_and_kwargs_function_caller)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_args_and_kwargs_function_caller, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_args_and_kwargs_function_caller, check_for_POST_params=False)\n", "    def test_cleanse_setting_dictionary(self):\n        setting = {'foo': 'bar', 'password': 'super_secret'}\n        cleansed = cleanse_setting('SETTING', setting)\n        self.assertEqual(cleansed['foo'], 'bar')\n        self.assertEqual(cleansed['password'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_dict(self):\n        setting = {'key1': 'value1', 'key2': 'value2'}\n        self.assertEqual(cleanse_setting('TEST', setting), setting)\n", "    def test_get_cleansed_multivaluedict(self):\n        request = RequestFactory().post('/some_url/', {'sensitive_key': 'sensitive_value'})\n        request.sensitive_post_parameters = ['sensitive_key']\n        reporter_filter = SafeExceptionReporterFilter()\n        multivaluedict = reporter_filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(multivaluedict['sensitive_key'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_dictionary(self):\n        setting = {'foo': 'bar', 'password': 'super_secret'}\n        cleansed_setting = cleanse_setting('SETTING', setting)\n        self.assertEqual(cleansed_setting['foo'], 'bar')\n        self.assertEqual(cleansed_setting['password'], CLEANSED_SUBSTITUTE)\n"], "sample_106": ["    def test_get_conditional_response_etag(self):\n        request = HttpRequest()\n        request.method = 'GET'\n        response = HttpResponse('Hello, World!')\n        etag = '12345'\n\n        # No ETag in request, return original response\n        conditional_response = get_conditional_response(request, etag=etag, response=response)\n        self.assertIs(conditional_response, response)\n\n        # Matching ETag in request, return 304 Not Modified\n        request.META['HTTP_IF_NONE_MATCH'] = etag\n        conditional_response = get_conditional_response(request, etag=etag, response=response)\n        self.assertIsInstance(conditional_response, HttpResponseNotModified)\n\n        # Non-matching ETag in request, return original response\n        request.META['HTTP_IF_NONE_MATCH'] = '67890'\n        conditional_response = get_conditional_response(request, etag=etag, response=response)\n        self.assertIs(conditional_response, response)\n", "    def test_add_never_cache_headers(self):\n        response = HttpResponse()\n        add_never_cache_headers(response)\n        self.assertIn('Cache-Control', response)\n        self.assertIn('no-cache', response['Cache-Control'])\n        self.assertIn('no-store', response['Cache-Control'])\n        self.assertIn('must-revalidate', response['Cache-Control'])\n        self.assertIn('private', response['Cache-Control'])\n        self.assertIn('max-age=0', response['Cache-Control'])\n", "def test_patch_cache_control_max_age_zero(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=0)\n    self.assertIn('max-age=0', response['Cache-Control'])\n    self.assertNotIn('Expires', response)\n\n    response = HttpResponse()\n    patch_cache_control(response, max_age=None)\n    self.assertNotIn('max-age=', response['Cache-Control'])\n    self.assertNotIn('Expires', response)\n", "def test_get_conditional_response_if_match(self):\n    request = self.factory.get('/view/', HTTP_IF_MATCH='\"1234567890abcdef\"')\n    response = HttpResponse()\n    etag = '\"1234567890abcdef\"'\n    response['ETag'] = etag\n    conditional_response = get_conditional_response(request, etag=etag, response=response)\n    self.assertIsInstance(conditional_response, HttpResponse)\n    self.assertEqual(conditional_response.status_code, 200)\n\n    request = self.factory.get('/view/', HTTP_IF_MATCH='\"fedcba0987654321\"')\n    conditional_response = get_conditional_response(request, etag=etag, response=response)\n    self.assertIsInstance(conditional_response, HttpResponse)\n    self.assertEqual(conditional_response.status_code, 412)\n", "    def test_add_headers(self):\n        response = HttpResponse()\n        patch_response_headers(response, cache_timeout=10)\n        self.assertIn('Expires', response)\n        self.assertIn('Cache-Control', response)\n"], "sample_105": ["    def test_setup_sets_request_on_self(self):\n        request = self.rf.get('/')\n        view = View()\n        view.setup(request)\n        self.assertEqual(view.request, request)\n", "def test_context_mixin_extra_context(self):\n    \"\"\"\n    Test that ContextMixin correctly handles extra_context.\n    \"\"\"\n    view = TemplateView(template_name='generic_views/about.html')\n    view.extra_context = {'title': 'Test Title'}\n    request = self.rf.get('/')\n    response = view(request)\n    self.assertEqual(response.context['title'], 'Test Title')\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data(test_key='test_value')\n        self.assertIn('view', context)\n        self.assertEqual(context['test_key'], 'test_value')\n", "def test_context_mixin_get_context_data(self):\n    class TestContextMixin(ContextMixin):\n            return super().get_context_data(**kwargs)\n\n    view = TestContextMixin()\n    context = view.get_context_data(test_key='test_value')\n    self.assertIn('view', context)\n    self.assertEqual(context['test_key'], 'test_value')\n    self.assertEqual(context['view'], view)\n", "    def test_get_context_data_no_extra_context(self):\n        view = ContextMixin()\n        context = view.get_context_data()\n        self.assertIn('view', context)\n"], "sample_108": ["    def test_reverse_value_error_means_no_match(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(NoReverseMatch):\n            reverse('dynamic', kwargs={'value': 'abc'})\n", "    def test_reverse_value_error_means_no_match(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(NoReverseMatch):\n            reverse('dynamic', kwargs={'value': 'abc'})\n", "    def test_reverse_value_error_means_no_match(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(NoReverseMatch):\n            reverse('dynamic', kwargs={'value': 'abc'})\n", "def test_resolve_runtime_error_propagates(self):\n    @DynamicConverter.register_to_python\n        raise RuntimeError('This runtime error propagates.')\n    with self.assertRaises(RuntimeError):\n        resolve('/dynamic/abc/')\n", "    def test_reverse_value_error_means_no_match(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(NoReverseMatch):\n            reverse('dynamic', kwargs={'value': 'abc'})\n"], "sample_109": ["def test_render_options_with_translation(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'band': beatles.pk})\n    with translation.override('fr'):\n        output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    option = '<option value=\"%s\">The Who</option>' % who.pk\n    self.assertIn(selected_option, output)\n    self.assertNotIn(option, output)\n", "def test_render_options_with_translation(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    with translation.override('fr'):\n        form = AlbumForm(initial={'band': beatles.pk})\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        option = '<option value=\"%s\">The Who</option>' % who.pk\n        self.assertIn(selected_option, output)\n        self.assertNotIn(option, output)\n", "def test_autocomplete_mixin_translation(self):\n    rel = Album._meta.get_field('band').remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    with translation.override('fr'):\n        media = w.media\n    self.assertIn('admin/js/vendor/select2/i18n/fr.js', str(media))\n", "def test_autocomplete_mixin_translations(self):\n    with translation.override('fr'):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['class'], 'my-class admin-autocomplete')\n        # Check that the French translations are loaded.\n        self.assertIn('admin/js/vendor/select2/i18n/fr.js', form.media._js)\n", "def test_autocomplete_mixin_translations(self):\n    with translation.override('fr'):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['class'], 'my-class admin-autocomplete')\n        self.assertContains(form.as_table(), '/static/admin/js/vendor/select2/i18n/fr.js')\n"], "sample_111": ["def test_get_ordering_field_columns(self):\n    \"\"\"\n    Test that get_ordering_field_columns() returns a dictionary of ordering field\n    column numbers and asc/desc.\n    \"\"\"\n    m = BandAdmin(Band, custom_site)\n    request = self.factory.get('/band/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    # Simulate an ordering on the 'name' field in descending order\n    cl.params[ORDER_VAR] = '-2'\n    ordering = cl.get_ordering_field_columns()\n    self.assertEqual(ordering, {2: 'desc'})\n", "def test_get_filters_params(self):\n    \"\"\"\n    Test that get_filters_params returns all params except IGNORED_PARAMS.\n    \"\"\"\n    request = self.factory.get('/band/', data={'genres': '0', ORDER_VAR: '1', SEARCH_VAR: 'search'})\n    m = BandAdmin(Band, custom_site)\n    cl = m.get_changelist_instance(request)\n    lookup_params = cl.get_filters_params()\n    self.assertEqual(lookup_params, {'genres': '0'})\n", "def test_get_ordering_field_columns(self):\n    \"\"\"\n    Ordering field columns are correctly determined from the model admin's \n    ordering and list display.\n    \"\"\"\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get('/child/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n\n    # When no ordering is specified on the model admin, use the model's ordering.\n    self.assertEqual(cl.get_ordering_field_columns(), {1: 'asc'})\n\n    # When a field name is specified in the ordering, its column number is used.\n    m.ordering = ['name']\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {2: 'asc'})\n\n    # When an expression is specified in the ordering, its asc or desc attribute \n    # is used to determine the ordering type.\n    m.ordering = [F('name').desc()]\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {2: 'desc'})\n\n    # When an invalid expression is specified in the ordering, it is ignored.\n    m.ordering = ['nonexistentfield']\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_ordering_field_columns(), {})\n", "def test_get_filters_params(self):\n    request = self.factory.get('/band/', data={'genres': '0', 'name': 'test'})\n    request.user = self.superuser\n    m = BandAdmin(Band, custom_site)\n    cl = m.get_changelist_instance(request)\n    lookup_params = cl.get_filters_params()\n    self.assertEqual(lookup_params, {'genres': '0', 'name': 'test'})\n\n    # Test that IGNORED_PARAMS are ignored\n    request = self.factory.get('/band/', data={'genres': '0', 'name': 'test', ALL_VAR: '1'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    lookup_params = cl.get_filters_params()\n    self.assertEqual(lookup_params, {'genres': '0', 'name': 'test'})\n\n    # Test that a parameter can be removed\n    request = self.factory.get('/band/', data={'genres': '0', 'name': 'test'})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    lookup_params = cl.get_filters_params(remove=['genres'])\n    self.assertEqual(lookup_params, {'name': 'test'})\n", "def test_get_filters_params(self):\n    \"\"\"\n    Test that get_filters_params() correctly removes parameters from the query string.\n    \"\"\"\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get('/child/', data={'age': '5', 'name': 'test', ALL_VAR: ''})\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    filters_params = cl.get_filters_params()\n    self.assertEqual(filters_params, {'age': '5', 'name': 'test'})\n"], "sample_110": ["    def test_pickle_expression(self):\n        expr = Expression()\n        self.assertEqual(pickle.loads(pickle.dumps(expr)), expr)\n", "    def setUpTestData(cls):\n        for i in range(1, 6):\n            Event.objects.create(title='Event {}'.format(i), group_id=1)\n", "    def test_combine(self):\n        expr1 = F('field1')\n        expr2 = F('field2')\n        combined = expr1 + expr2\n        self.assertIsInstance(combined, CombinedExpression)\n        self.assertEqual(combined.lhs, expr1)\n        self.assertEqual(combined.rhs, expr2)\n", "    def test_pickle_raw_sql(self):\n        raw_sql = RawSQL(\"1\", ())\n        self.assertEqual(pickle.loads(pickle.dumps(raw_sql)).sql, raw_sql.sql)\n        self.assertEqual(pickle.loads(pickle.dumps(raw_sql)).params, raw_sql.params)\n", "    def setUpTestData(cls):\n        cls.g1 = Group.objects.create(name='Group 1')\n        cls.g2 = Group.objects.create(name='Group 2')\n        Event.objects.create(title='Event 1', group=cls.g1)\n        Event.objects.create(title='Event 2', group=cls.g1)\n        Event.objects.create(title='Event 3', group=cls.g2)\n"], "sample_112": ["def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should return a context with \n    'prepopulated_fields' and 'prepopulated_fields_json'.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields', template_context)\n    self.assertIn('prepopulated_fields_json', template_context)\n    self.assertIsInstance(template_context['prepopulated_fields'], list)\n    self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should return a list of prepopulated fields.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIsInstance(template_context['prepopulated_fields'], list)\n    self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should create a list of prepopulated fields.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields', template_context)\n    self.assertIn('prepopulated_fields_json', template_context)\n    self.assertIsInstance(template_context['prepopulated_fields'], list)\n    self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should include prepopulated fields for \n    the admin form and inlines.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields', template_context)\n    self.assertIn('prepopulated_fields_json', template_context)\n    self.assertIsInstance(template_context['prepopulated_fields'], list)\n    self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should generate JSON for fields.\n    \"\"\"\n    request = self.request_factory.get(reverse('admin:auth_user_change', args=[self.superuser.pk]))\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields_json', context)\n    self.assertIsInstance(context['prepopulated_fields_json'], str)\n    try:\n        json.loads(context['prepopulated_fields_json'])\n    except ValueError:\n        self.fail(\"prepopulated_fields_js generated invalid JSON.\")\n"], "sample_113": ["def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected = '^<a>/b/<c>/$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected_output = '^<a>/b/(\\\\w+)$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = '^<a>/b/<c>/$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)'\n    expected_output = '^<a>/b/(\\\\w+)'\n    self.assertEqual(utils.replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    expected_output = '^<a>/b/<c>'\n    self.assertEqual(utils.replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected = r'^<a>/b/(\\w+)$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected = r'^<a>/b/<c>/$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)'\n    expected = r'^<a>/b/(\\w+)'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    expected = r'^<a>/b/<c>'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n        expected_output = '^<a>/b/<c>/$'\n        self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected = '^<a>/b/(\\\\w+)$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected = '^<a>/b/<c>/$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)'\n    expected = '^<a>/b/(\\\\w+)'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    expected = '^<a>/b/<c>'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n"], "sample_114": ["def test_generate_added_indexes_on_m2m_through_model(self):\n    changes = self.get_changes(\n        [self.author_empty, self.book_with_multiple_authors], \n        [self.author_empty, self.book_with_multiple_authors_through_attribution]\n    )\n    added_index = models.Index(fields=[\"author_id\", \"book_id\"], name=\"attribution_author_id_book_id_idx\")\n    # Right number of migrations?\n    self.assertEqual(len(changes['otherapp']), 1)\n    # Right number of actions?\n    migration = changes['otherapp'][0]\n    self.assertEqual(len(migration.operations), 4)\n    # Right actions order?\n    self.assertOperationTypes(\n        changes, 'otherapp', 0,\n        ['CreateModel', 'RemoveField', 'AlterField', 'AddIndex']\n    )\n    self.assertOperationAttributes(changes, 'otherapp', 0, 3, model_name='attribution', index=added_index)\n", "def test_alter_model_options_with_label(self):\n    \"\"\"Changing a model's options with label should make a change.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_options])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n    })\n\n    # Changing them back to empty should also make a change\n    changes = self.get_changes([self.author_with_options], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={})\n", "def test_add_field_with_default_callable(self):\n    \"\"\"Tests adding a field with a callable default.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_name_default])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n    # Make sure the default is serializable\n    operation = changes['testapp'][0].operations[0]\n    self.assertEqual(operation.field.default, \"Ada Lovelace\")\n    state = ModelState.from_model(operation.model_name, operation.field)\n    new_state = ModelState.from_model(operation.model_name, operation.field.clone())\n    self.assertEqual(state, new_state)\n", "def test_m2m_with_through_model_and_foo_together(self):\n    \"\"\"\n    Tests index/unique_together detection when a ManyToManyField with a through\n    model is used.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.book_with_multiple_authors],\n        [self.author_empty, self.book_with_multiple_authors_through_attribution],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\", \"RemoveField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"Attribution\")\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name=\"authors\", model_name='book')\n    self.assertOperationAttributes(changes, 'otherapp', 0, 2, name=\"authors\", model_name='book')\n", "def test_remove_model_with_m2m_through(self):\n    \"\"\"\n    Removing a model that is referenced by another model's ManyToManyField\n    through should work.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_m2m_through, self.publisher, self.contract],\n        [self.publisher]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"DeleteModel\", \"DeleteModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"publishers\", model_name='author')\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"Author\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 2, name=\"Contract\")\n"], "sample_115": ["    def test_url_pattern_with_regex(self):\n        response = self.client.get('/raises404/')\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, \"<code>not-in-urls</code>, didn't match\", status_code=404)\n        # Pattern and view name of a RegexURLPattern appear.\n        self.assertContains(response, r\"^regex-post/(?P&lt;pk&gt;[0-9]+)/$\", status_code=404)\n        self.assertContains(response, \"[name='regex-post']\", status_code=404)\n", "    def test_cleansed_multivaluedict(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().post('/some_url/', {'foo': 'bar'})\n        multivaluedict = request.POST.copy()\n\n        # Test that sensitive parameters are cleansed.\n        request.sensitive_post_parameters = ['foo']\n        cleansed_multivaluedict = reporter_filter.get_cleansed_multivaluedict(request, multivaluedict)\n        self.assertEqual(cleansed_multivaluedict['foo'], reporter_filter.cleansed_substitute)\n\n        # Test that non-sensitive parameters are not cleansed.\n        request.sensitive_post_parameters = []\n        cleansed_multivaluedict = reporter_filter.get_cleansed_multivaluedict(request, multivaluedict)\n        self.assertEqual(cleansed_multivaluedict['foo'], 'bar')\n", "    def test_empty_path_info(self):\n        request = self.rf.get('/')\n        exception = Http404('Not found')\n        response = technical_404_response(request, exception)\n        self.assertContains(response, '<h1>Page not found <span>(404)</span></h1>', status_code=404)\n", "    def test_technical_404_template_used(self):\n        response = self.client.get('/raises404/')\n        self.assertTemplateUsed(response, 'technical_404.html')\n", "    def test_sanitize_path_info(self):\n        request = self.rf.get('/path/ with spaces and special chars ~`!@#$%^&*()_-+={}[]:;\\'<>,.?/')\n        exception = Http404('Django tried these URL patterns')\n        response = technical_404_response(request, exception)\n        self.assertContains(response, '/path/_with_spaces_and_special_chars_____________<>,._______/', status_code=404)\n"], "sample_116": ["def test_make_template_fragment_key_rejects_unhashable_types(self):\n    with self.assertRaises(TypeError):\n        make_template_fragment_key('foo', [{'bar': 'baz'}])\n", "def test_make_template_fragment_key_hasher(self):\n    # Test that the hasher is properly updated with the fragment name and vary on values\n    key = make_template_fragment_key('foo', ['abc', 'def'])\n    hasher = hashlib.md5()\n    hasher.update(b'foo')\n    hasher.update(b':')\n    hasher.update(b'abc')\n    hasher.update(b':')\n    hasher.update(b'def')\n    self.assertEqual(key, 'template.cache.foo.%s' % hasher.hexdigest())\n", "def test_make_template_fragment_key_without_vary_on_but_with_colon(self):\n    key = make_template_fragment_key('a.fragment:with:colon')\n    self.assertEqual(key, 'template.cache.a.fragment:with:colon.d41d8cd98f00b204e9800998ecf8427e')\n", "def test_make_template_fragment_key_with_unhashable_vary_on(self):\n    with self.assertRaises(TypeError):\n        make_template_fragment_key('foo', [{'bar': 'baz'}])\n", "def test_make_template_fragment_key_without_string(self):\n    with self.assertRaisesMessage(TypeError, \"Fragment name must be a string\"):\n        make_template_fragment_key(None)\n"], "sample_117": ["    def test_bound_data(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertEqual(field.bound_data('new_value', 'initial_value'), 'initial_value')\n", "    def test_read_only_password_hash_widget_render(self):\n        widget = ReadOnlyPasswordHashWidget()\n        html = widget.render('password', 'test_password')\n        self.assertHTMLEqual(\n            html,\n            \"\"\"\n            <div>\n                <strong>password</strong>: **********\n            </div>\n            \"\"\"\n        )\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'unusable'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'unusable'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'unusable'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n"], "sample_118": ["def test_max_in_list_size(self):\n    max_in_list_size = connection.features.max_in_list_size\n    if max_in_list_size is None:\n        self.skipTest(\"max_in_list_size is not limited\")\n    \n    authors = Author.objects.bulk_create([Author() for _ in range(max_in_list_size * 2 + 1)])\n    ids = [author.id for author in authors]\n    \n    with self.assertNumQueries(2):\n        list(Author.objects.filter(id__in=ids))\n", "def test_max_in_list_size(self):\n    max_in_list_size = connection.ops.max_in_list_size()\n    if max_in_list_size is None:\n        self.skipTest(\"max_in_list_size is not defined for this database\")\n    ids = [i for i in range(max_in_list_size + 10)]\n    Article.objects.filter(id__in=ids)\n    with self.assertNumQueries(2):\n        list(Article.objects.filter(id__in=ids))\n", "def test_range_lookup_with_extra_select(self):\n    # Range lookup should work with extra select columns (#21461)\n    self.assertSequenceEqual(\n        Article.objects.extra(select={'id_plus_one': 'id + 1'}).filter(id__range=[self.a1.id, self.a5.id]).values('id', 'id_plus_one'),\n        [\n            {'id': self.a1.id, 'id_plus_one': self.a1.id + 1},\n            {'id': self.a2.id, 'id_plus_one': self.a2.id + 1},\n            {'id': self.a3.id, 'id_plus_one': self.a3.id + 1},\n            {'id': self.a4.id, 'id_plus_one': self.a4.id + 1},\n            {'id': self.a5.id, 'id_plus_one': self.a5.id + 1},\n        ]\n    )\n", "def test_exists_subquery_outerref(self):\n    subquery = Article.objects.filter(author=OuterRef('pk'))\n    authors = Author.objects.annotate(has_article=Exists(subquery)).filter(has_article=True)\n    self.assertCountEqual(authors, [self.au1, self.au2])\n", "def test_exists_with_subquery(self):\n    subquery = Article.objects.filter(author=self.au1)\n    self.assertTrue(Article.objects.filter(Exists(subquery)).exists())\n    self.assertFalse(Article.objects.filter(~Exists(subquery)).exists())\n\n    subquery = Article.objects.filter(author=self.au2)\n    self.assertTrue(Article.objects.filter(Exists(subquery)).exists())\n    self.assertFalse(Article.objects.filter(~Exists(subquery)).exists())\n\n    # Check that the subquery is properly nested.\n    subquery = Article.objects.filter(headline__startswith=OuterRef('headline'))\n    self.assertTrue(Article.objects.filter(Exists(subquery)).exists())\n\n    # Check that the subquery can reference the outer query using OuterRef.\n    subquery = Article.objects.filter(headline=OuterRef('headline'))\n    self.assertTrue(Article.objects.filter(Exists(subquery)).exists())\n\n    # Check that the subquery can be negated.\n    subquery = ~Article.objects.filter(author=self.au1)\n    self.assertFalse(Article.objects.filter(Exists(subquery)).exists())\n    self.assertTrue(Article.objects.filter(~Exists(subquery)).exists())\n"], "sample_119": ["def test_build_filter_with_filtered_relation(self):\n    query = Query(Item)\n    where = query.build_where(Q(creator__num__gt=2))\n    self.assertIsInstance(where, WhereNode)\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_add_filter(self):\n    query = Query(Author)\n    query.add_filter(('num__gt', 2))\n    where = query.where\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_build_lookup_with_rhs_as_list(self):\n    query = Query(Author)\n    where = query.build_where(Q(num__in=[1, 2, 3]))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup.rhs, tuple)\n    self.assertEqual(lookup.rhs, (1, 2, 3))\n", "def test_negated_nullable_foreign_key(self):\n    query = Query(Item)\n    where = query.build_where(~Q(creator__isnull=True))\n    self.assertTrue(where.negated)\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, IsNull)\n    self.assertEqual(lookup.lhs.target, Item._meta.get_field('creator'))\n", "def test_build_filter_with_annotation(self):\n    query = Query(Author)\n    annotation = F('num') + 1\n    query.add_annotation(annotation, 'annotated_num', is_summary=False)\n    where = query.build_where(Q(annotated_num__gt=2))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertIsInstance(lookup.lhs, SimpleCol)\n    self.assertEqual(lookup.lhs.target, annotation)\n"], "sample_120": ["def test_unregister_serializer(self):\n    class TestSerializer(BaseSerializer):\n            return 'test(%r)' % self.value, {}\n\n    Serializer.register(complex, TestSerializer)\n    Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+2j)\"):\n        MigrationWriter.serialize(1 + 2j)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    Serializer.unregister(complex)\n\n    with self.assertRaises(ValueError):\n        MigrationWriter.serialize(complex(1, 2))\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    Serializer.unregister(complex)\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+2j)\\n\"):\n        MigrationWriter.serialize(1 + 2j)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+1j)\\n\"):\n        MigrationWriter.serialize(1 + 1j)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+2j)\"):\n        MigrationWriter.serialize(1 + 2j)\n"], "sample_121": ["    def test_check_swappable(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        self.assertEqual(SwappableModel.check(), [\n            Error(\n                \"'TEST_SWAPPABLE_MODEL' is not of the form 'app_label.app_name'.\",\n                id='models.E001',\n            ),\n        ])\n\n        with override_settings(TEST_SWAPPABLE_MODEL='invalid_models_tests.Model'):\n            self.assertEqual(SwappableModel.check(), [])\n\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        with override_settings(TEST_SWAPPABLE_MODEL='invalid_models_tests.AbstractModel'):\n            self.assertEqual(SwappableModel.check(), [\n                Error(\n                    \"'TEST_SWAPPABLE_MODEL' references 'invalid_models_tests.AbstractModel', \"\n                    \"which has not been installed, or is abstract.\",\n                    id='models.E002',\n                ),\n            ])\n", "    def test_model_attribute(self):\n        class Model(models.Model):\n            foo = models.IntegerField()\n\n                super().__init__(*args, **kwargs)\n                self.bar = None\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The field 'bar' clashes with the attribute 'bar' from model \"\n                \"'invalid_models_tests.model'.\",\n                obj=Model,\n                id='models.E006',\n            )\n        ])\n", "    def test_check_constraint_with_deferred_model(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n        self.assertEqual(Model.check(), [])\n\n        class DeferredModel(Model):\n            class Meta:\n                proxy = True\n\n        self.assertEqual(DeferredModel.check(), [])\n", "    def test_multiple_primary_keys(self):\n        class Model(models.Model):\n            foo = models.IntegerField(primary_key=True)\n            bar = models.IntegerField(primary_key=True)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The model cannot have more than one field with 'primary_key=True'.\",\n                obj=Model,\n                id='models.E026',\n            )\n        ])\n", "    def test_check_single_primary_key(self):\n        class Model(models.Model):\n            foo = models.IntegerField(primary_key=True)\n            bar = models.IntegerField(primary_key=True)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The model cannot have more than one field with 'primary_key=True'.\",\n                obj=Model,\n                id='models.E026',\n            )\n        ])\n"], "sample_122": ["def test_has_vary_header(self):\n    response = HttpResponse()\n    self.assertFalse(has_vary_header(response, 'Accept-Encoding'))\n    patch_vary_headers(response, ['Accept-Encoding'])\n    self.assertTrue(has_vary_header(response, 'Accept-Encoding'))\n    self.assertTrue(has_vary_header(response, 'accept-encoding'))\n", "    def test_patch_response_headers(self):\n        response = HttpResponse()\n        patch_response_headers(response, cache_timeout=60)\n        self.assertIn('Expires', response)\n        self.assertIn('Cache-Control', response)\n", "def test_patch_cache_control_with_private_and_public(self):\n    response = HttpResponse()\n    patch_cache_control(response, private=True, public=True)\n    self.assertEqual(response['Cache-Control'], 'public')\n", "def test_patch_cache_control_with_max_age_zero(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=0)\n    self.assertEqual(response['Cache-Control'], 'max-age=0')\n    patch_cache_control(response, max_age=10)\n    self.assertEqual(response['Cache-Control'], 'max-age=0')\n", "    def test_patch_response_headers(self):\n        response = HttpResponse()\n        patch_response_headers(response, cache_timeout=10)\n        self.assertIn('Expires', response)\n        self.assertIn('Cache-Control', response)\n"], "sample_123": ["    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('example.com'), 'example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('///example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n"], "sample_125": ["    def test_close_called(self):\n        close_called = False\n\n        class MockStreamingContent:\n                yield b'Hello'\n\n                nonlocal close_called\n                close_called = True\n\n        response = HttpResponse(MockStreamingContent())\n        self.assertFalse(close_called)\n        response.close()\n        self.assertTrue(close_called)\n", "    def test_reason_phrase(self):\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n        response.status_code = 404\n        self.assertEqual(response.reason_phrase, 'Not Found')\n        response.reason_phrase = 'Custom Reason'\n        self.assertEqual(response.reason_phrase, 'Custom Reason')\n", "    def test_set_signed_cookie(self):\n        response = HttpResponse()\n        response.set_signed_cookie('name', 'value')\n        self.assertIn('name', response.cookies)\n        self.assertNotEqual(response.cookies['name'].value, 'value')\n", "    def test_set_signed_cookie(self):\n        response = HttpResponse()\n        response.set_signed_cookie('test', 'value')\n        self.assertIn('test', response.cookies)\n        self.assertNotEqual(response.cookies['test'].value, 'value')\n", "    def test_set_default(self):\n        response = HttpResponse()\n        response.set_cookie('foo', 'bar')\n        response.setdefault('foo', 'baz')\n        self.assertEqual(response.cookies['foo'].value, 'bar')\n"], "sample_126": ["def test_add_field_with_non_ascii_default(self):\n    \"\"\"\n    #25203 - Adding a field with a non-ASCII default should generate a migration\n    that's correctly formatted.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, default=\"caf\u00e9\")),\n    ])])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\")\n    migration = changes['testapp'][0]\n    self.assertEqual(migration.operations[0].field.default, \"caf\u00e9\")\n    migration_string = str(migration)\n    self.assertIn(\"default='caf\\\\xc3\\\\xa9'\", migration_string)\n", "def test_alter_model_table_with_custom_database(self):\n    \"\"\"\n    AlterModelTable operation is generated for model when its db_table is \n    changed and the model is in a custom database.\n    \"\"\"\n    author = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ], options={\"db_table\": \"author_one\"}, db_name=\"custom\")\n    new_author = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ], options={\"db_table\": \"author_two\"}, db_name=\"custom\")\n    changes = self.get_changes([author], [new_author])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\")\n", "def test_deep_deconstruct_with_unsupported_type(self):\n    \"\"\"Deep deconstruction should return original value for unsupported types.\"\"\"\n    class UnsupportedType:\n        pass\n\n    field = models.Field(default=UnsupportedType())\n    deconstructed = MigrationAutodetector.deep_deconstruct(field)\n    self.assertEqual(deconstructed, field.default)\n", "def test_alter_model_table_on_proxy_model(self):\n    changes = self.get_changes([self.author_proxy], [self.author_proxy_notproxy])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['DeleteModel', 'CreateModel'])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"AuthorProxy\")\n\n    changes = self.get_changes([self.author_proxy_notproxy], [self.author_proxy])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['DeleteModel', 'CreateModel'])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"AuthorProxy\")\n", "def test_remove_unique_together(self):\n    model_state = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ], options={\"unique_together\": {(\"name\",)}})\n    changes = self.get_changes([model_state], [self.author_name])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", unique_together=set())\n"], "sample_127": ["def test_bulk_create_ignore_conflicts(self):\n    TwoFields.objects.create(f1=1, f2=2)\n    TwoFields.objects.bulk_create([TwoFields(f1=1, f2=3)], ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.count(), 1)\n    self.assertEqual(TwoFields.objects.get().f2, 2)\n", "def test_bulk_create_ignore_conflicts(self):\n    TwoFields.objects.create(f1=1, f2=2)\n    TwoFields.objects.bulk_create([TwoFields(id=1, f1=10, f2=20)], ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.get(id=1).f1, 1)\n", "def test_bulk_create_with_conflicting_primary_keys(self):\n    Country.objects.bulk_create([\n        Country(id=1, name=\"United States of America\", iso_two_letter=\"US\"),\n        Country(id=1, name=\"Germany\", iso_two_letter=\"DE\")\n    ])\n    self.assertEqual(Country.objects.count(), 1)\n    self.assertEqual(Country.objects.get().name, \"United States of America\")\n", "def test_bulk_create_with_conflicting_primary_keys(self):\n    data = [\n        Country(id=1, name='Germany', iso_two_letter='DE'),\n        Country(id=1, name='Poland', iso_two_letter='PL'),\n    ]\n    with self.assertRaises(IntegrityError):\n        Country.objects.bulk_create(data)\n", "def test_ignore_conflicts(self):\n    TwoFields.objects.create(f1=1, f2=1)\n    TwoFields.objects.bulk_create([TwoFields(id=1, f1=2, f2=2)], ignore_conflicts=True)\n    self.assertEqual(TwoFields.objects.get(id=1).f1, 1)\n"], "sample_128": ["    def test_covering_index_with_opclasses(self):\n        index = Index(\n            name='covering_headline_idx',\n            fields=['headline'],\n            opclasses=['varchar_pattern_ops'],\n            include=['pub_date', 'published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s varchar_pattern_ops) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "def test_clone(self):\n    index = Index(\n        fields=['headline'],\n        name='test_idx',\n        db_tablespace='test_tablespace',\n        opclasses=['varchar_pattern_ops'],\n        condition=Q(pub_date__gt=datetime.datetime(2015, 1, 1)),\n        include=['pub_date'],\n    )\n    cloned_index = index.clone()\n    self.assertEqual(cloned_index.fields, index.fields)\n    self.assertEqual(cloned_index.name, index.name)\n    self.assertEqual(cloned_index.db_tablespace, index.db_tablespace)\n    self.assertEqual(cloned_index.opclasses, index.opclasses)\n    self.assertEqual(cloned_index.condition, index.condition)\n    self.assertEqual(cloned_index.include, index.include)\n", "    def test_covering_index_with_opclasses(self):\n        index = Index(\n            name='covering_headline_idx',\n            fields=['headline'],\n            opclasses=['varchar_pattern_ops'],\n            include=['pub_date', 'published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s varchar_pattern_ops) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "def test_index_deconstruction(self):\n    index = Index(fields=['headline', '-pub_date'], name='test_idx')\n    path, args, kwargs = index.deconstruct()\n    self.assertEqual(path, 'django.db.models.Index')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {\n        'fields': ['headline', '-pub_date'],\n        'name': 'test_idx',\n    })\n    reconstructed_index = Index(**kwargs)\n    self.assertEqual(index.fields, reconstructed_index.fields)\n    self.assertEqual(index.name, reconstructed_index.name)\n\n    # Test with opclasses and condition.\n    index = Index(\n        fields=['headline'],\n        name='test_ops_class',\n        opclasses=['varchar_pattern_ops'],\n        condition=Q(headline__contains='China'),\n    )\n    path, args, kwargs = index.deconstruct()\n    self.assertEqual(path, 'django.db.models.Index')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {\n        'fields': ['headline'],\n        'name': 'test_ops_class',\n        'opclasses': ['varchar_pattern_ops'],\n        'condition': Q(headline__contains='China'),\n    })\n    reconstructed_index = Index(**kwargs)\n    self.assertEqual(index.fields, reconstructed_index.fields)\n    self.assertEqual(index.name, reconstructed_index.name)\n    self.assertEqual(index.opclasses, reconstructed_index.opclasses)\n    self.assertEqual(index.condition, reconstructed_index.condition)\n", "    def test_covering_index_with_opclasses(self):\n        index = Index(\n            name='covering_headline_idx',\n            fields=['headline'],\n            opclasses=['varchar_pattern_ops'],\n            include=['pub_date', 'published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s varchar_pattern_ops) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n"], "sample_129": ["    def test_add01(self):\n        output = self.engine.render_to_string('add01', {\"a\": 5})\n        self.assertEqual(output, '10')\n", "    def test_add01(self):\n        output = self.engine.render_to_string('add01', {\"a\": 5})\n        self.assertEqual(output, '6')\n", "    def test_add(self):\n        self.assertEqual(add(1, 2), 3)\n        self.assertEqual(add(-1, 1), 0)\n        self.assertEqual(add(-1, -1), -2)\n        self.assertEqual(add('a', 'b'), 'ab')\n        self.assertEqual(add([1], [2]), [1, 2])\n        self.assertEqual(add(1, 'a'), '')\n        self.assertEqual(add(None, 1), '')\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string('stringformat01', {\"a\": 12345})\n        self.assertEqual(output, \"1.2345E+04\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string('add01', {\"a\": 5})\n        self.assertEqual(output, '10')\n"], "sample_130": ["def test_filtering_by_annotation_with_outer_ref(self):\n    query = Query(Author)\n    annotation = F('num') + 1\n    query.add_annotation(annotation, 'num_plus_one')\n    where = query.build_where(Q(num_plus_one__gt=OuterRef('id')))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.lhs.target, annotation)\n    self.assertIsInstance(lookup.rhs, OuterRef)\n", "def test_filtered_relation(self):\n    query = Query(Item)\n    filtered_relation = F('creator__extra')\n    query.add_filtered_relation(filtered_relation, 'filtered_extra')\n    where = query.build_where(Q(filtered_extra__num__gt=2))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, GreaterThan)\n    self.assertEqual(lookup.rhs, 2)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n", "def test_build_lookup_with_invalid_name(self):\n    query = Query(Author)\n    msg = \"Unsupported lookup 'invalid' for CharField or join on the field not permitted.\"\n    with self.assertRaisesMessage(FieldError, msg):\n        query.build_lookup(['invalid'], query.get_meta().get_field('name'), 'value')\n", "def test_clone_deferred_loading(self):\n    query = Query(Item)\n    query.add_deferred_loading(['name'])\n    clone = query.clone()\n    self.assertEqual(clone.deferred_loading, query.deferred_loading)\n    clone.add_deferred_loading(['modified'])\n    self.assertNotEqual(clone.deferred_loading, query.deferred_loading)\n", "def test_clone_annotations(self):\n    query = Query(Item)\n    query.add_annotation(Lower('name'), alias='lower_name', is_summary=False)\n    clone = query.clone()\n    self.assertEqual(clone.annotations, query.annotations)\n    clone.add_annotation(Lower('modified'), alias='lower_modified', is_summary=False)\n    self.assertNotEqual(clone.annotations, query.annotations)\n"], "sample_131": ["    def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['KEEPDB'] = True\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_create_test_db.assert_called_once_with(0, True, keepdb=True)\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['NAME'] = None\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            mocked_create_test_db.assert_called_once_with(verbosity=0, autoclobber=True, keepdb=True)\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n                mocked_create_test_db.assert_called_once_with(verbosity=0, autoclobber=True, keepdb=True)\n            mocked_migrate.assert_called_once_with(\n                verbosity=0,\n                interactive=False,\n                database=DEFAULT_DB_ALIAS,\n                run_syncdb=True,\n            )\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_create_test_db_calls_migrate(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n", "def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n    creation = connection.creation_class(connection)\n    saved_settings = copy.deepcopy(connection.settings_dict)\n    try:\n        connection.settings_dict['TEST']['MIGRATE'] = True\n        with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            mocked_create_test_db.assert_called_once_with(verbosity=0, autoclobber=True, keepdb=True)\n        mocked_migrate.assert_called_once_with(\n            verbosity=0,\n            interactive=False,\n            database=connection.alias,\n            run_syncdb=True,\n        )\n    finally:\n        connection.settings_dict = saved_settings\n"], "sample_132": ["    def test_technical_404_response(self):\n        exception = Http404('Non-existent URL')\n        request = RequestFactory().get('/non-existent-url')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, 'technical_404.html')\n", "    def test_technical_404_response(self):\n        exception = Http404('Page not found')\n        request = RequestFactory().get('/test/')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, 'technical_404.html')\n", "    def test_technical_404(self):\n        exception = Http404('Page not found')\n        request = RequestFactory().get('/test/')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, 'Page not found', status_code=404)\n", "    def test_templated_response(self):\n        exception = Http404('Custom message')\n        request = RequestFactory().get('/test_view/')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, 'technical_404.html')\n        self.assertIn('Custom message', response.content.decode())\n", "    def test_request_path_info(self):\n        request = self.rf.get('/test_view/')\n        exception = Http404('test exception')\n        response = technical_404_response(request, exception)\n        self.assertContains(response, '<h2>Page not found <span>(404)</span></h2>', status_code=404)\n        self.assertContains(response, 'Request path:', status_code=404)\n"], "sample_133": ["    def test_json_catalog_content_type(self):\n        response = self.client.get('/jsoni18n/')\n        self.assertEqual(response['Content-Type'], 'application/json')\n", "    def test_jsoncatalog(self):\n        response = self.client.get('/jsoni18n/')\n        data = json.loads(response.content.decode())\n        self.assertIn('catalog', data)\n        self.assertIn('formats', data)\n        self.assertEqual(data['formats']['TIME_INPUT_FORMATS'], ['%H:%M:%S', '%H:%M:%S.%f', '%H:%M'])\n        self.assertEqual(data['formats']['FIRST_DAY_OF_WEEK'], 0)\n        self.assertIn('plural', data)\n        self.assertEqual(data['catalog']['month name\\x04May'], 'Mai')\n        self.assertEqual(data['plural'], '(n != 1)')\n", "    def test_jsoncatalog(self):\n        \"\"\"The JSONCatalog view returns a valid JSON response.\"\"\"\n        response = self.client.get('/jsoni18n/')\n        self.assertEqual(response['Content-Type'], 'application/json')\n        data = json.loads(response.content.decode())\n        self.assertIn('catalog', data)\n        self.assertIn('formats', data)\n        self.assertIn('plural', data)\n", "    def test_packages_parameter(self):\n        view = JavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n        response = view(request, packages='django.contrib.admin+django.contrib.auth')\n        self.assertEqual(response.status_code, 200)\n        catalog = json.loads(response.content.decode())\n        self.assertIn('catalog', catalog)\n        self.assertIn('formats', catalog)\n        self.assertIn('plural', catalog)\n        self.assertGreater(len(catalog['catalog']), 0)\n", "def test_i18n_with_custom_language_domain(self):\n    \"\"\"\n    Test JavaScriptCatalog with a custom language domain.\n    \"\"\"\n    with self.settings(LANGUAGE_CODE='fr'), override('fr'):\n        response = self.client.get('/jsi18n_custom_domain/')\n        self.assertContains(response, 'ce texte de app6 doit \u00eatre traduit')\n        self.assertNotContains(response, 'il faut le traduire')\n"], "sample_135": ["def test_escaped_format_characters(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    self.assertEqual(dateformat.format(my_birthday, r'\\a'), 'a')\n    self.assertEqual(dateformat.format(my_birthday, r'\\A'), 'A')\n    self.assertEqual(dateformat.format(my_birthday, r'\\m'), 'm')\n    self.assertEqual(dateformat.format(my_birthday, r'\\M'), 'M')\n    self.assertEqual(dateformat.format(my_birthday, r'\\b'), 'b')\n    self.assertEqual(dateformat.format(my_birthday, r'\\B'), 'B')\n", "def test_ambiguous_date_format_specifiers(self):\n    ambiguous_date = datetime(2015, 10, 25, 2, 30, 0)\n\n    for specifier in ['I', 'O', 'T', 'Z']:\n        self.assertEqual(dateformat.format(ambiguous_date, specifier), '')\n", "def test_weekday_full_names(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    self.assertEqual(dateformat.format(my_birthday, 'l'), 'Sunday')\n    self.assertEqual(dateformat.format(datetime(1979, 7, 9, 22, 00), 'l'), 'Monday')\n    self.assertEqual(dateformat.format(datetime(1979, 7, 10, 22, 00), 'l'), 'Tuesday')\n    self.assertEqual(dateformat.format(datetime(1979, 7, 11, 22, 00), 'l'), 'Wednesday')\n    self.assertEqual(dateformat.format(datetime(1979, 7, 12, 22, 00), 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(datetime(1979, 7, 13, 22, 00), 'l'), 'Friday')\n    self.assertEqual(dateformat.format(datetime(1979, 7, 14, 22, 00), 'l'), 'Saturday')\n", "def test_format_escape_sequence(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, r'\\H\\e\\l\\l\\o'), 'Hello')\n    self.assertEqual(dateformat.format(my_birthday, r'\\a \\A \\e \\f \\g \\G \\h \\H \\i \\P \\r \\s \\u'), \n                     'a A e f g G h H i P r s u')\n", "def test_E_format(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, 'E'), MONTHS_ALT[my_birthday.month])\n"], "sample_134": ["def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+2j)\\n\"):\n        MigrationWriter.serialize(1 + 2j)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    self.assertEqual(Serializer._registry[complex], ComplexSerializer)\n\n    Serializer.unregister(complex)\n    self.assertNotIn(complex, Serializer._registry)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    self.assertEqual(Serializer._registry[complex], ComplexSerializer)\n    Serializer.unregister(complex)\n    self.assertNotIn(complex, Serializer._registry)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    self.assertIn(complex, Serializer._registry)\n\n    Serializer.unregister(complex)\n    self.assertNotIn(complex, Serializer._registry)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    complex_value = 1 + 2j\n    Serializer.register(complex, ComplexSerializer)\n    try:\n        string, imports = MigrationWriter.serialize(complex_value)\n        self.assertEqual(string, \"complex((1+2j))\")\n        self.assertEqual(imports, set())\n    finally:\n        Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize:\"):\n        MigrationWriter.serialize(complex_value)\n"], "sample_139": ["def test_sortable_by_with_callable(self):\n    m = BandAdmin(Band, custom_site)\n    m.sortable_by = lambda request: ('name', 'genres')\n    request = self.factory.get('/band/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_sortable_by(request), ('name', 'genres'))\n", "def test_get_deleted_objects(self):\n    \"\"\"\n    Test that get_deleted_objects returns a list of related objects to be deleted.\n    \"\"\"\n    band = Band.objects.create(name='Band')\n    concert1 = Concert.objects.create(name='Concert1', group=band)\n    concert2 = Concert.objects.create(name='Concert2', group=band)\n\n    deleted_objects, model_count, perms_needed, protected = self.get_deleted_objects([band], self.superuser)\n    self.assertEqual(deleted_objects, [concert1, concert2, band])\n    self.assertEqual(model_count, {Concert: 2, Band: 1})\n    self.assertEqual(perms_needed, set())\n    self.assertEqual(protected, [])\n", "def test_log_entry_user_url(self):\n    \"\"\"\n    LogEntry.get_admin_url() returns the correct URL for a user.\n    \"\"\"\n    user = User.objects.create_superuser(username='super', password='secret', email=None)\n    self.client.force_login(user)\n    admin_url = reverse('admin:auth_user_change', args=(user.pk,))\n    self.assertEqual(LogEntry.objects.log_action(user.pk, ContentType.objects.get_for_model(User).pk, user.pk, repr(user), 1).get_admin_url(), admin_url)\n", "    def test_get_formset(self):\n        class CustomInlineFormSet(BaseInlineFormSet):\n            pass\n\n        class MyModelAdmin(InlineModelAdmin):\n            formset = CustomInlineFormSet\n\n        model_admin = MyModelAdmin(Band, custom_site)\n        request = self.factory.get('/band/')\n        request.user = self.superuser\n        formset = model_admin.get_formset(request)\n        self.assertEqual(formset.__class__, CustomInlineFormSet)\n", "def test_get_changelist_instance_custom_sortable_by(self):\n    \"\"\"\n    Test that the get_changelist_instance method returns a ChangeList instance \n    with a custom sortable_by attribute.\n    \"\"\"\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get('/child/')\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.sortable_by, m.get_sortable_by(request))\n"], "sample_137": ["def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, '^<a>/b/<c>/$')\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, '^<a>/b/(\\\\w+)')\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, '^<a>/b/<c>')\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, '^<a>/b/<c>/$')\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, '^<a>/b/(\\w+)$')\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, '^<a>/b/<c>')\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = '^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected_output = '^<a>/b/(\\\\w+)$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    expected_output = '^<a>/b/<c>'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected_output = '^<a>/b/<c>/$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected_output = '^<a>/b/(\\\\w+)$'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)'\n    expected_output = '^<a>/b/<c>'\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    replaced_pattern = replace_named_groups(pattern)\n    self.assertEqual(replaced_pattern, '^<a>/b/<c>/$')\n"], "sample_138": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test', 'subdir'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n"], "sample_140": ["    def test_sensitive_variables_all(self):\n        @sensitive_variables()\n            raise Exception\n\n        try:\n            test_func('secret', 'user123')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n            reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n            html = reporter.get_traceback_html()\n            self.assertNotIn('secret', html)\n            self.assertNotIn('user123', html)\n", "    def test_sensitive_variables(self):\n        @sensitive_variables('password', 'credit_card')\n            password = user.pass_word\n            credit_card = user.credit_card_number\n            return {'password': password, 'credit_card': credit_card}\n\n        request = RequestFactory().get('/test_view/')\n        with self.settings(DEBUG=False):\n            response = technical_500_response(request, *my_function(User()))\n            self.assertNotContains(response, 'pass_word', status_code=500)\n            self.assertNotContains(response, 'credit_card_number', status_code=500)\n", "    def test_sensitive_variables_with_no_parameters(self):\n        @sensitive_variables()\n            raise Exception('Test')\n\n        request = RequestFactory().get('/test/')\n        try:\n            test_func('password123', 'username123')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertNotIn('password123', html)\n        self.assertNotIn('username123', html)\n", "    def test_sensitive_variables_all(self):\n        @sensitive_variables()\n            raise Exception('Test')\n\n        try:\n            test_func('secret', 'public')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertNotIn('password', html)\n        self.assertNotIn('username', html)\n        self.assertIn('test_func', html)\n", "    def test_cleanse_setting_with_bytes(self):\n        initial = {'login': 'cooper', 'password': b'secret'}\n        expected = {'login': 'cooper', 'password': CLEANSED_SUBSTITUTE}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n"], "sample_141": ["    def test_deserialization_error_with_deferred_fields(self):\n        test_string = \"\"\"[{\n            \"pk\": 1,\n            \"model\": \"serializers.article\",\n            \"fields\": {\n                \"headline\": \"Deserialization error with deferred fields\",\n                \"pub_date\": \"2006-06-16T15:00:00\",\n                \"categories\": [1],\n                \"author\": \"nonexistent-author\"\n            }\n        },\n        {\n            \"pk\": 1,\n            \"model\": \"serializers.category\",\n            \"fields\": {\n                \"name\": \"Reference\"\n            }\n        }]\"\"\"\n\n        with self.assertRaisesMessage(DeserializationError, \"(serializers.article:pk=1)\"):\n            list(serializers.deserialize('json', test_string))\n", "    def test_progress_bar(self):\n        output = StringIO()\n        progress_bar = ProgressBar(output, 10)\n        for i in range(10):\n            progress_bar.update(i + 1)\n        self.assertEqual(output.getvalue().count('\\n'), 1)\n", "    def test_deserialization(self):\n        # Create a non-seekable stream\n        stream = iter([\n            '{\"pk\": 1, \"model\": \"serializers.category\", \"fields\": {\"name\": \"Reference\"}}',\n            '{\"pk\": 2, \"model\": \"serializers.category\", \"fields\": {\"name\": \"Non-fiction\"}}',\n        ])\n\n        deserializer = serializers.get_deserializer(\"json\")()\n        objects = list(deserializer.deserialize(stream))\n\n        self.assertEqual(len(objects), 2)\n", "    def test_progress_bar(self):\n        output = StringIO()\n        progress_bar = ProgressBar(output, 5)\n        for i in range(5):\n            progress_bar.update(i + 1)\n        self.assertIn('[.....]', output.getvalue())\n", "    def test_progress_bar(self):\n        output = StringIO()\n        progress_bar = serializers.base.ProgressBar(output, 10)\n        for i in range(10):\n            progress_bar.update(i + 1)\n        self.assertEqual(output.getvalue().count('\\n'), 1)\n"], "sample_142": ["def test_modelform_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n\n    self.assertFalse(modelform_defines_fields(MyModelForm))\n", "def test_check_sublists_for_unknown_fields(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        fields = ['nonexistent', ('title', 'album')]\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'fields[0]' refers to 'nonexistent', which is not a \"\n            \"callable, an attribute of 'MyModelAdmin', or an attribute of \"\n            \"'admin_checks.Song'.\",\n            obj=MyModelAdmin,\n            id='admin.E035'\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_for_non_model_fields_in_fieldsets(self):\n    \"\"\"\n    Regression for ensuring ModelAdmin.fieldsets can contain non-model fields.\n    \"\"\"\n    class SongForm(forms.ModelForm):\n        extra_data = forms.CharField()\n\n    class FieldsetsOnFormOnlyAdmin(admin.ModelAdmin):\n        form = SongForm\n        fieldsets = (\n            (None, {\n                'fields': ('title', 'extra_data')\n            }),\n        )\n\n    errors = FieldsetsOnFormOnlyAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_modelform_defines_fields(self):\n    class SongForm(forms.ModelForm):\n        pass\n\n    self.assertFalse(modelform_defines_fields(SongForm))\n\n    class SongForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(SongForm))\n", "def test_modelform_defines_fields(self):\n    class SongForm(forms.ModelForm):\n        class Meta:\n            model = Song\n\n    self.assertFalse(modelform_defines_fields(SongForm))\n\n    class SongForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(SongForm))\n\n    class SongForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            exclude = ['title']\n\n    self.assertTrue(modelform_defines_fields(SongForm))\n"], "sample_143": ["def test_camel_case_to_spaces(self):\n    items = [\n        ('HelloWorld', 'hello world'),\n        ('helloWorld', 'hello world'),\n        ('HelloWORLD', 'hello world'),\n        ('helloworld', 'helloworld'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n    # Test with lazy string\n    self.assertEqual(text.camel_case_to_spaces(lazystr('HelloWorld')), 'hello world')\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('HelloWorld', 'hello world'),\n        ('helloWorld', 'hello world'),\n        ('helloHTMLWorld', 'hello html world'),\n        ('hello_world', 'hello_world'),  # underscore should not be treated as camel case\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('HelloWorld', 'hello world'),\n        ('helloWorld', 'hello world'),\n        ('HelloWORLD', 'hello world'),\n        ('HELLOworld', 'hello world'),\n        ('hello', 'hello'),\n        ('Hello', 'hello'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('CamelCase', 'camel case'),\n        ('camelCase', 'camel case'),\n        ('Camel Case', 'camel case'),\n        ('CAMELCASE', 'camelcase'),\n        ('_Camel_Case_', '_ camel _ case _'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('HelloWorld', 'hello world'),\n        ('helloWorld', 'hello world'),\n        ('helloHTMLWorld', 'hello html world'),\n        ('hello_world', 'hello_world'),  # underscore should remain\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n"], "sample_144": ["def test_inheritance_with_concrete_abstract_concrete(self):\n    # Test a model that has concrete -> abstract -> concrete inheritance\n    child = Child.objects.create(name='child', created=datetime.datetime(2008, 6, 26, 17, 0, 0))\n    self.assertEqual(child.parent_ptr.parent_ptr.id, child.id)\n    self.assertEqual(child.parent_ptr.parent_ptr.name, child.name)\n", "def test_model_inheritance_chain(self):\n    # Create a child-parent-grandparent chain\n    place1 = Place(name=\"Guido's House of Pasta\", address='944 W. Fullerton')\n    place1.save_base(raw=True)\n    restaurant = Restaurant(\n        place_ptr=place1,\n        serves_hot_dogs=True,\n        serves_pizza=False,\n    )\n    restaurant.save_base(raw=True)\n    italian_restaurant = ItalianRestaurant(restaurant_ptr=restaurant, serves_gnocchi=True)\n    italian_restaurant.save_base(raw=True)\n\n    # Check that the child instance is an instance of all parent classes\n    self.assertIsInstance(italian_restaurant, ItalianRestaurant)\n    self.assertIsInstance(italian_restaurant, Restaurant)\n    self.assertIsInstance(italian_restaurant, Place)\n\n    # Check that the child instance has all the attributes of the parent classes\n    self.assertEqual(italian_restaurant.name, \"Guido's House of Pasta\")\n    self.assertEqual(italian_restaurant.address, '944 W. Fullerton')\n    self.assertTrue(italian_restaurant.serves_hot_dogs)\n    self.assertFalse(italian_restaurant.serves_pizza)\n    self.assertTrue(italian_restaurant.serves_gnocchi)\n", "def test_save_base_calls_pre_save_and_post_save_signals(self):\n    pre_save_called = False\n    post_save_called = False\n\n        nonlocal pre_save_called\n        pre_save_called = True\n\n        nonlocal post_save_called\n        post_save_called = True\n\n    pre_save.connect(pre_save)\n    post_save.connect(post_save)\n\n    restaurant = Restaurant.objects.create(name='Test Restaurant', address='Test Address')\n\n    self.assertTrue(pre_save_called)\n    self.assertTrue(post_save_called)\n\n    pre_save.disconnect(pre_save)\n    post_save.disconnect(post_save)\n", "def test_inherited_model_form_fields(self):\n    # Test that ModelForm fields are correctly generated for inherited models.\n    class ProfileForm(forms.ModelForm):\n        class Meta:\n            model = Profile\n            fields = '__all__'\n\n    form = ProfileForm()\n    self.assertIn('username', form.fields)\n    self.assertIn('extra', form.fields)\n\n    class UserForm(forms.ModelForm):\n        class Meta:\n            model = User\n            fields = '__all__'\n\n    form = UserForm()\n    self.assertIn('username', form.fields)\n    self.assertNotIn('extra', form.fields)\n", "def test_check_model_name_db_lookup_clashes(self):\n    class ModelNameClash(Model):\n        class Meta:\n            app_label = 'myapp'\n\n    errors = ModelNameClash.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'models.E023')\n    self.assertIn(\"The model name 'ModelNameClash' cannot start or end with an underscore\", str(errors[0]))\n\n    class ModelNameWithUnderscore(Model):\n        class Meta:\n            db_table = 'myapp_mymodel'\n            app_label = 'myapp'\n\n    errors = ModelNameWithUnderscore.check()\n    self.assertEqual(errors, [])\n\n    class ModelNameWithDoubleUnderscore(Model):\n        class Meta:\n            db_table = 'myapp_my__model'\n            app_label = 'myapp'\n\n    errors = ModelNameWithDoubleUnderscore.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'models.E024')\n    self.assertIn(\"The model name 'ModelNameWithDoubleUnderscore' cannot contain double underscores\", str(errors[0]))\n"], "sample_145": ["def test_actions_unique_with_short_description(self):\n        pass\n    action1.short_description = 'Action 1'\n\n        pass\n    action2.short_description = 'Action 1'\n\n    class BandAdmin(ModelAdmin):\n        actions = (action1, action2)\n\n    self.assertIsInvalid(\n        BandAdmin, Band,\n        \"__name__ attributes of actions defined in BandAdmin must be \"\n        \"unique. Name 'action' is not unique.\",\n        id='admin.E130',\n    )\n", "    def test_view_on_site_is_callable(self):\n        class Admin(ModelAdmin):\n            view_on_site = lambda self, obj: '/'\n\n        self.assertIsValid(Admin, Band)\n", "    def test_readonly_fields_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            'admin.E034'\n        )\n", "    def test_actions_unique_with_different_names(self):\n            pass\n\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_not_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 'hello'\n\n        self.assertIsInvalid(\n            TestModelAdmin, ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            'admin.E025'\n        )\n"], "sample_146": ["def test_consistent_language_settings(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_check_language_settings_consistent_with_valid_language_code(self):\n    with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_consistent_language_settings(self):\n    for tag in ['en', 'fr']:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_147": ["def test_union_with_filtered_relation(self):\n    qs1 = Number.objects.annotate(filtered=FilteredRelation('related', condition=Q(related__num=5)))\n    qs2 = Number.objects.annotate(filtered=FilteredRelation('related', condition=Q(related__num=6)))\n    union_qs = qs1.union(qs2)\n    self.assertQuerysetEqual(union_qs, [(0, None), (1, None), (2, None), (3, None), (4, None), (5, 5), (6, 6), (7, None), (8, None), (9, None)], lambda x: (x.num, getattr(x.filtered, 'num', None)))\n", "def test_union_with_distinct(self):\n    Number.objects.create(num=1)\n    Number.objects.create(num=1)\n    qs1 = Number.objects.filter(num=1)\n    qs2 = Number.objects.filter(num=2)\n    self.assertEqual(len(list(qs1.union(qs2))), 2)\n    self.assertEqual(len(list(qs1.union(qs2, all=True))), 3)\n", "def test_exists_with_union(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__gte=8)\n    self.assertTrue(qs1.union(qs2).exists())\n    self.assertFalse(qs1.union(Number.objects.none()).exists())\n", "def test_union_with_extra_select(self):\n    qs1 = Number.objects.filter(num=1).extra(\n        select={'is_one': 'CASE WHEN num = 1 THEN 1 ELSE 0 END'},\n    )\n    qs2 = Number.objects.filter(num=2).extra(\n        select={'is_one': 'CASE WHEN num = 1 THEN 1 ELSE 0 END'},\n    )\n    self.assertCountEqual(qs1.union(qs2).values_list('num', 'is_one'), [(1, 1), (2, 0)])\n", "def test_union_with_subqueries(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__in=Number.objects.filter(num__gte=8).values('num'))\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 8, 9], ordered=False)\n"], "sample_148": ["def test_quote(self):\n    self.assertEqual(quote('test'), 'test')\n    self.assertEqual(quote('/test/'), '_2Ftest_2F')\n    self.assertEqual(quote('_/test/_'), '_5F_2Ftest_2F_5F')\n", "def test_quote(self):\n    \"\"\"\n    Test the quote() function to ensure it escapes special characters.\n    \"\"\"\n    tests = (\n        ('example', 'example'),\n        ('example/extras', 'example%2Fextras'),\n        ('example:extras', 'example%3Aextras'),\n        ('example,extras', 'example%2Cextras'),\n    )\n    for input_str, expected in tests:\n        self.assertEqual(quote(input_str), expected)\n", "def test_unquote(self):\n    quoted = quote(\"Hello/World\")\n    unquoted = unquote(quoted)\n    self.assertEqual(unquoted, \"Hello/World\")\n", "def test_quote_unquote(self):\n    tests = (\n        ('simple', 'simple'),\n        ('with/slash', 'with_2Fslash'),\n        ('with_underscore', 'with_5Funderscore'),\n        ('with?question', 'with_3Fquestion'),\n        ('with&amper', 'with_26amper'),\n    )\n    for s, expected in tests:\n        self.assertEqual(quote(s), expected)\n        self.assertEqual(unquote(expected), s)\n", "def test_help_text_for_field(self):\n    \"\"\"\n    Tests for help_text_for_field\n    \"\"\"\n    self.assertEqual(\n        help_text_for_field(\"title\", Article),\n        \"\"\n    )\n    self.assertEqual(\n        help_text_for_field(\"hist\", Article),\n        \"History of the Article\"\n    )\n    with self.assertRaisesMessage(AttributeError, \"Unable to lookup 'unknown' on Article\"):\n        help_text_for_field(\"unknown\", Article)\n"], "sample_151": ["def test_generate_altered_order_with_respect_to(self):\n    \"\"\"Tests order_with_respect_to detection.\"\"\"\n    changes = self.get_changes([self.author_with_book], [self.author_with_book_order_wrt])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=\"book\")\n\n    changes = self.get_changes([self.author_with_book_order_wrt], [self.author_with_book])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=None)\n\n    changes = self.get_changes([self.author_with_book_order_wrt], [self.author_with_book_order_wrt])\n    # No migration\n    self.assertEqual(len(changes), 0)\n", "def test_alter_model_options_add_permissions(self):\n    \"\"\"Changing a model's permissions adds a new operation.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_options])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={\n        \"permissions\": [('can_hire', 'Can hire')],\n    })\n\n    # Changing them back to empty should also make a change\n    changes = self.get_changes([self.author_with_options], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={})\n", "def test_alter_model_table_on_swappable_model(self):\n    \"\"\"\n    AlterModelTable operation should be created when the db_table of a swappable\n    model is changed.\n    \"\"\"\n    changes = self.get_changes([self.custom_user], [self.custom_user_no_inherit])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'thirdapp', 1)\n    self.assertOperationTypes(changes, 'thirdapp', 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"thirdapp\", 0, 0, name=\"customuser\", table=None)\n", "def test_alter_order_with_respect_to(self):\n    \"\"\"\n    Altering order_with_respect_to adds and removes fields in the correct order.\n    \"\"\"\n    changes = self.get_changes(\n        [self.book, self.author_with_book_order_wrt], [self.book, self.author_with_book]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=None)\n\n    changes = self.get_changes(\n        [self.book, self.author_with_book], [self.book, self.author_with_book_order_wrt]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to='book')\n", "def test_remove_unique_constraint(self):\n    changes = self.get_changes([self.author_with_options], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={})\n"], "sample_149": ["def test_is_anonymous_authenticated_attributes(self):\n    \"\"\"\n    <User Model>.is_anonymous/is_authenticated should be attributes or properties.\n    \"\"\"\n    class GoodUser(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        @property\n            return False\n\n        @property\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n\n    class GoodUser2(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        is_anonymous = False\n        is_authenticated = True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_required_fields_is_tuple(self):\n    \"\"\"REQUIRED_FIELDS can be a tuple.\"\"\"\n    class CustomUserTupleRequiredFields(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        date_of_birth = models.DateField()\n\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = ('date_of_birth',)\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_check_models_permissions_with_app_configs(self):\n    class Checked(models.Model):\n        pass\n\n    app_config = self.apps.get_app_config('auth_tests')\n    errors = check_models_permissions(app_configs=[app_config])\n    self.assertEqual(errors, [])\n\n    # Ensure the model is registered with the app.\n    self.assertIn(Checked, app_config.get_models())\n\n    # Test with an unregistered model.\n    class Unchecked(models.Model):\n        pass\n\n    errors = check_models_permissions(app_configs=[app_config])\n    self.assertEqual(errors, [])\n", "def test_check_models_permissions_app_configs_none(self):\n    \"\"\"Test check_models_permissions when app_configs is None.\"\"\"\n    Permission = self.apps.get_model('auth', 'Permission')\n    permission_name_max_length = Permission._meta.get_field('name').max_length\n    permission_codename_max_length = Permission._meta.get_field('codename').max_length\n\n    class Checked(models.Model):\n        class Meta:\n            verbose_name = 'some ridiculously long verbose name that is out of control' * 5\n\n    errors = check_models_permissions(app_configs=None)\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The verbose_name of model '%s.Checked' must be at most %d \"\n            \"characters for its builtin permission names to be at most %d characters.\" % (\n                self.__module__.split('.')[0],\n                permission_name_max_length - (len(Checked._meta.verbose_name_raw) - len(Checked._meta.object_name)),\n                permission_name_max_length,\n            ),\n            obj=Checked,\n            id='auth.E007',\n        ),\n    ])\n", "def test_check_models_permissions_empty_app_configs(self):\n    \"\"\"Test check_models_permissions with empty app_configs.\"\"\"\n    errors = check_models_permissions(app_configs=[])\n    self.assertEqual(errors, [])\n"], "sample_152": ["def test_delete_with_using(self):\n    child = RChild.objects.create()\n    parent_id = child.r_ptr_id\n    child.delete(using='default')\n    self.assertFalse(RChild.objects.filter(id=child.id).exists())\n    self.assertFalse(R.objects.filter(id=parent_id).exists())\n\n    # Ensure that using parameter is passed down to the collector\n    collector = Collector(using='other')\n    with mock.patch.object(RChild, 'delete') as delete_mock:\n        delete_mock.side_effect = RChild.delete\n        child.delete(using='other')\n        delete_mock.assert_called_once_with(using='other', keep_parents=False)\n", "def test_delete_with_fields(self):\n    r = R.objects.create()\n    s = S.objects.create(r=r)\n    t = T.objects.create(s=s)\n\n    # Delete a single field.\n    collector = Collector(using='default')\n    collector.add_field_update(S._meta.get_field('r'), None, [s])\n    collector.delete()\n\n    self.assertIsNone(S.objects.get(pk=s.pk).r)\n    self.assertTrue(R.objects.filter(pk=r.pk).exists())\n\n    # Delete multiple fields.\n    collector = Collector(using='default')\n    collector.add_field_update(S._meta.get_field('r'), None, [s])\n    collector.add_field_update(T._meta.get_field('s'), None, [t])\n    collector.delete()\n\n    self.assertIsNone(S.objects.get(pk=s.pk).r)\n    self.assertIsNone(T.objects.get(pk=t.pk).s)\n    self.assertTrue(R.objects.filter(pk=r.pk).exists())\n", "    def test_collector_dependency_cycle(self):\n        # Create a dependency cycle: a -> b -> c -> a.\n        a = A.objects.create()\n        b = B.objects.create(a=a)\n        c = Child.objects.create(b=b)\n        a.child = c\n        a.save()\n\n        # The collector should detect the cycle and raise an exception.\n        collector = Collector(using='default')\n        with self.assertRaises(RuntimeError) as cm:\n            collector.collect([a])\n        self.assertIn(\"Dependency cycle detected\", str(cm.exception))\n", "def test_collector_clear_restricted_objects_from_queryset(self):\n    collector = Collector(using='default')\n    model = R\n    qs = model.objects.all()\n    collector.restricted_objects[model] = {'field': [obj for obj in qs]}\n    collector.clear_restricted_objects_from_queryset(model, qs)\n    self.assertEqual(collector.restricted_objects[model], {'field': []})\n", "def test_fast_delete_with_signals(self):\n    \"\"\"\n    Test that fast deletion doesn't occur when signals are connected.\n    \"\"\"\n    u = User.objects.create()\n\n        pass\n\n    models.signals.pre_delete.connect(noop, sender=User)\n    with self.assertNumQueries(2):\n        u.delete()\n    models.signals.pre_delete.disconnect(noop, sender=User)\n\n    u = User.objects.create()\n    models.signals.post_delete.connect(noop, sender=User)\n    with self.assertNumQueries(2):\n        u.delete()\n    models.signals.post_delete.disconnect(noop, sender=User)\n"], "sample_150": ["    def test_base_command_handle_not_implemented(self):\n        cmd = BaseCommand()\n        with self.assertRaises(NotImplementedError):\n            cmd.handle()\n", "    def test_handle_default_options(self):\n        options = mock.Mock(settings='my_settings', pythonpath='/path/to/python')\n        with mock.patch.dict('os.environ', {}), mock.patch('sys.path', []):\n            handle_default_options(options)\n            self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], 'my_settings')\n            self.assertEqual(sys.path, ['/path/to/python'])\n", "    def test_get_version(self):\n        from django.core.management.base import BaseCommand\n        command = BaseCommand()\n        self.assertEqual(command.get_version(), django.get_version())\n", "    def test_command_error(self):\n        error = CommandError(\"Error message\")\n        self.assertEqual(str(error), \"Error message\")\n", "    def test_base_command_has_no_output(self):\n        command = BaseCommand()\n        self.assertIsNone(command.handle())\n"], "sample_153": ["    def test_check_swappable(self):\n        # Mock the get_model method to return None\n        with mock.patch('django.apps.Apps.get_model', return_value=None):\n            errors = Model.check()\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], checks.Error)\n            self.assertEqual(errors[0].id, 'models.E002')\n", "    def test_model_state_fields_cache_descriptor(self):\n        instance = ModelState()\n        self.assertEqual(instance.fields_cache, {})\n        instance.fields_cache['test'] = True\n        self.assertEqual(instance.fields_cache, {'test': True})\n", "    def test_model_unpickle(self):\n        model = Model()\n        unpickled_model = model_unpickle((model._meta.app_label, model._meta.object_name))\n        self.assertIsInstance(unpickled_model, Model)\n", "    def test_fields_cache_descriptor(self):\n        instance = ModelState()\n        self.assertIsNone(instance.fields_cache)\n        fields_cache = instance.fields_cache\n        self.assertEqual(fields_cache, {})\n        self.assertIs(instance.fields_cache, fields_cache)\n", "    def test_adding(self):\n        obj = ModelState()\n        self.assertTrue(obj.adding)\n        obj.adding = False\n        self.assertFalse(obj.adding)\n"], "sample_154": ["def test_database_checks_multiple_databases(self, mocked_connections):\n    mocked_conn1 = mock.MagicMock()\n    mocked_conn2 = mock.MagicMock()\n    mocked_connections.__getitem__.side_effect = [mocked_conn1, mocked_conn2]\n    check_database_backends(databases=self.databases)\n    self.assertTrue(mocked_conn1.validation.check.called)\n    self.assertTrue(mocked_conn2.validation.check.called)\n", "def test_database_checks_iterate_over_connections(self, mocked_connections):\n    check_database_backends(databases=self.databases)\n    self.assertTrue(mocked_connections.__getitem__.called)\n    self.assertEqual(mocked_connections.__getitem__.call_count, len(self.databases))\n", "def test_database_checks_issues(self, mocked_connections):\n    issues = ['issue1', 'issue2']\n    mocked_connections.__getitem__.return_value.validation.check.return_value = issues\n    result = check_database_backends(databases=self.databases)\n    self.assertEqual(result, issues * len(self.databases))\n", "def test_database_checks_are_collected_from_all_databases(self, mocked_connections):\n    issues = ['issue1', 'issue2']\n    mocked_connections.__getitem__.side_effect = [\n        mock.Mock(validation=mock.Mock(check=lambda **kwargs: issues)),\n        mock.Mock(validation=mock.Mock(check=lambda **kwargs: issues)),\n    ]\n    result = check_database_backends(databases=self.databases)\n    self.assertEqual(result, issues + issues)\n", "def test_database_checks_called_for_each_database(self, mocked_connections):\n    databases = {'default', 'other'}\n    check_database_backends(databases=databases)\n    self.assertEqual(mocked_connections.__getitem__.call_count, len(databases))\n    for alias in databases:\n        mocked_connections.__getitem__.assert_any_call(alias)\n"], "sample_155": ["def test_file_response_filename_with_non_ascii_characters(self):\n    filename = 'test_\u00e9\u00e0\u00fc.txt'\n    response = FileResponse(io.BytesIO(b'binary content'), filename=filename)\n    expected_filename = \"filename*=utf-8''{}\".format(quote(filename))\n    self.assertEqual(response['Content-Disposition'], 'inline; {}'.format(expected_filename))\n", "def test_file_response_with_filename(self):\n    response = FileResponse(io.BytesIO(b'binary content'), filename='example.txt')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    response.close()\n", "def test_file_from_disk_with_filename_override(self):\n    response = FileResponse(open(__file__, 'rb'), filename='custom_name.txt')\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_name.txt\"')\n    response.close()\n", "def test_file_from_disk_with_non_ascii_filename(self):\n    filename = 'test_fileresponse_\u00fcnicode.py'\n    with tempfile.NamedTemporaryFile(suffix=filename) as tmp:\n        response = FileResponse(tmp)\n        self.assertEqual(response['Content-Disposition'], f\"inline; filename*=utf-8''{filename}\")\n", "def test_file_from_disk_with_filename_response(self):\n    response = FileResponse(open(__file__, 'rb'), filename='custom_filename.py')\n    self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n    self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"custom_filename.py\"')\n    response.close()\n"], "sample_156": ["def test_split_datetime_field(self):\n    class SplitDateTimeForm(Form):\n        when = SplitDateTimeField()\n\n    f = SplitDateTimeForm(auto_id=False)\n    self.assertHTMLEqual(\n        str(f['when']),\n        '<input type=\"text\" name=\"when_0\" required>'\n        '<input type=\"text\" name=\"when_1\" required>'\n    )\n    data = {'when_0': '1992-01-01', 'when_1': '01:01'}\n    f = SplitDateTimeForm(data)\n    self.assertTrue(f.is_valid())\n    self.assertEqual(f.cleaned_data['when'], datetime.datetime(1992, 1, 1, 1, 1))\n    data = {'when_0': '1992-01-01', 'when_1': ''}\n    f = SplitDateTimeForm(data)\n    self.assertFalse(f.is_valid())\n    self.assertEqual(f.errors['when'], ['This field is required.'])\n    data = {'when_0': '', 'when_1': '01:01'}\n    f = SplitDateTimeForm(data)\n    self.assertFalse(f.is_valid())\n    self.assertEqual(f.errors['when'], ['This field is required.'])\n", "def test_field_order_with_key_error(self):\n    class TestFormParent(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field4 = CharField()\n        field5 = CharField()\n        field6 = CharField()\n\n    class TestForm(TestFormParent):\n        field3 = CharField()\n        field_order = ['field2', 'field4', 'field3', 'field5', 'field6']\n\n    class TestFormRemove(TestForm):\n        field1 = None\n\n    class TestFormMissing(TestForm):\n        field_order = ['field2', 'field4', 'field3', 'field5', 'field6', 'field1']\n        field1 = None\n\n    class TestFormInit(TestFormParent):\n        field3 = CharField()\n        field_order = None\n\n            super().__init__(**kwargs)\n            self.order_fields(field_order=TestForm.field_order)\n\n    with self.assertRaises(KeyError):\n        TestForm().order_fields(['field7', 'field2', 'field4'])\n", "def test_error_messages_passed_to_widget(self):\n    class MyForm(Form):\n        my_field = CharField(\n            widget=TextInput(attrs={'placeholder': 'Placeholder'}),\n            error_messages={'required': 'My custom required message.'}\n        )\n\n    form = MyForm(data={})\n    self.assertFalse(form.is_valid())\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<ul class=\"errorlist\"><li>My custom required message.</li></ul>'\n        '<p><label for=\"id_my_field\">My field:</label> '\n        '<input type=\"text\" name=\"my_field\" placeholder=\"Placeholder\" id=\"id_my_field\" required></p>'\n    )\n", "def test_add_prefix(self):\n    # A form prefix can be used to prefix field names, for example if the same\n    # form is used multiple times in a view.\n    class Person(Form):\n        first_name = CharField()\n        last_name = CharField()\n\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n\n    # If no prefix is specified, the field name will not be changed.\n    p = Person()\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n\n    # A form can also define a custom method to add prefixes.\n    class CustomForm(Form):\n            return field_name.upper()\n\n    f = CustomForm()\n    self.assertEqual(f.add_prefix('first_name'), 'FIRST_NAME')\n\n    # Fields that are created automatically should get the prefix of their parent.\n    class MyForm(Form):\n        parent = CharField()\n        prefix = 'parent'\n\n        class Media:\n            js = ('parent.js',)\n\n    child_form = type(str('ChildForm'), (Form,), {'child': CharField()})\n    child_form.prefix = 'child'\n    parent_form = MyForm()\n    parent_form.fields['child'] = child_form\n\n    # The parent form's media should include the child form's media with the correct prefix.\n    parent_media = parent_form.media\n    self.assertIn('child-child.js', parent_media._js)\n", "def test_renderer_override(self):\n    class CustomForm(Form):\n        default_renderer = CustomRenderer\n\n    custom = DjangoTemplates()\n    form = CustomForm(renderer=custom)\n    self.assertEqual(form.renderer, custom)\n"], "sample_157": ["def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n    test_connection = get_connection_copy()\n    creation = test_connection.creation_class(test_connection)\n    old_database_name = test_connection.settings_dict['NAME']\n    try:\n        with mock.patch.object(creation, '_create_test_db'):\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n        self.assertEqual(test_connection.settings_dict['NAME'], creation._get_test_db_name())\n    finally:\n        with mock.patch.object(creation, '_destroy_test_db'):\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n", "    def test_get_test_db_clone_settings(self, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'original_name'\n        creation = test_connection.creation_class(test_connection)\n        clone_settings = creation.get_test_db_clone_settings(suffix='clone_1')\n        self.assertEqual(clone_settings['NAME'], 'original_name_clone_1')\n", "    def test_serialize_db_to_string(self, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(serializers, 'serialize') as mocked_serialize:\n            creation.serialize_db_to_string()\n            mocked_serialize.assert_called_once_with('json', mock.ANY, indent=None, stream=mock.ANY)\n", "    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_clone_test_db') as mocked_clone:\n            creation.clone_test_db(suffix='test_suffix', verbosity=0)\n            mocked_clone.assert_called_once_with('test_suffix', 0, keepdb=False)\n", "    def test_clone_test_db(self, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_clone_test_db'):\n            creation.clone_test_db(suffix='clone', verbosity=0)\n        self.assertEqual(mocked_ensure_connection.call_count, 1)\n"], "sample_158": ["    def test_foreign_key_to_non_unique_field(self):\n        class Target(models.Model):\n            bad = models.IntegerField(unique=False)\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, models.CASCADE, to_field='bad')\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [\n            Error(\n                \"'Target.bad' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a UniqueConstraint '\n                    '(without condition) in the model Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n", "    def test_swappable_model_with_explicit_fk(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            explicit_fk = models.ForeignKey(\n                SwappableModel,\n                models.CASCADE,\n                related_name='explicit_fk',\n            )\n\n        field = Model._meta.get_field('explicit_fk')\n        self.assertEqual(field.check(), [])\n", "    def test_foreign_key_to_swapped_model(self):\n        class Replacement(models.Model):\n            pass\n\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            explicit_fk = models.ForeignKey(\n                SwappedModel,\n                models.CASCADE,\n                related_name='explicit_fk',\n            )\n            implicit_fk = models.ForeignKey(\n                'invalid_models_tests.SwappedModel',\n                models.CASCADE,\n                related_name='implicit_fk',\n            )\n\n        fields = [\n            Model._meta.get_field('explicit_fk'),\n            Model._meta.get_field('implicit_fk'),\n        ]\n\n        expected_error = Error(\n            (\"Field defines a relation with the model \"\n             \"'invalid_models_tests.SwappedModel', which has been swapped out.\"),\n            hint=\"Update the relation to point at 'settings.TEST_SWAPPABLE_MODEL'.\",\n            id='fields.E301',\n        )\n\n        for field in fields:\n            expected_error.obj = field\n            self.assertEqual(field.check(from_model=Model), [expected_error])\n", "    def test_swappable_model(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            explicit_fk = models.ForeignKey(\n                SwappableModel,\n                models.CASCADE,\n                related_name='explicit_fk',\n            )\n            implicit_fk = models.ForeignKey(\n                'invalid_models_tests.SwappableModel',\n                models.CASCADE,\n                related_name='implicit_fk',\n            )\n            explicit_m2m = models.ManyToManyField(SwappableModel, related_name='explicit_m2m')\n            implicit_m2m = models.ManyToManyField(\n                'invalid_models_tests.SwappableModel',\n                related_name='implicit_m2m',\n            )\n\n        fields = [\n            Model._meta.get_field('explicit_fk'),\n            Model._meta.get_field('implicit_fk'),\n            Model._meta.get_field('explicit_m2m'),\n            Model._meta.get_field('implicit_m2m'),\n        ]\n\n        expected_warning = DjangoWarning(\n            \"Model 'invalid_models_tests.Model' has a ForeignKey or ManyToManyField \"\n            \"to swappable model 'invalid_models_tests.SwappableModel'. You should \"\n            \"probably set the related_name or related_query_name argument \"\n            \"manually.\",\n            hint=(\n                \"This is because, when a model is swapped out, the field doesn't \"\n                \"change its related_name or related_query_name, which could \"\n                \"result in name clashes.\"\n            ),\n            obj=None,\n            id='fields.W342',\n        )\n\n        for field in fields:\n            with self.subTest(field=field.name):\n                errors = field.check(from_model=Model)\n                self.assertEqual(errors, [expected_warning])\n", "    def test_foreign_key_to_non_unique_field(self):\n        class Target(models.Model):\n            non_unique = models.IntegerField()\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, models.CASCADE, to_field='non_unique')\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [\n            Error(\n                \"'Target.non_unique' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a '\n                    'UniqueConstraint (without condition) in the model '\n                    'Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n"], "sample_159": ["def test_permission_name_max_length(self):\n    class Checked(models.Model):\n        class Meta:\n            verbose_name = 'some long verbose name that is close to the limit' * 2\n\n    Permission = self.apps.get_model('auth', 'Permission')\n    permission_name_max_length = Permission._meta.get_field('name').max_length\n    max_builtin_permission_name_length = (\n        len('Can change some long verbose name that is close to the limit' * 2)\n    )\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The verbose_name of model 'auth_tests.Checked' must be at most {} \"\n            \"characters for its builtin permission names to be at most {} characters.\".format(\n                permission_name_max_length - (max_builtin_permission_name_length - len(Checked._meta.verbose_name_raw)),\n                permission_name_max_length,\n            ),\n            obj=Checked,\n            id='auth.E007',\n        ),\n    ])\n", "def test_permission_name_length_at_max_limit(self):\n    custom_permission_name = 'x' * 255\n\n    class Checked(models.Model):\n        class Meta:\n            permissions = [\n                ('my_custom_permission', custom_permission_name),\n            ]\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_check_models_permissions_empty_app_configs(self):\n    \"\"\"Test check_models_permissions with empty app_configs.\"\"\"\n    errors = check_models_permissions(app_configs=[])\n    self.assertEqual(errors, [])\n", "def test_check_models_permissions_empty_app_configs(self):\n    errors = check_models_permissions(app_configs=[])\n    self.assertEqual(errors, [])\n", "def test_no_required_fields(self):\n    \"\"\"REQUIRED_FIELDS can be empty.\"\"\"\n    class CustomUserNoRequiredFields(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = []\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n"], "sample_160": ["def test_non_uniform_digit_grouping(self):\n    self.assertEqual(\n        nformat(123456789, '.', grouping=(3, 2), thousand_sep=',', force_grouping=True),\n        '123,45,6789'\n    )\n    self.assertEqual(\n        nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=',', force_grouping=True),\n        '123,45,6789'\n    )\n    self.assertEqual(\n        nformat(12345678901234, '.', grouping=(3, 2, 0), thousand_sep=',', force_grouping=True),\n        '12,3456,78,901234'\n    )\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,78,9')\n    self.assertEqual(nformat(123456789012, '.', grouping=(4, 3), thousand_sep=','), '12,3456,789,012')\n    self.assertEqual(nformat(123456789012, '.', grouping=(4, 3), thousand_sep=',', force_grouping=True), '12,3456,789,012')\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2), thousand_sep=','), '123,45,6789')\n    self.assertEqual(nformat(123456789, '.', grouping=(2, 3), thousand_sep=','), '12,345,6789')\n", "def test_non_uniform_digit_grouping(self):\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '1,23,45,67')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '12,34,56,789')\n    self.assertEqual(nformat(1234, '.', grouping=(3, 2, 0), thousand_sep=','), '1,234')\n    self.assertEqual(nformat(123456789012, '.', grouping=(3, 2, 0), thousand_sep=','), '123,45,67,89012')\n", "def test_non_uniform_digit_grouping(self):\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 2, 0), thousand_sep=','), '123,45,6789')\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 2, 0), thousand_sep=','), '123,45,67')\n    self.assertEqual(nformat(12345, '.', grouping=(3, 2, 0), thousand_sep=','), '12,345')\n    self.assertEqual(nformat(123456789, '.', grouping=(3, 0), thousand_sep=','), '123,456,789')\n    self.assertEqual(nformat(1234567, '.', grouping=(3, 0), thousand_sep=','), '1,234,567')\n"], "sample_161": ["    def test_swappable_model_foreign_key(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            fk = models.ForeignKey(SwappableModel, models.CASCADE)\n\n        self.assertEqual(Model.check(), [])\n", "    def test_m2m_table_uniqueness(self):\n        class Model1(models.Model):\n            m2m = models.ManyToManyField('Target')\n\n        class Model2(models.Model):\n            m2m = models.ManyToManyField('Target')\n\n        class Target(models.Model):\n            pass\n\n        field = Model2._meta.get_field('m2m')\n        self.assertEqual(field.check(from_model=Model2), [\n            Error(\n                \"The field's intermediary table 'myapp_model2_target' clashes with the table name of 'myapp.Model1.m2m'.\",\n                obj=field,\n                id='fields.E340',\n            ),\n        ])\n", "    def test_foreign_key_to_non_unique_field_under_explicit_model(self):\n        class Target(models.Model):\n            bad = models.IntegerField()\n\n        class Model(models.Model):\n            field = models.ForeignKey(Target, models.CASCADE, to_field='bad')\n\n        field = Model._meta.get_field('field')\n        self.assertEqual(field.check(), [\n            Error(\n                \"'Target.bad' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a UniqueConstraint '\n                    '(without condition) in the model Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n", "    def test_on_delete_do_nothing(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey('Target', models.DO_NOTHING)\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [])\n", "    def test_foreign_key_to_non_unique_field_with_unique_together(self):\n        class Target(models.Model):\n            source = models.IntegerField()\n            extra = models.IntegerField()\n\n            class Meta:\n                unique_together = (('source', 'extra'),)\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Target, models.CASCADE, to_field='source')\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [\n            Error(\n                \"'Target.source' must be unique because it is referenced by a foreign key.\",\n                hint=(\n                    'Add unique=True to this field or add a UniqueConstraint '\n                    '(without condition) in the model Meta.constraints.'\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n"], "sample_162": ["    def test_no_obsolete(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"Obsolete message\"', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n", "    def test_default_extensions(self):\n        \"\"\"\n        makemessages uses .html, .txt and .py extensions by default for domain 'django'.\n        \"\"\"\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('This literal should be included.', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        \"\"\"Behavior is correct if --no-obsolete switch is specified. See #20966.\"\"\"\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n"], "sample_163": ["    def test_password_change_form_with_request(self):\n        user = User.objects.get(username=\"testclient\")\n        request = HttpRequest()\n        request.user = user\n        form = PasswordChangeForm(user, data={\"old_password\": \"password\", \"new_password1\": \"password1\", \"new_password2\": \"password1\"})\n        self.assertTrue(form.is_valid())\n        form.save(request)\n        self.assertNotEqual(user.password, \"password\")\n", "    def test_get_success_url_next_param(self):\n        url = \"/login/next_page/?next=/test/\"\n        response = self.client.post(\n            url,\n            {\n                \"username\": \"testclient\",\n                \"password\": \"password\",\n            },\n        )\n        self.assertRedirects(response, \"/test/\", fetch_redirect_response=False)\n", "    def test_logout_then_login_next_page(self):\n        self.login()\n        req = HttpRequest()\n        req.method = \"POST\"\n        csrf_token = get_token(req)\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = csrf_token\n        req.POST = {\"csrfmiddlewaretoken\": csrf_token}\n        req.session = self.client.session\n        response = logout_then_login(req, login_url=\"/custom/?next=/somewhere/\")\n        self.assertRedirects(\n            response, \"/custom/?next=/somewhere/\", fetch_redirect_response=False\n        )\n", "    def test_login_with_next_page(self):\n        next_url = \"/next-page/\"\n        response = self.client.post(\n            \"/login/\",\n            {\n                \"username\": \"testclient\",\n                \"password\": \"password\",\n                \"next\": next_url,\n            },\n        )\n        self.assertRedirects(response, next_url, fetch_redirect_response=False)\n", "    def test_password_reset_confirm_view_post_reset_login(self):\n        url, path = self._test_confirm_start()\n        path = path.replace(\"/reset/\", \"/reset/post_reset_login/\")\n        response = self.client.post(\n            path, {\"new_password1\": \"anewpassword\", \"new_password2\": \"anewpassword\"}\n        )\n        self.assertRedirects(response, \"/reset/done/\", fetch_redirect_response=False)\n        self.assertIn(SESSION_KEY, self.client.session)\n"], "sample_164": ["    def test_uses_server_time(self):\n        formatter = ServerFormatter()\n        self.assertTrue(formatter.uses_server_time())\n        formatter = ServerFormatter('{message}')\n        self.assertFalse(formatter.uses_server_time())\n", "    def test_server_formatter_uses_server_time(self):\n        formatter = ServerFormatter()\n        record = logging.makeLogRecord({'msg': 'log message', 'server_time': '2023-02-20 14:30:00'})\n        self.assertEqual(formatter.format(record), '[2023-02-20 14:30:00] log message')\n", "def test_admin_email_handler_uses_logger_name_in_subject(self):\n    \"\"\"\n    The AdminEmailHandler should include the logger name in the email subject.\n    \"\"\"\n    message = \"Custom message that says '%s' and '%s'\"\n    token1 = 'ping'\n    token2 = 'pong'\n\n    admin_email_handler = self.get_admin_email_handler(self.logger)\n    # Backup then override original filters\n    orig_filters = admin_email_handler.filters\n    try:\n        admin_email_handler.filters = []\n\n        logger_name = 'my_logger'\n        logger = logging.getLogger(logger_name)\n        logger.error(message, token1, token2)\n\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, ['admin@example.com'])\n        self.assertIn(logger_name, mail.outbox[0].subject)\n    finally:\n        # Restore original filters\n        admin_email_handler.filters = orig_filters\n", "    def test_log_response(self):\n        logger = logging.getLogger('django')\n        request = RequestFactory().get('/')\n        response = views.HttpResponse('Hello, world!')\n\n        log_response(\n            'Test message',\n            response=response,\n            request=request,\n            logger=logger,\n            level='INFO',\n        )\n\n        self.assertEqual(len(logger.handlers[0].buffer), 1)\n        record = logger.handlers[0].buffer[0]\n        self.assertEqual(record.getMessage(), 'Test message')\n        self.assertEqual(record.status_code, 200)\n        self.assertEqual(record.request, request)\n", "    def test_log_response(self):\n        logger = logging.getLogger('django')\n        request = RequestFactory().get('/')\n        response = views.HttpResponse('OK')\n        log_response('message', response=response, request=request, logger=logger)\n        self.assertEqual(len(mail.outbox), 0)\n"], "sample_165": ["    def test_model_choice_iterator(self):\n        choices = [\n            ChoiceModel.objects.create(name='choice1'),\n            ChoiceModel.objects.create(name='choice2'),\n            ChoiceModel.objects.create(name='choice3'),\n        ]\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all())\n        iterator = f.iterator(f)\n        self.assertEqual(len(list(iterator)), len(choices) + 1)  # +1 for the empty choice\n", "    def test_modelform_factory(self):\n        # Create a model form factory for ChoiceModel.\n        ChoiceModelForm = modelform_factory(ChoiceModel, fields=('name',))\n\n        # Test that the form contains the correct fields.\n        self.assertEqual(list(ChoiceModelForm().fields.keys()), ['name'])\n\n        # Test that the form can be instantiated and validated.\n        form = ChoiceModelForm({'name': 'Test Choice'})\n        self.assertTrue(form.is_valid())\n\n        # Test that the form can be used to create a new ChoiceModel instance.\n        form.save()\n        self.assertEqual(ChoiceModel.objects.count(), 1)\n\n        # Test that the form can be used to update an existing ChoiceModel instance.\n        choice = ChoiceModel.objects.get(name='Test Choice')\n        form = ChoiceModelForm({'name': 'Updated Choice'}, instance=choice)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(ChoiceModel.objects.get(pk=choice.pk).name, 'Updated Choice')\n", "    def test_label_suffix(self):\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n\n        form = TestModelForm()\n        self.assertEqual(form.fields['name'].label_suffix, ':')\n\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n                label_suffix = '?'\n\n        form = TestModelForm()\n        self.assertEqual(form.fields['name'].label_suffix, '?')\n", "    def test_modelform_with_custom_formfield_callback(self):\n        # Define a form with a custom formfield_callback.\n        class CustomModelForm(ModelForm):\n                super().__init__(*args, **kwargs)\n\n                if db_field.name == 'name':\n                    return formfield(max_length=10)\n                return formfield\n\n        form_class = modelform_factory(ChoiceModel, form=CustomModelForm)\n        f = form_class()\n        self.assertEqual(f.fields['name'].max_length, 10)\n", "    def test_unique_validation(self):\n        # Create a model with a unique field.\n        from .models import UniqueModel\n        UniqueModel.objects.create(unique='abc')\n\n        # Create a form for the model and validate it.\n        from .forms import UniqueModelForm\n        form = UniqueModelForm({'unique': 'abc'})\n\n        # The form should not be valid due to the unique constraint.\n        self.assertFalse(form.is_valid())\n\n        # Check that the error message is correct.\n        self.assertEqual(form.errors['unique'], ['UniqueModel with this Unique already exists.'])\n"], "sample_166": ["    def test_get_random_string_length(self):\n        for length in range(1, 100):\n            self.assertEqual(len(get_random_string(length)), length)\n", "def test_get_random_string_length(self):\n    for length in range(1, 100):\n        self.assertEqual(len(get_random_string(length)), length)\n", "    def test_get_random_string_length(self):\n        for length in range(1, 100):\n            self.assertEqual(len(get_random_string(length)), length)\n", "    def test_get_random_string_length(self):\n        for length in range(1, 100):\n            self.assertEqual(len(get_random_string(length)), length)\n", "    def test_get_random_string_length(self):\n        for length in range(1, 100):\n            self.assertEqual(len(get_random_string(length)), length)\n"], "sample_167": ["def test_naturaltime_future_substrings(self):\n    \"\"\"\n    Test that naturaltime returns the correct future substrings.\n    \"\"\"\n    test_list = [\n        now + datetime.timedelta(days=1),\n        now + datetime.timedelta(days=7),\n        now + datetime.timedelta(days=30),\n        now + datetime.timedelta(days=365),\n    ]\n    result_list = [\n        '1\\xa0day from now',\n        '1\\xa0week from now',\n        '1\\xa0month from now',\n        '1\\xa0year from now',\n    ]\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_naturaltime_tzinfo(self):\n    # Test naturaltime with datetime objects that have tzinfo.\n    now = datetime.datetime.now(utc)\n    test_list = [\n        now,\n        now - datetime.timedelta(microseconds=1),\n        now - datetime.timedelta(seconds=1),\n        now - datetime.timedelta(seconds=30),\n        now - datetime.timedelta(minutes=1, seconds=30),\n        now - datetime.timedelta(minutes=2),\n        now - datetime.timedelta(hours=1, minutes=30, seconds=30),\n        now - datetime.timedelta(hours=23, minutes=50, seconds=50),\n        now - datetime.timedelta(days=1),\n        now + datetime.timedelta(seconds=1),\n        now + datetime.timedelta(seconds=30),\n        now + datetime.timedelta(minutes=1, seconds=30),\n        now + datetime.timedelta(minutes=2),\n        now + datetime.timedelta(hours=1, minutes=30, seconds=30),\n        now + datetime.timedelta(hours=23, minutes=50, seconds=50),\n        now + datetime.timedelta(days=1),\n    ]\n    result_list = [\n        'now',\n        'now',\n        'a second ago',\n        '30\\xa0seconds ago',\n        'a minute ago',\n        '2\\xa0minutes ago',\n        'an hour ago',\n        '23\\xa0hours ago',\n        '1\\xa0day ago',\n        'a second from now',\n        '30\\xa0seconds from now',\n        'a minute from now',\n        '2\\xa0minutes from now',\n        'an hour from now',\n        '23\\xa0hours from now',\n        '1\\xa0day from now',\n    ]\n\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "def test_naturaltime_with_microseconds(self):\n    # Regression for #28184\n    dt = now - datetime.timedelta(microseconds=1)\n    with translation.override('en'):\n        self.humanize_tester([dt], ['now'], 'naturaltime')\n", "def test_naturaltime_future_substrings(self):\n    # Test future time substrings for different languages\n    with translation.override('en'):\n        self.assertEqual(humanize.naturaltime(now + datetime.timedelta(days=1)), '1\\xa0day from now')\n        self.assertEqual(humanize.naturaltime(now + datetime.timedelta(weeks=1)), '1\\xa0week from now')\n        self.assertEqual(humanize.naturaltime(now + datetime.timedelta(months=1)), '1\\xa0month from now')\n        self.assertEqual(humanize.naturaltime(now + datetime.timedelta(years=1)), '1\\xa0year from now')\n\n    with translation.override('fr'):\n        self.assertEqual(humanize.naturaltime(now + datetime.timedelta(days=1)), 'dans 1\\xa0jour')\n        self.assertEqual(humanize.naturaltime(now + datetime.timedelta(weeks=1)), 'dans 1\\xa0semaine')\n        self.assertEqual(humanize.naturaltime(now + datetime.timedelta(months=1)), 'dans 1\\xa0mois')\n        self.assertEqual(humanize.naturaltime(now + datetime.timedelta(years=1)), 'dans 1\\xa0an')\n", "def test_naturaltime_overflow(self):\n    class overflow(datetime.tzinfo):\n            return datetime.timedelta(days=1)\n    test_list = [\n        now,\n        now + datetime.timedelta(seconds=2**32),\n        now + datetime.timedelta(seconds=-2**32),\n        now.replace(tzinfo=overflow()),\n    ]\n    result_list = [\n        'now',\n        'a second from now',\n        'a second ago',\n        'now',\n    ]\n    with translation.override('en'):\n        self.humanize_tester(test_list, result_list, 'naturaltime')\n"], "sample_168": ["def test_include_stale_apps(self):\n    \"\"\"\n    The --include-stale-apps option deletes content types for apps that have been\n    removed from INSTALLED_APPS.\n    \"\"\"\n    ContentType.objects.create(app_label='empty_models', model='Fake 1')\n    ContentType.objects.create(app_label='no_models', model='Fake 2')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n    self.assertIn(\n        \"Deleting stale content type 'empty_models | Fake 1'\",\n        stdout.getvalue(),\n    )\n    self.assertIn(\n        \"Deleting stale content type 'no_models | Fake 2'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_include_stale_apps(self):\n    \"\"\"\n    The --include-stale-apps option deletes stale content types from apps that\n    have been removed from INSTALLED_APPS.\n    \"\"\"\n    ContentType.objects.create(app_label='empty_models', model='Fake 1')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', verbosity=2, include_stale_apps=True)\n    self.assertIn(\n        \"Deleting stale content type 'empty_models | Fake 1'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "def test_include_stale_apps(self):\n    \"\"\"\n    The --include-stale-apps option allows deleting stale content types from\n    previously installed apps that have been removed from INSTALLED_APPS.\n    \"\"\"\n    ContentType.objects.create(app_label='removed_app', model='Fake')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n    self.assertIn(\n        \"Deleting stale content type 'removed_app | Fake'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_include_stale_apps(self):\n    \"\"\"\n    The --include-stale-apps option deletes stale content types from apps that\n    have been removed from INSTALLED_APPS.\n    \"\"\"\n    ContentType.objects.create(app_label='removed_app', model='Fake')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n    self.assertIn(\n        \"Deleting stale content type 'removed_app | Fake'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "def test_include_stale_apps(self):\n    \"\"\"--include-stale-apps option includes content types from previously installed apps.\"\"\"\n    ContentType.objects.create(app_label='empty_models', model='Fake 1')\n    with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', verbosity=2, include_stale_apps=True)\n    self.assertIn(\n        \"Deleting stale content type 'empty_models | Fake 1'\",\n        stdout.getvalue(),\n    )\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n"], "sample_169": ["    def test_serialize_deferred_fields(self):\n        # Create an object with deferred fields.\n        obj = JSONModel.objects.create(value={'key': 'value'})\n        obj.refresh_from_db()\n        obj.value = models.DeferredAttribute('value', JSONModel)\n\n        # Serialize the object.\n        data = serializers.serialize('xml', [obj])\n\n        # Check that the deferred field is serialized correctly.\n        self.assertXMLEqual(\n            data,\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.jsonmodel\" pk=\"{}\">'\n            '<field name=\"value\" type=\"JSONField\">{}'\n            '</field></object></django-objects>'.format(obj.pk, '{}')\n        )\n", "    def test_deconstruct(self):\n        field = models.JSONField()\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(path, 'django.db.models.JSONField')\n        self.assertEqual(args, [])\n        self.assertEqual(kwargs, {})\n", "    def test_serialize_xml(self):\n        obj = JSONModel.objects.create(value={'foo': 'bar', 'baz': [1, 2, 3]})\n        data = serializers.serialize('xml', [obj], fields=['value'])\n        expected = (\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.jsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">{&quot;foo&quot;: &quot;bar&quot;, &quot;baz&quot;: [1, 2, 3]}</field>'\n            '</object></django-objects>'\n        )\n        self.assertXMLEqual(data, expected)\n", "    def test_serialize_xml(self):\n        instance = JSONModel(value={'a': 'b', 'c': 14})\n        data = serializers.serialize('xml', [instance])\n        self.assertXMLEqual(\n            data,\n            '<django-objects version=\"1.0\">'\n            '<object model=\"model_fields.jsonmodel\">'\n            '<field name=\"value\" type=\"JSONField\">{&quot;a&quot;: &quot;b&quot;, &quot;c&quot;: 14}</field>'\n            '</object>'\n            '</django-objects>'\n        )\n", "def test_xml_serialization_with_deferred_fields(self):\n    instance = NullableJSONModel(value={'a': 1, 'b': 2})\n    data = serializers.serialize('xml', [instance], fields=['value'])\n    new_instance = list(serializers.deserialize('xml', data))[0].object\n    self.assertEqual(new_instance.value, instance.value)\n"], "sample_171": ["def test_migrate_run_syncdb_with_database_router(self):\n    \"\"\"\n    Running migrate --run-syncdb respects the database router's allow_syncdb().\n    \"\"\"\n    router = TestRouter()\n    with mock.patch('migrations.routers.TestRouter.allow_migrate', return_value=False) as allow_migrate:\n        with override_settings(DATABASE_ROUTERS=['migrations.routers.TestRouter']):\n            call_command('migrate', run_syncdb=True, verbosity=0)\n    allow_migrate.assert_called_once_with('default', 'unmigrated_app_syncdb')\n", "def test_migrate_fake_initial_interactive(self):\n    \"\"\"\n    --fake-initial with interactive=False should not prompt for user input.\n    \"\"\"\n    # Make sure no tables are created\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n    # Run the migrations to 0001 only\n    call_command(\"migrate\", \"migrations\", \"0001\", verbosity=0)\n    call_command(\"migrate\", \"migrations\", \"0001\", verbosity=0, database=\"other\")\n    # Fake a roll-back\n    call_command(\"migrate\", \"migrations\", \"zero\", fake=True, verbosity=0)\n    call_command(\"migrate\", \"migrations\", \"zero\", fake=True, verbosity=0, database=\"other\")\n    # Run initial migration with an explicit --fake-initial and interactive=False\n    out = io.StringIO()\n    with mock.patch('django.core.management.color.supports_color', lambda *args: False):\n        call_command(\"migrate\", \"migrations\", \"0001\", fake_initial=True, interactive=False, stdout=out, verbosity=1)\n        call_command(\"migrate\", \"migrations\", \"0001\", fake_initial=True, verbosity=0, database=\"other\")\n    self.assertIn(\n        \"migrations.0001_initial... faked\",\n        out.getvalue().lower()\n    )\n    # Run migrations all the way\n    call_command(\"migrate\", verbosity=0)\n    call_command(\"migrate\", verbosity=0, database=\"other\")\n    # Make sure the right tables exist\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n    self.assertTableExists(\"migrations_book\")\n    self.assertTableNotExists(\"migrations_author\", using=\"other\")\n    self.assertTableNotExists(\"migrations_tribble\", using=\"other\")\n    self.assertTableNotExists(\"migrations_book\", using=\"other\")\n    # Fake a roll-back\n    call_command(\"migrate\", \"migrations\", \"zero\", fake=True, verbosity=0)\n    call_command(\"migrate\", \"migrations\", \"zero\", fake=True, verbosity=0, database=\"other\")\n    # Make sure the tables still exist\n    self.assertTableExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n    self.assertTableExists(\"migrations_book\")\n", "def test_migrate_fake_initial_interactive(self):\n    \"\"\"\n    --fake-initial with interactive=True asks before faking initial migrations.\n    \"\"\"\n    # Make sure no tables are created\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n\n    # Run the migrations to 0001 only\n    call_command(\"migrate\", \"migrations\", \"0001\", verbosity=0)\n    call_command(\"migrate\", \"migrations\", \"zero\", fake=True, verbosity=0)\n\n    # Fake a roll-back\n    out = io.StringIO()\n    with mock.patch('builtins.input', return_value='y'):\n        call_command(\"migrate\", \"migrations\", \"0001\", fake_initial=True, interactive=True, stdout=out, verbosity=1)\n\n    self.assertIn(\n        \"migrations.0001_initial... faked\",\n        out.getvalue().lower()\n    )\n", "def test_migrate_syncdb_deferred_sql_executed_with_schemaeditor_for_unmigrated_apps(self):\n    \"\"\"\n    For unmigrated apps, editor.execute() is used for executing the syncdb\n    deferred SQL when running migrate --run-syncdb.\n    \"\"\"\n    stdout = io.StringIO()\n    with mock.patch.object(BaseDatabaseSchemaEditor, 'execute') as execute:\n        call_command('migrate', run_syncdb=True, verbosity=1, stdout=stdout, no_color=True)\n        create_table_count = len([call for call in execute.mock_calls if 'CREATE TABLE' in str(call)])\n        self.assertEqual(create_table_count, 2)\n        # There's at least one deferred SQL for creating the foreign key index.\n        self.assertGreater(len(execute.mock_calls), 2)\n    stdout = stdout.getvalue()\n    self.assertIn('Synchronize unmigrated apps:', stdout)\n    self.assertIn('Creating tables...', stdout)\n    table_name = truncate_name('unmigrated_app_syncdb_classroom', connection.ops.max_name_length())\n    self.assertIn('Creating table %s' % table_name, stdout)\n", "compilation error"], "sample_170": ["    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIn('DATABASES', settings)\n        self.assertNotIn('DATABASE_URL', settings['DATABASES'])\n", "    def test_get_traceback_frame_variables(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().get('/')\n        tb_frame = sys._getframe()\n        tb_frame.f_locals['foo'] = 'bar'\n        tb_frame.f_locals['password'] = 'secret'\n\n        variables = reporter_filter.get_traceback_frame_variables(request, tb_frame)\n        self.assertIn(('foo', 'bar'), variables)\n        self.assertNotIn(('password', 'secret'), variables)\n        self.assertIn(('password', reporter_filter.cleansed_substitute), variables)\n", "    def test_is_active(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertTrue(filter.is_active(request=None))\n", "    def test_technical_404_response(self):\n        exception = Http404('Page not found')\n        request = RequestFactory().get('/test_view/')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, 'technical_404.html')\n        self.assertIn(b'Page not found', response.content)\n", "    def test_404(self):\n        response = self.client.get('/raises404/')\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, \"<code>not-in-urls</code>, didn't match\", status_code=404)\n"], "sample_172": ["    def test_multiple_files(self):\n        self.client.force_login(self.superuser)\n        band = Band.objects.create(name='Linkin Park')\n        album = band.album_set.create(\n            name='Hybrid Theory', cover_art=r'albums\\hybrid_theory.jpg'\n        )\n        response = self.client.get(reverse('admin:admin_widgets_album_change', args=(album.id,)))\n        self.assertContains(\n            response,\n            '<div class=\"readonly\"><a href=\"%(STORAGE_URL)salbums/hybrid_theory.jpg\">'\n            r'albums\\hybrid_theory.jpg</a></div>' % {'STORAGE_URL': default_storage.url('')},\n            html=True,\n        )\n        self.assertNotContains(\n            response,\n            '<input type=\"file\" name=\"cover_art\" id=\"id_cover_art\">',\n            html=True,\n        )\n        # Add multiple files\n        with open(__file__, 'rb') as f:\n            post_data = {\n                'name': 'New Album',\n                'band': band.id,\n                'cover_art': [SimpleUploadedFile('test1.txt', b'Hello, World!'), SimpleUploadedFile('test2.txt', b'Hello again!')],\n            }\n            response = self.client.post(reverse('admin:admin_widgets_album_change', args=(album.id,)), post_data)\n            self.assertEqual(response.status_code, 302)\n            album.refresh_from_db()\n            self.assertEqual(album.cover_art.name, 'test1.txt')\n", "    def test_related_widget_wrapper_attrs(self):\n        class MyWidget(forms.Select):\n                if 'required' in attrs:\n                    self.required = True\n                return super().get_context(name, value, attrs)\n\n        widget = MyWidget()\n        rel = Album._meta.get_field('band').remote_field\n        wrapper = widgets.RelatedFieldWidgetWrapper(widget, rel, admin.site)\n        wrapper.get_context('band', None, {'required': True})\n        self.assertTrue(hasattr(wrapper.widget, 'required'))\n        self.assertTrue(wrapper.widget.required)\n", "    def test_render(self):\n        widget = widgets.AdminRadioSelect()\n        choices = [\n            (1, 'Option 1'),\n            (2, 'Option 2'),\n            (3, 'Option 3'),\n        ]\n        output = widget.render('test', 2, choices=choices)\n        expected_output = (\n            '<div class=\"form-row field-test\">'\n            '<div>'\n            '<label class=\"required\" for=\"id_test_0\">Test:</label>'\n            '</div>'\n            '<div class=\"form-field form-field-radio\">'\n            '<ul id=\"id_test\">'\n            '<li><label for=\"id_test_0\"><input type=\"radio\" id=\"id_test_0\" value=\"1\" name=\"test\" /> Option 1</label></li>'\n            '<li><label for=\"id_test_1\"><input checked type=\"radio\" id=\"id_test_1\" value=\"2\" name=\"test\" /> Option 2</label></li>'\n            '<li><label for=\"id_test_2\"><input type=\"radio\" id=\"id_test_2\" value=\"3\" name=\"test\" /> Option 3</label></li>'\n            '</ul>'\n            '</div>'\n            '</div>'\n        )\n        self.assertHTMLEqual(output, expected_output)\n", "    def test_readonly_fields(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        profile = Profile.objects.create(user=self.u1, name='Test Profile')\n        change_url = reverse('admin:admin_widgets_profile_change', args=(profile.id,))\n        self.selenium.get(self.live_server_url + change_url)\n\n        # Make sure the readonly fields are displayed as text instead of inputs.\n        readonly_fields = [\n            'id',  # AutoField\n            'user',  # ForeignKey\n            'readonly_field',  # Method\n            'computed_property',  # Computed property\n        ]\n        for field in readonly_fields:\n            field_id = f'id_{field}'\n            self.assertHTMLEqual(\n                self.selenium.find_element_by_id(field_id).get_attribute('outerHTML'),\n                f'<div class=\"readonly\" id=\"{field_id}\">{getattr(profile, field)}</div>',\n            )\n", "    def test_override_widget(self):\n        # Define a ModelAdmin class that overrides the widget for CharField\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {CharField: {'widget': forms.Textarea}}\n\n        # Create a ModelAdmin instance\n        ma = MyModelAdmin(Member, admin.site)\n\n        # Get the form field for the 'name' field, which is a CharField\n        ff = ma.formfield_for_dbfield(Member._meta.get_field('name'), request=None)\n\n        # Check that the widget is a Textarea\n        self.assertIsInstance(ff.widget, forms.Textarea)\n"], "sample_173": ["    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def test_integer_field_range(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        self.assertEqual(\n            ops.integer_field_range('SmallIntegerField'), \n            (-32768, 32767)\n        )\n        self.assertEqual(\n            ops.integer_field_range('IntegerField'), \n            (-2147483648, 2147483647)\n        )\n        self.assertEqual(\n            ops.integer_field_range('BigIntegerField'), \n            (-9223372036854775808, 9223372036854775807)\n        )\n        self.assertEqual(\n            ops.integer_field_range('PositiveBigIntegerField'), \n            (0, 9223372036854775807)\n        )\n        self.assertEqual(\n            ops.integer_field_range('PositiveSmallIntegerField'), \n            (0, 32767)\n        )\n        self.assertEqual(\n            ops.integer_field_range('PositiveIntegerField'), \n            (0, 2147483647)\n        )\n        self.assertEqual(\n            ops.integer_field_range('SmallAutoField'), \n            (-32768, 32767)\n        )\n        self.assertEqual(\n            ops.integer_field_range('AutoField'), \n            (-2147483648, 2147483647)\n        )\n        self.assertEqual(\n            ops.integer_field_range('BigAutoField'), \n            (-9223372036854775808, 9223372036854775807)\n        )\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n"], "sample_174": ["    def test_explain_query_prefix_not_supported(self):\n        msg = 'This backend does not support explaining query execution.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            connection.ops.explain_query_prefix()\n", "    def test_explain_query_raise_not_supported_error(self):\n        msg = 'This backend does not support explaining query execution.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            connection.ops.explain_query_prefix()\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def test_last_executed_query(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        sql = \"SELECT * FROM table\"\n        params = [\"param1\", \"param2\"]\n        result = ops.last_executed_query(None, sql, params)\n        self.assertIn(sql, result)\n        self.assertIn(str(params), result)\n"], "sample_175": ["def test_collector_sort(self):\n    parent = Parent.objects.create()\n    child = Child.objects.create(parent_ptr=parent)\n    collector = Collector(using='default')\n    collector.add([child], source=None, nullable=False, reverse_dependency=False)\n    collector.sort()\n    self.assertEqual(list(collector.data), [Child, Parent])\n", "    def test_add_field_update(self):\n        collector = Collector(using='default')\n        field = A._meta.get_field('setnull')\n        value = R.objects.create()\n        objs = [A.objects.create(setnull=R.objects.create()) for _ in range(3)]\n        collector.add_field_update(field, value, objs)\n        self.assertEqual(len(collector.field_updates), 1)\n        self.assertEqual(len(collector.field_updates[A]), 1)\n        self.assertEqual(\n            collector.field_updates[A][(field, value)],\n            set(objs)\n        )\n", "def test_fast_delete_on_queryset_with_no_related_fields(self):\n    \"\"\"\n    Test that fast delete works on a queryset with no related fields.\n    \"\"\"\n    User.objects.create()\n    User.objects.create()\n    with self.assertNumQueries(1):\n        User.objects.all().delete()\n    self.assertEqual(User.objects.count(), 0)\n", "    def test_collector_add(self):\n        collector = Collector(using='default')\n        model = User\n        objs = [User.objects.create() for _ in range(3)]\n        added_objs = collector.add(objs)\n        self.assertEqual(len(added_objs), 3)\n        self.assertEqual(collector.data[model], set(objs))\n", "def test_collector_collects_reverse_o2o_fields(self):\n    \"\"\"\n    Collector collects reverse one-to-one fields.\n    \"\"\"\n    a = A.objects.create()\n    r = R.objects.create(a=a)\n    collector = Collector(using='default')\n    collector.collect([a])\n    self.assertIn(r, collector.data[R])\n"], "sample_176": ["def test_mti_inheritance_model_removal_with_dependencies(self):\n    \"\"\"\n    Test that when removing an MTI model, its dependencies are correctly removed.\n    \"\"\"\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    Collar = ModelState('app', 'Collar', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"dog\", models.ForeignKey(\"app.Dog\", models.CASCADE)),\n    ])\n    changes = self.get_changes([Animal, Dog, Collar], [Animal])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'DeleteModel', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='dog', model_name='collar')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='Collar')\n    self.assertOperationAttributes(changes, 'app', 0, 2, name='Dog')\n", "def test_remove_model_with_unique_together(self):\n    \"\"\"\n    Removing a model that has a unique_together constraint doesn't cause an\n    error.\n    \"\"\"\n    changes = self.get_changes([self.knight, self.rabbit], [self.knight])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'eggs', 1)\n    self.assertOperationTypes(changes, 'eggs', 0, ['RemoveField', 'AlterUniqueTogether', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'eggs', 0, 0, name='parent')\n    self.assertOperationAttributes(changes, 'eggs', 0, 1, name='rabbit', unique_together=set())\n    self.assertOperationAttributes(changes, 'eggs', 0, 2, name='Rabbit')\n", "def test_mti_inheritance_model_removal_with_foreign_key(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"owner\", models.ForeignKey(\"app.Owner\", models.CASCADE)),\n    ], bases=('app.Animal',))\n    Owner = ModelState('app', 'Owner', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([Animal, Dog, Owner], [Animal, Owner])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='dog', name='owner')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n", "def test_alter_field_null_to_not_null_with_run_python(self):\n    \"\"\"#24809 - Altering a field from null=True to null=False works.\"\"\"\n    changes = self.get_changes([self.author_name_null], [self.author_name])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n    migration = changes['testapp'][0]\n    with connection.schema_editor() as editor:\n        operation = migration.operations[0]\n        operation.database_forwards('testapp', editor, project_state=None, new_state=None)\n    # It should have an AddRunPython operation for the data migration.\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\", \"RunPython\"])\n", "def test_alter_order_with_respect_to(self):\n    \"\"\"\n    #25012 - Altering order_with_respect_to should be detected and the\n    corresponding operation created.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_book_order_wrt, self.book],\n        [self.author_with_book, self.book]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", order_with_respect_to=None)\n"], "sample_177": ["def test_get_index_by_name(self):\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"Tag\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"hidden\", models.BooleanField()),\n        ],\n        options={\n            \"indexes\": [models.Index(fields=['name'], name='name_idx')]\n        },\n    ))\n    index = project_state.models['migrations', 'tag'].get_index_by_name('name_idx')\n    self.assertEqual(index.name, 'name_idx')\n    self.assertEqual(index.fields, ['name'])\n\n    with self.assertRaises(ValueError):\n        project_state.models['migrations', 'tag'].get_index_by_name('non_existent_idx')\n", "def test_get_related_models_recursive_with_custom_manager(self):\n    new_apps = Apps()\n\n    class Author(models.Model):\n        name = models.TextField()\n\n        objects = models.Manager()\n        custom = models.Manager()\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class Book(models.Model):\n        author = models.ForeignKey(Author, models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(Author))\n    project_state.add_model(ModelState.from_model(Book))\n\n    related_models = get_related_models_recursive(project_state.apps.get_model('migrations', 'author'))\n    self.assertEqual(len(related_models), 1)\n    self.assertIn(('migrations', 'book'), related_models)\n", "def test_get_related_models_tuples(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    B = self.create_model(\"B\", foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n    C = self.create_model(\"C\")\n    self.assertEqual(get_related_models_tuples(A), {('related_models_app', 'b'), ('related_models_app', 'c')})\n    self.assertEqual(get_related_models_tuples(B), {('related_models_app', 'a'), ('related_models_app', 'c')})\n    self.assertEqual(get_related_models_tuples(C), {('related_models_app', 'a'), ('related_models_app', 'b')})\n", "def test_get_related_models_tuples(self):\n    A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n    B = self.create_model(\"B\")\n    related_models = get_related_models_tuples(A)\n    self.assertEqual(related_models, {(A._meta.app_label, 'a'), (B._meta.app_label, 'b')})\n\n    C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('D', models.CASCADE)])\n    D = self.create_model(\"D\")\n    related_models = get_related_models_tuples(C)\n    self.assertEqual(related_models, {(C._meta.app_label, 'c'), (D._meta.app_label, 'd')})\n", "def test_get_related_models_recursive_with_self_referential_foreign_key(self):\n    class A(models.Model):\n        fk = models.ForeignKey('A', models.CASCADE)\n\n    related_models = get_related_models_recursive(A)\n    self.assertEqual(related_models, set())\n"], "sample_178": ["def test_all_valid_short_circuit(self):\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '',  # invalid form\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertIs(all_valid((formset1, formset2)), False)\n    expected_errors = [{}, {'votes': ['This field is required.']}]\n    self.assertEqual(formset1._errors, expected_errors)\n    self.assertIsNone(formset2._errors)  # second formset wasn't validated\n", "def test_formset_with_empty_permitted(self):\n    \"\"\"If empty_permitted is True, formsets validate even if unchanged.\"\"\"\n    class EmptyPermittedForm(Form):\n        field = IntegerField()\n\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n        'form-0-field': '',\n    }\n    EmptyPermittedFormSet = formset_factory(EmptyPermittedForm, extra=1)\n    formset = EmptyPermittedFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([{}], formset.cleaned_data)\n\n    # Now check with min_num and an invalid form.\n    data = {\n        'form-TOTAL_FORMS': '2',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '2',\n        'form-MAX_NUM_FORMS': '0',\n        'form-0-field': '',\n        'form-1-field': '',  # this is invalid but empty_permitted will allow it\n    }\n    EmptyPermittedFormSet = formset_factory(EmptyPermittedForm, extra=2, min_num=2)\n    formset = EmptyPermittedFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([{}, {}], formset.cleaned_data)\n", "def test_management_form_with_invalid_prefix(self):\n    \"\"\"Management form should not validate with an invalid prefix.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='wrong-prefix')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['ManagementForm data is missing or has been tampered with']\n    )\n", "def test_all_valid_short_circuits(self):\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '',  # invalid form\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertIs(all_valid((formset1, formset2)), False)\n    expected_errors = [{}, {'votes': ['This field is required.']}]\n    self.assertEqual(formset1._errors, expected_errors)\n    # Since all_valid short circuits, formset2's errors shouldn't be populated.\n    self.assertIsNone(formset2._errors)\n", "def test_formset_absolute_max_with_extra_and_initial(self):\n    \"\"\"\n    Test that absolute_max works correctly when both extra and initial are set.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=1, max_num=2, absolute_max=3)\n    initial = [{'choice': 'Calexico', 'votes': 100}, {'choice': 'Fergie', 'votes': 900}]\n    data = {\n        'choices-TOTAL_FORMS': '4',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '2',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-2-choice': 'The Decemberists',\n        'choices-2-votes': '500',\n        'choices-3-choice': 'Basia Bulat',\n        'choices-3-votes': '50',\n    }\n    formset = ChoiceFormSet(data, initial=initial, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.forms), 3)\n"], "sample_180": ["    def test_check_field_referencing_nonexistent_model(self):\n        class Model(models.Model):\n            field = models.ForeignKey('nonexistentapp.NonExistentModel', on_delete=models.CASCADE)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Field 'field' clashes with the field 'field' from model \"\n                \"'invalid_models_tests.model'.\",\n                obj=Model._meta.get_field('field'),\n                id='models.E006',\n            ),\n            Error(\n                \"Field 'field' references 'nonexistentapp.NonExistentModel', \"\n                \"which has not been installed, or is abstract.\",\n                obj=Model._meta.get_field('field'),\n                id='fields.E300',\n            ),\n        ])\n", "    def test_check_ordering_on_fields_with_transforms(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=100)\n\n            class Meta:\n                ordering = ['field__lower']\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'ordering' refers to the nonexistent field, related field, \"\n                \"or lookup 'field__lower'.\",\n                obj=Model,\n                id='models.E015',\n            )\n        ])\n", "    def test_model_clean(self):\n        class Model(models.Model):\n            foo = models.CharField(max_length=10)\n\n                raise Exception('clean() is not overridden')\n\n        model = Model()\n        with self.assertRaisesMessage(Exception, 'clean() is not overridden'):\n            model.full_clean()\n", "    def test_model_attribute_name_collision(self):\n        class Model(models.Model):\n            objects = models.IntegerField()\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Model %s must specify a custom Manager, because it has a \"\n                \"field named 'objects'.\" % Model.__name__,\n                obj=Model,\n                id='models.E020',\n            ),\n        ])\n", "    def test_swappable_model(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=10)\n\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        self.assertEqual(Model.check(), [])\n"], "sample_179": ["    def test_refresh_from_db_with_deferred_fields(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n        instance = Model(field1=1, field2=2)\n        instance.save()\n        instance.refresh_from_db(fields=['field1'])\n        self.assertEqual(instance.field1, 1)\n        self.assertEqual(instance.get_deferred_fields(), {'field2'})\n", "    def test_model_attribute(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=10)\n\n            attr1 = 'test'\n            attr2 = lambda x: x  # noqa: E731\n\n                pass\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'attr1' attribute defined outside of a model's Meta class \"\n                \"must be callable or a string.\",\n                obj=Model,\n                id='models.E022',\n            ),\n            Error(\n                \"'attr2' attribute defined outside of a model's Meta class \"\n                \"must be callable or a string.\",\n                obj=Model,\n                id='models.E022',\n            ),\n            Error(\n                \"'method' attribute defined outside of a model's Meta class \"\n                \"must be callable or a string.\",\n                obj=Model,\n                id='models.E022',\n            ),\n        ])\n", "    def test_validate_unique(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10, unique=True)\n\n        model = Model(name='test')\n        try:\n            model.validate_unique()\n        except ValidationError as e:\n            self.assertEqual(e.error_dict, {})\n        else:\n            self.fail('Expected a ValidationError')\n\n        model.save()\n\n        model2 = Model(name='test')\n        with self.assertRaises(ValidationError) as cm:\n            model2.validate_unique()\n        self.assertEqual(cm.exception.error_dict, {'name': ['Model with this Name already exists.']})\n", "    def test_field_clashes_with_related_field_accessor(self):\n        class Model(models.Model):\n            fk = models.ForeignKey('self', models.CASCADE)\n            fk_id = models.IntegerField()\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The field 'fk_id' clashes with the field 'fk' from model \"\n                \"'invalid_models_tests.model'.\",\n                obj=Model._meta.get_field('fk_id'),\n                id='models.E006',\n            )\n        ])\n", "    def test_model_validation(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n\n        instance = Model(name='a' * 11)\n        with self.assertRaises(ValidationError):\n            instance.full_clean()\n"], "sample_182": ["def test_union_with_foreign_key(self):\n    # Test that the union operation works correctly when the queries being\n    # unioned have foreign keys in the select clause.\n    ReservedName.objects.create(name='a', order=1)\n    Number.objects.create(num=1, other_num=2, reserved_name_id=1)\n    qs1 = Number.objects.values('num', 'reserved_name__name')\n    qs2 = Number.objects.values('num', 'reserved_name__name')\n    self.assertCountEqual(qs1.union(qs2), [{'num': 1, 'reserved_name__name': 'a'}])\n", "def test_union_with_values_and_limit(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.all()\n    self.assertEqual(len(list(qs1.union(qs1).values('name', 'order')[:1])), 1)\n", "def test_union_with_queryset_having_select_for_update(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.select_for_update()\n    with self.assertRaisesMessage(NotSupportedError, 'FOR UPDATE cannot be used in set operations'):\n        list(qs1.union(qs2))\n", "def test_union_with_empty_result(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__gte=10)\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1], ordered=False)\n", "def test_union_with_duplicates(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    self.assertEqual(len(qs1.union(qs2, all=True)), 10)\n    self.assertEqual(len(qs1.union(qs2, all=False)), 10)\n    qs3 = Number.objects.filter(num__lte=5)\n    self.assertEqual(len(qs1.union(qs2, qs3, all=True)), 15)\n    self.assertEqual(len(qs1.union(qs2, qs3, all=False)), 10)\n"], "sample_181": ["def test_filtered_aggregate_exists(self):\n    subquery = Book.objects.filter(\n        contact__pk=OuterRef('pk'),\n        rating__gt=4,\n    )\n    aggs = Author.objects.aggregate(\n        cnt=Count('pk', filter=Exists(subquery)),\n    )\n    self.assertEqual(aggs['cnt'], 2)\n", "def test_filtered_aggregate_ref_subquery_exists(self):\n    aggs = Author.objects.annotate(\n        has_books=Exists(Book.objects.filter(contact__pk=OuterRef('pk'))),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(has_books=True)),\n    )\n    self.assertEqual(aggs['cnt'], 3)\n", "def test_filtered_aggregate_ref_subquery_annotation_with_exists(self):\n    aggs = Author.objects.annotate(\n        has_book=Exists(\n            Book.objects.filter(\n                contact__pk=OuterRef('pk'),\n            )\n        ),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(has_book=True)),\n    )\n    self.assertEqual(aggs['cnt'], 3)\n", "def test_filtered_aggregate_ref_subquery_exists(self):\n    aggs = Author.objects.annotate(\n        has_books=Exists(\n            Book.objects.filter(contact=OuterRef('pk'))\n        ),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(has_books=True)),\n    )\n    self.assertEqual(aggs['cnt'], 3)\n", "def test_filtered_aggregate_ref_exists_annotation(self):\n    aggs = Author.objects.annotate(\n        has_books=Exists(Book.objects.filter(contact=OuterRef('pk'))),\n    ).aggregate(\n        cnt=Count('pk', filter=Q(has_books=True)),\n    )\n    self.assertEqual(aggs['cnt'], 3)\n"], "sample_183": ["    def test_combinable_not_implemented(self):\n        with self.assertRaises(NotImplementedError):\n            Combinable() & Combinable()\n        with self.assertRaises(NotImplementedError):\n            Combinable() | Combinable()\n", "    def setUpTestData(cls):\n        cls.objs = [CaseTestModel.objects.create(integer=i) for i in range(1, 6)]\n", "    def test_window_expression(self):\n        CaseTestModel.objects.create(integer=1, integer2=1, string='1')\n        CaseTestModel.objects.create(integer=2, integer2=3, string='2')\n        CaseTestModel.objects.create(integer=3, integer2=4, string='3')\n        CaseTestModel.objects.create(integer=2, integer2=2, string='2')\n        CaseTestModel.objects.create(integer=3, integer2=4, string='3')\n\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                row_number=Window(expression=RowNumber())\n            ).order_by('integer', 'pk'),\n            [\n                (1, 1), (2, 2), (2, 3), (3, 4), (3, 5)\n            ],\n            transform=lambda x: (x.integer, x.row_number)\n        )\n", "    def test_resolve_expression(self):\n        expression = F('integer') + 1\n        resolved = expression.resolve_expression(query=None, allow_joins=True, reuse=None, summarize=False, for_save=False)\n        self.assertEqual(resolved.is_summary, False)\n", "    def test_expression_wrapper(self):\n        wrapper = ExpressionWrapper(Value(1, output_field=IntegerField()), output_field=IntegerField())\n        self.assertEqual(wrapper.output_field.get_internal_type(), 'IntegerField')\n        self.assertEqual(wrapper.as_sql(None, None), ('%s', [1]))\n"], "sample_184": ["    def test_required_db_features_must_be_a_list(self):\n        class Model(models.Model):\n            class Meta:\n                required_db_features = 'supports_transactions'\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'required_db_features' must be a list or tuple.\",\n                obj=Model,\n                id='models.E042',\n            ),\n        ])\n", "    def test_valid_model(self):\n        class Model(models.Model):\n            order = models.PositiveIntegerField()\n\n            class Meta:\n                ordering = ['order']\n\n        self.assertEqual(Model.check(), [])\n", "    def test_unique_for_date(self):\n        class Model(models.Model):\n            title = models.CharField(max_length=200)\n            pub_date = models.DateTimeField(unique_for_date='title')\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'unique_for_date' must be a date or datetime field.\",\n                obj=Model._meta.get_field('pub_date'),\n                id='fields.E160',\n            ),\n        ])\n", "    def test_model_validation_with_custom_pk(self):\n        class Model(models.Model):\n            custom_pk = models.CharField(max_length=10, primary_key=True)\n            name = models.CharField(max_length=10)\n\n        # Make sure no error is raised when creating a model with a custom PK.\n        Model(custom_pk='some-pk', name='some-name').full_clean()\n", "    def test_model_attribute(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=10)\n\n                pass\n\n        self.assertEqual(Model.check(), [])\n\n        # Add a model attribute with the same name as a field.\n        Model.field = 'string'\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The field 'field' clashes with a model attribute.\",\n                obj=Model._meta.get_field('field'),\n                id='models.E006',\n            ),\n        ])\n\n        # Add a model attribute with the same name as a method.\n        Model.method = 'string'\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The field 'field' clashes with a model attribute.\",\n                obj=Model._meta.get_field('field'),\n                id='models.E006',\n            ),\n            Error(\n                \"The method 'method' clashes with a model attribute.\",\n                obj=Model,\n                id='models.E028',\n            ),\n        ])\n"], "sample_185": ["    def test_decimal_separator(self):\n        self.assertEqual(sanitize_separators('123.45'), '123.45')\n        with translation.override('fr', deactivate=True):\n            self.assertEqual(sanitize_separators('123,45'), '123.45')\n", "    def test_get_format_returns_none_if_language_is_not_set(self):\n        with self.settings(USE_L10N=True):\n            with translation.override(None):\n                self.assertIsNone(get_format('DATE_FORMAT'))\n", "    def test_get_format_lazy(self):\n        self.assertIsInstance(get_format_lazy('DATE_FORMAT'), str)\n", "    def test_get_format_use_l10n(self):\n        with translation.override('de'):\n            self.assertEqual(get_format('DATE_FORMAT', use_l10n=True), 'j. F Y')\n", "    def test_get_format_lazy(self):\n        # Test that get_format_lazy returns a lazy wrapper.\n        self.assertIsInstance(get_format_lazy('DATE_FORMAT'), str)\n"], "sample_186": ["def test_prepopulated_fields(self):\n    class SongAdmin(admin.ModelAdmin):\n        prepopulated_fields = {\"slug\": (\"title\",)}\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class InvalidPrepopulatedFieldsAdmin(admin.ModelAdmin):\n        prepopulated_fields = \"test\"\n\n    self.assertEqual(InvalidPrepopulatedFieldsAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'prepopulated_fields' must be a dictionary.\",\n            obj=InvalidPrepopulatedFieldsAdmin,\n            id='admin.E026',\n        )\n    ])\n\n    class PrepopulatedFieldDoesNotExistAdmin(admin.ModelAdmin):\n        prepopulated_fields = {\"slug\": (\"nonexistent\",)}\n\n    self.assertEqual(PrepopulatedFieldDoesNotExistAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'prepopulated_fields[\\\"slug\\\"][0]' refers to 'nonexistent', \"\n            \"which is not an attribute of 'admin_checks.Song'.\",\n            obj=PrepopulatedFieldDoesNotExistAdmin,\n            id='admin.E030',\n        )\n    ])\n", "def test_check_filter_vertical(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        filter_vertical = 'test'\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'filter_vertical' must be a list or tuple.\",\n            obj=MyModelAdmin,\n            id='admin.E017',\n        )\n    ]\n    self.assertEqual(errors, expected)\n\n    class MyModelAdmin(admin.ModelAdmin):\n        filter_vertical = ('invalid_field',)\n\n    errors = MyModelAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'filter_vertical[0]' refers to 'invalid_field', \"\n            \"which is not an attribute of 'admin_checks.Song'.\",\n            obj=MyModelAdmin,\n            id='admin.E019',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_autocomplete_fields_item(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"album\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class InvalidSongAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"nonexistent_field\"]\n\n    errors = InvalidSongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' refers to 'nonexistent_field', \"\n            \"which is not an attribute of 'admin_checks.Song'.\",\n            obj=InvalidSongAdmin,\n            id='admin.E037',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_filter_vertical_must_be_a_list_or_tuple(self):\n    class SongAdmin(admin.ModelAdmin):\n        filter_vertical = 'test'\n\n    self.assertEqual(SongAdmin(Song, AdminSite()).check(), [\n        checks.Error(\n            \"The value of 'filter_vertical' must be a list or tuple.\",\n            obj=SongAdmin,\n            id='admin.E017',\n        )\n    ])\n", "def test_filter_vertical_m2m(self):\n    class SongAdmin(admin.ModelAdmin):\n        filter_vertical = ('genres',)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n"], "sample_187": ["def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces(\"CamelCase\"), \"camel case\")\n    self.assertEqual(text.camel_case_to_spaces(\"lowerCamelCase\"), \"lower camel case\")\n    self.assertEqual(text.camel_case_to_spaces(\"ABCDEF\"), \"abcdef\")\n    self.assertEqual(text.camel_case_to_spaces(\"\"), \"\")\n    self.assertEqual(text.camel_case_to_spaces(lazystr(\"CamelCase\")), \"camel case\")\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('CamelCase', 'camel case'),\n        (' camel Case', 'camel case'),\n        ('Camel  Case', 'camel  case'),\n        ('MultipleCamelCaseWords', 'multiple camel case words'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        (' CamelCase', 'camel case'),\n        ('camel Case', 'camel case'),\n        ('CamelCase', 'camel case'),\n        ('CAMELCase', 'camel case'),\n        (' camel  Case ', 'camel case'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        ('CamelCase', 'camel case'),\n        ('CAMELCase', 'camel case'),\n        ('_camelCase', '_camel case'),\n        ('Camel_Case', 'camel _case'),\n        ('__Camel_Case__', '__camel _case__'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        ('anotherCamelCase', 'another camel case'),\n        ('noCamelcase', 'no camelcase'),\n        ('CamelCase', 'camel case'),\n        ('SingleCharacter', 'single character'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n    self.assertEqual(text.camel_case_to_spaces(lazystr('camelCase')), 'camel case')\n"], "sample_188": ["    def test_expression_list_repr(self):\n        expr1 = F('col')\n        expr2 = F('anothercol')\n        exprlist = ExpressionList(expr1, expr2)\n        self.assertEqual(repr(exprlist), 'ExpressionList(F(col), F(anothercol))')\n", "    def test_repr(self):\n        self.assertEqual(repr(Star()), \"'*'\")\n", "    def test_repr(self):\n        expr1 = F('col')\n        expr2 = F('anothercol')\n        exprlist = ExpressionList(expr1, expr2)\n        self.assertEqual(repr(exprlist), 'ExpressionList(F(col), F(anothercol))')\n", "    def test_empty_expression_list(self):\n        with self.assertRaisesMessage(ValueError, 'ExpressionList requires at least one expression'):\n            ExpressionList()\n", "    def test_no_params(self):\n        raw_sql = RawSQL('1', ())\n        self.assertEqual(raw_sql.sql, '1')\n        self.assertEqual(raw_sql.params, [])\n"], "sample_189": ["compilation error", "    def test_get_cache_key_with_query_params(self):\n        request = self.factory.get(self.path, {'test': 1})\n        response = HttpResponse()\n        learn_cache_key(request, response)\n        cache_key = get_cache_key(request)\n        self.assertEqual(\n            cache_key,\n            'views.decorators.cache.cache_page.cacheprefix.GET.'\n            'beaf87a9a99ee81c673ea2d67ccbec2a.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_without_vary_on(self):\n        key = make_template_fragment_key('a.fragment')\n        self.assertEqual(key, 'template.cache.a.fragment.d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_get_cache_key_with_headers(self):\n        request = HttpRequest()\n        request.method = 'GET'\n        request.META['HTTP_ACCEPT_ENCODING'] = 'gzip'\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-us'\n\n        response = HttpResponse()\n        response['Vary'] = 'accept-encoding, accept-language'\n\n        learn_cache_key(request, response)\n\n        cache_key = get_cache_key(request)\n        self.assertIn('gzip', cache_key)\n        self.assertIn('en-us', cache_key)\n", "    def tearDown(self):\n        cache.clear()\n"], "sample_190": ["def test_exists_subquery_with_limit(self):\n    subquery = Article.objects.filter(author=self.au1).order_by('id')[:2]\n    articles = Article.objects.filter(\n        Exists(subquery.filter(id=OuterRef('id')))\n    )\n    self.assertEqual(articles.count(), 2)\n    self.assertEqual(set(articles.values_list('id', flat=True)), {self.a1.id, self.a2.id})\n", "def test_nested_outerref_lhs_with_subquery(self):\n    tag = Tag.objects.create(name=self.au1.alias)\n    tag.articles.add(self.a1)\n    subquery = Author.objects.filter(alias=OuterRef(OuterRef('name')))\n    qs = Tag.objects.annotate(\n        has_author_alias_match=Exists(\n            Article.objects.annotate(\n                author_exists=Exists(subquery),\n            ).filter(author_exists=True)\n        ),\n    )\n    self.assertEqual(qs.get(has_author_alias_match=True), tag)\n", "def test_in_bulk_with_unhashable_values(self):\n    # Test that in_bulk() can handle unhashable values.\n    a1 = Article.objects.create(headline='Article 1')\n    a2 = Article.objects.create(headline='Article 2')\n\n    class UnhashableObject:\n            self.value = value\n\n            return self.value == other.value\n\n    unhashable_a1 = UnhashableObject(a1.pk)\n    unhashable_a2 = UnhashableObject(a2.pk)\n\n    articles = Article.objects.in_bulk([unhashable_a1, unhashable_a2])\n    self.assertEqual(articles[a1.pk], a1)\n    self.assertEqual(articles[a2.pk], a2)\n", "def test_in_bulk_with_unsupported_field(self):\n    msg = (\n        \"in_bulk()'s field_name must be a unique field but 'headline' isn't. \"\n        \"If it is a related field, consider using the ID of the related object.\"\n    )\n    with self.assertRaisesMessage(ValueError, msg):\n        Article.objects.in_bulk(field_name='headline')\n", "def test_nested_outerref_lhs_with_filter(self):\n    tag = Tag.objects.create(name=self.au1.alias)\n    tag.articles.add(self.a1)\n    qs = Tag.objects.annotate(\n        has_author_alias_match=Exists(\n            Article.objects.annotate(\n                author_exists=Exists(\n                    Author.objects.filter(alias=OuterRef(OuterRef('name')))\n                ),\n            ).filter(author_exists=True, headline__startswith='A')\n        ),\n    )\n    self.assertEqual(qs.get(has_author_alias_match=True), tag)\n"], "sample_191": ["    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_does_nothing_if_termios_is_not_available(self, mocked_termios):\n        mocked_termios.tcgetattr.side_effect = Exception()\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_termios.tcsetattr.called)\n", "    def test_thread_safety(self):\n        class MyException(Exception):\n            pass\n\n        # Create an exception\n        try:\n            raise MyException('Test Message')\n        except MyException:\n            exc_info = sys.exc_info()\n\n        with mock.patch('django.utils.autoreload._exception', exc_info):\n                autoreload.raise_last_exception()\n\n            thread1 = threading.Thread(target=raise_exception)\n            thread2 = threading.Thread(target=raise_exception)\n\n            thread1.start()\n            thread2.start()\n\n            thread1.join()\n            thread2.join()\n\n            # If the test reaches this point without crashing, it's a success.\n            self.assertTrue(True)\n", "    def test_ensure_echo_on(self, mocked_termios):\n        # Mock the tcgetattr and tcsetattr functions to do nothing.\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        mocked_termios.tcsetattr.return_value = None\n\n        # Call ensure_echo_on and verify that it calls tcgetattr and tcsetattr.\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_ensure_echo_on_calls_tcsetattr(self, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcsetattr.called)\n"], "sample_192": ["def test_all_valid_short_circuits(self):\n    \"\"\"all_valid() short-circuits on the first invalid formset.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '',  # missing value\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n\n        formset.is_valid()\n        return True\n\n    with mock.patch('django.forms.formsets.BaseFormSet.is_valid', side_effect=is_valid) as mock_is_valid:\n        self.assertIs(all_valid((formset1, formset2)), False)\n        self.assertEqual(mock_is_valid.call_count, 1)\n", "def test_formset_with_initial_and_max_num_equal_to_initial(self):\n    \"\"\"\n    Test that when max_num is equal to the number of initial forms, no extra\n    forms are displayed.\n    \"\"\"\n    initial = [\n        {'name': 'Fernet and Coke'},\n        {'name': 'Bloody Mary'},\n    ]\n    LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, extra=1, max_num=len(initial))\n    formset = LimitedFavoriteDrinkFormSet(initial=initial)\n    self.assertEqual(len(formset.forms), len(initial))\n    self.assertHTMLEqual(\n        '\\n'.join(str(form) for form in formset.forms),\n        \"\"\"<tr><th><label for=\"id_form-0-name\">Name:</label></th>", "def test_add_fields_with_empty_permitted(self):\n    \"\"\"add_fields is called with empty_permitted=True for extra forms.\"\"\"\n    class CheckForm(Form):\n        field = IntegerField()\n\n    data = {\n        'check-TOTAL_FORMS': '2',\n        'check-INITIAL_FORMS': '1',\n        'check-0-field': '200',\n        'check-1-field': '',\n    }\n    CheckFormSet = formset_factory(CheckForm, extra=1)\n    formset = CheckFormSet(data, prefix='check')\n    self.assertTrue(formset.forms[1].empty_permitted)\n    self.assertEqual(formset.errors, [{}, {}])\n", "def test_all_valid_short_circuit(self):\n    \"\"\"all_valid short-circuits and stops calling is_valid on the rest of formsets.\"\"\"\n    class ShortCircuitForm(Form):\n        field = IntegerField()\n\n            raise AssertionError(\"is_valid was called\")\n\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '',  # missing value\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = formset_factory(ShortCircuitForm)(data, auto_id=False, prefix='choices')\n    self.assertIs(all_valid((formset1, formset2)), False)\n", "def test_absolute_max_with_min_num(self):\n    data = {\n        'form-TOTAL_FORMS': '1001',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '1000',\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        min_num=1000,\n        absolute_max=1000,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 1000)\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit 1000 or fewer forms.'],\n    )\n"], "sample_193": ["def test_foreign_object_reverse_related_fields(self):\n    \"\"\"\n    ForeignObject.reverse_related_fields returns the related fields in reverse.\n    \"\"\"\n    new_apps = Apps()\n\n    class Author(models.Model):\n        name = models.CharField(max_length=255)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class Book(models.Model):\n        title = models.CharField(max_length=1000)\n        author = models.ForeignKey(Author, models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(Author))\n    project_state.add_model(ModelState.from_model(Book))\n\n    book_state = project_state.models['migrations', 'book']\n    self.assertEqual(len(book_state.fields['author'].reverse_related_fields), 1)\n    self.assertIsInstance(book_state.fields['author'].reverse_related_fields[0][0], models.AutoField)\n    self.assertIsInstance(book_state.fields['author'].reverse_related_fields[0][1], models.AutoField)\n", "def test_foreign_key_in_abstract_base(self):\n    new_apps = Apps()\n\n    class AbstractBase(models.Model):\n        fk = models.ForeignKey('migrations.Concrete', models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n            abstract = True\n\n    class Concrete(models.Model):\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class Subclass(AbstractBase):\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(Concrete))\n    project_state.add_model(ModelState.from_model(Subclass))\n    self.assertEqual(len(project_state.apps.get_models()), 2)\n", "    def test_contribute_to_class(self):\n        new_apps = Apps()\n\n        class Book(models.Model):\n            title = models.CharField(max_length=1000)\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        class Author(models.Model):\n            book = models.ForeignKey(Book, models.CASCADE)\n            related_book = models.ForeignKey('migrations.Book', models.CASCADE, related_name='+')\n\n            class Meta:\n                app_label = \"migrations\"\n                apps = new_apps\n\n        project_state = ProjectState()\n        project_state.add_model(ModelState.from_model(Book))\n        project_state.add_model(ModelState.from_model(Author))\n\n        self.assertEqual(len(project_state.apps.get_models()), 2)\n        self.assertEqual(\n            len([f for f in project_state.apps.get_model(\"migrations\", \"Author\")._meta.get_fields() if f.is_relation]),\n            2,\n        )\n", "def test_m2m_field_options(self):\n    \"\"\"\n    The options of a ManyToManyField are correctly represented in ModelState.\n    \"\"\"\n    new_apps = Apps()\n\n    class A(models.Model):\n        b = models.ManyToManyField('B', related_name='a')\n\n        class Meta:\n            app_label = 'migrations'\n            apps = new_apps\n\n    class B(models.Model):\n        class Meta:\n            app_label = 'migrations'\n            apps = new_apps\n\n    state = ModelState.from_model(A)\n    field_state = state.fields['b']\n    self.assertEqual(field_state.remote_field.related_name, 'a')\n    self.assertEqual(field_state.remote_field.model, 'migrations.B')\n", "    def test_related_fields(self):\n        model = ModelState('app', 'Model', [\n            ('field1', models.IntegerField()),\n            ('field2', models.CharField(max_length=255)),\n        ])\n        field = ForeignKey(to='app.Model', on_delete=models.CASCADE, from_fields=['field1'], to_fields=['field2'])\n        related_fields = field.related_fields\n        self.assertEqual(len(related_fields), 1)\n        self.assertEqual(related_fields[0][0].name, 'field1')\n        self.assertEqual(related_fields[0][1].name, 'field2')\n"], "sample_194": ["def test_opclasses_must_match_fields(self):\n    msg = (\n        'UniqueConstraint.fields and UniqueConstraint.opclasses must '\n        'have the same number of elements.'\n    )\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            name='uniq',\n            fields=['field1', 'field2'],\n            opclasses=['jsonb_path_ops'],\n        )\n", "def test_invalid_opclasses_length(self):\n    msg = 'UniqueConstraint.fields and UniqueConstraint.opclasses must have the same number of elements.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            name='uniq_opclasses',\n            fields=['field1', 'field2'],\n            opclasses=['jsonb_path_ops'],\n        )\n", "def test_invalid_opclasses_length(self):\n    msg = 'UniqueConstraint.fields and UniqueConstraint.opclasses must have the same number of elements.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            name='uniq_opclasses',\n            fields=['field1', 'field2'],\n            opclasses=['jsonb_path_ops'],\n        )\n", "def test_opclasses_length_mismatch(self):\n    msg = (\n        'UniqueConstraint.fields and UniqueConstraint.opclasses must '\n        'have the same number of elements.'\n    )\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            fields=['name', 'color'],\n            name='opclasses_length_mismatch',\n            opclasses=['text_pattern_ops'],\n        )\n", "def test_clone(self):\n    fields = ['foo', 'bar']\n    name = 'unique_fields'\n    condition = models.Q(foo=models.F('bar'))\n    deferrable = models.Deferrable.DEFERRED\n    include = ['baz_1', 'baz_2']\n    opclasses = ['text_pattern_ops', 'varchar_pattern_ops']\n\n    constraint = models.UniqueConstraint(\n        fields=fields,\n        name=name,\n        condition=condition,\n        deferrable=deferrable,\n        include=include,\n        opclasses=opclasses,\n    )\n\n    cloned_constraint = constraint.clone()\n    self.assertEqual(cloned_constraint.fields, tuple(fields))\n    self.assertEqual(cloned_constraint.name, name)\n    self.assertEqual(cloned_constraint.condition, condition)\n    self.assertEqual(cloned_constraint.deferrable, deferrable)\n    self.assertEqual(cloned_constraint.include, tuple(include))\n    self.assertEqual(cloned_constraint.opclasses, opclasses)\n"], "sample_195": ["    def setUp(self):\n        self.ops = connection.ops\n", "    def setUp(self):\n        self.ops = connection.ops\n", "    def test_bulk_batch_size(self):\n        ops = connection.ops\n        fields = [models.CharField(max_length=10)]\n        objs = [Author(name='author1'), Author(name='author2')]\n        self.assertEqual(ops.bulk_batch_size(fields, objs), 500)\n\n        fields = [models.CharField(max_length=10), models.IntegerField()]\n        objs = [Author(name='author1', num=1), Author(name='author2', num=2)]\n        self.assertLessEqual(ops.bulk_batch_size(fields, objs), 999)\n", "    def test_bulk_insert_sql(self):\n        ops = connection.ops\n        fields = ['field1', 'field2']\n        placeholder_rows = [\n            ['%s', '%s'],\n            ['%s', '%s'],\n        ]\n        sql = ops.bulk_insert_sql(fields, placeholder_rows)\n        self.assertEqual(sql, \" UNION ALL \".join(\n            \"SELECT %s, %s\" for _ in placeholder_rows\n        ))\n", "    def setUp(self):\n        self.ops = connection.ops\n"], "sample_196": ["    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n"], "sample_198": ["    def setUpTestData(cls):\n        cls.company = Company.objects.create(name='Example Inc.', num_employees=2300, num_chairs=5)\n        cls.ceo = Employee.objects.create(firstname='Joe', lastname='Smith', salary=10)\n        cls.manager = Manager.objects.create(name='John')\n        cls.ceo.manager = cls.manager\n        cls.ceo.save()\n        cls.company.ceo = cls.ceo\n        cls.company.save()\n", "    def test_repr(self):\n        self.assertEqual(repr(OuterRef('name')), \"OuterRef(name)\")\n", "    def test_repr(self):\n        raw_sql = RawSQL('table.col', ['param'], output_field=IntegerField())\n        self.assertEqual(repr(raw_sql), \"RawSQL(table.col, ['param'])\")\n", "    def test_repr(self):\n        ref = ResolvedOuterRef('name')\n        self.assertEqual(repr(ref), '<ResolvedOuterRef: name>')\n", "    def test_instantiation(self):\n        raw = RawSQL('table.col', ['param'])\n        self.assertEqual(raw.sql, 'table.col')\n        self.assertEqual(raw.params, ['param'])\n"], "sample_197": ["def test_depth_value_error(self):\n    \"\"\"Test that a ValueError is raised when depth is less than or equal to 0.\"\"\"\n    with self.assertRaises(ValueError):\n        timesince(self.t, self.t + self.oneday, depth=0)\n    with self.assertRaises(ValueError):\n        timeuntil(self.t + self.oneday, self.t, depth=-1)\n", "def test_invalid_depth(self):\n    \"\"\"Test timesince and timeuntil with invalid depth.\"\"\"\n    t = self.t + self.oneyear + self.onemonth\n    with self.assertRaises(ValueError):\n        timesince(self.t, t, depth=0)\n    with self.assertRaises(ValueError):\n        timeuntil(t, self.t, depth=-1)\n", "def test_depth_zero(self):\n    with self.assertRaises(ValueError):\n        timesince(self.t, self.t + self.oneday, depth=0)\n    with self.assertRaises(ValueError):\n        timeuntil(self.t + self.oneday, self.t, depth=0)\n", "def test_depth_zero(self):\n    with self.assertRaises(ValueError):\n        timesince(self.t, self.t + self.oneday, depth=0)\n    with self.assertRaises(ValueError):\n        timeuntil(self.t + self.oneday, self.t, depth=0)\n", "def test_invalid_depth(self):\n    \"\"\"Test that a ValueError is raised for invalid depth values.\"\"\"\n    t = self.t + self.oneyear\n    with self.assertRaises(ValueError):\n        timesince(self.t, t, depth=0)\n    with self.assertRaises(ValueError):\n        timeuntil(t, self.t, depth=-1)\n"], "sample_199": ["def test_annotate_with_when(self):\n    books = Book.objects.annotate(\n        price_category=Case(\n            When(price__gt=30, then=Value('expensive')),\n            When(price__lte=30, then=Value('cheap')),\n            output_field=CharField(),\n        )\n    ).values_list('price_category', flat=True)\n    self.assertCountEqual(books, ['expensive', 'cheap'])\n", "def test_subquery_annotation(self):\n    subquery = Book.objects.filter(author=OuterRef('pk')).values('author').annotate(count=Count('pk')).values('count')\n    authors = Author.objects.annotate(book_count=Subquery(subquery, output_field=IntegerField())).values('name', 'book_count')\n    self.assertCountEqual(\n        authors,\n        [\n            {'name': 'Adrian Holovaty', 'book_count': 1},\n            {'name': 'Jacob Kaplan-Moss', 'book_count': 1},\n            {'name': 'Brad Dayley', 'book_count': 1},\n            {'name': 'James Bennett', 'book_count': 1},\n            {'name': 'Jeffrey Forcier', 'book_count': 1},\n            {'name': 'Paul Bissex', 'book_count': 1},\n            {'name': 'Wesley J. Chun', 'book_count': 1},\n            {'name': 'Peter Norvig', 'book_count': 2},\n            {'name': 'Stuart Russell', 'book_count': 1},\n        ]\n    )\n", "def test_outerref_in_filter(self):\n    long_books_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n    ).values('publisher')\n    publisher_books_qs = Publisher.objects.annotate(\n        has_long_book=Exists(long_books_qs),\n    ).filter(has_long_book=True).values('name')\n    self.assertCountEqual(publisher_books_qs, [{'name': 'Sams'}, {'name': 'Morgan Kaufmann'}])\n", "def test_expression_wrapper(self):\n    wrapper = ExpressionWrapper(F('rating') + 1, output_field=FloatField())\n    qs = Book.objects.annotate(rating_plus_one=wrapper).values_list('rating', 'rating_plus_one')\n    for rating, rating_plus_one in qs:\n        self.assertEqual(rating + 1, rating_plus_one)\n", "def test_window_expression(self):\n    # Test Window function with a partition by clause\n    authors = Author.objects.annotate(\n        row_number=Window(\n            expression=Func('id', function='ROW_NUMBER'),\n            partition_by=[F('age')],\n        )\n    ).order_by('age')\n    for author in authors:\n        self.assertEqual(author.row_number, 1)\n\n    # Test Window function without a partition by clause\n    authors = Author.objects.annotate(\n        row_number=Window(\n            expression=Func('id', function='ROW_NUMBER'),\n        )\n    ).order_by('id')\n    for i, author in enumerate(authors, start=1):\n        self.assertEqual(author.row_number, i)\n"], "sample_200": ["def test_email_message_str(self):\n    email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    self.assertEqual(str(email), 'Subject: Subject\\nFrom: from@example.com\\nTo: to@example.com\\n\\nContent')\n", "def test_sanitize_address_with_non_ascii_local_part(self):\n    \"\"\"\n    Ensure EmailMessage sanitizes the local part of an email address when it contains non-ASCII characters.\n    \"\"\"\n    tests = [\n        ('test@\u00e9xample.com', 'test@xn--xample-9ua.com'),\n        ('t\u00e9st@example.com', '=?utf-8?q?t=C3=A9st?= <t%C3%A9st@example.com>'),\n        (('T\u00e9st Example', 'test@example.com'), '=?utf-8?q?T=C3=A9st_Example?= <test@example.com>'),\n    ]\n    for email_address, expected_result in tests:\n        with self.subTest(email_address=email_address):\n            self.assertEqual(sanitize_address(email_address, encoding='utf-8'), expected_result)\n", "def test_email_message_str(self):\n    email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n    self.assertEqual(str(email), 'Subject: Subject\\nFrom: from@example.com\\nTo: to@example.com\\n\\nContent')\n", "def test_attach_file_with_non_ascii_filename(self):\n    \"\"\"\n    Test attaching a file with a non-ASCII filename.\n    \"\"\"\n    msg = EmailMessage('subject', 'body', 'from@example.com', ['to@example.com'])\n    msg.attach_file(__file__, mimetype='text/plain')\n    filename, content, mimetype = self.get_decoded_attachments(msg)[0]\n    self.assertEqual(filename, Path(__file__).name)\n    self.assertEqual(mimetype, 'text/plain')\n    with open(__file__, 'rb') as f:\n        self.assertEqual(content, f.read())\n", "def test_sanitize_address_with_idna(self):\n    # Addresses with non-ASCII domains are properly encoded as IDN.\n    self.assertEqual(\n        sanitize_address('test@\u00f6\u00e4\u00fc.com', 'utf-8'),\n        'test@xn--4ca9at.com'\n    )\n    self.assertEqual(\n        sanitize_address(('Test', 'test@\u00f6\u00e4\u00fc.com'), 'utf-8'),\n        'Test <test@xn--4ca9at.com>'\n    )\n    # Addresses with both a non-ASCII local part and a non-ASCII domain.\n    self.assertEqual(\n        sanitize_address(('T\u00e9st', 'test@\u00f6\u00e4\u00fc.com'), 'utf-8'),\n        '=?utf-8?q?T=C3=A9st?= <test@xn--4ca9at.com>'\n    )\n"], "sample_201": ["def test_legacy_hash(self):\n    \"\"\"\n    The legacy hash function generates the correct hash for a given value.\n    \"\"\"\n    storage = self.get_storage()\n    value = 'test_value'\n    expected_hash = salted_hmac('django.contrib.messages', value).hexdigest()\n    self.assertEqual(storage._legacy_hash(value), expected_hash)\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    The not_finished sentinel value is correctly removed from the end of the\n    message list, and the all_retrieved flag is set accordingly.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    messages = [Message(constants.INFO, 'Test message 1'), Message(constants.INFO, 'Test message 2')]\n    encoded_data = storage._encode(messages + [CookieStorage.not_finished])\n    storage.request.COOKIES = {CookieStorage.cookie_name: encoded_data}\n    retrieved_messages, all_retrieved = storage._get()\n    self.assertEqual(retrieved_messages, messages)\n    self.assertFalse(all_retrieved)\n\n    # Test that the sentinel is not removed if it's not at the end of the list\n    messages = [Message(constants.INFO, 'Test message 1'), CookieStorage.not_finished, Message(constants.INFO, 'Test message 2')]\n    encoded_data = storage._encode(messages)\n    storage.request.COOKIES = {CookieStorage.cookie_name: encoded_data}\n    retrieved_messages, all_retrieved = storage._get()\n    self.assertEqual(retrieved_messages, messages)\n    self.assertTrue(all_retrieved)\n", "def test_legacy_hash(self):\n    \"\"\"\n    The _legacy_hash method returns the correct hash for the given value.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    value = 'test_value'\n    expected_hash = salted_hmac('django.contrib.messages', value).hexdigest()\n    self.assertEqual(storage._legacy_hash(value), expected_hash)\n\n    # Test that changing the secret key changes the hash\n    with override_settings(SECRET_KEY='new_secret_key'):\n        new_storage = self.storage_class(self.get_request())\n        new_expected_hash = salted_hmac('django.contrib.messages', value).hexdigest()\n        self.assertNotEqual(new_storage._legacy_hash(value), expected_hash)\n        self.assertEqual(new_storage._legacy_hash(value), new_expected_hash)\n", "def test_legacy_hash(self):\n    \"\"\"\n    The legacy hash method correctly produces the HMAC/SHA1 hash.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    value = 'test_value'\n    expected_hash = salted_hmac('django.contrib.messages', value).hexdigest()\n    self.assertEqual(storage._legacy_hash(value), expected_hash)\n", "def test_mark_safe(self):\n    \"\"\"\n    A message containing SafeData is marked as safe when retrieved from the message storage.\n    \"\"\"\n    storage = self.storage_class(self.get_request())\n    safe_message = Message(constants.DEBUG, mark_safe('<p>Hello</p>'))\n    encoded = storage._encode([safe_message])\n    decoded_messages = storage._decode(encoded)\n    self.assertIsInstance(decoded_messages[0].message, SafeData)\n"], "sample_202": ["def test_mark_safe(self):\n    storage = self.storage_class(self.get_request())\n    safe_message = Message(constants.INFO, mark_safe('Test message'))\n    encoded_data = storage._encode([safe_message])\n    decoded_messages = storage._decode(encoded_data)\n    self.assertIsInstance(decoded_messages[0].message, SafeData)\n", "def test_safedata(self):\n    storage = self.get_storage()\n    messages = [Message(constants.INFO, mark_safe('<b>Hello</b>'))]\n    set_cookie_data(storage, messages)\n    self.assertIsInstance(storage._decode(storage.request.COOKIES['messages'])[0].message, SafeData)\n    self.assertEqual(storage._decode(storage.request.COOKIES['messages'])[0].message, mark_safe('<b>Hello</b>'))\n", "def test_process_messages(self):\n    \"\"\"\n    Test the process_messages method of MessageDecoder.\n    \"\"\"\n    decoder = MessageDecoder()\n    messages = [\n        Message(constants.INFO, 'Test message'),\n        Message(constants.ERROR, 'Error message'),\n    ]\n    encoded_messages = MessageEncoder().encode(messages)\n    decoded_messages = decoder.process_messages(json.loads(encoded_messages))\n    self.assertEqual(decoded_messages, messages)\n\n    # Test nested lists and dictionaries\n    messages = [\n        Message(constants.INFO, 'Test message'),\n        {'nested': [Message(constants.ERROR, 'Error message')]},\n    ]\n    encoded_messages = MessageEncoder().encode(messages)\n    decoded_messages = decoder.process_messages(json.loads(encoded_messages))\n    self.assertEqual(decoded_messages, messages)\n", "def test_safedata_encoding(self):\n    \"\"\"\n    Messages containing SafeData are encoded with a '1' flag to indicate their\n    safe status.\n    \"\"\"\n    storage = self.get_storage()\n    message = Message(constants.DEBUG, mark_safe(\"Hello\"))\n    encoded = storage._encode([message])\n    hashed_value, value = encoded.split('$')\n    decoded_value = json.loads(value)\n    self.assertEqual(decoded_value[0], '__json_message')\n    self.assertEqual(decoded_value[1], 1)  # The '1' flag indicates SafeData\n    self.assertEqual(decoded_value[2], constants.DEBUG)\n    self.assertEqual(decoded_value[3], \"Hello\")\n", "def test_update_cookie_empty_data(self):\n    \"\"\"\n    The _update_cookie method should delete the cookie when there is no data to store.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage._update_cookie(None, response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n"], "sample_203": ["    def test_regex_validator_equality(self):\n        v1 = validators.RegexValidator('^[a-zA-Z]*$')\n        v2 = validators.RegexValidator('^[a-zA-Z]*$')\n        v3 = validators.RegexValidator('^[0-9]*$')\n\n        self.assertEqual(v1, v2)\n        self.assertNotEqual(v1, v3)\n", "    def test_regex_validator(self):\n        validator = validators.RegexValidator(regex='^[a-z]*$')\n        with self.assertRaises(ValidationError) as e:\n            validator('Hello')\n        self.assertEqual(e.exception.messages, ['Enter a valid value.'])\n", "    def test_prohibit_null_characters_validator(self):\n        validator = validators.ProhibitNullCharactersValidator()\n        with self.assertRaises(ValidationError) as e:\n            validator('Hello\\x00World')\n        self.assertEqual(e.exception.messages, ['Null characters are not allowed.'])\n\n        # Test that the validator does not raise an error for a string without null characters.\n        validator('Hello World')  # Should not raise an exception\n", "    def test_regex_validator_flags(self):\n        validator = validators.RegexValidator(\n            regex='^[a-z]*$',\n            message=\"Letters only.\",\n            flags=re.IGNORECASE,\n        )\n        self.assertIsNone(validator('hello'))\n        self.assertIsNone(validator('HELLO'))\n", "    def test_regex_validator(self):\n        # Test RegexValidator with string regex.\n        validator = validators.RegexValidator(regex='^[a-zA-Z]*$')\n        self.assertIsNone(validator('Hello'))\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid value.'):\n            validator('Hello1')\n\n        # Test RegexValidator with compiled regex.\n        validator = validators.RegexValidator(regex=re.compile('^[a-zA-Z]*$'))\n        self.assertIsNone(validator('Hello'))\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid value.'):\n            validator('Hello1')\n\n        # Test RegexValidator with regex and message.\n        validator = validators.RegexValidator(regex='^[a-zA-Z]*$', message='Letters only.')\n        self.assertIsNone(validator('Hello'))\n        with self.assertRaisesMessage(ValidationError, 'Letters only.'):\n            validator('Hello1')\n\n        # Test RegexValidator with inverse_match.\n        validator = validators.RegexValidator(regex='^[a-zA-Z]*$', inverse_match=True)\n        self.assertIsNone(validator('Hello1'))\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid value.'):\n            validator('Hello')\n"], "sample_204": ["def test_conflict_detection(self):\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {\"migrations\": [\"0002_second\", \"0002_conflicting\"]})\n", "def test_collect_sql(self):\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    recorder.record_applied('migrations', '0001_initial')\n    plan = loader.graph.forwards_plan(('migrations', '0002_second'))\n    sql_statements = loader.collect_sql(plan)\n    self.assertEqual(len(sql_statements), 4)\n    self.assertIn(\"CREATE TABLE\", sql_statements[0])\n    self.assertIn(\"ALTER TABLE\", sql_statements[1])\n    self.assertIn(\"CREATE TABLE\", sql_statements[2])\n    self.assertIn(\"ALTER TABLE\", sql_statements[3])\n", "def test_circular_dependency(self):\n    \"\"\"\n    Tests loading a set of migrations with a circular dependency.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    with self.assertRaises(CircularDependencyError):\n        loader.build_graph()\n", "def test_collect_sql(self):\n    \"\"\"\n    Makes sure collect_sql() returns the SQL statements for the plan.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    plan = [\n        (\"migrations\", \"0001_initial\"),\n        (\"migrations\", \"0002_second\"),\n    ]\n\n    sql_statements = loader.collect_sql(plan)\n    self.assertEqual(len(sql_statements), 4)\n    self.assertIn(\"CREATE TABLE\", sql_statements[0])\n    self.assertIn(\"ALTER TABLE\", sql_statements[2])\n", "def test_detect_conflicts(self):\n    \"\"\"\n    MigrationLoader.detect_conflicts() identifies apps with multiple leaf nodes.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0002_second', '0003_third']})\n"], "sample_205": ["def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError('message')\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {NON_FIELD_ERRORS: ['message']})\n\n    error_dict = {}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['message'], 'field2': ['other']})\n\n    error_dict = {'field1': ['existing']}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['existing', 'message'], 'field2': ['other']})\n", "def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError('message')\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'__all__': ['message']})\n\n    error_dict = {}\n    exception = ValidationError({'field1': 'message'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['message']})\n\n    error_dict = {'field1': ['existing message']}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['existing message', 'message'], 'field2': ['other']})\n", "def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError('message')\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'__all__': ['message']})\n\n    error_dict = {}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['message'], 'field2': ['other']})\n\n    error_dict = {'field1': ['existing']}\n    exception = ValidationError({'field1': 'message', 'field2': 'other'})\n    exception.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['existing', 'message'], 'field2': ['other']})\n", "def test_repr(self):\n    error1 = ValidationError('message')\n    error2 = ValidationError('message', code='my_code1')\n    error3 = ValidationError(\n        'error %(parm1)s %(parm2)s',\n        code='my_code1',\n        params={'parm1': 'val1', 'parm2': 'val2'},\n    )\n    error4 = ValidationError({'field1': 'message', 'field2': 'other'})\n    error5 = ValidationError([\n        ValidationError({'field1': 'field error', 'field2': 'other'}),\n        'message',\n    ])\n\n    self.assertEqual(repr(error1), \"ValidationError(['message'])\")\n    self.assertEqual(repr(error2), \"ValidationError(['message'])\")\n    self.assertEqual(\n        repr(error3),\n        \"ValidationError(['error val1 val2'])\",\n    )\n    self.assertEqual(\n        repr(error4),\n        \"ValidationError({'field1': ['message'], 'field2': ['other']})\",\n    )\n    self.assertEqual(\n        repr(error5),\n        \"ValidationError([{'field1': ['field error'], 'field2': ['other']}, 'message'])\",\n    )\n", "def test_update_error_dict(self):\n    error1 = ValidationError('message')\n    error2 = ValidationError({'field1': 'error1', 'field2': 'error2'})\n    error_dict = {}\n\n    error1.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'__all__': ['message']})\n\n    error_dict = {}\n    error2.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n    error_dict = {'field1': ['existing_error']}\n    error2.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['existing_error', 'error1'], 'field2': ['error2']})\n"], "sample_206": ["def test_closed(self):\n    \"\"\"\n    FieldFile.closed returns True when the file is closed and False otherwise.\n    \"\"\"\n    d = Document.objects.create(myfile='something.txt')\n    self.assertFalse(d.myfile.closed)\n    d.myfile.close()\n    self.assertTrue(d.myfile.closed)\n    d.myfile.open()\n    self.assertFalse(d.myfile.closed)\n", "def test_fieldfile_eq(self):\n    \"\"\"\n    FieldFile __eq__ method should return True when comparing with a string \n    representing the same file name, or another FieldFile instance pointing \n    to the same file.\n    \"\"\"\n    d = Document.objects.create(myfile='something.txt')\n    self.assertEqual(d.myfile, 'something.txt')\n    self.assertEqual(d.myfile, d.myfile)\n    self.assertNotEqual(d.myfile, 'other_file.txt')\n    self.assertNotEqual(d.myfile, None)\n    self.assertNotEqual(d.myfile, object())\n", "def test_save_with_pathlib(self):\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        with override_settings(MEDIA_ROOT=Path(tmp_dir)):\n            file1 = File(open(__file__, 'rb'), name='test_file.py')\n            document = Document(myfile='test_file.py')\n            document.myfile.save(Path('test_file.py'), file1)\n            try:\n                self.assertEqual(document.myfile.name, 'unused/test_file.py')\n                self.assertTrue(os.path.exists(os.path.join(tmp_dir, 'unused', 'test_file.py')))\n            finally:\n                document.myfile.delete()\n", "def test_closed_property(self):\n    \"\"\"\n    FieldFile.closed should be True when the file is closed and False otherwise.\n    \"\"\"\n    d = Document.objects.create(myfile='something.txt')\n    self.assertFalse(d.myfile.closed)\n    d.myfile.close()\n    self.assertTrue(d.myfile.closed)\n    d.myfile.open()\n    self.assertFalse(d.myfile.closed)\n", "def test_closed_property(self):\n    \"\"\"\n    FieldFile.closed returns True if the file is closed, False otherwise.\n    \"\"\"\n    d = Document.objects.create(myfile='something.txt')\n    self.assertFalse(d.myfile.closed)\n    d.myfile.close()\n    self.assertTrue(d.myfile.closed)\n"], "sample_207": ["    def test_check_constraints(self):\n        constraint = models.CheckConstraint(\n            check=models.Q(value__has_key='a'),\n            name='has_key_a',\n        )\n        with connection.schema_editor() as schema_editor:\n            schema_editor.add_constraint(NullableJSONModel, constraint)\n            try:\n                obj = NullableJSONModel.objects.create(value={'b': 'x'})\n                with self.assertRaises(IntegrityError):\n                    obj.save()\n                obj.value = {'a': 'y'}\n                obj.save()\n            finally:\n                schema_editor.remove_constraint(NullableJSONModel, constraint)\n", "    def test_db_check_constraints(self):\n        value = '{\"a\": \"b\", \"c\": 14, \"d\": [\"e\", {\"f\": \"g\"}]}'\n        with mock.patch.object(DjangoJSONEncoder, 'encode', return_value=value):\n            model_instance = JSONModel(value={'a': 'b', 'c': 14, 'd': ['e', {'f': 'g'}]})\n            errors = model_instance._meta.get_field('value').check()\n            self.assertEqual(errors, [])\n", "    def test_custom_encoder_decoder(self):\n        class CustomEncoder(json.JSONEncoder):\n                if isinstance(obj, uuid.UUID):\n                    return str(obj)\n                return super().default(obj)\n\n        class CustomDecoder(json.JSONDecoder):\n                super().__init__(object_hook=self.dict_to_object, *args, **kwargs)\n\n                for key, value in dictionary.items():\n                    if isinstance(value, str) and len(value) == 36:\n                        try:\n                            dictionary[key] = uuid.UUID(value)\n                        except ValueError:\n                            pass\n                return dictionary\n\n        model_field = models.JSONField(encoder=CustomEncoder, decoder=CustomDecoder)\n        value = {'uuid': uuid.UUID('{d85e2076-b67c-4ee7-8c3a-2bf5a2cc2475}')}\n        model_field.clean(value, None)\n        self.assertEqual(model_field.to_python(model_field.get_prep_value(value)), value)\n", "def test_key_transform_on_annotation(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.annotate(\n            foo=KeyTransform('foo', 'value'),\n        ).filter(foo='bar'),\n        [self.objs[7]],\n    )\n", "    def test_db_check_constraints(self):\n        model = NullableJSONModel(value_custom='invalid json')\n        with CaptureQueriesContext(connection) as queries:\n            model.full_clean()\n            model.save()\n            self.assertGreater(len(queries), 0)\n            for query in queries:\n                self.assertNotIn('CHECK', query['sql'])\n"], "sample_208": ["def test_circular_dependency_same_app(self):\n    \"\"\"\n    #23315 - The dependency resolver knows to put all CreateModel\n    before AddField and not become unsolvable.\n    \"\"\"\n    address = ModelState(\"a\", \"Address\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"country\", models.ForeignKey(\"a.DeliveryCountry\", models.CASCADE)),\n    ])\n    apackage = ModelState(\"a\", \"APackage\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"person\", models.ForeignKey(\"a.Person\", models.CASCADE)),\n    ])\n    country = ModelState(\"a\", \"DeliveryCountry\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    person = ModelState(\"a\", \"Person\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    changes = self.get_changes([], [address, apackage, country, person])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'a', 2)\n    self.assertOperationTypes(changes, 'a', 0, [\"CreateModel\", \"CreateModel\"])\n    self.assertOperationTypes(changes, 'a', 1, [\"AddField\", \"AddField\"])\n", "def test_add_model_with_field_removed_from_base_model_in_different_app(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name, even if they are in different apps.\n    \"\"\"\n    before = [\n        ModelState('app1', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app1', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app2', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app1.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app1', 1)\n    self.assertOperationTypes(changes, 'app1', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app1', 0, 0, name='title', model_name='readable')\n    self.assertNumberMigrations(changes, 'app2', 1)\n    self.assertOperationTypes(changes, 'app2', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app2', 0, 0, name='book')\n", "def test_alter_model_options_remove_permissions(self):\n    \"\"\"Changing a model's options should make a change.\"\"\"\n    before = self.make_project_state([self.author_with_options])\n    after = self.make_project_state([\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], {\n            \"verbose_name\": \"Authi\",\n        })\n    ])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={\n        \"permissions\": [],\n        \"verbose_name\": \"Authi\",\n    })\n", "def test_alter_model_table_on_proxy_model(self):\n    \"\"\"\n    AlterModelTable should not be generated when a proxy model's db_table is \n    changed to match the db_table of its concrete parent.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.author_proxy], \n        [self.author_empty, ModelState(\"testapp\", \"AuthorProxy\", [], {\"proxy\": True}, (\"testapp.author\",), options={\"db_table\": \"testapp_author\"})]\n    )\n    self.assertEqual(len(changes[\"testapp\"]), 0)\n", "def test_alter_model_table_with_existing_fk(self):\n    changes = self.get_changes(\n        [self.author_with_book, self.book],\n        [self.author_renamed_with_book, self.book_with_author_renamed],\n        MigrationQuestioner({\"ask_rename\": True, \"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterModelTable\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"Writer\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"writer\", table=\"testapp_author\")\n"], "sample_209": ["    def test_refresh_from_db(self):\n        # Create a model instance and save it to the database\n        article = Article.objects.create(headline=\"Original Headline\", pub_date=datetime.datetime.now())\n        \n        # Update the instance's headline attribute, but do not save it to the database\n        article.headline = \"Updated Headline\"\n        \n        # Refresh the instance from the database\n        article.refresh_from_db()\n        \n        # Check that the instance's headline attribute has been reverted to its original value\n        self.assertEqual(article.headline, \"Original Headline\")\n", "def test_get_deferred_fields(self):\n    obj = Article.objects.create(\n        headline=\"Look at me!\", pub_date=datetime.datetime.now()\n    )\n    deferred_obj = Article.objects.defer('headline').get(pk=obj.pk)\n    self.assertEqual(deferred_obj.get_deferred_fields(), {'headline'})\n\n    # After accessing a deferred field, it should be removed from the set of deferred fields.\n    getattr(deferred_obj, 'headline')\n    self.assertEqual(deferred_obj.get_deferred_fields(), set())\n", "    def test_fields_cache(self):\n        # Create an instance of Model1 and access a field to populate fields_cache.\n        m1 = Model1.objects.create(pkey=1000)\n        _ = m1.pkey\n\n        # Check that fields_cache is populated.\n        self.assertIn('pkey', m1._state.fields_cache)\n\n        # Update the instance's field value through setattr.\n        setattr(m1, 'pkey', 2000)\n\n        # Check that fields_cache is updated.\n        self.assertEqual(m1._state.fields_cache['pkey'], 2000)\n", "    def test_model_state_fields_cache_descriptor(self):\n        # Test that the fields_cache descriptor returns a dictionary\n        # when accessed on an instance and raises an AttributeError when\n        # accessed on a class.\n        obj = Model1()\n        self.assertIsInstance(obj._state.fields_cache, dict)\n\n        msg = \"type object 'ModelState' has no attribute 'fields_cache'\"\n        with self.assertRaisesMessage(AttributeError, msg):\n            ModelState.fields_cache\n", "    def test_model_state_fields_cache_descriptor(self):\n        model = Worker()\n        self.assertEqual(model._state.fields_cache, {})\n        model._state.fields_cache['test'] = 'value'\n        self.assertEqual(model._state.fields_cache, {'test': 'value'})\n        del model._state.fields_cache['test']\n        self.assertEqual(model._state.fields_cache, {})\n"], "sample_210": ["def test_template_response_mixin_render_to_response(self):\n    view = TemplateView.as_view(template_name='generic_views/about.html')\n    request = self.rf.get('/')\n    response = view(request)\n    self.assertIsInstance(response, TemplateResponse)\n    self.assertEqual(response.template_name, 'generic_views/about.html')\n    self.assertEqual(response.context['view'], view)\n    self.assertIsNone(response.content)\n    rendered_response = response.render()\n    self.assertIsNotNone(rendered_response.content)\n", "    def test_extra_context(self):\n        class TestView(ContextMixin, View):\n            extra_context = {'extra_key': 'extra_value'}\n\n        test_view = TestView()\n        context = test_view.get_context_data()\n\n        self.assertIn('extra_key', context)\n        self.assertEqual(context['extra_key'], 'extra_value')\n", "    def test_setup_sets_request_on_self(self):\n        view = View()\n        request = self.rf.get('/')\n        view.setup(request)\n        self.assertEqual(view.request, request)\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data()\n        self.assertIn('view', context)\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        view.extra_context = {'key': 'value'}\n        context = view.get_context_data()\n        self.assertIn('view', context)\n        self.assertEqual(context['key'], 'value')\n"], "sample_211": ["    def test_setup_method_not_called_twice(self):\n        class TestView(View):\n                if hasattr(self, 'setup_called'):\n                    self.setup_called += 1\n                else:\n                    self.setup_called = 1\n                super().setup(request, *args, **kwargs)\n\n        view = TestView()\n        view.setup(self.rf.get('/'))\n        self.assertEqual(view.setup_called, 1)\n        view.dispatch(self.rf.get('/'))\n        self.assertEqual(view.setup_called, 1)\n", "    def test_get_template_names_required(self):\n        \"\"\"\n        A template response mixin must provide a get_template_names method.\n        \"\"\"\n        class TestView(TemplateResponseMixin, View):\n            pass\n\n        msg = (\n            \"TemplateResponseMixin requires either a definition of \"\n            \"'template_name' or an implementation of 'get_template_names()'\"\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            TestView.as_view()(self.rf.get('/'))\n", "    def test_get_context_data(self):\n        class TestView(ContextMixin, View):\n                return self.get_context_data()\n\n        view = TestView()\n        context = view.get_context_data()\n        self.assertIn('view', context)\n        self.assertEqual(context['view'], view)\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data(test_key='test_value')\n        self.assertEqual(context['test_key'], 'test_value')\n        self.assertIn('view', context)\n        self.assertEqual(context['view'], view)\n", "    def test_get_context_data(self):\n        view = ContextMixin()\n        context = view.get_context_data(key='value')\n        self.assertEqual(context, {'key': 'value', 'view': view})\n"], "sample_213": ["    def test_file_descriptor_get(self):\n        # Test that the file descriptor returns None when the instance doesn't have the attribute.\n        obj = Storage()\n        self.assertIsNone(obj.normal_file_descriptor)\n\n        # Test that the file descriptor returns the FieldFile instance when the attribute is set.\n        obj.normal.save(\"test.txt\", ContentFile(\"content\"))\n        self.assertIsInstance(obj.normal_file_descriptor, FieldFile)\n        self.assertEqual(obj.normal_file_descriptor.instance, obj)\n", "    def test_descriptor_get(self):\n        instance = Storage()\n        fd = FileDescriptor('test')\n        fd.storage = default_storage\n\n        # Assigning to the descriptor should work as expected\n        fd.__set__(instance, 'test_value')\n        self.assertEqual(fd.__get__(instance), 'test_value')\n\n        # If no instance is provided, the descriptor should return itself\n        self.assertIs(fd.__get__(None), fd)\n\n        # If the value is a string, it should be wrapped in a FieldFile\n        fd.__set__(instance, 'test_string')\n        value = fd.__get__(instance)\n        self.assertIsInstance(value, FieldFile)\n        self.assertEqual(value.name, 'test_string')\n\n        # If the value is a File, it should be wrapped in a FieldFile\n        f = File(None, 'test_file')\n        fd.__set__(instance, f)\n        value = fd.__get__(instance)\n        self.assertIsInstance(value, FieldFile)\n        self.assertEqual(value.name, 'test_file')\n        self.assertEqual(value.file, f)\n\n        # If the value is already a FieldFile, it should be returned as-is\n        ff = FieldFile(instance, fd, 'test_fieldfile')\n        fd.__set__(instance, ff)\n        self.assertIs(fd.__get__(instance), ff)\n\n        # If the value is None, it should be returned as-is\n        fd.__set__(instance, None)\n        self.assertIsNone(fd.__get__(instance))\n", "    def setUp(self):\n        self.storage_dir = tempfile.mkdtemp()\n        self.storage = FileSystemStorage(location=self.storage_dir)\n        self.obj = Storage.objects.create()\n", "    def test_field_file_methods(self):\n        # Create a model with a FileField\n        obj = Storage()\n        obj.normal.save(\"django_test.txt\", ContentFile(\"content\"))\n\n        # Test methods of the FieldFile object\n        self.assertEqual(obj.normal.name, \"tests/django_test.txt\")\n        self.assertEqual(obj.normal.path, os.path.join(temp_storage.location, \"tests/django_test.txt\"))\n        self.assertEqual(obj.normal.url, \"/test_media_url/tests/django_test.txt\")\n        self.assertEqual(obj.normal.size, 7)\n\n        # Test saving a new file to the same field\n        obj.normal.save(\"new_name.txt\", ContentFile(\"new content\"))\n        self.assertEqual(obj.normal.name, \"tests/new_name.txt\")\n\n        # Test deleting the file\n        obj.normal.delete(save=False)\n        self.assertFalse(temp_storage.exists(\"tests/new_name.txt\"))\n", "    def test_field_file_methods(self):\n        obj = Storage()\n        obj.normal.save(\"django_test.txt\", ContentFile(\"content\"))\n        field_file = obj.normal\n\n        self.assertEqual(field_file.name, \"tests/django_test.txt\")\n        self.assertEqual(field_file.path, temp_storage.path(\"tests/django_test.txt\"))\n        self.assertEqual(field_file.url, temp_storage.url(\"tests/django_test.txt\"))\n        self.assertEqual(field_file.size, 7)\n\n        with field_file.open() as f:\n            self.assertEqual(f.read(), b\"content\")\n\n        with field_file.open('rb') as f:\n            self.assertEqual(f.read(), b\"content\")\n\n        with field_file.open('wb') as f:\n            f.write(b\"new content\")\n\n        with field_file.open('r') as f:\n            self.assertEqual(f.read(), b\"new content\")\n\n        field_file.close()\n        self.assertTrue(field_file.closed)\n\n        # Make sure the file is not removed when the FieldFile object is garbage collected.\n        del field_file\n        self.assertTrue(temp_storage.exists(\"tests/django_test.txt\"))\n"], "sample_212": ["def test_process_response_session_save(self):\n    class TestSessionMiddleware(SessionMiddleware):\n            super().__init__(get_response)\n            self.SessionStore = type('SessionStore', (), {\n                'accessed': True,\n                'modified': True,\n                'is_empty': lambda: False,\n                'get_expire_at_browser_close': lambda: False,\n                'get_expiry_age': lambda: 3600,\n                'save': lambda: None,\n            })\n\n    request = HttpRequest()\n    request.session = self.SessionStore()\n    response = HttpResponse()\n    middleware = TestSessionMiddleware(lambda req: response)\n    response = middleware.process_response(request, response)\n\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n    self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME]['max-age'], 3600)\n", "def test_session_middleware_process_response(self):\n    class DummySession:\n            self.session_key = session_key\n            self.accessed = True\n            self.modified = True\n            self.empty = False\n\n            return False\n\n            return 3600\n\n            pass\n\n    class DummyRequest:\n            self.COOKIES = {}\n            self.session = None\n\n    class DummyResponse:\n            self.status_code = 200\n            self.cookies = {}\n\n            self.cookies[key] = {\n                'value': value,\n                'max_age': max_age,\n                'expires': expires,\n                'domain': domain,\n                'path': path,\n                'secure': secure,\n                'httponly': httponly,\n                'samesite': samesite,\n            }\n\n            if key in self.cookies:\n                del self.cookies[key]\n\n    request = DummyRequest()\n    response = DummyResponse()\n\n    session_middleware = SessionMiddleware()\n    session_middleware.SessionStore = DummySession\n\n    # Test case 1: empty session\n    request.session = DummySession(None)\n    request.session.empty = True\n    session_middleware.process_response(request, response)\n    self.assertEqual(response.cookies, {})\n\n    # Test case 2: non-empty session\n    request.session = DummySession('session_key')\n    session_middleware.process_response(request, response)\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n    self.assertIsNotNone(response.cookies[settings.SESSION_COOKIE_NAME]['max_age'])\n    self.assertIsNotNone(response.cookies[settings.SESSION_COOKIE_NAME]['expires'])\n", "def test_session_middleware_process_response(self):\n    \"\"\"\n    Test SessionMiddleware.process_response correctly handles session saving and \n    cookie setting/deletion.\n    \"\"\"\n    class DummySession:\n            self.session_key = session_key\n            self.accessed = True\n            self.modified = True\n            self.empty = False\n\n            pass\n\n            pass\n\n            return False\n\n            return 60\n\n            return self.empty\n\n    class DummyRequest:\n            self.COOKIES = {}\n            self.session = DummySession('dummy_session_key')\n\n    class DummyResponse:\n            self.status_code = 200\n            self.cookies = {}\n\n            self.cookies[key] = {\n                'value': value,\n                'max_age': max_age,\n                'expires': expires,\n                'domain': domain,\n                'path': path,\n                'secure': secure,\n                'httponly': httponly,\n                'samesite': samesite,\n            }\n\n            if key in self.cookies:\n                del self.cookies[key]\n\n    request = DummyRequest()\n    response = DummyResponse()\n\n    # Test case 1: Session is not empty and modified\n    middleware = SessionMiddleware(get_response=lambda req: response)\n    middleware.process_response(request, response)\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test case 2: Session is empty\n    request.session.empty = True\n    middleware.process_response(request, response)\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n    # Test case 3: Session is not empty but not modified\n    request.session.empty = False\n    request.session.modified = False\n    middleware.process_response(request, response)\n    self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n", "def test_session_middleware_process_response(self):\n    request = HttpRequest()\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'session_key'\n    request.session = self.client.session\n    request.session.modified = True\n\n    response = HttpResponse()\n    session_middleware = SessionMiddleware(get_response=lambda req: response)\n    session_middleware.process_request(request)\n\n    # Test that the session is saved when the response status code is not 500.\n    response = session_middleware.process_response(request, response)\n    self.assertEqual(response.status_code, 200)\n\n    # Test that the session is not saved when the response status code is 500.\n    response.status_code = 500\n    response = session_middleware.process_response(request, response)\n    self.assertEqual(response.status_code, 500)\n\n    # Test that the session cookie is deleted when the session is empty.\n    request.session.clear()\n    response = session_middleware.process_response(request, response)\n    self.assertTrue(settings.SESSION_COOKIE_NAME in response.cookies)\n    self.assertTrue(response.cookies[settings.SESSION_COOKIE_NAME]['expires'] == 'Thu, 01-Jan-1970 00:00:00 GMT')\n", "def test_process_response_session_save(self):\n    class DummySession:\n            self.session_key = session_key\n            self.accessed = True\n            self.modified = True\n            self.empty = False\n\n            return False\n\n            return 3600\n\n            pass\n\n    class DummyRequest:\n        COOKIES = {}\n\n            self.session = DummySession('session_key')\n\n    class DummyResponse:\n        status_code = 200\n\n            pass\n\n            pass\n\n    middleware = SessionMiddleware(get_response=lambda req: DummyResponse())\n    response = middleware.process_response(DummyRequest(), DummyResponse())\n    self.assertEqual(response.status_code, 200)\n"], "sample_214": ["    def setUpTestData(cls):\n        cls.objs = [\n            NullableJSONModel.objects.create(value={'bool': True}),\n            NullableJSONModel.objects.create(value={'bool': False}),\n            NullableJSONModel.objects.create(value={'bool': None}),\n        ]\n", "    def test_db_check_constraints(self):\n        field = models.JSONField()\n        errors = field.check()\n        self.assertEqual(errors, [])\n", "    def test_db_check_constraints(self):\n        class Model(models.Model):\n            field = models.JSONField()\n\n        # Field with no default and not null should have a db check constraint.\n        self.assertEqual(Model._meta.get_field('field').db_check, True)\n\n        class Model(models.Model):\n            field = models.JSONField(default={})\n\n        # Field with a default should not have a db check constraint.\n        self.assertEqual(Model._meta.get_field('field').db_check, False)\n\n        class Model(models.Model):\n            field = models.JSONField(null=True)\n\n        # Field that's nullable should not have a db check constraint.\n        self.assertEqual(Model._meta.get_field('field').db_check, False)\n", "    def test_model_validation(self):\n        model = NullableJSONModel(value={'foo': 'bar'})\n        model.clean_fields()\n", "def test_key_transform_raw_expression_with_params(self):\n    raw_sql = '%s::jsonb' if connection.vendor == 'postgresql' else '%s'\n    expr = RawSQL(raw_sql, ['{\"x\": \"bar\"}'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr)),\n        [self.objs[7]],\n    )\n"], "sample_215": ["    def test_get_safe_settings(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        settings_dict = reporter_filter.get_safe_settings()\n        self.assertIsInstance(settings_dict, dict)\n        for key, value in settings_dict.items():\n            self.assertIsInstance(key, str)\n            if isinstance(value, CallableSettingWrapper):\n                continue\n            elif isinstance(value, (list, tuple)):\n                for item in value:\n                    self.assertNotIn('sensitive', str(item).lower())\n            elif isinstance(value, dict):\n                for k, v in value.items():\n                    self.assertNotIn('sensitive', str(k).lower())\n                    self.assertNotIn('sensitive', str(v).lower())\n            else:\n                self.assertNotIn('sensitive', str(value).lower())\n", "    def test_get_safe_settings(self):\n        class FakeSettings:\n            NOT_SENSITIVE_SETTING = 'public_value'\n            SECRET_KEY = 'secret_value'\n            DATABASES = {\n                'default': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                    'NAME': '/path/to/db.sqlite3',\n                    'PASSWORD': 'secret_password',\n                }\n            }\n\n        reporter_filter = SafeExceptionReporterFilter()\n        safe_settings = reporter_filter.get_safe_settings(FakeSettings)\n\n        self.assertEqual(safe_settings['NOT_SENSITIVE_SETTING'], 'public_value')\n        self.assertEqual(safe_settings['SECRET_KEY'], reporter_filter.cleansed_substitute)\n        self.assertEqual(safe_settings['DATABASES']['default']['ENGINE'], 'django.db.backends.sqlite3')\n        self.assertEqual(safe_settings['DATABASES']['default']['NAME'], '/path/to/db.sqlite3')\n        self.assertEqual(safe_settings['DATABASES']['default']['PASSWORD'], reporter_filter.cleansed_substitute)\n", "    def test_get_safe_settings(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        settings_dict = reporter_filter.get_safe_settings()\n        self.assertIn('SECRET_KEY', settings_dict)\n        self.assertNotEqual(settings_dict['SECRET_KEY'], settings.SECRET_KEY)\n        self.assertEqual(settings_dict['SECRET_KEY'], reporter_filter.cleansed_substitute)\n", "    def test_technical_404_response(self):\n        request = self.rf.get('/test_view/')\n        exception = Http404('Page not found')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, 'technical_404.html')\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIsInstance(settings, dict)\n        for key in dir(settings):\n            if key.isupper():\n                self.assertIn(key, settings)\n"], "sample_216": ["def test_field_references(self):\n    model_tuple = ('testapp', 'Author')\n    field_tuple = ('name',)\n    state = ProjectState()\n    state.add_model(ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ]))\n    state.add_model(ModelState('otherapp', 'Book', [\n        ('id', models.AutoField(primary_key=True)),\n        ('author', models.ForeignKey('testapp.Author', models.CASCADE, to_field='name')),\n    ]))\n\n    references = list(get_references(state, model_tuple, field_tuple))\n    self.assertEqual(len(references), 1)\n    model_state, name, field, reference = references[0]\n    self.assertEqual(model_state.app_label, 'otherapp')\n    self.assertEqual(model_state.name, 'Book')\n    self.assertEqual(name, 'author')\n    self.assertEqual(field.name, 'author')\n    self.assertIsInstance(reference, FieldReference)\n    self.assertIsNotNone(reference.to)\n    self.assertIsNone(reference.through)\n", "def test_field_references(self):\n    state = ProjectState()\n    model_state = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n        (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n    ])\n    state.add_model(model_state)\n    model_tuple = (\"testapp\", \"author\")\n    field_tuple = (\"book\",)\n\n    self.assertTrue(field_is_referenced(state, model_tuple, field_tuple))\n\n    # Check that a non-existent model returns False\n    self.assertFalse(field_is_referenced(state, (\"nonexistent\", \"model\"), (\"field\",)))\n\n    # Check that a non-existent field returns False\n    self.assertFalse(field_is_referenced(state, model_tuple, (\"nonexistent\",)))\n\n    # Check that a field that is not a reference returns False\n    self.assertFalse(field_is_referenced(state, model_tuple, (\"name\",)))\n", "def test_resolve_relation_with_recursive_relationship_constant(self):\n    model_tuple = ('testapp', 'model')\n    self.assertEqual(resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, *model_tuple), model_tuple)\n", "def test_field_references(self):\n    state = ProjectState()\n    model_tuple = ('testapp', 'Author')\n    field_tuple = ('name',)\n    state.add_model(ModelState(*model_tuple, [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n        ('biography', models.TextField()),\n    ]))\n    state.add_model(ModelState('testapp', 'Book', [\n        ('id', models.AutoField(primary_key=True)),\n        ('author', models.ForeignKey('testapp.Author', models.CASCADE, to_field='name')),\n        ('title', models.CharField(max_length=200)),\n    ]))\n    references = list(get_references(state, model_tuple, field_tuple))\n    self.assertEqual(len(references), 1)\n    model_state, name, field, reference = references[0]\n    self.assertEqual(model_state.app_label, 'testapp')\n    self.assertEqual(model_state.name, 'Book')\n    self.assertEqual(name, 'author')\n    self.assertEqual(field.remote_field.to_fields, ['name'])\n    self.assertIsInstance(reference, FieldReference)\n    self.assertEqual(reference.to, (field.remote_field, ['name']))\n    self.assertIsNone(reference.through)\n", "def test_resolve_relation_recursive_relationship(self):\n    model_tuple = ('testapp', 'model')\n    self.assertEqual(resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT, *model_tuple), model_tuple)\n    with self.assertRaises(TypeError):\n        resolve_relation(RECURSIVE_RELATIONSHIP_CONSTANT)\n"], "sample_217": ["def test_media_absolute_path(self):\n    m = Media(\n        css={'all': ('path/to/css1', '/path/to/css2')},\n        js=('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3'),\n    )\n    self.assertEqual(m.absolute_path('path/to/css1'), 'http://media.example.com/static/path/to/css1')\n    self.assertEqual(m.absolute_path('/path/to/css2'), '/path/to/css2')\n    self.assertEqual(m.absolute_path('http://media.other.com/path/to/js2'), 'http://media.other.com/path/to/js2')\n    self.assertEqual(m.absolute_path('https://secure.other.com/path/to/js3'), 'https://secure.other.com/path/to/js3')\n", "def test_merge_css_different_media_types(self):\n    widget1 = Media(css={'screen': ['a.css'], 'print': ['b.css']})\n    widget2 = Media(css={'screen': ['c.css'], 'all': ['d.css']})\n    merged = widget1 + widget2\n    self.assertEqual(merged._css, {'screen': ['a.css', 'c.css'], 'print': ['b.css'], 'all': ['d.css']})\n", "def test_widget_needs_multipart_form(self):\n    class MyWidget1(TextInput):\n        needs_multipart_form = True\n\n    class MyWidget2(TextInput):\n        needs_multipart_form = False\n\n    class MyMultiWidget(MultiWidget):\n            widgets = [MyWidget1, MyWidget2]\n            super().__init__(widgets, attrs)\n\n    mymulti = MyMultiWidget()\n    self.assertTrue(mymulti.needs_multipart_form)\n", "def test_absolute_path(self):\n    media = Media(js=['/path/to/js1', 'http://media.other.com/path/to/js2'])\n    self.assertEqual(media.absolute_path('/path/to/js1'), '/path/to/js1')\n    self.assertEqual(media.absolute_path('http://media.other.com/path/to/js2'), 'http://media.other.com/path/to/js2')\n    self.assertEqual(media.absolute_path('path/to/js3'), 'http://media.example.com/static/path/to/js3')\n", "def test_media_property_override(self):\n    class MyWidget(TextInput):\n            return Media(css={'all': ('/some/path',)}, js=('/some/js',))\n        media = property(_media)\n\n    class MySubclassWidget(MyWidget):\n            return Media(css={'all': ('/other/path',)}, js=('/other/js',))\n        media = property(_media)\n\n    w = MySubclassWidget()\n    self.assertEqual(str(w.media), \"\"\"<link href=\"/other/path\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_218": ["def test_trunc_func_with_timezone_transition_crossing(self):\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = datetime(2016, 10, 16, 0)\n    end_datetime = datetime(2016, 2, 21, 0)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n        with self.assertRaisesMessage(pytz.NonExistentTimeError, '2016-10-16 00:00:00'):\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=sao)\n            ).get()\n\n        with self.assertRaisesMessage(pytz.AmbiguousTimeError, '2016-02-20 23:00:00'):\n            DTModel.objects.annotate(\n                truncated=Trunc('end_datetime', kind, output_field=DateTimeField(), tzinfo=sao)\n            ).get()\n\n        model = DTModel.objects.annotate(\n            truncated_start=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=sao, is_dst=False),\n            truncated_end=Trunc('end_datetime', kind, output_field=DateTimeField(), tzinfo=sao, is_dst=True),\n        ).get()\n        self.assertEqual(model.truncated_start.dst(), timedelta(0))\n        self.assertEqual(model.truncated_end.dst(), timedelta(0, 3600))\n\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n", "def test_trunc_func_with_timezone_transition(self):\n    \"\"\"\n    If the truncated datetime transitions to a different offset (daylight\n    saving) then the returned value will have that new timezone/offset.\n    \"\"\"\n    start_datetime = datetime(2018, 3, 11, 1, 30, 50, 321)\n    end_datetime = datetime(2018, 11, 4, 1, 30, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n    nyc = pytz.timezone('America/New_York')\n\n    model = DTModel.objects.annotate(\n        truncated_start=TruncHour('start_datetime', tzinfo=nyc),\n        truncated_end=TruncHour('end_datetime', tzinfo=nyc),\n    ).get()\n\n    self.assertEqual(model.truncated_start.dst(), timedelta(0))\n    self.assertEqual(model.truncated_end.dst(), timedelta(0, 3600))\n", "def test_trunc_func_with_non_naive_input(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        qs = DTModel.objects.annotate(\n            truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb)\n        ).order_by('start_datetime')\n        for model in qs:\n            self.assertEqual(model.truncated.tzinfo, melb)\n\n        qs = DTModel.objects.annotate(\n            truncated=Trunc('start_date', kind, output_field=DateField(), tzinfo=melb)\n        ).order_by('start_datetime')\n        for model in qs:\n            self.assertIsNone(model.truncated.tzinfo)\n\n    test_datetime_kind('year')\n    test_datetime_kind('quarter')\n    test_datetime_kind('month')\n    test_datetime_kind('week')\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n    test_datetime_kind('minute')\n    test_datetime_kind('second')\n    test_date_kind('year')\n    test_date_kind('quarter')\n    test_date_kind('month')\n    test_date_kind('week')\n    test_date_kind('day')\n", "def test_trunc_func_with_is_dst(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n    self.create_model(end_datetime, start_datetime)\n\n    melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=melb, is_dst=True)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), kind, melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), kind, melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n", "def test_trunc_func_with_timezone_transition_crossing(self):\n    sao = pytz.timezone('America/Sao_Paulo')\n    start_datetime = datetime(2016, 10, 16, 1)\n    end_datetime = datetime(2017, 2, 19, 23)\n    start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n    end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n    self.create_model(start_datetime, end_datetime)\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=sao)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(sao), kind, sao)),\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n    test_datetime_kind('day')\n    test_datetime_kind('hour')\n"], "sample_219": ["    def test_resolve(self):\n        ref = ResolvedOuterRef('example')\n        self.assertEqual(ref.resolve_expression(), ref)\n", "    def test_resolve_output_field(self):\n        # output_field is required for RawSQL.\n        with self.assertRaisesMessage(FieldError, 'RawSQL must be provided an output_field'):\n            RawSQL('1', ()).output_field\n\n        # output_field is not resolved if provided.\n        raw_sql = RawSQL('1', (), output_field=IntegerField())\n        self.assertEqual(raw_sql.output_field, IntegerField())\n\n        # output_field is not resolved if provided as a model field.\n        raw_sql = RawSQL('1', (), output_field=Number._meta.get_field('integer'))\n        self.assertEqual(raw_sql.output_field, Number._meta.get_field('integer'))\n", "    def test_repr(self):\n        self.assertEqual(repr(OuterRef('name')), \"OuterRef(name)\")\n", "    def test_subquery_annotation(self):\n        inner = Company.objects.filter(point_of_contact=OuterRef('pk')).values('name')\n        outer = Employee.objects.annotate(\n            company_name=Subquery(inner[:1], output_field=CharField()),\n        ).filter(company_name='Test GmbH')\n        self.assertEqual(outer.get().company_name, 'Test GmbH')\n", "    def setUpTestData(cls):\n        cls.n1 = Number.objects.create(integer=-1)\n        cls.n2 = Number.objects.create(integer=42)\n        cls.n3 = Number.objects.create(integer=1337)\n"], "sample_220": ["    def test_reason_phrase(self):\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n        response.status_code = 404\n        self.assertEqual(response.reason_phrase, 'Not Found')\n", "    def test_reason_phrase(self):\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n        response.status_code = 404\n        self.assertEqual(response.reason_phrase, 'Not Found')\n        response.reason_phrase = 'Custom'\n        self.assertEqual(response.reason_phrase, 'Custom')\n", "    def test_serialize_headers(self):\n        response = HttpResponse()\n        response['Content-Type'] = 'text/plain; charset=utf-8'\n        self.assertEqual(\n            b'Content-Type: text/plain; charset=utf-8\\r\\n',\n            response.serialize_headers()\n        )\n", "    def test_serialize_headers(self):\n        response = HttpResponseBase()\n        response['Content-Type'] = 'text/plain'\n        self.assertEqual(response.serialize_headers(), b'Content-Type: text/plain\\r\\n')\n", "    def test_charset(self):\n        response = HttpResponse(content_type='text/html; charset=utf-8')\n        self.assertEqual(response.charset, 'utf-8')\n\n        response = HttpResponse()\n        self.assertEqual(response.charset, 'utf-8')  # default\n\n        response = HttpResponse(content_type='text/plain')\n        self.assertEqual(response.charset, 'utf-8')  # default\n\n        response = HttpResponse(content_type='application/octet-stream')\n        self.assertIsNone(response.charset)  # no charset for binary content type\n"], "sample_221": ["def test_pickle_with_additional_select(self):\n    groups = Group.objects.annotate(extra=models.Value('extra')).values('id', 'name', 'extra')\n    self.assert_pickles(groups)\n", "def test_pickle_queryset_with_deferred_fields(self):\n    event = Event.objects.create(title='Event 1', group=Group.objects.create(name='Group 1'))\n    qs = Event.objects.defer('title')\n    self.assert_pickles(qs)\n    reloaded_qs = pickle.loads(pickle.dumps(qs))\n    self.assertEqual(reloaded_qs.get().id, event.id)\n    with self.assertRaises(Event.DoesNotExist):\n        reloaded_qs.get(title='Event 1')\n", "def test_pickle_query_with_aggregate(self):\n    # Test pickling a query with an aggregate.\n    qs = Happening.objects.aggregate(Max('when'))\n    self.assert_pickles(qs)\n", "def test_filtering_with_aliased_annotation(self):\n    # Test that filtering on an aliased annotation doesn't raise an error.\n    qs = Happening.objects.alias(latest_time=models.Max('when')).filter(latest_time__isnull=False)\n    self.assert_pickles(qs)\n", "def test_pickle_queryset_with_nested_exists(self):\n    group = Group.objects.create(name='group')\n    Event.objects.create(title='event', group=group)\n    groups = Group.objects.annotate(\n        has_event=models.Exists(\n            Event.objects.filter(group_id=models.OuterRef('id')).annotate(\n                has_happening=models.Exists(\n                    Happening.objects.filter(event_id=models.OuterRef('id')),\n                ),\n            ),\n        ),\n    )\n    self.assert_pickles(groups)\n"], "sample_222": ["    def test_lock_exclusive(self):\n        with tempfile.NamedTemporaryFile() as temp_file:\n            self.assertIs(locks.lock(temp_file, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(temp_file), True)\n", "    def test_lock_flags(self):\n        if os.name == 'nt':\n            self.assertEqual(locks.LOCK_EX, 0x2)\n            self.assertEqual(locks.LOCK_SH, 0)\n            self.assertEqual(locks.LOCK_NB, 0x1)\n        else:\n            try:\n                import fcntl\n                self.assertEqual(locks.LOCK_EX, fcntl.LOCK_EX)\n                self.assertEqual(locks.LOCK_SH, fcntl.LOCK_SH)\n                self.assertEqual(locks.LOCK_NB, fcntl.LOCK_NB)\n            except ImportError:\n                pass\n", "    def test_lock_unlock(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path) as f:\n            self.assertIs(locks.lock(f, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(f), True)\n", "    def test_lock_unlock(self):\n        with tempfile.TemporaryFile() as f:\n            self.assertIs(locks.lock(f, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(f), True)\n", "    def test_lock_unlock(self):\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path, 'w') as f:\n            self.assertIs(locks.lock(f, locks.LOCK_EX), True)\n            self.assertIs(locks.unlock(f), True)\n"], "sample_223": ["    def test_ticket_25037(self):\n        \"\"\"\n        When combining querysets using 'or', the subqueries' aliases should be\n        relabeled to avoid collisions.\n        \"\"\"\n        Author.objects.create(name='Author1')\n        qs1 = Author.objects.filter(name='Author1')\n        qs2 = Author.objects.filter(name='Author2')\n        qs = qs1 | qs2\n        sql = str(qs.query)\n        self.assertEqual(sql.count('FROM'), 3)\n        self.assertEqual(sql.count('WHERE'), 2)\n", "    def test_ticket_24933(self):\n        \"\"\"\n        Ensure that the ORM correctly handles filtering on a field when it's\n        referenced in the SELECT clause with an alias.\n        \"\"\"\n        Tag.objects.create(name='tag1')\n        Tag.objects.create(name='tag2')\n\n        qs = Tag.objects.annotate(alias=F('name')).filter(name='tag1')\n        self.assertEqual(str(qs.query).count('JOIN'), 0)\n        self.assertSequenceEqual(qs, [Tag.objects.get(name='tag1')])\n", "    def test_ticket_24948(self):\n        \"\"\"\n        Filtering by an alias of an aggregate function should be allowed.\n        \"\"\"\n        note = Note.objects.create(note='n1', misc='foo')\n        Annotation.objects.create(note=note, name='a1')\n        Annotation.objects.create(note=note, name='a2')\n        qs = Note.objects.annotate(\n            annotations_count=Count('annotation')).filter(annotations_count__gt=1)\n        self.assertSequenceEqual(qs, [note])\n", "    def test_ticket_24874(self):\n        qs = Tag.objects.values('name').annotate(c=Count('id')).values('c')\n        self.assertEqual(list(qs), [{'c': 5}])\n", "    def test_ticket_24713(self):\n        # Test that filtering on a related field with a subquery doesn't crash\n        # when the subquery has an annotation with the same name as a model field.\n        school = School.objects.create()\n        student1 = Student.objects.create(school=school)\n        student2 = Student.objects.create(school=school)\n        Classroom.objects.create(school=school, students=[student1])\n        qs = Student.objects.filter(\n            classroom__in=Classroom.objects.annotate(school=F('school')).filter(school=school)\n        )\n        self.assertSequenceEqual(qs, [student1])\n"], "sample_224": ["def test_aggregation_subquery_annotation_with_filter(self):\n    \"\"\"Subquery annotations are excluded from the GROUP BY if they are\n    filtered against.\"\"\"\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n        count=Count('book'),\n    ).filter(latest_book_pubdate__gte=datetime.date(2008, 1, 1))\n    with self.assertNumQueries(1) as ctx:\n        list(publisher_qs)\n    self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n    # The GROUP BY should not be by alias either.\n    self.assertEqual(ctx[0]['sql'].lower().count('latest_book_pubdate'), 1)\n", "def test_alias(self):\n    authors = Author.objects.alias(\n        num_books=Count('book'),\n        num_friends=Count('friends'),\n    ).annotate(\n        total=Sum(F('num_books') + F('num_friends')),\n    ).order_by('total')\n    self.assertEqual(authors.count(), 9)\n", "def test_aggregate_over_case(self):\n    msg = \"Cannot compute Avg('age'): 'age' is an aggregate\"\n    with self.assertRaisesMessage(FieldError, msg):\n        Author.objects.annotate(\n            age_alias=Case(When(age__gt=30, then=F('age')), default=Value(0))\n        ).aggregate(\n            avg_age=Avg(F('age')),\n        )\n\n    author = Author.objects.annotate(\n        age_alias=Case(When(age__gt=30, then=F('age')), default=Value(0))\n    ).annotate(\n        sum_age=Sum(F('age_alias'))\n    ).get(name=\"Adrian Holovaty\")\n\n    other_author = Author.objects.annotate(\n        sum_age=Sum(Case(When(age__gt=30, then=F('age')), default=Value(0)))\n    ).get(name=\"Adrian Holovaty\")\n\n    self.assertEqual(author.sum_age, other_author.sum_age)\n", "def test_aggregation_with_duration_field(self):\n    vals = Publisher.objects.aggregate(Sum('duration'))\n    self.assertEqual(vals['duration__sum'], datetime.timedelta(days=3))\n", "def test_aggregation_subquery_annotation_collision(self):\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publisher_qs = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n        book_count=Count('book'),\n    ).annotate(\n        count=Count('book'),\n    )\n    with self.assertNumQueries(1) as ctx:\n        list(publisher_qs)\n    self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n    # The GROUP BY should not be by alias either.\n    self.assertEqual(ctx[0]['sql'].lower().count('latest_book_pubdate'), 1)\n"], "sample_225": ["    def setUp(self):\n        self.site = admin.AdminSite()\n        self.request_factory = RequestFactory()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUp(self):\n        self.site = admin.AdminSite(name=\"test_adminsite\")\n        self.request_factory = RequestFactory()\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n"], "sample_226": ["    def test_get_test_db_clone_settings(self):\n        test_connection = get_connection_copy()\n        test_database_name = 'test_hodor'\n        test_connection.settings_dict['NAME'] = test_database_name\n        creation = BaseDatabaseCreation(test_connection)\n        clone_settings = creation.get_test_db_clone_settings(suffix='1')\n        self.assertEqual(clone_settings['NAME'], f'{test_database_name}_1')\n", "    def test_create_test_db_with_migrate_false(self):\n        # Ensure create_test_db() doesn't run migrations when MIGRATE=False.\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = False\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_create_test_db'):\n            with mock.patch('django.db.migrations.executor.MigrationExecutor.migrate') as migrate:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n                migrate.assert_not_called()\n", "    def test_clone_test_db(self):\n        # Create a test database and clone it.\n        connection.creation.create_test_db(verbosity=0, autoclobber=True)\n        test_database_name = connection.settings_dict['NAME']\n        clone_suffix = 'clone'\n        clone_settings = connection.creation.get_test_db_clone_settings(clone_suffix)\n        connection.creation.clone_test_db(clone_suffix, verbosity=0)\n\n        # Check that the cloned database has the same tables as the original test database.\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n            original_tables = [row[0] for row in cursor.fetchall()]\n        clone_connection = connections['default']\n        clone_connection.settings_dict = clone_settings\n        with clone_connection.cursor() as cursor:\n            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n            clone_tables = [row[0] for row in cursor.fetchall()]\n        self.assertEqual(original_tables, clone_tables)\n\n        # Clean up.\n        connection.creation.destroy_test_db(test_database_name, verbosity=0)\n        clone_connection.creation.destroy_test_db(clone_settings['NAME'], verbosity=0)\n", "    def test_get_test_db_clone_settings(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        suffix = 'clone_1'\n        clone_settings = creation.get_test_db_clone_settings(suffix)\n        self.assertEqual(clone_settings['NAME'], f\"{test_connection.settings_dict['NAME']}_{suffix}\")\n        self.assertEqual(clone_settings['ENGINE'], test_connection.settings_dict['ENGINE'])\n        self.assertEqual(clone_settings['HOST'], test_connection.settings_dict['HOST'])\n        self.assertEqual(clone_settings['PORT'], test_connection.settings_dict['PORT'])\n", "    def test_keepdb(self):\n        # create_test_db() preserves the database when keepdb is True.\n        connection.creation.create_test_db(verbosity=0, autoclobber=True, keepdb=True)\n        test_db_name = connection.settings_dict['NAME']\n        connection.creation.destroy_test_db(old_database_name=None, verbosity=0, keepdb=True)\n        self.assertEqual(connection.settings_dict['NAME'], test_db_name)\n"], "sample_227": ["def test_emptylistfieldfilter_tuple(self):\n    class EmployeeAdminWithEmptyFieldListFilter(ModelAdmin):\n        list_filter = (\n            ('department', EmptyFieldListFilter),\n        )\n\n    modeladmin = EmployeeAdminWithEmptyFieldListFilter(Employee, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n\n    # Make sure the correct queryset is returned\n    queryset = modeladmin.get_changelist_instance(request).get_queryset(request)\n    self.assertEqual(list(queryset), [self.jack, self.john])\n\n    filterspec = modeladmin.get_changelist_instance(request).get_filters(request)[0][0]\n    self.assertEqual(filterspec.title, 'department')\n    choices = list(filterspec.choices(modeladmin.get_changelist_instance(request)))\n    self.assertEqual(len(choices), 3)\n\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], True)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    self.assertEqual(choices[1]['display'], 'Empty')\n    self.assertIs(choices[1]['selected'], False)\n    self.assertEqual(choices[1]['query_string'], '?department__isempty=1')\n\n    self.assertEqual(choices[2]['display'], 'Not empty')\n    self.assertIs(choices[2]['selected'], False)\n    self.assertEqual(choices[2]['query_string'], '?department__isempty=0')\n", "def test_genericrelation_with_custom_queryset(self):\n    django_bookmark = Bookmark.objects.create(url='https://www.djangoproject.com/')\n    python_bookmark = Bookmark.objects.create(url='https://www.python.org/')\n    kernel_bookmark = Bookmark.objects.create(url='https://www.kernel.org/')\n\n    TaggedItem.objects.create(content_object=django_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=python_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=kernel_bookmark, tag='linux')\n\n    class CustomBookmarkAdmin(ModelAdmin):\n            return super().get_queryset(request).filter(url__contains='django')\n\n    modeladmin = CustomBookmarkAdmin(Bookmark, site)\n    request = self.request_factory.get('/', {'tags__tag': 'python'})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n\n    expected = [django_bookmark]\n    self.assertEqual(list(queryset), expected)\n", "def test_emptylistfieldfilter_errors(self):\n    class EmployeeAdminWithEmptyFieldListFilter(ModelAdmin):\n        list_filter = [('department', EmptyFieldListFilter)]\n\n    modeladmin = EmployeeAdminWithEmptyFieldListFilter(Employee, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n\n    # Test field that doesn't allow nulls and empty strings.\n    Employee._meta.get_field('department').null = False\n    Employee._meta.get_field('department').blank = False\n    with self.assertRaises(ImproperlyConfigured):\n        modeladmin.get_changelist_instance(request)\n\n    # Test field that doesn't exist in the model.\n    class EmployeeAdminWithNonExistentField(ModelAdmin):\n        list_filter = [('non_existent_field', EmptyFieldListFilter)]\n\n    modeladmin = EmployeeAdminWithNonExistentField(Employee, site)\n    with self.assertRaises(FieldDoesNotExist):\n        modeladmin.get_changelist_instance(request)\n", "def test_genericrelation_list_filter(self):\n    class TaggedItemAdmin(ModelAdmin):\n        list_filter = ('content_type',)\n\n    modeladmin = TaggedItemAdmin(TaggedItem, site)\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    filterspec = changelist.get_filters(request)[0][0]\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(choices[0]['display'], 'All')\n    self.assertIs(choices[0]['selected'], True)\n    self.assertEqual(choices[0]['query_string'], '?')\n\n    # Make sure the correct choice is selected\n    request = self.request_factory.get('/', {'content_type__exact': ContentType.objects.get_for_model(Book).pk})\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    filterspec = changelist.get_filters(request)[0][0]\n    choices = list(filterspec.choices(changelist))\n    self.assertEqual(choices[1]['display'], 'book')\n    self.assertIs(choices[1]['selected'], True)\n    self.assertEqual(choices[1]['query_string'], '?content_type__exact=%s' % ContentType.objects.get_for_model(Book).pk)\n", "def test_genericrelation_prefetch(self):\n    django_bookmark = Bookmark.objects.create(url='https://www.djangoproject.com/')\n    python_bookmark = Bookmark.objects.create(url='https://www.python.org/')\n\n    TaggedItem.objects.create(content_object=django_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=python_bookmark, tag='python')\n    TaggedItem.objects.create(content_object=django_bookmark, tag='django')\n\n    modeladmin = BookmarkAdminGenericRelation(Bookmark, site)\n\n    request = self.request_factory.get('/')\n    request.user = self.alfred\n    changelist = modeladmin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n\n    # Make sure the correct number of queries are executed.\n    with self.assertNumQueries(2):\n        list(queryset)\n\n    # Make sure the correct results are returned.\n    expected = [\n        (django_bookmark, ['python', 'django']),\n        (python_bookmark, ['python']),\n    ]\n    for obj, tags in expected:\n        with self.subTest(obj=obj):\n            self.assertEqual([tag.tag for tag in obj.tags.all()], tags)\n"], "sample_228": ["def test_baseformset_absolute_max(self):\n    # reduce the default limit of 1000 temporarily for testing\n    _old_DEFAULT_MAX_NUM = formsets.DEFAULT_MAX_NUM\n    try:\n        formsets.DEFAULT_MAX_NUM = 1\n        ChoiceFormSet = formset_factory(Choice, absolute_max=4)\n        formset = ChoiceFormSet({\n            'choices-TOTAL_FORMS': '4',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',  # min number of forms\n            'choices-MAX_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n            'choices-2-choice': 'Two',\n            'choices-2-votes': '2',\n            'choices-3-choice': 'Three',\n            'choices-3-votes': '3',\n        }, prefix='choices')\n        self.assertEqual(len(formset.forms), 4)\n    finally:\n        formsets.DEFAULT_MAX_NUM = _old_DEFAULT_MAX_NUM\n", "def test_formset_non_field_errors(self):\n    \"\"\"A formset's non-field errors are correctly rendered.\"\"\"\n    class NonFieldErrorFormSet(BaseFormSet):\n            raise ValidationError('This is a non-field error')\n\n    NonFieldErrorForm = formset_factory(FavoriteDrinkForm, formset=NonFieldErrorFormSet)\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-0-name': 'Test',\n    }\n    formset = NonFieldErrorForm(data)\n    self.assertFalse(formset.is_valid())\n    self.assertHTMLEqual(\n        formset.non_form_errors(),\n        '<ul class=\"errorlist\"><li>This is a non-field error</li></ul>'\n    )\n", "def test_formset_empty_error_class(self):\n    \"\"\"A formset's error class is used for empty forms.\"\"\"\n    class CustomErrorList(ErrorList):\n        pass\n\n    formset = FavoriteDrinksFormSet(error_class=CustomErrorList)\n    self.assertEqual(formset.empty_form.error_class, CustomErrorList)\n", "def test_formset_non_field_errors(self):\n    \"\"\"A FormSet's non-form errors are reported.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    class BaseCustomFormSet(BaseFormSet):\n            raise ValidationError(\"This is a non-form error\")\n    ChoiceFormSet = formset_factory(Choice, formset=BaseCustomFormSet)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['This is a non-form error'])\n", "def test_formset_with_disabled_fields(self):\n    class DisabledForm(Form):\n        field = IntegerField(disabled=True)\n\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-0-field': '100',\n    }\n    DisabledFormSet = formset_factory(DisabledForm)\n    formset = DisabledFormSet(data, prefix='form')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(formset.cleaned_data, [{'field': 100}])\n    self.assertHTMLEqual(\n        str(formset),\n        '<input type=\"hidden\" name=\"form-TOTAL_FORMS\" value=\"1\">'\n        '<input type=\"hidden\" name=\"form-INITIAL_FORMS\" value=\"0\">'\n        '<input type=\"hidden\" name=\"form-MIN_NUM_FORMS\" value=\"0\">'\n        '<tr><th>Field:</th><td>'\n        '<input type=\"number\" name=\"form-0-field\" value=\"100\" disabled></td></tr>'\n    )\n"], "sample_229": ["def test_union_with_unsupported_values(self):\n    qs1 = Number.objects.all().values('num', 'other_num')\n    qs2 = Number.objects.all().values('num')\n    msg = 'Cannot combine queries with different shapes.'\n    with self.assertRaisesMessage(ValueError, msg):\n        list(qs1.union(qs2))\n", "def test_union_with_annotation(self):\n    qs1 = Number.objects.annotate(other_num_null=Value(None, IntegerField()))\n    qs2 = Number.objects.annotate(other_num_null=F('other_num'))\n    self.assertNumbersEqual(qs1.union(qs2).order_by('num'), list(range(10)))\n    self.assertQuerysetEqual(\n        qs1.union(qs2).values_list('num', 'other_num_null').order_by('num'),\n        [(i, None if i < 5 else 10 - i) for i in range(10)],\n    )\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num=1)\n    qs2 = Number.objects.none()\n    self.assertEqual(list(qs1.union(qs2)), list(qs1))\n", "def test_union_with_f_expression_and_alias(self):\n    qs1 = Number.objects.filter(num__lte=1).values(alias=F('num'))\n    qs2 = Number.objects.filter(num__gte=2, num__lte=3).values(alias=F('num'))\n    self.assertQuerysetEqual(\n        qs1.union(qs2).order_by('-alias'),\n        [3, 2, 1, 0],\n        operator.itemgetter('alias'),\n    )\n", "def test_union_with_queryset_chain(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__gte=8)\n    chained_qs = qs1.union(qs2).filter(num__lte=8)\n    self.assertNumbersEqual(chained_qs, [0, 1, 8], ordered=False)\n"], "sample_230": ["def test_bound_data(self):\n    field = JSONField(required=False)\n    tests = [\n        ('', None),\n        (None, None),\n        ('{\"a\": \"b\"}', {\"a\": \"b\"}),\n        ('invalid json', 'invalid json'),\n    ]\n    for input_value, expected_output in tests:\n        with self.subTest(input_value=input_value):\n            self.assertEqual(field.bound_data(input_value, None), expected_output)\n", "def test_bound_data(self):\n    field = JSONField(required=False)\n    self.assertIsNone(field.bound_data(None, None))\n    self.assertEqual(field.bound_data('{\"a\": 1}', '{\"b\": 2}'), {\"a\": 1})\n    invalid_input = '{\"a\" 1}'\n    self.assertIsInstance(field.bound_data(invalid_input, '{\"b\": 2}'), str)\n    self.assertEqual(field.bound_data(invalid_input, '{\"b\": 2}'), invalid_input)\n", "def test_invalid_input(self):\n    field = JSONField()\n    invalid_inputs = [\n        '{some badly formed: json}',\n        '[\"foo\", \"bar\",]',\n        '{\"key\": \"value\",}',\n        '{\"key\": \"value\" \"key2\": \"value2\"}',\n        'just some text',\n    ]\n    for invalid_input in invalid_inputs:\n        with self.subTest(invalid_input=invalid_input):\n            with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n                field.clean(invalid_input)\n", "def test_invalid_json_input(self):\n    field = JSONField()\n    invalid_json_input = InvalidJSONInput('{\"a\": \"b\"')\n    self.assertEqual(field.prepare_value(invalid_json_input), '{\"a\": \"b\"}')\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n        field.clean(invalid_json_input)\n", "def test_invalid_input(self):\n    field = JSONField()\n    invalid_inputs = [\n        '{some badly formed: json}',\n        '{\"a\": \"b\" \"c\": \"d\"}',\n        '[1, 2, 3,]',\n        '{\"key\": }',\n    ]\n    for invalid_input in invalid_inputs:\n        with self.subTest(invalid_input=invalid_input):\n            with self.assertRaises(ValidationError):\n                field.clean(invalid_input)\n"], "sample_231": ["    def test_cleanup(self):\n        get_default_exception_reporter_filter.cache_clear()\n        # Populate the cache.\n        get_default_exception_reporter_filter()\n        # Change the setting.\n        with override_settings(DEFAULT_EXCEPTION_REPORTER_FILTER='view_tests.tests.AnotherCustomExceptionReporterFilter'):\n            # Make sure the cache is cleaned up when the setting changes.\n            self.assertIsInstance(\n                get_default_exception_reporter_filter(),\n                AnotherCustomExceptionReporterFilter,\n            )\n", "    def test_template_does_not_exist(self):\n        with self.assertLogs('django.request', 'ERROR'):\n            response = self.client.get(reverse('raises_template_does_not_exist', kwargs={\"path\": \"nonexistent.html\"}))\n        self.assertContains(response, '<div class=\"context\" id=\"', status_code=500)\n        self.assertContains(\n            response,\n            \"No template engine was able to render the template 'nonexistent.html'. \"\n            \"The following template engines were tried:\",\n            status_code=500,\n        )\n        self.assertContains(response, \"django.template.loaders.app_directories.Loader\", status_code=500)\n        self.assertContains(\n            response,\n            \"django.template.loaders.base.Loader\",\n            count=1,\n            status_code=500,\n        )\n", "    def test_is_active(self):\n        filter = SafeExceptionReporterFilter()\n        request = RequestFactory().get('/')\n        with override_settings(DEBUG=True):\n            self.assertFalse(filter.is_active(request))\n        with override_settings(DEBUG=False):\n            self.assertTrue(filter.is_active(request))\n", "    def test_get_traceback_frame_variables(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().get('/')\n        frame = sys._getframe()\n        variables = reporter_filter.get_traceback_frame_variables(request, frame)\n        self.assertIn(('reporter_filter', '<SafeExceptionReporterFilter>'), variables)\n        self.assertIn(('request', '<WSGIRequest>'), variables)\n        self.assertIn(('frame', '<frame>'), variables)\n", "    def test_request_get_items(self):\n        request = self.rf.get('/test_view/?items=Oops')\n        reporter = ExceptionReporter(request, None, None, None)\n        data = reporter.get_traceback_data()\n        self.assertIn('request_GET_items', data)\n        self.assertEqual(data['request_GET_items'], [('items', 'Oops')])\n"], "sample_232": ["def test_key_transform_on_none_value(self):\n    obj = NullableJSONModel.objects.create(value=None)\n    tests = [\n        ('value__has_key', 'a'),\n        ('value__has_keys', ['a', 'c']),\n        ('value__has_any_keys', ['a', 'x']),\n        ('value__contains', {'a': 'b'}),\n        ('value__contained_by', {'a': 'b'}),\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup):\n            self.assertIs(NullableJSONModel.objects.filter(**{lookup: value}).exists(), False)\n", "    def test_json_field_check_constraints(self):\n        # Create a model with a JSONField that has a check constraint.\n        class JSONModelWithConstraint(models.Model):\n            value = models.JSONField(default=dict)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(value__has_key='required_key'),\n                        name='value_has_required_key',\n                    ),\n                ]\n\n        # Try to create an instance of the model without the required key.\n        with self.assertRaises(IntegrityError):\n            JSONModelWithConstraint.objects.create(value={})\n\n        # Create an instance of the model with the required key.\n        obj = JSONModelWithConstraint.objects.create(value={'required_key': 'value'})\n        self.assertEqual(obj.value, {'required_key': 'value'})\n", "def test_key_transform_text_lookup_mixin_with_key_transform_text(self):\n    transform = KeyTextTransform('test', 'value')\n    mixin = KeyTransformTextLookupMixin(transform)\n    self.assertIsInstance(mixin, KeyTransformTextLookupMixin)\n", "    def test_custom_decoder(self):\n        constraint = models.CheckConstraint(\n            check=models.Q(value_custom__foo='bar'),\n            name='custom_decoder_check',\n        )\n        with self.assertRaisesMessage(NotSupportedError, 'Custom JSON decoder'):\n            with connection.schema_editor() as schema_editor:\n                schema_editor.add_constraint(JSONModel, constraint)\n", "    def test_db_check_constraints(self):\n        field = models.JSONField()\n        errors = field.check()\n        self.assertEqual(errors, [])\n"], "sample_233": ["def test_token_with_invalid_timestamp(self):\n    \"\"\"A token with an invalid timestamp is rejected.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    ts_b36, _ = tk1.split(\"-\")\n    # Try to use a token with a timestamp that's not a base36-encoded integer.\n    with self.assertRaises(ValueError):\n        base36_to_int('invalid' + ts_b36[1:])\n    # Create a token with an invalid timestamp.\n    invalid_tk = 'invalid' + tk1[1:]\n    self.assertIs(p0.check_token(user, invalid_tk), False)\n    # Create a token with a negative timestamp.\n    negative_ts = -self._num_seconds(datetime.now())\n    negative_tk = int_to_base36(negative_ts) + '-' + tk1.split('-')[1]\n    self.assertIs(p0.check_token(user, negative_tk), False)\n", "def test_token_with_last_login_change(self):\n    \"\"\"\n    Updating the user's last_login field invalidates the token.\n    \"\"\"\n    user = User.objects.create_user('lastloginuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now() + timedelta(days=1)\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_invalid_timestamp(self):\n    \"\"\"A token with an invalid timestamp is rejected.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    # Parse the token\n    ts_b36, _ = tk1.split(\"-\")\n    # Convert to base 10 and subtract 1\n    ts = base36_to_int(ts_b36) - 1\n    # Re-encode the invalid timestamp\n    invalid_ts_b36 = int_to_base36(ts)\n    # Reconstruct the token with the invalid timestamp\n    invalid_tk = f\"{invalid_ts_b36}-{tk1.split('-')[1]}\"\n    self.assertIs(p0.check_token(user, invalid_tk), False)\n", "def test_token_with_legacy_algorithm(self):\n    \"\"\"\n    A token generated with the legacy algorithm is still valid.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    # Create a new token generator that uses the legacy algorithm\n    p1 = PasswordResetTokenGenerator()\n    p1.algorithm = 'sha1'\n    tk2 = p1.make_token(user)\n    # The token generated with the legacy algorithm should be valid\n    self.assertIs(p0.check_token(user, tk2), True)\n    # The token generated with the default algorithm should also be valid\n    self.assertIs(p1.check_token(user, tk1), True)\n", "def test_token_with_legacy_hashing_algorithm(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    generator = PasswordResetTokenGenerator()\n    with self.settings(DEFAULT_HASHING_ALGORITHM='sha256'):\n        token = generator.make_token(user)\n    # Simulate a legacy token by changing the algorithm to sha1.\n    generator.algorithm = 'sha1'\n    legacy_token = generator._make_token_with_timestamp(user, self._num_seconds(datetime.now()), legacy=True)\n    self.assertIs(generator.check_token(user, token), True)\n    self.assertIs(generator.check_token(user, legacy_token), True)\n"], "sample_234": ["def test_union_with_subqueries(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__in=Number.objects.filter(num__gte=8).values('num'))\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 8, 9], ordered=False)\n", "def test_union_with_outerref(self):\n    subquery = Number.objects.filter(num=OuterRef('num')).values('num')\n    qs1 = Number.objects.annotate(has_num=Exists(subquery)).filter(has_num=True)\n    qs2 = Number.objects.filter(num=9)\n    self.assertCountEqual(qs1.union(qs2).values_list('num', flat=True), [1, 9])\n", "def test_union_with_extra(self):\n    qs1 = Number.objects.filter(num=1).extra(\n        select={'count': 0},\n    )\n    qs2 = Number.objects.filter(num=2).extra(select={'count': 1})\n    self.assertNumbersEqual(qs1.union(qs2), [1, 2], ordered=False)\n", "def test_union_with_subqueries(self):\n    qs1 = Number.objects.filter(num__lte=1)\n    qs2 = Number.objects.filter(num__gte=8)\n    subquery = Number.objects.filter(num=5).values_list('num')\n    self.assertNumbersEqual(qs1.union(qs2).filter(num__in=subquery), [5], ordered=False)\n", "def test_union_with_empty_values_list(self):\n    qs1 = Number.objects.values_list('num', flat=True)\n    qs2 = Number.objects.none().values_list('num', flat=True)\n    self.assertEqual(list(qs1.union(qs2)), list(range(10)))\n"], "sample_235": ["def test_on_commit_with_set_autocommit(self):\n    with transaction.atomic():\n        self.do(1)\n        self.assertNotified([])\n    connection.set_autocommit(False)\n    connection.set_autocommit(True)\n    self.assertDone([1])\n", "def test_on_commit_hook_registration_outside_transaction(self):\n    with self.assertRaises(TransactionManagementError):\n        transaction.on_commit(lambda: self.notify(1))\n    with transaction.atomic():\n        pass  # ensure a connection is established\n    with self.assertRaises(TransactionManagementError):\n        with transaction.get_connection().cursor() as cursor:\n            cursor.execute('BEGIN')\n            transaction.on_commit(lambda: self.notify(1))\n", "def test_on_commit_hooks_run_even_if_autocommit_disabled_and_reenabled(self):\n    self.assertFalse(connection.get_autocommit())\n    connection.set_autocommit(False)\n    try:\n        with transaction.atomic():\n            self.do(1)\n        connection.set_autocommit(True)\n        self.assertDone([1])\n    finally:\n        connection.set_autocommit(False)\n", "def test_transaction_hooks_are_cleared_after_successful_commit_with_nested_transactions(self):\n    with transaction.atomic():\n        with transaction.atomic():\n            self.do(1)\n        self.do(2)\n\n    with transaction.atomic():\n        self.do(3)\n\n    self.assertDone([1, 2, 3])\n", "def test_run_and_clear_commit_hooks(self):\n        self.notify(1)\n\n        self.notify(2)\n\n    transaction.on_commit(hook1)\n    transaction.on_commit(hook2)\n\n    connection.run_and_clear_commit_hooks()\n\n    self.assertNotified([1, 2])\n    self.assertEqual(connection.run_on_commit, [])\n"], "sample_236": ["    def test_add_field_update(self):\n        collector = Collector(using='default')\n        model = A\n        field = model._meta.get_field('setnull')\n        value = None\n        objs = [model.objects.create(name='test')]\n        collector.add_field_update(field, value, objs)\n        self.assertIn(model, collector.field_updates)\n        self.assertIn((field, value), collector.field_updates[model])\n        self.assertEqual(collector.field_updates[model][(field, value)], set(objs))\n", "def test_can_fast_delete_proxy_model(self):\n    \"\"\"\n    can_fast_delete() should return False for a proxy model instance, even if\n    the concrete model it's based on could be fast deleted.\n    \"\"\"\n    user = User.objects.create()\n    collector = Collector(using='default')\n    self.assertTrue(collector.can_fast_delete(user))\n    proxy_user = HiddenUser.objects.get(pk=user.pk)\n    self.assertFalse(collector.can_fast_delete(proxy_user))\n", "def test_collector_clear(self):\n    collector = Collector(using='default')\n    model = R\n    instances = [model.objects.create() for _ in range(3)]\n    collector.data[model] = set(instances)\n    collector.clear_restricted_objects_from_set(model, instances[:2])\n    self.assertEqual(len(collector.restricted_objects[model]), 0)\n    qs = model.objects.filter(pk__in=[instance.pk for instance in instances])\n    collector.clear_restricted_objects_from_queryset(model, qs)\n    self.assertEqual(len(collector.restricted_objects[model]), 0)\n", "def test_delete_with_keeping_parents_m2m(self):\n    child = RChild.objects.create()\n    parent_id = child.r_ptr_id\n    m = M.objects.create()\n    MR.objects.create(r=child.r_ptr, m=m)\n    child.delete(keep_parents=True)\n    self.assertFalse(RChild.objects.filter(id=child.id).exists())\n    self.assertTrue(R.objects.filter(id=parent_id).exists())\n    self.assertFalse(MR.objects.filter(r=child.r_ptr).exists())\n", "def test_collector_add_dependency(self):\n    collector = Collector(using='default')\n    model = A\n    dependency = R\n    collector.add_dependency(model, dependency)\n    self.assertIn(dependency._meta.concrete_model, collector.dependencies[model._meta.concrete_model])\n    self.assertIn(model._meta.concrete_model, collector.data)\n"], "sample_237": ["def test_model_permission_codename_max_length_with_unique_constraint(self):\n    class Checked(models.Model):\n        field = models.CharField(max_length=10)\n\n        class Meta:\n            constraints = [\n                UniqueConstraint(fields=['field'], name='checked_field_uniqueness')\n            ]\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"The name of model 'auth_tests.Checked' must be at most 90 \"\n            \"characters for its builtin permission codenames to be at most 100 characters.\",\n            obj=Checked,\n            id='auth.E011',\n        ),\n    ])\n", "def test_username_unique_with_model_meta(self):\n    class CustomUserUniqueUsernameWithMeta(AbstractBaseUser):\n        username = models.CharField(max_length=30)\n        USERNAME_FIELD = 'username'\n\n        class Meta:\n            constraints = [\n                UniqueConstraint(fields=['username'], name='username_unique_on_meta'),\n            ]\n\n    self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n    with self.settings(AUTHENTICATION_BACKENDS=['my.custom.backend']):\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "def test_username_non_unique_with_model_constraint(self):\n    class CustomUserNonUniqueUsernameWithUniqueConstraint(AbstractBaseUser):\n        username = models.CharField(max_length=30)\n        USERNAME_FIELD = 'username'\n\n        class Meta:\n            constraints = [\n                UniqueConstraint(fields=['username'], name='username_unique', condition=Q(is_active=True)),\n            ]\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [\n        checks.Error(\n            \"'CustomUserNonUniqueUsernameWithUniqueConstraint.username' must be unique because \"\n            \"it is named as the 'USERNAME_FIELD'.\",\n            obj=CustomUserNonUniqueUsernameWithUniqueConstraint,\n            id='auth.E003',\n        ),\n    ])\n    with self.settings(AUTHENTICATION_BACKENDS=['my.custom.backend']):\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Warning(\n                \"'CustomUserNonUniqueUsernameWithUniqueConstraint.username' is named as the \"\n                \"'USERNAME_FIELD', but it is not unique.\",\n                hint=(\n                    'Ensure that your authentication backend(s) can '\n                    'handle non-unique usernames.'\n                ),\n                obj=CustomUserNonUniqueUsernameWithUniqueConstraint,\n                id='auth.W004',\n            ),\n        ])\n", "def test_is_anonymous_authenticated_attributes(self):\n    \"\"\"\n    <User Model>.is_anonymous/is_authenticated should not raise an error if they\n    are attributes or properties.\n    \"\"\"\n    class GoodUser(AbstractBaseUser):\n        username = models.CharField(max_length=30, unique=True)\n        USERNAME_FIELD = 'username'\n\n        @property\n            return False\n\n        @property\n            return True\n\n    errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n", "def test_permission_name_max_length_with_non_ascii_characters(self):\n    class Checked(models.Model):\n        class Meta:\n            verbose_name = '\u00e9 \u00e0'\n\n    Permission = self.apps.get_model('auth', 'Permission')\n    permission_name_max_length = Permission._meta.get_field('name').max_length\n\n    # The model's verbose name contains non-ASCII characters, so the maximum\n    # length of its permission names should be reduced.\n    max_builtin_permission_name_length = (\n        max(len(name) for name in _get_builtin_permissions(Checked._meta).values())\n        if _get_builtin_permissions(Checked._meta) else 0\n    )\n    verbose_name_max_length = (\n        permission_name_max_length - (max_builtin_permission_name_length - len(Checked._meta.verbose_name_raw))\n    )\n\n    # Ensure that the calculated verbose name max length takes into account the\n    # encoding of the non-ASCII characters.\n    self.assertGreater(verbose_name_max_length, 0)\n\n    errors = checks.run_checks(self.apps.get_app_configs())\n    self.assertEqual(errors, [])\n"], "sample_239": ["def test_formset_empty_management_form_error(self):\n    \"\"\"Empty management form raises a ValidationError.\"\"\"\n    msg = (\n        'ManagementForm data is missing or has been tampered with. '\n        'Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS'\n    )\n    with self.assertRaisesMessage(ValidationError, msg):\n        self.make_choiceformset(data={})\n", "def test_absolute_max_with_max_num_equal(self):\n    data = {\n        'form-TOTAL_FORMS': '1001',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '0',\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        max_num=1000,\n        absolute_max=1000,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 1000)\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at most 1000 forms.'],\n    )\n", "def test_absolute_max_higher_than_max_num(self):\n    \"\"\"absolute_max can be higher than max_num.\"\"\"\n    data = {\n        'form-TOTAL_FORMS': '5',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '0',\n        'form-MAX_NUM_FORMS': '5',\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        max_num=4,\n        absolute_max=5,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data=data)\n    self.assertIs(formset.is_valid(), True)\n    self.assertEqual(len(formset.forms), 5)\n", "def test_formset_empty_management_form_error(self):\n    \"\"\"An empty management form should raise an error.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['ManagementForm data is missing or has been tampered with. Missing fields: choices-TOTAL_FORMS.']\n    )\n", "def test_formset_media(self):\n    \"\"\"Media is available on formset.\"\"\"\n    class MediaForm(Form):\n        class Media:\n            js = ('some-file.js',)\n    self.assertIn('some-file.js', str(formset_factory(MediaForm)().media))\n    self.assertIn('some-file.js', str(formset_factory(MediaForm, extra=2)().media))\n    self.assertIn('some-file.js', str(formset_factory(MediaForm, extra=0)().media))\n"], "sample_240": ["def test_token_with_last_login_change(self):\n    \"\"\"\n    Updating the user last login date invalidates the token.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now() + timedelta(days=1)\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_last_login(self):\n    \"\"\"Updating the user's last login date invalidates the token.\"\"\"\n    user = User.objects.create_user('lastloginuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now() + timedelta(seconds=1)\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_algorithm(self):\n    \"\"\"\n    A valid token can be created with an algorithm other than the default by\n    using the PasswordResetTokenGenerator.algorithm attribute.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    new_algorithm = 'md5'\n    # Create and check a token with a different algorithm.\n    p0 = PasswordResetTokenGenerator()\n    p0.algorithm = new_algorithm\n    tk0 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk0), True)\n    # Create and check a token with the default algorithm.\n    p1 = PasswordResetTokenGenerator()\n    self.assertEqual(p1.algorithm, settings.DEFAULT_HASHING_ALGORITHM)\n    self.assertNotEqual(p1.algorithm, new_algorithm)\n    tk1 = p1.make_token(user)\n    # Tokens created with a different algorithm don't validate.\n    self.assertIs(p0.check_token(user, tk1), False)\n    self.assertIs(p1.check_token(user, tk0), False)\n", "def test_token_with_legacy_algorithm(self):\n    \"\"\"\n    A valid token can be created with the legacy algorithm (sha1) and is\n    correctly validated.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk0 = p0._make_token_with_timestamp(user, self._num_seconds(datetime.now()), legacy=True)\n    self.assertIs(p0.check_token(user, tk0), True)\n", "def test_token_with_different_last_login(self):\n    \"\"\"Updating the user last login invalidates the token.\"\"\"\n    user = User.objects.create_user('lastloginuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.last_login = datetime.now() + timedelta(days=1)\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_241": ["    def test_query_expression(self):\n        query = Company.objects.annotate(\n            foo=Value('bar', output_field=CharField())\n        )\n        self.assertEqual(query.get_queryset().first().foo, 'bar')\n", "    def setUpTestData(cls):\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n", "    def test_resolve_expression(self):\n        ref = Ref('alias', F('field'))\n        resolved = ref.resolve_expression(MockQuery(), allow_joins=True, reuse=None)\n        self.assertEqual(resolved, ref)\n", "    def test_subquery_with_annotate(self):\n        subquery = Company.objects.annotate(count=Count('ceo')).values('count')\n        Employee.objects.annotate(count=Subquery(subquery)).get()\n        self.assertNumQueries(2)\n", "    def test_group_by_cols_with_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.get_group_by_cols(alias='alias'), ['alias'])\n"], "sample_242": ["def test_year_lookup_bounds(self):\n    lookup = YearLookup(DateTimeField(), Value(2022))\n    with mock.patch('django.db.models.lookups.connection') as connection:\n        connection.ops.year_lookup_bounds_for_datetime_field.return_value = (datetime(2022, 1, 1), datetime(2022, 12, 31, 23, 59, 59, 999999))\n        start, finish = lookup.year_lookup_bounds(connection, 2022)\n        self.assertEqual(start, datetime(2022, 1, 1))\n        self.assertEqual(finish, datetime(2022, 12, 31, 23, 59, 59, 999999))\n", "def test_get_source_expressions(self):\n    lookup = Lookup(Value(1), Value(2))\n    self.assertEqual(lookup.get_source_expressions(), [lookup.lhs, lookup.rhs])\n    lookup = Lookup(Value(1), 2)\n    self.assertEqual(lookup.get_source_expressions(), [lookup.lhs])\n", "def test_year_lookup_bounds(self):\n    output_field = DateTimeField()\n    lookup = YearLookup(output_field, Value(2022))\n    bounds = lookup.year_lookup_bounds(None, 2022)\n    self.assertEqual(bounds, (datetime(2022, 1, 1), datetime(2022, 12, 31, 23, 59, 59)))\n", "def test_year_lookup_bounds(self):\n    output_field = DateTimeField()\n    year = 2022\n    lookup = YearLookup(output_field, Value(year))\n\n    with mock.patch.object(lookup, 'lhs', output_field):\n        bounds = lookup.year_lookup_bounds(None, year)\n        self.assertIsInstance(bounds, tuple)\n        self.assertEqual(len(bounds), 2)\n\n    output_field = DateTimeField()\n    year = 2022\n    lookup = YearLookup(output_field, Value(year))\n\n    with mock.patch.object(lookup, 'lhs', output_field):\n        bounds = lookup.year_lookup_bounds(None, year)\n        self.assertIsInstance(bounds, tuple)\n        self.assertEqual(len(bounds), 2)\n", "def test_year_lookup_bounds(self):\n    output_field = DateTimeField()\n    year = 2022\n\n    class Connection:\n        ops = mock.Mock()\n\n    connection = Connection()\n    connection.ops.year_lookup_bounds_for_datetime_field.return_value = (datetime(2022, 1, 1), datetime(2022, 12, 31, 23, 59, 59, 999999))\n\n    lookup = YearLookup(Value(1), Value(year))\n    lookup.lhs.output_field = output_field\n\n    bounds = lookup.year_lookup_bounds(connection, year)\n    self.assertEqual(bounds, (datetime(2022, 1, 1), datetime(2022, 12, 31, 23, 59, 59, 999999)))\n    connection.ops.year_lookup_bounds_for_datetime_field.assert_called_once_with(year)\n"], "sample_243": ["def test_add_distinct_fields(self):\n    query = Query(Item)\n    query.add_distinct_fields('name')\n    self.assertEqual(query.distinct_fields, ('name',))\n    self.assertTrue(query.distinct)\n", "def test_build_lookup_invalid_transform(self):\n    query = Query(Author)\n    msg = \"Unsupported lookup 'invalid' for CharField or join on the field not permitted.\"\n    with self.assertRaisesMessage(FieldError, msg):\n        query.build_lookup(['invalid'], Col(Author._meta.get_field('name'), 'author'), 'value')\n", "def test_build_filter_with_reuse(self):\n    query = Query(Author)\n    query.used_aliases = {'author'}\n    filter_expr = Q(num__gt=2)\n    where, needed_inner = query.build_filter(filter_expr, can_reuse={'author'})\n    self.assertEqual(where.connector, 'AND')\n    self.assertEqual(len(where.children), 1)\n    self.assertIn('author', needed_inner)\n", "def test_equality_with_annotations(self):\n    self.assertNotEqual(\n        Author.objects.annotate(bar=F('name')).query,\n        Author.objects.annotate(foo=F('name')).query,\n    )\n    self.assertEqual(\n        Author.objects.annotate(bar=F('name')).query,\n        Author.objects.annotate(bar=F('name')).query,\n    )\n    self.assertEqual(\n        Author.objects.annotate(bar=F('name')).filter(item__name='foo').query,\n        Author.objects.annotate(bar=F('name')).filter(item__name='foo').query,\n    )\n", "def test_build_where_with_filter_object(self):\n    query = Query(Item)\n    filter_obj = Q(name='foo') | ~Q(note__contains='bar')\n    where = query.build_where(filter_obj)\n    self.assertEqual(where.connector, OR)\n    name_exact = where.children[0]\n    self.assertIsInstance(name_exact, Exact)\n    self.assertEqual(name_exact.lhs.target, Item._meta.get_field('name'))\n    note_contains = where.children[1].children[0]\n    self.assertIsInstance(note_contains, Exact)\n    self.assertEqual(note_contains.lhs.func.__name__, 'Lower')\n    self.assertEqual(note_contains.rhs, 'bar')\n    note_isnull = where.children[1].children[1]\n    self.assertIsInstance(note_isnull, IsNull)\n    self.assertEqual(note_isnull.lhs.target, Item._meta.get_field('note'))\n"], "sample_244": ["def test_formset_absolute_max_with_empty_forms(self):\n    \"\"\"A formset's absolute_max is considered when checking empty forms.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=1001)\n    data = {\n        'choices-TOTAL_FORMS': '1001',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.forms), 1000)\n    self.assertEqual(\n        formset.non_form_errors(),\n        ['Please submit at most 1000 forms.'],\n    )\n", "def test_formset_absolute_max_can_be_overridden_by_inherited_formset(self):\n    \"\"\"Inherited formsets can override the absolute_max attribute.\"\"\"\n    class BaseFormSetWithAbsoluteMax(BaseFormSet):\n        absolute_max = 500\n\n    class InheritedFormSet(BaseFormSetWithAbsoluteMax):\n        absolute_max = 1000\n\n    self.assertEqual(InheritedFormSet.absolute_max, 1000)\n", "def test_management_form_required_fields(self):\n    \"\"\"ManagementForm requires TOTAL_FORMS and INITIAL_FORMS.\"\"\"\n    form = ManagementForm({\n        'TOTAL_FORMS': '1',\n    }, prefix='form')\n    self.assertFalse(form.is_valid())\n    self.assertEqual(\n        form.errors,\n        {\n            'INITIAL_FORMS': ['This field is required.'],\n        },\n    )\n    form = ManagementForm({\n        'INITIAL_FORMS': '1',\n    }, prefix='form')\n    self.assertFalse(form.is_valid())\n    self.assertEqual(\n        form.errors,\n        {\n            'TOTAL_FORMS': ['This field is required.'],\n        },\n    )\n", "def test_absolute_max_with_initial_data(self):\n    \"\"\"absolute_max can't be less than the number of initial forms.\"\"\"\n    initial = [\n        {'name': 'Gin Tonic'},\n        {'name': 'Bloody Mary'},\n        {'name': 'Jack and Coke'},\n    ]\n    LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, extra=1, absolute_max=2)\n    with self.assertRaisesMessage(ValueError, \"'absolute_max' must be greater or equal to the number of initial forms.\"):\n        LimitedFavoriteDrinkFormSet(initial=initial)\n", "def test_formset_is_multipart_if_any_form_has_file_field(self):\n    class FileForm(Form):\n        file = FileField()\n\n    class TextForm(Form):\n        text = CharField()\n\n    FileFormSet = formset_factory(FileForm)\n    TextFormSet = formset_factory(TextForm)\n\n    file_formset = FileFormSet()\n    text_formset = TextFormSet()\n\n    self.assertTrue(file_formset.is_multipart())\n    self.assertFalse(text_formset.is_multipart())\n\n    # A formset with a mix of forms is multipart if any form has a file field.\n    MixedFormSet = formset_factory(FileForm, extra=2)\n    mixed_formset = MixedFormSet(initial=[{'file': 'test.txt'}])\n    self.assertTrue(mixed_formset.is_multipart())\n\n    # If the extra form has no file field, it's not multipart.\n    MixedFormSet = formset_factory(TextForm, extra=1)\n    mixed_formset = MixedFormSet(initial=[{'text': 'test'}])\n    self.assertFalse(mixed_formset.is_multipart())\n"], "sample_245": ["    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n", "    def test_obsolete_messages(self):\n        # Create a .po file with an obsolete message.\n        po_file = os.path.join('locale', LOCALE, 'LC_MESSAGES', 'django.po')\n        os.makedirs(os.path.dirname(po_file), exist_ok=True)\n        with open(po_file, 'w') as fp:\n            fp.write('#: templates/test.html:1\\n')\n            fp.write('msgid \"Hello\"\\n')\n            fp.write('msgstr \"Hallo\"\\n')\n            fp.write('\\n')\n            fp.write('#: templates/test.html:2\\n')\n            fp.write('msgid \"Goodbye\"\\n')\n            fp.write('msgstr \"Auf Wiedersehen\"\\n')\n\n        # Run makemessages with the --no-obsolete option.\n        management.call_command('makemessages', locale=[LOCALE], no_obsolete=True, verbosity=0)\n\n        # Check that the obsolete message is removed from the .po file.\n        with open(po_file) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('msgid \"Goodbye\"', po_contents)\n", "    def test_custom_extensions(self):\n        management.call_command('makemessages', locale=[LOCALE], extensions=['txt', 'py'], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId('This is a string in a .txt file', po_contents)\n            self.assertMsgId('This is a string in a .py file', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        \"\"\"Behavior is correct if --no-obsolete switch is specified.\"\"\"\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"Obsolete message\"', po_contents)\n"], "sample_246": ["    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n", "    def test_no_obsolete_enabled(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0, no_obsolete=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn('#~ msgid \"\"', po_contents)\n", "    def test_obsolete(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertIn('#~ msgid \"Obsolete string\"', po_contents)\n", "    def test_no_obsolete_disabled_by_default(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertIn('#~ msgid \"Obsolete message\"', po_contents)\n"], "sample_247": ["def test_alias_with_m2m(self):\n    books = Book.objects.alias(\n        author_age=F('authors__age'),\n    ).filter(pk=self.b1.pk).order_by('author_age')\n    self.assertIs(hasattr(books.first(), 'author_age'), False)\n    self.assertEqual(books[0].authors.all()[0].age, 34)\n    self.assertEqual(books[1].authors.all()[0].age, 35)\n", "def test_alias_annotation_in_f_grouped_by_annotation(self):\n    qs = (\n        Publisher.objects.alias(multiplier=Value(3))\n        .annotate(multiplied_value_sum=Sum(F('multiplier') * F('num_awards')))\n        .values('name')\n        .annotate(avg_multiplied_value=Avg('multiplied_value_sum'))\n        .order_by()\n    )\n    self.assertCountEqual(\n        qs, [\n            {'avg_multiplied_value': 9.0, 'name': 'Apress'},\n            {'avg_multiplied_value': 0.0, 'name': \"Jonno's House of Books\"},\n            {'avg_multiplied_value': 27.0, 'name': 'Morgan Kaufmann'},\n            {'avg_multiplied_value': 21.0, 'name': 'Prentice Hall'},\n            {'avg_multiplied_value': 3.0, 'name': 'Sams'},\n        ]\n    )\n", "def test_alias_with_m2m(self):\n    qs = Author.objects.alias(\n        book_name=F('book__name'),\n    ).annotate(book_name_alias=F('book_name')).values_list('book_name_alias', flat=True)\n    self.assertIs(hasattr(qs.first(), 'book_name'), False)\n    self.assertCountEqual(qs, [\n        'The Definitive Guide to Django: Web Development Done Right',\n        'Sams Teach Yourself Django in 24 Hours',\n        'Practical Django Projects',\n        'Python Web Development with Django',\n        'Artificial Intelligence: A Modern Approach',\n        'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp'\n    ])\n", "def test_alias_referencing_m2m_through_field(self):\n    qs = Author.objects.alias(\n        book_through_id=F('book__id'),\n    ).annotate(book_id=F('book_through_id'))\n    self.assertIs(hasattr(qs.first(), 'book_through_id'), False)\n    for author in qs:\n        with self.subTest(author=author):\n            if author.book_set.exists():\n                self.assertEqual(author.book_id, author.book_set.first().id)\n            else:\n                self.assertIsNone(author.book_id)\n", "def test_subquery_alias(self):\n    subquery = Book.objects.filter(pages__gt=400).values('publisher')\n    qs = Publisher.objects.alias(\n        has_long_books=Exists(subquery.filter(publisher=OuterRef('pk'))),\n    ).filter(has_long_books=True)\n    self.assertIs(hasattr(qs.first(), 'has_long_books'), False)\n    self.assertEqual(qs.count(), 2)\n"], "sample_248": ["def test_shell_with_no_backend_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n", "def test_all_shells_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n", "def test_shell_with_bpython_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n        call_command('shell', interface='bpython')\n", "def test_shell_with_all_shells_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n", "def test_shell_with_bpython_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n        call_command('shell', interface='bpython')\n"], "sample_249": ["    def test_default_prefix(self):\n        # A test db name isn't set, use the default prefix.\n        prod_name = 'hodor'\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = prod_name\n        test_connection.settings_dict['TEST'] = {'NAME': None}\n        db_name = BaseDatabaseCreation(test_connection)._get_test_db_name()\n        self.assertEqual(db_name, TEST_DATABASE_PREFIX + prod_name)\n", "    def test_clone_test_db(self):\n        # Create a test database.\n        connection.creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        old_database_name = connection.settings_dict['NAME']\n        try:\n            # Clone the test database.\n            connection.creation.clone_test_db(suffix='clone', verbosity=0, autoclobber=True, keepdb=False)\n            # Check if the cloned database has the same tables as the original database.\n            with connection.cursor() as cursor:\n                cursor.execute('SHOW TABLES')\n                tables = [row[0] for row in cursor.fetchall()]\n                self.assertIn('backends_object', tables)\n                self.assertIn('backends_objectreference', tables)\n        finally:\n            # Destroy the cloned database.\n            connection.creation.destroy_test_db(old_database_name, verbosity=0, suffix='clone')\n            # Restore the original database name.\n            connection.settings_dict['NAME'] = old_database_name\n", "    def test_create_test_db_with_keepdb(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with mock.patch.object(creation, '_create_test_db'):\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            self.assertEqual(test_connection.settings_dict['NAME'], creation._get_test_db_name())\n", "    def test_create_test_db_keepdb(self):\n        # Ensure that the keepdb option works as expected.\n        connection.creation.create_test_db(keepdb=True)\n        test_database_name = connection.settings_dict['NAME']\n        self.assertTrue(test_database_name.startswith(TEST_DATABASE_PREFIX))\n        connection.close()\n        connection.creation.destroy_test_db(test_database_name, keepdb=True)\n        # If the database existed prior to the test and was not destroyed,\n        # ensure it can be reused on subsequent test runs.\n        with mock.patch.object(connection.creation, '_create_test_db') as mocked:\n            connection.creation.create_test_db(keepdb=True)\n            mocked.assert_not_called()\n", "    def test_create_test_db_keepdb(self):\n        # When keepdb is True, the test database is not deleted.\n        creation = connection.creation\n        old_database_name = connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            self.assertTrue(connection.database_exists())\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0, keepdb=False)\n"], "sample_250": ["def test_S_format(self):\n    # Test English ordinal suffix for the day of the month\n    dt = datetime(2009, 5, 1)\n    self.assertEqual(dateformat.format(dt, 'S'), 'st')\n\n    dt = datetime(2009, 5, 2)\n    self.assertEqual(dateformat.format(dt, 'S'), 'nd')\n\n    dt = datetime(2009, 5, 3)\n    self.assertEqual(dateformat.format(dt, 'S'), 'rd')\n\n    dt = datetime(2009, 5, 4)\n    self.assertEqual(dateformat.format(dt, 'S'), 'th')\n\n    dt = datetime(2009, 5, 11)\n    self.assertEqual(dateformat.format(dt, 'S'), 'th')\n\n    dt = datetime(2009, 5, 12)\n    self.assertEqual(dateformat.format(dt, 'S'), 'th')\n\n    dt = datetime(2009, 5, 13)\n    self.assertEqual(dateformat.format(dt, 'S'), 'th')\n", "def test_E_format(self):\n    # Test alternative month names as required by some locales.\n    dt = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(dt, 'E'), MONTHS_ALT[dt.month])\n", "def test_escaped_format_chars(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    self.assertEqual(dateformat.format(my_birthday, r'\\a \\A'), 'a A')\n    self.assertEqual(dateformat.format(my_birthday, r'\\d \\D'), 'd D')\n    self.assertEqual(dateformat.format(my_birthday, r'\\m \\M'), 'm M')\n    self.assertEqual(dateformat.format(my_birthday, r'\\y \\Y'), 'y Y')\n", "def test_weekday(self):\n    dt = datetime(2022, 7, 25)\n    self.assertEqual(dateformat.format(dt, 'l'), 'Monday')\n    self.assertEqual(dateformat.format(dt, 'D'), 'Mon')\n", "def test_E_format(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    # Test alternative month names as required by some locales.\n    self.assertEqual(dateformat.format(my_birthday, 'E'), MONTHS_ALT[7])\n"], "sample_251": ["def test_alias_with_m2m(self):\n    books = Book.objects.alias(author_age=F('authors__age')).filter(pk=self.b1.pk).order_by('author_age')\n    self.assertEqual(books[0].author_age, 34)\n    self.assertEqual(books[1].author_age, 35)\n", "def test_filter_alias_with_subquery(self):\n    long_books_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n    ).values('publisher').annotate(count=Count('pk')).values('count')\n    qs = Publisher.objects.alias(\n        total_books_alias=Subquery(long_books_qs, output_field=IntegerField()),\n    ).filter(total_books_alias=1)\n    self.assertIs(hasattr(qs.first(), 'total_books_alias'), False)\n    self.assertEqual(qs.count(), 2)\n", "def test_filter_alias_with_m2m(self):\n    qs = Author.objects.alias(\n        friends_age=F('friends__age'),\n    ).filter(friends_age=35)\n    self.assertIs(hasattr(qs.first(), 'friends_age'), False)\n    self.assertSequenceEqual(qs, [self.a1, self.a7])\n", "def test_resolve_combined_type(self):\n    combined = CombinedExpression(F('id'), Combinable.ADD, F('id'))\n    self.assertEqual(combined._resolve_output_field().__class__, fields.IntegerField)\n\n    combined = CombinedExpression(F('id'), Combinable.ADD, Value(1))\n    self.assertEqual(combined._resolve_output_field().__class__, fields.IntegerField)\n\n    combined = CombinedExpression(Value(1), Combinable.ADD, F('id'))\n    self.assertEqual(combined._resolve_output_field().__class__, fields.IntegerField)\n\n    combined = CombinedExpression(F('price'), Combinable.ADD, F('price'))\n    self.assertEqual(combined._resolve_output_field().__class__, fields.DecimalField)\n", "def test_values_list_alias(self):\n    qs = Book.objects.alias(\n        other_rating=F('rating'),\n    ).values_list('other_rating', flat=True)\n    msg = \"Cannot resolve keyword 'other_rating' into field.\"\n    with self.assertRaisesMessage(FieldError, msg):\n        list(qs)\n"], "sample_252": ["def test_custom_decoder(self):\n    value = '{\"a\": \"b\"}'\n    model_field = models.JSONField(decoder=CustomJSONDecoder)\n    form_field = model_field.formfield()\n    self.assertIs(form_field.decoder, CustomJSONDecoder)\n    form_field.clean(value)\n    with mock.patch.object(CustomJSONDecoder, 'decode') as decode_mock:\n        form_field.clean(value)\n        decode_mock.assert_called_once_with(value)\n", "def test_key_transform_lt(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__c__lt=15),\n        [self.objs[3], self.objs[4]],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__p__lt=5),\n        [self.objs[4]],\n    )\n", "    def test_supported(self):\n        field = models.JSONField()\n        errors = field.check(databases=['default'])\n        self.assertEqual(errors, [])\n", "def test_key_transform_on_json_field_with_custom_decoder(self):\n    # Make sure KeyTransform works with a custom decoder on the JSONField.\n    obj = NullableJSONModel.objects.create(value_custom={'foo': 'bar'})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value_custom__foo='bar'),\n        [obj],\n    )\n", "def test_key_transform_raw_expression_with_params(self):\n    expr = RawSQL(self.raw_sql, ['{\"x\": %s}'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr), params=['bar']),\n        [self.objs[7]],\n    )\n"], "sample_253": ["    def test_not_a_tty(self, mocked_isatty):\n        with mock.patch('termios.tcgetattr') as tcgetattr:\n            autoreload.ensure_echo_on()\n        self.assertFalse(tcgetattr.called)\n", "    def test_ensure_echo_on_calls_tcsetattr(self, mocked_termios):\n        # If stdin is a tty, ensure_echo_on calls tcsetattr.\n        with mock.patch.object(sys.stdin, 'isatty', return_value=True):\n            autoreload.ensure_echo_on()\n            self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_notify_file_changed(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.utils.autoreload.file_changed.send') as mocked_send:\n            with mock.patch('django.utils.autoreload.trigger_reload') as mocked_trigger:\n                mocked_send.return_value = [(None, False)]\n                reloader.notify_file_changed('/path/to/file')\n                self.assertTrue(mocked_trigger.called)\n                mocked_send.return_value = [(None, True)]\n                reloader.notify_file_changed('/path/to/file')\n                self.assertFalse(mocked_trigger.called)\n", "    def test_ensure_echo_on_calls_tcsetattr(self, mocked_termios):\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_not_a_tty(self, mocked_isatty):\n        autoreload.ensure_echo_on()\n        self.assertFalse(mocked_isatty.called)\n"], "sample_254": ["def test_inline_model_admin_has_change_permission(self):\n    # Make sure InlineModelAdmin.has_change_permission falls back to its\n    # ModelAdmin when the user doesn't have change permission on the inline model.\n    class NoChangePermissionInline(TabularInline):\n        model = Inner\n\n    class MyModelAdmin(ModelAdmin):\n        inlines = [NoChangePermissionInline]\n\n            return True\n\n    model_admin = MyModelAdmin(Holder, admin_site)\n    request = self.factory.get(reverse('admin:admin_inlines_holder_add'))\n    request.user = User(username='super', is_superuser=True)\n\n    # Simulate the user doesn't have change permission on the inline model.\n    with mock.patch.object(model_admin.inlines[0], 'has_change_permission', return_value=False):\n        self.assertTrue(model_admin.inlines[0].has_change_permission(request))\n", "    def test_missing_model_attribute(self):\n        class MyModelAdmin(ModelAdmin):\n            inlines = [StandaloneModelAdmin]\n\n        checks = ModelAdminChecks(MyModelAdmin, model=Author, obj=None)\n        errors = checks.check_inlines()\n        expected = [\n            Error(\n                \"The value of 'inlines' must be a list or tuple.\",\n                hint=\"Check for missing commas.\",\n                obj=MyModelAdmin,\n                id='admin.E101',\n            ),\n            Error(\n                \"Cannot resolve keyword 'model' into field. Choices are: author_ptr, \"\n                \"author_ptr_id, id, name, nonautopkbook_set, nonautopkbookchild_set\",\n                hint=None,\n                obj=MyModelAdmin,\n                id='admin.E108',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_missing_model(self):\n        class MyInline(TabularInline):\n            pass\n\n        model_admin = ModelAdmin(ProfileCollection, admin_site)\n        model_admin.inlines = [MyInline]\n\n        checks = model_admin.check()\n        self.assertEqual(len(checks), 1)\n        self.assertEqual(checks[0].id, 'admin.E103')\n        self.assertEqual(checks[0].msg, 'The value of \\'model\\' must be a Model.')\n", "    def setUp(self):\n        self.user = User.objects.create_user('testing', password='password', is_staff=True)\n        self.user.user_permissions.add(\n            Permission.objects.get(codename='change_teacher', content_type=ContentType.objects.get_for_model(Teacher))\n        )\n        self.user.user_permissions.add(\n            Permission.objects.get(codename='add_child', content_type=ContentType.objects.get_for_model(Child))\n        )\n", "def test_inline_formset_with_readonly_fields(self):\n    self.admin_login(username='super', password='secret')\n    self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n    fields = ['id_inner5stacked_set-0-dummy', 'id_inner5tabular_set-0-dummy']\n    show_links = self.selenium.find_elements_by_link_text('SHOW')\n    for show_index, field_name in enumerate(fields):\n        show_links[show_index].click()\n        self.wait_until_visible('#' + field_name)\n        self.selenium.find_element_by_id(field_name).send_keys(1)\n\n    # Make sure readonly fields are not editable\n    readonly_fields = [\n        'id_inner5stacked_set-0-readonly',\n        'id_inner5tabular_set-0-readonly',\n    ]\n    for field_name in readonly_fields:\n        with self.assertRaises(WebDriverException):\n            self.selenium.find_element_by_id(field_name).send_keys(1)\n\n    self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n\n    # Test the readonly fields are displayed as text\n    for field_name in readonly_fields:\n        element = self.selenium.find_element_by_id(field_name)\n        self.assertEqual(element.tag_name, 'p')\n"], "sample_256": ["    def test_html_autocomplete_attributes(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(user)\n        tests = (\n            ('password1', 'new-password'),\n            ('password2', 'new-password'),\n        )\n        for field_name, autocomplete in tests:\n            with self.subTest(field_name=field_name, autocomplete=autocomplete):\n                self.assertEqual(form.fields[field_name].widget.attrs['autocomplete'], autocomplete)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        self.assertHTMLEqual(\n            widget.render('name', value, {'id': 'id_password'}),\n            \"\"\"\n            <div id=\"id_password\">\n                %s\n            </div>\n            \"\"\" % _(\"No password set.\")\n        )\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + 'somevalue'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n"], "sample_255": ["def test_handle_one_request(self):\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n    # Simulate a valid request\n    handler.raw_requestline = b'GET / HTTP/1.1\\r\\n'\n    handler.rfile = BytesIO(b'Host: example.com\\r\\n\\r\\n')\n\n    with captured_stderr() as stderr:\n        handler.handle_one_request()\n\n    self.assertIn('HTTP/1.1\" 200', stderr.getvalue())\n", "def test_handle(self):\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n    # Mock the handle_one_request method to raise an exception\n        raise Exception('Test exception')\n\n    handler.handle_one_request = mock_handle_one_request\n\n    # Test that the handle method catches the exception and logs it\n    with captured_stderr() as stderr:\n        handler.handle()\n\n    self.assertIn('Exception occurred processing WSGI request.', stderr.getvalue())\n    self.assertIn('Test exception', stderr.getvalue())\n", "def test_get_environ(self):\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n    # Set headers with underscores\n    handler.headers['Some_Header'] = 'bad'\n    handler.headers['Other_Header'] = 'bad'\n\n    # get_environ should strip these headers\n    environ = handler.get_environ()\n    self.assertNotIn('HTTP_SOME_HEADER', environ)\n    self.assertNotIn('HTTP_OTHER_HEADER', environ)\n", "def test_handle_one_request(self):\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n    handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n    with captured_stderr() as stderr:\n        handler.handle_one_request()\n\n    self.assertIn('GET / HTTP/1.1\" 200', stderr.getvalue())\n", "def test_get_environ(self):\n    \"\"\"WSGIRequestHandler.get_environ() returns environ without headers with underscores.\"\"\"\n    request = WSGIRequest(self.request_factory.get('/').environ)\n    request.makefile = lambda *args, **kwargs: BytesIO()\n\n    handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n    # Add some headers to the request\n    handler.headers['Some-Header'] = 'good'\n    handler.headers['Some_Header'] = 'bad'\n\n    # Get the environ\n    environ = handler.get_environ()\n\n    # Check that the header with an underscore is not in the environ\n    self.assertNotIn('HTTP_SOME_HEADER', environ)\n\n    # Check that the header without an underscore is in the environ\n    self.assertIn('HTTP_SOME_HEADER', environ)\n    self.assertEqual(environ['HTTP_SOME-HEADER'], 'good')\n"], "sample_257": ["def test_key_transform_with_invalid_operator(self):\n    with self.assertRaisesMessage(ValueError, 'Invalid operator'):\n        KeyTransform('test', 'value').as_postgresql(\n            compiler=models.Model.objects.get_compiler(connection=connection),\n            connection=connection,\n            template='%(function)s',\n        )\n", "def test_key_transform_invalid_input(self):\n    tests = [\n        ('value__foo__', 'bar'),  # Double underscore.\n        ('value__foo__bar__baz', 'x'),  # Too many parts.\n        ('value__', 'x'),  # No key.\n    ]\n    for lookup, value in tests:\n        with self.subTest(lookup=lookup):\n            msg = 'No lookup type registered'\n            with self.assertRaisesMessage(FieldError, msg):\n                NullableJSONModel.objects.filter(**{lookup: value})\n", "    def test_m2m_field(self):\n        related_obj = RelatedJSONModel.objects.create(value={'foo': 'bar'})\n        obj = JSONModel.objects.create(value={'baz': 'qux'}, related=related_obj)\n        self.assertEqual(obj.related.value, {'foo': 'bar'})\n", "    def test_modelform_with_custom_encoder(self):\n        class JSONModelForm(forms.ModelForm):\n            class Meta:\n                model = JSONModel\n                fields = '__all__'\n\n        form = JSONModelForm()\n        self.assertIsInstance(form.fields['value'], forms.JSONField)\n        self.assertIsNone(form.fields['value'].encoder)\n\n        class CustomJSONEncoder(DjangoJSONEncoder):\n            pass\n\n        model_field = JSONModel._meta.get_field('value')\n        model_field.encoder = CustomJSONEncoder\n\n        form = JSONModelForm()\n        self.assertIsInstance(form.fields['value'].encoder, CustomJSONEncoder)\n", "def test_key_transform_with_invalid_key(self):\n    obj = NullableJSONModel.objects.create(value={'a': 'b'})\n    tests = [\n        ('value__1', \"Key '1' is not valid for this JSONField\"),\n        ('value__c', \"Key 'c' is not valid for this JSONField\"),\n        ('value__d__0', \"Key 'd' is not valid for this JSONField\"),\n        ('value__d__1__f', \"Key 'd' is not valid for this JSONField\"),\n    ]\n    for lookup, message in tests:\n        with self.subTest(lookup=lookup):\n            with self.assertRaisesMessage(ValueError, message):\n                NullableJSONModel.objects.get(**{lookup: 'x'})\n            with self.assertRaisesMessage(ValueError, message):\n                obj.refresh_from_db()\n                getattr(obj, lookup)\n"], "sample_258": ["def test_receiver_disconnect(self):\n    @receiver(a_signal)\n        self.state = val\n\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n\n    a_signal.disconnect(f)\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertFalse(self.state)\n", "def test_receiver_weak_references(self):\n    @receiver(a_signal)\n        pass\n\n    # Delete the strong reference to the receiver function\n    del f\n\n    # Force garbage collection to occur immediately\n    garbage_collect()\n\n    # Since we used a weak reference, the receiver should be gone now\n    self.assertFalse(a_signal.has_listeners())\n", "def test_receiver_single_signal_with_sender(self):\n    @receiver(a_signal, sender=self.__class__)\n        self.state = val\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n    a_signal.send(sender=object(), val=False)\n    self.assertTrue(self.state)  # Should not be changed\n", "def test_receiver_signal_list_with_sender(self):\n    @receiver([a_signal, b_signal, c_signal], sender=self.__class__)\n        self.state.append(val)\n\n    self.state = []\n    a_signal.send(sender=self, val=True)\n    b_signal.send(sender=self, val=True)\n    c_signal.send(sender=self, val=True)\n    a_signal.send(sender=object(), val=True)\n    self.assertEqual(self.state, [True, True, True])\n", "def test_receiver_single_signal_with_sender(self):\n    @receiver(a_signal, sender=self.__class__)\n        self.state = val\n    self.state = False\n    a_signal.send(sender=self, val=True)\n    self.assertTrue(self.state)\n    a_signal.send(sender=object(), val=False)\n    self.assertTrue(self.state)  # Should not be changed\n"], "sample_259": ["def test_prefetch_object_with_queryset(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects([book1], Prefetch('authors', queryset=Author.objects.filter(name__startswith='A')))\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.authors.all(), [self.author2])\n", "def test_prefetch_object_query(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects([book1], Prefetch('authors', queryset=Author.objects.filter(name='Charlotte')))\n\n    with self.assertNumQueries(0):\n        self.assertEqual(book1.authors.all().count(), 1)\n        self.assertEqual(book1.authors.get().name, 'Charlotte')\n", "def test_prefetch_object_query(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects([book1], Prefetch('authors', queryset=Author.objects.filter(name='Charlotte')))\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.authors.all(), [self.author1])\n", "def test_prefetch_object_with_queryset(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    with self.assertNumQueries(1):\n        prefetch_related_objects([book1], Prefetch('authors', queryset=Author.objects.filter(name='Charlotte')))\n\n    with self.assertNumQueries(0):\n        self.assertCountEqual(book1.authors.all(), [self.author1])\n", "def test_prefetch_object_queryset(self):\n    book1 = Book.objects.get(id=self.book1.id)\n    qs = Author.objects.filter(name='Charlotte')\n    with self.assertNumQueries(1):\n        prefetch_related_objects([book1], Prefetch('authors', queryset=qs))\n\n    with self.assertNumQueries(0):\n        self.assertEqual(book1.authors.all().count(), 1)\n        self.assertEqual(book1.authors.all()[0].name, 'Charlotte')\n"], "sample_260": ["def test_create_alter_model_managers(self):\n    \"\"\"\n    AlterModelManagers should optimize into CreateModel.\n    \"\"\"\n    managers = [('objects', EmptyManager())]\n    new_managers = [('new_objects', EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={'verbose_name': 'Foo'},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterModelManagers(name=\"Foo\", managers=new_managers),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n                options={'verbose_name': 'Foo'},\n                bases=(UnicodeModel,),\n                managers=new_managers,\n            ),\n        ],\n    )\n", "def test_add_remove_index(self):\n    \"\"\"\n    RemoveIndex should cancel AddIndex.\n    \"\"\"\n    index = models.Index(fields=[\"name\"], name=\"my_index\")\n    self.assertOptimizesTo(\n        [\n            migrations.AddIndex(\"Foo\", index),\n            migrations.RemoveIndex(\"Foo\", \"my_index\"),\n        ],\n        [],\n    )\n", "def test_optimize_through_alter_model_table(self):\n    \"\"\"\n    AlterModelTable operations should be optimized away if the model is later\n    deleted, or if they're followed by another AlterModelTable.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AlterModelTable(\"Foo\", \"myapp_foo\"),\n            migrations.DeleteModel(\"Foo\"),\n        ],\n        [],\n    )\n    self.assertOptimizesTo(\n        [\n            migrations.AlterModelTable(\"Foo\", \"myapp_foo\"),\n            migrations.AlterModelTable(\"Foo\", \"myapp_bar\"),\n        ],\n        [\n            migrations.AlterModelTable(\"Foo\", \"myapp_bar\"),\n        ],\n    )\n", "def test_create_alter_model_managers(self):\n    \"\"\"\n    AlterModelManagers should optimize into CreateModel.\n    \"\"\"\n    managers = [('objects', EmptyManager())]\n    new_managers = [('new_manager', EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={'verbose_name': 'Foo'},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterModelManagers(name=\"Foo\", managers=new_managers),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n                options={'verbose_name': 'Foo'},\n                bases=(UnicodeModel,),\n                managers=new_managers,\n            ),\n        ],\n    )\n", "def test_optimize_index_together(self):\n    \"\"\"\n    AlterIndexTogether should optimize into CreateModel.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AlterIndexTogether(\"Foo\", [[\"name\"]]),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))], options={'index_together': [['name']]}),\n        ],\n    )\n"], "sample_261": ["def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3Y6M4DT12H30M5S', timedelta(days=1369, hours=12, minutes=30, seconds=5)),\n        ('P23DT23H', timedelta(days=23, hours=23)),\n        ('PT0H0M0S', timedelta()),\n        ('P0D', timedelta()),\n        ('PT1H2M3.4S', timedelta(hours=1, minutes=2, seconds=3, milliseconds=400)),\n        ('-P3Y6M4DT12H30M5S', timedelta(days=-1369, hours=-12, minutes=-30, seconds=-5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P4DT12H30M5S', timedelta(days=4, hours=12, minutes=30, seconds=5)),\n        ('PT12H30M5S', timedelta(hours=12, minutes=30, seconds=5)),\n        ('P4D', timedelta(days=4)),\n        ('PT5S', timedelta(seconds=5)),\n        ('P0D', timedelta()),\n        ('-P4DT12H30M5S', timedelta(days=-4, hours=-12, minutes=-30, seconds=-5)),\n        ('-PT12H30M5S', timedelta(hours=-12, minutes=-30, seconds=-5)),\n        ('-P4D', timedelta(days=-4)),\n        ('-PT5S', timedelta(seconds=-5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('PT4H', timedelta(hours=4)),\n        ('PT5M', timedelta(minutes=5)),\n        ('PT6S', timedelta(seconds=6)),\n        ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n        ('P-1DT-2H-3M-4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n        ('P0D', timedelta(days=0)),\n        ('PT0S', timedelta(seconds=0)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_duration_iso8601(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('PT3H', timedelta(hours=3)),\n        ('PT3M', timedelta(minutes=3)),\n        ('PT3S', timedelta(seconds=3)),\n        ('P3DT3H', timedelta(days=3, hours=3)),\n        ('P3DT3H3M', timedelta(days=3, hours=3, minutes=3)),\n        ('P3DT3H3M3S', timedelta(days=3, hours=3, minutes=3, seconds=3)),\n        ('-P3D', timedelta(days=-3)),\n        ('-PT3H', timedelta(hours=-3)),\n        ('-PT3M', timedelta(minutes=-3)),\n        ('-PT3S', timedelta(seconds=-3)),\n        ('-P3DT3H', timedelta(days=-3, hours=-3)),\n        ('-P3DT3H3M', timedelta(days=-3, hours=-3, minutes=-3)),\n        ('-P3DT3H3M3S', timedelta(days=-3, hours=-3, minutes=-3, seconds=-3)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('P1DT12H', timedelta(days=1, hours=12)),\n        ('PT3H30M', timedelta(hours=3, minutes=30)),\n        ('PT3H30M5S', timedelta(hours=3, minutes=30, seconds=5)),\n        ('P-3D', timedelta(days=-3)),\n        ('-P3D', timedelta(days=-3)),\n        ('P0D', timedelta(days=0)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n"], "sample_262": ["def test_classproperty_getter_override(self):\n    class Foo:\n        foo_attr = 123\n\n        @classproperty\n            return cls.foo_attr\n\n    class Bar(Foo):\n        foo_attr = 456\n\n        @Foo.foo.getter\n            return cls.foo_attr + 1\n\n    self.assertEqual(Bar.foo, 457)\n", "def test_lazy_str_and_bytes(self):\n    lazy_str = lazy(lambda: 'Hello, World!', str)\n    lazy_bytes = lazy(lambda: b'Hello, World!', bytes)\n\n    self.assertEqual(str(lazy_str()), 'Hello, World!')\n    self.assertEqual(bytes(lazy_bytes()), b'Hello, World!')\n\n    # Test that calling str() or bytes() on a lazy object with the wrong type\n    # raises an error.\n    lazy_str = lazy(lambda: b'Hello, World!', bytes)\n    lazy_bytes = lazy(lambda: 'Hello, World!', str)\n\n    with self.assertRaises(TypeError):\n        str(lazy_bytes())\n    with self.assertRaises(TypeError):\n        bytes(lazy_str())\n", "def test_classproperty_override(self):\n    class Base:\n        base_attr = 123\n\n        @classproperty\n            return cls.base_attr\n\n    class Derived(Base):\n        base_attr = 456\n\n    self.assertEqual(Derived.foo, 456)\n", "def test_classproperty_getter_overrides_previous_definition(self):\n    class Foo:\n        @classproperty\n            return 123\n\n        @foo.getter\n            return 456\n\n    self.assertEqual(Foo.foo, 456)\n", "def test_classproperty_override(self):\n    class Base:\n        base_attr = 123\n\n        @classproperty\n            return cls.base_attr\n\n    class SubClass(Base):\n        base_attr = 456\n\n    self.assertEqual(Base.foo, 123)\n    self.assertEqual(SubClass.foo, 456)\n"], "sample_263": ["    def test_dumpdata_command_with_exclude_option(self):\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": '\n            '\"News Stories\"}}]',\n            exclude_list=['fixtures.Article']\n        )\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place on ESPN\", '\n            '\"pub_date\": \"2006-06-16T12:00:00\"}}, {\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": {\"headline\": '\n            '\"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]',\n            exclude_list=['fixtures.Category']\n        )\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[]',\n            exclude_list=['fixtures.Category', 'fixtures.Article']\n        )\n", "    def test_dumpdata_with_progress_bar(self):\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        new_io = StringIO()\n        new_io.isatty = lambda: True\n        management.call_command(\n            'dumpdata',\n            'fixtures',\n            stdout=new_io,\n            stderr=new_io,\n            verbosity=1,\n        )\n        command_output = new_io.getvalue()\n        self.assertIn('[', command_output)\n        self.assertIn(']', command_output)\n        self.assertIn('.', command_output)\n", "def test_dumpdata_with_database_option(self):\n    # Load fixture 1 into the default database\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    \n    # Dump the current contents of the default database as a JSON fixture\n    self._dumpdata_assert(\n        ['fixtures'],\n        '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": '\n        '\"News Stories\"}}, {\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place '\n        'on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, {\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": '\n        '{\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]',\n        database='default'\n    )\n\n    # Try dumping from a non-existent database\n    msg = \"No database found with alias 'nonexistent'.\"\n    with self.assertRaisesMessage(CommandError, msg):\n        self._dumpdata_assert(\n            ['fixtures'],\n            '',\n            database='nonexistent'\n        )\n", "def test_dumpdata_with_indent(self):\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    new_io = StringIO()\n    management.call_command(\n        'dumpdata',\n        'fixtures',\n        format='json',\n        indent=4,\n        stdout=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertIn('\"pk\": 1,\\n', command_output)\n    self.assertIn('    \"model\": \"fixtures.category\",\\n', command_output)\n    self.assertIn('    \"fields\": {\\n', command_output)\n    self.assertIn('        \"description\": \"Latest news stories\",\\n', command_output)\n", "def test_dumpdata_with_database_option(self):\n    management.call_command('loaddata', 'fixture1.json', verbosity=0)\n    new_io = StringIO()\n    management.call_command(\n        'dumpdata',\n        'fixtures',\n        database='default',\n        stdout=new_io,\n        stderr=new_io,\n    )\n    command_output = new_io.getvalue().strip()\n    self.assertJSONEqual(command_output, '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": \"News Stories\"}}, {\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, {\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]')\n"], "sample_264": ["def test_update_cookie(self):\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test setting the cookie with encoded data\n    messages = ['test', 'me']\n    encoded_data = storage._encode(messages)\n    storage._update_cookie(encoded_data, response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name].value, encoded_data)\n\n    # Test deleting the cookie when there's no encoded data\n    storage._update_cookie(None, response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n", "def test_update_cookie_empty_messages(self):\n    \"\"\"\n    If there are no messages, the cookie is deleted.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(response.cookies[storage.cookie_name]['max-age'], 0)\n", "def test_message_truncation(self):\n    \"\"\"\n    When adding a message that would exceed the maximum cookie size,\n    the message is truncated and the not_finished sentinel value is added.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Fill up the storage with messages until it's just below the max size\n    while stored_cookie_messages_count(storage, response) < 4:\n        storage.add(constants.INFO, 'test')\n        storage.update(response)\n\n    # Add one more message to push it over the limit\n    long_msg = 'a' * (CookieStorage.max_cookie_size - 54)\n    storage.add(constants.INFO, long_msg)\n    unstored_messages = storage.update(response)\n\n    # Check that the message was truncated and the sentinel value was added\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n    self.assertEqual(unstored_messages[0].message, long_msg[:msg_size])\n    self.assertEqual(storage._decode(response.cookies['messages'].value)[-1], CookieStorage.not_finished)\n", "def test_update_cookie_no_messages(self):\n    \"\"\"\n    If there are no messages, the cookie is deleted.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n", "def test_update_cookie(self):\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test setting the cookie with encoded data\n    messages = ['test', 'me']\n    encoded_data = storage._encode(messages)\n    storage._update_cookie(encoded_data, response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name].value, encoded_data)\n\n    # Test deleting the cookie when there's no data to store\n    storage._update_cookie(None, response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n"], "sample_265": ["def test_copy_exception(self):\n    exception = TemplateDoesNotExist('template.html')\n    new_exception = copy_exception(exception)\n    self.assertEqual(new_exception.args, exception.args)\n    self.assertEqual(new_exception.tried, exception.tried)\n    self.assertEqual(new_exception.backend, exception.backend)\n    self.assertEqual(new_exception.chain, exception.chain)\n    self.assertIsNone(new_exception.__traceback__)\n    self.assertIsNone(new_exception.__context__)\n    self.assertIsNone(new_exception.__cause__)\n", "def test_get_package_libraries(self):\n    # Test that get_package_libraries discovers libraries in subpackages.\n    package = import_module('django.templatetags')\n    libraries = list(get_package_libraries(package))\n    self.assertIn('django.templatetags.i18n', libraries)\n    self.assertIn('django.templatetags.static', libraries)\n\n    # Test that get_package_libraries ignores modules without a register attribute.\n    self.assertNotIn('django.templatetags.base', libraries)\n", "def test_get_templatetag_libraries_custom(self):\n    custom_libraries = {\n        'my_tags': 'path.to.my.templatetags.my_tags',\n    }\n    engine = DjangoTemplates({\n        'DIRS': [],\n        'APP_DIRS': False,\n        'NAME': 'django',\n        'OPTIONS': {\n            'libraries': custom_libraries,\n        },\n    })\n\n    # Custom libraries are added to the engine's libraries\n    self.assertEqual(engine.engine.libraries['my_tags'], 'path.to.my.templatetags.my_tags')\n\n    # Custom libraries do not override built-in libraries\n    self.assertEqual(engine.engine.libraries['static'], 'django.templatetags.static')\n", "def test_get_package_libraries_invalid_template_library(self):\n    pkg = import_module('django.templatetags')\n    with self.assertRaisesMessage(\n        InvalidTemplateLibrary,\n        \"ImportError raised when trying to load 'django.templatetags.invalid_tags'\"\n    ) as cm:\n        list(get_package_libraries(pkg))\n    self.assertIsInstance(cm.exception.__cause__, ImportError)\n", "def test_get_package_libraries(self):\n    # Test that get_package_libraries returns libraries from submodules.\n    package = import_module('django.templatetags')\n    libraries = list(get_package_libraries(package))\n    self.assertIn('django.templatetags.i18n', libraries)\n    self.assertIn('django.templatetags.static', libraries)\n\n    # Test that get_package_libraries raises InvalidTemplateLibrary on ImportError.\n    class MockModule:\n            self.__name__ = name\n\n            raise ImportError('Mock ImportError')\n\n    with self.assertRaisesMessage(\n        InvalidTemplateLibrary,\n        \"ImportError raised when trying to load 'mock.module'\"\n    ) as cm:\n        list(get_package_libraries(MockModule('mock')))\n    self.assertIsInstance(cm.exception.__cause__, ImportError)\n"], "sample_266": ["def test_detect_conflicts(self):\n    \"\"\"MigrationLoader.detect_conflicts() returns conflicting apps.\"\"\"\n    loader = MigrationLoader(connection)\n    # Introduce a conflict.\n    loader.graph.add_node(('migrations', '0002_second'), None)\n    loader.graph.add_node(('migrations', '0003_third'), None)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0002_second', '0003_third']})\n", "def test_collect_sql(self):\n    loader = MigrationLoader(connection)\n    recorder = MigrationRecorder(connection)\n    self.addCleanup(recorder.flush)\n\n    # Apply the first migration\n    recorder.record_applied('migrations', '0001_initial')\n\n    # Collect SQL for the second migration\n    plan = loader.collect_sql([('migrations', '0002_second'),])\n    self.assertEqual(len(plan), 2)\n    self.assertIn('CREATE TABLE', plan[0])\n    self.assertIn('ALTER TABLE', plan[1])\n", "def test_get_migration(self):\n    loader = MigrationLoader(connection)\n    migration = loader.get_migration('migrations', '0001_initial')\n    self.assertEqual(migration.name, '0001_initial')\n    with self.assertRaises(NodeNotFoundError):\n        loader.get_migration('migrations', 'nonexistent_migration')\n", "def test_detect_conflicts(self):\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {})\n    # Create a conflict by adding another migration with the same app label.\n    with override_settings(MIGRATION_MODULES={'migrations': 'migrations.test_migrations_conflict'}):\n        loader = MigrationLoader(connection)\n        conflicts = loader.detect_conflicts()\n        self.assertIn('migrations', conflicts)\n        self.assertEqual(len(conflicts['migrations']), 2)\n", "def test_detect_conflicts(self):\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {})\n    # Simulate a conflict by adding another leaf node for the same app.\n    loader.graph.add_node(('migrations', '0003_conflicting'), None)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0002_second', '0003_conflicting']})\n"], "sample_267": ["    def test_database_wrapper_close_reopens_connection_if_in_memory_db(self):\n        # Create a new in-memory database connection\n        settings_dict = {\n            'NAME': ':memory:',\n            'ENGINE': 'django.db.backends.sqlite3',\n        }\n        wrapper = DatabaseWrapper(settings_dict)\n        wrapper.ensure_connection()\n\n        # Close the connection and verify that it is reopened on next access\n        wrapper.close()\n        self.assertFalse(wrapper.connection.closed)\n", "    def test_is_in_memory_db(self):\n        db = DatabaseWrapper({\n            'NAME': ':memory:',\n        })\n        self.assertTrue(db.is_in_memory_db())\n\n        db = DatabaseWrapper({\n            'NAME': '/path/to/db',\n        })\n        self.assertFalse(db.is_in_memory_db())\n", "    def test_database_wrapper_close(self):\n        # Ensure the database connection is closed when the DatabaseWrapper is\n        # closed.\n        db_wrapper = connection.__class__(connection.settings_dict)\n        db_wrapper.connect()\n        self.assertFalse(db_wrapper.connection is None)\n        db_wrapper.close()\n        self.assertTrue(db_wrapper.connection is None)\n", "    def test_convert_query(self):\n        wrapper = SQLiteCursorWrapper(Database.Cursor(None))\n        query = \"SELECT %s FROM table\"\n        converted_query = wrapper.convert_query(query)\n        self.assertEqual(converted_query, \"SELECT ? FROM table\")\n", "    def test_convert_query(self):\n        query = \"SELECT * FROM table WHERE name = '%s'\"\n        wrapper = SQLiteCursorWrapper(None)\n        converted_query = wrapper.convert_query(query)\n        self.assertEqual(converted_query, \"SELECT * FROM table WHERE name = ?\")\n"], "sample_268": ["    def test_ensure_echo_on_when_termios_available(self, mocked_termios):\n        mocked_attr_list = [1, 2, 3, 4]\n        mocked_termios.tcgetattr.return_value = mocked_attr_list\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_trigger_reload(self, mock_exit):\n        autoreload.trigger_reload('/path/to/file.py')\n        self.assertEqual(mock_exit.call_count, 1)\n        self.assertEqual(mock_exit.call_args[0][0], 3)\n", "    def test_sets_echo_on_when_fd_is_a_tty(self, mocked_termios):\n        fd = mock.MagicMock()\n        fd.isatty.return_value = True\n        mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n        autoreload.ensure_echo_on(fd)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_file_changed_signal_sent(self):\n        reloader = autoreload.BaseReloader()\n        with mock.patch('django.dispatch.Signal.send') as mocked_send:\n            reloader.notify_file_changed('path/to/file.py')\n        self.assertEqual(mocked_send.call_count, 1)\n        self.assertEqual(mocked_send.call_args[0][0], reloader)\n        self.assertEqual(mocked_send.call_args[1]['file_path'], 'path/to/file.py')\n"], "sample_269": ["    def test_json_catalog(self):\n        response = self.client.get('/jsoni18n/')\n        data = json.loads(response.content.decode())\n        self.assertIn('catalog', data)\n        self.assertIn('formats', data)\n        self.assertEqual(data['formats']['TIME_INPUT_FORMATS'], ['%H:%M:%S', '%H:%M:%S.%f', '%H:%M'])\n        self.assertEqual(data['formats']['FIRST_DAY_OF_WEEK'], 0)\n        self.assertIn('plural', data)\n        self.assertEqual(data['catalog']['month name\\x04May'], 'Mai')\n", "def test_jsi18n_pluralidx(self):\n    \"\"\"\n    The jsi18n catalog should include a pluralidx function that returns the\n    correct plural form index for the current language.\n    \"\"\"\n    with override('ru'):\n        response = self.client.get('/jsi18n/')\n        self.assertContains(response, 'pluralidx: function(n) {')\n        self.assertContains(response, 'return (n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2);')\n\n    with override('fr'):\n        response = self.client.get('/jsi18n/')\n        self.assertContains(response, 'pluralidx: function(n) {')\n        self.assertContains(response, 'return (n != 1);')\n", "def test_get_formats_with_language_override(self):\n    \"\"\"\n    Test get_formats() with a language override.\n    \"\"\"\n    with override('fr'):\n        formats = get_formats()\n        self.assertEqual(formats['DECIMAL_SEPARATOR'], ',')\n        self.assertEqual(formats['THOUSAND_SEPARATOR'], '\u00a0')\n", "def test_i18n_context_gettext(self):\n    self.selenium.get(self.live_server_url + '/jsi18n_template/')\n\n    elem = self.selenium.find_element_by_id(\"gettext_ctx\")\n    self.assertEqual(elem.text, \"Kann\")\n    elem = self.selenium.find_element_by_id(\"ngettext_ctx_sing\")\n    self.assertEqual(elem.text, \"1 Resultat\")\n    elem = self.selenium.find_element_by_id(\"ngettext_ctx_plur\")\n    self.assertEqual(elem.text, \"455 Resultate\")\n    elem = self.selenium.find_element_by_id(\"pgettext_ctx\")\n    self.assertEqual(elem.text, \"Entfernen\")\n    elem = self.selenium.find_element_by_id(\"npgettext_ctx_sing\")\n    self.assertEqual(elem.text, \"1 Element\")\n    elem = self.selenium.find_element_by_id(\"npgettext_ctx_plur\")\n    self.assertEqual(elem.text, \"455 Elemente\")\n", "def test_get_formats_returns_all_format_strings(self):\n    formats = get_formats()\n    expected_formats = [\n        'DATE_FORMAT', 'DATETIME_FORMAT', 'TIME_FORMAT',\n        'YEAR_MONTH_FORMAT', 'MONTH_DAY_FORMAT', 'SHORT_DATE_FORMAT',\n        'SHORT_DATETIME_FORMAT', 'FIRST_DAY_OF_WEEK', 'DECIMAL_SEPARATOR',\n        'THOUSAND_SEPARATOR', 'NUMBER_GROUPING',\n        'DATE_INPUT_FORMATS', 'TIME_INPUT_FORMATS', 'DATETIME_INPUT_FORMATS'\n    ]\n    self.assertEqual(set(formats.keys()), set(expected_formats))\n"], "sample_270": ["    def test_unique_together_with_deferred_fields(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n        # Simulate deferred loading of field2.\n        model = Model(field1='value')\n        model._deferred = {'field2'}\n\n        self.assertEqual(model.validate_unique(), [\n            Error(\n                'Model cannot be validated with deferred fields (field2).',\n                obj=model,\n                id='models.E035',\n            ),\n        ])\n", "    def test_warning_on_auto_created_primary_key(self):\n        class Model(models.Model):\n            pass\n\n        self.assertEqual(Model.check(), [\n            Warning(\n                \"Auto-created primary key used when not defining a primary key type, \"\n                \"by default 'django.db.models.AutoField'.\",\n                hint=(\n                    \"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                    \"default_auto_field attribute on a per-app basis to point \"\n                    \"to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                ),\n                obj=Model,\n                id='models.W042',\n            ),\n        ])\n", "    def test_default_pk(self):\n        class Model(models.Model):\n            pass\n\n        self.assertEqual(Model.check(), [\n            Warning(\n                f\"Auto-created primary key used when not defining a primary key type, by default '{settings.DEFAULT_AUTO_FIELD}'.\",\n                hint=(\n                    f\"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                    f\"{Model._meta.app_config.__class__.__qualname__}.\"\n                    f\"default_auto_field attribute to point to a subclass \"\n                    f\"of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                ),\n                obj=Model,\n                id='models.W042',\n            ),\n        ])\n", "    def test_check_constraint(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(age__gte=18), name='age_gte_18'\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_default_pk(self):\n        class Model(models.Model):\n            pass\n\n        self.assertEqual(Model.check(), [\n            Warning(\n                f\"Auto-created primary key used when not defining a primary key type, by default '{settings.DEFAULT_AUTO_FIELD}'.\",\n                hint=(\n                    \"Configure the DEFAULT_AUTO_FIELD setting or the \"\n                    \"default_auto_field attribute on a custom AppConfig \"\n                    \"subclass to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n                ),\n                obj=Model,\n                id='models.W042',\n            ),\n        ])\n"], "sample_271": ["    def test_ensure_echo_on(self):\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            with mock.patch('termios') as mocked_termios:\n                mocked_termios.tcgetattr.return_value = [0, 0, 0, 0]\n                autoreload.ensure_echo_on()\n                self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n                self.assertEqual(mocked_termios.tcsetattr.call_args[0][1], termios.TCSANOW)\n                self.assertEqual(mocked_termios.tcsetattr.call_args[0][2][3], termios.ECHO)\n", "    def test_sets_echo_on(self, mocked_termios):\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n        self.assertSequenceEqual(\n            mocked_termios.tcsetattr.call_args[0],\n            [sys.stdin, mocked_termios.TCSANOW, (0, 0, 0, mocked_termios.ECHO)]\n        )\n", "    def test_autoreload_started_sent(self, mock_autoreload_started):\n        fake_reloader = mock.MagicMock()\n        autoreload.start_django(fake_reloader, lambda: None)\n        self.assertEqual(mock_autoreload_started.send.call_count, 1)\n        self.assertSequenceEqual(\n            mock_autoreload_started.send.call_args[0],\n            [fake_reloader]\n        )\n", "    def test_ensure_echo_on_calls_tcsetattr(self, mocked_termios):\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n", "    def test_should_stop(self):\n        reloader = autoreload.BaseReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n"], "sample_272": ["def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Replaced migrations are correctly handled in the migration plan.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    squash_impl = FakeMigration('squash_1_2')\n    squash = ('a', 'squash_1_2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(squash, squash_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_replacement(squash, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n\n    plan = executor.migration_plan([squash])\n\n    self.assertEqual(plan, [(squash_impl, False)])\n", "def test_migration_plan_with_clean_start(self):\n    \"\"\"\n    Tests running a migration plan with clean_start=True.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Make the initial plan, check it\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")], clean_start=True)\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Fake-apply all migrations\n    executor.migrate([\n        (\"migrations\", \"0002_second\"),\n    ], fake=True)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Now plan a second time and make sure it's not empty\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")], clean_start=True)\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n", "def test_migration_plan_with_replaced(self):\n    \"\"\"\n    Replaced migrations are not included in the migration plan.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    squash_impl = FakeMigration('squash_1_and_2')\n    squash = ('a', 'squash_1_and_2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(squash, squash_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_replacement(squash, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n        squash: squash_impl,\n    })\n\n    plan = executor.migration_plan({squash})\n\n    self.assertEqual(plan, [(squash_impl, False)])\n", "def test_migration_plan_with_fake_initial(self):\n    \"\"\"\n    Tests that the migration plan contains the correct migrations when using\n    --fake-initial.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Create a fake applied migration\n    executor.recorder.record_applied(\"migrations\", \"0001_initial\")\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")], clean_start=True)\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Test with --fake-initial\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")], clean_start=True, fake_initial=True)\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n", "def test_migration_plan_with_missing_dependency(self):\n    \"\"\"\n    Tests that a migration plan with a missing dependency raises an error.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Delete the '0001_initial' migration to simulate a missing dependency\n    del executor.loader.graph.nodes[(\"migrations\", \"0001_initial\")]\n    with self.assertRaises(InvalidMigrationPlan):\n        executor.migration_plan([(\"migrations\", \"0002_second\")])\n"], "sample_273": ["    def test_unique_together_with_deferred_fields(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n        model = Model()\n        model.field1 = 'value'\n        # Accessing a deferred field doesn't raise an exception.\n        with self.assertRaises(AttributeError):\n            model.field2\n\n        # Model validation still works even if it contains deferred fields.\n        try:\n            model.full_clean()\n        except ValidationError as e:\n            self.assertEqual(e.error_dict, {})\n", "    def test_model_clean_is_not_checked(self):\n        class Model(models.Model):\n                raise Exception('clean should not be called')\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_index_together(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n            class Meta:\n                index_together = [['field1', 'field2']]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_unique_constraint(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['field1', 'field2'], name='unique_fields'),\n                ]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_model_clean(self):\n        class Model(models.Model):\n                raise ValidationError('Error in clean')\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"clean() raised ValidationError\",\n                hint=\"Handle ValidationError in clean()\",\n                obj=Model,\n                id='models.E033',\n            ),\n        ])\n"], "sample_274": ["    def test_modelmultiplechoicefield_required(self):\n        # Create choices for the model multiple choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        # required is True by default\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all())\n        self.assertFormErrors(['This field is required.'], f.clean, '')\n        self.assertFormErrors(['This field is required.'], f.clean, [])\n\n        # required is False\n        f = ModelMultipleChoiceField(required=False, queryset=ChoiceModel.objects.all())\n        self.assertIsNone(f.clean(''))\n        self.assertEqual(f.clean([]), [])\n", "    def test_modelchoiceiterator(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all())\n        iterator = ModelChoiceIterator(f)\n        choices = list(iterator)\n        self.assertEqual(len(choices), 4)  # 3 choices + empty choice\n        self.assertEqual(choices[0], ('', '---------'))  # empty choice\n        self.assertEqual(choices[1], (1, 'a'))\n        self.assertEqual(choices[2], (2, 'b'))\n        self.assertEqual(choices[3], (3, 'c'))\n", "    def test_modelform_unique_error_message(self):\n        # Create a model with a unique field.\n        from django.db import models\n\n        class UniqueModel(models.Model):\n            name = models.CharField(max_length=10, unique=True)\n\n        UniqueModel.objects.create(name='a')\n        UniqueModel.objects.create(name='b')\n\n        # Create a ModelForm for the model.\n        from django import forms\n\n        class UniqueModelForm(forms.ModelForm):\n            class Meta:\n                model = UniqueModel\n                fields = ('name',)\n\n        # Test that the default unique error message is displayed.\n        form = UniqueModelForm({'name': 'a'})\n        self.assertFormErrors(['UniqueModel with this Name already exists.'], form.clean)\n\n        # Test that a custom unique error message is displayed.\n        e = {\n            'unique': 'CUSTOM UNIQUE ERROR MESSAGE',\n        }\n        UniqueModel._meta.get_field('name').error_messages = e\n        form = UniqueModelForm({'name': 'a'})\n        self.assertFormErrors(['CUSTOM UNIQUE ERROR MESSAGE'], form.clean)\n", "    def test_modelformset(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        class SomeModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = modelformset_factory(ChoiceModel, form=SomeModelForm, error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f({'form-0-name': ''}).clean())\n        self.assertFormErrors(['INVALID CHOICE'], f({'form-0-name': 'd'}).clean())\n\n        class SomeModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('name',)\n\n            name = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n\n        f = modelformset_factory(ChoiceModel, form=SomeModelForm, error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f({'form-0-name': ''}).clean())\n        self.assertFormErrors(['NOT A LIST OF VALUES'], f({'form-0-name': '3'}).clean())\n        self.assertFormErrors(['4 IS INVALID CHOICE'], f({'form-0-name': ['4']}).clean())\n", "    def test_unique(self):\n        class UniqueModel(models.Model):\n            unique_field = models.CharField(max_length=10, unique=True)\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = UniqueModel\n                fields = '__all__'\n\n        form1 = TestForm({'unique_field': 'test'})\n        self.assertTrue(form1.is_valid())\n        form1.save()\n\n        form2 = TestForm({'unique_field': 'test'})\n        self.assertFalse(form2.is_valid())\n        self.assertEqual(len(form2.errors), 1)\n        self.assertEqual(len(form2.errors['unique_field']), 1)\n        self.assertEqual(form2.errors['unique_field'][0], 'Unique model with this Unique field already exists.')\n"], "sample_275": ["    def test_queryset_delete_with_field_name(self):\n        book1 = Book.objects.create(pagecount=100)\n        book2 = Book.objects.create(pagecount=200)\n        self.assertEqual(Book.objects.count(), 2)\n        Book.objects.filter(pagecount=100).delete('id')\n        self.assertEqual(Book.objects.count(), 1)\n        self.assertEqual(Book.objects.get().pk, book2.pk)\n", "    def test_prefetch_related_with_none(self):\n        \"\"\"\n        Test that prefetch_related() with None as an argument doesn't cause any\n        errors.\n        \"\"\"\n        houses = list(House.objects.prefetch_related(None))\n        self.assertEqual(len(houses), House.objects.count())\n", "    def test_ticket_20577(self):\n        item = Item.objects.create()\n        Version.objects.create(item=item)\n        item.delete()\n        self.assertFalse(Item.objects.exists())\n        self.assertFalse(Version.objects.exists())\n", "    def setUpTestData(cls):\n        cls.book = Book.objects.create(pagecount=100)\n        cls.award = Award.objects.create(name='Nobel', content_object=cls.book)\n        cls.award_note = AwardNote.objects.create(note='a peace prize', award=cls.award)\n", "    def test_delete_queryset_with_prefetch_related(self):\n        \"\"\"\n        Deleting a QuerySet with prefetch_related() should work correctly.\n        \"\"\"\n        policy = Policy.objects.create(policy_number=\"1234\")\n        version = Version.objects.create(policy=policy)\n        location = Location.objects.create(version=version)\n        Item.objects.create(version=version, location=location)\n\n        # Create a QuerySet with prefetch_related()\n        queryset = Policy.objects.prefetch_related('version_set__item_set')\n        queryset.delete()\n\n        # Check that all objects are deleted\n        self.assertEqual(Policy.objects.count(), 0)\n        self.assertEqual(Version.objects.count(), 0)\n        self.assertEqual(Location.objects.count(), 0)\n        self.assertEqual(Item.objects.count(), 0)\n"], "sample_276": ["    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'), '/<sport_slug>/athletes/<athlete_slug>/')\n        self.assertEqual(simplify_regex(r'^polls/(?P<poll_id>\\d+)/$'), '/polls/<poll_id>/')\n        self.assertEqual(simplify_regex(r'^articles/(?P<year>[0-9]{4})/$'), '/articles/<year>/')\n", "    def test_simplify_regex(self):\n        self.assertEqual(\n            simplify_regex(r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"),\n            \"/<sport_slug>/athletes/<athlete_slug>/\"\n        )\n", "    def test_simplify_regex(self):\n        pattern = \"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        simplified_pattern = simplify_regex(pattern)\n        self.assertEqual(simplified_pattern, \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'), '/<sport_slug>/athletes/<athlete_slug>/')\n"], "sample_277": ["def test_register_lookup(self):\n    class MyLookup:\n        lookup_name = 'mylookup'\n\n    Q.register_lookup(MyLookup)\n    self.assertIn('mylookup', Q.get_lookups())\n\n    # Check that unregistering works too.\n    Q._unregister_lookup(MyLookup)\n    self.assertNotIn('mylookup', Q.get_lookups())\n", "def test_register_lookup(self):\n    class TestLookup:\n        lookup_name = 'test'\n\n    class TestModel:\n        pass\n\n    RegisterLookupMixin.register_lookup(TestLookup, 'test')\n    self.assertIn('test', RegisterLookupMixin.get_lookups())\n    self.assertEqual(RegisterLookupMixin._get_lookup('test'), TestLookup)\n\n    RegisterLookupMixin._unregister_lookup(TestLookup, 'test')\n    self.assertNotIn('test', RegisterLookupMixin.get_lookups())\n    self.assertIsNone(RegisterLookupMixin._get_lookup('test'))\n", "def test_deconstruct_nested_negated(self):\n    q = ~Q(Q(price__gt=F('discounted_price')))\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (Q(Q(price__gt=F('discounted_price'))),))\n    self.assertEqual(kwargs, {'_negated': True})\n", "def test_register_lookup(self):\n    class MyQ(Q):\n        pass\n\n    lookup = object()\n    MyQ.register_lookup(lookup, 'my_lookup')\n    self.assertIn('my_lookup', MyQ.get_lookups())\n    self.assertEqual(MyQ._get_lookup('my_lookup'), lookup)\n", "def test_deconstruct_multiple_levels_of_nesting(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q3 = ~Q(q1 | q2)\n    q4 = Q(q3 & Q(quantity__gt=10))\n    path, args, kwargs = q4.deconstruct()\n    self.assertEqual(args, (~Q((Q(price__gt=F('discounted_price')) | Q(price=F('discounted_price')))), Q(quantity__gt=10)))\n    self.assertEqual(kwargs, {})\n"], "sample_278": ["    def test_combine(self):\n        q1 = Q(a=1)\n        q2 = Q(b=2)\n        self.assertEqual((q1 & q2).children, [q1, q2])\n        self.assertEqual((q1 | q2).children, [q1, q2])\n", "    def test_attribute_access(self):\n        e = Employee.objects.create(firstname='John', lastname='Doe')\n        self.assertIsNone(e._DeferredAttribute_test)\n        e._DeferredAttribute_test = 'test'\n        self.assertEqual(e._DeferredAttribute_test, 'test')\n", "    def test_combine(self):\n        q1 = Q(x=1)\n        q2 = Q(y=2)\n        combined = q1 & q2\n        self.assertEqual(combined.children, [q1, q2])\n        self.assertEqual(combined.connector, Q.AND)\n", "    def test_deconstruct(self):\n        q = Q(name='test')\n        path, args, kwargs = q.deconstruct()\n        self.assertEqual(path, 'django.db.models.Q')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'name': 'test'})\n", "    def test_merge_dicts(self):\n        mixin = RegisterLookupMixin()\n        dicts = [{'a': 1, 'b': 2}, {'b': 3, 'c': 4}]\n        merged = mixin.merge_dicts(dicts)\n        self.assertEqual(merged, {'a': 1, 'b': 3, 'c': 4})\n"], "sample_279": ["    def test_database_constraint_opclasses(self):\n        UniqueConstraintProduct.objects.create(name='test', color='red')\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='Test', color='red')\n", "def test_opclasses_length_mismatch(self):\n    msg = 'UniqueConstraint.fields and UniqueConstraint.opclasses must have the same number of elements.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            name='uniq',\n            fields=['field1', 'field2'],\n            opclasses=['varchar_pattern_ops'],\n        )\n", "def test_invalid_opclasses_length(self):\n    msg = 'UniqueConstraint.fields and UniqueConstraint.opclasses must have the same number of elements.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            name='uniq_opclasses',\n            fields=['field1', 'field2'],\n            opclasses=['jsonb_path_ops'],\n        )\n", "def test_invalid_opclasses_length(self):\n    msg = 'UniqueConstraint.fields and UniqueConstraint.opclasses must have the same number of elements.'\n    with self.assertRaisesMessage(ValueError, msg):\n        models.UniqueConstraint(\n            name='uniq_opclasses',\n            fields=['field1', 'field2'],\n            opclasses=['jsonb_path_ops'],\n        )\n", "    def test_clone(self):\n        constraint = models.UniqueConstraint(fields=['foo', 'bar'], name='unique')\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n"], "sample_280": ["def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum('price', default=0.0),\n    )\n    self.assertEqual(result['value'], 0.0)\n", "def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum('price', default=0.0),\n    )\n    self.assertEqual(result['value'], 0.0)\n\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Avg('price', default=0.0),\n    )\n    self.assertEqual(result['value'], 0.0)\n", "def test_aggregation_filter_argument_with_subquery(self):\n    subquery = Book.objects.filter(pages__gt=300).values('publisher')\n    queryset = Publisher.objects.annotate(\n        count=Count('book', filter=Exists(subquery)),\n    )\n    self.assertQuerysetEqual(\n        queryset, [\n            (3, 1),\n            (0, 0),\n            (2, 1),\n            (1, 0),\n            (1, 0),\n        ],\n        lambda p: (p.num_awards, p.count)\n    )\n", "def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Sum('price', default=0.0),\n    )\n    self.assertEqual(result['value'], 0.0)\n", "def test_aggregation_default_using_float_from_python(self):\n    result = Book.objects.filter(rating__lt=3.0).aggregate(\n        value=Avg('price', default=0.0),\n    )\n    self.assertEqual(result['value'], 0.0)\n"], "sample_281": ["def test_process_request_params(self):\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    view = AutocompleteJsonView()\n    term, model_admin, source_field, to_field_name = view.process_request(request)\n    self.assertEqual(term, 'is')\n    self.assertIsInstance(model_admin, QuestionAdmin)\n    self.assertEqual(source_field.name, 'question')\n    self.assertEqual(to_field_name, 'id')\n", "def test_process_request(self):\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    term, model_admin, source_field, to_field_name = AutocompleteJsonView().process_request(request)\n    self.assertEqual(term, 'is')\n    self.assertIsInstance(model_admin, QuestionAdmin)\n    self.assertEqual(source_field.name, 'question')\n    self.assertEqual(to_field_name, 'id')\n", "def test_get_context_data(self):\n    q = Question.objects.create(question='Is this a question?')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    context = response.context_data\n    self.assertEqual(context['object_list'].count(), 1)\n    self.assertEqual(context['page_obj'].number, 1)\n    self.assertEqual(context['paginator'].count, 1)\n", "def test_pagination_info(self):\n    \"\"\"The 'pagination' info in the response is correct.\"\"\"\n    Question.objects.bulk_create(Question(question=str(i)) for i in range(PAGINATOR_SIZE + 10))\n    request = self.factory.get(self.url, {'term': '', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data['pagination'], {'more': True})\n\n    request = self.factory.get(self.url, {'term': '', 'page': '2', **self.opts})\n    request.user = self.superuser\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data['pagination'], {'more': False})\n", "def test_process_request_invalid_parameters(self):\n    request = self.factory.get(self.url, {'term': 'is'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': Answer._meta.app_label, 'model_name': 'invalid'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_282": ["def test_boundfield_css_classes(self):\n    form = ComplexFieldForm()\n    field = form['field1']\n    self.assertEqual(field.css_classes(), '')\n    self.assertEqual(field.css_classes('class1'), 'class1')\n\n    # Test required class is added if the field is required\n    self.field.required = True\n    field = BoundField(form, self.field, 'field1')\n    self.assertEqual(field.css_classes(), 'required')\n\n    # Test error class is added if the field has an error\n    form.add_error('field1', ValidationError('Error message'))\n    self.assertEqual(field.css_classes(), 'error required')\n", "def test_boundfield_subwidgets(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    subwidgets = bound_field.subwidgets\n    self.assertEqual(len(subwidgets), 3)\n    self.assertIsInstance(subwidgets[0], BoundWidget)\n    self.assertIsInstance(subwidgets[1], BoundWidget)\n    self.assertIsInstance(subwidgets[2], BoundWidget)\n    self.assertEqual(subwidgets[0].data, {'name': 'field1_0', 'value': None, 'attrs': {}})\n    self.assertEqual(subwidgets[1].data, {'name': 'field1_1', 'value': None, 'attrs': {}})\n    self.assertEqual(subwidgets[2].data, {'name': 'field1_2_0', 'value': None, 'attrs': {}})\n", "def test_partially_required_field(self):\n    form = PartiallyRequiredForm({'f_0': 'some text'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['f'], 'some text,')\n", "def test_boundfield_label_tag(self):\n    class TestForm(Form):\n        field1 = CharField(label='Test Field')\n\n    form = TestForm()\n    bound_field = form['field1']\n    self.assertHTMLEqual(\n        bound_field.label_tag(),\n        '<label for=\"id_field1\">Test Field:</label>'\n    )\n\n    # Test with custom label suffix\n    form = TestForm(label_suffix='')\n    bound_field = form['field1']\n    self.assertHTMLEqual(\n        bound_field.label_tag(),\n        '<label for=\"id_field1\">Test Field</label>'\n    )\n\n    # Test with custom attrs\n    form = TestForm()\n    bound_field = form['field1']\n    self.assertHTMLEqual(\n        bound_field.label_tag(attrs={'class': 'test-class'}),\n        '<label for=\"id_field1\" class=\"test-class\">Test Field:</label>'\n    )\n", "def test_boundfield_widget_type(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    self.assertEqual(bound_field.widget_type, 'complexmultiwidget')\n"], "sample_283": ["def test_runshell(self):\n    with mock.patch('django.db.backends.postgresql.client.super') as mocked_super:\n        client = DatabaseClient(connection)\n        parameters = ['--help']\n        client.runshell(parameters)\n        mocked_super().runshell.assert_called_once_with(parameters)\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        (['psql', 'postgres'], {}),\n    )\n", "def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        (['psql', 'postgres'], {}),\n    )\n", "def test_runshell_with_parameters(self):\n    db_client = DatabaseClient(connection)\n    parameters = ['--version']\n    with mock.patch('django.db.backends.base.client.subprocess') as mock_subprocess:\n        db_client.runshell(parameters)\n        mock_subprocess.run.assert_called_once_with(\n            db_client.settings_to_cmd_args_env(connection.settings_dict, parameters)[0],\n            env=db_client.settings_to_cmd_args_env(connection.settings_dict, parameters)[1],\n            check=True,\n        )\n", "def test_no_dbname_and_no_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\n            'USER': 'someuser',\n            'HOST': 'somehost',\n            'PORT': '444',\n        }), (\n            ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'],\n            {},\n        )\n    )\n"], "sample_284": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def test_max_post_process_passes(self):\n        storage.staticfiles_storage.max_post_process_passes = 1\n        with self.assertRaisesMessage(RuntimeError, 'Max post-process passes exceeded'):\n            self.run_collectstatic()\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        self.addCleanup(shutil.rmtree, temp_dir)\n"], "sample_285": ["def test_dirs_contains_non_existent_directory(self):\n    static_dir = Path(TEST_ROOT) / 'non_existent_directory'\n    with self.settings(STATICFILES_DIRS=[static_dir]):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                f\"The directory '{static_dir}' in the STATICFILES_DIRS setting \"\n                f\"does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n", "def test_finder_with_non_existent_directory(self):\n    static_dir = Path(TEST_ROOT) / 'non_existent_directory'\n    with self.settings(STATICFILES_DIRS=[static_dir]):\n        errors = check_finders(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Warning)\n        self.assertEqual(errors[0].msg, f\"The directory '{static_dir}' in the STATICFILES_DIRS setting does not exist.\")\n        self.assertEqual(errors[0].id, 'staticfiles.W004')\n", "def test_dirs_contains_nonexistent_directory(self):\n    nonexistent_dir = Path(TEST_ROOT) / 'nonexistent'\n    with self.settings(STATICFILES_DIRS=[nonexistent_dir]):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                f\"The directory '{nonexistent_dir}' in the STATICFILES_DIRS setting \"\n                f\"does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n", "def test_dirs_contains_nonexistent_path(self):\n    nonexistent_dir = '/path/to/nonexistent/directory'\n    with self.settings(STATICFILES_DIRS=[nonexistent_dir]):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                f\"The directory '{nonexistent_dir}' in the STATICFILES_DIRS setting \"\n                f\"does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n", "def test_dirs_contains_non_existent_directory(self):\n    self.assertEqual(check_finders(None), [\n        Warning(\n            \"The directory '/non/existent/directory' in the STATICFILES_DIRS setting \"\n            \"does not exist.\",\n            id='staticfiles.W004',\n        )\n    ])\n"], "sample_286": ["    def test_refresh_with_inheritance(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        FeaturedArticle.objects.create(article=a)\n        a.headline = 'New headline'\n        a.save()\n        fa = FeaturedArticle.objects.get(article=a)\n        with self.assertNumQueries(1):\n            fa.refresh_from_db(fields=['article'])\n            self.assertEqual(fa.article.headline, 'New headline')\n", "    def test_str(self):\n        a = Article.objects.create(\n            headline='Parrot programs in Python',\n            pub_date=datetime(2005, 7, 28),\n        )\n        self.assertEqual(str(a), 'Article object (%s)' % a.pk)\n", "    def test_full_clean(self):\n        # Make sure full_clean() works on both new and existing objects.\n        article = Article(headline='Full clean headline')\n        article.full_clean()\n\n        article.save()\n        article.full_clean()\n", "    def test_setting_state_adding(self):\n        a = Article(headline='Parrot programs in Python', pub_date=datetime(2005, 7, 28))\n        self.assertTrue(a._state.adding)\n        a.save()\n        self.assertFalse(a._state.adding)\n", "    def test_model_full_clean(self):\n        article = Article(headline='Parrot programs in Python', pub_date=datetime(2005, 7, 28))\n        # Make sure the model is valid initially\n        article.full_clean()\n\n        # Introduce an error and make sure it's caught\n        article.pub_date = 'not a date'\n        with self.assertRaisesMessage(ValidationError, \"value has an invalid date format. It must be in YYYY-MM-DD format.\"):\n            article.full_clean()\n"], "sample_287": ["def test_radio_fields_key(self):\n    class SongAdmin(admin.ModelAdmin):\n        radio_fields = {\"nonexistent\": admin.VERTICAL}\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'radio_fields' refers to 'nonexistent', which is not an \"\n            \"instance of ForeignKey, and does not have a 'choices' definition.\",\n            obj=SongAdmin,\n            id='admin.E023',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_radio_fields_value(self):\n    class SongAdmin(admin.ModelAdmin):\n        radio_fields = {\"title\": \"invalid\"}\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'radio_fields[\\\"title\\\"]' must be either admin.HORIZONTAL or admin.VERTICAL.\",\n            obj=SongAdmin,\n            id='admin.E024',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_radio_fields_value(self):\n    class SongAdmin(admin.ModelAdmin):\n        radio_fields = {\"group\": \"vertical\"}\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'radio_fields' refers to 'group', which is not an \"\n            \"instance of ForeignKey, and does not have a 'choices' definition.\",\n            obj=SongAdmin,\n            id='admin.E023',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_ordering_on_non_model_fields(self):\n    \"\"\"\n    Regression for ensuring ModelAdmin.ordering can handle non-model fields.\n    \"\"\"\n    class SongAdmin(admin.ModelAdmin):\n        ordering = ['nonexistent_field']\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'ordering[0]' refers to 'nonexistent_field', which is not a field of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id='admin.E033',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_filter_vertical_and_horizontal(self):\n    class SongAdmin(admin.ModelAdmin):\n        filter_vertical = \"test\"\n        filter_horizontal = (\"title\",)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'filter_vertical' must be a list or tuple.\",\n            obj=SongAdmin,\n            id='admin.E017',\n        ),\n        checks.Error(\n            \"The value of 'filter_horizontal[0]' refers to 'title', which is not a many-to-many field.\",\n            obj=SongAdmin,\n            id='admin.E020',\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_288": ["    def test_custom_encoder_decoder(self):\n        class CustomJSONEncoder(json.JSONEncoder):\n                if isinstance(obj, uuid.UUID):\n                    return str(obj)\n                return super().default(obj)\n\n        class CustomJSONDecoder(json.JSONDecoder):\n                super().__init__(object_hook=self.dict_to_object, *args, **kwargs)\n\n                for key, value in dictionary.items():\n                    if isinstance(value, str) and len(value) == 36:\n                        try:\n                            dictionary[key] = uuid.UUID(value)\n                        except ValueError:\n                            pass\n                return dictionary\n\n        value = {'uuid': uuid.UUID('{d85e2076-b67c-4ee7-8c3a-2bf5a2cc2475}')}\n        model = NullableJSONModel.objects.create(value_custom=value)\n        model.refresh_from_db()\n        self.assertEqual(model.value_custom, value)\n", "    def setUpTestData(cls):\n        cls.obj = NullableJSONModel.objects.create(value={'a': 'b', 'c': 14})\n", "    def setUpTestData(cls):\n        cls.objs = [\n            NullableJSONModel.objects.create(value={'a': 1, 'b': 2}),\n            NullableJSONModel.objects.create(value={'a': 3, 'b': 4}),\n            NullableJSONModel.objects.create(value={'c': 5, 'd': 6}),\n        ]\n", "def test_key_transform_raw_expression_with_params(self):\n    expr = RawSQL(self.raw_sql, ['{\"x\": \"bar\"}'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr)),\n        [self.objs[7]],\n    )\n    expr = RawSQL(self.raw_sql, ['{\"x\": %s}'], params=['bar'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr)),\n        [self.objs[7]],\n    )\n", "def test_key_transform_with_non_string_keys(self):\n    obj = NullableJSONModel.objects.create(value={'1': 'a', 2: 'b'})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__1='a'),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__2='b'),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(**{'value__%s' % 1: 'a'}),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(**{'value__%s' % 2: 'b'}),\n        [obj],\n    )\n"], "sample_289": ["    def test_create_with_empty_dict(self):\n        d = CaseInsensitiveMapping({})\n        self.assertEqual(d, {})\n        self.assertIsInstance(d, Mapping)\n", "def test_get(self):\n    self.assertEqual(self.dict1.get('Accept'), 'application/json')\n    self.assertEqual(self.dict1.get('accept'), 'application/json')\n    self.assertEqual(self.dict1.get('aCCept'), 'application/json')\n    self.assertEqual(self.dict1.get('content-type'), 'text/html')\n    self.assertEqual(self.dict1.get('Content-Type'), 'text/html')\n    self.assertIsNone(self.dict1.get('non-existent-key'))\n    self.assertEqual(self.dict1.get('non-existent-key', 'default-value'), 'default-value')\n", "    def test_update(self):\n        dict1 = CaseInsensitiveMapping({\n            'Accept': 'application/json',\n            'content-type': 'text/html',\n        })\n        dict1.update({'accept': 'application/xml'})\n        self.assertEqual(dict1['Accept'], 'application/xml')\n        self.assertEqual(len(dict1), 2)\n", "    def test_len(self):\n        dict1 = CaseInsensitiveMapping({\n            'Accept': 'application/json',\n            'content-type': 'text/html',\n        })\n        self.assertEqual(len(dict1), 2)\n", "    def test_update(self):\n        dict1 = CaseInsensitiveMapping({\n            'Accept': 'application/json',\n            'content-type': 'text/html',\n        })\n        dict2 = CaseInsensitiveMapping({\n            'ACCEPT': 'application/xml',\n            'Content-Type': 'text/plain',\n        })\n        dict1.update(dict2)\n        self.assertEqual(dict1, {\n            'Accept': 'application/xml',\n            'content-type': 'text/plain',\n        })\n"], "sample_290": ["def test_suggest_name_with_deconstructible_default(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.AddField(\n                model_name='person',\n                name='new_field',\n                field=models.CharField(max_length=200, default=DeconstructibleObject()),\n            ),\n        ]\n\n    migration = Migration('some_migration', 'test_app')\n    self.assertEqual(migration.suggest_name(), 'add_new_field_to_person')\n", "def test_migration_suggest_name_with_initial_and_operations(self):\n    class Migration(migrations.Migration):\n        initial = True\n        operations = [\n            migrations.CreateModel('Person', fields=[]),\n            migrations.DeleteModel('Animal'),\n        ]\n\n    migration = Migration('0001_initial', 'test_app')\n    self.assertEqual(migration.suggest_name(), 'initial')\n", "def test_suggest_name_with_custom_deconstructible(self):\n    class CustomDeconstructible:\n            self.name = name\n\n            return (\n                'path.to.module.CustomDeconstructible',\n                (self.name,),\n                {}\n            )\n\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel('Person', fields=[\n                ('custom_field', models.CharField(max_length=255, default=CustomDeconstructible('example'))),\n            ]),\n        ]\n\n    migration = Migration('some_migration', 'test_app')\n    self.assertEqual(migration.suggest_name(), 'person')\n", "def test_suggest_name_with_custom_migration_name_fragment(self):\n    class CustomOperation(migrations.operations.base.Operation):\n            self.name = name\n\n            return (\n                'django.db.migrations.operations.CustomOperation',\n                [],\n                {'name': self.name},\n            )\n\n            return f\"Custom operation {self.name}\"\n\n        @property\n            return f\"custom_{self.name}\"\n\n    class Migration(migrations.Migration):\n        operations = [\n            CustomOperation(\"create_user\"),\n            CustomOperation(\"delete_user\"),\n        ]\n\n    migration = Migration('some_migration', 'test_app')\n    self.assertEqual(migration.suggest_name(), \"custom_create_user_custom_delete_user\")\n", "def test_migration_suggest_name_with_custom_operation(self):\n    class CustomOperation(migrations.operations.base.Operation):\n            self.name = name\n\n            return (self.__class__.__name__, [self.name], {})\n\n            return f'Custom operation: {self.name}'\n\n        @property\n            return self.name.lower()\n\n    class Migration(migrations.Migration):\n        operations = [\n            CustomOperation('MyOperation'),\n        ]\n\n    migration = Migration('some_migration', 'test_app')\n    self.assertEqual(migration.suggest_name(), 'myoperation')\n"], "sample_291": ["    def test_render_to_response(self):\n        mixin = TemplateResponseMixin()\n        mixin.request = RequestFactory().get('/')\n        mixin.template_name = 'generic_views/about.html'\n        response = mixin.render_to_response({})\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, mixin.template_name)\n", "def test_view_dispatch_verbosity(self):\n    request = self.rf.get('/')\n    view = SimpleView.as_view()\n    with self.assertLogs('django.request', level='INFO') as cm:\n        response = view(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(len(cm.records), 0)\n\n    with self.assertLogs('django.request', level='WARNING') as cm:\n        response = view(self.rf.post('/'))\n    self.assertEqual(response.status_code, 405)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Method Not Allowed', cm.records[0].getMessage())\n\n    with self.assertLogs('django.request', level='WARNING') as cm:\n        response = view(self.rf.put('/'))\n    self.assertEqual(response.status_code, 405)\n    self.assertEqual(len(cm.records), 1)\n    self.assertIn('Method Not Allowed', cm.records[0].getMessage())\n", "    def test_options(self):\n        view = View()\n        request = RequestFactory().options('/')\n        response = view.dispatch(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('Allow', response.headers)\n        self.assertEqual(response.headers['Allow'], ', '.join(view._allowed_methods()))\n", "    def test_render_to_response(self):\n        view = TemplateView()\n        view.request = RequestFactory().get('/')\n        view.template_name = 'generic_views/about.html'\n        response = view.render_to_response(context={})\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, 'generic_views/about.html')\n", "    def test_http_method_not_allowed(self):\n        class TestView(View):\n                return HttpResponse('Hello')\n\n        response = TestView.as_view()(self.rf.post('/'))\n        self.assertEqual(response.status_code, 405)\n        self.assertEqual(response['Allow'], 'GET, HEAD')\n"], "sample_292": ["def test_process_view_csrf_trusted_origin_with_port(self):\n    \"\"\"\n    If the origin includes a port, it is still matched against trusted origins.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://dashboard.example.com:8080'\n    mw = CsrfViewMiddleware(post_form_view)\n    with override_settings(ALLOWED_HOSTS=['www.example.com'], CSRF_TRUSTED_ORIGINS=['https://dashboard.example.com:8080']):\n        self.assertIs(mw._origin_verified(req), True)\n    with override_settings(ALLOWED_HOSTS=['www.example.com'], CSRF_TRUSTED_ORIGINS=['https://dashboard.example.com']):\n        self.assertIs(mw._origin_verified(req), False)\n", "def test_csrf_cookie_domain_set(self):\n    \"\"\"\n    The CSRF cookie domain is set to the value of settings.CSRF_COOKIE_DOMAIN.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n    self.assertEqual(csrf_cookie['domain'], settings.CSRF_COOKIE_DOMAIN)\n", "def test_csrf_cookie_domain(self):\n    \"\"\"\n    The CSRF cookie domain can be set using settings.CSRF_COOKIE_DOMAIN.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n\n    DOMAIN = '.example.com'\n    with self.settings(CSRF_COOKIE_NAME='csrfcookie',\n                       CSRF_COOKIE_DOMAIN=DOMAIN,\n                       CSRF_COOKIE_PATH='/test/',\n                       CSRF_COOKIE_SECURE=True,\n                       CSRF_COOKIE_HTTPONLY=True):\n        # token_view calls get_token() indirectly\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        csrf_cookie = resp.cookies.get('csrfcookie')\n        self.assertEqual(csrf_cookie['domain'], DOMAIN)\n", "def test_csrf_trusted_origin_subdomain_wildcard(self):\n    \"\"\"\n    A POST request with an origin that matches a subdomain wildcard in\n    CSRF_TRUSTED_ORIGINS is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "def test_csrf_token_length(self):\n    \"\"\"\n    The CSRF token is of the expected length.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    csrf_token = resp.cookies.get(settings.CSRF_COOKIE_NAME).value\n    self.assertEqual(len(csrf_token), CSRF_TOKEN_LENGTH)\n"], "sample_293": ["    def test_locale_regex_descriptor(self):\n        resolver = get_resolver()\n        for pattern in resolver.url_patterns:\n            if hasattr(pattern, 'pattern'):\n                # Test that the regex is compiled correctly for the active language.\n                self.assertIsInstance(pattern.pattern.regex, re.Pattern)\n                # Test that the regex is cached correctly.\n                self.assertEqual(pattern.pattern.regex, pattern.pattern.regex)\n", "    def test_regex_compilation(self):\n        descriptor = LocaleRegexDescriptor('attr')\n        instance = type('Test', (), {'attr': 'regex'})\n        regex = descriptor.__get__(instance)\n        self.assertIsInstance(regex, re.Pattern)\n", "    def test_regex_pattern_repr(self):\n        pattern = RegexPattern(r'^/test/$', name='test-view')\n        self.assertEqual(repr(pattern), \"RegexPattern('^{}/test/$', name='test-view')\".format(settings.SECURE_PROXY_SSL_HEADER[1]))\n", "    def test_urlpattern_reverse_namespace_app(self):\n        resolver = get_resolver(None)\n        test_urls = [\n            ('test-ns1:urlobject-view', [], {}, '/test1/inner/'),\n            ('test-ns1:urlobject-view', [37, 42], {}, '/test1/inner/37/42/'),\n            ('test-ns1:urlobject-view', [], {'arg1': 42, 'arg2': 37}, '/test1/inner/42/37/'),\n        ]\n        for name, args, kwargs, expected in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs):\n                self.assertEqual(resolver.reverse(name, args=args, kwargs=kwargs), expected)\n", "    def test_check_url_pattern(self):\n        resolver = get_resolver()\n        messages = check_resolver(resolver)\n        self.assertEqual(messages, [])\n"], "sample_294": ["def test_rotate_token(self):\n    \"\"\"The rotate_token() function changes the token in use for a request.\"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    old_token = req.META.get('CSRF_COOKIE')\n    rotate_token(req)\n    new_token = req.META.get('CSRF_COOKIE')\n    self.assertNotEqual(old_token, new_token)\n", "def test_csrf_rotate_token(self):\n    \"\"\"\n    The rotate_token() function changes the token in use for a request.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    initial_token = get_token(req)\n    rotate_token(req)\n    new_token = get_token(req)\n    self.assertNotEqual(initial_token, new_token)\n    self.assertEqual(len(new_token), CSRF_TOKEN_LENGTH)\n", "def test_csrf_trusted_origins_wildcard(self):\n    \"\"\"\n    Test that CSRF_TRUSTED_ORIGINS with wildcard works correctly.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://subdomain.example.com'\n\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n\n    with override_settings(CSRF_TRUSTED_ORIGINS=['http://*.example.com']):\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), False)\n", "def test_rotate_token(self):\n    \"\"\"\n    The rotate_token() function changes the token in the session or cookie.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    token1 = get_token(req)\n    rotate_token(req)\n    token2 = get_token(req)\n    self.assertNotEqual(token1, token2)\n", "def test_rotate_token(self):\n    \"\"\"\n    The rotate_token() function changes the token value and sets\n    request.csrf_cookie_needs_reset.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    initial_token = get_token(req)\n    rotate_token(req)\n    new_token = get_token(req)\n    self.assertNotEqual(initial_token, new_token)\n    self.assertTrue(getattr(req, 'csrf_cookie_needs_reset', False))\n"], "sample_295": ["    def test_deconstruct(self):\n        frame = WindowFrame(start=1, end=10)\n        path, args, kwargs = frame.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.WindowFrame')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'start': 1, 'end': 10})\n", "    def test_resolve_output_field(self):\n        frame = WindowFrame()\n        msg = 'Cannot resolve expression type, unknown output_field'\n        with self.assertRaisesMessage(FieldError, msg):\n            frame.output_field\n", "    def test_subquery_in_filter(self):\n        inner = Company.objects.filter(name='Example Inc.').values('pk')\n        self.assertCountEqual(\n            Employee.objects.filter(company_ceo_set__in=Subquery(inner)),\n            [self.example_inc.ceo],\n        )\n        self.assertCountEqual(\n            Employee.objects.filter(company_point_of_contact_set__in=Subquery(inner)),\n            [],\n        )\n", "    def test_over_clause(self):\n        Number.objects.create(integer=1)\n        Number.objects.create(integer=2)\n        qs = Number.objects.annotate(\n            over_value=Window(expression=Sum('integer'), partition_by=[F('integer')]),\n        ).values_list('over_value', flat=True)\n        self.assertEqual(list(qs), [1, 2])\n", "    def test_resolve_expression(self):\n        expr1 = F('field1')\n        expr2 = F('field2')\n        expr_list = ExpressionList(expr1, expr2)\n        resolved = expr_list.resolve_expression(None, allow_joins=True, reuse=None, summarize=False, for_save=False)\n        self.assertEqual(resolved.is_summary, False)\n        self.assertEqual(len(resolved.source_expressions), 2)\n"], "sample_296": ["def test_compress_uncompress(self):\n    \"\"\"\n    Compressed messages are properly uncompressed by the MessageSerializer.\n    \"\"\"\n    storage = self.get_storage()\n    message = Message(constants.INFO, 'This is a long message that should be compressed.')\n    encoded = storage._encode([message])\n    decoded = storage._decode(encoded)\n    self.assertEqual(decoded[0].message, message.message)\n    self.assertEqual(decoded[0].level, message.level)\n\n    # Test with multiple messages\n    messages = [Message(constants.INFO, 'Message 1'), Message(constants.ERROR, 'Message 2')]\n    encoded = storage._encode(messages)\n    decoded = storage._decode(encoded)\n    self.assertEqual(len(decoded), len(messages))\n    for original, decoded_message in zip(messages, decoded):\n        self.assertEqual(decoded_message.message, original.message)\n        self.assertEqual(decoded_message.level, original.level)\n", "def test_safe_string_encoding(self):\n    \"\"\"\n    Test that a message containing SafeData is encoded and decoded properly.\n    \"\"\"\n    storage = self.get_storage()\n    safe_message = mark_safe(\"This is a <b>safe</b> message.\")\n    message = Message(constants.DEBUG, safe_message)\n    encoded = storage._encode([message])\n    decoded_messages = storage._decode(encoded)\n    self.assertEqual(len(decoded_messages), 1)\n    self.assertIsInstance(decoded_messages[0].message, SafeData)\n    self.assertEqual(decoded_messages[0].message, safe_message)\n", "def test_safedata(self):\n    storage = self.get_storage()\n    message = Message(constants.DEBUG, mark_safe('<p>Hello</p>'))\n    encoded = storage._encode([message])\n    decoded_messages = storage._decode(encoded)\n    self.assertEqual(len(decoded_messages), 1)\n    self.assertIsInstance(decoded_messages[0].message, SafeData)\n    self.assertEqual(decoded_messages[0].message, mark_safe('<p>Hello</p>'))\n", "def test_safedata(self):\n    storage = self.get_storage()\n    safe_message = mark_safe(\"<p>Hello, world!</p>\")\n    message = Message(constants.DEBUG, safe_message)\n    encoded = storage._encode([message])\n    decoded_messages = storage._decode(encoded)\n    self.assertEqual(len(decoded_messages), 1)\n    self.assertIsInstance(decoded_messages[0].message, SafeData)\n    self.assertEqual(str(decoded_messages[0].message), str(safe_message))\n", "def test_update_cookie(self):\n    \"\"\"\n    The _update_cookie method sets the cookie with the encoded data if there is\n    any data to store, or deletes the cookie.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Test setting the cookie with encoded data\n    messages = [Message(constants.INFO, 'Test message')]\n    encoded_data = storage._encode(messages)\n    storage._update_cookie(encoded_data, response)\n    self.assertIn(CookieStorage.cookie_name, response.cookies)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name].value, encoded_data)\n\n    # Test deleting the cookie when there is no data to store\n    storage._update_cookie(None, response)\n    self.assertIn(CookieStorage.cookie_name, response.cookies)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n"], "sample_298": ["def test_check_token_with_invalid_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    ts_b36, _ = tk1.split(\"-\")\n    # Try to parse an invalid timestamp\n    invalid_ts = 'abcdefg'  # invalid base36\n    token = f'{invalid_ts}-{tk1.split(\"-\")[1]}'\n    self.assertIs(p0.check_token(user, token), False)\n\n    # Try to use a timestamp that is too large\n    ts = base36_to_int(ts_b36) + (60 * 60 * 24 * 365 * 100)  # add 100 years\n    token = f'{int_to_base36(ts)}-{tk1.split(\"-\")[1]}'\n    self.assertIs(p0.check_token(user, token), False)\n", "def test_token_with_invalid_input(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n\n    # Test with invalid token format\n    self.assertIs(p0.check_token(user, 'invalid-token'), False)\n\n    # Test with token that has been tampered with\n    ts_b36, _ = tk1.split(\"-\")\n    tampered_token = f\"{ts_b36}-tampered-hash\"\n    self.assertIs(p0.check_token(user, tampered_token), False)\n\n    # Test with token that has an invalid timestamp\n    invalid_ts_b36 = \" invalid-ts\"\n    invalid_token = f\"{invalid_ts_b36}-{tk1.split('-')[1]}\"\n    self.assertIs(p0.check_token(user, invalid_token), False)\n", "def test_check_token_with_invalid_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    # Parse the token\n    ts_b36, _ = tk1.split(\"-\")\n    # Try to increment the timestamp by 100 years\n    ts_b36 = int_to_base36(base36_to_int(ts_b36) + 3153600000)\n    # Reconstruct the token with the invalid timestamp\n    invalid_tk = \"%s-%s\" % (ts_b36, tk1.split(\"-\")[1])\n    self.assertIs(p0.check_token(user, invalid_tk), False)\n", "def test_token_with_invalid_input(self):\n    \"\"\"\n    The token generation and checking should fail with invalid input.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n\n    # Check with an empty string as the token\n    self.assertIs(p0.check_token(user, ''), False)\n\n    # Check with a token that's not a string\n    self.assertIs(p0.check_token(user, 123), False)\n\n    # Check with a token that's split into two parts incorrectly\n    self.assertIs(p0.check_token(user, tk1.replace('-', '.')), False)\n\n    # Check with a token that has an invalid base36 timestamp\n    ts_b36, _ = tk1.split(\"-\")\n    invalid_ts_b36 = ts_b36 + 'a'\n    invalid_tk1 = f'{invalid_ts_b36}-{tk1.split(\"-\")[1]}'\n    self.assertIs(p0.check_token(user, invalid_tk1), False)\n", "def test_make_hash_value(self):\n    user = User.objects.create_user('hashvaluetestuser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = p0._num_seconds(p0._now())\n    hash_value = p0._make_hash_value(user, timestamp)\n    expected_hash_value = f'{user.pk}{user.password}{user.last_login.replace(microsecond=0, tzinfo=None)}{timestamp}{user.email}'\n    self.assertEqual(hash_value, expected_hash_value)\n\n    # Test with last_login being None\n    user.last_login = None\n    user.save()\n    hash_value = p0._make_hash_value(user, timestamp)\n    expected_hash_value = f'{user.pk}{user.password}\"\"{timestamp}{user.email}'\n    self.assertEqual(hash_value, expected_hash_value)\n"], "sample_299": ["    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path',\n            },\n        }):\n            warning = Warning(\n                \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\",\n                id='caches.W003',\n            )\n            self.assertEqual(check_file_based_cache_is_absolute(None), [warning])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',  # relative path\n            },\n        }):\n            warning = Warning(\n                \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\",\n                id='caches.W003',\n            )\n            self.assertEqual(check_file_based_cache_is_absolute(None), [warning])\n", "    def test_non_file_based_cache(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',  # relative path\n            },\n        }):\n            warning = Warning(\n                \"Your 'default' cache LOCATION path is relative. Use an absolute path instead.\",\n                id='caches.W003',\n            )\n            self.assertEqual(check_file_based_cache_is_absolute(None), [warning])\n", "    def test_non_existent_absolute_path(self):\n        non_existent_path = pathlib.Path.cwd() / 'non_existent_directory'\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': non_existent_path,\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    f\"Your 'default' cache LOCATION path is relative. Use an \"\n                    f\"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n"], "sample_300": ["def test_build_lookup_with_expression(self):\n    query = Query(Item)\n    expr = F('name')\n    lookup = query.build_lookup(['exact'], expr, 'foo')\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.lhs, expr)\n    self.assertEqual(lookup.rhs, 'foo')\n", "def test_build_filter_with_isnull(self):\n    query = Query(Author)\n    where = query.build_where(Q(name__isnull=True))\n    lookup = where.children[0]\n    self.assertIsInstance(lookup, IsNull)\n    self.assertEqual(lookup.rhs, True)\n    self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n", "def test_filter_conditional_m2m_join(self):\n    query = Query(Item)\n    filter_expr = Func('categories__name', output_field=BooleanField())\n    msg = 'Joined field references are not permitted in this query'\n    with self.assertRaisesMessage(FieldError, msg):\n        query.build_where(filter_expr)\n", "def test_query_set_limits(self):\n    query = Query(Item)\n    query.set_limits(low=10, high=20)\n    self.assertEqual(query.low_mark, 10)\n    self.assertEqual(query.high_mark, 20)\n\n    query.clear_limits()\n    self.assertEqual(query.low_mark, 0)\n    self.assertIsNone(query.high_mark)\n\n    query.set_limits(low=5)\n    self.assertEqual(query.low_mark, 5)\n    self.assertIsNone(query.high_mark)\n\n    query.set_limits(high=15)\n    self.assertEqual(query.low_mark, 5)\n    self.assertEqual(query.high_mark, 15)\n\n    query.set_limits(low=25)\n    self.assertEqual(query.low_mark, 5)\n    self.assertEqual(query.high_mark, 15)\n", "def test_build_filter_check_filterable(self):\n    query = Query(Item)\n    with self.assertRaisesMessage(NotSupportedError, \"Func is disallowed in the filter clause.\"):\n        query.build_where(Func(output_field=BooleanField()))\n"], "sample_301": ["    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_non_tty(self, mock_isatty):\n        autoreload.ensure_echo_on()\n        self.assertFalse(mock_isatty.called)\n", "    def test_echo_on_called(self):\n        with mock.patch('django.utils.autoreload.termios') as mocked_termios:\n            autoreload.ensure_echo_on()\n            self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n            self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_ensure_echo_on_called(self, mocked_termios):\n        autoreload.ensure_echo_on()\n        self.assertTrue(mocked_termios.tcgetattr.called)\n        self.assertTrue(mocked_termios.tcsetattr.called)\n", "    def test_base_reloader_should_stop(self):\n        reloader = autoreload.BaseReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n"], "sample_302": ["    def test_settings_to_cmd_args_env_minimal(self):\n        settings_dict = {\n            'NAME': 'mydb',\n            'USER': 'myuser',\n            'PASSWORD': 'mypassword',\n            'HOST': 'localhost',\n            'PORT': 5432,\n        }\n        parameters = []\n        client = DatabaseClient(connection=None)\n        args, env = client.settings_to_cmd_args_env(settings_dict, parameters)\n        self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n        self.assertEqual(env, {'PGPASSWORD': 'mypassword'})\n", "    def setUp(self):\n        self.client = DatabaseClient(connection=connection)\n", "    def setUp(self):\n        self.client = DatabaseClient(connection=connection)\n", "    def setUp(self):\n        self.client = DatabaseClient(connection=connection)\n", "    def setUp(self):\n        self.client = DatabaseClient(connection=connection)\n"], "sample_303": ["def test_runshell(self):\n    with mock.patch('subprocess.run') as mock_run:\n        with self.assertRaises(NotImplementedError):\n            self.client.runshell(None)\n        mock_run.assert_not_called()\n", "def test_runshell_raises_not_implemented_error(self):\n    msg = (\n        'subclasses of BaseDatabaseClient must provide a '\n        'settings_to_cmd_args_env() method or override a runshell().'\n    )\n    with self.assertRaisesMessage(NotImplementedError, msg):\n        self.client.runshell(None)\n", "def test_runshell_raises_not_implemented_error(self):\n    msg = (\n        'subclasses of BaseDatabaseClient must provide a '\n        'settings_to_cmd_args_env() method or override a runshell().'\n    )\n    with self.assertRaisesMessage(NotImplementedError, msg):\n        self.client.runshell(None)\n", "def test_runshell_raises_not_implemented_error(self):\n    msg = (\n        'subclasses of BaseDatabaseClient must provide a '\n        'settings_to_cmd_args_env() method or override a runshell().'\n    )\n    with self.assertRaisesMessage(NotImplementedError, msg):\n        self.client.runshell(None)\n", "def test_runshell_raises_not_implemented_error(self):\n    msg = (\n        'subclasses of BaseDatabaseClient must provide a '\n        'settings_to_cmd_args_env() method or override a runshell().'\n    )\n    with self.assertRaisesMessage(NotImplementedError, msg):\n        self.client.runshell(None)\n"], "sample_304": ["    def test_repr(self):\n        # Check that __repr__ can be successfully called on all validators\n        for validator, value, expected in TEST_DATA:\n            repr(validator)\n", "    def test_limit_value_callable(self):\n        class DynamicLimit:\n                return 10\n\n        limit = DynamicLimit()\n        validator = MinValueValidator(limit_value=limit)\n        self.assertEqual(validator.limit_value(), 10)\n", "    def test_domain_allowlist(self):\n        validator = EmailValidator(allowlist=['localdomain'])\n        self.assertEqual(validator.domain_allowlist, ['localdomain'])\n        self.assertIsNone(validator('email@localdomain'))\n", "    def test_allowlist(self):\n        validator = EmailValidator(allowlist=['localdomain'])\n        self.assertIsNone(validator('email@localdomain'))\n", "    def test_regex_validator(self):\n        validator = RegexValidator(r'^[a-z]+$')\n        with self.assertRaisesMessage(ValidationError, 'This field is required.'):\n            validator('')\n"], "sample_305": ["    def test_lookup_inheritance(self):\n        # Ensure lookups are inherited correctly when defining a subclass\n        # with a custom lookup name.\n        class MyExact(IExact):\n            lookup_name = 'myexact'\n\n        self.assertEqual(MyExact.lookup_name, 'myexact')\n        self.assertEqual(MyExact.prepare_rhs, False)\n", "def test_distinct_with_annotate(self):\n    # Test that annotate() works correctly with distinct()\n    books = Book.objects.annotate(num_authors=Count('authors')).distinct().order_by('name')\n    self.assertSequenceEqual(\n        books.values_list('name', 'num_authors'),\n        [\n            ('Artificial Intelligence: A Modern Approach', 2),\n            ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 1),\n            ('Practical Django Projects', 1),\n            ('Python Web Development with Django', 3),\n            ('Sams Teach Yourself Django in 24 Hours', 1),\n            ('The Definitive Guide to Django: Web Development Done Right', 2)\n        ]\n    )\n", "def test_aggregate_over_expression(self):\n    # Aggregates can be used over expressions\n    vals = Book.objects.aggregate(total_price=Sum(F('price') * 2))\n    self.assertEqual(vals, {\n        'total_price': Decimal('372.74')\n    })\n\n    vals = Publisher.objects.annotate(\n        total_price=Sum(F('book__price') * 2)\n    ).values_list('name', 'total_price').order_by('name')\n    self.assertSequenceEqual(vals, [\n        ('Apress', Decimal('118.38')),\n        (\"Jonno's House of Books\", None),\n        ('Morgan Kaufmann', Decimal('150.00')),\n        ('Prentice Hall', Decimal('332.80')),\n        ('Sams', Decimal('46.18'))\n    ])\n", "    def setUpTestData(cls):\n        cls.b1 = Book.objects.create(\n            isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n            pages=447, rating=4.5, price=Decimal('30.00'), pubdate=datetime.date(2007, 12, 6)\n        )\n        cls.b2 = Book.objects.create(\n            isbn='067232959', name='Sams Teach Yourself Django in 24 Hours',\n            pages=528, rating=3.0, price=Decimal('23.09'), pubdate=datetime.date(2008, 3, 3)\n        )\n", "def test_annotate_with_expression(self):\n    # Test that annotations with expressions are correctly evaluated.\n    qs = Book.objects.annotate(double_price=F('price') * 2).order_by('double_price')\n    self.assertEqual(len(qs), 6)\n    for book in qs:\n        self.assertEqual(book.double_price, book.price * 2)\n"], "sample_306": ["def test_parse_duration_iso8601(self):\n    test_values = (\n        ('P4DT12H30M5S', timedelta(days=4, hours=12, minutes=30, seconds=5)),\n        ('PT12H30M5S', timedelta(hours=12, minutes=30, seconds=5)),\n        ('P4D', timedelta(days=4)),\n        ('PT1H', timedelta(hours=1)),\n        ('PT1M', timedelta(minutes=1)),\n        ('PT1S', timedelta(seconds=1)),\n        ('-P4DT12H30M5S', timedelta(days=-4, hours=-12, minutes=-30, seconds=-5)),\n        ('-PT12H30M5S', timedelta(hours=-12, minutes=-30, seconds=-5)),\n        ('-P4D', timedelta(days=-4)),\n        ('-PT1H', timedelta(hours=-1)),\n        ('-PT1M', timedelta(minutes=-1)),\n        ('-PT1S', timedelta(seconds=-1)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration_parsing(self):\n    test_values = (\n        ('P3Y6M4DT12H30M5S', timedelta(days=1368, hours=12, minutes=30, seconds=5)),\n        ('P23DT23H', timedelta(days=23, hours=23)),\n        ('P4Y', timedelta(days=1461)),\n        ('PT1H2M3S', timedelta(hours=1, minutes=2, seconds=3)),\n        ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n        ('-P1DT2H3M4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n        ('P0D', timedelta()),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration_parsing(self):\n    test_values = (\n        ('P3DT12H30M5S', timedelta(days=3, hours=12, minutes=30, seconds=5)),\n        ('PT12H30M5S', timedelta(hours=12, minutes=30, seconds=5)),\n        ('P3DT12H30M', timedelta(days=3, hours=12, minutes=30)),\n        ('PT12H30M', timedelta(hours=12, minutes=30)),\n        ('P3DT12H', timedelta(days=3, hours=12)),\n        ('PT12H', timedelta(hours=12)),\n        ('P3D', timedelta(days=3)),\n        ('PT5S', timedelta(seconds=5)),\n        ('-P3DT12H30M5S', timedelta(days=-3, hours=-12, minutes=-30, seconds=-5)),\n        ('-PT12H30M5S', timedelta(hours=-12, minutes=-30, seconds=-5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_duration_iso8601(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('PT3H', timedelta(hours=3)),\n        ('PT3M', timedelta(minutes=3)),\n        ('PT3S', timedelta(seconds=3)),\n        ('P3DT12H30M5S', timedelta(days=3, hours=12, minutes=30, seconds=5)),\n        ('-P3D', timedelta(days=-3)),\n        ('-PT3H', timedelta(hours=-3)),\n        ('-PT3M', timedelta(minutes=-3)),\n        ('-PT3S', timedelta(seconds=-3)),\n        ('-P3DT12H30M5S', timedelta(days=-3, hours=-12, minutes=-30, seconds=-5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration_parsing(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('PT4H', timedelta(hours=4)),\n        ('PT5M', timedelta(minutes=5)),\n        ('PT6S', timedelta(seconds=6)),\n        ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n        ('P-1DT-2H-3M-4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n        ('PT0.5S', timedelta(milliseconds=500)),\n        ('PT0,5S', timedelta(milliseconds=500)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n"], "sample_307": ["def test_E_format(self):\n    # Test alternative month names as required by some locales.\n    dt = datetime(2022, 5, 16)\n    self.assertEqual(dateformat.format(dt, 'E'), MONTHS_ALT[dt.month])\n", "def test_E_format_alternative_month_names(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, 'E'), 'July')\n    with translation.override('fr'):\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'juillet')\n", "def test_E_format_alternative_month_names(self):\n    dt = datetime(2022, 5, 1)\n    self.assertEqual(dateformat.format(dt, 'E'), MONTHS_ALT[dt.month])\n", "def test_E_format_month_names(self):\n    # Test alternative month names as required by some locales.\n    dt = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(dt, 'E'), MONTHS_ALT[dt.month])\n", "def test_Z_format_with_imaginary_datetime(self):\n    # dt is imaginary in Europe/Copenhagen. pytz raises an exception for\n    # the ambiguity, which results in an empty string.\n    dt = datetime(2015, 3, 29, 2, 30, 0)\n\n    # Try Z format specifier that involves self.timezone.\n    self.assertEqual(format(dt, 'Z'), '')\n"], "sample_308": ["def test_Z_format_with_imaginary_datetime(self):\n    # dt is imaginary in Europe/Copenhagen. pytz raises an exception for\n    # the ambiguity, which results in an empty string.\n    dt = datetime(2015, 3, 29, 2, 30, 0)\n\n    # Try Z format specifier that involves self.timezone.\n    self.assertEqual(format(dt, 'Z'), '')\n", "def test_weekday_names(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n\n    self.assertEqual(dateformat.format(my_birthday, 'D'), 'Sun')\n    self.assertEqual(dateformat.format(my_birthday, 'l'), 'Sunday')\n\n    # Test weekday names in different locales\n    with translation.override('fr'):\n        self.assertEqual(dateformat.format(my_birthday, 'D'), 'dim.')\n        self.assertEqual(dateformat.format(my_birthday, 'l'), 'dimanche')\n\n    with translation.override('de'):\n        self.assertEqual(dateformat.format(my_birthday, 'D'), 'So')\n        self.assertEqual(dateformat.format(my_birthday, 'l'), 'Sonntag')\n", "def test_f_format(self):\n    dt = datetime(2022, 9, 8, 5, 30)\n    self.assertEqual(dateformat.format(dt, 'f'), '5:30')\n\n    dt = datetime(2022, 9, 8, 5, 0)\n    self.assertEqual(dateformat.format(dt, 'f'), '5')\n", "def test_o_format_year_before_1000(self):\n    self.assertEqual(dateformat.format(datetime(1, 1, 1), 'o'), '0001')\n    self.assertEqual(dateformat.format(datetime(999, 1, 1), 'o'), '0999')\n    self.assertEqual(dateformat.format(datetime(1979, 7, 8, 22, 00), 'o'), '1979')\n", "def test_E_format(self):\n    my_birthday = datetime(1979, 7, 8, 22, 00)\n    self.assertEqual(dateformat.format(my_birthday, 'E'), 'July')\n    with translation.override('fr'):\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'juillet')\n"], "sample_309": ["    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('example.com'), 'example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2F/example.com')\n        self.assertEqual(escape_leading_slashes('/example.com'), '/example.com')\n        self.assertEqual(escape_leading_slashes('example.com'), 'example.com')\n", "    def test_escape_leading_slashes(self):\n        url = '//example.com'\n        self.assertEqual(escape_leading_slashes(url), '/%2Fexample.com')\n", "    def test_escape_leading_slashes(self):\n        url = '//example.com/path'\n        self.assertEqual(escape_leading_slashes(url), '/%2Fexample.com/path')\n"], "sample_310": ["    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        expected = \"/<sport_slug>/athletes/<athlete_slug>/\"\n        self.assertEqual(simplify_regex(pattern), expected)\n", "    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^$'), '/')\n        self.assertEqual(simplify_regex(r'^example/$'), '/example/')\n        self.assertEqual(simplify_regex(r'^example/(?P<slug>\\w+)/$'), '/example/<slug>/')\n", "    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, \"/<sport_slug>/athletes/<athlete_slug>/\")\n"], "sample_312": ["def test_add_squash_node(self):\n    node1 = Node([('a', 1), ('b', 2)])\n    node2 = Node([('c', 3), ('d', 4)], connector='DEFAULT')\n    added_node = node1.add(node2, Node.default)\n    self.assertEqual(added_node, node2)\n    self.assertEqual(len(node1), 4)\n    self.assertEqual(str(node1), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "def test_add_squash_node_children(self):\n    node = Node([('a', 1), ('b', 2)], connector='OR')\n    node_to_add = Node([('c', 3), ('d', 4)], connector='OR')\n    node.add(node_to_add, 'OR')\n    self.assertEqual(len(node), 4)\n    self.assertEqual(str(node), \"(OR: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "def test_add_squash(self):\n    node = Node([('a', 1), ('b', 2)], 'OR')\n    node_to_add = Node([('c', 3), ('d', 4)], 'OR')\n    added_node = node.add(node_to_add, 'OR')\n    self.assertEqual(added_node, node_to_add)\n    self.assertEqual(len(node), 4)\n    self.assertEqual(str(node), \"(OR: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "def test_add_squash(self):\n    node = Node([('a', 1), ('b', 2)], connector='AND')\n    node_to_add = Node([('c', 3), ('d', 4)], connector='AND')\n    node.add(node_to_add, 'AND')\n    self.assertEqual(len(node), 4)\n    self.assertEqual(str(node), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n\n    # Test that adding a single child doesn't change the connector\n    node_single_child = Node([('e', 5)])\n    node.add(node_single_child, 'OR')\n    self.assertEqual(len(node), 5)\n    self.assertEqual(str(node), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4), (OR: ('e', 5)))\")\n", "def test_add_squash_single_child_node(self):\n    node1 = Node([('a', 1)], 'OR')\n    node2 = Node([('b', 2)])\n    node1.add(node2, 'OR')\n    self.assertEqual(len(node1.children), 2)\n    self.assertEqual(str(node1), \"(OR: ('a', 1), ('b', 2))\")\n"], "sample_313": ["def test_get_template_directories_filters_django_paths(self, mock_is_django_path):\n    mock_is_django_path.return_value = True\n    with override_settings(\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [ROOT / 'templates'],\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.request',\n                ],\n                'loaders': [\n                    'django.template.loaders.filesystem.Loader',\n                    'django.template.loaders.app_directories.Loader',\n                ]\n            },\n        }]\n    ):\n        self.assertSetEqual(autoreload.get_template_directories(), set())\n", "def test_get_template_directories_with_non_django_templates(self, mock_to_path):\n    mock_engine = mock.MagicMock()\n    mock_engine.dirs = ['non_django_dir']\n    engines.all.return_value = [DjangoTemplates(mock_engine)]\n\n    autoreload.get_template_directories()\n\n    mock_to_path.assert_not_called()\n", "def test_get_template_directories_loader_get_dirs(self, mock_get_dirs):\n    mock_get_dirs.return_value = ['/loader/dir1', '/loader/dir2']\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            ROOT / 'templates_extra',\n            ROOT / 'templates',\n            Path.cwd() / 'loader/dir1',\n            Path.cwd() / 'loader/dir2',\n        }\n    )\n", "    def test_reset_locmem_loader(self, mock_reset):\n        autoreload.reset_loaders()\n        mock_reset.assert_called_once()\n", "def test_get_template_directories_excludes_django_templates(self, mock_is_django_path):\n    mock_is_django_path.return_value = True\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        set()\n    )\n    mock_is_django_path.assert_called()\n"], "sample_314": ["    def test_get_context(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5udm0='\n        context = widget.get_context(name='password', value=value, attrs={'id': 'id_password'})\n        self.assertEqual(context['summary'], [\n            {'label': _('algorithm'), 'value': 'pbkdf2_sha256'},\n            {'label': _('iterations'), 'value': 100000},\n            {'label': _('salt'), 'value': 'a6Pucb******'},\n            {'label': _('hash'), 'value': 'WmCkn9**************************************'}\n        ])\n", "    def test_custom_user_model(self):\n        user = CustomUser.objects.get(username='testclient')\n        data = {'username': 'testclient', 'email': 'testclient@example.com'}\n        form = UserChangeForm(data, instance=user)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(form.cleaned_data['username'], 'testclient')\n        self.assertEqual(form.cleaned_data['email'], 'testclient@example.com')\n", "    def test_custom_username_field(self):\n        class CustomUserCreationForm(UserCreationForm):\n            class Meta(UserCreationForm.Meta):\n                model = CustomUser\n                fields = ('email',)\n\n        data = {\n            'email': 'test@example.com',\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = CustomUserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.email, data['email'])\n", "    def test_custom_user_email(self):\n        user = CustomUser.objects.create_user('testclient', email='testclient@example.com', password='password')\n        data = {'email': 'testclient@example.com'}\n        form = PasswordResetForm(data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, ['testclient@example.com'])\n", "    def test_changed_data(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.changed_data, ['password'])\n"], "sample_315": ["    def test_language_activated(self):\n        request = RequestFactory().get('/en/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        response = middleware.process_request(request)\n        self.assertIsNone(response)\n        self.assertEqual(translation.get_language(), 'en')\n        response = HttpResponse()\n        response = middleware.process_response(request, response)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers['content-language'], 'en')\n", "    def test_process_request_language_from_path(self):\n        request = RequestFactory().get('/nl/account/register/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        with translation.override('en'):\n            middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def test_vary_header(self):\n        response = self.client.get('/en/account/register/')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('Vary', response.headers)\n        self.assertEqual(response.headers['Vary'], 'Accept-Language')\n", "    def test_language_prefix_with_subdomain(self):\n        response = self.client.get('/prefixed/', HTTP_ACCEPT_LANGUAGE='en', HTTP_HOST='example.com')\n        self.assertRedirects(response, 'http://en.example.com/prefixed/')\n\n        response = self.client.get('/prefixed/', HTTP_ACCEPT_LANGUAGE='nl', HTTP_HOST='example.com')\n        self.assertRedirects(response, 'http://nl.example.com/prefixed/')\n\n        response = self.client.get('/prefixed/', HTTP_HOST='en.example.com')\n        self.assertEqual(response.status_code, 404)\n\n        response = self.client.get('/prefixed/', HTTP_HOST='nl.example.com')\n        self.assertEqual(response.status_code, 404)\n", "    def test_request_language_code_set(self):\n        request = RequestFactory().get('/en/account/register/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'en')\n"], "sample_316": ["    def test_image_file_properties(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n", "    def test_width_and_height_properties(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n", "    def test_image_file_properties(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, Image.open(fh).size[0])\n            self.assertEqual(image_file.height, Image.open(fh).size[1])\n"], "sample_317": ["def test_rss2_feed_with_categories(self):\n    \"\"\"\n    Test the structure and content of feeds generated by Rss201rev2Feed\n    with categories.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/categories/')\n    doc = minidom.parseString(response.content)\n    chan = doc.getElementsByTagName('rss')[0].getElementsByTagName('channel')[0]\n    items = chan.getElementsByTagName('item')\n    for item in items:\n        categories = item.getElementsByTagName('category')\n        self.assertEqual(len(categories), 2)\n        self.assertEqual(\n            {c.firstChild.wholeText for c in categories},\n            {'Category 1', 'Category 2'}\n        )\n", "def test_latest_post_date_with_no_items(self):\n    \"\"\"\n    Test that the latest post date is the current date when there are no items.\n    \"\"\"\n    feed = Rss201rev2Feed(\n        title='My blog',\n        link='http://example.com/blog/',\n        description='A more thorough description of my blog.',\n    )\n    self.assertEqual(feed.latest_post_date(), datetime.datetime.now(tz=timezone.utc))\n", "def test_rss2_feed_with_categories(self):\n    \"\"\"\n    Test that RSS 2 feeds with categories generate correctly.\n    \"\"\"\n    response = self.client.get('/syndication/rss2/categories/')\n    doc = minidom.parseString(response.content)\n\n    # Check that the channel has the correct categories\n    chan = doc.getElementsByTagName('channel')[0]\n    self.assertCategories(chan, ['python', 'django'])\n\n    # Check that each item has the correct categories\n    items = chan.getElementsByTagName('item')\n    for i, item in enumerate(items):\n        if i == 0:\n            self.assertCategories(item, ['python', 'testing'])\n        else:\n            self.assertCategories(item, ['python'])\n", "def test_get_tag_uri(self):\n    \"\"\"\n    The get_tag_uri() function correctly generates TagURIs.\n    \"\"\"\n    url = \"http://example.com/entries/123/\"\n    date = datetime.date(2022, 9, 1)\n    tag_uri = get_tag_uri(url, date)\n    self.assertEqual(tag_uri, \"tag:example.com,2022-09-01:/entries/123/\")\n", "def test_latest_post_date_no_items(self):\n    \"\"\"\n    Test that the latest post date returns the current UTC date/time when there are no items.\n    \"\"\"\n    feed = Rss201rev2Feed(\n        title=\"My blog\",\n        link=\"http://example.com/blog/\",\n        description=\"A more thorough description of my blog.\",\n    )\n    self.assertEqual(feed.latest_post_date(), datetime.datetime.now(tz=utc))\n"], "sample_318": ["    def test_pattern_name_check(self):\n        msg = (\n            \"Your URL pattern %s has a name including a ':'. Remove the colon, to \"\n            \"avoid ambiguous namespace references.\"\n        )\n        pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test:name')\n        with self.assertRaisesMessage(Warning, msg % pattern.describe()):\n            pattern.check()\n", "    def test_reset_urlconf(self):\n        \"\"\"\n        The URL resolver cache is reset when the URLconf is changed.\n        \"\"\"\n        old_resolver = get_resolver()\n        new_resolver = get_resolver('urlpatterns_reverse.urls')\n        self.assertNotEqual(old_resolver, new_resolver)\n\n        # Reset to the original URLconf\n        get_resolver(None)\n        current_resolver = get_resolver()\n        self.assertEqual(old_resolver.urlconf_module, current_resolver.urlconf_module)\n", "    def test_describe(self):\n        pattern = RegexPattern(r'^foo/$', name='foo')\n        self.assertEqual(pattern.describe(), \"'foo' [name='foo']\")\n", "    def test_resolver_match_namespace_not_in_pattern(self):\n        resolver = get_resolver('urlpatterns_reverse.namespace_urls')\n        match = resolver.resolve('/test1/inner/42/37/')\n        self.assertEqual(match.namespace, 'test-ns1')\n        self.assertEqual(match.namespaces, ['test-ns1'])\n        self.assertEqual(match.app_name, 'testapp')\n        self.assertEqual(match.app_names, ['testapp'])\n", "    def test_regex_compilation(self):\n        pattern = RegexPattern(r'^test/')\n        locale_regex_descriptor = LocaleRegexDescriptor('regex')\n        compiled_regex = locale_regex_descriptor.__get__(pattern)\n        self.assertIsInstance(compiled_regex, re.Pattern)\n"], "sample_321": ["def test_rotate_token(self):\n    \"\"\"\n    The rotate_token function updates the CSRF token in the session and sets\n    the 'Vary: Cookie' header to ensure the response is not cached.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    resp = HttpResponse()\n    rotate_token(req)\n    self.assertNotEqual(req.session.get(CSRF_SESSION_KEY), None)\n    self.assertIn('Cookie', resp.get('Vary', ''))\n", "def test_process_response_get_token_used_with_secure_cookie(self):\n    \"\"\"\n    If get_token() is called, the view middleware adds a secure CSRF cookie.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    req._is_secure_override = True\n    mw = CsrfViewMiddleware(ensure_csrf_cookie_view)\n    mw.process_view(req, ensure_csrf_cookie_view, (), {})\n    resp = mw(req)\n    csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n    self.assertIsNotNone(csrf_cookie)\n    self.assertTrue(csrf_cookie['secure'])\n", "def test_csrf_trusted_origin_allows_subdomains(self):\n    \"\"\"\n    A POST request with an origin that matches a CSRF_TRUSTED_ORIGINS\n    wildcard is accepted, including subdomains.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://subdomain.dashboard.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    with override_settings(\n        ALLOWED_HOSTS=['www.example.com'],\n        CSRF_TRUSTED_ORIGINS=['https://*.dashboard.example.com'],\n    ):\n        self.assertIs(mw._origin_verified(req), True)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n", "def test_csrf_trusted_origins_wildcard_allowed(self):\n    \"\"\"\n    A POST request with an origin that matches a CSRF_TRUSTED_ORIGINS wildcard\n    is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req._is_secure_override = True\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://subdomain.example.co.uk'\n    mw = CsrfViewMiddleware(post_form_view)\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.example.co.uk']):\n        response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_rotate_token(self):\n    \"\"\"\n    The rotate_token() function changes the CSRF token in use for a request.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    token1 = get_token(req)\n    rotate_token(req)\n    token2 = get_token(req)\n    self.assertNotEqual(token1, token2)\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n    self._check_token_present(resp, csrf_id=token2)\n"], "sample_322": ["def test_check_replacements(self):\n    \"\"\"\n    Check that replacement migrations are marked as applied when their replaced\n    migrations are all applied.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Mark the replaced migrations as applied\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    # Run the check_replacements method\n    executor.check_replacements()\n    # The replacement migration should be marked as applied\n    self.assertIn(\n        (\"migrations\", \"0001_squashed_0002\"),\n        recorder.applied_migrations(),\n    )\n", "def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Test that the migration plan correctly handles replaced migrations.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    squash_impl = FakeMigration('squash')\n    squash = ('a', 'squash')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(squash, squash_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_replacement(squash, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n\n    plan = executor.migration_plan({squash})\n\n    self.assertEqual(plan, [(squash_impl, False)])\n", "def test_create_project_state(self):\n    \"\"\"\n    Tests that the _create_project_state method correctly includes all\n    the applications without migrations and applied migrations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    state = executor._create_project_state(with_applied_migrations=True)\n\n    # Assert that the state includes all the applications without migrations\n    self.assertIn('migrations', state.apps)\n    self.assertIn('django.contrib.auth', state.apps)\n    self.assertIn('django.contrib.contenttypes', state.apps)\n\n    # Assert that the state includes all the applied migrations\n    migration_recorder = MigrationRecorder(connection)\n    applied_migrations = migration_recorder.applied_migrations()\n    for app_label, name in applied_migrations:\n        self.assertIn((app_label, name), state.applied_migrations)\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Replaced migrations are properly handled in the migration plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Fake-apply all migrations\n    executor.migrate([(\"migrations\", \"0002_second\")], fake=True)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Replace a migration\n    executor.loader.replace_migrations = True\n    # Make a plan to migrate backwards\n    plan = executor.migration_plan([(\"migrations\", None)])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], True),\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], True),\n        ],\n    )\n", "def test_migration_plan_with_replacement(self):\n    \"\"\"\n    Test migration plan with replacement migrations.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    squash_impl = FakeMigration('squash_1_2')\n    squash = ('a', 'squash_1_2')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(squash, squash_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_replacement(squash, [a1, a2])\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        a2: a2_impl,\n    })\n\n    plan = executor.migration_plan({squash})\n\n    self.assertEqual(plan, [(squash_impl, False)])\n"], "sample_323": ["def test_migration_plan_with_missing_dependencies(self):\n    \"\"\"\n    Tests that migration plan raises an error when a dependency is missing.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Fake-apply 0001_initial, but not its dependency\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n    executor.loader.build_graph()\n    # Remove the applied migration record for the dependency\n    executor.recorder.record_unapplied(\"migrations\", \"0001_initial\")\n    # Now try to migrate to 0002_second\n    with self.assertRaisesMessage(InvalidMigrationPlan, \"unfulfilled dependencies\"):\n        executor.migration_plan([(\"migrations\", \"0002_second\")])\n", "def test_check_replacements(self):\n    \"\"\"\n    Mark replacement migrations applied if their replaced set all are.\n\n    Do this unconditionally on every migrate, rather than just when\n    migrations are applied or unapplied.\n    \"\"\"\n    graph = MigrationGraph()\n    applied = {\n        ('a', '0001_initial'): FakeMigration('a1'),\n        ('a', '0002_second'): FakeMigration('a2'),\n    }\n    loader = FakeLoader(graph, applied)\n    executor = MigrationExecutor(None)\n    executor.loader = loader\n\n    # Test the case where all replaced migrations are applied\n    replacements = {\n        ('a', '0003_squashed_0002'): FakeMigration('a3')\n    }\n    loader.replacements = replacements\n    executor.check_replacements()\n    self.assertIn(('a', '0003_squashed_0002'), applied)\n\n    # Test the case where not all replaced migrations are applied\n    applied.pop(('a', '0002_second'))\n    executor.check_replacements()\n    self.assertNotIn(('a', '0003_squashed_0002'), applied)\n", "def test_migration_plan_with_empty_targets(self):\n    \"\"\"\n    Tests running a migration plan with empty targets.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    plan = executor.migration_plan([])\n    self.assertEqual(plan, [])\n    # The resulting state should include applied migrations.\n    state = executor.migrate([])\n    self.assertIsNotNone(state)\n", "def test_check_replacements(self):\n    \"\"\"\n    Mark replacement migrations applied if their replaced set all are.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Record individual replacements as applied\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    # Build the graph to ensure we have nodes\n    executor.loader.build_graph()\n    # Check replacements should mark the squashed migration as applied\n    executor.check_replacements()\n    self.assertIn(\n        (\"migrations\", \"0001_squashed_0002\"),\n        recorder.applied_migrations(),\n    )\n", "def test_migration_plan_with_missing_dependencies(self):\n    \"\"\"\n    Tests migration plan with missing dependencies.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Make the initial plan, check it\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Remove a dependency\n    del executor.loader.graph.nodes[\"migrations\", \"0001_initial\"]\n    # Make a new plan, should raise an error\n    with self.assertRaises(KeyError):\n        executor.migration_plan([(\"migrations\", \"0002_second\")])\n"], "sample_324": ["def test_wildcard_trusted_origin(self):\n    \"\"\"\n    A POST request with an origin that matches a wildcard in CSRF_TRUSTED_ORIGINS\n    is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_HOST'] = 'www.example.com'\n    req.META['HTTP_ORIGIN'] = 'https://foo.example.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n    self.assertEqual(mw.allowed_origins_exact, set())\n    self.assertEqual(mw.allowed_origin_subdomains, {'https': ['.example.com']})\n", "def test_rotate_token(self):\n    \"\"\"\n    The rotate_token function should change the CSRF token in the session and\n    the CSRF cookie.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    token_before = req.session.get(CSRF_SESSION_KEY)\n    rotate_token(req)\n    token_after = req.session.get(CSRF_SESSION_KEY)\n    self.assertNotEqual(token_before, token_after)\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw(req)\n    self.assertEqual(resp.cookies[settings.CSRF_COOKIE_NAME].value, token_after)\n", "def test_csrf_cookie_set_on_response(self):\n    \"\"\"\n    The CSRF cookie is set on the response when it's not already set.\n    \"\"\"\n    req = self._get_GET_no_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    resp = mw(req)\n    self.assertIn(settings.CSRF_COOKIE_NAME, resp.cookies)\n", "def test_csrf_token_in_header_with_multiple_values(self):\n    \"\"\"\n    The token may be passed in a header with multiple values instead of in the form.\n    \"\"\"\n    req = self._get_POST_csrf_cookie_request(meta_token=f'{self._csrf_id},another-value')\n    mw = CsrfViewMiddleware(post_form_view)\n    mw.process_request(req)\n    resp = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(resp)\n", "def test_rotate_token(self):\n    \"\"\"\n    The rotate_token() function changes the CSRF token in use for a request.\n    \"\"\"\n    req = self._get_GET_csrf_cookie_request()\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    initial_csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, None)\n\n    rotate_token(req)\n    mw = CsrfViewMiddleware(token_view)\n    mw.process_request(req)\n    mw.process_view(req, token_view, (), {})\n    resp = mw(req)\n    rotated_csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, None)\n\n    self.assertNotEqual(initial_csrf_cookie.value, rotated_csrf_cookie.value)\n    self.assertEqual(len(rotated_csrf_cookie.value), CSRF_TOKEN_LENGTH)\n"], "sample_325": ["def test_boundfield_subwidgets_with_id(self):\n    class BeatleForm(Form):\n        name = ChoiceField(\n            choices=[('john', 'John'), ('paul', 'Paul'), ('george', 'George'), ('ringo', 'Ringo')],\n            widget=RadioSelect,\n        )\n\n    f = BeatleForm(auto_id='id_%s')\n    subwidgets = list(f['name'])\n    self.assertEqual(len(subwidgets), 4)\n\n    self.assertEqual(subwidgets[0].data['attrs']['id'], 'id_name_0')\n    self.assertEqual(subwidgets[1].data['attrs']['id'], 'id_name_1')\n    self.assertEqual(subwidgets[2].data['attrs']['id'], 'id_name_2')\n    self.assertEqual(subwidgets[3].data['attrs']['id'], 'id_name_3')\n", "def test_subwidgets_rendering(self):\n    class MyForm(Form):\n        field = MultipleChoiceField(\n            choices=[('a', 'A'), ('b', 'B')],\n            widget=CheckboxSelectMultiple,\n        )\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form['field'].as_widget(),\n        \"\"\"<ul id=\"id_field\">", "def test_render_with_custom_renderer(self):\n    class CustomForm(Form):\n        field = CharField()\n\n    custom_renderer = CustomRenderer()\n    form = CustomForm(renderer=custom_renderer)\n    self.assertEqual(form.renderer, custom_renderer)\n    rendered_form = form.as_p()\n    self.assertIn('field', rendered_form)\n", "def test_boundfield_widget_type(self):\n    class MyForm(Form):\n        field1 = CharField()\n        field2 = IntegerField()\n        field3 = FloatField()\n\n    form = MyForm()\n    self.assertEqual(form['field1'].widget_type, 'text')\n    self.assertEqual(form['field2'].widget_type, 'number')\n    self.assertEqual(form['field3'].widget_type, 'number')\n", "def test_custom_renderer(self):\n    class CustomRenderer(DjangoTemplates):\n            return 'Custom renderer output'\n\n    class CustomForm(Form):\n        default_renderer = CustomRenderer\n\n    form = CustomForm()\n    self.assertEqual(form.as_table(), 'Custom renderer output')\n"], "sample_326": ["def test_urlize_nofollow(self):\n    tests = (\n        (\n            'Search for google.com and see.',\n            'Search for <a href=\"http://google.com\" rel=\"nofollow\">google.com</a> and see.'\n        ),\n        (\n            lazystr('Search for google.com'),\n            'Search for <a href=\"http://google.com\" rel=\"nofollow\">google.com</a>'\n        ),\n        ('foo@example.com', '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('Check out www.google.com!', 'Check out <a href=\"http://www.google.com\">www.google.com</a>!'),\n        ('Check out www.google.com?', 'Check out <a href=\"http://www.google.com\">www.google.com</a>?'),\n        ('Check out www.google.com:', 'Check out <a href=\"http://www.google.com\">www.google.com</a>:'),\n        ('Check out www.google.com;', 'Check out <a href=\"http://www.google.com\">www.google.com</a>;'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            'Search for google.com.',\n            'Search for <a href=\"http://google.com/\">google.com</a>.'\n        ),\n        (\n            'Search for google.com!?',\n            'Search for <a href=\"http://google.com/\">google.com</a>!?'\n        ),\n        (\n            'Search for google.com...',\n            'Search for <a href=\"http://google.com/\">google.com</a>...'\n        ),\n        (\n            lazystr('Search for google.com.'),\n            'Search for <a href=\"http://google.com/\">google.com</a>.'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        (\n            'Search for google.com.',\n            'Search for <a href=\"http://google.com/\">google.com</a>.'\n        ),\n        (\n            'Search for google.com! and see.',\n            'Search for <a href=\"http://google.com/\">google.com</a>! and see.'\n        ),\n        (\n            'Search for google.com: there you go.',\n            'Search for <a href=\"http://google.com/\">google.com</a>: there you go.'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Search for google.com.', 'Search for <a href=\"http://google.com/\">google.com</a>.'),\n        ('Search for google.com,', 'Search for <a href=\"http://google.com/\">google.com</a>,'),\n        ('Search for google.com?', 'Search for <a href=\"http://google.com/\">google.com</a>?'),\n        ('Search for google.com!', 'Search for <a href=\"http://google.com/\">google.com</a>!'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_327": ["def test_invalid_json_input(self):\n    class JSONForm(Form):\n        json_field = JSONField(required=True)\n\n    form = JSONForm({'json_field': '{some badly formed: json}'})\n    self.assertEqual(form['json_field'].value(), '{some badly formed: json}')\n    self.assertIn('{some badly formed: json}</textarea>', form.as_p())\n    self.assertEqual(form.errors['json_field'], ['Enter a valid JSON.'])\n", "def test_bound_data(self):\n    class JSONForm(Form):\n        json_field = JSONField()\n\n    tests = [\n        ('{\"a\": \"b\"}', {'a': 'b'}),\n        ('{\"a\": 1}', {'a': 1}),\n        ('\"foo\"', 'foo'),\n        ('42', 42),\n        ('3.14', 3.14),\n        ('true', True),\n        ('false', False),\n        ('null', None),\n    ]\n    for data, expected in tests:\n        with self.subTest(data=data):\n            form = JSONForm({'json_field': data})\n            self.assertEqual(form['json_field'].value(), expected)\n\n    # Invalid JSON should be preserved.\n    invalid_json = '{\"a\": \"b\",}'\n    form = JSONForm({'json_field': invalid_json})\n    self.assertEqual(form['json_field'].value(), invalid_json)\n", "def test_bound_data(self):\n    field = JSONField(required=False)\n    tests = [\n        ('null', None),\n        ('\"foo\"', 'foo'),\n        ('[1, 2]', [1, 2]),\n        ('{\"a\": \"b\"}', {'a': 'b'}),\n        (None, None),\n    ]\n    for data, expected in tests:\n        with self.subTest(data=data):\n            bound_field = field.bound_data(data, None)\n            if data is None:\n                self.assertIsNone(bound_field)\n            elif isinstance(bound_field, InvalidJSONInput):\n                self.assertEqual(bound_field, data)\n            else:\n                self.assertEqual(bound_field, expected)\n", "def test_bound_data(self):\n    field = JSONField()\n    self.assertIsNone(field.bound_data(None, None))\n    tests = [\n        ('{\"a\": \"b\"}', {\"a\": \"b\"}),\n        ('\"foo\"', \"foo\"),\n        ('123', 123),\n        ('true', True),\n        ('false', False),\n        ('null', None),\n    ]\n    for input_data, expected in tests:\n        with self.subTest(input_data=input_data):\n            self.assertEqual(field.bound_data(input_data, None), expected)\n\n    invalid_json_input = '{\"a\": \"b\"'\n    self.assertIsInstance(field.bound_data(invalid_json_input, None), field.InvalidJSONInput)\n", "def test_bound_data(self):\n    field = JSONField(required=False)\n    tests = [\n        ('{\"a\": \"b\"}', {\"a\": \"b\"}),\n        ('null', None),\n        ('\"foo\"', \"foo\"),\n        ('[1, 2]', [1, 2]),\n    ]\n    for input_value, expected_value in tests:\n        with self.subTest(input_value=input_value):\n            self.assertEqual(field.bound_data(input_value, None), expected_value)\n\n    # Test invalid input.\n    invalid_input = '{\"a\": \"b\"'\n    bound_data = field.bound_data(invalid_input, None)\n    self.assertIsInstance(bound_data, field.InvalidJSONInput)\n    self.assertEqual(bound_data, invalid_input)\n"], "sample_328": ["def test_bulk_update_with_nullable_jsonfield(self):\n    obj1 = JSONFieldNullable.objects.create(data=None)\n    obj2 = JSONFieldNullable.objects.create(data='{}')\n    obj3 = JSONFieldNullable.objects.create(data='{\"key\": \"value\"}')\n    obj1.data = '{}'\n    obj2.data = '{\"key\": \"value\"}'\n    obj3.data = None\n    JSONFieldNullable.objects.bulk_update([obj1, obj2, obj3], ['data'])\n    self.assertCountEqual(JSONFieldNullable.objects.filter(data='{}'), [obj1])\n    self.assertCountEqual(JSONFieldNullable.objects.filter(data='{\"key\": \"value\"}'), [obj2])\n    self.assertCountEqual(JSONFieldNullable.objects.filter(data__isnull=True), [obj3])\n", "def test_bulk_update_with_ignore_conflicts(self):\n    if not connections[self.db].features.supports_ignore_conflicts:\n        self.skipTest('This database backend does not support ignoring conflicts.')\n    custom_pks = [\n        CustomPk.objects.create(name='pk-%s' % i, extra='')\n        for i in range(10)\n    ]\n    for model in custom_pks:\n        model.extra = 'extra-%s' % model.pk\n    CustomPk.objects.bulk_update(custom_pks, ['extra'], ignore_conflicts=True)\n    self.assertCountEqual(\n        CustomPk.objects.values_list('extra', flat=True),\n        [cat.extra for cat in custom_pks]\n    )\n", "def test_bulk_update_jsonfield_nullable(self):\n    json_models = [\n        JSONFieldNullable.objects.create(data={'key': 'value'})\n        for _ in range(10)\n    ]\n    for model in json_models:\n        model.data = None\n    JSONFieldNullable.objects.bulk_update(json_models, ['data'])\n    self.assertCountEqual(JSONFieldNullable.objects.filter(data__isnull=True), json_models)\n", "def test_bulk_update_nullable_json_field(self):\n    json_models = [\n        JSONFieldNullable.objects.create(data={'test': 'data'})\n        for _ in range(10)\n    ]\n    for model in json_models:\n        model.data = None\n    JSONFieldNullable.objects.bulk_update(json_models, ['data'])\n    self.assertCountEqual(JSONFieldNullable.objects.filter(data__isnull=True), json_models)\n", "def test_bulk_update_with_json_field(self):\n    json_models = [\n        JSONFieldNullable.objects.create(data={'key': 'value'})\n        for _ in range(10)\n    ]\n    for model in json_models:\n        model.data = {'key': 'new_value'}\n    JSONFieldNullable.objects.bulk_update(json_models, ['data'])\n    self.assertCountEqual(\n        JSONFieldNullable.objects.filter(data={'key': 'new_value'}),\n        json_models\n    )\n"], "sample_329": ["def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+1j)\\nThere are some values Django cannot serialize into migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/topics/migrations/#migration-serializing\" % get_docs_version()):\n        MigrationWriter.serialize(1 + 1j)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    try:\n        string = MigrationWriter.serialize(complex(1))[0]\n        self.assertEqual(string, \"complex(1)\")\n    finally:\n        Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+1j)\"):\n        MigrationWriter.serialize(complex(1))\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    self.assertIn(complex, Serializer._registry)\n    Serializer.unregister(complex)\n    self.assertNotIn(complex, Serializer._registry)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return 'complex(%r)' % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    self.assertEqual(Serializer._registry[complex], ComplexSerializer)\n    Serializer.unregister(complex)\n    self.assertNotIn(complex, Serializer._registry)\n", "def test_unregister_serializer(self):\n    class TestSerializer(BaseSerializer):\n            return 'test(%r)' % self.value, {}\n\n    Serializer.register(complex, TestSerializer)\n    string, imports = MigrationWriter.serialize(complex(1, 2))\n    self.assertEqual(string, \"test((1+2j))\")\n    Serializer.unregister(complex)\n    string, imports = MigrationWriter.serialize(complex(1, 2))\n    self.assertNotEqual(string, \"test((1+2j))\")\n"], "sample_330": ["    def test_typecast_timestamp(self):\n        # Test that typecast_timestamp correctly handles timestamps with time zone information\n        timestamp_with_tz = \"2022-07-25 14:30:00-05\"\n        timestamp_without_tz = \"2022-07-25 14:30:00\"\n\n        self.assertIsInstance(typecast_timestamp(timestamp_with_tz), datetime.datetime)\n        self.assertIsInstance(typecast_timestamp(timestamp_without_tz), datetime.datetime)\n\n        # Test that typecast_timestamp correctly handles timestamps without seconds or microseconds\n        timestamp_without_seconds = \"2022-07-25 14:30\"\n        self.assertIsInstance(typecast_timestamp(timestamp_without_seconds), datetime.datetime)\n\n        # Test that typecast_timestamp correctly handles invalid timestamps\n        invalid_timestamp = \"2022-07-25 14:30:00:123456\"  # extra colon\n        self.assertIsNone(typecast_timestamp(invalid_timestamp))\n", "    def test_truncate_name(self):\n        long_name = 'VeryLongModelNameZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ'\n        max_length = 25\n        truncated_name = truncate_name(long_name, length=max_length)\n        self.assertEqual(len(truncated_name), max_length)\n", "    def test_truncate_name_length(self):\n        long_name = 'a' * 100\n        truncated_name = truncate_name(long_name, length=30)\n        self.assertEqual(len(truncated_name), 30)\n", "    def test_truncate_name_length(self):\n        long_name = 'very_long_table_name_that_exceeds_max_length'\n        truncated_name = truncate_name(long_name, length=30)\n        self.assertEqual(len(truncated_name), 30)\n", "    def test_truncation(self):\n        long_name = \"a\" * 1000\n        truncated_name = truncate_name(long_name, length=30)\n        self.assertEqual(len(truncated_name), 34)  # 30 + 4 for hash\n"], "sample_331": ["def test_iso8601_duration(self):\n    test_values = (\n        ('P3Y6M4DT12H30M5S', timedelta(days=1267, hours=12, minutes=30, seconds=5)),\n        ('P23DT23H59M59S', timedelta(days=23, hours=23, minutes=59, seconds=59)),\n        ('PT11H22M33S', timedelta(hours=11, minutes=22, seconds=33)),\n        ('PT11H22M', timedelta(hours=11, minutes=22)),\n        ('PT11H', timedelta(hours=11)),\n        ('P3Y', timedelta(days=1095)),\n        ('P1M', timedelta(days=30)),\n        ('P1W', timedelta(days=7)),\n        ('P1D', timedelta(days=1)),\n        ('PT1H', timedelta(hours=1)),\n        ('PT1M', timedelta(minutes=1)),\n        ('PT1S', timedelta(seconds=1)),\n        ('-P3Y6M4DT12H30M5S', timedelta(days=-1267, hours=-12, minutes=-30, seconds=-5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('PT4H', timedelta(hours=4)),\n        ('PT5M', timedelta(minutes=5)),\n        ('PT6S', timedelta(seconds=6)),\n        ('P3DT4H5M6S', timedelta(days=3, hours=4, minutes=5, seconds=6)),\n        ('-P3D', timedelta(days=-3)),\n        ('-PT4H', timedelta(hours=-4)),\n        ('-PT5M', timedelta(minutes=-5)),\n        ('-PT6S', timedelta(seconds=-6)),\n        ('-P3DT4H5M6S', timedelta(days=-3, hours=-4, minutes=-5, seconds=-6)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P4DT12H30M5S', timedelta(days=4, hours=12, minutes=30, seconds=5)),\n        ('PT12H30M5S', timedelta(hours=12, minutes=30, seconds=5)),\n        ('P4D', timedelta(days=4)),\n        ('PT5S', timedelta(seconds=5)),\n        ('-P4DT12H30M5S', timedelta(days=-4, hours=-12, minutes=-30, seconds=-5)),\n        ('P0D', timedelta(days=0)),\n        ('PT0S', timedelta(seconds=0)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('P1DT12H', timedelta(days=1, hours=12)),\n        ('PT3H30M', timedelta(hours=3, minutes=30)),\n        ('PT3H30M5S', timedelta(hours=3, minutes=30, seconds=5)),\n        ('PT5S', timedelta(seconds=5)),\n        ('P1W', None),  # Not supported by timedelta\n        ('P1Y', None),  # Not supported by timedelta\n        ('P1M', None),  # Not supported by timedelta\n        ('-P3D', timedelta(days=-3)),\n        ('-P1DT12H', timedelta(days=-1, hours=-12)),\n        ('-PT3H30M', timedelta(hours=-3, minutes=-30)),\n        ('-PT3H30M5S', timedelta(hours=-3, minutes=-30, seconds=-5)),\n        ('-PT5S', timedelta(seconds=-5)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3Y6M4DT12H30M5S', timedelta(days=1267, hours=12, minutes=30, seconds=5)),\n        ('P23DT23H', timedelta(days=23, hours=23)),\n        ('P4Y', timedelta(days=1461)),\n        ('PT4H', timedelta(hours=4)),\n        ('PT0.001S', timedelta(microseconds=1000)),\n        ('-P3Y6M4DT12H30M5S', timedelta(days=-1267, hours=-12, minutes=-30, seconds=-5)),\n        ('P0D', timedelta()),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n"], "sample_332": ["def test_formset_media(self):\n    \"\"\"Media is available on formset.\"\"\"\n    class MediaForm(Form):\n        class Media:\n            js = ('some-file.js',)\n    self.assertIn('some-file.js', str(formset_factory(MediaForm)().media))\n", "def test_all_valid_with_one_invalid_formset(self):\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '',  # invalid form\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertIs(all_valid((formset1, formset2)), False)\n    expected_errors = [{}, {'votes': ['This field is required.']}]\n    self.assertEqual(formset1._errors, expected_errors)\n    self.assertEqual(formset2._errors, expected_errors)\n", "def test_all_valid_with_non_form_errors(self):\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'Zero',  # non-form error: duplicate choice\n        'choices-1-votes': '1',\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertIs(all_valid((formset1, formset2)), False)\n    self.assertEqual(formset1.non_form_errors(), ['Please submit at most 1 form.'])\n    self.assertEqual(formset2.non_form_errors(), ['Please submit at most 1 form.'])\n", "def test_media_property(self):\n    class MediaForm(Form):\n        class Media:\n            js = ('some-file.js',)\n    formset = formset_factory(MediaForm, extra=2)()\n    self.assertEqual(formset.media, formset.forms[0].media)\n    self.assertEqual(str(formset.media), '<script src=\"some-file.js\"></script>')\n    self.assertIn('some-file.js', str(formset.media))\n", "def test_add_fields(self):\n    \"\"\"\n    Test that the add_fields method is called during form construction.\n    \"\"\"\n    class BaseAddFieldsFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields['custom_field'] = CharField()\n\n    AddFieldsFormSet = formset_factory(FavoriteDrinkForm, formset=BaseAddFieldsFormSet)\n    formset = AddFieldsFormSet()\n    for form in formset:\n        self.assertIn('custom_field', form.fields)\n"], "sample_333": ["def test_use_required_attribute_overridden(self):\n    class MyForm(Form):\n        use_required_attribute = True\n        f1 = CharField(max_length=30)\n        f2 = CharField(max_length=30, required=False)\n        f3 = CharField(widget=Textarea)\n        f4 = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')])\n\n    form = MyForm(use_required_attribute=False)\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\"></p>'\n        '<p><label for=\"id_f2\">F2:</label> <input id=\"id_f2\" maxlength=\"30\" name=\"f2\" type=\"text\"></p>'\n        '<p><label for=\"id_f3\">F3:</label> <textarea cols=\"40\" id=\"id_f3\" name=\"f3\" rows=\"10\">'\n        '</textarea></p>'\n        '<p><label for=\"id_f4\">F4:</label> <select id=\"id_f4\" name=\"f4\">'\n        '<option value=\"P\">Python</option>'\n        '<option value=\"J\">Java</option>'\n        '</select></p>',\n    )\n", "def test_field_ordering_with_keyerrors(self):\n    class MyForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n\n            super().__init__(*args, **kwargs)\n            self.fields['field4'] = CharField()\n\n    form = MyForm(field_order=['field2', 'field1'])\n    self.assertEqual(list(form.fields), ['field2', 'field1', 'field3', 'field4'])\n\n    # Test that KeyError is raised when an invalid field name is provided.\n    with self.assertRaises(KeyError):\n        MyForm(field_order=['field5', 'field1'])\n", "def test_empty_data_files_multi_value_dict_with_kwargs(self):\n    p = Person(data={}, files={})\n    self.assertIsInstance(p.data, MultiValueDict)\n    self.assertIsInstance(p.files, MultiValueDict)\n", "def test_field_order_with_fields_named_data_and_files(self):\n    class MyForm(Form):\n        data = CharField(max_length=10)\n        files = CharField(max_length=10)\n\n    form = MyForm(field_order=['files', 'data'])\n    self.assertEqual(list(form.fields), ['files', 'data'])\n\n    # Check that fields named \"data\" and \"files\" do not cause any issues when\n    # instantiating a form.\n    form = MyForm(data={'data': 'xyzzy'}, files={'files': 'abcde'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data, {'data': 'xyzzy', 'files': 'abcde'})\n", "def test_boundfield_initial_with_custom_renderer(self):\n    class MyForm(Form):\n        name = CharField(initial='John', renderer=CustomRenderer())\n\n    form = MyForm()\n    self.assertEqual(form['name'].value(), 'John')\n"], "sample_334": ["def test_custom_renderer(self):\n    class CustomForm(Form):\n        default_renderer = None\n\n    custom = CustomRenderer()\n    form = CustomForm(renderer=custom)\n    self.assertEqual(form.renderer, custom)\n", "def test_custom_renderer_as_class(self):\n    class CustomForm(Form):\n        default_renderer = CustomRenderer\n\n    form = CustomForm()\n    self.assertIsInstance(form.renderer, CustomForm.default_renderer)\n    self.assertIsNot(form.renderer, CustomForm.default_renderer)\n\n    # Ensure forms with different renderers are isolated.\n    form1 = CustomForm()\n    form2 = Form()\n    self.assertNotIsInstance(form2.renderer, CustomForm.default_renderer)\n    self.assertIsInstance(form1.renderer, CustomForm.default_renderer)\n", "def test_cleaned_data_gets_merged_from_form_and_instance(self):\n    class PersonForm(Form):\n        name = CharField()\n        age = IntegerField()\n\n    form = PersonForm(data={'name': 'John', 'age': 30})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data, {'name': 'John', 'age': 30})\n\n    # Add an instance to the form and verify that the cleaned data gets merged.\n    class Person:\n            self.name = name\n            self.age = age\n\n    person = Person('Jane', 25)\n    form = PersonForm(instance=person, data={'name': 'John'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data, {'name': 'John', 'age': 25})\n", "def test_field_with_callable_widget(self):\n    class MyWidget(TextInput):\n            super().__init__(**kwargs)\n            self.attrs['class'] = 'my-widget'\n\n    class MyForm(Form):\n        field = CharField(widget=lambda: MyWidget())\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_field\">Field:</label> <input type=\"text\" name=\"field\" id=\"id_field\" class=\"my-widget\" required></p>'\n    )\n", "def test_form_media(self):\n    class CustomForm(Form):\n        class Media:\n            css = {\n                'all': ('path/to/css.css',)\n            }\n            js = ('path/to/js.js',)\n\n    form = CustomForm()\n    self.assertIsInstance(form.media, Media)\n    self.assertEqual(str(form.media), '<link href=\"path/to/css.css\" type=\"text/css\" media=\"all\">'\n                                    '<script src=\"path/to/js.js\"></script>')\n"], "sample_335": ["def test_decimalfield_disabled(self):\n    f = DecimalField(max_digits=4, decimal_places=2, disabled=True)\n    self.assertWidgetRendersTo(\n        f,\n        '<input step=\"0.01\" name=\"f\" type=\"number\" id=\"id_f\" disabled>',\n    )\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean('')\n    self.assertIsNone(f.clean(None))\n    self.assertEqual(f.clean('1'), decimal.Decimal(\"1\"))\n    self.assertIsInstance(f.clean('1'), decimal.Decimal)\n    self.assertEqual(f.clean('23'), decimal.Decimal(\"23\"))\n    self.assertEqual(f.clean('3.14'), decimal.Decimal(\"3.14\"))\n    self.assertEqual(f.clean(3.14), decimal.Decimal(\"3.14\"))\n    self.assertEqual(f.clean(decimal.Decimal('3.14')), decimal.Decimal(\"3.14\"))\n    self.assertEqual(f.clean('1.0 '), decimal.Decimal(\"1.0\"))\n    self.assertEqual(f.clean(' 1.0'), decimal.Decimal(\"1.0\"))\n    self.assertEqual(f.clean(' 1.0 '), decimal.Decimal(\"1.0\"))\n    self.assertFalse(f.has_changed(decimal.Decimal(\"1\"), '1'))\n    self.assertFalse(f.has_changed(None, '1'))\n", "def test_decimalfield_normalize(self):\n    f = DecimalField(max_digits=5, decimal_places=2, normalize=True)\n    self.assertEqual(f.clean('1.200'), decimal.Decimal(\"1.20\"))\n    self.assertEqual(f.clean('1200E-2'), decimal.Decimal(\"12.00\"))\n    self.assertEqual(f.clean('1.20000'), decimal.Decimal(\"1.20\"))\n    self.assertEqual(f.clean('00001.20000'), decimal.Decimal(\"1.20\"))\n\n    f = DecimalField(max_digits=5, decimal_places=2, normalize=False)\n    self.assertEqual(f.clean('1.200'), decimal.Decimal(\"1.200\"))\n    self.assertEqual(f.clean('1200E-2'), decimal.Decimal(\"1200E-2\"))\n    self.assertEqual(f.clean('1.20000'), decimal.Decimal(\"1.20000\"))\n    self.assertEqual(f.clean('00001.20000'), decimal.Decimal(\"00001.20000\"))\n", "def test_decimalfield_disabled(self):\n    f = DecimalField(max_digits=4, decimal_places=2, disabled=True)\n    self.assertWidgetRendersTo(\n        f,\n        '<input step=\"0.01\" name=\"f\" type=\"number\" id=\"id_f\" disabled>',\n    )\n    self.assertIsNone(f.clean(''))\n    self.assertEqual(f.clean('1'), decimal.Decimal(\"1\"))\n    self.assertFalse(f.has_changed(decimal.Decimal(\"1\"), '1'))\n", "def test_decimalfield_disabled(self):\n    f = DecimalField(max_digits=4, decimal_places=2, disabled=True)\n    self.assertIsNone(f.clean(''))\n    self.assertIsNone(f.clean(None))\n    self.assertEqual(f.clean('1.23'), decimal.Decimal(\"1.23\"))\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean('')\n    self.assertFalse(f.has_changed(decimal.Decimal(\"1.23\"), '1.23'))\n    self.assertFalse(f.has_changed(None, '1.23'))\n    self.assertWidgetRendersTo(\n        f,\n        '<input step=\"0.01\" name=\"f\" type=\"number\" id=\"id_f\" disabled>',\n    )\n", "def test_decimalfield_disabled(self):\n    f = DecimalField(max_digits=4, decimal_places=2, disabled=True)\n    self.assertWidgetRendersTo(\n        f,\n        '<input step=\"0.01\" name=\"f\" type=\"number\" id=\"id_f\" disabled>',\n    )\n    self.assertIsNone(f.clean(''))\n    self.assertIsNone(f.clean(None))\n    self.assertEqual(f.clean('1'), decimal.Decimal(\"1\"))\n    self.assertEqual(f.max_digits, 4)\n    self.assertEqual(f.decimal_places, 2)\n    self.assertIsNone(f.max_value)\n    self.assertIsNone(f.min_value)\n\n    # Ensure has_changed always returns False for a disabled field\n    d = decimal.Decimal(\"0.1\")\n    self.assertFalse(f.has_changed(d, '0.101'))\n"], "sample_336": ["    def test_url_pattern_check(self):\n        resolver = get_resolver()\n        checks = resolver.check()\n        self.assertEqual(len(checks), 0)\n", "    def test_describe(self):\n        pattern = RegexPattern(r'^example/$', name='example-view')\n        self.assertEqual(pattern.describe(), \"'example-view' [name='example-view']\")\n", "    def test_warning_on_capturing_prefix_in_url_pattern(self):\n        msg = (\n            \"Your URL pattern '^/prefix/(?P<prefix>[^/]+)/$' has a route \"\n            \"that contains '(?P<', begins with a '^', or ends with a '$'. \"\n            \"This was likely an oversight when migrating to \"\n            \"django.urls.path().\"\n        )\n        with self.assertRaisesMessage(Warning, msg):\n            path('/prefix/<str:prefix>/', views.empty_view)\n", "    def test_get(self):\n        descriptor = LocaleRegexDescriptor('_regex')\n        instance = type('Test', (), {'_regex': 'test'})\n        self.assertEqual(descriptor.__get__(instance), 'test')\n", "    def test_url_resolver_repr(self):\n        resolver = get_resolver()\n        self.assertIsInstance(resolver, URLResolver)\n        self.assertIn('<URLResolver', repr(resolver))\n"], "sample_337": ["def test_csrf_trusted_origins_is_required_for_https(self):\n    \"\"\"\n    HTTPS requests require at least one entry in CSRF_TRUSTED_ORIGINS.\n    \"\"\"\n    with self.settings(ALLOWED_HOSTS=['www.example.com'], CSRF_TRUSTED_ORIGINS=[]):\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_REFERER'] = 'https://www.example.com/somepage'\n        mw = CsrfViewMiddleware(post_form_view)\n        with self.assertRaises(ImproperlyConfigured) as cm:\n            mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(\n            str(cm.exception),\n            'CSRF_TRUSTED_ORIGINS is required for HTTPS requests. '\n            'Configure it to include the domains that are expected to be '\n            'allowed to make requests to your site.',\n        )\n", "def test_process_response_csrf_cookie_not_reset_for_get_request(self):\n    \"\"\"\n    The CSRF cookie is not reset for GET requests, even if the view is decorated\n    with @requires_csrf_token.\n    \"\"\"\n    req = self._get_request()\n    mw = CsrfViewMiddleware(protected_view)\n    mw.process_view(req, protected_view, (), {})\n    resp = mw(req)\n    csrf_cookie = self._read_csrf_cookie(req, resp)\n    self.assertIsNone(csrf_cookie)\n", "def test_origin_header_precedence(self):\n    \"\"\"\n    When both Origin and Referer headers are present, Origin takes precedence.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'https://example.com'\n    req.META['HTTP_REFERER'] = 'https://evil.com'\n    mw = CsrfViewMiddleware(post_form_view)\n    self.assertIs(mw._origin_verified(req), True)\n    response = mw.process_view(req, post_form_view, (), {})\n    self.assertIsNone(response)\n", "def test_csrf_exempt_decorator_preserves_token(self):\n    \"\"\"\n    The csrf_exempt decorator preserves the CSRF token in the session.\n    \"\"\"\n    req = self._get_request()\n    get_token(req)\n    csrf_token = req.session.get(CSRF_SESSION_KEY)\n    self.assertIsNotNone(csrf_token)\n\n    @csrf_exempt\n        return HttpResponse()\n\n    CsrfViewMiddleware(view).process_view(req, view, (), {})\n    self.assertEqual(req.session.get(CSRF_SESSION_KEY), csrf_token)\n", "def test_csrf_trusted_origins_wildcard_allowed(self):\n    \"\"\"\n    A POST request with an origin that matches a wildcard in\n    CSRF_TRUSTED_ORIGINS is accepted.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'https://subdomain.example.com'\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n"], "sample_338": ["def test_generate_altered_foo_together_for_multiple_fields(self):\n    \"\"\"\n    index/unique_together correctly handles multiple fields.\n    \"\"\"\n    initial_author = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n        ('age', models.IntegerField()),\n    ], {\n        'unique_together': {('name',), ('age',)},\n        'index_together': {('name',), ('age',)},\n    })\n    author_reversed_constraints = ModelState('testapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200, unique=True)),\n        ('age', models.IntegerField(unique=True)),\n    ], {\n        'index_together': {('age',), ('name',)},\n    })\n    changes = self.get_changes([initial_author], [author_reversed_constraints])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\n        'AlterUniqueTogether',\n        'AlterField',\n        'AlterField',\n        'AlterIndexTogether',\n    ])\n    self.assertOperationAttributes(\n        changes, 'testapp', 0, 0, name='author', unique_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes, 'testapp', 0, 1, model_name='author', name='age',\n    )\n    self.assertOperationAttributes(\n        changes, 'testapp', 0, 2, model_name='author', name='name',\n    )\n    self.assertOperationAttributes(\n        changes, 'testapp', 0, 3, name='author',\n        index_together={('age',), ('name',)},\n    )\n", "def test_alter_model_options_update(self):\n    \"\"\"\n    Changing a model's options should make a change.\n    \"\"\"\n    changes = self.get_changes([self.author_with_options], [self.author_with_options_updated])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={\n        \"permissions\": [('can_hire', 'Can hire updated')],\n        \"verbose_name\": \"Authi updated\",\n    })\n\n    # Changing them back to empty should also make a change\n    changes = self.get_changes([self.author_with_options_updated], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={})\n", "def test_generate_renamed_fields_with_deconstruct(self):\n    before = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.IntegerField(validators=[RegexValidator(re.compile('^[-a-zA-Z0-9_]+\\\\Z'))])),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('renamed_field', models.IntegerField(validators=[validate_slug])),\n        ]),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameField'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, model_name='foo', old_name='field',\n        new_name='renamed_field',\n    )\n", "def test_alter_model_options_with_proxy(self):\n    \"\"\"Changing a model's options when there is a proxy model should also make a change.\"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.author_proxy], \n        [self.author_with_options, self.author_proxy]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n    })\n", "def test_generate_altered_unique_together_with_existing_index(self):\n    \"\"\"\n    When adding a unique constraint on an existing index, the autodetector\n    should remove the index to avoid database consistency errors.\n    \"\"\"\n    before = [\n        ModelState('app', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field1', models.IntegerField()),\n            ('field2', models.IntegerField()),\n        ], options={'index_together': {('field1', 'field2')}}),\n    ]\n    after = [\n        ModelState('app', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field1', models.IntegerField()),\n            ('field2', models.IntegerField()),\n        ], options={'unique_together': {('field1', 'field2')}}),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterUniqueTogether', 'RemoveIndex'])\n"], "sample_339": ["def test_modelformset_factory_with_custom_form(self):\n    class CustomBookForm(forms.ModelForm):\n            super().__init__(*args, **kwargs)\n            self.fields['title'].widget.attrs['class'] = 'custom-class'\n\n        class Meta:\n            model = Book\n            fields = '__all__'\n\n    BookFormSet = modelformset_factory(Book, form=CustomBookForm)\n    formset = BookFormSet()\n    self.assertEqual(len(formset.forms), 1)\n    self.assertHTMLEqual(\n        str(formset.forms[0]['title']),\n        '<input class=\"custom-class\" id=\"id_form-0-title\" maxlength=\"100\" name=\"form-0-title\" type=\"text\">'\n    )\n", "def test_modelformset_factory_widgets_with_field_names(self):\n    widgets = {\n        'name': forms.TextInput(attrs={'class': 'poet'})\n    }\n    PoetFormSet = modelformset_factory(Poet, fields=(\"name\",), widgets=widgets)\n    form = PoetFormSet.form()\n    self.assertHTMLEqual(\n        str(form['name']),\n        '<input id=\"id_name\" maxlength=\"100\" type=\"text\" class=\"poet\" name=\"name\" required>'\n    )\n", "def test_modelformset_factory_with_invalid_field_name(self):\n    with self.assertRaisesMessage(\n        FieldError,\n        \"Unknown field(s) (invalid_field) specified for Author\",\n    ):\n        modelformset_factory(Author, fields=('name', 'invalid_field'))\n", "def test_modelformset_factory_field_classes_overrides_with_widget(self):\n    author = Author.objects.create(pk=1, name='Charles Baudelaire')\n    BookFormSet = modelformset_factory(Book, fields=\"__all__\", field_classes={\n        'title': forms.SlugField,\n    }, widgets={\n        'title': forms.TextInput(attrs={'class': 'custom-title'}),\n    })\n    form = BookFormSet.form(data={'title': 'Foo', 'author': author.id})\n    self.assertIs(Book._meta.get_field('title').__class__, models.CharField)\n    self.assertIsInstance(form.fields['title'], forms.SlugField)\n    self.assertHTMLEqual(\n        str(form['title']),\n        '<input class=\"custom-title\" id=\"id_title\" maxlength=\"50\" name=\"title\" type=\"text\">'\n    )\n", "def test_modelformset_factory_with_empty_queryset(self):\n    AuthorFormSet = modelformset_factory(Author, fields='__all__', extra=0)\n    formset = AuthorFormSet(queryset=Author.objects.none())\n    self.assertEqual(len(formset), 0)\n    self.assertEqual(formset.initial_form_count(), 0)\n"], "sample_340": ["def test_get_migration_by_prefix_invalid_app_label(self):\n    loader = MigrationLoader(connection)\n    with self.assertRaisesMessage(KeyError, \"No installed app with label 'nonexistent'\"):\n        loader.get_migration_by_prefix('nonexistent', '0001')\n", "def test_detect_conflicts(self):\n    \"\"\"MigrationLoader.detect_conflicts() returns conflicting apps.\"\"\"\n    loader = MigrationLoader(connection)\n    # Make a conflict by adding an additional leaf node to the graph.\n    loader.graph.add_node(('migrations', '0003_third'), Migration('0003_third', 'migrations'))\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0002_second', '0003_third']})\n", "def test_detect_conflicts(self):\n    \"\"\"MigrationLoader can detect conflicting apps.\"\"\"\n    loader = MigrationLoader(connection)\n    self.assertEqual(loader.detect_conflicts(), {})\n    # Fake a conflict by adding a migration with the same app label.\n    loader.disk_migrations[('migrations', '0003_third')] = type(\n        'Migration', (object,), {'app_label': 'migrations'}\n    )\n    self.assertEqual(loader.detect_conflicts(), {'migrations': ['0002_second', '0003_third']})\n", "def test_detect_conflicts(self):\n    \"\"\"Tests detection of conflicting apps.\"\"\"\n    loader = MigrationLoader(connection)\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {})\n    # Simulate a conflict by adding a migration to the graph with a different app label.\n    loader.graph.add_node(('other_app', '0001_initial'), None)\n    loader.graph.add_dependency(loader.get_migration('migrations', '0001_initial'), ('other_app', '0001_initial'))\n    conflicts = loader.detect_conflicts()\n    self.assertEqual(conflicts, {'migrations': ['0001_initial']})\n", "def test_cyclic_dependency_detection(self):\n    \"\"\"\n    MigrationLoader raises an exception when a cyclic dependency is detected.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    with self.assertRaises(CircularDependencyError):\n        loader.build_graph()\n"], "sample_341": ["def test_all_valid_with_non_form_errors(self):\n    \"\"\"all_valid() returns False if any formset has non-form errors.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '1',  # max_num is less than total forms\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    ChoiceFormSet = formset_factory(Choice, validate_max=True)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertIs(all_valid((formset1, formset2)), False)\n    self.assertEqual(formset1.non_form_errors(), ['Please submit at most 1 form.'])\n    self.assertEqual(formset2.non_form_errors(), ['Please submit at most 1 form.'])\n", "def test_formset_total_error_count_with_min_num_validation(self):\n    \"\"\"A formset's total error count includes min_num validation errors.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    data = {\n        'choices-TOTAL_FORMS': '1',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.total_error_count(), 1)\n", "def test_formset_with_disabled_fields(self):\n    class DisabledForm(Form):\n        choice = CharField(disabled=True)\n        votes = IntegerField()\n\n    DisabledFormSet = formset_factory(DisabledForm)\n    formset = DisabledFormSet()\n    self.assertHTMLEqual(\n        str(formset),\n        \"\"\"<input type=\"hidden\" name=\"form-TOTAL_FORMS\" value=\"1\">", "def test_formset_non_field_errors(self):\n    \"\"\"A formset has a non_form_errors method that returns non-field errors.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '2',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    ChoiceFormSet = formset_factory(Choice, max_num=1, validate_max=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at most 1 form.'])\n    self.assertEqual(\n        str(formset.non_form_errors()),\n        '<ul class=\"errorlist nonform\"><li>Please submit at most 1 form.</li></ul>',\n    )\n", "def test_formset_add_fields(self):\n    \"\"\"A formset has an add_fields method.\"\"\"\n    class AddFieldFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields['extra_field'] = CharField()\n\n    AddFieldFormset = formset_factory(Choice, formset=AddFieldFormSet)\n    formset = AddFieldFormset()\n    self.assertIn('extra_field', formset.forms[0].fields)\n"], "sample_342": ["def test_serialize_result_custom_to_field(self):\n    class AutocompleteJsonSerializeResultView(AutocompleteJsonView):\n            return {\n                **super().serialize_result(obj, to_field_name),\n                'question_text': str(obj.question),\n            }\n\n    q = Question.objects.create(question='Is this a question?')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts, 'field_name': 'question_with_to_field'})\n    request.user = self.superuser\n    response = AutocompleteJsonSerializeResultView.as_view(**self.as_view_args)(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data, {\n        'results': [{'id': str(q.uuid), 'text': q.question, 'question_text': q.question}],\n        'pagination': {'more': False},\n    })\n", "def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': ' invalid', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_bad_parameters(self):\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    # Test missing 'app_label' parameter.\n    request.GET = {'model_name': 'answer', 'field_name': 'question'}\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    # Test missing 'model_name' parameter.\n    request.GET = {'app_label': 'admin_views', 'field_name': 'question'}\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    # Test missing 'field_name' parameter.\n    request.GET = {'app_label': 'admin_views', 'model_name': 'answer'}\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    # Test invalid 'app_label' parameter.\n    request.GET = {'app_label': ' invalid_app', 'model_name': 'answer', 'field_name': 'question'}\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n    # Test invalid 'model_name' parameter.\n    request.GET = {'app_label': 'admin_views', 'model_name': ' invalid_model', 'field_name': 'question'}\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': ' invalid', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_343": ["def test_get_content_type_with_object(self):\n    question = Question.objects.create(text='Who?')\n    self.assertEqual(Answer.question.get_content_type(obj=question), ContentType.objects.get_for_model(Question))\n", "def test_check_field_name(self):\n    class Model(models.Model):\n        field_ = GenericForeignKey('content_type', 'object_id')\n    errors = Model.field_._check_field_name()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'fields.E001')\n    self.assertEqual(errors[0].msg, 'Field names must not end with an underscore.')\n    self.assertEqual(errors[0].obj, Model.field_)\n", "def test_get_prefetch_queryset(self):\n    questions = [Question.objects.create(text=f'Question {i}') for i in range(3)]\n    posts = [Post.objects.create(title=f'Answer {i}', parent=question) for question in questions]\n\n    prefetch_qs, _, _, _, _, _ = Post.parent.get_prefetch_queryset(posts)\n    self.assertEqual(len(prefetch_qs), 3)\n    self.assertEqual(set(obj.pk for obj in prefetch_qs), set(question.pk for question in questions))\n", "def test_generic_foreign_key_checks(self):\n    class Model(models.Model):\n        content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\n        object_id = models.IntegerField()\n        field = GenericForeignKey()\n\n    model_field = Model._meta.get_field('field')\n    errors = model_field.check()\n    self.assertEqual(errors, [])\n\n    class Model(models.Model):\n        content_type = models.IntegerField()\n        object_id = models.IntegerField()\n        field = GenericForeignKey()\n\n    model_field = Model._meta.get_field('field')\n    errors = model_field.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'contenttypes.E003')\n\n    class Model(models.Model):\n        content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\n        object_id = models.CharField(max_length=10)\n        field = GenericForeignKey('content_type', 'object_id')\n\n    model_field = Model._meta.get_field('field')\n    errors = model_field.check()\n    self.assertEqual(errors, [])\n\n    class Model(models.Model):\n        content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\n        field = GenericForeignKey('content_type', 'object_id')\n\n    model_field = Model._meta.get_field('field')\n    errors = model_field.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'contenttypes.E001')\n", "def test_get_prefetch_queryset(self):\n    questions = [Question.objects.create(text='Who?') for _ in range(3)]\n    posts = [Post.objects.create(title='Answer', parent=question) for question in questions]\n\n    prefetch_queryset, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = Post.parent.get_prefetch_queryset(posts)\n\n    self.assertEqual(len(prefetch_queryset), len(questions))\n    self.assertEqual(rel_obj_attr(Post.objects.get(pk=posts[0].pk)), (posts[0].pk, Post))\n    self.assertEqual(instance_attr(posts[0]), (questions[0].pk, Question))\n    self.assertFalse(single)\n    self.assertEqual(cache_name, 'parent')\n    self.assertFalse(is_descriptor)\n"], "sample_344": ["def test_state_apps_bulk_update_rollback(self):\n    \"\"\"\n    StateApps.bulk_update() should reset the apps.ready to its previous value\n    even if an exception occurs.\n    \"\"\"\n    project_state = ProjectState()\n    apps = project_state.apps\n    original_ready = apps.ready\n    try:\n        with apps.bulk_update():\n            self.assertFalse(apps.ready)\n            raise Exception(\"Mock error\")\n    except Exception:\n        pass\n    self.assertEqual(apps.ready, original_ready)\n", "def test_rename_model_with_through_field(self):\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"Tag\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    ))\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"SubTag\",\n        fields=[\n            ('tag_ptr', models.OneToOneField(\n                'migrations.Tag',\n                models.CASCADE,\n                auto_created=True,\n                parent_link=True,\n                primary_key=True,\n                to_field='id',\n                serialize=False,\n            )),\n        ],\n        bases=(\"migrations.Tag\",),\n    ))\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"Book\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"tags\", models.ManyToManyField(\"migrations.SubTag\", through=\"migrations.ThroughModel\")),\n        ],\n    ))\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"ThroughModel\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"book\", models.ForeignKey(\"migrations.Book\", models.CASCADE)),\n            (\"subtag\", models.ForeignKey(\"migrations.SubTag\", models.CASCADE)),\n        ],\n    ))\n\n    new_apps = project_state.apps\n\n    operation = RenameModel(\"subtag\", \"newsubtag\")\n    operation.state_forwards(\"migrations\", project_state)\n\n    self.assertEqual(project_state.models[\"migrations\", \"book\"].fields['tags'].remote_field.through, \"migrations.ThroughModel\")\n    self.assertEqual(project_state.models[\"migrations\", \"throughmodel\"].fields['subtag'].remote_field.model, \"migrations.NewSubTag\")\n\n    # Rendering should still work after the rename.\n    project_state.apps  # This will raise an exception if rendering fails.\n", "def test_get_concrete_model_key(self):\n    project_state = ProjectState()\n    model_state = ModelState('app', 'Model', [], options={'proxy': True})\n    project_state.add_model(model_state)\n    with self.assertRaises(KeyError):\n        project_state.get_concrete_model_key(model_state)\n\n    concrete_model_state = ModelState('app', 'ConcreteModel', [])\n    project_state.add_model(concrete_model_state)\n    model_state.bases = ('app.ConcreteModel',)\n    project_state.models[('app', 'model')].bases = ('app.ConcreteModel',)\n    self.assertEqual(project_state.get_concrete_model_key(model_state), ('app', 'concretemodel'))\n", "def test_rename_model_with_m2m(self):\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"Tag\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"hidden\", models.BooleanField()),\n            (\"m2m\", models.ManyToManyField(\"Tag\")),\n        ],\n    ))\n\n    new_state = project_state.clone()\n    operation = RenameModel(\"Tag\", \"NewTag\")\n    operation.state_forwards(\"migrations\", new_state)\n    new_state.apps  # Render the apps to see if there's any issue\n\n    self.assertEqual(len(new_state.models), 1)\n    model_state = list(new_state.models.values())[0]\n    self.assertEqual(model_state.name, \"NewTag\")\n    self.assertEqual([f[1].remote_field.model for f in model_state.fields.items() if f[0] == 'm2m'], [\"migrations.NewTag\"])\n", "def test_render_model_with_custom_manager(self):\n    class CustomManager(models.Manager):\n        pass\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState(\n        app_label=\"migrations\",\n        name=\"Tag\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        managers=[\n            ('objects', CustomManager()),\n        ]\n    ))\n\n    new_apps = project_state.apps\n    self.assertEqual(new_apps.get_model(\"migrations\", \"Tag\")._meta.get_manager('objects').__class__, CustomManager)\n"], "sample_345": ["    def test_trigger_reload(self, mocked_exit):\n        autoreload.trigger_reload('test.py')\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_ensure_echo_on(self, tcsetattr_mock, tcgetattr_mock, isatty_mock):\n        attr_list = [0, 0, 0, 0]\n        tcgetattr_mock.return_value = attr_list\n        autoreload.ensure_echo_on()\n        tcsetattr_mock.assert_called_once_with(sys.stdin, termios.TCSANOW, attr_list)\n", "    def test_trigger_reload(self, mocked_exit):\n        autoreload.trigger_reload('/path/to/file')\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n", "    def test_stop(self):\n        reloader = autoreload.BaseReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n", "    def test_trigger_reload(self, mocked_logger, mocked_exit):\n        filename = 'test_file.py'\n        autoreload.trigger_reload(filename)\n        self.assertEqual(mocked_exit.call_args[0][0], 3)\n        self.assertEqual(mocked_logger.info.call_args[0][0], '%s changed, reloading.')\n        self.assertEqual(mocked_logger.info.call_args[0][1], filename)\n"], "sample_346": ["    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n        self.assertTrue(middleware.sync_capable)\n        self.assertTrue(middleware.async_capable)\n", "    def test_method_decorator_with_classmethod(self):\n        class MyClass:\n            @classmethod\n            @method_decorator(myattr_dec)\n                return True\n\n        self.assertIs(getattr(MyClass.my_classmethod, 'myattr', False), True)\n        self.assertTrue(MyClass.my_classmethod())\n", "    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n\n        self.assertTrue(my_middleware.sync_capable)\n        self.assertTrue(my_middleware.async_capable)\n", "    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n\n        self.assertTrue(my_middleware.sync_capable)\n        self.assertTrue(my_middleware.async_capable)\n", "    def test_decorator_from_middleware(self):\n        class MyMiddleware:\n                self.get_response = get_response\n\n                return HttpResponse('Hello, world!')\n\n        decorator = decorator_from_middleware(MyMiddleware)\n        view = decorator(lambda request: HttpResponse('Not hello, world!'))\n\n        response = view(HttpRequest())\n        self.assertEqual(response.content, b'Hello, world!')\n"], "sample_347": ["def test_get_current_timezone_name(self):\n    \"\"\"\n    Test that get_current_timezone_name() returns the name of the currently\n    active time zone.\n    \"\"\"\n    timezone.activate(EAT)\n    self.assertEqual(timezone.get_current_timezone_name(), 'UTC+0300')\n    timezone.deactivate()\n    self.assertEqual(timezone.get_current_timezone_name(), 'America/Chicago')\n", "def test_get_current_timezone(self):\n    default = timezone.get_default_timezone()\n    try:\n        timezone.activate(ICT)\n        self.assertIs(timezone.get_current_timezone(), ICT)\n        self.assertEqual(timezone.get_current_timezone_name(), 'UTC+07:00')\n    finally:\n        timezone.deactivate()\n        self.assertIs(timezone.get_current_timezone(), default)\n", "def test_make_aware_is_dst(self):\n    # 2:30 happens twice, once before DST ends and once after\n    ambiguous = datetime.datetime(2015, 10, 25, 2, 30)\n\n    with self.assertRaises(pytz.AmbiguousTimeError):\n        timezone.make_aware(ambiguous, timezone=CET)\n\n    std = timezone.make_aware(ambiguous, timezone=CET, is_dst=False)\n    dst = timezone.make_aware(ambiguous, timezone=CET, is_dst=True)\n\n    self.assertEqual(std.tzinfo.dst(std), datetime.timedelta(hours=0))\n    self.assertEqual(dst.tzinfo.dst(dst), datetime.timedelta(hours=1))\n\n    # Test is_dst=None with pytz\n    with override_settings(TIME_ZONE='Europe/Paris'):\n        aware = timezone.make_aware(ambiguous, is_dst=None)\n        self.assertEqual(aware.tzinfo.dst(aware), datetime.timedelta(hours=1))\n", "def test_get_fixed_timezone_minutes(self):\n    minutes = 135  # 2 hours and 15 minutes\n    tz = timezone.get_fixed_timezone(minutes)\n    self.assertEqual(tz.utcoffset(None), datetime.timedelta(minutes=minutes))\n    self.assertEqual(tz.tzname(None), '+0215')\n", "def test_make_naive_ambiguous(self):\n    # 2:30 happens twice, once before DST ends and once after\n    ambiguous = datetime.datetime(2015, 10, 25, 2, 30, tzinfo=CET)\n\n    std = timezone.make_naive(ambiguous, timezone=CET)\n    dst = timezone.make_naive(ambiguous.replace(tzinfo=None), timezone=CET)\n    self.assertEqual(std - dst, datetime.timedelta(hours=1))\n    self.assertEqual(std.hour, 2)\n    self.assertEqual(dst.hour, 3)\n"], "sample_348": ["    def test_actions_must_define_short_description(self):\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"No description provided for action 'action'.\",\n            id='admin.E131',\n        )\n", "    def test_actions_label(self):\n        @admin.action(label='Test Action')\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_description_not_string(self):\n        @admin.action(description=123)\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The 'description' attribute of the 'action' action must be a string.\",\n            id='admin.E131',\n        )\n", "    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n", "    def test_valid_case(self):\n        class TestModelAdmin(ModelAdmin):\n            formset = BaseModelFormSet\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n"], "sample_349": ["def test_optgroups_with_unsaved_instance(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    album = Album(band=beatles)\n    form = AlbumForm(instance=album)\n    output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.uuid\n    self.assertIn(selected_option, output)\n", "def test_autocomplete_select_multiple(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option1 = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    selected_option2 = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    self.assertIn(selected_option1, output)\n    self.assertIn(selected_option2, output)\n\n    # Test that the AutocompleteSelectMultiple widget allows multiple selections.\n    request = self.client.get('/autocomplete/')\n    response = autocomplete(request, self.album_admin)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(len(data['results']), 2)\n", "def test_autocomplete_mixin_translations(self):\n    # Test that the AutocompleteMixin uses the correct translations for the\n    # language set in the settings.\n    with translation.override('fr'):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['lang'], 'fr')\n\n    # Test that the AutocompleteMixin falls back to English if the language is not supported.\n    with translation.override('unsupported_language'):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['lang'], 'en')\n", "def test_build_attrs_with_i18n(self):\n    with translation.override('fr'):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['lang'], 'fr')\n", "def test_autocomplete_select_multiple(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'featuring': [beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option1 = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    selected_option2 = '<option value=\"%s\" selected>The Who</option>' % who.pk\n    self.assertIn(selected_option1, output)\n    self.assertIn(selected_option2, output)\n"], "sample_350": ["def test_difference_with_values_list_and_order_on_annotation(self):\n    qs1 = Number.objects.annotate(\n        annotation=Value(-1),\n        multiplier=F('annotation'),\n    ).filter(num__gte=6)\n    qs2 = Number.objects.annotate(\n        annotation=Value(2),\n        multiplier=F('annotation'),\n    ).filter(num__lte=5)\n    self.assertSequenceEqual(\n        qs1.difference(qs2).order_by('annotation', 'num').values_list('num', flat=True),\n        [6, 7, 8, 9],\n    )\n    self.assertQuerysetEqual(\n        qs1.difference(qs2).order_by(\n            F('annotation') * F('multiplier'),\n            'num',\n        ).values('num'),\n        [6, 7, 8, 9],\n        operator.itemgetter('num'),\n    )\n", "def test_union_with_distinct(self):\n    qs1 = Number.objects.filter(num__lte=5).values('num')\n    qs2 = Number.objects.filter(num__gte=5).values('num')\n    self.assertEqual(len(list(qs1.union(qs2).distinct())), 10)\n    self.assertEqual(len(list(qs1.union(qs2).distinct().union(qs1).distinct())), 10)\n    self.assertEqual(len(list(qs1.union(qs2, all=True).distinct())), 10)\n", "def test_get_with_empty_filter_supported_on_combined_qs(self):\n    qs = Number.objects.all()\n    combinators = ['union']\n    if connection.features.supports_select_difference:\n        combinators.append('difference')\n    if connection.features.supports_select_intersection:\n        combinators.append('intersection')\n    for combinator in combinators:\n        with self.subTest(combinator=combinator):\n            result = getattr(qs, combinator)(qs).get()\n            self.assertIsNotNone(result)\n", "def test_combine_with_values_list_and_flat(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.all()\n    reserved_name = qs1.union(qs1).values_list('order', flat=True).get()\n    self.assertEqual(reserved_name, 2)\n", "def test_union_with_annotate(self):\n    qs1 = Number.objects.annotate(is_even=F('num') % 2 == 0).filter(is_even=True)\n    qs2 = Number.objects.annotate(is_even=F('num') % 2 == 0).filter(is_even=False)\n    self.assertNumbersEqual(qs1.union(qs2).order_by('num'), list(range(10)))\n"], "sample_351": ["def test_choices_prefetch_related(self):\n    f = forms.ModelChoiceField(Category.objects.prefetch_related('article_set'))\n    self.assertEqual(len(f.choices), 4)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n    with self.assertNumQueries(0):\n        # No additional query is made when accessing the prefetched objects.\n        for category in f.queryset:\n            list(category.article_set.all())\n", "def test_modelchoiceiterator_bool(self):\n    iterator = ModelChoiceIterator(forms.ModelChoiceField(Category.objects.all()))\n    self.assertIs(bool(iterator), True)\n    Category.objects.all().delete()\n    self.assertIs(bool(iterator), False)\n", "def test_modelchoicefield_with_custom_queryset(self):\n    class CustomCategoryManager(models.Manager):\n            return super().get_queryset().filter(slug__contains='test')\n\n    class CategoryWithCustomManager(Category):\n        objects = CustomCategoryManager()\n\n    f = forms.ModelChoiceField(CategoryWithCustomManager.objects.all())\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_choice_iterator_value_str(self):\n    value = ModelChoiceIteratorValue(self.c1.pk, self.c1)\n    self.assertEqual(str(value), str(self.c1.pk))\n", "def test_initial_value_not_instance_of_queryset_model(self):\n    class ModelChoiceForm(forms.Form):\n        category = forms.ModelChoiceField(Category.objects.all())\n\n    form = ModelChoiceForm(initial={'category': Book.objects.create()})\n    self.assertEqual(form.initial, {'category': None})\n"], "sample_352": ["    def test_ticket_24711(self):\n        \"\"\"\n        A subquery with an Exists node should be able to use an alias from the\n        outer query in its filter.\n        \"\"\"\n        Note.objects.create(note='n1', misc='foo')\n        Note.objects.create(note='n2', misc='bar')\n        Tag.objects.create(name='t1')\n        Annotation.objects.create(name='a1', tag=Tag.objects.get(name='t1'))\n        Annotation.objects.create(name='a2', tag=Tag.objects.get(name='t1'))\n\n        self.assertSequenceEqual(\n            Note.objects.annotate(\n                has_annotation=Exists(\n                    Annotation.objects.filter(\n                        notes=OuterRef('pk'),\n                        tag__name='t1',\n                    )\n                )\n            ).filter(has_annotation=True),\n            []\n        )\n", "    def test_ticket_24863(self):\n        \"\"\"\n        When filtering on an annotation that's a boolean field, the filter\n        should work correctly.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        i2 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(Individual.objects.annotate(alive_annotated=F('alive')).filter(alive_annotated=True), [i1])\n        self.assertSequenceEqual(Individual.objects.annotate(alive_annotated=F('alive')).filter(alive_annotated=False), [i2])\n", "    def test_ticket_24661(self):\n        \"\"\"\n        The __in lookup with a subquery should work when the subquery has an \n        ordering.\n        \"\"\"\n        tag1 = Tag.objects.create(name='tag1')\n        tag2 = Tag.objects.create(name='tag2')\n        note1 = Note.objects.create(note='note1', misc='misc1')\n        note2 = Note.objects.create(note='note2', misc='misc2')\n        annotation1 = Annotation.objects.create(name='annotation1', tag=tag1)\n        annotation1.notes.add(note1)\n        annotation2 = Annotation.objects.create(name='annotation2', tag=tag2)\n        annotation2.notes.add(note2)\n\n        subquery = Annotation.objects.order_by('name').values_list('tag_id', flat=True)\n        self.assertSequenceEqual(Tag.objects.filter(id__in=subquery), [tag1, tag2])\n", "    def test_ticket_24748(self):\n        \"\"\"\n        Test that filtering on a subquery with an IN clause doesn't produce a SQL syntax error.\n        \"\"\"\n        tag1 = Tag.objects.create(name='tag1')\n        tag2 = Tag.objects.create(name='tag2')\n        annotation = Annotation.objects.create(name='annotation', tag=tag1)\n        self.assertSequenceEqual(\n            Tag.objects.filter(pk__in=Tag.objects.filter(annotation=annotation).values_list('pk', flat=True)),\n            [tag1],\n        )\n        self.assertSequenceEqual(\n            Tag.objects.filter(pk__in=Tag.objects.exclude(annotation=annotation).values_list('pk', flat=True)),\n            [tag2],\n        )\n", "    def test_ticket_24869(self):\n        \"\"\"\n        A subquery with a single expression in values() should not include an\n        unnecessary join.\n        \"\"\"\n        apple = Food.objects.create(name=\"apple\")\n        lunch = Eaten.objects.create(food=apple, meal=\"lunch\")\n        qs = Eaten.objects.filter(food__in=Eaten.objects.values('food'))\n        self.assertEqual(str(qs.query).count('JOIN'), 1)\n        self.assertSequenceEqual(qs, [lunch])\n"], "sample_353": ["    def test_get_input_data(self):\n        field = User._meta.get_field('username')\n        message = 'Username: '\n        default = None\n        raw_value = 'joe'\n        expected_val = 'joe'\n\n        command = createsuperuser.Command()\n        val = command.get_input_data(field, message, default)\n        with mock.patch.object(builtins, 'input', return_value=raw_value):\n            self.assertEqual(command.get_input_data(field, message, default), expected_val)\n", "    def setUp(self):\n        self.command = createsuperuser.Command()\n        self.field = User._meta.get_field('username')\n", "    def test_get_default_username_with_custom_user_model(self):\n        management.get_system_username = lambda: 'joe'\n        self.assertEqual(management.get_default_username(), 'joe')\n", "    def test_custom_user_model_validation(self):\n        new_io = StringIO()\n\n        @mock_inputs({\n            'password': 'janedoe',\n            'username': 'jane',\n            'email': 'jane@example.com',\n            'first_name': 'Jane',\n            'date_of_birth': '1970-01-01',\n            'bypass': 'n',\n        })\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                stdout=new_io,\n                stderr=new_io,\n            )\n            self.assertEqual(\n                new_io.getvalue().strip(),\n                \"The password is too similar to the first name.\\n\"\n                \"Superuser created successfully.\"\n            )\n\n        test(self)\n", "def test_fields_with_fk_non_existent(self):\n    new_io = StringIO()\n    email = Email.objects.create(email='mymail@gmail.com')\n\n    @mock_inputs({\n        'password': 'nopasswd',\n        'Username (Email.id): ': email.pk + 1,\n        'Email (Email.email): ': email.email,\n        'Group (Group.id): ': Group.objects.create(name='mygroup').pk,\n    })\n        msg = 'email instance with id %s does not exist.' % (email.pk + 1)\n        with self.assertRaisesMessage(CommandError, msg):\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdout=new_io,\n                stdin=MockTTY(),\n            )\n\n    test(self)\n"], "sample_354": ["    def test_get_default_username_with_custom_user_model(self):\n        # Mock the system username to match an existing CustomUser instance.\n        management.get_system_username = lambda: 'existing@example.com'\n        CustomUser.objects.create(email='existing@example.com')\n\n        default_username = get_default_username()\n        self.assertEqual(default_username, '')\n\n        # Test with a non-existent system username.\n        management.get_system_username = lambda: 'nonexistent@example.com'\n        default_username = get_default_username()\n        self.assertEqual(default_username, 'nonexistent@example.com')\n", "    def test_custom_user_model_with_required_fields(self):\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            email=\"joe@somewhere.org\",\n            date_of_birth=\"1976-04-01\",\n            first_name='Joe',\n            stdout=new_io,\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        u = CustomUser._default_manager.get(email=\"joe@somewhere.org\")\n        self.assertEqual(u.date_of_birth, date(1976, 4, 1))\n        self.assertEqual(u.first_name, 'Joe')\n", "    def test_get_default_username_with_custom_user_model(self):\n        default_username = get_default_username()\n        self.assertIsInstance(default_username, str)\n        # CustomUser uses 'email' as the username field\n        self.assertNotEqual(default_username, '')\n", "    def test_get_input_data(self):\n        command = createsuperuser.Command()\n        field = User._meta.get_field('username')\n        message = 'Username: '\n        input_value = 'test_username'\n        with mock.patch.object(builtins, 'input', return_value=input_value):\n            value = command.get_input_data(field, message)\n        self.assertEqual(value, input_value)\n", "    def test_custom_user_required_fields(self):\n        new_io = StringIO()\n        with self.assertRaisesMessage(CommandError, 'You must use --date_of_birth with --noinput.'):\n            call_command(\n                \"createsuperuser\",\n                interactive=False,\n                email=\"joe@somewhere.org\",\n                username=\"joe\",\n                first_name=\"Joe\",\n                stdout=new_io\n            )\n\n        with self.assertRaisesMessage(CommandError, 'You must use --first_name with --noinput.'):\n            call_command(\n                \"createsuperuser\",\n                interactive=False,\n                email=\"joe@somewhere.org\",\n                username=\"joe\",\n                date_of_birth=\"1976-04-01\",\n                stdout=new_io\n            )\n\n        call_command(\n            \"createsuperuser\",\n            interactive=False,\n            email=\"joe@somewhere.org\",\n            username=\"joe\",\n            date_of_birth=\"1976-04-01\",\n            first_name=\"Joe\",\n            stdout=new_io\n        )\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n"], "sample_355": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.content_type = ContentType.objects.get_for_model(Group)\n        cls.perm = Permission.objects.create(\n            name='test', content_type=cls.content_type, codename='test'\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.content_type = ContentType.objects.get_for_model(Group)\n        cls.perm1 = Permission.objects.create(\n            name='test_perm1', content_type=cls.content_type, codename='test_perm1'\n        )\n        cls.perm2 = Permission.objects.create(\n            name='test_perm2', content_type=cls.content_type, codename='test_perm2'\n        )\n        cls.perm3 = Permission.objects.create(\n            name='test_perm3', content_type=cls.content_type, codename='test_perm3'\n        )\n", "    def setUpTestData(cls):\n        cls.content_type = ContentType.objects.get_for_model(Group)\n        cls.permission1 = Permission.objects.create(name='test1', content_type=cls.content_type, codename='test1')\n        cls.permission2 = Permission.objects.create(name='test2', content_type=cls.content_type, codename='test2')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        content_type = ContentType.objects.get_for_model(Group)\n        perm1 = Permission.objects.create(name='test1', content_type=content_type, codename='test1')\n        perm2 = Permission.objects.create(name='test2', content_type=content_type, codename='test2')\n        cls.user.user_permissions.add(perm1, perm2)\n", "    def test_permission_model(self):\n        permission = Permission.objects.create(\n            name='test_permission',\n            content_type=ContentType.objects.get_for_model(Group),\n            codename='test_codename'\n        )\n        self.assertEqual(permission.__str__(), 'group | test_permission')\n        self.assertEqual(permission.natural_key(), ('test_codename', 'auth', 'group'))\n"], "sample_356": ["def test_autodetector_swappable_model_inheritance(self):\n    \"\"\"\n    If a swappable model is used as a base in a model inheritance,\n    the dependency on the swappable model should be included.\n    \"\"\"\n    changes = self.get_changes(\n        [],\n        [\n            ModelState(\"app\", \"MyUser\", [], bases=(\"auth.User\",)),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertMigrationDependencies(changes, \"app\", 0, [(\"auth\", \"__first__\")])\n", "def test_generate_altered_order_with_respect_to_with_existing_index(self):\n    \"\"\"\n    AlterOrderWithRespectTo does not recreate an index on _order when one\n    already exists.\n    \"\"\"\n    model_state = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"_order\", models.IntegerField()),\n    ], options={'order_with_respect_to': 'book'}, bases=('otherapp.Book',))\n    project_state = self.make_project_state([model_state])\n    project_state.apps.register_model('otherapp', 'Book', model_state.bases[0])\n    new_state = project_state.clone()\n    new_state.models[\"testapp\", \"author\"].fields[\"_order\"].db_tablespace = 'new_tablespace'\n    autodetector = MigrationAutodetector(project_state, new_state)\n    changes = autodetector._detect_changes()\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', name='_order')\n    self.assertEqual(changes['testapp'][0].operations[0].field.db_tablespace, 'new_tablespace')\n", "def test_alter_model_table_with_custom_database_table_name(self):\n    \"\"\"\n    AlterModelTable operation should be generated when custom database table name is changed.\n    \"\"\"\n    before = [\n        ModelState('app', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={'db_table': 'custom_table_name'}),\n    ]\n    after = [\n        ModelState('app', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={'db_table': 'new_custom_table_name'}),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='model', table='new_custom_table_name')\n", "def test_add_model_with_foreign_key_to_proxy_model(self):\n    \"\"\"\n    Adding a model with a foreign key to a proxy model should create the \n    foreign key field before creating the model.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.author_proxy],\n        [self.author_empty, self.author_proxy, self.book_proxy_fk]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"Book\")\n    self.assertEqual(\n        changes['otherapp'][0].operations[0].fields[2][1].remote_field.model,\n        'testapp.AuthorProxy'\n    )\n", "def test_rename_model_with_mti_inheritance(self):\n    \"\"\"\n    Renaming a model that has multi-table inheritance children should also\n    update the bases of the children.\n    \"\"\"\n    before = [\n        ModelState('app', 'Animal', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Dog', [], bases=('app.Animal',)),\n    ]\n    after = [\n        ModelState('app', 'Creature', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app', 'Dog', [], bases=('app.Creature',)),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename_model': True}))\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, old_name='Animal', new_name='Creature')\n"], "sample_357": ["def test_generate_altered_db_table_with_model_change_same_table(self):\n    \"\"\"\n    Tests when model changes but db_table stays as-is, autodetector must not\n    create more than one operation.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_renamed_with_db_table_options],\n        [self.author_renamed_with_db_table_options],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 0)\n", "def test_m2m_renamed_through_model_with_different_app_label(self):\n    \"\"\"\n    Tests autodetection of renamed models that are used in M2M relations as\n    through models when the app label has changed.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_m2m_through, self.publisher, self.contract],\n        [\n            ModelState(\"newapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"publishers\", models.ManyToManyField(\"testapp.Publisher\", through=\"newapp.Deal\")),\n            ]),\n            self.publisher,\n            ModelState(\"newapp\", \"Deal\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"newapp.Author\", models.CASCADE)),\n                (\"publisher\", models.ForeignKey(\"testapp.Publisher\", models.CASCADE)),\n            ]),\n        ],\n        MigrationQuestioner({'ask_rename_model': True})\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'newapp', 1)\n    self.assertOperationTypes(changes, 'newapp', 0, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'newapp', 0, 0, old_name='Contract', new_name='Deal')\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['DeleteModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author')\n", "def test_only_relation_agnostic_fields(self):\n    \"\"\"Tests only_relation_agnostic_fields().\"\"\"\n    model_state = ModelState(\n        \"testapp\", \"Model\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field1\", models.CharField(max_length=200)),\n            (\"field2\", models.ForeignKey(\"otherapp.Model\", models.CASCADE)),\n        ]\n    )\n    fields_def = MigrationAutodetector.only_relation_agnostic_fields(model_state.fields)\n    self.assertEqual(len(fields_def), 3)\n    self.assertEqual(fields_def[0][0], 'django.db.models.AutoField')\n    self.assertEqual(fields_def[1][0], 'django.db.models.CharField')\n    self.assertEqual(fields_def[2][0], 'django.db.models.ForeignKey')\n    # field2's to is removed.\n    self.assertNotIn('to', fields_def[2][2])\n", "def test_create_model_with_unique_together(self):\n    \"\"\"Test creation of new model with unique_together already defined.\"\"\"\n    author = ModelState('otherapp', 'Author', [\n        ('id', models.AutoField(primary_key=True)),\n        ('name', models.CharField(max_length=200)),\n    ], {\n        \"unique_together\": {(\"name\",)},\n    })\n    changes = self.get_changes([], [author])\n    # Right number of migrations?\n    self.assertEqual(len(changes['otherapp']), 1)\n    # Right number of actions?\n    migration = changes['otherapp'][0]\n    self.assertEqual(len(migration.operations), 2)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AlterUniqueTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Author')\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='author', unique_together={(\"name\",)})\n", "def test_custom_migration_name_with_suffix(self):\n    \"\"\"Tests custom naming of migrations for graph matching with suffix.\"\"\"\n    # Make a fake graph\n    graph = MigrationGraph()\n    graph.add_node((\"testapp\", \"0001_initial\"), None)\n    graph.add_node((\"testapp\", \"0002_foobar\"), None)\n    graph.add_node((\"otherapp\", \"0001_initial\"), None)\n    graph.add_dependency(\"testapp.0002_foobar\", (\"testapp\", \"0002_foobar\"), (\"testapp\", \"0001_initial\"))\n\n    # Use project state to make a new migration change set\n    before = self.make_project_state([])\n    after = self.make_project_state([self.author_empty, self.other_pony, self.other_stable])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n\n    # Run through arrange_for_graph\n    migration_name = 'custom_name'\n    changes = autodetector.arrange_for_graph(changes, graph, migration_name + '_suffix')\n\n    # Make sure there's a new name, deps match, etc.\n    self.assertEqual(changes[\"testapp\"][0].name, \"0003_%s\" % (migration_name + '_suffix'))\n    self.assertEqual(changes[\"testapp\"][0].dependencies, [(\"testapp\", \"0002_foobar\")])\n    self.assertEqual(changes[\"otherapp\"][0].name, \"0002_%s\" % (migration_name + '_suffix'))\n    self.assertEqual(changes[\"otherapp\"][0].dependencies, [(\"otherapp\", \"0001_initial\")])\n"], "sample_358": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=['ASC', 'DESC'], opclasses=['text_ops', 'int_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=('ASC', 'DESC'), opclasses=('varchar_pattern_ops', 'text_pattern_ops')\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=('', ' DESC'), opclasses=('integer_ops', 'text_ops')\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            opclasses=['varchar_ops', 'text_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            opclasses=['varchar_pattern_ops', 'int4_ops']\n        )\n"], "sample_359": ["def test_references_field_by_through_fields(self):\n    operation = FieldOperation(\n        'Model', 'field', models.ManyToManyField('Other', through='Through', through_fields=('field1', 'field2'))\n    )\n    self.assertIs(operation.references_field('Through', 'field1', 'migrations'), True)\n    self.assertIs(operation.references_field('Through', 'field2', 'migrations'), True)\n    self.assertIs(operation.references_field('Through', 'whatever', 'migrations'), False)\n    self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), True)\n    self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "def test_reduce_references_model(self):\n    operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE))\n    other_operation = FieldOperation('Model', 'field', models.BooleanField(default=False))\n    # Model name match.\n    self.assertIs(operation.reduce(other_operation, 'mOdEl', 'migrations'), False)\n    # Referenced field.\n    self.assertIs(operation.reduce(other_operation, 'oTher', 'migrations'), False)\n    # Doesn't reference.\n    self.assertIs(operation.reduce(other_operation, 'Whatever', 'migrations'), True)\n", "def test_alter_field_type_when_field_in_unique_constraint(self):\n    app_label = 'test_alter_field_type_unique_constraint'\n    project_state = self.apply_operations(app_label, ProjectState(), operations=[\n        migrations.CreateModel('Pony', fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('pink', models.CharField(max_length=20, unique=True)),\n        ]),\n    ])\n    operation = migrations.AlterField(\n        model_name='Pony',\n        name='pink',\n        field=models.TextField(unique=True),\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    Pony = new_state.apps.get_model(app_label, 'Pony')\n    pony = Pony.objects.create(pink='some pink')\n    with self.assertRaises(IntegrityError), transaction.atomic():\n        Pony.objects.create(pink='some pink')\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    Pony = project_state.apps.get_model(app_label, 'Pony')\n    pony = Pony.objects.create(pink='some pink')\n    with self.assertRaises(IntegrityError), transaction.atomic():\n        Pony.objects.create(pink='some pink')\n", "def test_reduce_create_model_with_index(self):\n    operation = migrations.CreateModel(\n        \"Pony\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"pink\", models.IntegerField(default=1)),\n        ],\n        options={\n            'indexes': [models.Index(fields=['pink'], name='pony_pink_idx')]\n        },\n    )\n    self.assertEqual(operation.describe(), \"Create model Pony\")\n    self.assertEqual(operation.migration_name_fragment, 'pony')\n    # Test the state alteration\n    project_state = ProjectState()\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_crmo\", new_state)\n    self.assertEqual(new_state.models[\"test_crmo\", \"pony\"].name, \"Pony\")\n    self.assertEqual(len(new_state.models[\"test_crmo\", \"pony\"].fields), 2)\n    self.assertEqual(len(new_state.models[\"test_crmo\", \"pony\"].options.get('indexes', [])), 1)\n    # Test the database alteration\n    self.assertTableNotExists(\"test_crmo_pony\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_crmo\", editor, project_state, new_state)\n    self.assertTableExists(\"test_crmo_pony\")\n    self.assertIndexExists(\"test_crmo_pony\", [\"pink\"])\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_crmo\", editor, new_state, project_state)\n    self.assertTableNotExists(\"test_crmo_pony\")\n    # And deconstruction\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"CreateModel\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(sorted(definition[2]), [\"fields\", \"name\", \"options\"])\n", "def test_references_field_by_through_fields(self):\n    operation = FieldOperation(\n        'Model', 'field', models.ManyToManyField('Other', through='Through', through_fields=('source', 'target'))\n    )\n    self.assertIs(operation.references_field('Through', 'source', 'migrations'), True)\n    self.assertIs(operation.references_field('Through', 'target', 'migrations'), True)\n    self.assertIs(operation.references_field('Through', 'whatever', 'migrations'), False)\n    self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), True)\n    self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n"], "sample_360": ["    def tearDown(self):\n        cache.clear()\n", "    def tearDown(self):\n        cache.clear()\n", "    def tearDown(self):\n        cache.clear()\n", "    def test_cache_versioning_get_set(self):\n        cache.set('answer1', 42, version=2)\n        self.assertIsNone(cache.get('answer1'))\n        self.assertEqual(cache.get('answer1', version=2), 42)\n", "    def tearDown(self):\n        cache.clear()\n"], "sample_361": ["def test_urlize_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('Check out www.google.com!', 'Check out <a href=\"http://www.google.com\">www.google.com</a>!'),\n        ('Check out www.google.com?', 'Check out <a href=\"http://www.google.com\">www.google.com</a>?'),\n        ('Check out www.google.com:', 'Check out <a href=\"http://www.google.com\">www.google.com</a>:'),\n        ('Check out www.google.com;', 'Check out <a href=\"http://www.google.com\">www.google.com</a>;'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            'Search for google.com/?q=!.',\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!.'\n        ),\n        (\n            'Search for google.com/?q=1&lt!?',\n            'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>!?'\n        ),\n        (\n            lazystr('Search for google.com/?q=!.'),\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!.'\n        ),\n        ('foo@example.com.', '<a href=\"mailto:foo@example.com\">foo@example.com</a>.'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_no_follow(self):\n    tests = (\n        (\n            'Search for google.com/?q=! and see.',\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n        ),\n        (\n            'foo@example.com',\n            '<a href=\"mailto:foo@example.com\">foo@example.com</a>'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        ('Check out www.google.com.', 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        ('Check out www.google.com!', 'Check out <a href=\"http://www.google.com\">www.google.com</a>!'),\n        ('Check out www.google.com?', 'Check out <a href=\"http://www.google.com\">www.google.com</a>?'),\n        ('Check out www.google.com,', 'Check out <a href=\"http://www.google.com\">www.google.com</a>,'),\n        ('Check out www.google.com;', 'Check out <a href=\"http://www.google.com\">www.google.com</a>;'),\n        ('Check out www.google.com:', 'Check out <a href=\"http://www.google.com\">www.google.com</a>:'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            'Search for google.com.',\n            'Search for <a href=\"http://google.com/\">google.com</a>.'\n        ),\n        (\n            'Search for google.com!?',\n            'Search for <a href=\"http://google.com/\">google.com</a>!?'\n        ),\n        (\n            lazystr('Search for google.com...'),\n            'Search for <a href=\"http://google.com/\">google.com</a>...'\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_362": ["def test_alter_field_with_same_name_different_type(self):\n    \"\"\"Tests autodetection of altered fields with same name but different type.\"\"\"\n    changes = self.get_changes([self.author_name], [self.author_dates_of_birth_auto_now])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now=True)\n", "def test_alter_model_options_empty(self):\n    \"\"\"\n    Changing a model's options to empty should make a change.\n    \"\"\"\n    changes = self.get_changes([self.author_with_options], [self.author_empty])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", options={})\n", "def test_generate_added_indexes_unique_together(self):\n    before = [\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ], options={\n            'indexes': [models.Index(fields=['name'], name='author_name_idx')],\n            'unique_together': {('name',)},\n        }),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number of migrations?\n    self.assertEqual(len(changes['testapp']), 1)\n    # Right number of actions?\n    migration = changes['testapp'][0]\n    self.assertEqual(len(migration.operations), 3)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddIndex', 'AlterUniqueTogether'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author', index=models.Index(fields=['name'], name='author_name_idx'))\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='author', unique_together={('name',)})\n", "def test_alter_model_table_with_custom_database_table(self):\n    \"\"\"\n    AlterModelTable correctly handles a custom db_table when renaming a model.\n    \"\"\"\n    initial_model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        options={\"db_table\": \"custom_author_table\"},\n    )\n    altered_model_state = ModelState(\n        \"testapp\",\n        \"NewAuthor\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        options={\"db_table\": \"new_author_table\"},\n    )\n\n    changes = self.get_changes([initial_model_state], [altered_model_state])\n\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"newauthor\", table=\"new_author_table\")\n", "def test_alter_unique_together_with_conditions(self):\n    before = [\n        ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field1', models.IntegerField()),\n            ('field2', models.IntegerField()),\n        ], options={\n            'unique_together': {('field1', 'field2')},\n        }),\n    ]\n    after = [\n        ModelState('app', 'Model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field1', models.IntegerField()),\n            ('field2', models.IntegerField()),\n        ], options={\n            'unique_together': UniqueConstraint(\n                fields=('field1', 'field2'), condition=models.Q(field1=1), name='custom_unique'\n            ),\n        }),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterUniqueTogether'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, name='model',\n        unique_together={('field1', 'field2')},\n    )\n    constraint = changes['app'][0].operations[0].constraints[0]\n    self.assertIsInstance(constraint, UniqueConstraint)\n    self.assertEqual(constraint.fields, ('field1', 'field2'))\n    self.assertEqual(constraint.condition, models.Q(field1=1))\n    self.assertEqual(constraint.name, 'custom_unique')\n"], "sample_363": ["    def test_build_attrs(self):\n        w = widgets.AutocompleteSelect(Member._meta.get_field('band').remote_field, admin.site)\n        attrs = w.build_attrs({'id': 'id_band'})\n        self.assertEqual(attrs['data-ajax--delay'], 250)\n        self.assertEqual(attrs['data-ajax--cache'], 'true')\n        self.assertEqual(attrs['data-theme'], 'admin-autocomplete')\n        self.assertEqual(attrs['data-allow-clear'], 'false')\n", "    def test_get_url(self):\n        class MyAutocompleteMixin(AutocompleteMixin):\n            url_name = 'myapp:my_url'\n\n        mixin = MyAutocompleteMixin(None, admin.site)\n        self.assertEqual(mixin.get_url(), reverse('myapp:my_url'))\n", "    def test_build_attrs(self):\n        w = widgets.AutocompleteMixin(\n            field=Member._meta.get_field('band'),\n            admin_site=admin.site,\n            attrs={'class': 'my-class'},\n        )\n        attrs = w.build_attrs({'id': 'my-id'}, extra_attrs={'required': True})\n        self.assertEqual(attrs['id'], 'my-id')\n        self.assertEqual(attrs['class'], 'my-class')\n        self.assertEqual(attrs['required'], True)\n        self.assertEqual(attrs['data-ajax--delay'], 250)\n        self.assertEqual(attrs['data-theme'], 'admin-autocomplete')\n        self.assertEqual(attrs['lang'], SELECT2_TRANSLATIONS.get(get_language()))\n", "    def test_get_url(self):\n        w = widgets.AutocompleteMixin(field=None, admin_site=admin.site)\n        self.assertEqual(w.get_url(), reverse('admin:autocomplete'))\n", "    def test_attrs(self):\n        w = widgets.AdminUUIDInputWidget()\n        self.assertHTMLEqual(\n            w.render('test', '550e8400-e29b-41d4-a716-446655440000'),\n            '<input value=\"550e8400-e29b-41d4-a716-446655440000\" type=\"text\" class=\"vUUIDField\" name=\"test\">',\n        )\n        w = widgets.AdminUUIDInputWidget(attrs={'class': 'myUUIDInput'})\n        self.assertHTMLEqual(\n            w.render('test', '550e8400-e29b-41d4-a716-446655440000'),\n            '<input value=\"550e8400-e29b-41d4-a716-446655440000\" type=\"text\" class=\"myUUIDInput\" name=\"test\">',\n        )\n"], "sample_364": ["    def test_include_with_namespace(self):\n        match = resolve('/included_urls/namespaced/something/')\n        self.assertEqual(match.url_name, 'inner-namespaced')\n        self.assertEqual(match.namespace, 'namespaced')\n", "    def test_include_with_app_name(self):\n        match = resolve('/included_urls/extra/something/')\n        self.assertEqual(match.url_name, 'inner-extra')\n        self.assertEqual(match.app_name, 'testapp')\n        self.assertEqual(match.kwargs, {'extra': 'something'})\n", "def test_include_with_missing_namespace(self):\n    msg = (\n        'Specifying a namespace in include() without providing an app_name '\n        'is not supported. Set the app_name attribute in the included '\n        'module, or pass a 2-tuple containing the list of patterns and '\n        'app_name instead.',\n    )\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        include('urlpatterns.included_urls', namespace='namespace')\n", "    def test_include_with_tuple(self):\n        urlconf_module, app_name, namespace = include(('path.urls', 'app_name'))\n        self.assertEqual(urlconf_module.__name__, 'path.urls')\n        self.assertEqual(app_name, 'app_name')\n        self.assertEqual(namespace, 'app_name')\n", "    def test_include(self):\n        match = resolve('/included/extra/something/')\n        self.assertEqual(match.url_name, 'inner-extra')\n        self.assertEqual(match.route, 'extra/<extra>/')\n"], "sample_365": ["def test_classproperty(self):\n    class Foo:\n        foo_attr = 123\n\n        @classproperty\n            return cls.foo_attr\n\n    self.assertEqual(Foo.foo, 123)\n    self.assertEqual(Foo().foo, 123)\n\n    Foo.foo_attr = 456\n    self.assertEqual(Foo.foo, 456)\n    self.assertEqual(Foo().foo, 456)\n", "def test_classproperty(self):\n    class Foo:\n        foo_attr = 123\n\n            self.foo_attr = 456\n\n        @classproperty\n            return cls.foo_attr\n\n    self.assertEqual(Foo.foo, 123)\n    self.assertEqual(Foo().foo, 123)\n", "def test_classproperty(self):\n    class Foo:\n        foo_attr = 123\n\n        @classproperty\n            return cls.foo_attr\n\n    self.assertEqual(Foo.foo, 123)\n    self.assertEqual(Foo().foo, 123)\n\n    Foo.foo_attr = 456\n    self.assertEqual(Foo.foo, 456)\n    self.assertEqual(Foo().foo, 456)\n\n    # Test that classproperty works with inheritance\n    class Bar(Foo):\n        foo_attr = 789\n\n    self.assertEqual(Bar.foo, 789)\n    self.assertEqual(Bar().foo, 789)\n", "def test_lazy_object_eq_lt_gt(self):\n    lazy_obj = lazy(lambda: 5, int)\n    self.assertEqual(lazy_obj(), 5)\n    self.assertLess(lazy_obj(), 6)\n    self.assertGreater(lazy_obj(), 4)\n", "def test_classproperty_getter_with_instance(self):\n    class Foo:\n        foo_attr = 123\n\n            self.foo_attr = 456\n\n        @classproperty\n            return cls.foo_attr\n\n    foo = Foo()\n    self.assertEqual(Foo.foo, 123)\n    with self.assertRaises(AttributeError):\n        foo.foo\n"], "sample_366": ["def test_parse_duration_iso8601(self):\n    test_values = (\n        ('P3DT12H30M5S', timedelta(days=3, hours=12, minutes=30, seconds=5)),\n        ('PT3H5M', timedelta(hours=3, minutes=5)),\n        ('PT5S', timedelta(seconds=5)),\n        ('P5D', timedelta(days=5)),\n        ('-P3DT12H30M5S', timedelta(days=-3, hours=-12, minutes=-30, seconds=-5)),\n        ('P0D', timedelta(days=0)),\n        ('PT0S', timedelta(seconds=0)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_duration_iso8601_format(self):\n    test_values = (\n        ('P3DT12H30M5S', timedelta(days=3, hours=12, minutes=30, seconds=5)),\n        ('PT3H15M10S', timedelta(hours=3, minutes=15, seconds=10)),\n        ('P1D', timedelta(days=1)),\n        ('PT1H', timedelta(hours=1)),\n        ('PT1M', timedelta(minutes=1)),\n        ('PT1S', timedelta(seconds=1)),\n        ('-P3DT12H30M5S', timedelta(days=-3, hours=-12, minutes=-30, seconds=-5)),\n        ('P0D', timedelta()),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('P1DT12H', timedelta(days=1, hours=12)),\n        ('PT23H59M59S', timedelta(hours=23, minutes=59, seconds=59)),\n        ('P0.5D', timedelta(days=0.5)),\n        ('PT0.5H', timedelta(hours=0.5)),\n        ('P0DT0.5H', timedelta(hours=0.5)),\n        ('-P3D', timedelta(days=-3)),\n        ('-PT6H', timedelta(hours=-6)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('P1DT12H', timedelta(days=1, hours=12)),\n        ('PT4H2M', timedelta(hours=4, minutes=2)),\n        ('PT10M', timedelta(minutes=10)),\n        ('PT0S', timedelta(seconds=0)),\n        ('P0D', timedelta(days=0)),\n        ('-P3D', timedelta(days=-3)),\n        ('-PT10M', timedelta(minutes=-10)),\n        ('P3DT12H30M5S', timedelta(days=3, hours=12, minutes=30, seconds=5)),\n        ('P23DT23H59M59S', timedelta(days=23, hours=23, minutes=59, seconds=59)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n", "def test_parse_iso8601_duration(self):\n    test_values = (\n        ('P3D', timedelta(days=3)),\n        ('PT10H', timedelta(hours=10)),\n        ('PT10M', timedelta(minutes=10)),\n        ('PT10S', timedelta(seconds=10)),\n        ('P3DT10H', timedelta(days=3, hours=10)),\n        ('P3DT10M', timedelta(days=3, minutes=10)),\n        ('P3DT10S', timedelta(days=3, seconds=10)),\n        ('PT10H30M', timedelta(hours=10, minutes=30)),\n        ('PT10H30S', timedelta(hours=10, seconds=30)),\n        ('PT10M30S', timedelta(minutes=10, seconds=30)),\n        ('P3DT10H30M', timedelta(days=3, hours=10, minutes=30)),\n        ('P3DT10H30S', timedelta(days=3, hours=10, seconds=30)),\n        ('P3DT10M30S', timedelta(days=3, minutes=10, seconds=30)),\n        ('-P3D', timedelta(days=-3)),\n        ('-PT10H', timedelta(hours=-10)),\n        ('-PT10M', timedelta(minutes=-10)),\n        ('-PT10S', timedelta(seconds=-10)),\n    )\n    for source, expected in test_values:\n        with self.subTest(source=source):\n            self.assertEqual(parse_duration(source), expected)\n"], "sample_367": ["    def test_cache_page_decorator(self, mock_cache_middleware):\n        @cache_page(60)\n            return HttpResponse()\n\n        request = HttpRequest()\n        a_view(request)\n\n        mock_cache_middleware.assert_called_once_with(\n            get_response=mock.ANY,\n            page_timeout=60,\n            cache_alias=None,\n            key_prefix=None,\n        )\n", "    def test_cache_page_decorator(self, mock_process_response, mock_process_request):\n        @cache_page(123)\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = a_view(request)\n\n        self.assertTrue(mock_process_request.called)\n        self.assertTrue(mock_process_response.called)\n        self.assertEqual(mock_process_response.call_args[0][0], request)\n        self.assertEqual(mock_process_response.call_args[0][1], response)\n", "    def test_cache_page_decorator_with_timeout(self):\n        @cache_page(60)\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = a_view(request)\n        self.assertEqual(response.status_code, 200)\n", "    def test_cache_page_decorator(self, mock_CacheMiddleware):\n        @cache_page(60)\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = a_view(request)\n\n        self.assertEqual(response, HttpResponse())\n        mock_CacheMiddleware.assert_called_once_with(\n            page_timeout=60, cache_alias=None, key_prefix=None\n        )\n", "    def test_cache_page_decorator_http_request(self):\n        class MyClass:\n            @cache_page(60)\n                return HttpResponse()\n\n        request = HttpRequest()\n        response = MyClass().a_view(request)\n        self.assertIsInstance(response, HttpResponse)\n"], "sample_368": ["def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Tests that replaced migrations are correctly handled in the migration plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Load the graph with replaced migrations\n    executor.loader.replace_migrations = True\n    executor.loader.build_graph()\n    # Make a plan that includes a replaced migration\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Rebuild the graph without replacements\n    executor.loader.replace_migrations = False\n    executor.loader.build_graph()\n    # The plan should now include the individual migrations\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0001_1_auto\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Tests that replaced migrations are properly handled in the migration plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Make sure the replaced migration is not applied\n    self.assertNotIn((\"migrations\", \"0001_initial\"), executor.loader.applied_migrations)\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Fake-apply the first migration\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # The replaced migration should be marked as applied\n    self.assertIn((\"migrations\", \"0001_initial\"), executor.loader.applied_migrations)\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n", "def test_check_replacements(self):\n    \"\"\"\n    Replaced migrations are correctly marked as applied.\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    # Place the database in a state where the replaced migrations are\n    # partially applied: 0001 is applied, 0002 is not.\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    executor = MigrationExecutor(connection)\n    # Use fake because we don't actually have the first migration\n    # applied, so the second will fail. And there's no need to actually\n    # create/modify tables here, we're just testing the\n    # MigrationRecord, which works the same with or without fake.\n    executor.migrate([(\"migrations\", \"0002_second\")], fake=True)\n\n    # Because we've now applied 0001 and 0002 both, their squashed\n    # replacement should be marked as applied.\n    self.assertIn(\n        (\"migrations\", \"0001_squashed_0002\"),\n        recorder.applied_migrations(),\n    )\n\n    # Unapply all migrations to check that check_replacements doesn't mark\n    # the squashed migration as applied when its replaced migrations are unapplied.\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", None)])\n\n    # The squashed migration should be marked as unapplied.\n    self.assertNotIn(\n        (\"migrations\", \"0001_squashed_0002\"),\n        recorder.applied_migrations(),\n    )\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Tests that replaced migrations are properly ignored in the migration plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Load the graph with replaced migrations\n    executor.loader.replace_migrations = True\n    executor.loader.build_graph()\n    # Make a plan that would involve replaced migrations\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Now, let's fake-apply the first migration and rebuild the graph\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n    executor.loader.build_graph()\n    # The next plan should not include the replaced migration\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [(executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False)],\n    )\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Tests that replaced migrations are not included in the migration plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Fake-apply all migrations\n    executor.migrate([\n        (\"migrations\", \"0002_second\")\n    ], fake=True)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Check the plan\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(plan, [])\n    # Add a replacement migration\n    class ReplacementMigration(migrations.Migration):\n        replaces = [(\"migrations\", \"0001_initial\"), (\"migrations\", \"0002_second\")]\n    executor.loader.add_node((\"migrations\", \"0003_replacement\"), ReplacementMigration)\n    # Rebuild the graph to reflect the new migration\n    executor.loader.build_graph()\n    # Check the plan again\n    plan = executor.migration_plan([(\"migrations\", \"0003_replacement\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0003_replacement\"], False),\n        ],\n    )\n"], "sample_369": ["def test_rename_field_with_indexes(self):\n    before = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.IntegerField()),\n        ], options={\n            'index_together': {('field',)},\n        }),\n    ]\n    after = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('renamed_field', models.IntegerField(db_column='field')),\n        ], options={\n            'index_together': {('renamed_field',)},\n        }),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['AlterField', 'RenameField'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, model_name='foo', name='field',\n    )\n    self.assertEqual(changes['app'][0].operations[0].field.deconstruct(), (\n        'field', 'django.db.models.IntegerField', [], {'db_column': 'field'},\n    ))\n    self.assertOperationAttributes(\n        changes, 'app', 0, 1, model_name='foo', old_name='field',\n        new_name='renamed_field',\n    )\n", "def test_remove_model_with_mti_inheritance(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n", "def test_custom_migration_name_fragment(self):\n    class CustomMigrationNameFragment(migrations.CreateModel):\n            return 'custom_name'\n\n    class Migration(migrations.Migration):\n        operations = [CustomMigrationNameFragment(name='Person', fields=[])]\n\n    migration = Migration('some_migration', 'test_app')\n    self.assertEqual(migration.suggest_name(), 'custom_name')\n", "def test_alter_field_with_choices(self):\n    \"\"\"\n    Altering a field with choices should correctly preserve the choices.\n    \"\"\"\n    before = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, choices=[('A', 'A'), ('B', 'B')])),\n    ])\n    after = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, choices=[('A', 'A'), ('C', 'C')])),\n    ])\n    changes = self.get_changes([before], [after])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n    self.assertEqual(\n        changes['testapp'][0].operations[0].field.choices,\n        [('A', 'A'), ('C', 'C')]\n    )\n", "def test_alter_model_table_on_unmanaged_model(self):\n    changes = self.get_changes([self.author_unmanaged], [self.author_unmanaged_custom_pk])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"AuthorUnmanaged\", table=None)\n"], "sample_370": ["    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.author2 = Author.objects.create(name='Author 2')\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n        cls.book3 = Book.objects.create(title='Book 3')\n        cls.author1.books.add(cls.book1, cls.book2)\n        cls.author2.books.add(cls.book2, cls.book3)\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.author2 = Author.objects.create(name='Author 2')\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n        cls.book3 = Book.objects.create(title='Book 3')\n\n        cls.book1.authors.add(cls.author1)\n        cls.book2.authors.add(cls.author1, cls.author2)\n        cls.book3.authors.add(cls.author2)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n        cls.author1 = Author.objects.create(name='Author 1', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Author 2', first_book=cls.book2)\n", "    def setUpTestData(cls):\n        cls.department = Department.objects.create(name='Department')\n        cls.employee = Employee.objects.create(department=cls.department)\n"], "sample_371": ["    def test_get_cleansed_multivaluedict(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().post('/some_url/', {'foo': 'bar'})\n        multivaluedict = request.POST\n        self.assertEqual(reporter_filter.get_cleansed_multivaluedict(request, multivaluedict), multivaluedict)\n\n        request.sensitive_post_parameters = ['foo']\n        self.assertEqual(\n            reporter_filter.get_cleansed_multivaluedict(request, multivaluedict),\n            {'foo': reporter_filter.cleansed_substitute},\n        )\n", "    def test_cleanse_setting_with_callable(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        setting_value = lambda: 'super_secret'\n        cleansed_value = reporter_filter.cleanse_setting('SETTING_NAME', setting_value)\n        self.assertIsInstance(cleansed_value, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed_value), repr(setting_value))\n", "    def test_technical_404(self):\n        exception = Http404('Testing technical 404.')\n        request = self.rf.get('/test_view/')\n        response = technical_404_response(request, exception)\n        self.assertContains(\n            response,\n            '<h1>Page not found <span>(404)</span></h1>',\n            status_code=404,\n            html=True,\n        )\n        self.assertContains(response, 'Testing technical 404.', status_code=404)\n", "    def test_get_cleansed_multivaluedict(self):\n        request = RequestFactory().get('/')\n        filter = SafeExceptionReporterFilter()\n        multivaluedict = {'key1': ['value1'], 'key2': ['value2']}\n        cleansed_dict = filter.get_cleansed_multivaluedict(request, multivaluedict)\n        self.assertEqual(cleansed_dict, multivaluedict)\n\n        request.sensitive_post_parameters = ['key1']\n        cleansed_dict = filter.get_cleansed_multivaluedict(request, multivaluedict)\n        self.assertEqual(cleansed_dict, {'key1': filter.cleansed_substitute, 'key2': ['value2']})\n\n        request.sensitive_post_parameters = '__ALL__'\n        cleansed_dict = filter.get_cleansed_multivaluedict(request, multivaluedict)\n        self.assertEqual(cleansed_dict, {'key1': filter.cleansed_substitute, 'key2': filter.cleansed_substitute})\n", "    def test_technical_404(self):\n        exception = Http404('Testing technical 404.')\n        request = self.rf.get('/test_view/')\n        response = technical_404_response(request, exception)\n        self.assertContains(response, '<h1>Page not found <span>(404)</span></h1>', status_code=404)\n        self.assertContains(response, '<pre class=\"exception_value\">Testing technical 404.</pre>', status_code=404)\n"], "sample_372": ["    def test_warning_on_unnamed_pattern(self):\n        msg = (\n            \"URL route '<URLPattern '^unnamed/' [name=None]>' has a name \"\n            \"including a ':'. Remove the colon, to avoid ambiguous namespace \"\n            \"references.\"\n        )\n        warnings = check_resolver(get_resolver('urlpatterns_reverse.urls'))\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(warnings[0].msg, msg)\n", "    def test_locale_regex_descriptor(self):\n        resolver = get_resolver('urlpatterns_reverse.locale_urls')\n        pattern = resolver.url_patterns[0].pattern\n        self.assertEqual(pattern.regex.pattern, '^en/normal/$')\n", "    def test_locale_regex_descriptor(self):\n        resolver = get_resolver()\n        pattern = RegexPattern(r'^example/(?P<slug>[\\w-]+)/$')\n        pattern.regex = LocaleRegexDescriptor('_regex')\n        self.assertEqual(pattern.regex, re.compile(r'^example/(?P<slug>[\\w-]+)/$'))\n", "    def test_pattern_name_with_colon(self):\n        msg = (\n            \"URL route name 'my:view' may not contain a colon. Consider \"\n            \"using a different name or namespace for this route.\"\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            URLPattern(\n                RegexPattern(r'^test/$'),\n                views.empty_view,\n                name='my:view',\n            )\n", "    def test_locale_regex_descriptor(self):\n        pattern = RegexPattern(r'^example/')\n        self.assertIsInstance(pattern.regex, re.Pattern)\n"], "sample_373": ["    def test_simplify_regex(self):\n        cases = [\n            (\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\", \"/<sport_slug>/athletes/<athlete_slug>/\"),\n            (\"^$\", \"/\"),\n            (\"^abc/$\", \"/abc/\"),\n            (\"^(?P<pk>\\d+)/$\", \"/<pk>/\"),\n            (\"^abc/(?P<pk>\\d+)/$\", \"/abc/<pk>/\"),\n        ]\n        for pattern, expected in cases:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), expected)\n", "    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        simplified_pattern = simplify_regex(pattern)\n        self.assertEqual(simplified_pattern, \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        self.assertEqual(simplify_regex(pattern), \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        self.assertEqual(simplify_regex(pattern), \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_simplify_regex(self):\n        pattern = r\"^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$\"\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, \"/<sport_slug>/athletes/<athlete_slug>/\")\n"], "sample_374": ["    def setUpTestData(cls):\n        house = House.objects.create(name='Big house', address='123 Main St')\n        cls.room = Room.objects.create(name='Kitchen', house=house)\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.author2 = Author.objects.create(name='Author 2')\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n        cls.book1.authors.add(cls.author1, cls.author2)\n        cls.book2.authors.add(cls.author1)\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.author2 = Author.objects.create(name='Author 2')\n        cls.book1 = Book.objects.create(title='Book 1')\n        cls.book2 = Book.objects.create(title='Book 2')\n        cls.book3 = Book.objects.create(title='Book 3')\n        cls.book1.authors.add(cls.author1)\n        cls.book2.authors.add(cls.author2)\n        cls.book3.authors.add(cls.author1, cls.author2)\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.book1 = Book.objects.create(title='Book 1', author=cls.author1)\n        cls.book2 = Book.objects.create(title='Book 2', author=cls.author1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title='Poems')\n        cls.book2 = Book.objects.create(title='Jane Eyre')\n        cls.author1 = Author.objects.create(name='Charlotte', first_book=cls.book1)\n        cls.author2 = Author.objects.create(name='Anne', first_book=cls.book1)\n        cls.author3 = Author.objects.create(name='Emily', first_book=cls.book1)\n        cls.author4 = Author.objects.create(name='Jane', first_book=cls.book2)\n"], "sample_375": ["def test_remove_model_from_apps(self):\n    project_state = ProjectState()\n    model_state = ModelState(\n        app_label='migrations',\n        name='Tag',\n        fields=[('id', models.AutoField(primary_key=True))],\n    )\n    project_state.add_model(model_state)\n    apps = project_state.apps\n\n    # Remove the model from the apps\n    project_state.remove_model('migrations', 'tag')\n    self.assertNotIn(('migrations', 'tag'), project_state.models)\n\n    # The model should be removed from the apps\n    with self.assertRaises(LookupError):\n        apps.get_model('migrations', 'Tag')\n\n    # The model should not be in the apps' cache\n    self.assertNotIn(('migrations', 'tag'), apps.all_models['migrations'])\n", "def test_get_related_models_tuples(self):\n    \"\"\"\n    get_related_models_tuples() returns a set of (app_label, model_name) tuples\n    for all related models for the given model.\n    \"\"\"\n    new_apps = Apps()\n\n    class A(models.Model):\n        class Meta:\n            app_label = 'tests'\n            apps = new_apps\n\n    class B(models.Model):\n        a = models.ForeignKey(A, models.CASCADE)\n\n        class Meta:\n            app_label = 'tests'\n            apps = new_apps\n\n    class C(models.Model):\n        b = models.ForeignKey(B, models.CASCADE)\n        a = models.ForeignKey(A, models.CASCADE)\n\n        class Meta:\n            app_label = 'tests'\n            apps = new_apps\n\n    related_models = get_related_models_tuples(A)\n    self.assertEqual(related_models, {('tests', 'b'), ('tests', 'c')})\n", "def test_get_related_models_tuples(self):\n    class A(models.Model):\n        pass\n\n    class B(models.Model):\n        a = models.ForeignKey(A, models.CASCADE)\n\n    class C(models.Model):\n        b = models.ForeignKey(B, models.CASCADE)\n\n    related_models = get_related_models_tuples(A)\n    self.assertEqual(related_models, {('tests', 'b'), ('tests', 'c')})\n", "def test_model_state_with_custom_manager(self):\n    class CustomManager(models.Manager):\n        use_in_migrations = True\n\n    class ModelWithCustomManager(models.Model):\n        objects = CustomManager()\n\n        class Meta:\n            app_label = 'migrations'\n            apps = Apps()\n\n    state = ModelState.from_model(ModelWithCustomManager)\n    self.assertEqual(state.managers, [('objects', CustomManager())])\n    self.assertIsInstance(state.managers[0][1], models.Manager)\n", "def test_get_related_models_tuples(self):\n    new_apps = Apps()\n\n    class Author(models.Model):\n        name = models.CharField(max_length=255)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    class Book(models.Model):\n        author = models.ForeignKey(Author, models.CASCADE)\n\n        class Meta:\n            app_label = \"migrations\"\n            apps = new_apps\n\n    project_state = ProjectState()\n    project_state.add_model(ModelState.from_model(Author))\n    project_state.add_model(ModelState.from_model(Book))\n\n    related_models = get_related_models_tuples(Book)\n    self.assertEqual(related_models, {('migrations', 'author')})\n"], "sample_376": ["def test_update_cookie_empty_value(self):\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name].value, '')\n    self.assertEqual(\n        response.cookies[CookieStorage.cookie_name]['expires'],\n        'Thu, 01 Jan 1970 00:00:00 GMT',\n    )\n", "def test_process_messages(self):\n    decoder = MessageDecoder()\n    message_list = [\n        Message(constants.INFO, 'Test message'),\n        [Message(constants.INFO, 'message 1'), Message(constants.INFO, 'message 2')],\n        {'key': Message(constants.ERROR, 'error')},\n        Message(constants.INFO, 'message 3')\n    ]\n    decoded_messages = decoder.process_messages(message_list)\n    self.assertEqual(len(decoded_messages), 4)\n    self.assertIsInstance(decoded_messages[0], Message)\n    self.assertIsInstance(decoded_messages[1], list)\n    self.assertIsInstance(decoded_messages[1][0], Message)\n    self.assertIsInstance(decoded_messages[2], dict)\n    self.assertIsInstance(decoded_messages[2]['key'], Message)\n    self.assertIsInstance(decoded_messages[3], Message)\n", "def test_process_messages(self):\n    decoder = MessageDecoder()\n    messages = [\n        ['__json_message', 1, constants.INFO, 'Test message'],\n        ['__json_message', 0, constants.ERROR, 'Error message'],\n        {'message': ['__json_message', 1, constants.INFO, 'Nested message']},\n    ]\n    processed_messages = decoder.process_messages(messages)\n    self.assertIsInstance(processed_messages[0], Message)\n    self.assertIsInstance(processed_messages[1], Message)\n    self.assertIsInstance(processed_messages[2]['message'], Message)\n    self.assertEqual(processed_messages[0].level, constants.INFO)\n    self.assertEqual(processed_messages[1].level, constants.ERROR)\n    self.assertEqual(processed_messages[2]['message'].level, constants.INFO)\n    self.assertIsInstance(processed_messages[0].message, SafeData)\n    self.assertNotIsInstance(processed_messages[1].message, SafeData)\n    self.assertIsInstance(processed_messages[2]['message'].message, SafeData)\n", "def test_not_finished_sentinel(self):\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Add enough messages to exceed the max cookie size.\n    for i in range(10):\n        storage.add(constants.INFO, get_random_string(200))\n\n    storage.update(response)\n\n    # Check that the not_finished sentinel value was added.\n    data = storage._decode(response.cookies['messages'].value)\n    self.assertEqual(data[-1], CookieStorage.not_finished)\n\n    # Check that the sentinel value is removed when retrieving messages.\n    request = self.get_request()\n    request.COOKIES = response.cookies\n    storage = self.storage_class(request)\n    messages = list(storage)\n    self.assertNotIn(CookieStorage.not_finished, messages)\n", "def test_process_messages_with_empty_list(self):\n    \"\"\"\n    Test that MessageDecoder.process_messages correctly handles an empty list.\n    \"\"\"\n    decoder = MessageDecoder()\n    self.assertEqual(decoder.process_messages([]), [])\n"], "sample_377": ["    def test_technical_404_response(self):\n        exception = Http404(\"Testing technical 404.\")\n        request = RequestFactory().get(\"/test_view/\")\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, \"Testing technical 404.\", status_code=404)\n", "    def test_get_cleansed_multivaluedict(self):\n        request = HttpRequest()\n        request.sensitive_post_parameters = [\"password\"]\n        multivaluedict = MultiValueDict({\"password\": [\"secret\"], \"other\": [\"safe\"]})\n        reporter_filter = SafeExceptionReporterFilter()\n        cleansed_dict = reporter_filter.get_cleansed_multivaluedict(\n            request, multivaluedict\n        )\n        self.assertEqual(cleansed_dict[\"password\"], \"********************\")\n        self.assertEqual(cleansed_dict[\"other\"], [\"safe\"])\n", "    def test_file_upload(self):\n        with self.assertLogs(\"django.request\", \"ERROR\"):\n            response = self.client.post(\n                \"/raises/\",\n                data={\"file_data\": SimpleUploadedFile(\"file_data.txt\", b\"haha\")},\n            )\n        self.assertContains(response, \"file_data.txt\", status_code=500)\n        self.assertContains(response, \"haha\", count=0, status_code=500)\n", "    def test_request_with_files(self):\n        request = self.rf.post(\"/some_url/\", {\"file\": \"content\"})\n        reporter = ExceptionReporter(request, None, None, None)\n        data = reporter.get_traceback_data()\n        self.assertIn(\"request\", data)\n        self.assertIn(\"request_FILES_items\", data)\n", "    def test_url_pattern_with_converter(self):\n        response = self.client.get(\"/path-post/1/\")\n        self.assertEqual(response.status_code, 200)\n        try:\n            response = self.client.get(\"/path-post/abc/\")\n        except Http404 as e:\n            exception = e\n        else:\n            self.fail(\"Http404 not raised.\")\n        response = technical_404_response(self.client.request(), exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, \"<h2>URL dispatcher</h2>\", status_code=404)\n        self.assertContains(\n            response,\n            '<pre class=\"exception_value\">Path error at /path-post/abc/</pre>',\n            status_code=404,\n            html=True,\n        )\n        self.assertContains(\n            response,\n            '<p>The current path, <code>path-post/abc/</code>, didn\u2019t match '\n            \"any of these.</p>\",\n            status_code=404,\n            html=True,\n        )\n"], "sample_378": ["def test_bulk_update_on_model_with_multi_table_inheritance(self):\n    valid = Valid.objects.create(valid='test')\n    member = Member.objects.create(name='test', details=None, valid=valid)\n    detail = Detail.objects.create(data='test')\n    member.details = detail\n    Member.objects.bulk_update([member], ['details'])\n    member.refresh_from_db()\n    self.assertEqual(member.details, detail)\n", "def test_bulk_update_on_concrete_inheritance(self):\n    special_categories = [\n        SpecialCategory.objects.create(name=str(i), special_name=str(i))\n        for i in range(10)\n    ]\n    for category in special_categories:\n        category.name = 'test-%s' % category.id\n        category.special_name = 'special-test-%s' % category.special_name\n    Category.objects.bulk_update(special_categories, ['name'])\n    self.assertCountEqual(\n        Category.objects.values_list('name', flat=True),\n        [cat.name for cat in special_categories]\n    )\n", "def test_bulk_update_with_select_for_update(self):\n    # Create a few objects to update.\n    notes = [Note.objects.create(note='test_note', misc='test_misc') for _ in range(10)]\n\n    # Perform the update with select_for_update().\n    with transaction.atomic():\n        Note.objects.select_for_update().bulk_update(notes, ['misc'])\n\n    # Verify that the update was successful.\n    self.assertCountEqual(Note.objects.filter(misc='test_misc'), notes)\n", "def test_bulk_update_with_unique_fields(self):\n    # Test bulk update with unique fields.\n    notes = [\n        Note.objects.create(note='test_note', misc='test_misc')\n        for _ in range(10)\n    ]\n    for note in notes:\n        note.note = 'updated_note'\n    Note.objects.bulk_update(notes, ['note'], unique_fields=['id'])\n    self.assertCountEqual(Note.objects.filter(note='updated_note'), notes)\n", "def test_bulk_update_with_objects_from_different_databases(self):\n    note1 = Note.objects.create(note='test_note', misc='test_misc')\n    note2 = Note.objects.using('other').create(note='test_note', misc='test_misc')\n    note1.note = 'updated'\n    note2.note = 'updated'\n    msg = \"Cannot update model instance created in a different database.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        Note.objects.bulk_update([note1, note2], ['note'])\n"], "sample_379": ["def test_safe_string_addition(self):\n    s1 = SafeString('a&b')\n    s2 = SafeString('c&d')\n\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'a&bc&d')\n\n    result = s1 + 'e&f'\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, 'a&be&f')\n\n    result = 'g&h' + s1\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, 'g&ha&b')\n", "def test_safe_string_addition(self):\n    s1 = SafeString('a&b')\n    s2 = SafeString('c&d')\n\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'a&bc&d')\n\n    s3 = 'e&f'\n    result = s1 + s3\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, 'a&be&f')\n", "def test_add_safe_string_and_other_type(self):\n    s = SafeString('a&b')\n    result = s + 123\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(str(result), 'a&b123')\n\n    result = s + None\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(str(result), 'a&bNone')\n", "def test_safe_string_addition(self):\n    \"\"\"\n    Test that adding a SafeString to another SafeString or a regular string\n    preserves the safe status.\n    \"\"\"\n    s1 = mark_safe('a&b')\n    s2 = mark_safe('c&d')\n\n    self.assertIsInstance(s1 + s2, SafeString)\n    self.assertIsInstance(s1 + 'e&f', str)  # not SafeString\n    self.assertIsInstance('g&h' + s1, str)  # not SafeString\n\n    self.assertRenderEqual('{{ s }}', 'a&bc&d', s=s1 + s2)\n    self.assertRenderEqual('{{ s }}', 'a&be&f', s=s1 + 'e&f')\n    self.assertRenderEqual('{{ s }}', 'g&ha&b', s='g&h' + s1)\n", "def test_safestring_addition(self):\n    s1 = SafeString('Hello, ')\n    s2 = SafeString('world!')\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, 'Hello, world!')\n\n    s3 = 'unsafe string'\n    result = s1 + s3\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, 'Hello, unsafe string')\n"], "sample_380": ["def test_aggregation_default_with_filter(self):\n    result = Publisher.objects.filter(num_awards__gt=3).aggregate(\n        value=Sum('duration', default=datetime.timedelta(0), filter=Q(name='Apress')),\n    )\n    self.assertEqual(result['value'], datetime.timedelta(days=1))\n", "def test_aggregation_default_with_filter(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__gt=3.0), default=0),\n    )\n    self.assertAlmostEqual(result['value'], Decimal('270.27'), places=2)\n\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__lt=3.0), default=0),\n    )\n    self.assertEqual(result['value'], 0)\n", "def test_aggregate_filter_with_expression(self):\n    result = Book.objects.aggregate(\n        value=Sum('price', filter=Q(rating__gt=F('pages') / 100.0)),\n    )\n    self.assertAlmostEqual(result['value'], Decimal('221.58'), places=2)\n", "def test_aggregate_over_subquery_annotation(self):\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).values('publisher').annotate(count=Count('id')).values('count')\n    publishers = Publisher.objects.annotate(book_count=Subquery(subquery)).aggregate(total_books=Sum('book_count'))\n    self.assertEqual(publishers['total_books'], 6)\n", "def test_aggregate_over_subquery_annotation(self):\n    latest_book_pubdate_qs = Book.objects.filter(\n        publisher=OuterRef('pk')\n    ).order_by('-pubdate').values('pubdate')[:1]\n    publishers = Publisher.objects.annotate(\n        latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n    ).aggregate(Max('latest_book_pubdate'))\n    self.assertEqual(\n        publishers['latest_book_pubdate__max'],\n        datetime.date(2008, 12, 6)\n    )\n"], "sample_381": ["def test_m2m_through_model_ordering(self):\n    \"\"\"\n    M2M through models are created before the model that uses them.\n    \"\"\"\n    changes = self.get_changes([], [self.author_with_m2m_through, self.publisher, self.contract])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\n        'CreateModel', 'CreateModel', 'CreateModel', 'AddField',\n    ])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Contract')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='Author')\n    self.assertOperationAttributes(changes, 'testapp', 0, 2, name='Publisher')\n    self.assertOperationAttributes(changes, 'testapp', 0, 3, model_name='author', name='publishers')\n", "def test_alter_field_with_choices(self):\n    \"\"\"Tests autodetection of new fields.\"\"\"\n    before = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ])\n    after = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, choices=[('A', 'A'), ('B', 'B')])),\n    ])\n    changes = self.get_changes([before], [after])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", preserve_default=True)\n", "def test_custom_migration_name_with_suffix(self):\n    \"\"\"Tests custom naming of migrations for graph matching with suffix.\"\"\"\n    # Make a fake graph\n    graph = MigrationGraph()\n    graph.add_node((\"testapp\", \"0001_initial\"), None)\n    graph.add_node((\"testapp\", \"0002_foobar\"), None)\n    graph.add_node((\"otherapp\", \"0001_initial\"), None)\n    graph.add_dependency(\"testapp.0002_foobar\", (\"testapp\", \"0002_foobar\"), (\"testapp\", \"0001_initial\"))\n\n    # Use project state to make a new migration change set\n    before = self.make_project_state([self.publisher, self.other_pony])\n    after = self.make_project_state([\n        self.author_empty, self.publisher, self.other_pony, self.other_stable,\n    ])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n\n    # Run through arrange_for_graph\n    migration_name = 'custom_name'\n    changes = autodetector.arrange_for_graph(changes, graph, migration_name + '_suffix')\n\n    # Make sure there's a new name, deps match, etc.\n    self.assertEqual(changes[\"testapp\"][0].name, \"0003_%s\" % migration_name)\n    self.assertEqual(changes[\"testapp\"][0].dependencies, [(\"testapp\", \"0002_foobar\")])\n    self.assertEqual(changes[\"otherapp\"][0].name, \"0002_%s\" % migration_name)\n    self.assertEqual(changes[\"otherapp\"][0].dependencies, [(\"otherapp\", \"0001_initial\")])\n", "def test_check_dependency_for_swappable_model(self):\n    dependency = ('__setting__', 'AUTH_USER_MODEL')\n    operation = migrations.CreateModel('CustomUser', fields=[\n        ('id', models.AutoField(primary_key=True)),\n    ])\n    self.assertTrue(MigrationAutodetector.check_dependency(operation, dependency))\n", "def test_rename_field_referenced_by_custom_fk(self):\n    before = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.IntegerField(unique=True)),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE, to_field='field')),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('renamed_field', models.IntegerField(unique=True)),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE, to_field='renamed_field')),\n        ]),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameField'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, model_name='foo', old_name='field',\n        new_name='renamed_field',\n    )\n"], "sample_382": ["def test_get_template_directories_with_locmem_loader(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / 'templates',\n        }\n    )\n", "def test_get_template_directories_filter_django_templates(self, mock_is_django_path):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n        }\n    )\n    mock_is_django_path.assert_called_once_with(str(ROOT / 'templates'))\n", "def test_get_template_directories_with_custom_loader(self):\n    # Mock the CustomLoader to return a specific directory\n    with mock.patch('template_tests.loaders.CustomLoader.get_dirs', return_value=['/custom/templates']):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n                Path('/custom/templates'),\n            }\n        )\n", "def test_reset_locmem_loader(self, mock_reset):\n    autoreload.reset_loaders()\n    mock_reset.assert_called_once()\n", "def test_get_template_directories_with_loader_without_get_dirs(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / 'templates',\n        }\n    )\n"], "sample_383": ["    def test_ticket_24924(self):\n        # Check that subqueries with F expressions don't evaluate prematurely.\n        Individual.objects.create(alive=True)\n        Individual.objects.create(alive=False)\n        qs = Individual.objects.filter(alive=F(\"alive\"))\n        self.assertEqual(qs.count(), 2)\n        alive_individuals = Individual.objects.filter(alive=True)\n        not_alive_individuals = Individual.objects.filter(alive=False)\n        qs = alive_individuals | not_alive_individuals\n        self.assertEqual(qs.count(), 2)\n        qs = Individual.objects.filter(\n            Q(alive=True) & (Q(id__in=alive_individuals) | Q(id__in=not_alive_individuals))\n        )\n        self.assertEqual(qs.count(), 1)\n", "    def test_ticket_24998(self):\n        a = Author.objects.create(name=\"Author1\", num=1)\n        r = Report.objects.create(creator=a, name=\"Report1\")\n        rc = ReportComment.objects.create(report=r)\n\n        # Test that filtering on a subquery with an OuterRef works.\n        subquery = ReportComment.objects.filter(report=OuterRef(\"pk\")).values(\"pk\")[:1]\n        qs = Report.objects.annotate(comment_id=Subquery(subquery)).filter(\n            comment_id__isnull=False\n        )\n        self.assertEqual(qs.get(), r)\n", "    def test_ticket_24766(self):\n        \"\"\"\n        QuerySet.annotate() should not change the related fields selected by\n        select_related() (#24766).\n        \"\"\"\n        a1 = Author.objects.create(name=\"Author1\", num=1)\n        e1 = ExtraInfo.objects.create(info=\"Extra Info 1\", note=None)\n        a1.extra = e1\n        a1.save()\n        Report.objects.create(name=\"Report1\", creator=a1)\n\n        reports = Report.objects.select_related(\"creator__extra\")\n        self.assertIn(\"creator__extra\", str(reports.query))\n\n        reports = reports.annotate(is_null=ExpressionWrapper(Q(creator__extra__isnull=True), output_field=BooleanField()))\n        self.assertIn(\"creator__extra\", str(reports.query))\n        report = reports.first()\n        with self.assertNumQueries(0):\n            report.creator.extra\n", "    def test_ticket_24775(self):\n        \"\"\"\n        The ORM should raise a ValueError if extra(select=...) is used with \n        values() or values_list() and the select arguments are not included in \n        the values/values_list call.\n        \"\"\"\n        msg = \"You have requested a non-existent field 'foo' in your values.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            Note.objects.extra(select={\"foo\": \"%s\"}, select_params=(1,)).values(\"id\")\n        with self.assertRaisesMessage(ValueError, msg):\n            Note.objects.extra(select={\"foo\": \"%s\"}, select_params=(1,)).values_list(\n                \"id\", flat=True\n            )\n", "    def test_ticket_24786(self):\n        \"\"\"\n        Subquery table names should be quoted in the subquery.\n        \"\"\"\n        o1 = Order.objects.create(id=1)\n        o2 = Order.objects.create(id=2)\n        OrderItem.objects.create(order=o1, status=0)\n        OrderItem.objects.create(order=o2, status=1)\n\n        self.assertSequenceEqual(\n            Order.objects.filter(items__status__in=[OrderItem.objects.values(\"status\")]),\n            [o1],\n        )\n        self.assertSequenceEqual(\n            Order.objects.filter(items__status__in=OrderItem.objects.filter(status=1)),\n            [o2],\n        )\n"], "sample_384": ["def test_bulk_update_with_unique_fields(self):\n    notes = [\n        Note.objects.create(note=\"test_note\", misc=\"test_misc\") for _ in range(10)\n    ]\n    for note in notes:\n        note.note = F(\"misc\")\n    with self.assertNumQueries(1):\n        updated = Note.objects.bulk_update(notes, [\"note\"], unique_fields=[\"misc\"])\n    self.assertEqual(updated, 10)\n    self.assertCountEqual(Note.objects.filter(note=F(\"misc\")), notes)\n", "def test_bulk_update_with_unique_fields(self):\n    # Create some Food instances with unique names.\n    foods = [Food.objects.create(name=str(i)) for i in range(10)]\n    # Update the names to new unique values.\n    for food, new_name in zip(foods, range(10, 20)):\n        food.name = str(new_name)\n    # Perform a bulk update on the name field with unique constraint.\n    Food.objects.bulk_update(foods, fields=[\"name\"], unique_fields=[\"name\"])\n    self.assertCountEqual(Food.objects.values_list(\"name\", flat=True), [str(i) for i in range(10, 20)])\n", "def test_bulk_update_with_unique_fields(self):\n    # Test bulk update with unique fields.\n    notes = [\n        Note.objects.create(note=\"test_note\", misc=\"test_misc\") for _ in range(10)\n    ]\n    for note in notes:\n        note.note = F(\"misc\")\n    Note.objects.bulk_update(notes, [\"note\"], unique_fields=[\"misc\"])\n    self.assertCountEqual(Note.objects.filter(note=\"test_misc\"), notes)\n", "def test_bulk_update_with_unique_fields(self):\n    Food.objects.create(name=\"Apple\", calories=95)\n    food = Food.objects.create(name=\"Banana\", calories=105)\n    food.name = \"Orange\"\n    with self.assertNumQueries(1):\n        Food.objects.bulk_update([food], fields=[\"name\"], unique_fields=[\"name\"])\n    food.refresh_from_db()\n    self.assertEqual(food.name, \"Orange\")\n    self.assertEqual(Food.objects.count(), 2)\n", "def test_database_routing_with_batch_size(self):\n    notes = [Note.objects.create(note=str(i)) for i in range(10)]\n    for note in notes:\n        note.note = \"bulk_update\"\n    with self.assertNumQueries(2, using=\"other\"):\n        Note.objects.bulk_update(notes, fields=[\"note\"], batch_size=5)\n"], "sample_385": ["def test_i18n_language(self):\n    # Switch to a supported language.\n    with translation.override(\"fr\"):\n        form = AlbumForm()\n        attrs = form[\"band\"].field.widget.get_context(\n            name=\"my_field\", value=None, attrs={}\n        )[\"widget\"][\"attrs\"]\n        self.assertEqual(attrs[\"lang\"], \"fr\")\n\n    # Switch to an unsupported language.\n    with translation.override(\"eo\"):\n        form = AlbumForm()\n        attrs = form[\"band\"].field.widget.get_context(\n            name=\"my_field\", value=None, attrs={}\n        )[\"widget\"][\"attrs\"]\n        self.assertIsNone(attrs.get(\"lang\"))\n\n    # Switch to a language that has a different variant supported.\n    with translation.override(\"zh-hant-tw\"):\n        form = AlbumForm()\n        attrs = form[\"band\"].field.widget.get_context(\n            name=\"my_field\", value=None, attrs={}\n        )[\"widget\"][\"attrs\"]\n        self.assertEqual(attrs[\"lang\"], \"zh-TW\")\n", "def test_autocomplete_select_multiple(self):\n    beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n    who = Band.objects.create(name=\"The Who\", style=\"rock\")\n    form = AlbumForm(initial={\"featuring\": [beatles.pk, who.pk]})\n    output = form.as_table()\n    selected_option1 = (\n        '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    )\n    selected_option2 = (\n        '<option value=\"%s\" selected>The Who</option>' % who.pk\n    )\n    self.assertIn(selected_option1, output)\n    self.assertIn(selected_option2, output)\n\n    # Test that the autocomplete select multiple widget allows multiple selections\n    form_data = {\"featuring\": [beatles.pk, who.pk]}\n    form = AlbumForm(data=form_data)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data[\"featuring\"], [beatles, who])\n", "def test_autocomplete_mixin_get_select2_language(self):\n    # Test that the get_select2_language method returns the correct language code.\n    with translation.override(\"fr\"):\n        w = AutocompleteSelect(Album._meta.get_field(\"band\"), admin.site)\n        self.assertEqual(w.i18n_name, \"fr\")\n\n    # Test that the method returns None for an unsupported language.\n    with translation.override(\"unsupported-language\"):\n        w = AutocompleteSelect(Album._meta.get_field(\"band\"), admin.site)\n        self.assertIsNone(w.i18n_name)\n\n    # Test that the method returns the correct language code for a language with a variant.\n    with translation.override(\"zh-hant\"):\n        w = AutocompleteSelect(Album._meta.get_field(\"band\"), admin.site)\n        self.assertEqual(w.i18n_name, \"zh-TW\")\n", "def test_get_select2_language(self):\n    with translation.override(\"zh-hant-tw\"):\n        self.assertEqual(get_select2_language(), \"zh-TW\")\n    with translation.override(\"zh-hans\"):\n        self.assertEqual(get_select2_language(), \"zh-CN\")\n    with translation.override(\"en\"):\n        self.assertEqual(get_select2_language(), \"en\")\n    with translation.override(\"unsupported-language-code\"):\n        self.assertIsNone(get_select2_language())\n", "def test_i18n(self):\n    with translation.override(\"fr\"):\n        form = AlbumForm()\n        attrs = form[\"band\"].field.widget.get_context(\n            name=\"my_field\", value=None, attrs={}\n        )[\"widget\"][\"attrs\"]\n        self.assertEqual(attrs[\"lang\"], \"fr\")\n"], "sample_386": ["def test_safe_string_addition(self):\n    s1 = SafeString(\"a&b\")\n    s2 = SafeString(\"c&d\")\n\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&bc&d\")\n\n    s3 = \"e&f\"\n    result = s1 + s3\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&be&f\")\n", "def test_safestring_addition(self):\n    s1 = SafeString(\"a&b\")\n    s2 = SafeString(\"c&d\")\n\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&bc&d\")\n\n    s3 = \"e&f\"\n    result = s1 + s3\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&be&f\")\n", "def test_safe_string_addition(self):\n    \"\"\"\n    Test that adding two SafeString instances together results in another\n    SafeString instance.\n    \"\"\"\n    s1 = mark_safe(\"a&b\")\n    s2 = mark_safe(\"c&d\")\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(str(result), \"a&bc&d\")\n", "def test_safe_string_addition(self):\n    s1 = SafeString(\"a&b\")\n    s2 = SafeString(\"c&d\")\n\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&bc&d\")\n\n    s3 = \"e&f\"\n    result = s1 + s3\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(result, \"a&be&f\")\n", "def test_safe_string_addition(self):\n    safe_s = SafeString(\"a&b\")\n    self.assertIsInstance(safe_s + \"c\", SafeString)\n    self.assertNotIsInstance(safe_s + \"<c>\", SafeString)\n\n    s = \"a\" + safe_s\n    self.assertNotIsInstance(s, SafeString)\n\n    s = safe_s + safe_s\n    self.assertIsInstance(s, SafeString)\n"], "sample_388": ["    def test_process_request_no_remote_user(self):\n        request = RequestFactory().get(\"/remote_user/\")\n        middleware = RemoteUserMiddleware(get_response=lambda req: None)\n        middleware.process_request(request)\n        self.assertIsNone(request.user)\n", "    def setUp(self):\n        self.patched_settings = modify_settings(\n            AUTHENTICATION_BACKENDS={\"append\": self.backend},\n            MIDDLEWARE={\"append\": self.middleware},\n        )\n        self.patched_settings.enable()\n", "    def test_has_module_perms(self):\n        # Create a user with a permission\n        user = User.objects.create_user('testuser')\n        perm = Permission.objects.get(codename='add_user')\n        user.user_permissions.add(perm)\n\n        # Test that the user has the permission\n        self.assertTrue(ModelBackend().has_module_perms(user, 'auth'))\n\n        # Test that the user doesn't have a permission for a different app\n        self.assertFalse(ModelBackend().has_module_perms(user, 'contenttypes'))\n", "    def setUp(self):\n        self.patched_settings = modify_settings(\n            AUTHENTICATION_BACKENDS={\"append\": self.backend},\n            MIDDLEWARE={\"append\": self.middleware},\n        )\n        self.patched_settings.enable()\n", "    def test_has_perm(self):\n        user = User.objects.create_user('testuser')\n        perm = Permission.objects.get(codename='add_user')\n        user.user_permissions.add(perm)\n        self.assertTrue(ModelBackend().has_perm(user, 'auth.add_user'))\n"], "sample_390": ["def test_was_modified_since_overflow(self):\n    \"\"\"\n    was_modified_since handles large mtime values without overflowing (#18675).\n    \"\"\"\n    mtime = 2**31 - 1  # largest possible int value\n    header = http_date(mtime)\n    self.assertFalse(was_modified_since(header, mtime))\n", "def test_was_modified_since_header_with_length(self):\n    \"\"\"\n    Test was_modified_since with a header that includes a length value.\n    \"\"\"\n    mtime = 1343416141\n    size = 100\n    header = f\"Thu, 26 Jul 2012 23:49:01 GMT; length={size}\"\n    self.assertFalse(was_modified_since(header, mtime, size))\n    self.assertTrue(was_modified_since(header, mtime, size + 1))\n", "def test_was_modified_since_overflow(self):\n    \"\"\"\n    A modification time that is greater than the max Unix time does not cause\n    an OverflowError in was_modified_since (#32536).\n    \"\"\"\n    mtime = 2147483648  # Max Unix time + 1\n    header = http_date(mtime - 3600)  # 1 hour before mtime\n    self.assertTrue(was_modified_since(header, mtime))\n", "def test_was_modified_since_header_with_length(self):\n    \"\"\"\n    A response with a Content-Length header is correctly handled by was_modified_since.\n    \"\"\"\n    mtime = 1343416141\n    header = http_date(mtime) + \"; length=100\"\n    self.assertFalse(was_modified_since(header, mtime, 100))\n    self.assertTrue(was_modified_since(header, mtime, 101))\n", "def test_was_modified_since_header_with_length(self):\n    \"\"\"\n    A header with a length does not disturb was_modified_since.\n    \"\"\"\n    mtime = 1343416141\n    size = 100\n    header = http_date(mtime) + \"; length=\" + str(size)\n    self.assertFalse(was_modified_since(header, mtime, size))\n"], "sample_391": ["def test_optimize_through_fields_with_m2m(self):\n    \"\"\"\n    field-level through checking is working with M2M fields.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n            migrations.AddField(\n                \"Foo\",\n                \"bars\",\n                models.ManyToManyField(\"migrations.Bar\", through=\"migrations.FooBar\"),\n            ),\n            migrations.CreateModel(\n                \"FooBar\",\n                [\n                    (\n                        \"foo\",\n                        models.ForeignKey(\"migrations.Foo\", models.CASCADE),\n                    ),\n                    (\n                        \"bar\",\n                        models.ForeignKey(\"migrations.Bar\", models.CASCADE),\n                    ),\n                ],\n            ),\n            migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n            migrations.AddField(\"Bar\", \"width\", models.IntegerField()),\n            migrations.AlterField(\"Foo\", \"age\", models.IntegerField()),\n            migrations.RenameField(\"Bar\", \"size\", \"dimensions\"),\n            migrations.RemoveField(\"Foo\", \"age\"),\n            migrations.RenameModel(\"Foo\", \"Phou\"),\n            migrations.RemoveField(\"Bar\", \"dimensions\"),\n            migrations.RenameModel(\"Phou\", \"Fou\"),\n            migrations.DeleteModel(\"Fou\"),\n        ],\n        [\n            migrations.CreateModel(\"Bar\", [(\"width\", models.IntegerField())]),\n            migrations.CreateModel(\n                \"FooBar\",\n                [\n                    (\n                        \"foo\",\n                        models.ForeignKey(\"migrations.Fou\", models.CASCADE),\n                    ),\n                    (\n                        \"bar\",\n                        models.ForeignKey(\"migrations.Bar\", models.CASCADE),\n                    ),\n                ],\n            ),\n        ],\n    )\n", "def test_add_remove_index(self):\n    self.assertOptimizesTo(\n        [\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"bar\"], name=\"baz\")),\n            migrations.RemoveIndex(\"Foo\", \"baz\"),\n        ],\n        [],\n    )\n", "def test_optimize_through_indexes(self):\n    \"\"\"\n    index-level through checking is working. This should manage to collapse\n    model Foo to a single index.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [\n                    (\"name\", models.CharField(max_length=255)),\n                    (\"age\", models.IntegerField()),\n                ],\n            ),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"])),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"age\"])),\n            migrations.RemoveIndex(\"Foo\", models.Index(fields=[\"name\"])),\n        ],\n        [\n            migrations.CreateModel(\n                \"Foo\",\n                [\n                    (\"name\", models.CharField(max_length=255)),\n                    (\"age\", models.IntegerField()),\n                ],\n            ),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"age\"])),\n        ],\n    )\n", "def test_add_remove_index(self):\n    \"\"\"\n    AddIndex and RemoveIndex operations cancel each other out.\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"])),\n            migrations.RemoveIndex(\"Foo\", \"foo_name_idx\"),\n        ],\n        [],\n    )\n", "def test_optimize_remove_index(self):\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AddIndex(\"Foo\", models.Index(fields=[\"name\"], name=\"foo_name_idx\")),\n            migrations.RemoveIndex(\"Foo\", \"foo_name_idx\"),\n        ],\n        [migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))])],\n    )\n\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AddConstraint(\n                \"Foo\",\n                models.UniqueConstraint(fields=[\"name\"], name=\"foo_name_uniq\"),\n            ),\n            migrations.RemoveConstraint(\"Foo\", \"foo_name_uniq\"),\n        ],\n        [migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))])],\n    )\n"], "sample_392": ["def test_key_transform_in_with_invalid_rhs(self):\n    msg = \"The given value must be a list or a tuple.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        NullableJSONModel.objects.filter(value__in=Value(\"invalid\"))\n", "    def test_subquery_key_transform_raw_expression(self):\n        subquery = NullableJSONModel.objects.filter(value__foo=\"bar\").values(\n            \"value__baz\"\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__baz__in=Subquery(subquery)\n            ),\n            [self.objs[7]],\n        )\n", "    def setUpTestData(cls):\n        cls.objs = [\n            RelatedJSONModel.objects.create(\n                value={\"a\": \"b\", \"c\": 14},\n                related_value={\"d\": \"e\", \"f\": 15},\n            ),\n            RelatedJSONModel.objects.create(\n                value={\"g\": \"h\", \"i\": 16},\n                related_value={\"j\": \"k\", \"l\": 17},\n            ),\n        ]\n", "def test_key_transform_with_annotation(self):\n    obj = NullableJSONModel.objects.create(value={\"a\": {\"b\": \"c\"}})\n    qs = NullableJSONModel.objects.annotate(\n        key=KeyTransform(\"a\", \"value\"),\n        nested_key=KeyTransform(\"b\", KeyTransform(\"a\", \"value\")),\n    )\n    self.assertEqual(qs.get().key, {\"b\": \"c\"})\n    self.assertEqual(qs.get().nested_key, \"c\")\n    self.assertSequenceEqual(\n        qs.filter(key__b=\"c\"),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        qs.filter(nested_key=\"c\"),\n        [obj],\n    )\n", "    def test_custom_encoder(self):\n        field = models.JSONField(encoder=DjangoJSONEncoder)\n        value = uuid.UUID(\"{d85e2076-b67c-4ee7-8c3a-2bf5a2cc2475}\")\n        with self.assertRaisesMessage(ValidationError, \"Value must be valid JSON.\"):\n            field.clean(value, None)\n"], "sample_393": ["    def test_obsolete_messages(self):\n        \"\"\"\n        makemessages removes obsolete messages when --no-obsolete is specified.\n        \"\"\"\n        management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE, encoding=\"utf-8\") as fp:\n            po_contents = fp.read()\n            self.assertIn(\"#~ msgid \\\"Obsolete message\\\"\", po_contents)\n\n        management.call_command(\n            \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n        )\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE, encoding=\"utf-8\") as fp:\n            po_contents = fp.read()\n            self.assertNotIn(\"#~ msgid \\\"Obsolete message\\\"\", po_contents)\n", "    def test_no_extension(self):\n        out, po_contents = self._run_makemessages(extensions=[])\n        self.assertMsgId(\"This literal should be included.\", po_contents)\n        self.assertNotMsgId(\"This should be ignored.\", po_contents)\n", "    def test_obsolete_message_removal(self):\n        management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId(\"This literal should be included.\", po_contents)\n\n        # Remove the source string\n        os.remove(os.path.join(self.test_dir, \"templates/test.html\"))\n\n        # Run makemessages again with --no-obsolete option\n        management.call_command(\n            \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n        )\n\n        # The obsolete message should be removed\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotMsgId(\"This literal should be included.\", po_contents)\n", "    def test_obsolete_messages_removed(self):\n        \"\"\"\n        Obsolete messages are removed when the --no-obsolete option is used.\n        \"\"\"\n        management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertIn(\"#~ msgid \\\"Obsolete message\\\"\", po_contents)\n\n        management.call_command(\n            \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n        )\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn(\"#~ msgid \\\"Obsolete message\\\"\", po_contents)\n", "    def test_obsolete_message_removal_enabled(self):\n        management.call_command(\n            \"makemessages\", locale=[LOCALE], verbosity=0, no_obsolete=True\n        )\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertNotIn(\"#~ msgid \\\"Obsolete message\\\"\", po_contents)\n"], "sample_395": ["def test_get_template_directories_with_locmem_loader(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / \"templates\",\n        },\n    )\n", "def test_get_template_directories_to_path_called(self, mock_to_path):\n    autoreload.get_template_directories()\n    self.assertGreater(mock_to_path.call_count, 0)\n", "def test_get_template_directories_non_django_backend(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / \"templates\",\n        },\n    )\n", "def test_get_template_directories_loader_without_get_dirs(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / \"templates\",\n        },\n    )\n", "def test_get_template_directories_with_dummy_backend(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n        },\n    )\n"], "sample_397": ["def test_find_template_loader(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"loaders\": [\n                    (\"django.template.loaders.cached.Loader\", [\n                        \"django.template.loaders.filesystem.Loader\",\n                        \"django.template.loaders.app_directories.Loader\",\n                    ]),\n                ],\n            },\n        }\n    )\n\n    # Test that find_template_loader returns a loader class instance.\n    loader = engine.engine.find_template_loader(engine.engine.loaders[0])\n    self.assertIsInstance(loader, engine.engine.loaders[0][0])\n\n    # Test that find_template_loader raises an exception for invalid loaders.\n    with self.assertRaises(ImproperlyConfigured):\n        engine.engine.find_template_loader(\"invalid.loader\")\n\n    # Test that find_template_loader raises an exception for loaders with incorrect args.\n    with self.assertRaises(ImproperlyConfigured):\n        engine.engine.find_template_loader((\"loader.with.args\", 1, 2))\n", "def test_get_template_loaders(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"loaders\": [\n                    \"django.template.loaders.filesystem.Loader\",\n                    (\"django.template.loaders.cached.Loader\", [\n                        \"django.template.loaders.app_directories.Loader\"\n                    ])\n                ]\n            }\n        }\n    )\n    loaders = engine.engine.get_template_loaders(engine.engine.loaders)\n    self.assertEqual(len(loaders), 2)\n    self.assertIsInstance(loaders[0], engine.engine.find_template_loader(\"django.template.loaders.filesystem.Loader\"))\n    self.assertIsInstance(loaders[1], engine.engine.find_template_loader((\"django.template.loaders.cached.Loader\", [])))\n", "def test_find_template_loader(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {},\n        }\n    )\n\n    # Test finding a loader by string\n    loader_class = engine.engine.find_template_loader(\"django.template.loaders.filesystem.Loader\")\n    self.assertIsNotNone(loader_class)\n\n    # Test finding a loader by tuple\n    loader_class = engine.engine.find_template_loader((\"django.template.loaders.cached.Loader\", [\"django.template.loaders.filesystem.Loader\"]))\n    self.assertIsNotNone(loader_class)\n\n    # Test invalid loader configuration\n    with self.assertRaisesMessage(ImproperlyConfigured, \"Invalid value in template loaders configuration: 'invalid'\"):\n        engine.engine.find_template_loader(\"invalid\")\n\n    # Test invalid loader class\n    with self.assertRaisesMessage(ImproperlyConfigured, \"Invalid value in template loaders configuration: 123\"):\n        engine.engine.find_template_loader(123)\n", "def test_get_template_loaders(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"loaders\": [\n                    \"django.template.loaders.filesystem.Loader\",\n                    (\"django.template.loaders.cached.Loader\", [\n                        \"django.template.loaders.app_directories.Loader\",\n                    ]),\n                ],\n            },\n        }\n    )\n\n    loaders = engine.engine.get_template_loaders(engine.engine.loaders)\n    self.assertEqual(len(loaders), 2)\n    self.assertIsInstance(loaders[0], import_string(\"django.template.loaders.filesystem.Loader\"))\n    self.assertIsInstance(loaders[1], import_string(\"django.template.loaders.cached.Loader\"))\n", "def test_find_template_loader(self):\n    engine = DjangoTemplates(\n        {\n            \"DIRS\": [],\n            \"APP_DIRS\": False,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\n                \"loaders\": [\n                    \"django.template.loaders.filesystem.Loader\",\n                    (\"django.template.loaders.cached.Loader\", [\n                        \"django.template.loaders.app_directories.Loader\"\n                    ]),\n                ],\n            },\n        }\n    )\n\n    # Test that a single loader is correctly loaded\n    loader = engine.engine.find_template_loader(\"django.template.loaders.filesystem.Loader\")\n    self.assertIsNotNone(loader)\n\n    # Test that a tuple loader is correctly loaded\n    cached_loader = engine.engine.find_template_loader((\"django.template.loaders.cached.Loader\", [\n        \"django.template.loaders.app_directories.Loader\"\n    ]))\n    self.assertIsNotNone(cached_loader)\n\n    # Test that an invalid loader raises an exception\n    with self.assertRaisesMessage(ImproperlyConfigured, \"Invalid value in template loaders configuration:\"):\n        engine.engine.find_template_loader(\"InvalidLoader\")\n"], "sample_398": ["    def test_password_change_done_anonymous_user(self):\n        response = self.client.get(reverse(\"password_change_done\"))\n        self.assertRedirects(\n            response,\n            \"/login/?next=/password_change/done/\",\n            fetch_redirect_response=False,\n        )\n", "    def test_logout_next_page(self):\n        self.login()\n        response = self.client.post(\"/logout/\", {\"next\": \"/somewhere/\"})\n        self.assertRedirects(response, \"/somewhere/\", fetch_redirect_response=False)\n", "    def test_login_view_context(self):\n        response = self.client.get(reverse(\"login\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertIsInstance(response.context[\"form\"], AuthenticationForm)\n", "    def test_login_view_post(self):\n        response = self.client.post(\n            \"/login/\", {\"username\": \"testclient\", \"password\": \"password\"}\n        )\n        self.assertRedirects(response, \"/accounts/profile/\")\n", "    def test_context_contains_site(self):\n        response = self.client.get(reverse(\"logout\"))\n        self.assertEqual(response.status_code, 200)\n        if apps.is_installed(\"django.contrib.sites\"):\n            Site = apps.get_model(\"sites.Site\")\n            site = Site.objects.get_current()\n            self.assertEqual(response.context[\"site\"], site)\n            self.assertEqual(response.context[\"site_name\"], site.name)\n        else:\n            self.assertIsInstance(response.context[\"site\"], RequestSite)\n"], "sample_399": ["def test_aggregation_with_expression_wrapper(self):\n    result = Book.objects.aggregate(\n        value=Sum(ExpressionWrapper(F(\"price\"), output_field=DecimalField())),\n    )\n    self.assertAlmostEqual(result[\"value\"], Decimal(\"270.27\"), places=2)\n", "def test_aggregation_with_custom_database_function(self):\n    class CustomSum(Func):\n        function = \"SUM\"\n        template = \"%(function)s(%(expressions)s::float)\"\n\n    result = Book.objects.aggregate(value=CustomSum(\"price\"))\n    self.assertAlmostEqual(result[\"value\"], 270.27, places=2)\n", "def test_annotate_with_expression_and_filter(self):\n    authors = Author.objects.annotate(\n        combined_ages=Sum(F(\"age\") + F(\"friends__age\")),\n        filtered_combined_ages=Sum(\n            F(\"age\") + F(\"friends__age\"), filter=Q(friends__name=\"Adrian Holovaty\")\n        ),\n    ).order_by(\"name\")\n\n    self.assertQuerysetEqual(\n        authors,\n        [\n            (\"Adrian Holovaty\", 132, 63),\n            (\"Brad Dayley\", None, None),\n            (\"Jacob Kaplan-Moss\", 129, 63),\n            (\"James Bennett\", 63, 63),\n            (\"Jeffrey Forcier\", 128, None),\n            (\"Paul Bissex\", 120, None),\n            (\"Peter Norvig\", 103, None),\n            (\"Stuart Russell\", 103, None),\n            (\"Wesley J. Chun\", 176, 92),\n        ],\n        lambda a: (a.name, a.combined_ages, a.filtered_combined_ages),\n    )\n", "def test_aggregate_subquery_with_outer_ref_in_having(self):\n    subquery = Book.objects.filter(\n        publisher=OuterRef(\"pk\"), pages__gt=OuterRef(\"book__pages\")\n    ).values(\"pk\")\n    queryset = Publisher.objects.annotate(\n        has_longer_book=Exists(subquery), max_pages=Max(\"book__pages\")\n    ).filter(has_longer_book=True)\n    self.assertEqual(queryset.count(), 2)\n", "def test_aggregate_with_output_field(self):\n    # Test that output_field argument works with aggregate functions\n    publishers = Publisher.objects.annotate(\n        earliest_book=Min(\"book__pubdate\", output_field=DateField())\n    )\n    self.assertIsInstance(publishers.first().earliest_book, datetime.date)\n\n    publishers = Publisher.objects.annotate(\n        earliest_book=Min(\"book__pubdate\", output_field=DateTimeField())\n    )\n    self.assertIsInstance(publishers.first().earliest_book, datetime.datetime)\n"], "sample_401": ["def test_formset_non_field_error(self):\n    class NonFieldErrorFormSet(BaseFormSet):\n            raise ValidationError(\"Non-field error\")\n\n    NonFieldErrorFormset = formset_factory(\n        FavoriteDrinkForm, formset=NonFieldErrorFormSet, extra=1\n    )\n    formset = NonFieldErrorFormset(\n        data={\n            \"form-TOTAL_FORMS\": \"1\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-0-name\": \"Test\",\n        },\n        prefix=\"form\",\n    )\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [\"Non-field error\"])\n", "def test_formset_template_context(self):\n    \"\"\"A formset's template context has 'formset' variable.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    context = formset.get_context()\n    self.assertIn('formset', context)\n    self.assertIs(context['formset'], formset)\n", "def test_formset_absolute_max_dynamic(self):\n    \"\"\"Absolute max can be dynamically set on a formset.\"\"\"\n    class DynamicBaseFormSet(BaseFormSet):\n        @property\n            return 2\n\n    LimitedFavoriteDrinkFormSet = formset_factory(\n        FavoriteDrinkForm,\n        formset=DynamicBaseFormSet,\n    )\n    data = {\n        \"form-TOTAL_FORMS\": \"3\",  # the number of forms rendered\n        \"form-INITIAL_FORMS\": \"0\",  # the number of forms with initial data\n        \"form-MIN_NUM_FORMS\": \"0\",  # min number of forms\n        \"form-MAX_NUM_FORMS\": \"0\",  # max number of forms\n        \"form-0-name\": \"Gin Tonic\",\n        \"form-1-name\": \"Bloody Mary\",\n        \"form-2-name\": \"Jack and Coke\",\n    }\n    formset = LimitedFavoriteDrinkFormSet(data=data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 2)\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"Please submit at most 1000 forms.\"],\n    )\n", "def test_formset_with_disabled_fields(self):\n    \"\"\"Formsets with disabled fields.\"\"\"\n    DisabledForm = formset_factory(\n        type(\"DisabledChoice\", (Choice,), {\"choice\": CharField(disabled=True)})\n    )\n    data = {\n        \"form-TOTAL_FORMS\": \"1\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-MIN_NUM_FORMS\": \"0\",\n        \"form-MAX_NUM_FORMS\": \"0\",\n        \"form-0-choice\": \"Calexico\",\n        \"form-0-votes\": \"100\",\n    }\n    formset = DisabledForm(data, auto_id=False, prefix=\"form\")\n    self.assertTrue(formset.is_valid())\n    self.assertEqual([form.cleaned_data for form in formset.forms], [{\"votes\": 100}])\n    # Check that the field is rendered as disabled.\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        \"\"\"", "def test_all_valid_short_circuit(self):\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"0\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"\",  # invalid form\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n\n    self.assertIs(all_valid((formset1, formset2)), False)\n    self.assertIsNotNone(formset1._errors)\n    self.assertIsNone(formset2._errors)  # second formset wasn't validated\n"], "sample_402": ["def test_prepend_www_append_slash_with_query_string(self):\n    request = self.rf.get(\"/path?a=1&b=2\")\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/path/?a=1&b=2\")\n", "def test_prepend_www_append_slash_redirect_querystring(self):\n    \"\"\"\n    PREPEND_WWW and APPEND_SLASH should preserve querystrings when redirecting.\n    \"\"\"\n    request = self.rf.get(\"/slash?test=1\")\n    resp = CommonMiddleware(get_response_404)(request)\n    self.assertEqual(resp.url, \"http://www.testserver/slash/?test=1\")\n", "def test_prepend_www_append_slash_with_query_string(self):\n    \"\"\"\n    Test that prepending www and appending slash works correctly with query strings.\n    \"\"\"\n    request = self.rf.get(\"/path?query=string\")\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/path/?query=string\")\n", "def test_prepend_www_append_slash_with_non_ascii_query_string(self):\n    request = self.rf.get(\"/slash?test=caf\u00e9\")\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/slash/?test=caf%C3%A9\")\n", "def test_prepend_www_append_slash_with_non_ascii_path(self):\n    \"\"\"\n    Prepending www and appending slash works with non-ASCII paths.\n    \"\"\"\n    request = self.rf.get(\"/r\u00e9sum\u00e9\")\n    r = CommonMiddleware(get_response_empty).process_request(request)\n    self.assertEqual(r.status_code, 301)\n    self.assertEqual(r.url, \"http://www.testserver/r%C3%A9sum%C3%A9/\")\n"], "sample_403": ["def test_reduce_references_model(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n    )\n    next_operation = FieldOperation(\"Other\", \"other_field\", models.BooleanField())\n    self.assertEqual(\n        operation.reduce(next_operation, \"migrations\"), [next_operation, operation]\n    )\n    next_operation = FieldOperation(\"Whatever\", \"whatever\", models.BooleanField())\n    self.assertIs(operation.reduce(next_operation, \"migrations\"), False)\n", "def test_reduce_references_model(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n    )\n    self.assertIs(\n        operation.reduce(operation, [\"migrations\"]),\n        [operation],\n    )\n    self.assertIs(\n        operation.reduce(operation, [\"migrations\", \"Model\"]),\n        [],\n    )\n    self.assertIs(\n        operation.reduce(operation, [\"migrations\", \"Other\"]),\n        [],\n    )\n", "def test_alter_field_with_default(self):\n    \"\"\"\n    Altering a field with a default doesn't lose the default on PostgreSQL (#24492).\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_alflwd\")\n    # Test the state alteration\n    operation = migrations.AlterField(\n        \"Pony\",\n        \"pink\",\n        models.IntegerField(default=42),\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_alflwd\", new_state)\n    self.assertEqual(new_state.models[\"test_alflwd\", \"pony\"].fields[\"pink\"].default, 42)\n    # Test the database alteration\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_alflwd\", editor, project_state, new_state)\n    Pony = new_state.apps.get_model(\"test_alflwd\", \"Pony\")\n    pony = Pony.objects.create(weight=4.0)\n    self.assertEqual(pony.pink, 42)\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_alflwd\", editor, new_state, project_state)\n    Pony = project_state.apps.get_model(\"test_alflwd\", \"Pony\")\n    pony = Pony.objects.create(weight=4.0)\n    self.assertEqual(pony.pink, 1)\n", "def test_alter_field_type_to_imagefield(self):\n    project_state = self.set_up_test_model(\"test_alflif\")\n    # Test the state alteration\n    operation = migrations.AlterField(\n        \"Pony\", \"pink\", models.ImageField(upload_to=\"images\", null=True)\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_alflif\", new_state)\n    self.assertIs(\n        project_state.models[\"test_alflif\", \"pony\"].fields[\"pink\"].null, False\n    )\n    self.assertIs(new_state.models[\"test_alflif\", \"pony\"].fields[\"pink\"].null, True)\n    # Test the database alteration\n    self.assertColumnNotNull(\"test_alflif_pony\", \"pink\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_alflif\", editor, project_state, new_state)\n    self.assertColumnNull(\"test_alflif_pony\", \"pink\")\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_alflif\", editor, new_state, project_state)\n    self.assertColumnNotNull(\"test_alflif_pony\", \"pink\")\n", "def test_references_field_by_through_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\"Other\", through=\"Through\", through_fields=(\"a\", \"b\")),\n    )\n    self.assertIs(operation.references_field(\"Through\", \"a\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Through\", \"b\", \"migrations\"), True)\n    self.assertIs(\n        operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n"], "sample_404": ["def test_filter_expression_escaped_string(self):\n    engine = self._engine()\n    template = engine.from_string('{{ \"Hello \\\\\"World\\\\\"\" }}')\n    context = Context()\n    self.assertEqual(template.render(context), 'Hello \"World\"')\n", "def test_invalid_template_name(self):\n    \"\"\"\n    #26247 -- Template names should not contain unescaped quotes.\n    \"\"\"\n    engine = self._engine()\n    with self.assertRaises(TemplateSyntaxError) as e:\n        engine.from_string('{% extends \"foo\"bar.html\" %}')\n\n    if self.debug_engine:\n        self.assertEqual(e.exception.template_debug[\"during\"], '{% extends \"foo\"bar.html\" %}')\n", "def test_template_localtime(self):\n    \"\"\"\n    #27558 -- Make sure template_localtime filter respects USE_TZ.\n    \"\"\"\n    engine = self._engine()\n    t = engine.from_string(\"{% load tz %}{{ value|localtime }}\")\n    c = Context({\"value\": \"2016-01-01 12:00:00\"})\n    with self.settings(USE_TZ=True):\n        self.assertEqual(t.render(c), \"Jan. 1, 2016, 12 p.m.\")\n    with self.settings(USE_TZ=False):\n        self.assertEqual(t.render(c), \"Jan. 1, 2016, noon\")\n", "def test_render_annotated(self):\n    \"\"\"\n    #28056 -- Template.render() shouldn't hide the origin of exceptions.\n    \"\"\"\n    e = self._engine(app_dirs=True)\n    template = e.get_template(\"annotated_render.html\")\n    context = Context()\n    try:\n        template.render(context)\n    except Exception as e:\n        if self.debug_engine:\n            self.assertEqual(e.template_debug[\"during\"], \"{{ undefined_variable }}\")\n    else:\n        self.fail(\"Template rendering should fail with an exception.\")\n", "def test_template_origin(self):\n    \"\"\"\n    #27061 -- Template.origin should be available on exception even if an error\n    occurs in a child template.\n    \"\"\"\n    e = self._engine(app_dirs=True)\n    template = e.get_template(\"test_extends_error.html\")\n    context = Context()\n    with self.assertRaises(TemplateSyntaxError) as cm:\n        template.render(context)\n    if self.debug_engine:\n        self.assertEqual(cm.exception.template_debug[\"name\"], \"included_content.html\")\n"], "sample_406": ["    def test_manager_descriptor_access(self):\n        # Accessing a manager through its descriptor should return the manager.\n        self.assertIsInstance(Article.objects, Manager)\n", "    def test_creation_counter(self):\n        manager1 = BaseManager()\n        manager2 = BaseManager()\n        self.assertLess(manager1.creation_counter, manager2.creation_counter)\n", "    def test_manager_descriptor(self):\n        manager = Article.objects\n        self.assertIsInstance(manager, ManagerDescriptor)\n        self.assertEqual(manager.__get__(None, Article), Article._meta.managers_map[\"objects\"])\n", "    def test_manager_descriptor(self):\n        manager = Article.objects\n        self.assertIsInstance(manager, ManagerDescriptor)\n        self.assertEqual(manager.__get__(None, Article), manager.manager)\n", "    def test_empty_manager_queryset(self):\n        self.assertIsInstance(Article.objects.none(), EmptyQuerySet)\n        self.assertEqual(len(Article.objects.none()), 0)\n"], "sample_407": ["def test_foreign_key_with_to_field_and_db_column(self):\n    parent = Parent.objects.create(name=\"Parent\")\n    child = ToFieldChild.objects.create(parent=parent)\n    self.assertEqual(ToFieldChild._meta.get_field(\"parent\").get_attname_column()[1], \"parent_name\")\n    self.assertEqual(child.parent_id, parent.name)\n", "def test_clear_related_object_cache(self):\n    parent = Parent.objects.create(name=\"jeff\")\n    child = Child.objects.create(name=\"frank\", parent=parent)\n    parent.bestchild = child\n    parent.save()\n    self.assertTrue(Parent.bestchild.is_cached(parent))\n    parent.refresh_from_db()\n    self.assertFalse(Parent.bestchild.is_cached(parent))\n    parent.bestchild_id = None\n    self.assertFalse(Parent.bestchild.is_cached(parent))\n    parent.bestchild_id = child.pk\n    self.assertFalse(Parent.bestchild.is_cached(parent))\n    parent.save()\n    self.assertFalse(Parent.bestchild.is_cached(parent))\n    parent.refresh_from_db()\n    self.assertFalse(Parent.bestchild.is_cached(parent))\n", "def test_model_get_deferred_fields(self):\n    article = Article.objects.defer('headline').get(id=self.a.id)\n    self.assertEqual(article.get_deferred_fields(), {'headline'})\n\n    # A second call should return the same set, not create a new one.\n    self.assertIs(article.get_deferred_fields(), article.get_deferred_fields())\n", "def test_cached_foreign_key_with_to_field_not_cleared_by_save_with_update_fields(self):\n    parent = Parent.objects.create(name=\"a\")\n    child = ToFieldChild.objects.create(parent=parent)\n    with self.assertNumQueries(0):\n        self.assertIs(child.parent, parent)\n    child.save(update_fields=[\"name\"])\n    with self.assertNumQueries(0):\n        self.assertIs(child.parent, parent)\n", "def test_delete_with_foreign_key_to_field(self):\n    parent = Parent.objects.create(name=\"jeff\")\n    child1 = ToFieldChild.objects.create(parent=parent)\n    child2 = ToFieldChild.objects.create(parent=parent)\n    parent.delete()\n    self.assertSequenceEqual(ToFieldChild.objects.all(), [])\n"], "sample_409": ["def test_blocktranslate_with_trimmed_option(self):\n    output = self.engine.render_to_string(\"template\", {\"name\": \"John\"})\n    self.assertEqual(output, \"Hello, John!\")\n", "    def test_render_token_list(self):\n        node = BlockTranslateNode(None, None)\n        token_list = [\n            Token(TokenType.TEXT, \"Hello \"),\n            Token(TokenType.VAR, \"name\"),\n            Token(TokenType.TEXT, \" \"),\n        ]\n        result, vars = node.render_token_list(token_list)\n        self.assertEqual(result, \"Hello %%(%s)s \" % \"name\")\n        self.assertEqual(vars, [\"name\"])\n", "    def test_i18n_language_info(self):\n        output = self.engine.render_to_string(\"template\", {\"LANGUAGE_CODE\": \"de\"})\n        self.assertEqual(output, \"de German Deutsch Deutsch uni-directional\")\n", "    def test_render(self):\n        template = Template('{% load i18n %}{% language \"de\" %}{{ lang }}{% endlanguage %}')\n        context = Context({'lang': 'German'})\n        with translation.override('fr'):\n            self.assertEqual(template.render(context), 'German')\n", "    def test_language_info(self):\n        output = self.engine.render_to_string(\"template\", {\"LANGUAGE_CODE\": \"fr\"})\n        self.assertEqual(output, \"fr French French fran\\u00e7ais uni-directional\")\n"], "sample_410": ["    def test_set_password_does_not_clear_session_auth_hash(self):\n        user = User.objects.create_user(username=\"test\", password=\"password\")\n        initial_session_auth_hash = user.get_session_auth_hash()\n        user.set_password(\"new_password\")\n        user.save()\n        self.assertEqual(initial_session_auth_hash, user.get_session_auth_hash())\n", "    def test_normalize_username(self):\n        # Test that the normalize_username method is called when setting the username.\n        class CustomUser(AbstractBaseUser):\n            USERNAME_FIELD = 'username'\n\n                return username.upper()\n\n        user = CustomUser(username='test')\n        user.clean()\n        self.assertEqual(user.username, 'TEST')\n", "    def test_get_email_field_name(self):\n        user = AbstractBaseUser()\n        self.assertEqual(user.get_email_field_name(), \"email\")\n", "    def test_get_session_auth_hash(self):\n        user = User.objects.create_user(\"testuser\")\n        self.assertIsNotNone(user.get_session_auth_hash())\n", "    def test_set_unusable_password(self):\n        user = User.objects.create_user(\"testuser\")\n        user.set_unusable_password()\n        self.assertFalse(user.has_usable_password())\n"], "sample_411": ["def test_command_error_returncode(self):\n    error = CommandError(\"An error\", returncode=5)\n    self.assertEqual(error.returncode, 5)\n    error = CommandError(\"An error\")\n    self.assertEqual(error.returncode, 1)\n", "def test_command_error_returncode(self):\n    \"\"\"Test CommandError return code\"\"\"\n    error = CommandError(\"An error occurred\", returncode=42)\n    self.assertEqual(error.returncode, 42)\n\n    error = CommandError(\"An error occurred\")\n    self.assertEqual(error.returncode, 1)\n", "def test_suppressed_base_arguments(self):\n    parser = BaseCommand()\n    parser.suppressed_base_arguments = {\"--version\"}\n    with mock.patch.object(parser, \"add_argument\") as add_argument:\n        parser.add_base_argument(mock.Mock(), \"--version\")\n        self.assertEqual(add_argument.call_count, 1)\n        self.assertEqual(add_argument.call_args[1][\"help\"], argparse.SUPPRESS)\n", "def test_find_commands_empty_dir(self):\n    \"\"\"\n    find_commands() returns an empty list when the given directory is empty.\n    \"\"\"\n    with mock.patch(\"os.listdir\", return_value=[]):\n        self.assertEqual(find_commands(\"/path/to/empty/dir\"), [])\n", "def test_suppressed_base_arguments(self):\n    class Command(BaseCommand):\n        suppressed_base_arguments = {\"--version\"}\n\n    parser = Command().create_parser(\"prog_name\", \"subcommand\")\n    actions = [action.dest for action in parser._actions]\n    self.assertNotIn(\"version\", actions)\n"], "sample_412": ["def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            \"Search for google.com/?q=!.\",\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!.',\n        ),\n        (\n            \"Search for google.com/?q=!?\",\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!?',\n        ),\n        (\n            \"Search for google.com/?q=!;\",\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!;',\n        ),\n        (\n            \"Search for google.com/?q=!:\",\n            'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!:',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        (\n            \"Check out www.google.com.\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>.',\n        ),\n        (\n            \"Check out www.google.com!\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>!',\n        ),\n        (\n            \"Check out www.google.com?\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>?',\n        ),\n        (\n            \"Check out www.google.com:\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>:',\n        ),\n        (\n            \"Check out www.google.com;\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>;',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            \"Search for google.com.\",\n            'Search for <a href=\"http://google.com/\">google.com</a>.',\n        ),\n        (\n            \"Search for google.com! and see.\",\n            'Search for <a href=\"http://google.com/\">google.com</a>! and see.',\n        ),\n        (\n            \"Search for google.com, and see.\",\n            'Search for <a href=\"http://google.com/\">google.com</a>, and see.',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            \"Check out www.google.com.\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>.',\n        ),\n        (\n            \"Check out www.google.com!\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>!',\n        ),\n        (\n            \"Check out www.google.com?\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>?',\n        ),\n        (\n            \"Check out www.google.com,\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>,',\n        ),\n        (\n            \"Check out www.google.com;\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>;',\n        ),\n        (\n            \"Check out www.google.com:\",\n            'Check out <a href=\"http://www.google.com\">www.google.com</a>:',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\"Search for google.com! and see.\", 'Search for <a href=\"http://google.com/\">google.com</a>! and see.'),\n        (\"Search for google.com? and see.\", 'Search for <a href=\"http://google.com/\">google.com</a>? and see.'),\n        (\"Search for google.com. and see.\", 'Search for <a href=\"http://google.com/\">google.com</a>. and see.'),\n        (\"Search for google.com, and see.\", 'Search for <a href=\"http://google.com/\">google.com</a>, and see.'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_413": ["    def test_template_tags_with_empty_templates_setting(self):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_3.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_3.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def test_template_tags_with_same_name_and_empty_libraries(self):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [\n                Error(\n                    E003.msg.format(\n                        \"'same_tags'\",\n                        \"'check_framework.template_test_apps.same_tags_app_1.\"\n                        \"templatetags.same_tags', \"\n                        \"'check_framework.template_test_apps.same_tags_app_2.\"\n                        \"templatetags.same_tags'\",\n                    ),\n                    id=E003.id,\n                )\n            ],\n        )\n"], "sample_414": ["    def test_readonly_field_widget(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        readonly_field = self.selenium.find_element(By.ID, \"id_readonly_field\")\n        self.assertEqual(readonly_field.get_attribute(\"readonly\"), \"true\")\n\n        # Test that the readonly field is not included in the POST data\n        self.selenium.find_element(By.NAME, \"_save\").click()\n        post_data = self.selenium.get_log(\"performance\")[0][\"message\"]\n        self.assertNotIn(\"readonly_field\", post_data)\n", "    def test_widget(self):\n        widget = admin.widgets.AutocompleteSelect(ForeignKey, widget_admin_site)\n        self.assertIsInstance(widget, admin.widgets.AutocompleteSelect)\n        self.assertEqual(\n            widget.media.render(),\n            '<script src=\"/static/admin/js/vendor/select2/select2.full.js\"></script>\\n'\n            '<link href=\"/static/admin/css/vendor/select2/select2.css\" type=\"text/css\" media=\"screen\" rel=\"stylesheet\">',\n        )\n", "    def setUp(self):\n        super().setUp()\n        self.user = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUp(self):\n        self.user = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        self.band1 = Band.objects.create(id=42, name=\"Bogey Blues\")\n        self.band2 = Band.objects.create(id=98, name=\"Green Potatoes\")\n", "    def test_non_ascii_query_params(self):\n        widget = widgets.AdminURLFieldWidget()\n        url = \"http://example.com/?param=%C3%A4%C3%B6%C3%BC\"\n        rendered_widget = widget.render(\"test\", url)\n        self.assertHTMLEqual(\n            rendered_widget,\n            '<p class=\"url\">Currently: <a href=\"{}\">{}</a><br>'\n            'Change:<input class=\"vURLField\" name=\"test\" type=\"url\" value=\"{}\"></p>'.format(\n                url, url, url\n            ),\n        )\n"], "sample_415": ["def test_validate_expression_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper())\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n    # Test excluding a field not present in the constraint.\n    with self.assertRaisesMessage(\n        ValidationError, \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    ):\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"color\"},\n        )\n", "def test_unique_constraint_clone(self):\n    constraint = models.UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        condition=models.Q(foo=models.F(\"bar\")),\n        deferrable=models.Deferrable.IMMEDIATE,\n        include=[\"baz_1\", \"baz_2\"],\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n", "def test_validate_expression_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper())\n    msg = \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    # Expression used by the unique constraint is excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\": Lower(\"name\")},\n    )\n    # Exclude a different expression.\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"name\": F(\"color\")},\n        )\n", "def test_validate_expression_with_excluded_fields(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"), F(\"color\"), name=\"name_color_uniq\"\n    )\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name, color=self.p1.color)\n    msg = \"Constraint \u201cname_color_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, non_unique_product)\n    # Unique fields are excluded.\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"color\"},\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\", \"color\"},\n    )\n", "    def test_validate_with_exclude(self):\n        constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n        existing_product = UniqueConstraintProduct.objects.create(name=\"existing\")\n        new_product = UniqueConstraintProduct(name=\"existing\")\n\n        with self.assertRaises(ValidationError):\n            constraint.validate(UniqueConstraintProduct, new_product)\n\n        constraint.validate(UniqueConstraintProduct, new_product, exclude={\"name\"})\n\n        # If the exclude parameter contains a field that is not in the unique constraint,\n        # it should still raise an error.\n        with self.assertRaises(ValidationError):\n            constraint.validate(UniqueConstraintProduct, new_product, exclude={\"color\"})\n"], "sample_416": ["def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_runshell_calls_super_with_parameters(self):\n    parameters = [\"--help\"]\n    with mock.patch.object(super(DatabaseClient, self.client), \"runshell\") as runshell:\n        self.client.runshell(parameters)\n        runshell.assert_called_once_with(parameters)\n", "def test_runshell_parameters(self):\n    client = DatabaseClient(connection)\n    runshell_patch = mock.patch.object(\n        BaseDatabaseClient, \"runshell\", return_value=None\n    )\n    with runshell_patch as runshell_mock:\n        client.runshell([\"--help\"])\n        runshell_mock.assert_called_once_with([\"--help\"])\n", "def test_runshell_calls_super_with_parameters(self):\n    with mock.patch.object(super(DatabaseClient, DatabaseClient), 'runshell') as runshell_mock:\n        client = DatabaseClient(connection)\n        parameters = [\"--help\"]\n        client.runshell(parameters)\n        runshell_mock.assert_called_once_with(parameters)\n", "def test_runshell_calls_super_with_parameters(self):\n    parameters = [\"--help\"]\n    with mock.patch.object(super(DatabaseClient, DatabaseClient), \"runshell\") as super_runshell:\n        DatabaseClient(None).runshell(parameters)\n        super_runshell.assert_called_once_with(parameters)\n"], "sample_417": ["    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5})\n        self.assertEqual(output, \"6\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 12345})\n        self.assertEqual(output, \"1.2345E+4\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 1.23})\n        self.assertEqual(output, \"1.230000E+00\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5})\n        self.assertEqual(output, \"10\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 1.0})\n        self.assertEqual(output, \"1.000000E+00\")\n"], "sample_418": ["def test_floatformat01(self):\n    output = self.engine.render_to_string(\"floatformat01\", {\"num\": 34.23234})\n    self.assertEqual(output, \"34.2\")\n", "def test_add01(self):\n    output = self.engine.render_to_string(\"add01\", {\"four\": 4})\n    self.assertEqual(output, \"6\")\n", "def test_add_filter(self):\n    output = self.engine.render_to_string(\"add01\", {\"four\": 4})\n    self.assertEqual(output, \"8\")\n", "def test_add(self):\n    output = self.engine.render_to_string(\"add01\", {\"num1\": 4, \"num2\": 6})\n    self.assertEqual(output, \"10\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"value\": 5, \"arg\": 3})\n        self.assertEqual(output, \"8\")\n"], "sample_419": ["def test_formset_media_with_empty_media(self):\n    \"\"\"Media is available on formset, even if Media class is empty.\"\"\"\n    class MediaForm(Form):\n        class Media:\n            pass\n\n    self.assertEqual(str(formset_factory(MediaForm)().media), \"\")\n", "def test_template_name_div(self):\n    \"\"\"Test that formset template name div is used.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertHTMLEqual(\n        str(formset),\n        \"\"\"<div>", "    def test_absolute_max_with_initial(self):\n        data = {\n            \"form-TOTAL_FORMS\": \"3\",\n            \"form-INITIAL_FORMS\": \"2\",\n            \"form-MIN_NUM_FORMS\": \"0\",\n            \"form-MAX_NUM_FORMS\": \"0\",\n            \"form-0-name\": \"Zero\",\n            \"form-1-name\": \"One\",\n            \"form-2-name\": \"Two\",\n        }\n        AbsoluteMaxFavoriteDrinksFormSet = formset_factory(\n            FavoriteDrinkForm,\n            absolute_max=2,\n        )\n        formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(len(formset.forms), 2)\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\"Please submit at most 1000 forms.\"],\n        )\n", "def test_all_valid_calls_full_clean(self):\n    class CleanFormSet(BaseFormSet):\n            raise ValidationError(\"Clean method called\")\n\n    CleanFormSetInstance = formset_factory(\n        FavoriteDrinkForm, extra=2, formset=CleanFormSet\n    )\n    data = {\n        \"form-TOTAL_FORMS\": \"2\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-MIN_NUM_FORMS\": \"0\",\n        \"form-0-name\": \"One\",\n        \"form-1-name\": \"Two\",\n    }\n    formset = CleanFormSetInstance(data, prefix=\"form\")\n    self.assertFalse(all_valid([formset]))\n    self.assertEqual(formset.non_form_errors(), [\"Clean method called\"])\n", "    def test_add_fields_modifies_form(self):\n        class BaseDynamicFormSet(BaseFormSet):\n                super().add_fields(form, index)\n                form.fields[\"custom\"] = CharField()\n\n        DynamicFormSet = formset_factory(Choice, formset=BaseDynamicFormSet)\n        formset = DynamicFormSet()\n        for form in formset.forms:\n            self.assertIn(\"custom\", form.fields)\n"], "sample_420": ["    def test_modelform_media(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = TextFile\n                fields = \"__all__\"\n\n            class Media:\n                css = {\"all\": (\"/some/form/css\",)}\n                js = (\"/some/form/javascript\",)\n\n        f = MyModelForm()\n        self.assertEqual(\n            str(f.media),\n            '<link href=\"/some/form/css\" media=\"all\" rel=\"stylesheet\">'\n            '<script src=\"/some/form/javascript\"></script>',\n        )\n", "    def test_save_with_foreign_key(self):\n        class BookForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = (\"author\", \"title\")\n\n        author = Writer.objects.create(name=\"Author Name\")\n        book_data = {\"author\": str(author.pk), \"title\": \"Book Title\"}\n\n        form = BookForm(data=book_data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(Book.objects.count(), 1)\n\n        book = Book.objects.get(title=\"Book Title\")\n        self.assertEqual(book.author, author)\n\n        # Test that changing the foreign key and saving again works\n        new_author = Writer.objects.create(name=\"New Author Name\")\n        book_data[\"author\"] = str(new_author.pk)\n        form = BookForm(data=book_data, instance=book)\n        self.assertTrue(form.is_valid())\n        form.save()\n        book.refresh_from_db()\n        self.assertEqual(book.author, new_author)\n", "    def test_modelform_with_media_and_template(self):\n        class ProductForm(forms.ModelForm):\n            class Media:\n                js = (\"/some/form/javascript\",)\n                css = {\"all\": (\"/some/form/css\",)}\n\n            class Meta:\n                model = Product\n                fields = \"__all__\"\n\n        form = ProductForm()\n        self.assertHTMLEqual(\n            str(form.media),\n            '<link href=\"/some/form/css\" media=\"all\" rel=\"stylesheet\">'\n            '<script src=\"/some/form/javascript\"></script>',\n        )\n\n        template = Template(\"{% load static %}{{ form.media }}\")\n        rendered = template.render(Context({\"form\": form}))\n        self.assertHTMLEqual(\n            rendered,\n            '<link href=\"/some/form/css\" media=\"all\" rel=\"stylesheet\">'\n            '<script src=\"/some/form/javascript\"></script>',\n        )\n", "    def test_construct_instance(self):\n        form = modelform_factory(Person, fields=\"__all__\")({\"name\": \"John Doe\"})\n        self.assertTrue(form.is_valid())\n        instance = construct_instance(form, Person())\n        self.assertEqual(instance.name, \"John Doe\")\n", "    def test_construct_instance_with_fields(self):\n        instance = Article()\n        form = ArticleForm(\n            {\n                \"headline\": \"Test headline\",\n                \"slug\": \"test-headline\",\n                \"pub_date\": \"1988-01-04\",\n                \"writer\": \"1\",\n                \"article\": \"Hello.\",\n            }\n        )\n        self.assertTrue(form.is_valid())\n        construct_instance(form, instance, fields=[\"headline\", \"slug\"])\n        self.assertEqual(instance.headline, \"Test headline\")\n        self.assertEqual(instance.slug, \"test-headline\")\n        self.assertIsNone(instance.pub_date)\n        self.assertIsNone(instance.writer)\n        self.assertIsNone(instance.article)\n"], "sample_421": ["    def test_expression_wrapper(self):\n        wrapper = ExpressionWrapper(Value(1), output_field=IntegerField())\n        self.assertEqual(wrapper.output_field.get_internal_type(), \"IntegerField\")\n        self.assertEqual(wrapper.as_sql(None, None), (\"1\", []))\n", "    def test_init(self):\n        expression = F(\"field\")\n        wrapper = ExpressionWrapper(expression, output_field=IntegerField())\n        self.assertEqual(wrapper.expression, expression)\n        self.assertEqual(wrapper.output_field, IntegerField())\n", "    def test_expression_wrapper(self):\n        expression = F(\"field\")\n        wrapper = ExpressionWrapper(expression, output_field=IntegerField())\n        self.assertEqual(wrapper.expression, expression)\n        self.assertEqual(wrapper.output_field, IntegerField())\n", "    def test_window_expression(self):\n        qs = CaseTestModel.objects.annotate(\n            row_number=Window(\n                expression=Func(F(\"integer\"), function=\"ROW_NUMBER\"),\n                partition_by=[F(\"string\")],\n                order_by=[F(\"integer\")],\n            )\n        ).order_by(\"pk\")\n        self.assertQuerysetEqual(\n            qs,\n            [\n                (1, 1),\n                (2, 1),\n                (3, 1),\n                (2, 2),\n                (3, 2),\n                (3, 3),\n                (4, 1),\n            ],\n            transform=attrgetter(\"integer\", \"row_number\"),\n        )\n", "    def test_repr(self):\n        expr = ExpressionWrapper(Value(1), output_field=IntegerField())\n        self.assertEqual(repr(expr), \"ExpressionWrapper(Value(1))\")\n"], "sample_422": ["    def setUpTestData(cls):\n        cls.book = Book.objects.create(title=\"Poems\")\n        cls.author = Author.objects.create(name=\"Jane\", first_book=cls.book)\n        TaggedItem.objects.create(content_object=cls.book, tag=\"django\")\n        TaggedItem.objects.create(content_object=cls.author, tag=\"python\")\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name=\"John\")\n        cls.author2 = Author.objects.create(name=\"Jane\")\n        cls.book1 = Book.objects.create(title=\"Book 1\")\n        cls.book2 = Book.objects.create(title=\"Book 2\")\n        cls.book1.authors.add(cls.author1)\n        cls.book2.authors.add(cls.author2)\n", "    def setUpTestData(cls):\n        cls.house1 = House.objects.create(name=\"House 1\", address=\"123 Main St\")\n        cls.room1_1 = Room.objects.create(name=\"Dining room\", house=cls.house1)\n        cls.room1_2 = Room.objects.create(name=\"Lounge\", house=cls.house1)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title=\"Book 1\")\n        cls.book2 = Book.objects.create(title=\"Book 2\")\n        cls.author1 = Author.objects.create(name=\"Author 1\", first_book=cls.book1)\n        cls.author2 = Author.objects.create(name=\"Author 2\", first_book=cls.book2)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(title=\"Book 1\")\n        cls.book2 = Book.objects.create(title=\"Book 2\")\n        cls.author1 = Author.objects.create(name=\"Author 1\", first_book=cls.book1)\n        cls.author2 = Author.objects.create(name=\"Author 2\", first_book=cls.book1)\n"], "sample_425": ["def test_unregister_serializer(self):\n    class TestSerializer(BaseSerializer):\n            return \"test(%r)\" % self.value, {}\n\n    Serializer.register(int, TestSerializer)\n    try:\n        string = MigrationWriter.serialize(42)[0]\n        self.assertEqual(string, \"test(42)\")\n    finally:\n        Serializer.unregister(int)\n\n    string = MigrationWriter.serialize(42)[0]\n    self.assertEqual(string, \"42\")\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    class Complex:\n        pass\n\n    Serializer.register(Complex, ComplexSerializer)\n    self.assertEqual(Serializer._registry[Complex], ComplexSerializer)\n    Serializer.unregister(Complex)\n    self.assertNotIn(Complex, Serializer._registry)\n", "def test_serialize_dict_keys(self):\n    # Test that dictionary keys are serialized correctly\n    self.assertSerializedEqual({1: 2, \"a\": 3})\n    self.assertSerializedResultEqual(\n        {1: 2, \"a\": 3},\n        (\"{1: 2, 'a': 3}\", set()),\n    )\n    self.assertSerializedEqual({\"a\": 1, \"b\": 2})\n    self.assertSerializedResultEqual(\n        {\"a\": 1, \"b\": 2},\n        (\"{'a': 1, 'b': 2}\", set()),\n    )\n\n    # Test that dictionary keys with non-standard types are serialized correctly\n    self.assertSerializedEqual({True: 1, False: 2})\n    self.assertSerializedResultEqual(\n        {True: 1, False: 2},\n        (\"{True: 1, False: 2}\", set()),\n    )\n    self.assertSerializedEqual({None: 1, \"a\": 2})\n    self.assertSerializedResultEqual(\n        {None: 1, \"a\": 2},\n        (\"{None: 1, 'a': 2}\", set()),\n    )\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    try:\n        self.assertEqual(Serializer._registry[complex], ComplexSerializer)\n        Serializer.unregister(complex)\n        self.assertNotIn(complex, Serializer._registry)\n    finally:\n        if complex in Serializer._registry:\n            Serializer.unregister(complex)\n", "def test_serialize_type_error(self):\n    class TypeErrorType:\n            raise TypeError(\"Cannot serialize\")\n\n    with self.assertRaisesMessage(\n        ValueError, \"Cannot serialize: <migrations.test_writer.WriterTests.\"\n    ):\n        MigrationWriter.serialize(TypeErrorType())\n"], "sample_426": ["def test_timeuntil(self):\n    \"\"\"Test timeuntil function.\"\"\"\n    t = datetime.datetime(2007, 8, 14, 13, 46, 0)\n    tests = [\n        (t + self.onemicrosecond, \"0\\xa0minutes\"),\n        (t + self.onesecond, \"0\\xa0minutes\"),\n        (t + self.oneminute, \"1\\xa0minute\"),\n        (t + self.onehour, \"1\\xa0hour\"),\n        (t + self.oneday, \"1\\xa0day\"),\n        (t + self.oneweek, \"1\\xa0week\"),\n        (t + self.onemonth, \"1\\xa0month\"),\n        (t + self.oneyear, \"1\\xa0year\"),\n    ]\n    for value, expected in tests:\n        with self.subTest():\n            self.assertEqual(timeuntil(t, value), expected)\n", "def test_depth_zero(self):\n    with self.assertRaises(ValueError):\n        timesince(self.t, self.t + self.oneday, depth=0)\n    with self.assertRaises(ValueError):\n        timeuntil(self.t + self.oneday, self.t, depth=0)\n", "def test_depth_with_time_strings(self):\n    t = (\n        self.t\n        + self.oneyear\n        + self.onemonth\n        + self.oneweek\n        + self.oneday\n        + self.onehour\n    )\n    time_strings = {\n        \"year\": npgettext_lazy(\"naturaltime\", \"%(num)d year\", \"%(num)d years\", \"num\"),\n        \"month\": npgettext_lazy(\"naturaltime\", \"%(num)d month\", \"%(num)d months\", \"num\"),\n    }\n    tests = [\n        (t, 1, \"1\\xa0year\"),\n        (t, 2, \"1\\xa0year, 1\\xa0month\"),\n        (t, 3, \"1\\xa0year, 1\\xa0month\"),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(\n                timesince(self.t, value, time_strings=time_strings, depth=depth),\n                expected,\n            )\n            self.assertEqual(\n                timeuntil(value, self.t, time_strings=time_strings, depth=depth),\n                expected,\n            )\n", "def test_depth_with_zero_values(self):\n    t = self.t + self.oneyear + self.onemonth\n    tests = [\n        (t, 1, \"1\\xa0year\"),\n        (t, 2, \"1\\xa0year, 1\\xa0month\"),\n        (t, 3, \"1\\xa0year, 1\\xa0month\"),\n        (t, 4, \"1\\xa0year, 1\\xa0month\"),\n        (self.t + self.onehour, 5, \"1\\xa0hour\"),\n        (self.t + (4 * self.oneminute), 3, \"4\\xa0minutes\"),\n        (self.t + self.onehour + self.oneminute, 1, \"1\\xa0hour\"),\n        (self.t + self.oneday + self.onehour, 1, \"1\\xa0day\"),\n        (self.t + self.oneweek + self.oneday, 1, \"1\\xa0week\"),\n        (self.t + self.onemonth + self.oneweek, 1, \"1\\xa0month\"),\n        (self.t + self.oneyear + self.onemonth, 1, \"1\\xa0year\"),\n        (self.t + self.oneyear + self.oneweek + self.oneday, 3, \"1\\xa0year\"),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timesince(self.t, value, depth=depth), expected)\n            self.assertEqual(timeuntil(value, self.t, depth=depth), expected)\n", "def test_depth_zero(self):\n    with self.assertRaises(ValueError):\n        timesince(self.t, self.t + self.oneday, depth=0)\n    with self.assertRaises(ValueError):\n        timeuntil(self.t + self.oneday, self.t, depth=0)\n"], "sample_427": ["def test_formset_template_name(self):\n    \"\"\"Test that the formset template name is correct.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"django/forms/formsets/default.html\")\n    # Test that the template name can be overridden.\n    class CustomTemplateFormSet(BaseFormSet):\n        template_name = \"custom/formset.html\"\n\n    CustomChoiceFormSet = formset_factory(Choice, formset=CustomTemplateFormSet)\n    custom_formset = CustomChoiceFormSet()\n    self.assertEqual(custom_formset.template_name, \"custom/formset.html\")\n", "def test_formset_template_name(self):\n    \"\"\"Test formset's template_name attribute.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"django/forms/formsets/default.html\")\n", "    def test_add_fields_with_empty_form(self):\n        class BaseDynamicFormSet(BaseFormSet):\n                super().add_fields(form, index)\n                if index is not None:\n                    form.fields[\"extra\"] = CharField()\n\n        DynamicFormSet = formset_factory(Choice, formset=BaseDynamicFormSet)\n        formset = DynamicFormSet()\n        self.assertIn(\"extra\", formset.forms[0].fields)\n", "def test_formset_template_name(self):\n    \"\"\"A FormSet's template_name can be overridden.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"django/forms/formsets/default.html\")\n\n    class CustomTemplateFormSet(BaseFormSet):\n        template_name = \"myapp/my_template.html\"\n\n    CustomTemplateFormSet = formset_factory(Choice, formset=CustomTemplateFormSet)\n    formset = CustomTemplateFormSet()\n    self.assertEqual(formset.template_name, \"myapp/my_template.html\")\n", "def test_formset_template_name_div(self):\n    \"\"\"Test that formset template name is set to div.html.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"django/forms/formsets/div.html\")\n\n    class CustomTemplateFormSet(BaseFormSet):\n        template_name = \"custom/template.html\"\n\n    CustomFormSet = formset_factory(Choice, formset=CustomTemplateFormSet)\n    custom_formset = CustomFormSet()\n    self.assertEqual(custom_formset.template_name, \"custom/template.html\")\n"], "sample_428": ["def test_decimal_subclass_formatting(self):\n    class EuroDecimal(Decimal):\n            amount = super().__format__(specifier, **kwargs)\n            return \"\u20ac {}\".format(amount)\n\n    euro = EuroDecimal(\"1234.56\")\n    self.assertEqual(nformat(euro, \".\"), \"\u20ac 1234.56\")\n    self.assertEqual(nformat(euro, \".\", decimal_pos=2), \"\u20ac 1234.56\")\n    self.assertEqual(nformat(euro, \".\", grouping=3, thousand_sep=\",\"), \"\u20ac 1,234.56\")\n    self.assertEqual(\n        nformat(euro, \".\", grouping=3, thousand_sep=\",\", force_grouping=True),\n        \"\u20ac 1,234.56\",\n    )\n", "def test_decimal_subclass_formatting(self):\n    class EuroDecimal(Decimal):\n            amount = super().__format__(specifier, **kwargs)\n            return \"\u20ac {}\".format(amount)\n\n    euro_value = EuroDecimal(\"1234.56\")\n    self.assertEqual(nformat(euro_value, \".\"), \"\u20ac 1234.56\")\n    self.assertEqual(nformat(euro_value, \".\", decimal_pos=2), \"\u20ac 1234.56\")\n    self.assertEqual(\n        nformat(euro_value, \".\", grouping=3, thousand_sep=\",\"), \"\u20ac 1,234.56\"\n    )\n    self.assertEqual(\n        nformat(\n            euro_value,\n            \".\",\n            grouping=3,\n            thousand_sep=\",\",\n            force_grouping=True,\n        ),\n        \"\u20ac 1,234.56\",\n    )\n", "def test_decimal_subclass_formatting(self):\n    class EuroDecimal(Decimal):\n        \"\"\"\n        Wrapper for Decimal which prefixes each amount with the \u20ac symbol.\n        \"\"\"\n\n            amount = super().__format__(specifier, **kwargs)\n            return \"\u20ac {}\".format(amount)\n\n    euro_value = EuroDecimal(\"1234.56\")\n    self.assertEqual(nformat(euro_value, \".\"), \"\u20ac 1234.56\")\n    self.assertEqual(nformat(euro_value, \".\", decimal_pos=2), \"\u20ac 1234.56\")\n    self.assertEqual(\n        nformat(euro_value, \".\", grouping=3, thousand_sep=\",\"), \"\u20ac 1,234.56\"\n    )\n    self.assertEqual(\n        nformat(euro_value, \".\", grouping=3, thousand_sep=\",\", force_grouping=True),\n        \"\u20ac 1,234.56\",\n    )\n", "def test_decimal_subclass_formatting(self):\n    class EuroDecimal(Decimal):\n        \"\"\"\n        Wrapper for Decimal which prefixes each amount with the \u20ac symbol.\n        \"\"\"\n\n            amount = super().__format__(specifier, **kwargs)\n            return \"\u20ac {}\".format(amount)\n\n    euro_value = EuroDecimal(\"1234.56\")\n    self.assertEqual(nformat(euro_value, \".\"), \"\u20ac 1234.56\")\n    self.assertEqual(nformat(euro_value, \".\", decimal_pos=2), \"\u20ac 1234.56\")\n    self.assertEqual(\n        nformat(euro_value, \".\", grouping=3, thousand_sep=\",\"), \"\u20ac 1,234.56\"\n    )\n    self.assertEqual(\n        nformat(euro_value, \".\", grouping=3, thousand_sep=\",\", force_grouping=True),\n        \"\u20ac 1,234.56\",\n    )\n", "def test_decimal_subclass_formatting(self):\n    class EuroDecimal(Decimal):\n            amount = super().__format__(specifier, **kwargs)\n            return \"\u20ac {}\".format(amount)\n\n    euro_amount = EuroDecimal(\"1234.56\")\n    self.assertEqual(nformat(euro_amount, \".\"), \"\u20ac 1234.56\")\n    self.assertEqual(nformat(euro_amount, \".\", decimal_pos=2), \"\u20ac 1234.56\")\n    self.assertEqual(\n        nformat(euro_amount, \".\", grouping=3, thousand_sep=\",\"), \"\u20ac 1,234.56\"\n    )\n    self.assertEqual(\n        nformat(euro_amount, \".\", grouping=3, thousand_sep=\",\", force_grouping=True),\n        \"\u20ac 1,234.56\",\n    )\n"], "sample_429": ["    def test_validate_url_with_git_scheme(self):\n        url = \"git://example.com/repo.git\"\n        validator = URLValidator(EXTENDED_SCHEMES)\n        try:\n            validator(url)\n        except ValidationError:\n            self.fail(\"ValidationError was raised unexpectedly\")\n", "def test_prohibit_null_characters_validator_message(self):\n    v = ProhibitNullCharactersValidator(message=\"Null characters are prohibited.\")\n    with self.assertRaisesMessage(ValidationError, \"Null characters are prohibited.\"):\n        v(\"\\x00something\")\n", "def test_prohibit_null_characters_validator_message(self):\n    v = ProhibitNullCharactersValidator(message=\"Null characters are bad.\")\n    with self.assertRaisesMessage(ValidationError, \"Null characters are bad.\"):\n        v(\"\\x00something\")\n", "    def test_prohibit_null_characters_validator(self):\n        validator = ProhibitNullCharactersValidator()\n        self.assertIsNone(validator(\"Hello, World!\"))\n        with self.assertRaisesMessage(\n            ValidationError, \"Null characters are not allowed.\"\n        ):\n            validator(\"\\x00Hello, World!\")\n", "    def test_prohibit_null_characters_validator(self):\n        validator = ProhibitNullCharactersValidator()\n        with self.assertRaisesMessage(\n            ValidationError, validator.message\n        ):\n            validator(\"\\x00something\")\n\n        try:\n            validator(\"something\")\n        except ValidationError:\n            self.fail(\"ValidationError was raised unexpectedly\")\n\n        try:\n            validator(None)\n        except ValidationError:\n            self.fail(\"ValidationError was raised unexpectedly\")\n"], "sample_431": ["    def test_model_validation(self):\n        # Create a model with field validation rules.\n        class ValidatedModel(models.Model):\n            name = models.CharField(max_length=10)\n            age = models.IntegerField()\n\n                if self.age < 0:\n                    raise ValidationError(\"Age cannot be negative.\")\n\n        # Create an instance of the model and try to save it with invalid data.\n        instance = ValidatedModel(name=\"John\", age=-1)\n\n        # full_clean should raise a ValidationError.\n        msg = \"{'__all__': ['Age cannot be negative.']}\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            instance.full_clean()\n", "    def test_equality_with_deferred_instance(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        b = Article.objects.defer(\"headline\").get(pk=a.pk)\n        self.assertEqual(a, b)\n", "    def test_model_full_clean(self):\n        # Create an Article.\n        article = Article(\n            id=None,\n            headline=\"Swallow programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        # Save it into the database. You have to call save() explicitly.\n        article.save()\n\n        # Call full clean on the instance\n        article.full_clean()\n\n        # Now update the instance to be invalid\n        article.pub_date = \"invalid date\"\n\n        # Calling full clean should throw an error\n        with self.assertRaises(ValidationError):\n            article.full_clean()\n", "    def test_full_clean(self):\n        # Full clean is called when you call model validation.\n        article = Article(headline=\"\", pub_date=datetime.now())\n        with self.assertRaises(ValidationError):\n            article.full_clean()\n\n        # A value is provided for field with a default value.\n        article = Article(headline=\"Test\", pub_date=datetime.now())\n        article.full_clean()\n        self.assertEqual(article.headline, \"Test\")\n", "    def test_cloning_model(self):\n        a = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        b = a\n        b.headline = \"Parrot programs in Python 2.0\"\n        b.save()\n        self.assertEqual(a.headline, \"Parrot programs in Python 2.0\")\n"], "sample_432": ["def test_sortable_by(self):\n    \"\"\"\n    Test that the `sortable_by` attribute can be used to customize the columns\n    that can be sorted.\n    \"\"\"\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get(\"/child/\")\n    request.user = self.superuser\n\n    # By default, all list_display columns are sortable.\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.sortable_by, cl.list_display)\n\n    # If sortable_by is set, only those columns are sortable.\n    m.sortable_by = [\"name\"]\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.sortable_by, [\"name\"])\n\n    # If sortable_by is an empty list, no columns are sortable.\n    m.sortable_by = []\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.sortable_by, [])\n", "def test_change_list_template_with_invalid_formset(self):\n    \"\"\"\n    The changelist template is rendered even if the formset is invalid.\n    \"\"\"\n    m = ChildAdmin(Child, custom_site)\n    superuser = self._create_superuser(\"superuser\")\n    request = self._mocked_authenticated_request(\"/child/\", superuser)\n    request.method = \"POST\"\n    request.POST = {\"form-TOTAL_FORMS\": 1, \"form-INITIAL_FORMS\": 0}\n    response = m.changelist_view(request)\n    self.assertEqual(response.status_code, 200)\n", "def test_result_list_with_html_title(self):\n    \"\"\"\n    Inclusion tag result_list generates a table when with default\n    ModelAdmin settings and uses the html title attribute for column headers.\n    \"\"\"\n    new_parent = Parent.objects.create(name=\"parent\")\n    new_child = Child.objects.create(name=\"name\", parent=new_parent)\n    request = self.factory.get(\"/child/\")\n    request.user = self.superuser\n    m = ChildAdmin(Child, custom_site)\n    cl = m.get_changelist_instance(request)\n    cl.formset = None\n    template = Template(\n        \"{% load admin_list %}{% spaceless %}{% result_list cl %}{% endspaceless %}\"\n    )\n    context = Context({\"cl\": cl, \"opts\": Child._meta})\n    table_output = template.render(context)\n    link = reverse(\"admin:admin_changelist_child_change\", args=(new_child.id,))\n    row_html = build_tbody_html(\n        new_child.id,\n        link,\n        '<th class=\"field-parent nowrap\"><a href=\"{}\">Parent object</a></th>'.format(\n            reverse(\"admin:admin_changelist_parent_changelist\")\n        ),\n    )\n    self.assertNotEqual(\n        table_output.find(row_html),\n        -1,\n        \"Failed to find expected row element: %s\" % table_output,\n    )\n", "def test_get_changelist_instance_sortable_by(self):\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get(\"/child/\")\n    request.user = self.superuser\n    cl = m.get_changelist_instance(request)\n    # If ModelAdmin.sortable_by is not set, all list_display fields are sortable.\n    self.assertEqual(cl.get_sortable_by(request), [\"name\", \"age\"])\n    # Make only 'age' sortable.\n    m.sortable_by = (\"age\",)\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_sortable_by(request), [\"age\"])\n    # Make only 'age' and 'name' sortable.\n    m.sortable_by = (\"age\", \"name\")\n    cl = m.get_changelist_instance(request)\n    self.assertEqual(cl.get_sortable_by(request), [\"age\", \"name\"])\n", "def test_get_changeform_initial_data(self):\n    parent = Parent.objects.create(name=\"anything\")\n    child = Child.objects.create(name=\"name\", parent=parent)\n    m = ChildAdmin(Child, custom_site)\n    request = self.factory.get(\"/child/\", data={\"object_id\": child.pk})\n    request.user = self.superuser\n    initial_data = m.get_changeform_initial_data(request)\n    self.assertEqual(initial_data[\"object_id\"], str(child.pk))\n"], "sample_433": ["def test_suggest_name_with_custom_name(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.DeleteModel(\"Animal\"),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    custom_name = \"custom_name\"\n    self.assertEqual(\n        migration.suggest_name(custom_name=custom_name), f\"0001_{custom_name}\"\n    )\n", "def test_migration_suggest_name_timestamp(self):\n    migration = Migration(\"some_migration\", \"test_app\")\n    suggested_name = migration.suggest_name()\n    self.assertRegex(suggested_name, r\"^auto_\\d{14}$\")\n", "def test_suggest_name_with_custom_migration_name_fragment(self):\n    class Migration(migrations.Migration):\n        operations = [migrations.CreateModel(\"Person\", fields=[])]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    custom_name = \"custom_person_model\"\n    suggested_name = migration.suggest_name(custom_name)\n    self.assertEqual(suggested_name, f\"0001_{custom_name}\")\n", "def test_migrations_suggest_name_with_custom_migration_name(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\"Person\", fields=[]),\n            migrations.CreateModel(\"Animal\", fields=[]),\n        ]\n\n    migration = Migration(\"0001_initial\", \"test_app\")\n    custom_name = \"create_models\"\n    self.assertEqual(\n        migration.suggest_name(custom_name), f\"0002_{custom_name}\"\n    )\n", "def test_suggest_name_for_long_operation_names(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\"VeryLongModelNameThatWillBeTruncated\", fields=[]),\n            migrations.DeleteModel(\"AnotherVeryLongModelNameThatWillBeTruncated\"),\n        ]\n\n    migration = Migration(\"some_migration\", \"test_app\")\n    self.assertEqual(\n        migration.suggest_name(),\n        \"verylongmodelnamethatwillbetruncated_delete_anotherv\",\n    )\n"], "sample_434": ["    def test_render_to_response(self):\n        mixin = TemplateResponseMixin()\n        mixin.request = RequestFactory().get(\"/\")\n        mixin.template_name = \"template.html\"\n        response = mixin.render_to_response({})\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, \"template.html\")\n", "    def test_template_view_renders_correctly(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n\n        view = TestTemplateView.as_view()\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/\")\n        response = view(request)\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 200)\n", "    def test_get_template_names(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n\n        view = TestTemplateView()\n        self.assertEqual(view.get_template_names(), [\"test_template.html\"])\n", "    def test_template_view_renders_template(self):\n        class MyTemplateView(TemplateView):\n            template_name = \"template.html\"\n\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/\")\n        response = MyTemplateView.as_view()(request)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, \"template.html\")\n", "    def test_template_view_renders_template(self):\n        class TestTemplateView(TemplateView):\n            template_name = \"test_template.html\"\n\n        request_factory = RequestFactory()\n        request = request_factory.get(\"/\")\n        view = TestTemplateView.as_view()\n        response = view(request)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.template_name, \"test_template.html\")\n"], "sample_435": ["    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable password\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_custom_username_field(self):\n        class CustomUserCreationForm(UserCreationForm):\n            class Meta(UserCreationForm.Meta):\n                model = CustomUser\n                fields = (\"email\",)\n\n        data = {\n            \"email\": \"test@client.com\",\n            \"password1\": \"testclient\",\n            \"password2\": \"testclient\",\n        }\n        form = CustomUserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.email, \"test@client.com\")\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + \"unusable\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_custom_username_field(self):\n        class CustomUserCreationForm(UserCreationForm):\n            class Meta(UserCreationForm.Meta):\n                model = CustomUser\n                fields = (\"email\",)\n\n        data = {\n            \"email\": \"test@client222.com\",\n            \"password1\": \"testclient\",\n            \"password2\": \"testclient\",\n        }\n        form = CustomUserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.email, data[\"email\"])\n        self.assertTrue(user.check_password(data[\"password1\"]))\n"], "sample_436": ["    def test_help_text(self):\n        out, err = self.run_manage([\"runserver\", \"--help\"])\n        self.assertNoOutput(err)\n        self.assertOutput(out, \"Starts a lightweight development Web server.\")\n        self.assertOutput(out, \"--addrport\")\n        self.assertOutput(out, \"--ipv6\")\n        self.assertOutput(out, \"--nothreading\")\n        self.assertOutput(out, \"--noreload\")\n        self.assertOutput(out, \"--skip-checks\")\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\"settings.py\")\n", "    def test_check_command_closes_connections(self):\n        \"\"\"\n        The check command should close connections after it's done.\n        \"\"\"\n        with mock.patch(\"django.core.management.base.connections\") as mock_connections:\n            call_command(\"check\")\n        self.assertTrue(mock_connections.close_all.called)\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\"settings.py\")\n", "    def test_default_verbosity(self):\n        out, err = self.run_manage([\"runserver\", \"--verbosity=1\"])\n        self.assertNoOutput(err)\n        self.assertOutput(out, \"Starting development server\")\n"], "sample_437": ["    def test_timezone(self):\n        with override_settings(USE_TZ=True, TIME_ZONE=\"America/New_York\"):\n            connection.settings_dict[\"TIME_ZONE\"] = \"Europe/Paris\"\n            self.assertEqual(connection.timezone_name, \"Europe/Paris\")\n\n        with override_settings(USE_TZ=False):\n            self.assertIsNone(connection.timezone)\n            connection.settings_dict[\"TIME_ZONE\"] = None\n            self.assertEqual(connection.timezone_name, settings.TIME_ZONE)\n", "    def test_validate_thread_sharing(self):\n        connection = connections[DEFAULT_DB_ALIAS]\n        with patch.object(connection, \"_thread_ident\", 123):\n            with self.assertRaises(DatabaseError):\n                connection.validate_thread_sharing()\n", "    def test_timezone_constructor(self):\n        with override_settings(USE_DEPRECATED_PYTZ=True):\n            import pytz\n\n            tz = timezone_constructor(\"UTC\")\n            self.assertIsInstance(tz, pytz.tzinfo.BaseTzInfo)\n\n        with override_settings(USE_DEPRECATED_PYTZ=False):\n            import zoneinfo\n\n            tz = timezone_constructor(\"UTC\")\n            self.assertIsInstance(tz, zoneinfo.ZoneInfo)\n", "    def setUp(self):\n        # All test cases here need newly configured and created connections.\n        # Use the default db connection for convenience.\n        connection.close()\n        self.addCleanup(connection.close)\n", "    def test_atomic(self):\n        conn = connections[DEFAULT_DB_ALIAS]\n        self.assertFalse(conn.in_atomic_block)\n        with transaction.atomic():\n            self.assertTrue(conn.in_atomic_block)\n        self.assertFalse(conn.in_atomic_block)\n"], "sample_438": ["def test_generic_foreign_key_field_name_clash(self):\n    class Model(models.Model):\n        field = GenericForeignKey()\n        field_id = models.IntegerField()\n\n    errors = Model.check()\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], models.FieldError)\n    self.assertEqual(\n        errors[0].args[0],\n        \"'field' clashes with the field 'field_id' from model 'contenttypes_tests.model'.\",\n    )\n", "def test_generic_foreign_key_get_content_type(self):\n    answer = Answer.objects.create(text=\"Answer\")\n    self.assertIsNone(answer.question.get_content_type())\n    question = Question.objects.create(text=\"Who?\")\n    answer.question = question\n    self.assertEqual(answer.question.get_content_type(), question.content_type)\n", "def test_generic_foreign_key_on_model_with_custom_primary_key(self):\n    class CustomPrimaryKeyModel(models.Model):\n        custom_id = models.AutoField(primary_key=True)\n        content_type = models.ForeignKey(\n            \"contenttypes.ContentType\", on_delete=models.CASCADE\n        )\n        object_id = models.IntegerField()\n        content_object = GenericForeignKey()\n\n    instance = CustomPrimaryKeyModel.objects.create(\n        content_type=ContentType.objects.get_for_model(Question), object_id=1\n    )\n\n    with self.assertNumQueries(1):\n        self.assertIsNone(instance.content_object)\n\n    question = Question.objects.create(text=\"What is your name?\")\n    instance.object_id = question.pk\n    instance.save()\n\n    with self.assertNumQueries(1):\n        self.assertEqual(instance.content_object, question)\n", "def test_deferred_generic_foreign_key(self):\n    class Model(models.Model):\n        field = GenericForeignKey()\n\n    instance = Model()\n    self.assertIs(instance.field, models.DEFERRED)\n    with self.assertRaises(AttributeError):\n        instance.field.__class__\n", "def test_model_state_fields_cache_descriptor(self):\n    model = Answer()\n    self.assertEqual(model._state.fields_cache, {})\n    model._state.fields_cache[\"test\"] = True\n    self.assertEqual(model._state.fields_cache, {\"test\": True})\n"], "sample_440": ["def test_bulk_create_with_m2m_relation(self):\n    countries = [\n        Country(name=\"United States of America\", iso_two_letter=\"US\"),\n        Country(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n    ]\n    Country.objects.bulk_create(countries)\n    restaurant = Restaurant.objects.create(name=\"McDonald's\")\n    restaurant.countries.set(countries)\n    self.assertEqual(restaurant.countries.count(), 2)\n", "def test_update_conflicts_upsert(self):\n    UpsertConflict.objects.bulk_create(\n        [\n            UpsertConflict(number=1, rank=1, name=\"John\"),\n            UpsertConflict(number=2, rank=2, name=\"Mary\"),\n            UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n        ]\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 3)\n\n    new_objects = [\n        UpsertConflict(number=1, rank=4, name=\"Steve\"),\n        UpsertConflict(number=4, rank=4, name=\"Mark\"),\n    ]\n    UpsertConflict.objects.bulk_create(\n        new_objects,\n        update_conflicts=True,\n        update_fields=[\"name\", \"rank\"],\n        unique_fields=[\"number\"],\n    )\n    self.assertEqual(UpsertConflict.objects.count(), 4)\n    self.assertCountEqual(\n        UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n        [\n            {\"number\": 1, \"rank\": 4, \"name\": \"Steve\"},\n            {\"number\": 2, \"rank\": 2, \"name\": \"Mary\"},\n            {\"number\": 3, \"rank\": 3, \"name\": \"Hannah\"},\n            {\"number\": 4, \"rank\": 4, \"name\": \"Mark\"},\n        ],\n    )\n    # The pk of upserted objects should be set.\n    self.assertIsNotNone(new_objects[0].pk)\n    # The pk of inserted objects should be set.\n    self.assertIsNotNone(new_objects[1].pk)\n", "def test_update_conflicts_update_fields_pk(self):\n    msg = \"bulk_create() cannot be used with primary key fields in update_fields.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        UpsertConflict.objects.bulk_create(\n            [UpsertConflict(number=1, rank=1, name=\"John\")],\n            update_conflicts=True,\n            update_fields=[\"number\"],\n        )\n", "def test_update_conflicts_unique_fields_pk(self):\n    self._test_update_conflicts(unique_fields=[\"pk\"])\n", "def test_update_conflicts_non_unique_fields(self):\n    msg = (\n        \"This database backend does not support updating conflicts with \"\n        \"specifying unique fields that can trigger the upsert.\"\n    )\n    with self.assertRaisesMessage(NotSupportedError, msg):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n            ],\n            update_conflicts=True,\n            update_fields=[\"name\", \"rank\"],\n            unique_fields=[\"rank\"],\n        )\n"], "sample_441": ["    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable password\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"!\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable_password\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n"], "sample_442": ["    def test_sign_unsign(self):\n        signer = signing.Signer(key=\"predictable-secret\", algorithm=\"sha512\")\n        examples = [\n            \"q;wjmbk;wkmb\",\n            \"3098247529087\",\n            \"3098247:529:087:\",\n            \"jkw osanteuh ,rcuh nthu aou oauh ,ud du\",\n            \"\\u2019\",\n        ]\n        for example in examples:\n            signed = signer.sign(example)\n            self.assertIsInstance(signed, str)\n            self.assertNotEqual(example, signed)\n            self.assertEqual(example, signer.unsign(signed))\n", "    def test_dumps(self):\n        data = {\"foo\": \"bar\", \"baz\": [1, 2, 3]}\n        serializer = signing.JSONSerializer()\n        self.assertEqual(serializer.dumps(data), '{\"foo\":\"bar\",\"baz\":[1,2,3]}'.encode(\"latin-1\"))\n", "    def test_dumps(self):\n        serializer = signing.JSONSerializer()\n        data = {\"a\": 1, \"b\": \"hello\"}\n        self.assertEqual(serializer.dumps(data), '{\"a\":1,\"b\":\"hello\"}'.encode(\"latin-1\"))\n", "    def test_json_serializer(self):\n        serializer = signing.JSONSerializer()\n        tests = [\n            [\"a\", \"list\"],\n            \"a string \\u2019\",\n            {\"a\": \"dictionary\"},\n        ]\n        for obj in tests:\n            with self.subTest(obj=obj):\n                serialized = serializer.dumps(obj)\n                self.assertIsInstance(serialized, bytes)\n                self.assertNotEqual(obj, serialized)\n                self.assertEqual(obj, serializer.loads(serialized))\n", "    def test_bad_signature(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        signed_value = signer.sign(\"hello\")\n        with self.assertRaises(signing.BadSignature):\n            signer.unsign(signed_value + \"a\")\n"], "sample_443": ["    def test_cache_key_with_control_characters(self):\n        key = \"key-with-\\x01-control-character\"\n        msg = KEY_ERRORS_WITH_MEMCACHED_MSG % cache.make_key(key)\n        with self.assertWarnsMessage(CacheKeyWarning, msg):\n            cache.get(key)\n", "    def setUp(self):\n        self.cache_dir = tempfile.mkdtemp()\n        with override_settings(CACHES={\"default\": {\"LOCATION\": self.cache_dir}}):\n            self.cache = caches[\"default\"]\n", "    def setUp(self):\n        self.cache = caches[\"default\"]\n        self.key = \"test_key\"\n        self.value = \"test_value\"\n", "    def setUp(self):\n        self.cache_dir = tempfile.mkdtemp()\n        with override_settings(\n            CACHES={\n                \"default\": {\n                    \"BACKEND\": \"django.core.cache.backends.filebased.FileBasedCache\",\n                    \"LOCATION\": self.cache_dir,\n                },\n            }\n        ):\n            cache.set(\"test\", \"value\")\n", "    def test_path_traversal(self):\n        cache = caches[\"default\"]\n        key = \"../../../etc/passwd\"\n        value = \"test\"\n\n        # Attempt to set a cache key that would result in a path traversal attack\n        with self.assertRaises(ValueError):\n            cache.set(key, value)\n\n        # Ensure the cache key was not created\n        self.assertIsNone(cache.get(key))\n"], "sample_444": ["    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, \"test\"))\n        self.addCleanup(shutil.rmtree, temp_dir)\n", "    def setUp(self):\n        super().setUp()\n        self.manifest_path = Path(tempfile.mkdtemp())\n        self.addCleanup(shutil.rmtree, self.manifest_path)\n", "    def setUp(self):\n        super().setUp()\n        self._temp_dir = temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, \"test\"))\n        self.addCleanup(shutil.rmtree, temp_dir)\n\n        # Create an intermediate file that will be referenced by a CSS file\n        intermediate_filename = \"intermediate_file.css\"\n        intermediate_file_path = self._get_filename_path(intermediate_filename)\n        with open(intermediate_file_path, \"w\") as f:\n            f.write(\"/* Intermediate file */\")\n\n        # Update the STATICFILES_DIRS setting to include the temporary directory\n        self.settings(STATICFILES_DIRS=[self._temp_dir])\n", "    def test_manifest_hash_invalidation(self):\n        manifest_hash_orig = storage.staticfiles_storage.manifest_hash\n        # Modify a file and check that the manifest hash changes.\n        with open(os.path.join(settings.STATIC_ROOT, \"test.txt\"), \"w\") as f:\n            f.write(\"changed\")\n        self.run_collectstatic()\n        manifest_hash_new = storage.staticfiles_storage.manifest_hash\n        self.assertNotEqual(manifest_hash_orig, manifest_hash_new)\n", "    def test_hashed_name_with_query_parameters(self):\n        relpath = self.hashed_file_path(\"cached/css/window.css?param=value\")\n        self.assertEqual(relpath, \"cached/css/window.5d5c10836967.css?param=value\")\n        with storage.staticfiles_storage.open(\n            \"cached/css/window.5d5c10836967.css\"\n        ) as relfile:\n            content = relfile.read()\n            self.assertNotIn(b\"url(img/window.png)\", content)\n            self.assertIn(b'url(\"img/window.acae32e4532b.png\")', content)\n"], "sample_445": ["def test_timeuntil(self):\n    \"\"\"Test timeuntil with various inputs.\"\"\"\n    tests = [\n        (self.t, self.t, \"0\\xa0minutes\"),\n        (self.t, self.t + self.onemicrosecond, \"0\\xa0minutes\"),\n        (self.t, self.t + self.onesecond, \"0\\xa0minutes\"),\n        (self.t, self.t + self.oneminute, \"1\\xa0minute\"),\n        (self.t, self.t + self.onehour, \"1\\xa0hour\"),\n        (self.t, self.t + self.oneday, \"1\\xa0day\"),\n        (self.t, self.t + self.oneweek, \"1\\xa0week\"),\n        (self.t, self.t + self.onemonth, \"1\\xa0month\"),\n        (self.t, self.t + self.oneyear, \"1\\xa0year\"),\n    ]\n    for d, now, expected in tests:\n        with self.subTest():\n            self.assertEqual(timeuntil(d, now), expected)\n", "def test_invalid_time_strings(self):\n    msg = \"time_strings must be a dict.\"\n    with self.assertRaisesMessage(TypeError, msg):\n        timesince(self.t, self.t, time_strings=\"invalid\")\n    with self.assertRaisesMessage(TypeError, msg):\n        timeuntil(self.t, self.t, time_strings=\"invalid\")\n\n    msg = \"time_strings must contain all required keys.\"\n    invalid_time_strings = {\"year\": \"1 year\"}\n    with self.assertRaisesMessage(ValueError, msg):\n        timesince(self.t, self.t, time_strings=invalid_time_strings)\n    with self.assertRaisesMessage(ValueError, msg):\n        timeuntil(self.t, self.t, time_strings=invalid_time_strings)\n", "def test_timeuntil_depth(self):\n    t = (\n        self.t\n        + self.oneyear\n        + self.onemonth\n        + self.oneweek\n        + self.oneday\n        + self.onehour\n    )\n    tests = [\n        (t, 1, \"1\\xa0year\"),\n        (t, 2, \"1\\xa0year, 1\\xa0month\"),\n        (t, 3, \"1\\xa0year, 1\\xa0month, 1\\xa0week\"),\n        (t, 4, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day\"),\n        (t, 5, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n        (t, 6, \"1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour\"),\n        (self.t + self.onehour, 5, \"1\\xa0hour\"),\n        (self.t + (4 * self.oneminute), 3, \"4\\xa0minutes\"),\n        (self.t + self.onehour + self.oneminute, 1, \"1\\xa0hour\"),\n        (self.t + self.oneday + self.onehour, 1, \"1\\xa0day\"),\n        (self.t + self.oneweek + self.oneday, 1, \"1\\xa0week\"),\n        (self.t + self.onemonth + self.oneweek, 1, \"1\\xa0month\"),\n        (self.t + self.oneyear + self.onemonth, 1, \"1\\xa0year\"),\n        (self.t + self.oneyear + self.oneweek + self.oneday, 3, \"1\\xa0year\"),\n    ]\n    for value, depth, expected in tests:\n        with self.subTest():\n            self.assertEqual(timeuntil(self.t, value, depth=depth), expected)\n", "def test_time_strings_invalid(self):\n    msg = \"time_strings must be a dict\"\n    with self.assertRaisesMessage(TypeError, msg):\n        timesince(self.t, self.t, time_strings=\"invalid\")\n\n    msg = \"time_strings must contain all required keys\"\n    with self.assertRaisesMessage(ValueError, msg):\n        timesince(self.t, self.t, time_strings={\"year\": \"%(num)d year\"})\n", "def test_time_strings(self):\n    time_strings = {\n        \"year\": npgettext_lazy(\"naturaltime-past\", \"%(num)d year\", \"%(num)d years\", \"num\"),\n        \"month\": npgettext_lazy(\"naturaltime-past\", \"%(num)d month\", \"%(num)d months\", \"num\"),\n        \"week\": npgettext_lazy(\"naturaltime-past\", \"%(num)d week\", \"%(num)d weeks\", \"num\"),\n        \"day\": npgettext_lazy(\"naturaltime-past\", \"%(num)d day\", \"%(num)d days\", \"num\"),\n        \"hour\": npgettext_lazy(\"naturaltime-past\", \"%(num)d hour\", \"%(num)d hours\", \"num\"),\n        \"minute\": npgettext_lazy(\"naturaltime-past\", \"%(num)d minute\", \"%(num)d minutes\", \"num\"),\n    }\n    t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n    expected = \"1\\xa0ano, 1\\xa0m\u00eas\"\n    with translation.override(\"pt\"):\n        self.assertEqual(timesince(self.t, t, time_strings=time_strings), expected)\n"], "sample_446": ["    def test_localize(self):\n        with translation.override(\"de\", deactivate=True):\n            self.assertEqual(floatformat(66666.666, \"2\"), \"66666,67\")\n            self.assertEqual(floatformat(66666.666, \"2l\"), \"66666,67\")\n            with self.settings(\n                USE_THOUSAND_SEPARATOR=True,\n                NUMBER_GROUPING=3,\n                THOUSAND_SEPARATOR=\"!\",\n            ):\n                self.assertEqual(floatformat(66666.666, \"2gl\"), \"66!666,67\")\n                self.assertEqual(floatformat(66666.666, \"2lg\"), \"66!666,67\")\n            # Invalid suffix.\n            self.assertEqual(floatformat(66666.666, \"l2\"), \"66666.666\")\n", "    def test_add_filter(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5})\n        self.assertEqual(output, \"6\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 1.23})\n        self.assertEqual(output, \"1.230000e+00\")\n", "    def test_rounding(self):\n        # Test rounding for values exactly halfway between two numbers.\n        with localcontext() as ctx:\n            ctx.rounding = ROUND_HALF_UP\n            self.assertEqual(floatformat(0.5), \"1\")\n            self.assertEqual(floatformat(0.05), \"0.1\")\n            self.assertEqual(floatformat(0.005), \"0.01\")\n            self.assertEqual(floatformat(0.0005), \"0.001\")\n            self.assertEqual(floatformat(1.5), \"2\")\n            self.assertEqual(floatformat(1.05), \"1.1\")\n            self.assertEqual(floatformat(1.005), \"1.01\")\n            self.assertEqual(floatformat(1.0005), \"1.001\")\n\n        # Test rounding for values not exactly halfway between two numbers.\n        self.assertEqual(floatformat(0.4), \"0.4\")\n        self.assertEqual(floatformat(0.04), \"0.0\")\n        self.assertEqual(floatformat(0.004), \"0.0\")\n        self.assertEqual(floatformat(0.0004), \"0.0004\")\n        self.assertEqual(floatformat(1.4), \"1.4\")\n        self.assertEqual(floatformat(1.04), \"1.0\")\n        self.assertEqual(floatformat(1.004), \"1.0\")\n        self.assertEqual(floatformat(1.0004), \"1.0004\")\n", "    def test_round_half_up(self):\n        with localcontext() as ctx:\n            ctx.rounding = Decimal(\"ROUND_HALF_UP\")\n            self.assertEqual(floatformat(0.5), \"1\")\n            self.assertEqual(floatformat(-0.5), \"-1\")\n            self.assertEqual(floatformat(1.5), \"2\")\n            self.assertEqual(floatformat(-1.5), \"-2\")\n"], "sample_447": ["def test_alias_with_m2m(self):\n    qs = (\n        Author.objects.alias(\n            book_isbn=F(\"book__isbn\"),\n        )\n        .filter(name=\"Adrian Holovaty\")\n        .order_by(\"book_isbn\")\n    )\n    self.assertIs(hasattr(qs.first(), \"book_isbn\"), False)\n    self.assertQuerySetEqual(\n        qs,\n        [\n            (\"159059725\",),\n        ],\n        lambda a: (a.book_isbn,),\n    )\n", "def test_expression_list(self):\n    tests = [\n        ([Value(1), Value(2)], [1, 2]),\n        ([F(\"pk\"), F(\"name\")], [(b.pk, b.name) for b in Book.objects.all()]),\n        ([Case(When(pk=1, then=Value(1)), default=Value(0))], [1 if b.pk == 1 else 0 for b in Book.objects.all()]),\n    ]\n    for expressions, expected_values in tests:\n        with self.subTest(expressions=expressions):\n            qs = Book.objects.annotate(\n                result=ExpressionList(*expressions),\n            ).values_list(\"result\", flat=True)\n            self.assertSequenceEqual(qs, expected_values)\n", "def test_alias_overwrite_annotation(self):\n    qs = Book.objects.annotate(is_book=Value(1)).alias(is_book=Value(2))\n    self.assertIs(hasattr(qs.first(), \"is_book\"), False)\n", "def test_alias_in_expression_wrapper(self):\n    qs = Book.objects.alias(\n        rating_alias=F(\"rating\") - 1,\n    ).annotate(\n        wrapped_rating=ExpressionWrapper(F(\"rating_alias\"), output_field=FloatField()),\n    )\n    self.assertIs(hasattr(qs.first(), \"rating_alias\"), False)\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertEqual(book.wrapped_rating, book.rating - 1)\n", "def test_alias_in_expressionwrapper(self):\n    qs = Book.objects.alias(\n        rating_alias=F(\"rating\"),\n    ).annotate(other_rating=ExpressionWrapper(F(\"rating_alias\"), output_field=FloatField()))\n    self.assertIs(hasattr(qs.first(), \"rating_alias\"), False)\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertEqual(book.other_rating, book.rating)\n"], "sample_448": ["def test_contains_expressions(self):\n    constraint = models.UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n    self.assertIs(constraint.contains_expressions, False)\n\n    constraint_with_expression = models.UniqueConstraint(\n        Lower(\"title\"),\n        F(\"author\"),\n        name=\"book_func_uq\",\n    )\n    self.assertIs(constraint_with_expression.contains_expressions, True)\n", "def test_validate_expression_with_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    existing_product = UniqueConstraintProduct.objects.create(name=\"test\")\n    new_product = UniqueConstraintProduct(name=\"Test\")\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, new_product)\n    constraint.validate(\n        UniqueConstraintProduct,\n        new_product,\n        exclude={\"name\"},\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        existing_product,\n    )\n", "def test_validate_with_exclude(self):\n    constraint = models.UniqueConstraint(fields=[\"name\", \"color\"], name=\"unique\")\n    existing_product = UniqueConstraintProduct.objects.create(name=\"p1\", color=\"red\")\n    new_product = UniqueConstraintProduct(name=existing_product.name, color=existing_product.color)\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, new_product)\n    # Excluding one of the fields should still raise ValidationError.\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, new_product, exclude={\"name\"})\n    # Excluding all fields should not raise ValidationError.\n    constraint.validate(UniqueConstraintProduct, new_product, exclude={\"name\", \"color\"})\n", "def test_validate_expression_with_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    non_unique_product = UniqueConstraintProduct(name=self.p1.name.upper())\n    constraint.validate(\n        UniqueConstraintProduct,\n        non_unique_product,\n        exclude={\"name\"},\n    )\n    with self.assertRaises(ValidationError):\n        constraint.validate(\n            UniqueConstraintProduct,\n            non_unique_product,\n            exclude={\"other_field\"},\n        )\n", "def test_validate_with_exclude_on_expression(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_uniq\",\n    )\n    instance = UniqueConstraintProduct(name=self.p1.name.upper())\n    # Excluding a field used by the expression should skip validation.\n    constraint.validate(UniqueConstraintProduct, instance, exclude={\"name\"})\n    # Excluding another field should still raise ValidationError.\n    with self.assertRaisesMessage(\n        ValidationError, \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    ):\n        constraint.validate(UniqueConstraintProduct, instance, exclude={\"color\"})\n"], "sample_449": ["def test_handle_one_request_close_connection(self):\n    request = Stub(makefile=lambda mode, *a, **kw: BytesIO())\n    server = Stub(base_environ={}, get_app=lambda: None)\n\n    handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n    handler.close_connection = False\n\n    # Simulate a response that requires closing the connection.\n        handler.send_response(200)\n        handler.send_header(\"Connection\", \"close\")\n        handler.end_headers()\n\n    server.get_app = lambda: run\n\n    with captured_stderr() as stderr:\n        handler.handle_one_request()\n\n    self.assertIn(\"Connection: close\", stderr.getvalue())\n    self.assertTrue(handler.close_connection)\n", "def test_close_connection(self):\n        \"\"\"A WSGI app that returns a hello world.\"\"\"\n        start_response(\"200 OK\", [(\"Connection\", \"close\")])\n        return [b\"<!DOCTYPE html><html><body>Hello World</body></html>\"]\n\n    rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n    rfile.seek(0)\n\n    wfile = UnclosableBytesIO()\n\n        if mode == \"rb\":\n            return rfile\n        elif mode == \"wb\":\n            return wfile\n\n    request = Stub(makefile=makefile)\n    server = Stub(base_environ={}, get_app=lambda: test_app)\n\n    # Prevent logging from appearing in test output.\n    with self.assertLogs(\"django.server\", \"INFO\"):\n        # Instantiating a handler runs the request as side effect.\n        handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n        self.assertTrue(handler.close_connection)\n\n    wfile.seek(0)\n    lines = list(wfile.readlines())\n    self.assertIn(b\"Connection: close\\r\\n\", lines)\n", "def test_connection_close_header(self):\n    \"\"\"WSGIRequestHandler sets Connection: close for certain responses.\"\"\"\n\n        \"\"\"A WSGI app that returns a hello world.\"\"\"\n        start_response(\"200 OK\", [])\n        return [b\"<!DOCTYPE html><html><body>Hello World</body></html>\"]\n\n    rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n    rfile.seek(0)\n\n    wfile = UnclosableBytesIO()\n\n        if mode == \"rb\":\n            return rfile\n        elif mode == \"wb\":\n            return wfile\n\n    request = Stub(makefile=makefile)\n    server = Stub(base_environ={}, get_app=lambda: test_app)\n\n    # Prevent logging from appearing in test output.\n    with self.assertLogs(\"django.server\", \"INFO\"):\n        # Instantiating a handler runs the request as side effect.\n        WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n    wfile.seek(0)\n    lines = list(wfile.readlines())\n    self.assertNotIn(b\"Connection: close\\r\\n\", lines)\n\n    # Now test with no Content-Length header.\n        \"\"\"A WSGI app that returns a hello world.\"\"\"\n        start_response(\"200 OK\", [(\"Content-Type\", \"text/html\")])\n        return [b\"<!DOCTYPE html><html><body>Hello World</body></html>\"]\n\n    rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n    rfile.seek(0)\n    wfile = UnclosableBytesIO()\n\n    with self.assertLogs(\"django.server\", \"INFO\"):\n        WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n    wfile.seek(0)\n    lines = list(wfile.readlines())\n    self.assertIn(b\"Connection: close\\r\\n\", lines)\n", "def test_handle_one_request(self):\n    request = Stub(makefile=lambda *args, **kwargs: BytesIO())\n    handler = WSGIRequestHandler(request, \"192.168.0.2\", None)\n\n    # Test that handle_one_request doesn't crash when given an empty request\n    handler.raw_requestline = b\"\"\n    handler.handle_one_request()\n\n    # Test that handle_one_request calls send_error when the request is too long\n    handler.raw_requestline = b\"GET \" + b\"a\" * 65536 + b\" HTTP/1.1\\r\\n\"\n    handler.handle_one_request()\n    self.assertEqual(handler.requestline, \"\")\n    self.assertEqual(handler.request_version, \"\")\n    self.assertEqual(handler.command, \"\")\n\n    # Test that handle_one_request closes the connection when an error occurs\n    handler.close_connection = False\n    handler.raw_requestline = b\"Invalid request\"\n    handler.handle_one_request()\n    self.assertTrue(handler.close_connection)\n", "def test_address_string(self):\n    request = Stub(META={\"REMOTE_ADDR\": \"192.168.0.2\"})\n    handler = WSGIRequestHandler(request, \"192.168.0.2\", None)\n    self.assertEqual(handler.address_string(), \"192.168.0.2\")\n"], "sample_450": ["def test_get_admin_log_template_tag(self):\n    \"\"\"\n    Test that the get_admin_log template tag returns the correct log entries.\n    \"\"\"\n    # Create some log entries\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something\",\n    )\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something else\",\n    )\n\n    # Test that the template tag returns the correct log entries\n    template = template.Template(\"{% load admin_utils %}{% get_admin_log 10 as admin_log %}{{ admin_log }}\")\n    context = template.Context({\"log_entries\": LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(len(rendered.strip().splitlines()), 2)\n\n    # Test that the template tag returns the correct log entries for a specific user\n    template = template.Template(\"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user user %}{{ admin_log }}\")\n    context = template.Context({\"log_entries\": LogEntry.objects.all(), \"user\": self.user})\n    rendered = template.render(context)\n    self.assertEqual(len(rendered.strip().splitlines()), 2)\n\n    # Test that the template tag returns no log entries for an invalid user\n    template = template.Template(\"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user ' invalid_user' %}{{ admin_log }}\")\n    context = template.Context({\"log_entries\": LogEntry.objects.all()})\n    rendered = template.render(context)\n    self.assertEqual(rendered.strip(), \"\")\n\n    # Test that the template tag raises a TemplateSyntaxError for an invalid syntax\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.Template(\"{% load admin_utils %}{% get_admin_log %}\")\n", "def test_get_admin_log_template_tag(self):\n    \"\"\"\n    The get_admin_log template tag returns the correct admin log entries.\n    \"\"\"\n    # Create a new user and log some actions\n    new_user = User.objects.create_superuser(\n        username=\"newuser\", password=\"secret\", email=\"newuser@example.com\"\n    )\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        new_user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something else\",\n    )\n\n    # Test the template tag with different parameters\n    template = template.Template(\n        \"{% load admin_list %}{% get_admin_log 10 as admin_log for_user user %}\"\n    )\n    context = template.Context({\"user\": new_user, \"log_entries\": LogEntry.objects.all()})\n    output = template.render(context)\n    self.assertEqual(len(context[\"admin_log\"]), 1)\n\n    template = template.Template(\"{% load admin_list %}{% get_admin_log 10 as admin_log %}\")\n    context = template.Context({\"log_entries\": LogEntry.objects.all()})\n    output = template.render(context)\n    self.assertEqual(len(context[\"admin_log\"]), 2)\n\n    template = template.Template(\n        \"{% load admin_list %}{% get_admin_log 1 as admin_log for_user user %}\"\n    )\n    context = template.Context({\"user\": new_user, \"log_entries\": LogEntry.objects.all()})\n    output = template.render(context)\n    self.assertEqual(len(context[\"admin_log\"]), 1)\n", "def test_get_admin_log_template_tag(self):\n    \"\"\"\n    The get_admin_log template tag returns a list of LogEntry objects for the\n    given user and content type.\n    \"\"\"\n    log_entries = LogEntry.objects.filter(content_type__model__iexact=\"article\")\n    context = {\"log_entries\": log_entries}\n    template = Template('{% load get_admin_log %}{% get_admin_log 10 as admin_log %}{{ admin_log|length }}')\n    rendered = template.render(Context(context))\n    self.assertEqual(rendered, '1')\n\n    # Test with a specific user\n    template = Template('{% load get_admin_log %}{% get_admin_log 10 as admin_log for_user \"super\" %}{{ admin_log|length }}')\n    rendered = template.render(Context(context))\n    self.assertEqual(rendered, '1')\n\n    # Test with an invalid user\n    template = Template('{% load get_admin_log %}{% get_admin_log 10 as admin_log for_user \"invalid\" %}{{ admin_log|length }}')\n    rendered = template.render(Context(context))\n    self.assertEqual(rendered, '0')\n", "def test_get_admin_log_template_tag(self):\n    \"\"\"\n    The get_admin_log template tag returns a list of admin log entries for the\n    given user.\n    \"\"\"\n    user_id = self.user.pk\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        user_id,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something\",\n    )\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n        \"user\": self.user,\n    }\n    template = template.Template(\"{% load admin_utils %}{% get_admin_log 10 as admin_log for_user user %}\")\n    rendered = template.render(context)\n    self.assertEqual(len(context[\"admin_log\"]), 1)\n", "def test_get_admin_log_template_tag(self):\n    \"\"\"\n    The get_admin_log template tag returns a list of LogEntry objects for the\n    given criteria.\n    \"\"\"\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something else\",\n    )\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n        \"user_id\": self.user.pk,\n    }\n    template = template.Template(\n        \"{% load admin_log %}{% get_admin_log 10 as admin_log for_user user_id %}\"\n    )\n    output = template.render(context)\n    self.assertEqual(output, \"\")\n    self.assertIn(\"admin_log\", context)\n    self.assertEqual(len(context[\"admin_log\"]), 2)\n"], "sample_451": ["def test_replace_named_groups(self):\n    pattern = r\"^(?P<a>\\w+)/b/(?P<c>\\w+)/$\"\n    expected_output = \"^<a>/b/<c>/$\"\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n\n    pattern = r\"^(?P<a>\\w+)/b/(\\w+)$\"\n    expected_output = \"^<a>/b/(\\\\w+)$\"\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_named_groups(self):\n    pattern = r\"^(?P<a>\\w+)/b/(?P<c>\\w+)$\"\n    expected_output = \"^<a>/b/<c>$\"\n    self.assertEqual(replace_named_groups(pattern), expected_output)\n", "def test_replace_metacharacters(self):\n    pattern = \"^Hello, world\\\\?\"\n    expected_output = \"^Hello, world?\"\n    self.assertEqual(replace_metacharacters(pattern), expected_output)\n\n    pattern = \"\\\\^Hello, world\\\\$\"\n    expected_output = \"^Hello, world$\"\n    self.assertEqual(replace_metacharacters(pattern), expected_output)\n", "def test_get_view_name(self):\n    from django.views.generic import View\n\n    class TestView(View):\n        pass\n\n    view_func = TestView.as_view()\n    self.assertEqual(get_view_name(view_func), \"django.views.generic.View\")\n\n        pass\n\n    self.assertEqual(get_view_name(test_function_view), f\"{__name__}.test_function_view\")\n", "def test_replace_metacharacters(self):\n    pattern = \"^Hello, world?$\"\n    expected_output = \"^Hello, world$\"\n    self.assertEqual(replace_metacharacters(pattern), expected_output)\n\n    pattern = \"^Hello, world\\\\?$\"\n    expected_output = \"^Hello, world\\\\?$\"\n    self.assertEqual(replace_metacharacters(pattern), expected_output)\n\n    pattern = \"^Hello, world*?$\"\n    expected_output = \"^Hello, world*$\"\n    self.assertEqual(replace_metacharacters(pattern), expected_output)\n"], "sample_453": ["def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should pass whole context and \n    generate JSON for prepopulated fields.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    extra_context = {\"extra\": True}\n    response = admin.change_view(\n        request, str(self.superuser.pk), extra_context=extra_context\n    )\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIs(template_context[\"extra\"], True)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n", "def test_cell_count(self):\n    \"\"\"\n    The cell_count filter returns the correct number of cells for a tabular inline.\n    \"\"\"\n    modeladmin = ModelAdmin(Article, site)\n    inline_admin_formset = modeladmin.get_inline_formsets(None, Article.objects.all()[0])[0]\n    inline_admin_form = inline_admin_formset.forms[0]\n    self.assertEqual(cell_count(inline_admin_form), 3)  # 1 hidden cell + 2 visible fields\n\n    # Add a visible field to the inline form\n    class NewInlineAdminForm(inline_admin_form.__class__):\n            super().__init__(*args, **kwargs)\n            self.fields['new_field'] = 'New Field'\n\n    new_inline_admin_form = NewInlineAdminForm(instance=Article(), prefix='new')\n    self.assertEqual(cell_count(new_inline_admin_form), 4)  # 1 hidden cell + 3 visible fields\n\n    # Make one of the fields hidden\n    new_inline_admin_form.fields['new_field'].is_hidden = True\n    self.assertEqual(cell_count(new_inline_admin_form), 3)  # 1 hidden cell + 2 visible fields\n\n    # Test with can_delete\n    inline_admin_formset.can_delete = True\n    self.assertEqual(cell_count(inline_admin_form), 4)  # 1 hidden cell + 2 visible fields + 1 delete checkbox\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should include all prepopulated fields from \n    the admin form and inlines.\n    \"\"\"\n    article = Article.objects.all()[0]\n    request = self.request_factory.get(\n        reverse(\"admin:admin_views_article_change\", args=[article.pk])\n    )\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.change_view(request, str(article.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n\n    # Test that prepopulated fields from inlines are included\n    inline_admin_formsets = template_context.get(\"inline_admin_formsets\")\n    if inline_admin_formsets:\n        for inline_admin_formset in inline_admin_formsets:\n            for inline_admin_form in inline_admin_formset:\n                if inline_admin_form.original is None:\n                    self.assertIn(\n                        inline_admin_form.prepopulated_fields,\n                        template_context[\"prepopulated_fields\"],\n                    )\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should return context with \n    prepopulated fields for admin form and inlines.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn(\"prepopulated_fields\", template_context)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    self.assertIsInstance(template_context[\"prepopulated_fields\"], list)\n    self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should create a list of\n    prepopulated fields that should render JavaScript for the prepopulated\n    fields for both the admin form and inlines.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn(\"prepopulated_fields\", template_context)\n    self.assertIn(\"prepopulated_fields_json\", template_context)\n    self.assertIsInstance(template_context[\"prepopulated_fields\"], list)\n    self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n"], "sample_455": ["def test_validate_expression_with_exclude(self):\n    constraint = models.UniqueConstraint(Lower(\"name\"), name=\"name_lower_uniq\")\n    instance = UniqueConstraintProduct(name=self.p1.name.upper())\n    # Exclude a field used by the expression.\n    constraint.validate(UniqueConstraintProduct, instance, exclude={\"name\"})\n    # Exclude a field not used by the expression.\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, instance, exclude={\"color\"})\n", "def test_clone(self):\n    constraint = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        condition=models.Q(foo=models.F(\"bar\")),\n        deferrable=models.Deferrable.IMMEDIATE,\n        include=[\"baz_1\", \"baz_2\"],\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n        violation_error_message=\"Custom message\",\n        violation_error_code=\"custom_code\",\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n", "def test_clone(self):\n    constraint = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique\",\n        condition=models.Q(foo=models.F(\"bar\")),\n        deferrable=models.Deferrable.IMMEDIATE,\n        include=[\"baz_1\", \"baz_2\"],\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n        violation_error_message=\"Custom error message\",\n        violation_error_code=\"custom_error_code\",\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertIsNot(constraint, cloned_constraint)\n", "def test_unique_constraint_with_multiple_expressions(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        F(\"color\"),\n        name=\"name_color_uniq\",\n    )\n    # Create a product with a unique name and color.\n    UniqueConstraintProduct.objects.create(name=\"p1\", color=\"red\")\n    # Attempt to create another product with the same name and color.\n    msg = \"Constraint \u201cname_color_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=\"P1\", color=\"red\"),\n        )\n    # Attempt to create another product with the same name but different color.\n    constraint.validate(\n        UniqueConstraintProduct,\n        UniqueConstraintProduct(name=\"P1\", color=\"blue\"),\n    )\n", "def test_validate_with_excluded_fields(self):\n    constraint = models.UniqueConstraint(fields=[\"name\", \"color\"], name=\"unique\")\n    product = UniqueConstraintProduct(name=self.p1.name, color=self.p1.color)\n    constraint.validate(UniqueConstraintProduct, product, exclude={\"name\"})\n    constraint.validate(UniqueConstraintProduct, product, exclude={\"color\"})\n    constraint.validate(UniqueConstraintProduct, product, exclude={\"name\", \"color\"})\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, product)\n"], "sample_456": ["def test_formset_add_fields_overrides_fields(self):\n    \"\"\"\n    BaseFormSet.add_fields() overrides existing fields when a new field with the same name is provided.\n    \"\"\"\n    class CustomBaseFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields[\"choice\"] = IntegerField()\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomBaseFormSet)\n    formset = ChoiceFormSet()\n    self.assertIsInstance(formset.forms[0].fields[\"choice\"], IntegerField)\n", "def test_formset_media_with_empty_media(self):\n    \"\"\"Media is available on formset even if one of the forms has empty media.\"\"\"\n\n    class MediaForm(Form):\n        class Media:\n            js = (\"some-file.js\",)\n\n    class EmptyMediaForm(Form):\n        class Media:\n            pass\n\n    self.assertIn(\n        \"some-file.js\",\n        str(formset_factory(MediaForm, extra=1, formset=formset_factory(EmptyMediaForm)).media),\n    )\n", "def test_all_valid_short_circuits(self):\n    class AlwaysValidFormSet(BaseFormSet):\n            return True\n\n    class AlwaysInvalidFormSet(BaseFormSet):\n            return False\n\n    valid_formset = AlwaysValidFormSet()\n    invalid_formset = AlwaysInvalidFormSet()\n\n    # all_valid short-circuits and doesn't validate the second formset.\n    self.assertFalse(all_valid([invalid_formset, valid_formset]))\n    # all_valid checks the second formset if the first one is valid.\n    self.assertFalse(all_valid([valid_formset, invalid_formset]))\n", "def test_formset_template_name(self):\n    \"\"\"\n    Test that the template name for a formset is correctly set.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"django/forms/formsets/default.html\")\n\n    class CustomTemplateFormSet(BaseFormSet):\n        template_name = \"custom/formset.html\"\n\n    CustomChoiceFormSet = formset_factory(Choice, formset=CustomTemplateFormSet)\n    custom_formset = CustomChoiceFormSet()\n    self.assertEqual(custom_formset.template_name, \"custom/formset.html\")\n", "def test_formset_add_fields_with_empty_forms(self):\n    \"\"\"add_fields() is called for each form, even if the form has no data.\"\"\"\n    class CheckForm(Form):\n        field = IntegerField()\n\n    class BaseCheckFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            # Add a flag to the form to check that it was called.\n            form.add_fields_called = True\n\n    CheckFormSet = formset_factory(CheckForm, formset=BaseCheckFormSet, extra=2)\n    formset = CheckFormSet()\n    for form in formset.forms:\n        self.assertTrue(hasattr(form, \"add_fields_called\"))\n"], "sample_457": ["def test_contains_expressions(self):\n    constraint = models.UniqueConstraint(fields=[\"foo\", \"bar\"], name=\"unique\")\n    self.assertIs(constraint.contains_expressions, False)\n\n    constraint = models.UniqueConstraint(Lower(\"title\"), F(\"author\"), name=\"book_func_uq\")\n    self.assertIs(constraint.contains_expressions, True)\n", "def test_clone_with_expressions(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        F(\"author\"),\n        name=\"book_func_uq\",\n        violation_error_message=\"custom error message\",\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint.name, cloned_constraint.name)\n    self.assertEqual(constraint.expressions, cloned_constraint.expressions)\n    self.assertEqual(\n        constraint.violation_error_message,\n        cloned_constraint.violation_error_message,\n    )\n", "def test_contains_expressions(self):\n    constraint = UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n    self.assertIs(constraint.contains_expressions, False)\n    constraint = UniqueConstraint(Lower(\"name\"), name=\"unique_name\")\n    self.assertIs(constraint.contains_expressions, True)\n", "    def test_clone(self):\n        constraint = UniqueConstraint(\n            fields=[\"foo\", \"bar\"],\n            name=\"unique_fields\",\n            condition=models.Q(foo=models.F(\"bar\")),\n            deferrable=models.Deferrable.IMMEDIATE,\n            include=[\"baz_1\", \"baz_2\"],\n            opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n        )\n        cloned_constraint = constraint.clone()\n        self.assertEqual(constraint, cloned_constraint)\n        self.assertIsNot(constraint, cloned_constraint)\n", "def test_validate_with_exclude_for_expression(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_uniq\",\n    )\n    instance = UniqueConstraintProduct(name=self.p1.name.upper())\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, instance)\n    constraint.validate(\n        UniqueConstraintProduct,\n        instance,\n        exclude={\"name\"},\n    )\n    constraint.validate(\n        UniqueConstraintProduct,\n        instance,\n        exclude={\"non_unique_field\"},\n    )\n    with self.assertRaises(ValidationError):\n        constraint.validate(\n            UniqueConstraintProduct,\n            instance,\n            exclude={\"other_non_unique_field\"},\n        )\n"], "sample_458": ["    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 1.23})\n        self.assertEqual(output, \"1.230000e+00\")\n", "    def test_add_filter(self):\n        self.assertEqual(add(5, 3), 8)\n        self.assertEqual(add(-5, 3), -2)\n        self.assertEqual(add(\"5\", 3), \"53\")\n        self.assertEqual(add(\"a\", 3), \"a3\")\n        self.assertEqual(add(None, 3), \"\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 1.23})\n        self.assertEqual(output, \"1.230000e+00\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5})\n        self.assertEqual(output, \"10\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 1.23})\n        self.assertEqual(output, \"1.230000e+00\")\n"], "sample_459": ["    def test_lookup_class(self):\n        instance = IntegerModel.objects.create(value=1)\n        self.assertEqual(IntegerModel.objects.get(value__exact=1), instance)\n        self.assertEqual(IntegerModel.objects.get(value__gt=0), instance)\n        self.assertEqual(IntegerModel.objects.get(value__gte=1), instance)\n        self.assertEqual(IntegerModel.objects.get(value__lt=2), instance)\n        self.assertEqual(IntegerModel.objects.get(value__lte=1), instance)\n", "    def test_uuidfield_cleans_valid_string(self):\n        f = models.UUIDField()\n        valid_uuid = \"123e4567-e89b-12d3-a456-426655440000\"\n        self.assertEqual(f.clean(valid_uuid, None), uuid.UUID(valid_uuid))\n", "    def test_in_lookup_integer_field(self):\n        IntegerModel.objects.create(value=1)\n        IntegerModel.objects.create(value=2)\n        IntegerModel.objects.create(value=3)\n\n        self.assertEqual(IntegerModel.objects.filter(value__in=[1, 2]).count(), 2)\n        self.assertEqual(IntegerModel.objects.filter(value__in=(1, 2)).count(), 2)\n        self.assertEqual(IntegerModel.objects.filter(value__in={1, 2}).count(), 2)\n        self.assertEqual(IntegerModel.objects.filter(value__in=[1, 'a']).count(), 1)\n", "    def test_boolean_field_lookup(self):\n        # Create a model with a BooleanField\n        from django.db import models\n\n        class BooleanModel(models.Model):\n            value = models.BooleanField()\n\n        # Create an instance of the model\n        BooleanModel.objects.create(value=True)\n\n        # Use the exact lookup to filter the model\n        qs = BooleanModel.objects.filter(value__exact=True)\n\n        # Check that the lookup returns the correct results\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs[0].value, True)\n\n        # Check that the lookup works with False values\n        BooleanModel.objects.create(value=False)\n        qs = BooleanModel.objects.filter(value__exact=False)\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs[0].value, False)\n\n        # Check that the lookup raises an exception for non-boolean values\n        with self.assertRaises(ValueError):\n            BooleanModel.objects.filter(value__exact=\"not a boolean\")\n", "    def test_uuidfield_cleans_valid_string(self):\n        f = models.UUIDField()\n        self.assertEqual(f.clean(\"123e4567-e89b-12d3-a456-426655440000\", None), uuid.UUID(\"123e4567-e89b-12d3-a456-426655440000\"))\n"], "sample_461": ["def test_urlfield_disabled(self):\n    f = URLField(disabled=True)\n    self.assertIsNone(f.clean(None))\n    self.assertEqual(f.clean(\"http://example.com\"), \"http://example.com\")\n    self.assertEqual(f.clean(\"\"), \"\")\n    self.assertEqual(f.clean(\"invalid url\"), \"invalid url\")\n", "def test_urlfield_validators(self):\n    f = URLField(validators=[lambda x: None])\n    self.assertEqual(len(f.validators), 2)\n    f = URLField(validators=[lambda x: None], max_length=20, min_length=10)\n    self.assertEqual(len(f.validators), 4)\n", "def test_urlfield_localization(self):\n    f = URLField(localize=True)\n    self.assertWidgetRendersTo(f, '<input type=\"url\" name=\"f\" id=\"id_f\" required>')\n", "def test_urlfield_disabled(self):\n    f = URLField(disabled=True)\n    self.assertEqual(f.clean(\"http://example.com\"), \"http://example.com\")\n    self.assertEqual(f.clean(\"\"), \"\")\n    self.assertEqual(f.clean(None), \"\")\n    msg = \"'This field is required.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"invalid\")\n", "def test_urlfield_max_length(self):\n    f = URLField(max_length=20)\n    msg = \"'Ensure this value has at most 20 characters (it has 37).'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"http://abcdefghijklmnopqrstuvwxyz.com\")\n"], "sample_462": ["def test_choicefield_iterable_choices(self):\n    choices = [(\"J\", \"John\"), (\"P\", \"Paul\")]\n    f = ChoiceField(choices=iter(choices))\n    self.assertEqual(\"J\", f.clean(\"J\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_label_from_model_field(self):\n    class Model(models.Model):\n        choice = models.CharField(max_length=1, choices=[(\"J\", \"John\"), (\"P\", \"Paul\")])\n\n    f = ChoiceField(choices=Model._meta.get_field(\"choice\").choices)\n    self.assertEqual(\"J\", f.clean(\"J\"))\n    msg = \"'Select a valid choice. X is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"X\")\n", "def test_choicefield_iterable_choices(self):\n    choices = [(\"J\", \"John\"), (\"P\", \"Paul\")]\n    f = ChoiceField(choices=iter(choices))\n    self.assertEqual(\"J\", f.clean(\"J\"))\n    msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        f.clean(\"3\")\n", "def test_choicefield_valid_value(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n    self.assertTrue(f.valid_value(\"1\"))\n    self.assertTrue(f.valid_value(1))\n    self.assertFalse(f.valid_value(\"3\"))\n    self.assertFalse(f.valid_value(None))\n", "def test_choicefield_widget_choices(self):\n    f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n    self.assertEqual(f.widget.choices, [(\"1\", \"One\"), (\"2\", \"Two\")])\n    f.widget.choices = [(\"3\", \"Three\"), (\"4\", \"Four\")]\n    self.assertEqual(f.widget.choices, [(\"3\", \"Three\"), (\"4\", \"Four\")])\n    self.assertEqual(f.choices, [(\"3\", \"Three\"), (\"4\", \"Four\")])\n"], "sample_464": ["def test_file_response_with_async_iterator(self):\n    async def async_iterator():\n        yield b\"Hello, \"\n        yield b\"world!\"\n\n    response = FileResponse(async_iterator())\n    self.assertEqual(list(response), [b\"Hello, world!\"])\n", "def test_response_content_type_with_charset(self):\n    response = FileResponse(io.BytesIO(b\"binary content\"), content_type=\"text/plain; charset=utf-8\")\n    self.assertEqual(response.headers[\"Content-Type\"], \"text/plain; charset=utf-8\")\n", "def test_file_response_with_async_iterator(self):\n    async def async_generator():\n        yield b\"binary\"\n        yield b\"content\"\n\n    response = FileResponse(async_generator())\n    self.assertEqual(list(response), [b\"binary\", b\"content\"])\n", "def test_close_called_on_buffer(self):\n    class TestBuffer(io.BytesIO):\n            self.closed = True\n\n    buffer = TestBuffer(b\"binary content\")\n    response = FileResponse(buffer)\n    response.close()\n    self.assertTrue(buffer.closed)\n", "def test_file_response_close_called(self):\n    class TestFile:\n            self._file = open(path, *args, **kwargs)\n            self.close_called = False\n\n            return self._file.read(n_bytes)\n\n            return self._file.seek(offset, whence)\n\n            self.close_called = True\n            self._file.close()\n\n    file = TestFile(__file__, \"rb\")\n    response = FileResponse(file)\n    response.close()\n    self.assertTrue(file.close_called)\n"], "sample_465": ["def test_has_module_permission(self):\n    \"\"\"\n    has_module_permission returns True for users who can view the module and\n    False for users who can't.\n    \"\"\"\n    ma = ModelAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockViewUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockAddUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockChangeUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockDeleteUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockUser()\n    self.assertFalse(ma.has_module_permission(request))\n", "def test_has_module_permission(self):\n    \"\"\"\n    has_module_permission returns True for users who have any permission on the\n    module and False for users who don't.\n    \"\"\"\n    ma = ModelAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockViewUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockAddUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockChangeUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockDeleteUser()\n    self.assertTrue(ma.has_module_permission(request))\n\n    class MockNoPermissionsUser(self.MockUser):\n            return False\n\n    request.user = self.MockNoPermissionsUser()\n    self.assertFalse(ma.has_module_permission(request))\n", "def test_get_inline_formsets_with_no_fields(self):\n    \"\"\"\n    get_inline_formsets() returns an empty list when there are no fields to display.\n    \"\"\"\n\n    class EmptyInline(TabularInline):\n        model = Song\n        fields = []\n\n    class BandAdmin(ModelAdmin):\n        inlines = [EmptyInline]\n\n    ma = BandAdmin(Band, self.site)\n    inline_admin_formsets = ma.get_inline_formsets(\n        request, formsets=[EmptyInline(Band, self.site).get_formset(request)], obj=None\n    )\n    self.assertEqual(inline_admin_formsets, [])\n", "def test_has_module_permission(self):\n    \"\"\"\n    has_module_permission returns True for users who have any permission on\n    the module and False for users who don't.\n    \"\"\"\n    ma = ModelAdmin(Band, AdminSite())\n    request = MockRequest()\n    request.user = self.MockViewUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockAddUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockChangeUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockDeleteUser()\n    self.assertTrue(ma.has_module_permission(request))\n    request.user = self.MockUser()\n    self.assertFalse(ma.has_module_permission(request))\n", "def test_get_inline_formsets(self):\n    class ConcertInline(TabularInline):\n        model = Concert\n        fk_name = \"main_band\"\n\n    class BandAdmin(ModelAdmin):\n        inlines = [ConcertInline]\n\n    ma = BandAdmin(Band, self.site)\n    formsets, inline_instances = ma.get_inline_formsets(request)\n    self.assertEqual(len(formsets), 1)\n    self.assertIsInstance(formsets[0], forms.BaseInlineFormSet)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(inline_instances[0], ConcertInline)\n"], "sample_466": ["def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    MigrationWriter.unregister_serializer(complex)\n\n    with self.assertRaisesMessage(\n        ValueError,\n        \"Cannot serialize: complex(1+2j)\\nThere is no serialization handler declared for complex\",\n    ):\n        MigrationWriter.serialize(1 + 2j)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    MigrationWriter.unregister_serializer(complex)\n\n    with self.assertRaisesMessage(\n        ValueError,\n        \"Cannot serialize: complex(1+1j)\\nThere is no serialization \"\n        \"function available for builtins.complex.\",\n    ):\n        MigrationWriter.serialize(complex(1 + 1j))\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    MigrationWriter.unregister_serializer(complex)\n\n    with self.assertRaisesMessage(\n        ValueError, \"No serializer found for type complex\"\n    ):\n        MigrationWriter.serialize(complex(1 + 2j))\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    self.assertIn(complex, MigrationWriter.custom_serializers)\n\n    MigrationWriter.unregister_serializer(complex)\n    self.assertNotIn(complex, MigrationWriter.custom_serializers)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    self.assertIn(complex, MigrationWriter.serializer_factory._registry)\n\n    MigrationWriter.unregister_serializer(complex)\n    self.assertNotIn(complex, MigrationWriter.serializer_factory._registry)\n"], "sample_467": ["def test_invalid_years(self):\n    with self.assertRaisesMessage(ValueError, \"Invalid year string\"):\n        SelectDateWidget(years=(\"abc\",))\n", "def test_value_from_datadict_invalid_day(self):\n    data = {\"field_year\": \"2000\", \"field_month\": \"2\", \"field_day\": \"31\"}\n    self.assertEqual(\n        self.widget.value_from_datadict(data, {}, \"field\"), \"2000-02-31\"\n    )\n    self.assertEqual(\n        self.widget.value_from_datadict({\"field_year\": \"2000\"}, {}, \"field\"), None\n    )\n    self.assertEqual(\n        self.widget.value_from_datadict({\"field_month\": \"12\"}, {}, \"field\"), None\n    )\n    self.assertEqual(\n        self.widget.value_from_datadict({\"field_day\": \"1\"}, {}, \"field\"), None\n    )\n", "def test_use_required_attribute(self):\n    widget = SelectDateWidget()\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertTrue(widget.use_required_attribute(\"\"))\n    self.assertFalse(widget.use_required_attribute(date(2007, 1, 1)))\n", "def test_value_from_datadict_invalid_date(self):\n    data = {\"field_day\": \"31\", \"field_month\": \"2\", \"field_year\": \"2010\"}\n    self.assertEqual(\n        self.widget.value_from_datadict(data, {}, \"field\"), \"2010-02-31\"\n    )\n", "def test_use_required_attribute(self):\n    widget = SelectDateWidget()\n    self.assertIs(widget.use_required_attribute(None), True)\n    self.assertIs(widget.use_required_attribute(date(2010, 1, 1)), True)\n\n    widget = SelectDateWidget(required=False)\n    self.assertIs(widget.use_required_attribute(None), False)\n    self.assertIs(widget.use_required_attribute(date(2010, 1, 1)), False)\n\n    widget = SelectDateWidget()\n    with override_settings(DATE_FORMAT=\"d-m-Y\"):\n        self.assertIs(widget.use_required_attribute(None), True)\n        self.assertIs(widget.use_required_attribute(date(2010, 1, 1)), True)\n"], "sample_469": ["def test_alias_with_custom_database_function(self):\n    class CustomFunc(Func):\n        function = \"COALESCE\"\n        arity = 2\n\n    qs = Book.objects.alias(\n        coalesced_name=CustomFunc(F(\"name\"), Value(\"default\")),\n    ).annotate(name=F(\"coalesced_name\"))\n    self.assertIs(hasattr(qs.first(), \"coalesced_name\"), False)\n    for book in qs:\n        with self.subTest(book=book):\n            self.assertEqual(book.name, book.name or \"default\")\n", "def test_combine_queries_with_annotations(self):\n    books = Book.objects.annotate(is_book=Value(1))\n    combined_books = books & books\n    for book in combined_books:\n        self.assertEqual(book.is_book, 1)\n", "def test_subquery_alias(self):\n    subquery = Book.objects.filter(pages__gt=400).values(\"publisher\")\n    qs = Publisher.objects.alias(\n        has_long_books=Exists(subquery.filter(publisher=OuterRef(\"pk\")))\n    ).filter(has_long_books=True)\n    self.assertCountEqual(qs, [self.p1, self.p2, self.p3, self.p4])\n", "def test_alias_ref_alias(self):\n    qs = Book.objects.alias(\n        pages_alias=F(\"pages\"),\n    ).alias(pages_ref_alias=F(\"pages_alias\"))\n    msg = \"Cannot resolve keyword 'pages_alias' into field.\"\n    with self.assertRaisesMessage(FieldError, msg):\n        qs.annotate(pages_ref=F(\"pages_ref_alias\")).first()\n", "def test_alias_expression_with_m2m(self):\n    qs = Author.objects.alias(\n        num_friends=Count(\"friends\"),\n    ).filter(num_friends__gt=1)\n    self.assertIs(hasattr(qs.first(), \"num_friends\"), False)\n    self.assertSequenceEqual(qs, [self.a5, self.a7])\n"], "sample_468": ["def test_make_context(self):\n    context = {\"a\": 1}\n    request = self.request_factory.get(\"/\")\n    ctx = make_context(context, request)\n    self.assertIsInstance(ctx, RequestContext)\n    self.assertEqual(ctx[\"a\"], 1)\n\n    ctx = make_context(context)\n    self.assertIsInstance(ctx, Context)\n    self.assertEqual(ctx[\"a\"], 1)\n\n    with self.assertRaises(TypeError):\n        make_context(\"not a dict\")\n", "def test_make_context(self):\n    context = make_context({\"a\": 1})\n    self.assertIsInstance(context, Context)\n    self.assertEqual(context[\"a\"], 1)\n\n    request = self.request_factory.get(\"/\")\n    context = make_context({\"a\": 1}, request)\n    self.assertIsInstance(context, RequestContext)\n    self.assertEqual(context[\"a\"], 1)\n    self.assertEqual(context.request, request)\n\n    with self.assertRaises(TypeError):\n        make_context(\"not a dict\")\n\n    context = make_context({\"a\": 1}, use_l10n=True, use_tz=False, autoescape=False)\n    self.assertTrue(context.use_l10n)\n    self.assertFalse(context.use_tz)\n    self.assertFalse(context.autoescape)\n", "def test_bind_template_twice(self):\n    request = self.request_factory.get(\"/\")\n    ctx = RequestContext(request, {})\n    template1 = Template(\"{{ foo }}\")\n    template2 = Template(\"{{ bar }}\")\n\n    with ctx.bind_template(template1):\n        self.assertEqual(ctx.template, template1)\n        with self.assertRaises(RuntimeError):\n            with ctx.bind_template(template2):\n                pass\n", "def test_make_context(self):\n    context = make_context({\"a\": 1})\n    self.assertIsInstance(context, Context)\n    self.assertEqual(context[\"a\"], 1)\n\n    request = self.request_factory.get(\"/\")\n    context = make_context({\"a\": 1}, request)\n    self.assertIsInstance(context, RequestContext)\n    self.assertEqual(context[\"a\"], 1)\n    self.assertEqual(context.request, request)\n\n    with self.assertRaises(TypeError):\n        make_context(\"not a dict\")\n\n    context = make_context(None)\n    self.assertIsInstance(context, Context)\n    self.assertEqual(context.dicts, [{\"False\": False, \"None\": None, \"True\": True}])\n", "def test_render_context_push_state(self):\n    \"\"\"\n    Test RenderContext.push_state() to ensure it correctly pushes and pops state.\n    \"\"\"\n    render_context = RenderContext({\"a\": 1})\n    template = Template(\"\")\n\n    with render_context.push_state(template, isolated_context=True):\n        self.assertEqual(render_context.template, template)\n        self.assertEqual(len(render_context.dicts), 2)\n\n    self.assertIsNone(render_context.template)\n    self.assertEqual(len(render_context.dicts), 1)\n\n    with render_context.push_state(template, isolated_context=False):\n        self.assertEqual(render_context.template, template)\n        self.assertEqual(len(render_context.dicts), 1)\n\n    self.assertIsNone(render_context.template)\n    self.assertEqual(len(render_context.dicts), 1)\n"], "sample_470": ["def test_classproperty_get(self):\n    class Foo:\n        foo_attr = 123\n\n            self.foo_attr = 456\n\n        @classproperty\n            return cls.foo_attr\n\n    self.assertEqual(Foo.foo, 123)\n    self.assertEqual(Foo().foo, 123)\n\n    Foo.foo_attr = 789\n    self.assertEqual(Foo.foo, 789)\n    self.assertEqual(Foo().foo, 789)\n", "def test_classproperty_getter_on_instance(self):\n    class Foo:\n        foo_attr = 123\n\n        @classproperty\n            return cls.foo_attr\n\n    foo = Foo()\n    self.assertEqual(foo.foo, 123)\n", "def test_keep_lazy(self):\n    counter = 0\n\n    @keep_lazy(int)\n        nonlocal counter\n        counter += 1\n        return a + b\n\n    lazy_4 = lazy(lambda: 4, int)\n    lazy_5 = lazy(lambda: 5, int)\n\n    self.assertEqual(lazy_add(lazy_4(), lazy_5()), 9)\n    self.assertEqual(counter, 1)\n\n    self.assertEqual(lazy_add(4, 5), 9)\n    self.assertEqual(counter, 2)\n\n    self.assertIsInstance(lazy_add(lazy_4(), lazy_5()), Promise)\n    self.assertEqual(counter, 2)\n", "def test_classproperty_setter(self):\n    class Foo:\n        foo_attr = 123\n\n        @classproperty\n            return cls.foo_attr\n\n        @foo.setter\n            cls.foo_attr = value\n\n    Foo.foo = 456\n    self.assertEqual(Foo.foo, 456)\n    self.assertEqual(Foo.foo_attr, 456)\n", "def test_keep_lazy(self):\n    counter = 0\n\n        nonlocal counter\n        counter += 1\n        return \"result\"\n\n    lazy_func = lazy(func, str)\n    wrapped_func = keep_lazy(str)(lambda x: x)(lazy_func())\n    self.assertEqual(counter, 0)\n    self.assertEqual(wrapped_func, \"result\")\n    self.assertEqual(counter, 1)\n"], "sample_471": ["def test_integerfield_widget_attrs(self):\n    f = IntegerField(max_length=10, min_value=5)\n    widget = f.widget\n    self.assertEqual(widget.attrs[\"maxlength\"], 10)\n    self.assertEqual(widget.attrs[\"min\"], 5)\n\n    f = IntegerField(max_length=None, min_value=None)\n    widget = f.widget\n    self.assertNotIn(\"maxlength\", widget.attrs)\n    self.assertNotIn(\"min\", widget.attrs)\n\n    f = IntegerField(max_length=10, min_value=5, step_size=2)\n    widget = f.widget\n    self.assertEqual(widget.attrs[\"maxlength\"], 10)\n    self.assertEqual(widget.attrs[\"min\"], 5)\n    self.assertEqual(widget.attrs[\"step\"], 2)\n", "def test_integerfield_widget_instance(self):\n    widget = Textarea()\n    f = IntegerField(widget=widget)\n    self.assertWidgetRendersTo(\n        f, '<textarea name=\"f\" cols=\"40\" rows=\"10\" id=\"id_f\" required></textarea>'\n    )\n", "def test_integerfield_widget_instance(self):\n    f = IntegerField(widget=Textarea)\n    self.assertWidgetRendersTo(\n        f, '<textarea name=\"f\" cols=\"40\" rows=\"10\" required id=\"id_f\">\\r\\n</textarea>'\n    )\n    with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n        f.clean(\"\")\n    self.assertEqual(1, f.clean(\"1\"))\n", "def test_integerfield_widget_attrs(self):\n    f = IntegerField(max_value=10, min_value=5)\n    widget = f.widget\n    self.assertIsInstance(widget, type(f.widget))\n    attrs = f.widget_attrs(widget)\n    self.assertEqual(attrs, {\"min\": 5, \"max\": 10})\n    f = IntegerField()\n    attrs = f.widget_attrs(widget)\n    self.assertEqual(attrs, {})\n    f = IntegerField(max_value=10)\n    attrs = f.widget_attrs(widget)\n    self.assertEqual(attrs, {\"max\": 10})\n    f = IntegerField(min_value=5)\n    attrs = f.widget_attrs(widget)\n    self.assertEqual(attrs, {\"min\": 5})\n", "def test_integerfield_widget(self):\n    f = IntegerField(widget=Textarea)\n    self.assertWidgetRendersTo(\n        f, '<textarea cols=\"40\" id=\"id_f\" name=\"f\" required></textarea>'\n    )\n"], "sample_472": ["def test_page_representation(self):\n    paginator = Paginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertEqual(repr(page), \"<Page 1 of 2>\")\n    self.assertEqual(str(page), \"<Page 1 of 2>\")\n", "def test_paginating_queryset_with_select_related(self):\n    paginator = Paginator(Article.objects.select_related(\"author\").order_by(\"id\"), 5)\n    p = paginator.page(1)\n    self.assertEqual(\"<Page 1 of 2>\", str(p))\n    self.assertSequenceEqual(p.object_list, self.articles[:5])\n    self.assertTrue(p.has_next())\n    self.assertFalse(p.has_previous())\n    self.assertTrue(p.has_other_pages())\n    self.assertEqual(2, p.next_page_number())\n    with self.assertRaises(InvalidPage):\n        p.previous_page_number()\n    self.assertEqual(1, p.start_index())\n    self.assertEqual(5, p.end_index())\n", "def test_paginating_queryset_with_zero_orphans(self):\n    paginator = Paginator(Article.objects.order_by(\"id\"), 5, orphans=0)\n    self.assertEqual(paginator.num_pages, 2)\n    self.assertEqual(paginator.page_range, range(1, 3))\n", "def test_paginating_empty_list_does_not_warn(self):\n    with warnings.catch_warnings(record=True) as recorded:\n        Paginator([], 5)\n    self.assertEqual(len(recorded), 0)\n", "def test_paginating_queryset_with_custom_count_method(self):\n    class CustomCountQuerySet(models.QuerySet):\n            # Simulate a custom count method that returns a different count\n            return super().count() + 1\n\n    queryset = CustomCountQuerySet(model=Article).all()\n    paginator = Paginator(queryset, 5)\n    self.assertEqual(paginator.count, len(self.articles) + 1)\n"], "sample_473": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_474": ["    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.author2 = Author.objects.create(name=\"\u00c9lena\", alias=\"elena\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena\", alias=\"elena\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n", "    def setUpTestData(cls):\n        cls.author = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n"], "sample_475": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_actions_unique_with_custom_name(self):\n        @admin.action(name=\"My Action\")\n            pass\n\n        @admin.action(name=\"My Action\")\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"__name__ attributes of actions defined in BandAdmin must be \"\n            \"unique. Name 'My Action' is not unique.\",\n            id=\"admin.E130\",\n        )\n", "    def test_readonly_fields_is_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            id=\"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_view_on_site_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"non-callable\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n"], "sample_476": ["    def test_serialization(self):\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1)\n        p.save()\n\n        # Serialize and deserialize the model instance.\n        serialized_p = serializers.serialize(\"json\", [p])\n        deserialized_objects = serializers.deserialize(\"json\", serialized_p)\n\n        # Check that the deserialized object has the correct dimensions.\n        for deserialized_p in deserialized_objects:\n            self.check_dimensions(deserialized_p.object, 4, 8)\n", "    def test_image_field_form(self):\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = Person\n                fields = (\"mugshot\",)\n\n        form = TestForm()\n        self.assertIsInstance(form.fields[\"mugshot\"], forms.ImageField)\n", "    def test_update_dimension_fields(self):\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1)\n        p.save()\n\n        # Dimensions should be set after saving.\n        self.check_dimensions(p, 4, 8)\n\n        # Update dimensions fields manually.\n        p.mugshot_width = 10\n        p.mugshot_height = 20\n\n        # Update dimension fields with force=True to override manual changes.\n        self.PersonModel.mugshot.update_dimension_fields(p, force=True)\n\n        # Dimensions should be reset to actual image dimensions.\n        self.check_dimensions(p, 4, 8)\n", "    def test_file_mode(self):\n        \"\"\"\n        Test that the file is opened in the correct mode.\n        \"\"\"\n        instance = Person()\n        field = instance._meta.get_field(\"mugshot\")\n        fieldfile = field.attr_class(instance, field, \"test.txt\")\n\n        with patch.object(field.storage, \"open\") as mock_open:\n            fieldfile.open(mode=\"wb\")\n            mock_open.assert_called_once_with(\"test.txt\", \"wb\")\n\n        with patch.object(field.storage, \"open\") as mock_open:\n            fieldfile.open(mode=\"rb\")\n            mock_open.assert_called_once_with(\"test.txt\", \"rb\")\n", "    def test_update_dimensions_on_save(self):\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1)\n        p.save()\n        self.check_dimensions(p, 4, 8)\n\n        # Update the image file\n        p.mugshot.save(\"mug\", self.file2)\n        self.check_dimensions(p, 8, 4)\n\n        # Check that the dimensions are updated in the database\n        p = self.PersonModel.objects.get(name=\"Joe\")\n        self.check_dimensions(p, 8, 4)\n"], "sample_477": ["def test_lower01(self):\n    output = self.engine.render_to_string(\"lower01\", {\"a\": \"HELLO\"})\n    self.assertEqual(output, \"hello\")\n", "def test_floatformat01(self):\n    output = self.engine.render_to_string(\"floatformat01\", {\"num\": 34.23234})\n    self.assertEqual(output, \"34.2\")\n", "def test_floatformat01(self):\n    output = self.engine.render_to_string(\"floatformat01\", {\"a\": 34.23234})\n    self.assertEqual(output, \"34.2\")\n", "def test_default01(self):\n    output = self.engine.render_to_string(\"default01\", {\"a\": \"val\"})\n    self.assertEqual(output, \"val\")\n\n    output = self.engine.render_to_string(\"default01\", {\"a\": \"\"})\n    self.assertEqual(output, \"(N/A)\")\n\n    output = self.engine.render_to_string(\"default01\", {})\n    self.assertEqual(output, \"(N/A)\")\n", "def test_add01(self):\n    output = self.engine.render_to_string(\"add01\", {\"a\": 1, \"b\": 2})\n    self.assertEqual(output, \"3\")\n"], "sample_478": ["    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_readonly_fields_is_none(self):\n        class Admin(ModelAdmin):\n            readonly_fields = None\n\n        self.assertIsValid(Admin, ValidationTestModel)\n", "    def test_view_on_site_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"not a callable or boolean\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            id=\"admin.E025\",\n        )\n", "    def test_actions_unique_with_default_action(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1,)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_not_boolean_or_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"hello\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n"], "sample_479": ["def test_rename_index_through_create_model(self):\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Pony\", [(\"weight\", models.IntegerField())]),\n            migrations.AddIndex(\"Pony\", models.Index(fields=[\"weight\"], name=\"old_name\")),\n            migrations.RenameIndex(\"Pony\", new_name=\"new_name\", old_name=\"old_name\"),\n        ],\n        [\n            migrations.CreateModel(\n                \"Pony\",\n                [(\"weight\", models.IntegerField())],\n                options={\"indexes\": [models.Index(fields=[\"weight\"], name=\"new_name\")]},\n            ),\n        ],\n    )\n", "def test_rename_index_same_name(self):\n    self.assertOptimizesTo(\n        [\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_fields=(\"weight\", \"pink\")\n            ),\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_fields=(\"weight\", \"pink\")\n            ),\n        ],\n        [\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_fields=(\"weight\", \"pink\")\n            ),\n        ],\n    )\n    self.assertOptimizesTo(\n        [\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_name=\"old_name\"\n            ),\n            migrations.RenameIndex(\n                \"Pony\", new_name=\"new_name\", old_name=\"old_name\"\n            ),\n        ],\n        [migrations.RenameIndex(\"Pony\", new_name=\"new_name\", old_name=\"old_name\")],\n    )\n", "def test_create_alter_table_comment(self):\n    \"\"\"\n    AlterModelTableComment should optimize into CreateModel.\n    \"\"\"\n    managers = [(\"objects\", EmptyManager())]\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[(\"name\", models.CharField(max_length=255))],\n                options={\"verbose_name\": \"Foo\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n            migrations.AlterModelTableComment(name=\"Foo\", table_comment=\"new comment\"),\n        ],\n        [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n                options={\"verbose_name\": \"Foo\", \"db_table_comment\": \"new comment\"},\n                bases=(UnicodeModel,),\n                managers=managers,\n            ),\n        ],\n    )\n", "def test_create_model_add_remove_field(self):\n    \"\"\"\n    RemoveField should cancel AddField even if there's a CreateModel in between\n    \"\"\"\n    self.assertOptimizesTo(\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n            migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n            migrations.RemoveField(\"Foo\", \"age\"),\n        ],\n        [\n            migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n            migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n        ],\n    )\n", "def test_add_constraint(self):\n    \"\"\"\n    AddConstraint should not optimize into CreateModel.\n    \"\"\"\n    self.assertDoesNotOptimize(\n        [\n            migrations.CreateModel(\n                \"Foo\", [(\"name\", models.CharField(max_length=255))]\n            ),\n            migrations.AddConstraint(\n                model_name=\"Foo\",\n                constraint=models.CheckConstraint(check=models.Q(name__length__gt=1)),\n            ),\n        ],\n    )\n"], "sample_480": ["    def test_migrate_to_text(self):\n        # Create a model with a JSONField.\n        model = models.Model(\"TestModel\", fields=[models.JSONField(name=\"value\")])\n        editor = connection.schema_editor(atomic=True)\n        editor.create_model(model)\n\n        # Migrate to TextField.\n        new_field = models.TextField(name=\"value\")\n        editor.alter_field(model, model._meta.get_field(\"value\"), new_field)\n\n        # Check that the data is preserved.\n        obj = model.objects.create(value={\"key\": \"value\"})\n        obj.refresh_from_db()\n        self.assertEqual(obj.value, '{\"key\": \"value\"}')\n\n        # Revert the migration.\n        editor.alter_field(model, new_field, model._meta.get_field(\"value\"))\n        obj.refresh_from_db()\n        self.assertEqual(obj.value, {\"key\": \"value\"})\n\n        # Clean up.\n        editor.delete_model(model)\n", "def test_key_transform_with_value_expression(self):\n    obj = NullableJSONModel.objects.create(value={\"a\": 1})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(\n            value__a=KeyTransform(\"a\", Value({\"a\": 1}, output_field=JSONField()))\n        ),\n        [obj],\n    )\n", "    def setUpTestData(cls):\n        cls.objs = [\n            JSONModel.objects.create(value={\"a\": 1, \"b\": 2}),\n            JSONModel.objects.create(value={\"a\": 3, \"b\": 4}),\n            JSONModel.objects.create(value={\"c\": 5, \"d\": 6}),\n        ]\n", "    def test_m2m_serialization(self):\n        related = RelatedJSONModel.objects.create(value={\"d\": [\"e\", \"f\"]})\n        obj = JSONModel.objects.create(value=[1, 2])\n        obj.related.add(related)\n        serialized_obj = serializers.serialize(\"json\", [obj])\n        self.assertIn(str(related.pk), serialized_obj)\n", "    def test_key_transform_text_lookup_mixin(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__foo__exact=\"bar\"),\n            [self.objs[7]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__baz__a__exact=\"b\"),\n            [self.objs[7]],\n        )\n"], "sample_481": ["    def test_pprint01(self):\n        output = self.engine.render_to_string(\"pprint01\", {\"data\": {\"foo\": \"bar\", \"baz\": [1, 2, 3]}})\n        expected_output = \"\"\"{'baz': [1, 2, 3], 'foo': 'bar'}\"\"\"\n        self.assertEqual(output, expected_output)\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 12345})\n        self.assertEqual(output, \"1.2345E+4\")\n", "    def test_make_list(self):\n        output = self.engine.render_to_string(\"make_list\", {\"a\": \"hello\"})\n        self.assertEqual(output, \"['h', 'e', 'l', 'l', 'o']\")\n", "    def test_add(self):\n        output = self.engine.render_to_string(\"add01\", {\"i\": 5})\n        self.assertEqual(output, \"10\")\n", "    def test_floatformat01(self):\n        output = self.engine.render_to_string(\"floatformat01\", {\"num\": 34.23234})\n        self.assertEqual(output, \"34.2\")\n"], "sample_482": ["    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [\"x&y\", \"<p>\"]},\n        )\n        self.assertEqual(output, \"x&amp;y, &lt;p&gt; -- x&y, <p>\")\n", "    def test_float(self):\n        output = self.engine.render_to_string(\"stringformat_float\", {\"num\": 3.14})\n        self.assertEqual(output, \"3.140000\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n", "    def test_basic(self):\n        output = self.engine.render_to_string(\n            \"safeseq_basic\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n"], "sample_483": ["def test_list_filter_item_not_a_field(self):\n    class SongAdmin(admin.ModelAdmin):\n        list_filter = (\"nonexistent\",)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent', which does not \"\n            \"refer to a Field.\",\n            obj=SongAdmin,\n            id=\"admin.E116\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_list_filter_item_with_through(self):\n    class BookAdmin(admin.ModelAdmin):\n        list_filter = [\"authors\"]\n\n    errors = BookAdmin(Book, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'authors', which does not \"\n            \"refer to a Field.\",\n            obj=BookAdmin,\n            id=\"admin.E116\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_autocomplete_fields_item(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"album\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"nonexistent\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' refers to 'nonexistent', \"\n            \"which is not a field of 'admin_checks.Song'.\",\n            obj=SongAdmin,\n            id=\"admin.E037\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = [\"title\"]\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' must be a foreign key or a \"\n            \"many-to-many field.\",\n            obj=SongAdmin,\n            id=\"admin.E038\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_date_hierarchy(self):\n    class SongAdmin(admin.ModelAdmin):\n        date_hierarchy = \"release_date\"\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class SongAdmin(admin.ModelAdmin):\n        date_hierarchy = \"nonexistent\"\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'date_hierarchy' refers to 'nonexistent', which does \"\n            \"not refer to a Field.\",\n            obj=SongAdmin,\n            id=\"admin.E127\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n\n    class SongAdmin(admin.ModelAdmin):\n        date_hierarchy = \"album\"\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'date_hierarchy' must be a DateField or DateTimeField.\",\n            obj=SongAdmin,\n            id=\"admin.E128\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_autocomplete_fields_item(self):\n    class SongAdmin(admin.ModelAdmin):\n        autocomplete_fields = (\"album\",)\n\n    errors = SongAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class InvalidSongAdmin(admin.ModelAdmin):\n        autocomplete_fields = (\"invalid_field\",)\n\n    errors = InvalidSongAdmin(Song, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'autocomplete_fields[0]' refers to 'invalid_field', \"\n            \"which is not a field of 'admin_checks.Song'.\",\n            obj=InvalidSongAdmin,\n            id=\"admin.E037\",\n        )\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_484": ["    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n"], "sample_485": ["def test_html_safe_subclass_without_str_override(self):\n    class BaseClass:\n            # defines __html__ on its own\n            return \"some html content\"\n\n            return \"some non html content\"\n\n    @html_safe\n    class Subclass(BaseClass):\n        pass  # does not override __str__\n\n    subclass_obj = Subclass()\n    self.assertEqual(str(subclass_obj), subclass_obj.__html__())\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            \"Search for google.com.\",\n            'Search for <a href=\"http://google.com\">google.com</a>.',\n        ),\n        (\n            \"Search for google.com!?\",\n            'Search for <a href=\"http://google.com\">google.com</a>!?',\n        ),\n        (\n            \"Search for google.com,\",\n            'Search for <a href=\"http://google.com\">google.com</a>,',\n        ),\n        (\n            \"Search for google.com;\",\n            'Search for <a href=\"http://google.com\">google.com</a>;',\n        ),\n        (\n            \"Search for google.com:\",\n            'Search for <a href=\"http://google.com\">google.com</a>:',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        (\"Search for google.com.\", 'Search for <a href=\"http://google.com/\">google.com</a>.'),\n        (\"Search for google.com!?\", 'Search for <a href=\"http://google.com/\">google.com</a>!?'),\n        (\"Search for google.com...\", 'Search for <a href=\"http://google.com/\">google.com</a>...'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_nofollow(self):\n    tests = (\n        (\n            \"Search for google.com/?q=! and see.\",\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.',\n        ),\n        (\n            lazystr(\"Search for google.com/?q=!\"),\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!',\n        ),\n        (\n            \"foo@example.com\",\n            '<a href=\"mailto:foo@example.com\">foo@example.com</a>',\n        ),  # mailto links shouldn't have nofollow\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\n            \"Search for google.com.\",\n            'Search for <a href=\"http://google.com/\">google.com</a>.',\n        ),\n        (\n            \"Search for google.com,\",\n            'Search for <a href=\"http://google.com/\">google.com</a>,',\n        ),\n        (\n            \"Search for google.com!\",\n            'Search for <a href=\"http://google.com/\">google.com</a>!',\n        ),\n        (\n            \"Search for google.com?\",\n            'Search for <a href=\"http://google.com/\">google.com</a>?',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_486": ["def test_inlineformset_factory_uses_to_field_on_foreign_key(self):\n    \"\"\"\n    #24377 - Inlines with a foreign key to a model with a custom primary key\n    should use the 'to_field' attribute of that foreign key to determine the\n    value of the parent's field in the inline form.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        ParentWithUUIDAlternateKey, ChildRelatedViaAK, fields=\"__all__\"\n    )\n    formset = FormSet()\n    self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n    self.assertEqual(\n        formset.forms[0].fields[\"parent\"].widget.attrs[\"name\"], \"childrelatedviak_set-0-parent\"\n    )\n", "def test_inlineformset_factory_with_to_field(self):\n    \"\"\"\n    #27035 - Test that inlineformset_factory works when the foreign key has a\n    to_field attribute.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        ParentWithUUIDAlternateKey, ChildRelatedViaAK, fields=\"__all__\", fk_name=\"parent\"\n    )\n    formset = FormSet()\n    self.assertIsNotNone(formset.forms[0].fields[\"parent\"].initial)\n", "def test_inlineformset_factory_validates_unique_fields(self):\n    \"\"\"\n    #25542 - If a model has a unique field, inlineformset_factory should\n    validate that the field is unique across all forms.\n    \"\"\"\n    FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n    formset = FormSet(\n        {\n            \"uuidpkchild_set-TOTAL_FORMS\": 2,\n            \"uuidpkchild_set-INITIAL_FORMS\": 0,\n            \"uuidpkchild_set-MAX_NUM_FORMS\": \"\",\n            \"uuidpkchild_set-0-name\": \"Foo\",\n            \"uuidpkchild_set-1-name\": \"Foo\",\n        }\n    )\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n", "def test_inlineformset_factory_with_empty_queryset(self):\n    \"\"\"\n    Test that inlineformset_factory works correctly with an empty queryset.\n    \"\"\"\n    FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n    formset = FormSet(queryset=UUIDPKChild.objects.none())\n    self.assertEqual(formset.total_form_count(), 1)\n    self.assertEqual(formset.initial_form_count(), 0)\n", "def test_inlineformset_factory_with_to_field(self):\n    \"\"\"\n    #25354 - Test that inlineformset_factory works when the ForeignKey has a\n    to_field parameter.\n    \"\"\"\n    FormSet = inlineformset_factory(\n        ParentWithUUIDAlternateKey, ChildRelatedViaAK, fields=\"__all__\", fk_name=\"parent\"\n    )\n    formset = FormSet()\n    self.assertIsNotNone(formset.forms[0].fields[\"parent\"].initial)\n    self.assertIsInstance(formset.forms[0].fields[\"parent\"], InlineForeignKeyField)\n    self.assertEqual(\n        formset.forms[0].fields[\"parent\"].to_field_name,\n        \"uuid_alternate_key\",\n    )\n"], "sample_487": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_list_editable_primary_key(self):\n        \"\"\"\n        Primary key fields cannot be in list_editable.\n        \"\"\"\n\n        class ProductAdmin(ModelAdmin):\n            list_display = [\"id\", \"name\"]\n            list_editable = [\"id\"]\n\n        self.assertIsInvalid(\n            ProductAdmin,\n            ValidationTestModel,\n            \"The value of 'list_editable[0]' refers to 'id', which is not editable \"\n            \"through the admin.\",\n            id=\"admin.E125\",\n        )\n", "    def test_exclude_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"hello\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_view_on_site_not_callable_or_bool(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"string\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            id=\"admin.E025\",\n        )\n"], "sample_488": ["def test_get_containing_app_config(self):\n    \"\"\"\n    apps.get_containing_app_config() returns the app config for the inner\n    application in case of nesting.\n    \"\"\"\n    self.assertEqual(\n        apps.get_containing_app_config(\"apps.apps.MyAdmin\"),\n        apps.get_app_config(\"myadmin\")\n    )\n    self.assertIsNone(apps.get_containing_app_config(\"nonexistent\"))\n", "def test_set_available_apps(self):\n    \"\"\"\n    Tests apps.set_available_apps() and apps.unset_available_apps().\n    \"\"\"\n    original_app_configs = apps.app_configs.copy()\n    try:\n        with self.settings(INSTALLED_APPS=SOME_INSTALLED_APPS):\n            apps.clear_cache()\n            all_app_configs = apps.get_app_configs()\n\n            # Make 'django.contrib.admin' unavailable.\n            available = [ac.name for ac in all_app_configs if ac.name != \"admin\"]\n            apps.set_available_apps(available)\n\n            # Check that 'django.contrib.admin' is no longer available.\n            self.assertIsNone(apps.get_app_config(\"admin\", raise_exception=False))\n\n            # Check that the other apps are still available.\n            for app_name in available:\n                self.assertIsNotNone(apps.get_app_config(app_name, raise_exception=False))\n\n            # Revert and check that 'django.contrib.admin' is available again.\n            apps.unset_available_apps()\n            self.assertIsNotNone(apps.get_app_config(\"admin\"))\n    finally:\n        apps.app_configs = original_app_configs\n        apps.clear_cache()\n", "def test_set_available_apps(self):\n    \"\"\"\n    Apps.set_available_apps() restricts the set of installed apps.\n    \"\"\"\n    available = [\"apps.apps.MyAdmin\", \"django.contrib.contenttypes\"]\n    apps.set_available_apps(available)\n    try:\n        self.assertEqual(\n            [app_config.name for app_config in apps.get_app_configs()],\n            [\"django.contrib.admin\", \"django.contrib.contenttypes\"],\n        )\n        with self.assertRaises(LookupError):\n            apps.get_app_config(\"django.contrib.auth\")\n    finally:\n        apps.unset_available_apps()\n", "    def test_populate_calls_ready(self):\n        \"\"\"\n        The ready() method of each AppConfig is called during populate().\n        \"\"\"\n        class ReadyAppConfig(AppConfig):\n            ready_called = False\n\n                self.ready_called = True\n\n        apps = Apps([ReadyAppConfig(\"ready_app\", Stub(__path__=[\"a\"]))])\n        self.assertTrue(apps.get_app_config(\"ready_app\").ready_called)\n", "def test_set_available_apps(self):\n    \"\"\"\n    Tests apps.set_available_apps().\n    \"\"\"\n    old_app_configs = apps.app_configs\n    try:\n        apps.set_available_apps([\"django.contrib.admin\"])\n        self.assertEqual(len(apps.app_configs), 1)\n        self.assertIn(\"admin\", apps.app_configs)\n    finally:\n        apps.unset_available_apps()\n        self.assertEqual(apps.app_configs, old_app_configs)\n"], "sample_489": ["def test_update_conflicts_with_fields_db_column(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(column1=\"foo\", column2=\"bar\"),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 1)\n\n    conflicting_objects = [\n        FieldsWithDbColumns(column1=\"foo\", column2=\"baz\"),\n    ]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"column2\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 1)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"column1\", \"column2\"),\n        [\n            {\"column1\": \"foo\", \"column2\": \"baz\"},\n        ],\n    )\n", "def test_db_returning_fields(self):\n    # Test that the db_returning_fields attribute of Model._meta is populated\n    # correctly.\n    self.assertEqual(\n        FieldsWithDbColumns._meta.db_returning_fields,\n        [\"integerfield\", \"charfield\"],\n    )\n    # Test that a model without any fields with db_returning set doesn't have\n    # db_returning_fields set.\n    self.assertIsNone(NoFields._meta.db_returning_fields)\n", "def test_bulk_create_with_fields_db_column(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(char_field=\"foo\", integer_field=1),\n            FieldsWithDbColumns(char_field=\"bar\", integer_field=2),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"char_field\", \"integer_field\"),\n        [\n            {\"char_field\": \"foo\", \"integer_field\": 1},\n            {\"char_field\": \"bar\", \"integer_field\": 2},\n        ],\n    )\n", "def test_update_conflicts_updating_all_fields(self):\n    Country.objects.bulk_create(self.data)\n    self.assertEqual(Country.objects.count(), 4)\n\n    new_data = [\n        # Conflicting countries.\n        Country(\n            name=\"Germany\",\n            iso_two_letter=\"DE\",\n            description=(\"Germany is a country in Central Europe.\"),\n        ),\n        Country(\n            name=\"Czech Republic\",\n            iso_two_letter=\"CZ\",\n            description=(\n                \"The Czech Republic is a landlocked country in Central Europe.\"\n            ),\n        ),\n        # New countries.\n        Country(name=\"Australia\", iso_two_letter=\"AU\"),\n        Country(\n            name=\"Japan\",\n            iso_two_letter=\"JP\",\n            description=(\"Japan is an island country in East Asia.\"),\n        ),\n    ]\n    results = Country.objects.bulk_create(\n        new_data,\n        update_conflicts=True,\n        update_fields=[\"name\", \"iso_two_letter\", \"description\"],\n    )\n    self.assertEqual(len(results), len(new_data))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(Country.objects.count(), 6)\n    self.assertCountEqual(\n        Country.objects.values(\"iso_two_letter\", \"description\"),\n        [\n            {\"iso_two_letter\": \"US\", \"description\": \"\"},\n            {\"iso_two_letter\": \"NL\", \"description\": \"\"},\n            {\n                \"iso_two_letter\": \"DE\",\n                \"description\": (\"Germany is a country in Central Europe.\"),\n            },\n            {\n                \"iso_two_letter\": \"CZ\",\n                \"description\": (\n                    \"The Czech Republic is a landlocked country in Central Europe.\"\n                ),\n            },\n            {\"iso_two_letter\": \"AU\", \"description\": \"\"},\n            {\n                \"iso_two_letter\": \"JP\",\n                \"description\": (\"Japan is an island country in East Asia.\"),\n            },\n        ],\n    )\n", "def test_update_conflicts_with_inheritance(self):\n    parent = FieldsWithDbColumns.objects.create()\n    child = NullableFields.objects.create(auto_field=parent)\n    data = [\n        NullableFields(auto_field=parent, integer_field=88),\n    ]\n    results = NullableFields.objects.bulk_create(\n        data,\n        update_conflicts=True,\n        update_fields=[\"integer_field\"],\n    )\n    self.assertEqual(len(results), len(data))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(NullableFields.objects.count(), 1)\n    self.assertEqual(NullableFields.objects.get().integer_field, 88)\n"], "sample_490": ["def test_validate_expression_nulls_distinct(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"), name=\"name_lower_uniq\", nulls_distinct=False\n    )\n    # NULL should be treated as equal to another NULL.\n    with self.assertRaisesMessage(ValidationError, \"Constraint \u201cname_lower_uniq\u201d is violated.\"):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=None),\n        )\n    # Non-NULL values should still raise a ValidationError.\n    with self.assertRaisesMessage(ValidationError, \"Constraint \u201cname_lower_uniq\u201d is violated.\"):\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper()),\n        )\n", "    def test_validate_with_exclude(self):\n        constraint = UniqueConstraint(fields=[\"name\", \"color\"], name=\"unique_fields\")\n        existing_product = UniqueConstraintProduct.objects.create(name=\"test\", color=\"red\")\n        new_product = UniqueConstraintProduct(name=existing_product.name, color=existing_product.color)\n        \n        # Validation should fail without exclude\n        with self.assertRaises(ValidationError):\n            constraint.validate(UniqueConstraintProduct, new_product)\n\n        # Validation should pass with exclude\n        constraint.validate(UniqueConstraintProduct, new_product, exclude={\"name\"})\n\n        # Validation should also pass when excluding both fields\n        constraint.validate(UniqueConstraintProduct, new_product, exclude={\"name\", \"color\"})\n", "def test_clone(self):\n    constraint = UniqueConstraint(\n        fields=[\"foo\", \"bar\"],\n        name=\"unique_fields\",\n        condition=models.Q(foo=models.F(\"bar\")),\n        deferrable=models.Deferrable.IMMEDIATE,\n        include=[\"baz_1\", \"baz_2\"],\n        opclasses=[\"text_pattern_ops\", \"varchar_pattern_ops\"],\n        nulls_distinct=False,\n        violation_error_code=\"custom_code\",\n        violation_error_message=\"Custom message\",\n    )\n    cloned_constraint = constraint.clone()\n    self.assertEqual(constraint, cloned_constraint)\n    self.assertEqual(constraint.name, cloned_constraint.name)\n    self.assertEqual(constraint.fields, cloned_constraint.fields)\n    self.assertEqual(constraint.condition, cloned_constraint.condition)\n    self.assertEqual(constraint.deferrable, cloned_constraint.deferrable)\n    self.assertEqual(constraint.include, cloned_constraint.include)\n    self.assertEqual(constraint.opclasses, cloned_constraint.opclasses)\n    self.assertEqual(constraint.nulls_distinct, cloned_constraint.nulls_distinct)\n    self.assertEqual(\n        constraint.violation_error_code, cloned_constraint.violation_error_code\n    )\n    self.assertEqual(\n        constraint.violation_error_message, cloned_constraint.violation_error_message\n    )\n", "def test_validate_expression_with_nulls_distinct(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        nulls_distinct=False,\n        name=\"name_lower_uniq\",\n    )\n    UniqueConstraintProduct.objects.create(name=\"test\")\n    instance = UniqueConstraintProduct(name=None)\n    constraint.validate(UniqueConstraintProduct, instance)\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        nulls_distinct=True,\n        name=\"name_lower_uniq\",\n    )\n    with self.assertRaises(ValidationError):\n        constraint.validate(UniqueConstraintProduct, instance)\n", "def test_validate_expression_with_nulls_distinct(self):\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_uniq\",\n        nulls_distinct=False,\n    )\n    UniqueConstraintProduct.objects.create(name=None)\n    # No ValidationError as NULL values are not distinct.\n    constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=None))\n    constraint = models.UniqueConstraint(\n        Lower(\"name\"),\n        name=\"name_lower_uniq\",\n        nulls_distinct=True,\n    )\n    UniqueConstraintProduct.objects.create(name=None)\n    msg = \"Constraint \u201cname_lower_uniq\u201d is violated.\"\n    with self.assertRaisesMessage(ValidationError, msg):\n        constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=None))\n"], "sample_492": ["def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    MigrationWriter.unregister_serializer(complex)\n\n    with self.assertRaisesMessage(\n        ValueError,\n        \"Cannot serialize: complex(1+2j)\\nThere are no serializers for \"\n        \"<class 'complex'>.\",\n    ):\n        MigrationWriter.serialize(1 + 2j)\n", "def test_unregister_serializer(self):\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    self.assertEqual(MigrationWriter.serialize(complex(1, 2))[0], \"complex((1+2j))\")\n    MigrationWriter.unregister_serializer(complex)\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+2j)\"):\n        MigrationWriter.serialize(complex(1, 2))\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    MigrationWriter.unregister_serializer(complex)\n\n    with self.assertRaisesMessage(\n        ValueError,\n        \"Cannot serialize: complex() There is no serialization handler for complex.\",\n    ):\n        MigrationWriter.serialize(complex())\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    MigrationWriter.unregister_serializer(complex)\n    with self.assertRaisesMessage(\n        ValueError,\n        \"Cannot serialize: complex(1+2j)\\nThere are no serializers for \"\n        \"<class 'complex'>.\",\n    ):\n        MigrationWriter.serialize(1 + 2j)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    MigrationWriter.register_serializer(complex, ComplexSerializer)\n    MigrationWriter.unregister_serializer(complex)\n\n    with self.assertRaisesMessage(\n        ValueError, \"Cannot serialize function: <class 'complex'>\"\n    ):\n        MigrationWriter.serialize(complex(1, 2))\n"], "sample_493": ["def test_subquery_in_aggregate(self):\n    subquery = Book.objects.filter(publisher=OuterRef(\"pk\")).order_by(\"-rating\")[:1]\n    publishers = Publisher.objects.annotate(\n        highest_rated_book=Subquery(subquery.values(\"rating\"))\n    ).aggregate(avg_highest_rating=Avg(\"highest_rated_book\"))\n    self.assertAlmostEqual(publishers[\"avg_highest_rating\"], 4.0, places=2)\n", "def test_aggregation_with_having(self):\n    publishers = (\n        Publisher.objects.annotate(num_books=Count(\"book__id\"))\n        .filter(num_books__gt=1)\n        .order_by(\"pk\")\n    )\n    self.assertQuerySetEqual(\n        publishers, [\"Apress\", \"Prentice Hall\"], lambda p: p.name\n    )\n\n    publishers = Publisher.objects.filter(book__price__lt=Decimal(\"40.0\")).annotate(\n        num_books=Count(\"book__id\")\n    )\n    publishers = publishers.filter(num_books__gt=1).order_by(\"pk\")\n    self.assertQuerySetEqual(publishers, [\"Apress\"], lambda p: p.name)\n\n    publishers = (\n        Publisher.objects.annotate(num_books=Count(\"book__id\"))\n        .filter(num_books__range=[1, 3])\n        .order_by(\"pk\")\n    )\n    self.assertQuerySetEqual(\n        publishers,\n        [\n            \"Apress\",\n            \"Sams\",\n            \"Prentice Hall\",\n            \"Morgan Kaufmann\",\n        ],\n        lambda p: p.name,\n    )\n\n    publishers = (\n        Publisher.objects.annotate(num_books=Count(\"book__id\"))\n        .filter(num_books__in=[1, 3])\n        .order_by(\"pk\")\n    )\n    self.assertQuerySetEqual(\n        publishers,\n        [\n            \"Sams\",\n            \"Morgan Kaufmann\",\n        ],\n        lambda p: p.name,\n    )\n\n    publishers = Publisher.objects.annotate(num_books=Count(\"book__id\")).filter(\n        num_books__isnull=True\n    )\n    self.assertEqual(len(publishers), 0)\n", "def test_aggregation_with_database_functions(self):\n    result = Book.objects.aggregate(\n        earliest_pubdate=Min(Func(F(\"pubdate\"), function=\"TRUNC\")),\n        latest_pubdate=Max(Func(F(\"pubdate\"), function=\"TRUNC\")),\n    )\n    self.assertEqual(result[\"earliest_pubdate\"], datetime.date(1991, 10, 15))\n    self.assertEqual(result[\"latest_pubdate\"], datetime.date(2008, 12, 6))\n", "def test_values_list_aggregate_with_expression(self):\n    result = Book.objects.values_list(\n        F(\"pages\") + 1,\n    ).annotate(\n        total_books=Count(\"*\"),\n    )\n    self.assertEqual(list(result), [(101, 2), (1001, 1)])\n", "def test_aggregation_with_expressionwrapper(self):\n    from django.db.models import ExpressionWrapper\n\n    wrapper = ExpressionWrapper(F(\"price\") * 1.5, output_field=FloatField())\n    aggregate = Book.objects.aggregate(sum_price=Sum(wrapper))\n    self.assertAlmostEqual(aggregate[\"sum_price\"], 405.41, places=2)\n"], "sample_494": ["def test_serialize_datetime_timezone(self):\n    tz = datetime.timezone(datetime.timedelta(hours=1))\n    dt = datetime.datetime(2022, 1, 1, tzinfo=tz)\n    string, imports = MigrationWriter.serialize(dt)\n    self.assertEqual(\n        string,\n        \"datetime.datetime(2022, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)\",\n    )\n    self.assertEqual(imports, {\"import datetime\"})\n", "def test_serialize_nested_functools_partial(self):\n        return x + y\n\n    value = functools.partial(functools.partial(add, 1), 2)\n    result = self.serialize_round_trip(value)\n    self.assertEqual(result.func.func, value.func.func)\n    self.assertEqual(result.args, value.args)\n    self.assertEqual(result.keywords, value.keywords)\n", "def test_unregister_serializer(self):\n    class TestSerializer(BaseSerializer):\n            return \"test(%r)\" % self.value, {}\n\n    Serializer.register(complex, TestSerializer)\n    Serializer.unregister(complex)\n\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+1j)\"):\n        MigrationWriter.serialize(1 + 1j)\n", "def test_unregister_serializer(self):\n    class ComplexSerializer(BaseSerializer):\n            return \"complex(%r)\" % self.value, {}\n\n    Serializer.register(complex, ComplexSerializer)\n    self.assertEqual(Serializer._registry[complex], ComplexSerializer)\n\n    Serializer.unregister(complex)\n    self.assertNotIn(complex, Serializer._registry)\n", "def test_unregister_serializer(self):\n    class TestSerializer(BaseSerializer):\n            return \"test(%r)\" % self.value, {}\n\n    Serializer.register(complex, TestSerializer)\n    string = MigrationWriter.serialize(complex(1))[0]\n    self.assertEqual(string, \"test((1+0j))\")\n    Serializer.unregister(complex)\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+1j)\"):\n        MigrationWriter.serialize(complex(1))\n"], "sample_495": ["def test_page_repr(self):\n    \"\"\"\n    Paginator page __repr__ returns a string in the format <Page X of Y>.\n    \"\"\"\n    paginator = Paginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertEqual(repr(page), \"<Page 1 of 2>\")\n", "def test_paginating_queryset_with_count_method(self):\n    class CountingQuerySet:\n            return 42\n\n    paginator = Paginator(CountingQuerySet(), 10)\n    self.assertEqual(paginator.count, 42)\n    self.assertEqual(paginator.num_pages, 5)\n    self.assertEqual(list(paginator.page_range), [1, 2, 3, 4, 5])\n", "def test_page_repr(self):\n    paginator = Paginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertEqual(repr(page), '<Page 1 of 2>')\n", "def test_page_representation(self):\n    paginator = Paginator([1, 2, 3], 2)\n    page = paginator.page(1)\n    self.assertEqual(repr(page), '<Page 1 of 2>')\n", "def test_empty_page_queryset_iterator(self):\n    \"\"\"\n    Paginator with an empty page still allows iterating over the queryset.\n    \"\"\"\n    paginator = Paginator(Article.objects.order_by('id'), 5)\n    p = paginator.page(1)\n    self.assertIsInstance(p.object_list, type(Article.objects.all()))\n    # Empty the page's object list to simulate an empty page.\n    p.object_list = Article.objects.none()\n    # Iterating over the page should not raise an exception.\n    for _ in p:\n        pass\n"], "sample_496": ["    def setUp(self):\n        self.write_settings('settings.py')\n", "    def test_get_output_wrapper(self):\n        wrapper = OutputWrapper(StringIO())\n        self.assertIs(wrapper._out, wrapper.stdout)\n", "    def test_styles(self):\n        wrapper = OutputWrapper(StringIO())\n        self.assertEqual(wrapper.style_func(\"message\"), \"message\")\n\n        wrapper.style_func = lambda x: x.upper()\n        self.assertEqual(wrapper.style_func(\"message\"), \"MESSAGE\")\n", "    def test_handle_default_options(self):\n        \"\"\"handle_default_options sets the correct environment variables.\"\"\"\n        options = {'settings': 'test_project.settings', 'pythonpath': '/tmp/pythonpath'}\n        with mock.patch.dict('os.environ', clear=True) as environ:\n            handle_default_options(options)\n            self.assertEqual(environ['DJANGO_SETTINGS_MODULE'], 'test_project.settings')\n            self.assertEqual(sys.path[0], '/tmp/pythonpath')\n", "    def test_command_error(self):\n        class Command(BaseCommand):\n                raise CommandError(\"Something went wrong\")\n\n        out = StringIO()\n        err = StringIO()\n        command = Command(stdout=out, stderr=err)\n        with self.assertRaises(SystemExit):\n            command.run_from_argv(['', ''])\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"CommandError: Something went wrong\")\n"], "sample_497": ["def test_axis_label_position():\n    fig, ax = plt.subplots()\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n\n    # Test default positions\n    assert ax.xaxis.get_label_position() == 'bottom'\n    assert ax.yaxis.get_label_position() == 'left'\n\n    # Test setting positions\n    ax.xaxis.set_label_position('top')\n    ax.yaxis.set_label_position('right')\n    assert ax.xaxis.get_label_position() == 'top'\n    assert ax.yaxis.get_label_position() == 'right'\n\n    # Test invalid positions\n    with pytest.raises(ValueError):\n        ax.xaxis.set_label_position('invalid')\n    with pytest.raises(ValueError):\n        ax.yaxis.set_label_position('invalid')\n", "def test_remove_overlapping_locs():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.xaxis.set_major_locator(mticker.MultipleLocator(1))\n    ax.xaxis.set_minor_locator(mticker.MultipleLocator(0.5))\n    ax.xaxis.remove_overlapping_locs = False\n    ticks_to_draw = ax.xaxis._update_ticks()\n    assert len(ticks_to_draw) == len(ax.xaxis.get_major_ticks()) + len(ax.xaxis.get_minor_ticks())\n    ax.xaxis.remove_overlapping_locs = True\n    ticks_to_draw = ax.xaxis._update_ticks()\n    assert len(ticks_to_draw) < len(ax.xaxis.get_major_ticks()) + len(ax.xaxis.get_minor_ticks())\n", "def test_axis_set_label_coords():\n    fig, ax = plt.subplots()\n    ax.set_xlabel('X Axis')\n    ax.set_ylabel('Y Axis')\n\n    # Check default label positions\n    assert ax.xaxis.label.get_position() == (0.5, 0)\n    assert ax.yaxis.label.get_position() == (0, 0.5)\n\n    # Set label coordinates and check\n    ax.xaxis.set_label_coords(0.2, 0.1)\n    ax.yaxis.set_label_coords(0.3, 0.8)\n    assert ax.xaxis.label.get_position() == (0.2, 0.1)\n    assert ax.yaxis.label.get_position() == (0.3, 0.8)\n\n    # Check that tick labels are updated correctly\n    ax.set_xticks([1, 2, 3])\n    ax.set_yticks([4, 5, 6])\n    ax.xaxis.set_tick_params(labelbottom=True, labeltop=False)\n    ax.yaxis.set_tick_params(labelleft=True, labelright=False)\n    assert all(tick.label.get_visible() for tick in ax.xaxis.get_major_ticks())\n    assert all(not tick.label2.get_visible() for tick in ax.xaxis.get_major_ticks())\n    assert all(tick.label.get_visible() for tick in ax.yaxis.get_major_ticks())\n    assert all(not tick.label2.get_visible() for tick in ax.yaxis.get_major_ticks())\n\n    # Reset label positions to default\n    ax.xaxis.set_label_coords(0.5, 0)\n    ax.yaxis.set_label_coords(0, 0.5)\n    assert ax.xaxis.label.get_position() == (0.5, 0)\n    assert ax.yaxis.label.get_position() == (0, 0.5)\n", "def test_axis_remove_overlapping_ticks():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.xaxis.set_major_locator(mticker.MultipleLocator(1))\n    ax.xaxis.remove_overlapping_locs = False\n    assert len(ax.get_xticks()) == 11\n\n    ax.xaxis.remove_overlapping_locs = True\n    assert len(ax.get_xticks()) < 11\n", "def test_smart_bounds():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.xaxis.set_smart_bounds(True)\n    ax.yaxis.set_smart_bounds(True)\n    assert ax.xaxis.get_smart_bounds() == True\n    assert ax.yaxis.get_smart_bounds() == True\n    ax.xaxis.set_smart_bounds(False)\n    ax.yaxis.set_smart_bounds(False)\n    assert ax.xaxis.get_smart_bounds() == False\n    assert ax.yaxis.get_smart_bounds() == False\n"], "sample_498": ["def test_legend_labels_from_line2d():\n    # Test that labels can be copied for legend lines\n    fig, ax = plt.subplots()\n    lines = [mlines.Line2D([0], [0], label=\"foo\"),\n             mlines.Line2D([0], [0], label=\"bar\")]\n    legend = ax.legend(handles=lines)\n\n    new_labels = [text.get_text() for text in legend.get_texts()]\n    assert new_labels == [\"foo\", \"bar\"]\n", "def test_legend_with_no_artists():\n    # Test that creating a legend with no artists doesn't raise an exception.\n    fig, ax = plt.subplots()\n    ax.legend()\n    assert len(ax.get_legend_handles_labels()[0]) == 0\n    assert len(ax.get_legend_handles_labels()[1]) == 0\n", "def test_legend_empty_handles():\n    # Test that legend can handle empty handles\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(handles=[])\n\n    assert leg.get_texts() == []\n    assert leg.legend_handles == []\n", "def test_legend_bbox_to_anchor():\n    # Test that legend can be placed outside of the axes with bbox_to_anchor.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    assert leg.get_window_extent().x0 > ax.get_window_extent().x1\n", "def test_legend_handle_length():\n    # Test that the handle length can be set correctly.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(handlelength=10)\n    assert leg.handlelength == 10\n    leg.handlelength = 5\n    assert leg.handlelength == 5\n"], "sample_499": ["def test_legend_with_invalid_transform():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    with pytest.raises(ValueError):\n        ax.legend(bbox_to_anchor=(0.5, 0.5), bbox_transform='invalid')\n", "def test_legend_title_visibility():\n    # Test that the legend title is only visible when a title is set.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(title='Legend Title')\n    assert leg.get_title().get_visible()\n\n    leg = ax.legend()\n    assert not leg.get_title().get_visible()\n", "def test_legend_empty_label():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2], [3, 4], label='line1')\n    line2, = ax.plot([1, 2], [5, 6], label='')\n    leg = ax.legend(handles=[line1, line2])\n    assert len(leg.get_texts()) == 1\n    assert leg.get_texts()[0].get_text() == 'line1'\n", "def test_legend_multiple_handles_per_label():\n    # Test that multiple handles can be assigned to the same label.\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2], [3, 4], label='line')\n    line2, = ax.plot([1, 2], [4, 5], label='line')\n    leg = ax.legend()\n\n    assert len(leg.legendHandles) == 1\n    assert len(leg.get_texts()) == 1\n", "def test_legend_labelspacing():\n    # Test legend label spacing\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    ax.plot([1, 2], [4, 5], label='another line')\n    leg = ax.legend(labelspacing=2)\n\n    assert leg.labelspacing == 2\n    assert leg._legend_box.sep == 2 * leg._fontsize\n"], "sample_500": ["def test_colorbar_with_no_norm(fig_ref, fig_test):\n    ax = fig_test.add_subplot()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig_test.colorbar(cm.ScalarMappable(cmap='viridis'), cax=ax)\n    cb.set_ticks([0, 50, 100])\n", "def test_colorbar_orientation():\n    # Test that colorbar orientation is correctly set\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    # Test vertical orientation\n    plt.contourf(data, levels=levels)\n    cbar = plt.colorbar(orientation='vertical')\n    assert cbar.orientation == 'vertical'\n\n    # Test horizontal orientation\n    plt.contourf(data, levels=levels)\n    cbar = plt.colorbar(orientation='horizontal')\n    assert cbar.orientation == 'horizontal'\n", "def test_colorbar_no_ticks():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10)+1)\n    cb = fig.colorbar(pc, ax=ax)\n    cb.set_ticks([])\n    assert len(cb.ax.yaxis.get_ticklocs()) == 0\n", "def test_colorbar_update_ticks():\n    # Test that update_ticks updates the ticks and ticklabels correctly.\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    plt.contourf(data, levels=levels)\n\n    cbar = plt.colorbar(orientation='horizontal')\n    ticks, labels = cbar._ticker(cbar.locator, cbar.formatter)\n\n    # Set new ticks and labels\n    new_ticks = [200, 600, 1000]\n    new_labels = ['A', 'B', 'C']\n    cbar.set_ticks(new_ticks)\n    cbar.set_ticklabels(new_labels)\n\n    # Update ticks\n    cbar.update_ticks()\n\n    # Check if ticks and labels have been updated correctly\n    assert cbar.get_ticks().tolist() == new_ticks\n    assert [label.get_text() for label in cbar.ax.xaxis.get_ticklabels()] == new_labels\n", "def test_colorbar_set_ticks_after_set_alpha():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_alpha(0.5)\n    cb.set_ticks([20, 40, 60])\n    np.testing.assert_allclose(cb.get_ticks(), [20, 40, 60])\n"], "sample_501": ["def test_legend_handles_labels_unique():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], label='A')\n    ax.plot([2, 3], label='B')\n    ax.plot([3, 4], label='A')  # duplicate label\n    handles, labels = ax.get_legend_handles_labels()\n    assert len(handles) == 2\n    assert len(labels) == 2\n    assert labels == ['A', 'B']\n", "def test_legend_handler_map_warn():\n    fig, ax = plt.subplots()\n    with pytest.warns(UserWarning) as record:\n        ax.legend(handler_map={'1': 2})\n    assert len(record) == 1\n    assert str(record[0].message) == (\n        \"You have mixed positional and keyword arguments, some input may \"\n        \"be discarded.\")\n", "def test_legend_bbox_to_anchor():\n    # Test that bbox_to_anchor works with different types of input.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    \n    # Test with BboxBase\n    bbox = mpl.transforms.Bbox.from_bounds(0, 0, 1, 1)\n    leg = ax.legend(bbox_to_anchor=bbox)\n    assert leg.get_bbox_to_anchor() == bbox\n    \n    # Test with 2-tuple\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5))\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n    \n    # Test with 4-tuple\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5, 0.2, 0.2))\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n    assert leg.get_bbox_to_anchor().width == 0.2\n    assert leg.get_bbox_to_anchor().height == 0.2\n    \n    # Test with None\n    leg = ax.legend(bbox_to_anchor=None)\n    assert leg.get_bbox_to_anchor() is None\n", "def test_legend_multiple_columns():\n    # Test that specifying ncol works as expected.\n    fig, ax = plt.subplots()\n    for i in range(4):\n        ax.plot([0, 1], [i, i], label=f\"Line {i}\")\n    leg = ax.legend(ncol=2)\n    assert len(leg._legend_handle_box.get_children()) == 2\n    for column in leg._legend_handle_box.get_children():\n        assert len(column.get_children()) == 2\n", "def test_legend_handles_without_labels():\n    # Test legend handles without labels are ignored (#18564)\n    fig, ax = plt.subplots()\n    line1, = ax.plot([0, 1], label='A')\n    line2, = ax.plot([1, 2])  # no label\n    line3, = ax.plot([2, 3], label='B')\n    leg = ax.legend()\n    assert len(leg.get_texts()) == 2\n    assert leg.get_texts()[0].get_text() == 'A'\n    assert leg.get_texts()[1].get_text() == 'B'\n"], "sample_502": ["def test_subplots_sharex_sharey():\n    fig, axs = plt.subplots(2, 2, sharex='col', sharey='row')\n    assert axs[0, 0].get_shared_x_axes().joined(axs[0, 0], axs[1, 0])\n    assert not axs[0, 0].get_shared_x_axes().joined(axs[0, 0], axs[0, 1])\n    assert axs[0, 0].get_shared_y_axes().joined(axs[0, 0], axs[0, 1])\n    assert not axs[0, 0].get_shared_y_axes().joined(axs[0, 0], axs[1, 0])\n\n    fig, axs = plt.subplots(2, 2, sharex='row', sharey='col')\n    assert axs[0, 0].get_shared_x_axes().joined(axs[0, 0], axs[0, 1])\n    assert not axs[0, 0].get_shared_x_axes().joined(axs[0, 0], axs[1, 0])\n    assert axs[0, 0].get_shared_y_axes().joined(axs[0, 0], axs[1, 0])\n    assert not axs[0, 0].get_shared_y_axes().joined(axs[0, 0], axs[0, 1])\n\n    fig, axs = plt.subplots(2, 2, sharex='all', sharey='all')\n    for ax1 in axs.flat:\n        for ax2 in axs.flat:\n            assert ax1.get_shared_x_axes().joined(ax1, ax2)\n            assert ax1.get_shared_y_axes().joined(ax1, ax2)\n\n    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n    for ax1 in axs.flat:\n        for ax2 in axs.flat:\n            assert ax1.get_shared_x_axes().joined(ax1, ax2)\n            assert ax1.get_shared_y_axes().joined(ax1, ax2)\n", "def test_subplot_mosaic_reuse():\n    # create a mosaic of axes\n    fig, axes = plt.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    # check that the axes are created\n    assert len(axes) == 4\n    # try to create another mosaic with the same layout\n    fig2, axes2 = plt.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    # check that the axes are reused\n    assert axes['A'] is axes2['A']\n    assert axes['B'] is axes2['B']\n    assert axes['C'] is axes2['C']\n    assert axes['D'] is axes2['D']\n    # check that creating a new mosaic with a different layout creates new axes\n    fig3, axes3 = plt.subplot_mosaic([['E', 'F'], ['G', 'H']])\n    assert axes['A'] is not axes3['E']\n    assert axes['B'] is not axes3['F']\n    assert axes['C'] is not axes3['G']\n    assert axes['D'] is not axes3['H']\n", "def test_suptitle():\n    fig, ax = plt.subplots()\n    title = \"Test Suptitle\"\n    plt.suptitle(title)\n    assert fig._suptitle.get_text() == title\n    plt.close()\n", "def test_xkcd():\n    with plt.xkcd():\n        fig = plt.figure()\n        assert fig.get_tight_layout() == False\n        ax = fig.add_subplot(111)\n        line, = ax.plot([1, 2, 3])\n        assert line.get_linewidth() == 2.0\n\n    # Check that xkcd mode is properly restored after context\n    fig = plt.figure()\n    assert fig.get_tight_layout() != False\n    ax = fig.add_subplot(111)\n    line, = ax.plot([1, 2, 3])\n    assert line.get_linewidth() != 2.0\n", "def test_fignum_exists():\n    fig = plt.figure()\n    assert plt.fignum_exists(fig.number)\n    plt.close(fig)\n    assert not plt.fignum_exists(fig.number)\n\n    fig_label = 'my_figure'\n    fig = plt.figure(label=fig_label)\n    assert plt.fignum_exists(fig_label)\n    plt.close(fig)\n    assert not plt.fignum_exists(fig_label)\n"], "sample_503": ["def test_axline(fig_test, fig_ref):\n    fig_test.add_subplot().axline((1, 2), (3, 4))\n    fig_ref.add_subplot().plot([0, 1], [1, 3])\n", "def test_axline(fig_test, fig_ref):\n    ax1 = fig_test.add_subplot()\n    ax2 = fig_ref.add_subplot()\n\n    ax1.axline((0.5, 0.5), (1, 1))\n    ax2.plot([0.5, 1], [0.5, 1])\n", "def test_axline():\n    fig, ax = plt.subplots()\n    line1 = mlines._AxLine((0, 0), (1, 1))\n    ax.add_artist(line1)\n    assert_array_equal(line1.get_xydata(), [[0, 0], [1, 1]])\n\n    line2 = mlines._AxLine((0, 0), slope=1)\n    ax.add_artist(line2)\n    assert_array_equal(line2.get_xydata(), [[0, 0], [1, 1]])\n\n    with pytest.raises(TypeError):\n        mlines._AxLine((0, 0))\n\n    with pytest.raises(TypeError):\n        mlines._AxLine((0, 0), (1, 1), slope=1)\n\n    # Smoke test for drawing.\n    fig.canvas.draw()\n", "def test_markevery_extreme_floats(fig_test, fig_ref):\n    np.random.seed(42)\n    x = np.linspace(0, 1, 14)\n    y = np.random.rand(len(x))\n\n    # Test very small and very large float values\n    fig_test.add_subplot().plot(x, y, \"-gD\", markevery=(1e-10, 1e-5))\n    fig_ref.add_subplot().plot(x, y, \"-gD\", markevery=[0, -1])\n\n    fig_test.add_subplot().plot(x, y, \"-gD\", markevery=(1e10, 1e5))\n    fig_ref.add_subplot().plot(x, y, \"-gD\", markevery=[0])\n", "def test_update_from():\n    line1 = mlines.Line2D([0, 1], [0, 1])\n    line1.set_color(\"red\")\n    line1.set_linestyle(\"--\")\n    line1.set_marker(\"o\")\n\n    line2 = mlines.Line2D([0, 1], [0, 1])\n    line2.update_from(line1)\n\n    assert line2.get_color() == \"red\"\n    assert line2.get_linestyle() == \"--\"\n    assert line2.get_marker() == \"o\"\n"], "sample_504": ["def test_colorbar_lognorm_with_zero_vmin():\n    # Test that colorbar with lognorm is handled correctly when vmin is zero.\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1e-2, 1], [1e2, 1e4]], norm=LogNorm(vmin=0))\n    cbar = fig.colorbar(im)\n    assert np.isclose(cbar.vmin, 1e-2)\n", "def test_colorbar_set_label_with_custom_kwargs():\n    \"\"\"\n    Test that set_label method accepts custom kwargs and applies them to the label.\n    \"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im)\n\n    # Set label with custom kwargs\n    cbar.set_label('Custom Label', fontsize=20, color='red')\n\n    # Get the label text object\n    label = cbar.ax.yaxis.get_label()\n\n    # Check if custom kwargs were applied\n    assert label.get_fontsize() == 20\n    assert label.get_color() == 'red'\n", "def test_colorbar_set_label_multiline():\n    \"\"\"\n    Test that a multiline label for the colorbar is properly aligned.\n    \"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im)\n    cbar.set_label('This is a\\nmultiline label')\n    fig.draw_without_rendering()\n    np.testing.assert_allclose(cbar.ax.yaxis.label.get_position(),\n                               (0., 0.5), atol=0.01)\n", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    sm.set_array(np.arange(10))\n    cb = fig.colorbar(sm, alpha=0.5)\n    assert cb.alpha == 0.5\n    cb.set_alpha(0.8)\n    assert cb.alpha == 0.8\n\n    # Test that array-like alpha is properly set to None\n    sm.set_alpha(np.array([0.1, 0.2, 0.3]))\n    cb = fig.colorbar(sm)\n    assert cb.alpha is None\n", "def test_colorbar_set_alpha():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc)\n    assert cb.alpha is None\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n    cb.set_alpha([0.2, 0.4])\n    assert cb.alpha is None\n    assert cb.mappable.get_alpha() == [0.2, 0.4]\n"], "sample_505": ["def test_num2date_out_of_bounds():\n    # Test that num2date raises an error for dates outside the valid range\n    with pytest.raises(ValueError):\n        mdates.num2date(-1e10)\n    with pytest.raises(ValueError):\n        mdates.num2date(1e10)\n", "def test_date2num_roundtrip():\n    dates = np.array([\n        np.datetime64('2000-01-01T00:00:00.000000'),\n        np.datetime64('2000-01-01T12:00:00.123456'),\n        np.datetime64('9999-12-31T23:59:59.999999')\n    ])\n\n    numbers = mdates.date2num(dates)\n    roundtripped_dates = mdates.num2date(numbers)\n\n    np.testing.assert_array_equal(dates, roundtripped_dates)\n", "def test_date2num_microsecond_precision():\n    # Test that date2num preserves microsecond precision.\n    dt = datetime.datetime(2022, 1, 1, 0, 0, 0, 123456)\n    num = mdates.date2num(dt)\n    dt_roundtrip = mdates.num2date(num)\n    assert dt_roundtrip.microsecond == dt.microsecond\n", "def test_julian2num():\n    dates = [datetime.datetime(2020, 1, 1), datetime.datetime(2022, 7, 25)]\n    julian_dates = [2458849.5, 2459676.5]\n    np.testing.assert_allclose(mdates.date2num(dates), mdates.julian2num(julian_dates))\n", "def test_MicrosecondLocator():\n    dates = np.arange('2020-01-01T00:00:00.000000', '2020-01-01T00:00:00.001000',\n                      dtype='datetime64[us]')\n    locator = mdates.MicrosecondLocator(interval=100)\n    ticks = locator.tick_values(dates[0], dates[-1])\n    expected_ticks = np.array(['2020-01-01T00:00:00.000000',\n                               '2020-01-01T00:00:00.000100',\n                               '2020-01-01T00:00:00.000200',\n                               '2020-01-01T00:00:00.000300',\n                               '2020-01-01T00:00:00.000400',\n                               '2020-01-01T00:00:00.000500',\n                               '2020-01-01T00:00:00.000600',\n                               '2020-01-01T00:00:00.000700',\n                               '2020-01-01T00:00:00.000800',\n                               '2020-01-01T00:00:00.000900',\n                               '2020-01-01T00:00:00.001000'])\n    np.testing.assert_array_equal(mdates.num2date(ticks), expected_ticks)\n"], "sample_506": ["def test_spine_get_bounds():\n    \"\"\"Test Spine get_bounds method.\"\"\"\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() is None\n\n    spine.set_bounds(0, 1)\n    assert spine.get_bounds() == (0, 1)\n\n    spine.set_bounds(None, 2)\n    assert spine.get_bounds() == (0, 2)\n\n    spine.set_bounds(3, None)\n    assert spine.get_bounds() == (3, 2)\n\n    spine.set_bounds(None, None)\n    assert spine.get_bounds() == (3, 2)\n", "def test_spines_circular():\n    fig, ax = plt.subplots()\n    ax.set_aspect('equal', adjustable='box')\n    ax.spines['circle'] = ax.spines.circular_spine(ax, (0.5, 0.5), 0.4)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n", "def test_spine_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() is None\n\n    spine.set_bounds(0, 1)\n    assert spine.get_bounds() == (0, 1)\n\n    spine.set_bounds(None, 2)\n    assert spine.get_bounds() == (0, 2)\n\n    spine.set_bounds(-1, None)\n    assert spine.get_bounds() == (-1, 2)\n\n    with pytest.raises(ValueError):\n        spine.set_bounds('foo', 'bar')\n\n    with pytest.raises(ValueError):\n        spine.set_bounds([1, 2, 3], 4)\n", "def test_spine_circular_and_arc():\n    \"\"\"Test Spine with circular and arc patches.\"\"\"\n    fig, ax = plt.subplots()\n    circle_center = (0.5, 0.5)\n    circle_radius = 0.3\n    circle_spine = Spines.circular_spine(ax, circle_center, circle_radius)\n    ax.add_patch(circle_spine)\n\n    arc_center = (0.7, 0.7)\n    arc_radius = 0.2\n    theta1, theta2 = 0, np.pi/2\n    arc_spine = Spines.arc_spine(ax, 'top', arc_center, arc_radius, theta1, theta2)\n    ax.add_patch(arc_spine)\n\n    assert isinstance(circle_spine.get_path(), mpath.Path)\n    assert isinstance(arc_spine.get_path(), mpath.Path)\n    assert circle_spine.get_patch_transform() is not None\n    assert arc_spine.get_patch_transform() is not None\n\n    # check get_window_extent method\n    extent = circle_spine.get_window_extent()\n    assert isinstance(extent, mtransforms.Bbox)\n\n    # check set_bounds method\n    with pytest.raises(ValueError):\n        circle_spine.set_bounds(0, 1)\n\n    # check get_bounds method\n    assert circle_spine.get_bounds() is None\n", "def test_spines_circular():\n    fig, ax = plt.subplots()\n    ax.set_aspect('equal', adjustable='box')\n    ax.set_xlim(-1.5, 1.5)\n    ax.set_ylim(-1.5, 1.5)\n    circle = plt.Circle((0, 0), 1, fill=False)\n    ax.add_artist(circle)\n    ax.spines['circle'] = Spines.circular_spine(ax, (0, 0), 1)\n    ax.spines['circle'].set_color('red')\n    ax.spines['circle'].set_linewidth(2)\n    for spine in ['top', 'right', 'bottom', 'left']:\n        ax.spines[spine].set_visible(False)\n"], "sample_507": ["def test_convert_with_none_unit():\n    cc = cat.StrCategoryConverter()\n    with pytest.raises(ValueError):\n        cc.convert([\"a\", \"b\"], None, FakeAxis(None))\n", "    def test_convert_with_missing_category_information(self):\n        cc = cat.StrCategoryConverter()\n        with pytest.raises(ValueError):\n            cc.convert(\"hello\", None, FakeAxis(None))\n", "    def test_convert_empty_list(self):\n        cc = cat.StrCategoryConverter()\n        unit = cat.UnitData()\n        ax = FakeAxis(unit)\n        assert cc.convert([], unit, ax) == []\n", "    def setup(self):\n        self.cc = cat.StrCategoryConverter()\n        self.unit = cat.UnitData()\n        self.ax = FakeAxis(self.unit)\n", "def test_StrCategoryConverter_convert_with_none_unit():\n    cc = cat.StrCategoryConverter()\n    with pytest.raises(ValueError):\n        cc.convert(['a', 'b'], None, FakeAxis(None))\n"], "sample_508": ["def test_findobj():\n    fig, ax = plt.subplots()\n    ln, = ax.plot(range(5))\n\n    # Test findobj with no match\n    assert martist.Artist.findobj(ln, match=lambda x: False) == []\n\n    # Test findobj with a match on self\n    assert martist.Artist.findobj(ln, match=lambda x: True) == [ln]\n\n    # Test findobj with include_self=False\n    assert martist.Artist.findobj(ln, match=lambda x: True, include_self=False) == []\n\n    # Test findobj with a match on children\n    assert martist.Artist.findobj(fig, match=lambda x: isinstance(x, type(ax))) == [ax]\n", "def test_artist_sticky_edges():\n    art = martist.Artist()\n    assert isinstance(art.sticky_edges, martist._XYPair)\n    assert len(art.sticky_edges.x) == 0\n    assert len(art.sticky_edges.y) == 0\n\n    art.sticky_edges.x[:] = (1, 2)\n    art.sticky_edges.y[:] = (3, 4)\n    assert art.sticky_edges.x == [1, 2]\n    assert art.sticky_edges.y == [3, 4]\n\n    with pytest.raises(AttributeError):\n        art.sticky_edges = None\n", "def test_findobj():\n    fig, ax = plt.subplots()\n    ln, = ax.plot(range(5))\n    txt = ax.text(0.5, 0.5, 'Hello')\n\n    # Find all artists.\n    assert set(fig.findobj()) == {fig, fig.patch, ax, ax.patch, ln, txt}\n\n    # Find by type.\n    assert set(fig.findobj(mlines.Line2D)) == {ln}\n    assert set(fig.findobj(martist.Artist)) == {fig, fig.patch, ax, ax.patch, ln, txt}\n\n    # Find using a callable.\n        return isinstance(artist, mlines.Line2D)\n\n    assert set(fig.findobj(is_line)) == {ln}\n\n    # Test include_self=False.\n    assert set(fig.findobj(include_self=False)) == {fig.patch, ax, ax.patch, ln, txt}\n", "def test_artist_inspector_pprint_setters_rest():\n    class TestArtist(martist.Artist):\n            \"\"\"Set the artist's foo.\n\n            ACCEPTS: int\n            \"\"\"\n            pass\n\n    ai = martist.ArtistInspector(TestArtist)\n    setters_rest = ai.pprint_setters_rest()\n    assert len(setters_rest) == 7  # Length includes table header and footer.\n    assert setters_rest[3].startswith('    f : int')\n", "def test_artist_inspector_pprint_setters_rest():\n    class TestArtist(martist.Artist):\n            \"\"\"Set f.\"\"\"\n            \"\"\"Set g.\n            \n            ACCEPTS: foo or bar\n            \"\"\"\n    \n    sio = io.StringIO()\n    martist.ArtistInspector(TestArtist).pprint_setters_rest(file=sio)\n    assert sio.getvalue() == \"\"\"\\\n"], "sample_509": ["def test_microsecond_locator():\n    # Test MicrosecondLocator with various intervals\n    locator = mdates.MicrosecondLocator(interval=1000)  # millisecond interval\n    ticks = locator.tick_values(0.5, 1.5)\n    np.testing.assert_allclose(ticks, np.arange(1, 2, 1/86400))\n\n    locator = mdates.MicrosecondLocator(interval=500)  # half-millisecond interval\n    ticks = locator.tick_values(0.5, 1.5)\n    np.testing.assert_allclose(ticks, np.arange(1, 2, 0.5/86400))\n\n    locator = mdates.MicrosecondLocator(interval=1000000)  # second interval\n    ticks = locator.tick_values(0.5, 1.5)\n    np.testing.assert_allclose(ticks, np.arange(1, 2, 1))\n", "def test_date2num_datetime64_with_timezone():\n    dt = np.datetime64('2022-01-10T12:00:00', 's')\n    dt_tz = dt.astype('datetime64[s]').astype(object)\n    dt_tz = dt_tz.replace(tzinfo=datetime.timezone.utc)\n\n    np.testing.assert_equal(mdates.date2num(dt), mdates.date2num(dt_tz))\n", "def test_rrulewrapper_set():\n    rrule = mdates.rrulewrapper(dateutil.rrule.DAILY, interval=1)\n    dtstart = datetime.datetime(2017, 4, 1, 0)\n    rrule.set(dtstart=dtstart)\n    assert rrule._rrule._dtstart == dtstart\n\n    freq = dateutil.rrule.WEEKLY\n    rrule.set(freq=freq)\n    assert rrule._rrule._freq == freq\n", "def test_MicrosecondLocator():\n    locator = mdates.MicrosecondLocator()\n    assert locator._get_unit() == 1. / mdates.MUSECONDS_PER_DAY\n\n    # Test setting the interval and getting it back\n    locator = mdates.MicrosecondLocator(interval=10)\n    assert locator._get_interval() == 10\n\n    # Test nonsingular\n    assert locator.nonsingular(0, np.inf) == (10957.0, 14610.0)\n    assert locator.nonsingular(0, 1) == (0, 1)\n    assert locator.nonsingular(1, 0) == (0, 1)\n    assert locator.nonsingular(0, 0) == (-2e-6, 2e-6)\n\n    # Test tick_values\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(0, 1)\n    ticks = locator.tick_values(0, 1)\n    np.testing.assert_allclose(ticks, np.arange(0, 1, 1e-6))\n\n    # Test raise_if_exceeds\n    ticks = locator.raise_if_exceeds(ticks)\n    np.testing.assert_allclose(ticks, np.arange(0, 1, 1e-6))\n", "def test_AutoDateLocator_nonsingular():\n    locator = mdates.AutoDateLocator()\n    assert locator.nonsingular(0, np.inf) == (mdates.date2num(np.datetime64('2000-01-01')),\n                                              mdates.date2num(np.datetime64('2010-01-01')))\n    assert locator.nonsingular(0, 1) == (0, 1)\n    assert locator.nonsingular(1, 0) == (0, 1)\n    assert locator.nonsingular(0, 0) == (mdates.date2num(np.datetime64('1999-12-31')),\n                                         mdates.date2num(np.datetime64('2001-01-01')))\n"], "sample_510": ["def test_switch_backend():\n    original_backend = plt.get_backend()\n    plt.switch_backend('agg')\n    assert plt.get_backend() == 'agg'\n    plt.switch_backend(original_backend)\n    assert plt.get_backend() == original_backend\n", "def test_subplot2grid():\n    fig = plt.figure()\n    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=2)\n    ax2 = plt.subplot2grid((3, 3), (0, 2))\n    ax3 = plt.subplot2grid((3, 3), (1, 0), colspan=3)\n    ax4 = plt.subplot2grid((3, 3), (2, 0), colspan=2)\n    ax5 = plt.subplot2grid((3, 3), (2, 2))\n\n    assert len(fig.axes) == 5\n\n    # Check that all axes are created at the correct positions.\n    np.testing.assert_allclose(ax1.bbox.get_points(),\n                               [[0.125, 0.733333], [0.583333, 0.9]])\n    np.testing.assert_allclose(ax2.bbox.get_points(),\n                               [[0.583333, 0.733333], [0.9, 0.9]])\n    np.testing.assert_allclose(ax3.bbox.get_points(),\n                               [[0.125, 0.366667], [0.9, 0.633333]])\n    np.testing.assert_allclose(ax4.bbox.get_points(),\n                               [[0.125, 0.033333], [0.583333, 0.3]])\n    np.testing.assert_allclose(ax5.bbox.get_points(),\n                               [[0.583333, 0.033333], [0.9, 0.3]])\n", "def test_subplot_mosaic():\n    fig, axes_dict = plt.subplot_mosaic(\n        [['A panel', 'A panel', 'edge'],\n         ['C panel', '.',       'edge']]\n    )\n    assert len(axes_dict) == 3\n    assert set(axes_dict.keys()) == {'A panel', 'C panel', 'edge'}\n    assert all(isinstance(ax, mpl.axes.SubplotBase) for ax in axes_dict.values())\n\n    # Test that the axes are in the correct positions.\n    grid_spec = axes_dict['A panel'].get_subplotspec().get_gridspec()\n    assert grid_spec.nrows == 2\n    assert grid_spec.ncols == 3\n\n    assert axes_dict['A panel'].get_subplotspec().colspan == (0, 2)\n    assert axes_dict['A panel'].get_subplotspec().rowspan == (0, 1)\n\n    assert axes_dict['C panel'].get_subplotspec().colspan == (0, 1)\n    assert axes_dict['C panel'].get_subplotspec().rowspan == (1, 2)\n\n    assert axes_dict['edge'].get_subplotspec().colspan == (2, 3)\n    assert axes_dict['edge'].get_subplotspec().rowspan == (0, 2)\n", "def test_set_cmap():\n    cmap = plt.get_cmap('viridis')\n    plt.set_cmap(cmap)\n    assert plt.rcParams['image.cmap'] == cmap.name\n\n    # Check that setting the colormap to an invalid value raises an error.\n    with pytest.raises(ValueError):\n        plt.set_cmap('invalid_cmap')\n\n    # Check that setting the colormap to None sets it to the default.\n    plt.set_cmap(None)\n    assert plt.rcParams['image.cmap'] == 'viridis'\n", "def test_subplot_with_subplotspec():\n    fig = plt.figure()\n    gs = mpl.gridspec.GridSpec(1, 1)\n    ss = gs[0]\n    ax1 = plt.subplot(ss)\n    assert ax1 is plt.gca()\n    ax2 = plt.subplot(ss)\n    assert ax1 is ax2\n    ax1.remove()\n    ax3 = plt.subplot(ss, projection='polar')\n    assert ax3 is not ax1\n    assert ax3 is plt.gca()\n    assert ax1 not in fig.axes\n"], "sample_511": ["def test_subplot_mosaic():\n    fig, axes = plt.subplot_mosaic([['left', 'right'], ['bottom', 'bottom']])\n    assert len(axes) == 3\n    assert set(axes.keys()) == {'left', 'right', 'bottom'}\n    for key, ax in axes.items():\n        assert ax.get_label() == key\n\n    fig, axes = plt.subplot_mosaic(\n        [['A', 'B'], ['C', 'D']], empty_sentinel='X'\n    )\n    assert len(axes) == 4\n    assert set(axes.keys()) == {'A', 'B', 'C', 'D'}\n    for key, ax in axes.items():\n        assert ax.get_label() == key\n\n    fig, axes = plt.subplot_mosaic(\n        [['top', 'top'], ['bottom left', 'bottom right']]\n    )\n    assert len(axes) == 3\n    assert set(axes.keys()) == {'top', 'bottom left', 'bottom right'}\n    for key, ax in axes.items():\n        assert ax.get_label() == key\n", "def test_figure_kwargs():\n    fig = plt.figure(figsize=(8, 6), dpi=100, facecolor='white', edgecolor='black')\n    assert fig.get_figwidth() == 8\n    assert fig.get_figheight() == 6\n    assert fig.dpi == 100\n    assert fig.patch.get_facecolor() == (1.0, 1.0, 1.0, 1.0)\n    assert fig.patch.get_edgecolor() == (0.0, 0.0, 0.0, 1.0)\n", "def test_non_interactive_warning():\n    # Test that a warning is emitted when starting an interactive GUI outside\n    # of the main thread.\n    import threading\n\n        plt.plot([1, 2, 3])\n        plt.show()\n\n    with pytest.warns(UserWarning, match=\"Starting a Matplotlib GUI outside\"):\n        thread = threading.Thread(target=target)\n        thread.start()\n        thread.join()\n", "def test_non_interactive_warning():\n    # Ensure that a warning is emitted if a GUI backend is used in a\n    # non-interactive context.\n    with pytest.warns(UserWarning, match=\"Starting a Matplotlib GUI outside\"):\n        plt.switch_backend(\"TkAgg\")\n    plt.switch_backend(\"Agg\")  # Reset to a non-GUI backend.\n", "def test_subplot_mosaic_reuses_axes():\n    fig, axd = plt.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    assert axd['A'] is plt.subplot(2, 2, 1)\n    assert axd['B'] is plt.subplot(2, 2, 2)\n    assert axd['C'] is plt.subplot(2, 2, 3)\n    assert axd['D'] is plt.subplot(2, 2, 4)\n\n    fig, axd = plt.subplot_mosaic([['A', 'B'], ['C', 'D']], sharex=True)\n    assert axd['A'] is plt.subplot(2, 2, 1, sharex=axd['A'])\n    assert axd['B'] is plt.subplot(2, 2, 2, sharex=axd['B'])\n    assert axd['C'] is plt.subplot(2, 2, 3, sharex=axd['A'])\n    assert axd['D'] is plt.subplot(2, 2, 4, sharex=axd['B'])\n\n    fig, axd = plt.subplot_mosaic([['A', 'B'], ['C', 'D']], sharey=True)\n    assert axd['A'] is plt.subplot(2, 2, 1, sharey=axd['A'])\n    assert axd['B'] is plt.subplot(2, 2, 2, sharey=axd['B'])\n    assert axd['C'] is plt.subplot(2, 2, 3, sharey=axd['A'])\n    assert axd['D'] is plt.subplot(2, 2, 4, sharey=axd['B'])\n"], "sample_512": ["def test_gcf_returns_none():\n    # Test that gcf() returns None when there are no figures.\n    plt.close('all')\n    assert plt.gcf() is None\n\n    # Test that gcf() returns the figure instance after creating a figure.\n    fig = plt.figure()\n    assert plt.gcf() is fig\n\n    # Test that gcf() returns None after closing all figures again.\n    plt.close('all')\n    assert plt.gcf() is None\n", "def test_figure_kwargs():\n    # Test that figure kwargs are passed to the Figure constructor.\n    fig = plt.figure(figsize=(8, 6), dpi=100)\n    assert fig.get_size_inches() == (8, 6)\n    assert fig.dpi == 100\n\n    # Test that figure kwargs can be used with num.\n    fig = plt.figure(1, figsize=(8, 6), dpi=100)\n    assert fig.get_size_inches() == (8, 6)\n    assert fig.dpi == 100\n\n    # Test that figure kwargs can be used when creating a new figure.\n    fig = plt.figure(figsize=(8, 6), dpi=100)\n    assert fig.get_size_inches() == (8, 6)\n    assert fig.dpi == 100\n", "def test_subplot_mosaic():\n    fig, axes = plt.subplot_mosaic([['a', 'b'], ['c', 'd']])\n    assert len(axes) == 4\n    for ax in axes.values():\n        assert isinstance(ax, mpl.axes.Axes)\n    assert axes['a'] is not axes['b']\n    assert axes['a'] is not axes['c']\n    assert axes['a'] is not axes['d']\n\n    fig, axes = plt.subplot_mosaic(\n        [['a', 'b'], ['c', 'd']], subplot_kw={'projection': 'polar'})\n    for ax in axes.values():\n        assert isinstance(ax, mpl.projections.polar.PolarAxes)\n\n    with pytest.raises(ValueError):\n        plt.subplot_mosaic('not a list or array')\n", "def test_autoscale():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    ax.autoscale(enable=True)\n    assert ax.get_xlim() == (1.0, 3.0)\n    assert ax.get_ylim() == (1.0, 3.0)\n    ax.autoscale(enable=False)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.autoscale(enable=True)\n    assert ax.get_xlim() == (1.0, 3.0)\n    assert ax.get_ylim() == (1.0, 3.0)\n", "def test_xkcd():\n    with plt.xkcd():\n        fig, ax = plt.subplots()\n        assert 'xkcd' in fig.get_facecolor()\n        assert ax.spines['bottom'].get_linewidth() == 1.5\n    fig, ax = plt.subplots()\n    assert 'xkcd' not in fig.get_facecolor()\n    assert ax.spines['bottom'].get_linewidth() == 0.8\n"], "sample_513": ["def test_legend_title_fontproperties():\n    # Test that fontproperties are correctly applied to legend title\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    title_fontprops = FontProperties(family='serif', style='italic')\n    leg = ax.legend(title='Legend Title', title_fontproperties=title_fontprops)\n\n    assert leg.get_title().get_family()[0] == 'serif'\n    assert leg.get_title().get_style() == 'italic'\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n\n    # Test setting bbox_to_anchor with a BboxBase instance\n    bb = mtransforms.Bbox.from_bounds(0.5, 0.5, 0.2, 0.2)\n    leg.set_bbox_to_anchor(bb)\n    assert leg._bbox_to_anchor == bb\n\n    # Test setting bbox_to_anchor with a tuple of four floats\n    leg.set_bbox_to_anchor((0.5, 0.5, 0.2, 0.2))\n    assert isinstance(leg._bbox_to_anchor, mtransforms.BboxBase)\n\n    # Test setting bbox_to_anchor with a tuple of two floats\n    leg.set_bbox_to_anchor((0.5, 0.5))\n    assert isinstance(leg._bbox_to_anchor, mtransforms.BboxBase)\n\n    # Test setting bbox_to_anchor to None\n    leg.set_bbox_to_anchor(None)\n    assert leg._bbox_to_anchor is None\n", "def test_legend_set_bbox_to_anchor_transform():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], label='line')\n    leg = ax.legend(handles=[line])\n    leg.set_bbox_to_anchor((0.5, 0.5), transform=ax.transAxes)\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n", "def test_legend_title_empty_string():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(title=\"\")\n    assert leg.get_title().get_text() == \"\"\n    assert not leg.get_title().get_visible()\n", "def test_legend_bbox_to_anchor():\n    # Test that legend can be placed using bbox_to_anchor argument.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    assert leg.get_bbox_to_anchor().x0 == 1.05\n    assert leg.get_bbox_to_anchor().y0 == 1\n"], "sample_514": ["def test_colorbar_with_alpha():\n    np.random.seed(seed=19680808)\n    fig, ax = plt.subplots(figsize=(2, 2))\n    pc = ax.pcolormesh(np.random.randn(10, 10), cmap='RdBu_r', alpha=0.5)\n    cb = fig.colorbar(pc, ax=ax)\n", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    sm = cm.ScalarMappable(norm=mcolors.Normalize(), cmap='viridis')\n    cb = fig.colorbar(sm)\n    cb.set_alpha(0.5)\n    assert cb.alpha == 0.5\n    cb.set_alpha([0.2, 0.3])\n    assert cb.alpha is None\n", "def test_colorbar_loglocator():\n    fig, ax = plt.subplots()\n    data = np.logspace(-3, 3, 100)\n    im = ax.imshow(data.reshape(1, -1), cmap='viridis', norm=LogNorm())\n    cb = fig.colorbar(im)\n    cb.set_ticks([1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3])\n    fig.draw_without_rendering()\n    # Check that the ticks are correctly placed and labeled.\n    np.testing.assert_allclose(cb.ax.yaxis.get_ticklocs(),\n                               np.log10([1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]))\n    ticklabels = [label.get_text() for label in cb.ax.yaxis.get_ticklabels()]\n    np.testing.assert_array_equal(ticklabels,\n                                  ['$\\\\mathdefault{10^{-3}}$',\n                                   '$\\\\mathdefault{10^{-2}}$',\n                                   '$\\\\mathdefault{10^{-1}}$',\n                                   '$\\\\mathdefault{10^{0}}$',\n                                   '$\\\\mathdefault{10^{1}}$',\n                                   '$\\\\mathdefault{10^{2}}$',\n                                   '$\\\\mathdefault{10^{3}}$'])\n", "def test_colorbar_set_ticks():\n    # check that the set_ticks method works as expected:\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    ticks = np.arange(0, 2, 0.25)\n    cb.set_ticks(ticks)\n    assert np.array_equal(cb.get_ticks(), ticks)\n\n    # check that setting ticks with labels works as expected:\n    labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n    cb.set_ticks(ticks, labels=labels)\n    assert np.array_equal(cb.get_ticks(), ticks)\n    for i, label in enumerate(labels):\n        assert cb.ax.yaxis.get_ticklabels()[i].get_text() == label\n\n    # check that setting ticks with a FixedLocator works as expected:\n    loc = FixedLocator(ticks)\n    cb.locator = loc\n    assert np.array_equal(cb.get_ticks(), ticks)\n\n    # check that setting minor ticks works as expected:\n    minor_ticks = np.arange(0, 2, 0.05)\n    cb.set_ticks(minor_ticks, minor=True)\n    assert np.array_equal(cb.get_ticks(minor=True), minor_ticks)\n\n    # check that setting minor ticks with a FixedLocator works as expected:\n    minor_loc = FixedLocator(minor_ticks)\n    cb.minorlocator = minor_loc\n    assert np.array_equal(cb.get_ticks(minor=True), minor_ticks)\n", "def test_drawedges():\n    plt.rcParams['svg.fonttype'] = 'none'\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    fig, ax = plt.subplots(figsize=(6, 1))\n    fig.subplots_adjust(bottom=0.5)\n\n    cmap = cm.get_cmap(\"viridis\", len(levels) - 1)\n    norm = BoundaryNorm(levels, cmap.N)\n    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n    cbar = fig.colorbar(mappable, cax=ax, orientation=\"horizontal\",\n                        drawedges=True)\n"], "sample_515": ["def test_colorbar_no_ticks():\n    plt.rcParams['svg.fonttype'] = 'none'\n    data = np.random.randn(10, 10)\n    fig, ax = plt.subplots()\n    im = ax.imshow(data)\n    fig.colorbar(im, ticks=[])\n", "def test_colorbar_with_non_default_alpha():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]], alpha=0.5)\n    fig.colorbar(im)\n    np.testing.assert_allclose(fig.colorbar(im).alpha, 0.5)\n\n    # Test when the mappable has an array of alphas\n    im.set_alpha(np.array([[0.5, 0.6], [0.7, 0.8]]))\n    fig.colorbar(im)\n    assert fig.colorbar(im).alpha is None\n", "def test_colorbar_remove_from_axes():\n    fig, ax = plt.subplots()\n    cbar = fig.colorbar(cm.ScalarMappable(), cax=ax)\n    cbar.remove()\n    assert cbar.ax not in fig.axes\n", "def test_colorbar_no_ticks():\n    plt.rcParams['svg.fonttype'] = 'none'\n    data = [1, 2, 3, 4, 5]\n\n    fig, ax = plt.subplots(figsize=(6, 1))\n    fig.subplots_adjust(bottom=0.5)\n\n    cmap = mpl.colormaps[\"viridis\"].resampled(len(data))\n    mappable = cm.ScalarMappable(cmap=cmap)\n    mappable.set_array(data)\n    cbar = fig.colorbar(mappable, cax=ax, orientation=\"horizontal\")\n    cbar.set_ticks([])\n", "def test_colorbar_update_ticks():\n    # test that update_ticks does not raise an exception when\n    # minorlocator is None.\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.minorlocator = None\n    cb.update_ticks()\n"], "sample_516": ["def test_encode_string():\n    pdf_file = PdfFile(io.BytesIO())\n    assert pdf_file.encode_string('a', 3) == b'a'\n    assert pdf_file.encode_string('', 42) == b''\n    assert pdf_file.encode_string('\u20ac', 1) == b'\\x80'\n    assert pdf_file.encode_string('\u20ac', 3) == b'\\x20\\xac'\n    assert pdf_file.encode_string('\u20ac', 42) == b'\\x20\\xac'\n", "def test_multi_byte_charprocs():\n    \"\"\"Test for correct rendering of multi-byte characters.\"\"\"\n    fig = plt.figure()\n    fig.text(0.5, 0.5, '\\u4f60\\u597d', ha='center')\n    rcParams['pdf.fonttype'] = 3\n    fig.savefig(io.BytesIO(), format=\"pdf\")\n", "def test_pdf_pages_close_multiple_times():\n    pdf = PdfPages(io.BytesIO())\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    pdf.savefig(fig)\n    pdf.close()\n    # Closing again should not raise an exception.\n    pdf.close()\n", "def test_pdf_line_dash():\n    \"\"\"Test for correct line dash style in PDF output\"\"\"\n    fig, ax = plt.subplots()\n    for i, (offset, dash) in enumerate([(0, [1, 2]), (2, [3, 4])]):\n        ax.plot([0, 1], [i, i], linestyle=(offset, dash), linewidth=5)\n    ax.set_ylim(-1, 2)\n    ax.set_xlim(-1, 2)\n", "def test_cidfont():\n    \"\"\"Test for CID font embedding\"\"\"\n    fig = plt.figure()\n    s = \"\u20ac\u03bc\"\n    fig.text(0, .5, s, size=20, fontname='DejaVu Sans')\n    rcParams['pdf.fonttype'] = 42\n"], "sample_517": ["def test_text_set_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    text.set_bbox(dict(facecolor='red', alpha=0.5))\n    assert text.get_bbox_patch() is not None\n    assert text.get_bbox_patch().get_facecolor()[0] == (1.0, 0.0, 0.0, 0.5)\n", "def test_set_text():\n    text = Text(0, 0, 'initial')\n    assert text.get_text() == 'initial'\n    text.set_text('new text')\n    assert text.get_text() == 'new text'\n    text.set_text(None)\n    assert text.get_text() == ''\n", "def test_update_bbox_position_size():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    renderer = fig.canvas.get_renderer()\n    text.update_bbox_position_size(renderer)\n    assert text.get_bbox_patch() is None\n\n    text.set_bbox(dict(facecolor='red'))\n    text.update_bbox_position_size(renderer)\n    assert isinstance(text.get_bbox_patch(), mpatches.FancyBboxPatch)\n", "def test_text_rotation_mode():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, 'hello', rotation=45, rotation_mode='default',\n            ha='center', va='center', size=30)\n    ax.text(0.7, 0.5, 'hello', rotation=45, rotation_mode='anchor',\n            ha='center', va='center', size=30)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_get_rotation_mode():\n    text = Text(0, 0, 'test')\n    assert text.get_rotation_mode() is None\n\n    text.set_rotation_mode('anchor')\n    assert text.get_rotation_mode() == 'anchor'\n\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n"], "sample_518": ["def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n", "def test_patch_scale_hatch():\n    fig, ax = plt.subplots()\n    rect1 = Rectangle((0.1, 0.1), 0.8, 0.8, hatch='/', transform=ax.transAxes)\n    rect2 = Rectangle((0.1, 0.1), 0.8, 0.8, hatch='/', transform=ax.transAxes)\n    rect2.set_transform(rect2.get_transform() + transforms.ScaledTranslation(1, 0, fig.dpi_scale_trans))\n    ax.add_patch(rect1)\n    ax.add_patch(rect2)\n", "def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n", "def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n", "def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n"], "sample_519": ["def test_subfigure_repr():\n    fig = Figure()\n    subfig = fig.subfigures(1)[0]\n    assert repr(subfig) == \"<SubFigure size 640x480 with 0 Axes>\"\n    ax = subfig.subplots()\n    assert repr(subfig) == \"<SubFigure size 640x480 with 1 Axes>\"\n", "def test_add_artist_without_transform():\n    fig, ax = plt.subplots()\n    line = mpl.lines.Line2D([0, 1], [0, 1])\n    fig.add_artist(line)\n    assert line.get_transform() == fig.transSubfigure\n", "def test_subfigure_repr():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1)\n    assert repr(sub_fig[0]) == f\"<{sub_fig[0].__class__.__name__} size {fig.get_size_inches()[0]:g}x{fig.get_size_inches()[1]:g} with 0 Axes>\"\n", "def test_set_layout_engine():\n    fig = plt.figure()\n    assert fig.get_layout_engine() is None\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    class CustomLayoutEngine:\n        pass\n\n    fig.set_layout_engine(CustomLayoutEngine())\n    assert isinstance(fig.get_layout_engine(), CustomLayoutEngine)\n\n    with pytest.raises(ValueError):\n        fig.set_layout_engine('unknown')\n\n    with pytest.raises(TypeError):\n        fig.set_layout_engine(123)\n", "def test_pickle():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1)\n    ax = sub_fig.subplots()\n\n    fig_pickled = pickle.loads(pickle.dumps(fig))\n    assert len(fig_pickled.axes) == 1\n    assert isinstance(fig_pickled.axes[0], Axes)\n"], "sample_520": ["def test_line_collection_2d_to_3d(fig_test, fig_ref):\n    # Create a LineCollection and convert it to 3D.\n    lc = LineCollection([[[0, 0], [1, 1]], [[1, 1], [2, 2]]])\n    art3d.line_collection_2d_to_3d(lc, zs=1)\n\n    ax_test = fig_test.add_subplot(projection=\"3d\")\n    ax_test.add_collection3d(lc)\n\n    # Manually create a Line3DCollection for reference.\n    segments_3d = _paths_to_3d_segments_with_codes(\n        lc.get_paths(), zs=1, zdir='z'\n    )\n    lc_ref = art3d.Line3DCollection(segments_3d, colors=lc.get_colors())\n\n    ax_ref = fig_ref.add_subplot(projection=\"3d\")\n    ax_ref.add_collection3d(lc_ref)\n", "def test_line3d_collection_modification(fig_test, fig_ref):\n    # Modifying the Line3DCollection properties after creation should work correctly.\n    x, y, z = [0, 1], [2, 3], [4, 5]\n    c = art3d.Line3DCollection([list(zip(x, y, z))])\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(c)\n    c.set_color('C1')\n    c.set_linewidths(3)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.add_collection3d(art3d.Line3DCollection([list(zip(x, y, z))],\n                                                  colors='C1', linewidths=3))\n", "def test_line_3d(fig_test, fig_ref):\n    ax = fig_ref.add_subplot(projection=\"3d\")\n    line = lines.Line2D([0.5, 1], [0.5, 1])\n    art3d.line_2d_to_3d(line, zs=1)\n    ax.add_artist(line)\n    assert line.get_data_3d() == ([0.5, 1], [0.5, 1], [1, 1])\n\n    ax = fig_test.add_subplot(projection=\"3d\")\n    l3d = art3d.Line3D([0.5, 1], [0.5, 1], [1, 1])\n    ax.add_artist(l3d)\n    assert l3d.get_data_3d() == ([0.5, 1], [0.5, 1], [1, 1])\n", "def test_line_3d(fig_test, fig_ref):\n    ax = fig_ref.add_subplot(projection=\"3d\")\n    line = lines.Line2D([0.5, 1], [0.5, 1])\n    art3d.line_2d_to_3d(line, zs=1)\n    ax.add_artist(line)\n    assert line.get_data_3d() == ([0.5, 1], [0.5, 1], [1, 1])\n\n    ax = fig_test.add_subplot(projection=\"3d\")\n    l3d = art3d.Line3D([0.5, 1], [0.5, 1], [1, 1])\n    ax.add_artist(l3d)\n    assert l3d.get_data_3d() == ([0.5, 1], [0.5, 1], [1, 1])\n", "def test_line_collection_3d(fig_test, fig_ref):\n    x = np.array([0, 1, 2, 3, 4])\n    y = np.array([0, 1, 2, 3, 4])\n    z = np.array([0, 1, 2, 3, 4])\n\n    points = np.array([x, y, z]).T.reshape(-1, 1, 3)\n    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n\n    fig_test = plt.figure()\n    ax_test = fig_test.add_subplot(projection='3d')\n    lc_test = LineCollection(segments[:, :, :2])\n    lc_test.set_array(z[:-1])\n    art3d.line_collection_2d_to_3d(lc_test, zs=z[:-1], zdir='z')\n    ax_test.add_collection3d(lc_test)\n\n    fig_ref = plt.figure()\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    lc_ref = art3d.Line3DCollection(segments, array=z[:-1])\n    ax_ref.add_collection3d(lc_ref)\n"], "sample_521": ["def test_line3d_collection_modification(fig_test, fig_ref):\n    # Test that modifying Line3DCollection properties after creation works.\n    xs = np.array([0, 1])\n    ys = np.array([0, 1])\n    zs = np.array([0, 1])\n    segments = [(xs, ys, zs)]\n    c = art3d.Line3DCollection(segments, linewidths=3)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(c)\n    c.set_edgecolor('C2')\n    c.set_alpha(0.7)\n    assert c.get_depthshade()\n    c.set_depthshade(False)\n    assert not c.get_depthshade()\n\n    xs = np.array([0, 1])\n    ys = np.array([0, 1])\n    zs = np.array([0, 1])\n    segments = [(xs, ys, zs)]\n    c = art3d.Line3DCollection(segments, edgecolor='C2', alpha=0.7,\n                               depthshade=False)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.add_collection3d(c)\n", "def test_line3d_collection_modification(fig_test, fig_ref):\n    # Test that modifying Line3DCollection properties after creation works.\n    x = [0, 1]\n    y = [2, 3]\n    z = [4, 5]\n    c = art3d.Line3DCollection([list(zip(x, y, z))], linewidths=3)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(c)\n    c.set_edgecolor('C2')\n    c.set_alpha(0.7)\n    assert c.get_depthshade()\n    c.set_depthshade(False)\n    assert not c.get_depthshade()\n\n    x = [0, 1]\n    y = [2, 3]\n    z = [4, 5]\n    c = art3d.Line3DCollection([list(zip(x, y, z))], linewidths=3,\n                               edgecolor='C2', alpha=0.7, depthshade=False)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.add_collection3d(c)\n", "def test_line_2d_to_3d(fig_test, fig_ref):\n    ax = fig_ref.add_subplot(projection=\"3d\")\n    line = lines.Line2D([0, 1], [0, 1])\n    art3d.line_2d_to_3d(line, zs=1)\n    ax.add_artist(line)\n\n    ax = fig_test.add_subplot(projection=\"3d\")\n    l3d = art3d.Line3D([0, 1], [0, 1], [1, 1])\n    ax.add_artist(l3d)\n", "def test_line_3d(fig_test, fig_ref):\n    ax = fig_ref.add_subplot(projection=\"3d\")\n    line = lines.Line2D([0, 1], [0, 1])\n    art3d.line_2d_to_3d(line, zs=1)\n    ax.add_artist(line)\n    assert line.get_data_3d() == ([0, 1], [0, 1], [1, 1])\n\n    ax = fig_test.add_subplot(projection=\"3d\")\n    l3d = art3d.Line3D([0, 1], [0, 1], [1, 1])\n    ax.add_artist(l3d)\n    assert l3d.get_data_3d() == ([0, 1], [0, 1], [1, 1])\n", "def test_line_collection_3d(fig_test, fig_ref):\n    ax = fig_ref.add_subplot(projection=\"3d\")\n    points = np.array([[[0, 0], [1, 1]], [[0, 1], [1, 0]]])\n    lc = LineCollection(points)\n    art3d.line_collection_2d_to_3d(lc, zs=1)\n    ax.add_artist(lc)\n\n    ax = fig_test.add_subplot(projection=\"3d\")\n    points3d = np.array([[[0, 0, 1], [1, 1, 1]], [[0, 1, 1], [1, 0, 1]]])\n    lc3d = art3d.Line3DCollection(points3d)\n    ax.add_artist(lc3d)\n"], "sample_522": ["def test_subfigure_add_colorbar():\n    fig = plt.figure()\n    sfigs = fig.subfigures(1, 2)\n    ax = sfigs[0].subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cbar = sfigs[0].colorbar(pc)\n    assert cbar.ax.get_figure() is fig\n    assert cbar.ax.get_parent_subplot() is sfigs[0]\n", "def test_colorbar_with_vmin_vmax():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    cs = ax.contourf(data, levels=levels)\n    cbar = plt.colorbar(cs, ax=ax, orientation='vertical')\n    assert cbar.vmin == levels[0]\n    assert cbar.vmax == levels[-1]\n\n    # Test that setting vmin and vmax changes the ticks.\n    cbar.set_clim(vmin=100, vmax=1100)\n    assert cbar.vmin == 100\n    assert cbar.vmax == 1100\n\n    # Test that changing the norm changes the ticks.\n    norm = Normalize(vmin=200, vmax=1000)\n    cs.set_norm(norm)\n    assert cbar.vmin == 200\n    assert cbar.vmax == 1000\n", "def test_subfigure_add_subplot():\n    fig = plt.figure()\n    subfig = fig.subfigures(1, 2)[0]\n    ax = subfig.add_subplot(111)\n    assert ax in subfig.axes\n    assert ax not in fig.axes\n    assert ax.get_figure() == fig\n", "def test_colorbar_with_subplots(fig_ref, fig_test):\n    # Test that colorbars work correctly with subplots.\n    x = np.linspace(0, 1, 100)\n    y = np.sin(x)\n\n    for fig in [fig_ref, fig_test]:\n        ax1 = fig.add_subplot(121)\n        ax2 = fig.add_subplot(122)\n        im1 = ax1.imshow([[1, 2], [3, 4]])\n        im2 = ax2.imshow([[5, 6], [7, 8]])\n\n        if fig is fig_test:\n            fig.subplots_adjust(right=0.8)\n            cax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n            fig.colorbar(im1, cax=cax)\n            fig.colorbar(im2, cax=cax)\n        else:\n            fig.colorbar(im1, ax=ax1)\n            fig.colorbar(im2, ax=ax2)\n", "def test_figure_suptitle():\n    fig, ax = plt.subplots()\n    suptitle = fig.suptitle('Test Title')\n    assert suptitle.get_text() == 'Test Title'\n    assert suptitle.get_position() == (0.5, 0.98)\n    fig.suptitle('New Title', x=0.4, y=0.9)\n    assert suptitle.get_text() == 'New Title'\n    assert suptitle.get_position() == (0.4, 0.9)\n"], "sample_523": ["def test_legend_handles_labels_from_ax():\n    # Test that legend handles and labels are correctly retrieved from ax\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='line')\n    ax.scatter([1, 2, 3], [2, 3, 4], label='scatter')\n    handles, labels = ax.get_legend_handles_labels()\n    assert len(handles) == 2\n    assert len(labels) == 2\n    assert isinstance(handles[0], mlines.Line2D)\n    assert isinstance(handles[1], mcollections.PathCollection)\n    assert labels == ['line', 'scatter']\n", "def test_legend_empty_handles():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError):\n        ax.legend(handles=[])\n", "def test_legend_scatterpoints():\n    # Test that scatterpoints parameter works correctly\n    fig, ax = plt.subplots()\n    ax.scatter([0, 1], [0, 1], label=\"scatter\")\n    leg = ax.legend(scatterpoints=3)\n    assert len(leg.legendHandles[0].get_data()[0]) == 3\n", "def test_legend_bbox_to_anchor():\n    # Test legend's bbox_to_anchor argument with different types of input.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    \n    # Test with a tuple of two floats.\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5))\n    assert isinstance(leg.get_bbox_to_anchor(), mtransforms.BboxBase)\n    \n    # Test with a tuple of four floats.\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5, 0.2, 0.2))\n    assert isinstance(leg.get_bbox_to_anchor(), mtransforms.BboxBase)\n    \n    # Test with a BboxBase instance.\n    bbox = mtransforms.Bbox.from_bounds(0.5, 0.5, 0.2, 0.2)\n    leg = ax.legend(bbox_to_anchor=bbox)\n    assert leg.get_bbox_to_anchor() is bbox\n    \n    # Test with a Transform instance.\n    trans = mtransforms.Affine2D().translate(0.5, 0.5) + ax.transAxes\n    leg = ax.legend(bbox_to_anchor=(0, 0), bbox_transform=trans)\n    assert isinstance(leg.get_bbox_to_anchor(), mtransforms.BboxBase)\n", "def test_legend_draggable_get_set():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    legend = ax.legend()\n    assert not legend.get_draggable()\n    legend.set_draggable(True)\n    assert legend.get_draggable()\n    legend.set_draggable(False)\n    assert not legend.get_draggable()\n"], "sample_524": ["def test_subfigure_colorbar():\n    # Test that a colorbar can be added to a subfigure.\n    fig = plt.figure()\n    sfigs = fig.subfigures(1, 2)\n    axsL = sfigs[0].subplots(1, 2)\n    axsR = sfigs[1].subplots(2, 1)\n\n    pc = axsL[0].pcolormesh(np.random.randn(10, 10))\n    sfigs[0].colorbar(pc, ax=axsL)\n\n    pc = axsR[0].pcolormesh(np.random.randn(10, 10))\n    sfigs[1].colorbar(pc, ax=axsR)\n", "def test_colorbar_renorm_with_ticklabels():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im)\n    cbar.ax.set_yticklabels(['a', 'b', 'c'])\n    im.set_norm(LogNorm(vmin=0.1, vmax=10))\n    fig.draw_without_rendering()\n    assert len(cbar.ax.yaxis.get_ticklabels()) > 0\n", "def test_subplots_adjust():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    fig.subplots_adjust(top=0.8)\n    assert np.isclose(fig.subplotpars.top, 0.8)\n", "def test_colorbar_remove_add_norm():\n    fig, ax = plt.subplots()\n    norm = Normalize(vmin=0, vmax=1)\n    cmap = mpl.colormaps[\"viridis\"]\n    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n    cb = fig.colorbar(mappable, cax=ax)\n\n    # remove and add the same norm should not raise an exception\n    cb.remove()\n    cb.norm = norm\n\n    # check that the norm is properly set on the colorbar axes\n    assert cb.ax.yaxis.get_major_locator() is not None\n", "def test_colorbar_no_mappable():\n    fig, ax = plt.subplots(figsize=(2, 2))\n    cmap = mpl.colormaps[\"viridis\"].resampled(5)\n    cb = fig.colorbar(cm.ScalarMappable(cmap=cmap), cax=ax, orientation=\"vertical\")\n    cb.set_ticks([0.1, 0.3, 0.5, 0.7, 0.9])\n    cb.set_ticklabels([\"a\", \"b\", \"c\", \"d\", \"e\"])\n"], "sample_525": ["def test_pickle_subfigure():\n    fig = plt.figure()\n    subfig = fig.subfigures(1, 1)\n    ax = subfig[0].subplots()\n\n    buffer = io.BytesIO()\n    pickle.dump(fig, buffer)\n\n    buffer.seek(0)\n    loaded_fig = pickle.load(buffer)\n\n    assert isinstance(loaded_fig, Figure)\n    assert len(loaded_fig.subfigs) == 1\n    assert isinstance(loaded_fig.subfigs[0], SubFigure)\n    assert len(loaded_fig.subfigs[0].axes) == 1\n", "def test_subfigure_pickling():\n    fig = plt.figure()\n    subfig = fig.subfigures(1, 1)[0]\n    ax = subfig.add_subplot(111)\n    ax.plot([1, 2, 3])\n    data = pickle.dumps(fig)\n    fig2 = pickle.loads(data)\n    assert len(fig2.axes) == 1\n    assert fig2.axes[0].lines[0].get_xydata().tolist() == [[1, 2, 3], [1, 2, 3]]\n", "def test_subfigure_remove():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1)\n    assert sub_fig in fig.subfigs\n    sub_fig.remove()\n    assert sub_fig not in fig.subfigs\n", "def test_set_layout_engine_none():\n    fig, ax = plt.subplots(layout=\"constrained\")\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    fig.set_layout_engine(None)\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n    assert fig.get_layout_engine().adjust_compatible is False\n    fig.set_layout_engine(\"tight\")\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    fig.set_layout_engine(None)\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n    assert fig.get_layout_engine().adjust_compatible is True\n", "def test_subfigure_repr():\n    fig = plt.figure()\n    subfig = fig.subfigures(1, 1)[0]\n    assert repr(subfig) == f\"<SubFigure size {subfig.get_size_inches()[0]:g}x{subfig.get_size_inches()[1]:g} with 0 Axes>\"\n"], "sample_526": ["def test_rrulewrapper_until():\n    rrule = mdates.rrulewrapper(dateutil.rrule.YEARLY, until=datetime.datetime(2020, 1, 1))\n    assert rrule._rrule.until == datetime.datetime(2020, 1, 1)\n    rrule.set(until=datetime.datetime(2021, 1, 1))\n    assert rrule._rrule.until == datetime.datetime(2021, 1, 1)\n", "def test_date2num_roundtrip():\n    dates = [datetime.datetime(2022, 1, 1),\n             datetime.datetime(2022, 12, 31, 23, 59, 59, 999999)]\n    nums = mdates.date2num(dates)\n    roundtripped_dates = mdates.num2date(nums)\n    for date, roundtripped_date in zip(dates, roundtripped_dates):\n        assert date == roundtripped_date\n", "def test_RRuleLocator_viewlim_to_dt():\n    locator = mdates.RRuleLocator(mdates.rrulewrapper(dateutil.rrule.DAILY))\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(0, 10)\n    dmin, dmax = locator.viewlim_to_dt()\n    assert isinstance(dmin, datetime.datetime)\n    assert isinstance(dmax, datetime.datetime)\n", "def test_MicrosecondLocator():\n    locator = mdates.MicrosecondLocator(interval=100)\n    assert locator._get_unit() == 1. / mdates.MUSECONDS_PER_DAY\n    assert locator._get_interval() == 100\n\n    # Ensure tick values are correctly generated\n    view_start = datetime.datetime(2000, 1, 1, 0, 0, 0)\n    view_end = datetime.datetime(2000, 1, 1, 0, 0, 1)\n    ticks = locator.tick_values(view_start, view_end)\n    expected_ticks = np.arange(mdates.date2num(view_start),\n                               mdates.date2num(view_end), 100 / mdates.MUSECONDS_PER_DAY)\n    np.testing.assert_array_equal(ticks, expected_ticks)\n\n    # Ensure that out-of-bounds dates do not cause errors\n    d1 = datetime.datetime(year=1, month=1, day=1, tzinfo=mdates.UTC)\n    d2 = datetime.datetime(year=1, month=1, day=16, tzinfo=mdates.UTC)\n    locator.tick_values(d1, d2)\n", "def test_date2num_dtype():\n    # Test date2num with different input types and shapes.\n    import numpy as np\n\n    # datetime object\n    dt = datetime.datetime(2022, 1, 1)\n    assert mdates.date2num(dt) == 18993.0\n\n    # numpy datetime64 array\n    dt64 = np.array([dt], dtype='datetime64[ns]')\n    assert mdates.date2num(dt64) == 18993.0\n\n    # list of datetime objects\n    dt_list = [dt, dt]\n    assert np.all(mdates.date2num(dt_list) == [18993.0, 18993.0])\n\n    # numpy array of datetime objects\n    dt_array = np.array(dt_list)\n    assert np.all(mdates.date2num(dt_array) == [18993.0, 18993.0])\n"], "sample_527": ["def test_subfigure():\n    fig = plt.figure()\n    sf1, sf2 = fig.subfigures(1, 2)\n    ax11, ax12 = sf1.subplots(1, 2)\n    ax21, ax22 = sf2.subplots(1, 2)\n\n    assert ax11.get_figure() is fig\n    assert ax12.get_figure() is fig\n    assert ax21.get_figure() is fig\n    assert ax22.get_figure() is fig\n\n    assert ax11.get_subplotspec().get_gridspec() is not None\n    assert ax12.get_subplotspec().get_gridspec() is not None\n    assert ax21.get_subplotspec().get_gridspec() is not None\n    assert ax22.get_subplotspec().get_gridspec() is not None\n", "def test_figure_repr():\n    fig = Figure()\n    assert re.match(\n        r\"<Figure size \\d+\\.\\d+x\\d+\\.\\d+ with 0 Axes>\",\n        repr(fig)\n    )\n    ax = fig.add_subplot(111)\n    assert re.match(\n        r\"<Figure size \\d+\\.\\d+x\\d+\\.\\d+ with 1 Axes>\",\n        repr(fig)\n    )\n", "def test_figure_add_subplot():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    assert len(fig.axes) == 1\n    assert ax in fig.axes\n\n    # Add a subplot with a different projection\n    ax2 = fig.add_subplot(122, projection='polar')\n    assert len(fig.axes) == 2\n    assert ax2 in fig.axes\n\n    # Try to add an axes that already exists in the figure\n    with pytest.raises(ValueError):\n        fig.add_subplot(ax)\n\n    # Try to add an axes that belongs to a different figure\n    fig2 = plt.figure()\n    ax3 = fig2.add_subplot(111)\n    with pytest.raises(ValueError):\n        fig.add_subplot(ax3)\n", "def test_subfigure_add_remove():\n    fig = plt.figure()\n    subfigs = fig.subfigures(1, 2)\n    axs1 = subfigs[0].subplots(1, 2)\n    axs2 = subfigs[1].subplots(2, 1)\n\n    assert len(fig.axes) == 4\n    assert len(subfigs[0].axes) == 2\n    assert len(subfigs[1].axes) == 2\n\n    # Remove axes from subfigure\n    subfigs[0].delaxes(axs1[0])\n    assert len(fig.axes) == 3\n    assert len(subfigs[0].axes) == 1\n    assert len(subfigs[1].axes) == 2\n\n    # Remove subfigure\n    fig.delaxes(subfigs[1])\n    assert len(fig.axes) == 1\n    assert len(subfigs[0].axes) == 1\n", "def test_figure_repr():\n    fig = plt.figure(figsize=(8, 6), dpi=100)\n    expected_repr = (f\"<{type(fig).__name__} size {fig.get_size_inches()[0]:g}x\"\n                     f\"{fig.get_size_inches()[1]:g} with 0 Axes>\")\n    assert repr(fig) == expected_repr\n\n    ax = fig.add_subplot()\n    expected_repr = (f\"<{type(fig).__name__} size {fig.get_size_inches()[0]:g}x\"\n                     f\"{fig.get_size_inches()[1]:g} with 1 Axes>\")\n    assert repr(fig) == expected_repr\n"], "sample_528": ["def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library_clears_deprecated_styles():\n    # Ensure that reloading the library clears any deprecated styles\n    assert \"seaborn-bright\" not in style.library\n    with pytest.warns(mpl._api.MatplotlibDeprecationWarning):\n        style.use(\"seaborn-bright\")\n    assert \"seaborn-bright\" in style.library\n    style.reload_library()\n    assert \"seaborn-bright\" not in style.library\n", "def test_reload_library():\n    original_library = style.library.copy()\n    original_available = style.available.copy()\n\n    # Add a temporary style to the library\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.available\n\n    # Reload the library and check that it returns to its original state\n    style.reload_library()\n    assert style.library == original_library\n    assert style.available == original_available\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n"], "sample_529": ["def test_legend_with_no_lines():\n    fig, ax = plt.subplots()\n    ax.set_title('Test Legend with No Lines')\n    leg = ax.legend()\n    assert leg is None\n", "def test_legend_handle_length():\n    # Test that the handle length of a legend can be set correctly.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend(handlelength=10)\n    assert leg.handlelength == 10\n    leg.handlelength = 5\n    assert leg.handlelength == 5\n", "def test_legend_labelcolor_inherit():\n    # test the labelcolor for labelcolor='inherit'\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), np.arange(10)*1, label='#1', color='r')\n    ax.plot(np.arange(10), np.arange(10)*2, label='#2', color='g')\n    ax.plot(np.arange(10), np.arange(10)*3, label='#3', color='b')\n\n    mpl.rcParams['legend.labelcolor'] = 'inherit'\n    leg = ax.legend()\n    for text in leg.get_texts():\n        assert mpl.colors.same_color(text.get_color(), 'black')\n", "def test_legend_handles_labels_with_parasite_axes():\n    # Test that _get_legend_handles_labels returns the correct handles and labels\n    # when dealing with parasite axes.\n    from mpl_toolkits.axes_grid1 import host_subplot\n\n    host = host_subplot(111)\n    par = host.twinx()\n\n    p1, = host.plot([0, 1, 2], [0, 1, 2], label=\"Density\")\n    p2, = par.plot([0, 1, 2], [0, 3, 2], label=\"Temperature\")\n\n    handles, labels = mlegend._get_legend_handles_labels([host])\n    assert len(handles) == 2\n    assert len(labels) == 2\n    assert handles[0] is p1\n    assert handles[1] is p2\n    assert labels[0] == \"Density\"\n    assert labels[1] == \"Temperature\"\n", "def test_legend_draggable():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n\n    # Check that the legend is not draggable by default\n    assert not leg.get_draggable()\n\n    # Make the legend draggable and check that it is now draggable\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n\n    # Make the legend not draggable again and check that it is no longer draggable\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n"], "sample_530": ["def test_anchoredoffsetbox_frameon():\n    fig, ax = plt.subplots()\n    ao = AnchoredOffsetbox(loc='upper right', child=TextArea('test'),\n                           frameon=True)\n    ax.add_artist(ao)\n    assert ao.patch.get_visible()\n\n    ao.frameon = False\n    assert not ao.patch.get_visible()\n", "def test_anchoredoffsetbox_frameon():\n    fig, ax = plt.subplots()\n    offsetbox = AnchoredOffsetbox(\n        loc='center',\n        child=TextArea(\"Test\"),\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.\n    )\n    ax.add_artist(offsetbox)\n    assert not offsetbox.patch.get_visible()\n    offsetbox.set_frame_on(True)\n    assert offsetbox.patch.get_visible()\n    offsetbox.frameon = False\n    assert not offsetbox.patch.get_visible()\n", "def test_drawingarea_add_artist():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    bg = mpatches.Rectangle((0, 0), 100, 100,\n                            facecolor='#CCCCCC',\n                            edgecolor='None',\n                            linewidth=0)\n    da.add_artist(bg)\n    assert len(da._children) == 1\n    assert da._children[0] is bg\n    assert bg.axes is ax\n    assert bg.figure is fig\n", "def test_textarea_get_extent():\n    # Test TextArea.get_extent with different font sizes and text content.\n    fig, ax = plt.subplots()\n\n    text_area = TextArea(\"Test Text\", textprops={\"fontsize\": 12})\n    extent1 = text_area.get_extent(fig.canvas.renderer)\n\n    text_area = TextArea(\"Test Text\", textprops={\"fontsize\": 24})\n    extent2 = text_area.get_extent(fig.canvas.renderer)\n\n    assert extent1.width < extent2.width\n    assert extent1.height < extent2.height\n\n    text_area = TextArea(\"Test Text\\nSecond Line\", textprops={\"fontsize\": 12})\n    extent3 = text_area.get_extent(fig.canvas.renderer)\n\n    assert extent1.width < extent3.width\n    assert extent1.height < extent3.height\n", "def test_offsetbox_get_window_extent():\n    # Test get_window_extent method for OffsetBox\n    fig, ax = plt.subplots()\n    offsetbox = OffsetBox()\n    ax.add_artist(offsetbox)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    extent = offsetbox.get_window_extent(renderer)\n    assert isinstance(extent, mtransforms.Bbox)\n"], "sample_531": ["def test_subfigure_repr():\n    fig = Figure()\n    subfig = fig.subfigures(1)[0]\n    assert repr(subfig).startswith(\"<SubFigure size\")\n", "def test_subfigure_repr():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1)\n    assert repr(sub_fig[0]).startswith(\"<SubFigure\")\n", "def test_subfigure_empty(fig_test, fig_ref):\n    fig_test.subfigures(1, 2)\n    fig_ref.add_subplot(121)\n    fig_ref.add_subplot(122)\n", "def test_subfigure_colorbar():\n    fig = plt.figure(layout='constrained')\n    sub = fig.subfigures(1, 2)\n    axs = sub[0].subplots(2, 2)\n    pc = axs[0, 0].pcolormesh(np.random.randn(30, 30), vmin=-2, vmax=2)\n    sub[0].colorbar(pc, ax=axs)\n\n    axs = sub[1].subplots(1, 3)\n    for ax in axs.flat:\n        pc = ax.pcolormesh(np.random.randn(30, 30), vmin=-2, vmax=2)\n    sub[1].colorbar(pc, ax=axs, location='bottom')\n\n    fig.suptitle('Figure suptitle', fontsize='xx-large')\n", "def test_supxlabel_title():\n    fig, ax = plt.subplots()\n    fig.supxlabel(\"xlabel\")\n    fig.suptitle(\"Title\")\n    assert fig._supxlabel.get_position()[1] < fig._suptitle.get_position()[1]\n"], "sample_532": ["def test_contour_label_text_colors():\n    ax = plt.figure().add_subplot()\n    data = [[0, 1], [1, 0]]\n    cs = ax.contour(data, levels=[0.5])\n    label = cs.clabel(cs.levels, colors='red', inline=True)\n    assert label[0].get_color() == 'red'\n    label = cs.clabel(cs.levels, colors=['blue'], inline=True)\n    assert label[0].get_color() == 'blue'\n", "def test_contour_with_all_nans():\n    fig, ax = plt.subplots()\n    data = np.full((10, 10), np.nan)\n    cs = ax.contour(data)\n    assert len(cs.allsegs) == 0\n", "def test_contour_levels():\n    ax = plt.figure().add_subplot()\n    data = [[0, 1], [1, 0]]\n    cs = ax.contour(data, levels=[0.5])\n    assert len(cs.levels) == 1\n    assert cs.levels[0] == 0.5\n", "def test_contourf_with_nchunk():\n    # Test contourf with nchunk\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.linspace(-3.0, 3.0, 100), np.linspace(-2.0, 2.0, 100))\n    z = np.exp(-(x**2) - y**2)\n    ax.contourf(x, y, z, nchunk=10)\n", "def test_contour_clabel_rotation():\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    cs = ax.contour(x, y, z)\n    cl = ax.clabel(cs, use_clabeltext=True)\n    assert len(cl) == len(cs.levels)\n"], "sample_533": ["def test_contour_label_rotation():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    ax.clabel(cs, rightside_up=False)\n", "def test_contour_labeling_noinline():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    pts = np.array([(1.0, 3.0), (1.0, 4.4), (1.0, 6.0)])\n    ax.clabel(cs, manual=pts, inline=False)\n    assert len(ax.texts) == len(pts)\n", "def test_contour_levels_and_colors():\n    # Test that contour correctly handles levels and colors\n    x, y = np.meshgrid(np.linspace(-10, 10, 100), np.linspace(-10, 10, 100))\n    z = np.sin(x) * np.cos(y)\n\n    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\n    axs[0, 0].contour(z, levels=5, colors='k')\n    axs[0, 1].contour(z, levels=[-1, 0, 1], colors=['r', 'g', 'b'])\n    axs[1, 0].contour(z, levels=5, cmap='viridis')\n    axs[1, 1].contour(z, levels=[-1, 0, 1], cmap='plasma')\n\n    for ax in axs.flat:\n        ax.set_title('Contour with levels and colors')\n", "def test_contourf_with_negative_levels():\n    # Test that contourf works correctly with negative levels.\n    x, y = np.meshgrid(np.linspace(-1, 1, 10), np.linspace(-1, 1, 10))\n    z = -x**2 - y**2\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(x, y, z, levels=[-1, -0.5, 0, 0.5, 1])\n    assert len(cs.collections) == 4  # Number of contour levels\n", "def test_contourf_legend_elements_variable_name():\n    from matplotlib.patches import Rectangle\n    x = np.arange(1, 10)\n    y = x.reshape(-1, 1)\n    h = x * y\n\n    cs = plt.contourf(h, levels=[10, 30, 50],\n                      colors=['#FFFF00', '#FF00FF', '#00FFFF'],\n                      extend='both')\n    cs.cmap.set_over('red')\n    cs.cmap.set_under('blue')\n    cs.changed()\n    artists, labels = cs.legend_elements(variable_name='z')\n    assert labels == ['$z \\\\leq -1e+250s$',\n                      '$10.0 < z \\\\leq 30.0$',\n                      '$30.0 < z \\\\leq 50.0$',\n                      '$z > 1e+250s$']\n    expected_colors = ('blue', '#FFFF00', '#FF00FF', 'red')\n    assert all(isinstance(a, Rectangle) for a in artists)\n    assert all(same_color(a.get_facecolor(), c)\n               for a, c in zip(artists, expected_colors))\n"], "sample_534": ["def test_contour_set_alpha():\n    ax = plt.figure().add_subplot()\n    cs = ax.contour(np.arange(16).reshape((4, 4)))\n    assert cs.get_alpha() is None\n    cs.set_alpha(0.5)\n    assert cs.get_alpha() == 0.5\n    for collection in cs.collections:\n        assert collection.get_alpha() == 0.5\n", "def test_contour_label_rot_and_inline():\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n    cs = ax.contour(x, y, z)\n    segs = cs.allsegs[0]\n    seg = segs[0]\n    lw = 5\n    lc = seg.copy()\n    rotation, nlc = cs.calc_label_rot_and_inline(\n        ax.transData.transform(seg), 0, lw, lc, 5)\n    assert rotation is not None\n    assert len(nlc) > 0\n", "def test_contour_label_text_colors():\n    fig, ax = plt.subplots()\n    cs = ax.contour(np.arange(16).reshape((4, 4)))\n    label_texts = cs.clabel()\n    assert all(same_color(label.get_color(), 'black') for label in label_texts)\n    cs = ax.contour(np.arange(16).reshape((4, 4)), colors='red')\n    label_texts = cs.clabel()\n    assert all(same_color(label.get_color(), 'red') for label in label_texts)\n    cs = ax.contour(np.arange(16).reshape((4, 4)), colors=['red', 'blue'])\n    label_texts = cs.clabel()\n    assert all(same_color(label.get_color(), color) for label, color in zip(label_texts, ['red', 'blue']))\n", "def test_contour_alpha():\n    fig, ax = plt.subplots()\n    z = np.random.rand(10, 10)\n    cs = ax.contour(z, alpha=0.5)\n    assert cs.get_alpha() == 0.5\n\n    cs = ax.contourf(z, alpha=0.3)\n    assert cs.get_alpha() == 0.3\n\n    cs.set_alpha(0.7)\n    assert cs.get_alpha() == 0.7\n", "def test_contour_add_label_near():\n    x, y = np.meshgrid(np.arange(0, 10), np.arange(0, 10))\n    z = np.max(np.dstack([abs(x), abs(y)]), 2)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    cs.add_label_near(1.0, 3.0, inline=True, transform=None)\n    cs.add_label_near(5, 5, inline=False, transform=False)\n"], "sample_535": ["def test_table_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    table.edges = 'horizontal'\n    cell = table.add_cell(1, 2, 1, 1)\n    assert cell.visible_edges == 'horizontal'\n\n    table.edges = 'open'\n    cell2 = table.add_cell(2, 1, 1, 2)\n    assert cell2.visible_edges == 'open'\n\n    table.edges = 'closed'\n    cell3 = table.add_cell(3, 1, 1, 1)\n    assert cell3.visible_edges == 'closed'\n\n    # Test that setting edges after adding cells doesn't change existing cells\n    table.edges = 'vertical'\n    assert cell.visible_edges == 'horizontal'\n    assert cell2.visible_edges == 'open'\n    assert cell3.visible_edges == 'closed'\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    # Test setting edges on table\n    table.edges = 'horizontal'\n    assert table.edges == 'horizontal'\n\n    # Test setting invalid edges raises error\n    try:\n        table.edges = 'invalid'\n        assert False\n    except ValueError:\n        pass\n\n    # Test setting edges on cell\n    cell = table.add_cell(1, 2, 1, 1)\n    cell.visible_edges = 'vertical'\n    assert cell.visible_edges == 'vertical'\n\n    # Test setting invalid edges on cell raises error\n    try:\n        cell.visible_edges = 'invalid'\n        assert False\n    except ValueError:\n        pass\n", "def test_table_visible_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    # Test setting visible_edges on the table\n    table.edges = 'horizontal'\n    cell = table.add_cell(1, 2, 1, 1)\n    assert cell.visible_edges == 'horizontal'\n\n    # Test setting visible_edges on individual cells\n    cell.visible_edges = 'vertical'\n    assert cell.visible_edges == 'vertical'\n\n    # Test setting invalid visible_edges value\n    try:\n        table.edges = 'invalid'\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        pass\n\n    try:\n        cell.visible_edges = 'invalid'\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        pass\n", "def test_table_edges(fig_test, fig_ref):\n    fig_test.subplots()\n    table = Table(fig_test.axes[0], edges='horizontal')\n    fig_test.axes[0].add_table(table)\n\n    fig_ref.subplots()\n    table = Table(fig_ref.axes[0])\n    table.edges = 'horizontal'\n    fig_ref.axes[0].add_table(table)\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    table.edges = 'horizontal'\n    assert table.edges == 'BT'\n\n    table.edges = 'closed'\n    assert table.edges == 'BRTL'\n\n    table.edges = 'open'\n    assert table.edges == ''\n\n    with pytest.raises(ValueError):\n        table.edges = 'invalid'\n"], "sample_537": ["def test_csd_onesided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    v = np.array([0, 2, 1, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.fft.fft(u) * dt / (dt * u.size)\n    Sv = np.fft.fft(v) * dt / (dt * v.size)\n    SuSv = Su * np.conj(Sv)\n    C, f = mlab.csd(u, v, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='onesided')\n    SuSv_1side = np.append([SuSv[0]], SuSv[1:4] + SuSv[4:][::-1])\n    assert_allclose(C, SuSv_1side, atol=1e-06)\n", "def test_psd_twosided_norm():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='twosided')\n    assert_allclose(P, Su, atol=1e-06)\n", "def test_psd_empty_input():\n    with pytest.raises(ValueError):\n        mlab.psd([])\n", "def test_specgram_auto_default_complex_equal(self):\n    \"\"\"\n    Test that mlab.specgram without mode and with mode 'default' are the same\n    for complex inputs.\n    \"\"\"\n    speca, freqspeca, ta = mlab.specgram(x=self.y,\n                                         NFFT=self.NFFT_specgram,\n                                         Fs=self.Fs,\n                                         noverlap=self.nover_specgram,\n                                         pad_to=self.pad_to_specgram,\n                                         sides=self.sides)\n    specb, freqspecb, tb = mlab.specgram(x=self.y,\n                                         NFFT=self.NFFT_specgram,\n                                         Fs=self.Fs,\n                                         noverlap=self.nover_specgram,\n                                         pad_to=self.pad_to_specgram,\n                                         sides=self.sides,\n                                         mode='default')\n    assert_array_equal(speca, np.abs(specb))\n    assert_array_equal(freqspeca, freqspecb)\n    assert_array_equal(ta, tb)\n", "def test_specgram_axis0_2D_input():\n    \"\"\"Test that specgram handles 2D input with axis=0 correctly.\"\"\"\n    Fs = 100.\n    t = np.arange(0, 10, 1 / Fs)\n    y1 = np.sin(2 * np.pi * 5 * t) + 0.5 * np.sin(2 * np.pi * 10 * t)\n    y2 = np.sin(2 * np.pi * 7 * t) + 0.5 * np.sin(2 * np.pi * 15 * t)\n    y = np.vstack([y1, y2])\n\n    freqs, t, Pxx = mlab.specgram(y, Fs=Fs, axis=0)\n\n    assert Pxx.shape[1] == len(freqs)\n    assert Pxx.shape[2] == len(t)\n"], "sample_538": ["def test_transformed_bbox():\n    bbox = mtransforms.Bbox([[0, 0], [1, 1]])\n    trans = mtransforms.Affine2D().scale(2)\n    tbbox = mtransforms.TransformedBbox(bbox, trans)\n    assert_array_equal(tbbox.get_points(), [[0, 0], [2, 2]])\n\n    # Changing the transform should change the result.\n    trans.scale(2)\n    assert_array_equal(tbbox.get_points(), [[0, 0], [4, 4]])\n\n    # Changing the bbox should change the result.\n    bbox.set_points([[0, 0], [2, 2]])\n    assert_array_equal(tbbox.get_points(), [[0, 0], [8, 8]])\n", "def test_affine_delta_transform():\n    points = np.array([[1, 2], [3, 4]])\n    t = mtransforms.AffineDeltaTransform(mtransforms.Affine2D().scale(2))\n    assert_array_almost_equal(t.transform(points), [[2, 4], [6, 8]])\n\n    t = mtransforms.AffineDeltaTransform(mtransforms.Affine2D().translate(1, 2))\n    assert_array_almost_equal(t.transform(points), [[1, 2], [3, 4]])\n\n    t = mtransforms.AffineDeltaTransform(mtransforms.Affine2D().rotate(np.pi/2))\n    assert_array_almost_equal(t.transform(points), [[-2, 1], [-4, 3]])\n", "def test_transform_wrapper_eq():\n    t1 = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    t2 = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    assert t1 == t2\n\n    t3 = mtransforms.TransformWrapper(mtransforms.Affine2D().translate(1, 2))\n    assert t1 != t3\n", "def test_transform_wrapper_eq():\n    t1 = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    t2 = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    assert t1 == t2\n\n    t3 = mtransforms.TransformWrapper(mtransforms.Affine2D().translate(1, 2))\n    assert t1 != t3\n", "def test_affine_delta_transform():\n    t = mtransforms.AffineDeltaTransform(mtransforms.Affine2D().translate(1, 2))\n    assert_array_almost_equal(t.get_matrix(),\n                              mtransforms.Affine2D().translate(0, 0).get_matrix())\n    p = np.array([[0, 0], [1, 1]])\n    assert_array_almost_equal(t.transform(p), p + [1, 2])\n    assert_array_almost_equal(t.transform_path(Path(p)).vertices, p + [1, 2])\n"], "sample_540": ["def test_blit_draw_artists(tmpdir):\n    # Test that blitting draws artists in the correct order.\n\n    fig, ax = plt.subplots()\n    line1, = ax.plot([0, 1], [0, 0], color='blue', zorder=1)\n    line2, = ax.plot([0, 1], [0, 1], color='red', zorder=2)\n\n        line1.set_data([0, 1], [i, i])\n        line2.set_data([0, 1], [i+1, i+1])\n        return line1, line2\n\n    anim = animation.FuncAnimation(fig, animate, frames=5, blit=True,\n                                   repeat=False)\n\n    with tmpdir.as_cwd():\n        anim.save(\"test.gif\", writer='pillow')\n", "def test_blit_cache_clear(anim):\n    # Test that blit cache is cleared when artists are removed\n    fig, ax = plt.subplots()\n    line1, = ax.plot([], [])\n    line2, = ax.plot([], [])\n\n        line1.set_data([], [])\n        line2.set_data([], [])\n        return line1, line2\n\n        line1.set_data([0, 1], [0, i])\n        line2.set_data([0, 1], [0, i])\n        if i == 2:\n            line2.remove()\n            return line1,\n        else:\n            return line1, line2\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=5, blit=True)\n    anim._init_draw()\n    for i in range(5):\n        anim._step()\n    assert len(anim._blit_cache) == 1\n", "def test_html_writer(tmpdir, anim):\n    # Test HTMLWriter with animation.\n    writer = animation.HTMLWriter()\n    with tmpdir.as_cwd():\n        anim.save(\"test.html\", writer=writer)\n    assert Path(\"test.html\").exists()\n    # Check that the html file contains the correct number of frames.\n    with open(\"test.html\", \"r\") as f:\n        html_content = f.read()\n    assert html_content.count(\"frames.push\") == anim._save_count\n", "def test_adjusted_figsize():\n    w, h = 10, 5\n    dpi = 100\n    n = 2\n\n    adjusted_w, adjusted_h = animation.adjusted_figsize(w, h, dpi, n)\n\n    assert adjusted_w * dpi % n == 0\n    assert adjusted_h * dpi % n == 0\n", "def test_artist_animation(tmpdir):\n    fig, ax = plt.subplots()\n    line1, = ax.plot([], [])\n    line2, = ax.plot([], [])\n\n        line1.set_data([], [])\n        line2.set_data([], [])\n        return line1, line2\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line1.set_data(x, y)\n        line2.set_data(x, -y)\n        return line1, line2\n\n    artists = [[line1], [line2]]\n    anim = animation.ArtistAnimation(fig, artists, init_func=init)\n\n    with tmpdir.as_cwd():\n        anim.save(\"test.gif\", writer='pillow')\n"], "sample_542": ["def test_update_bbox_position_size_with_linespacing():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Hello\\nWorld\", linespacing=2)\n    renderer = fig.canvas.get_renderer()\n    text.update_bbox_position_size(renderer)\n    assert text.get_window_extent().height > 20\n", "def test_get_textbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Hello\\nWorld', ha='center', va='center')\n    renderer = fig.canvas.get_renderer()\n    x_box, y_box, w_box, h_box = _get_textbox(text, renderer)\n    assert w_box > 0\n    assert h_box > 0\n", "def test_set_text_invalid_input():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"test\")\n    with pytest.raises(TypeError):\n        text.set_text(None)\n", "def test_get_rotation_mode():\n    text = Text(0, 0, 'test')\n    assert text.get_rotation_mode() is None\n\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() == 'default'\n\n    text.set_rotation_mode('anchor')\n    assert text.get_rotation_mode() == 'anchor'\n", "def test_update_from():\n    t1 = Text(0, 0, \"foo\")\n    t2 = Text(1, 1, \"bar\", fontproperties=FontProperties(weight=\"bold\"))\n    t1.update_from(t2)\n    assert t1.get_position() == (1, 1)\n    assert t1.get_text() == \"bar\"\n    assert t1.get_fontweight() == \"bold\"\n"], "sample_544": ["def test_image_size_rounding(fig_test, fig_ref):\n    # Test that image size rounding works correctly.\n    arr = np.random.rand(5, 5)\n    ax = fig_test.subplots()\n    ax.imshow(arr, interpolation='nearest')\n    ax.set_position([0.1, 0.1, 0.8, 0.8])\n    ax = fig_ref.subplots()\n    ax.imshow(arr, interpolation='nearest')\n    ax.set_position([0.1, 0.1, 0.80001, 0.8])\n", "def test_nonuniformimage_interpolation(fig_test, fig_ref):\n    x = np.linspace(0, 10, 5)\n    y = np.linspace(0, 10, 3)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) + np.cos(Y)\n\n    ax_test = fig_test.subplots()\n    im_test = NonUniformImage(ax_test, interpolation='nearest')\n    im_test.set_data(x, y, Z)\n    ax_test.add_image(im_test)\n    ax_test.set_xlim(0, 10)\n    ax_test.set_ylim(0, 10)\n\n    ax_ref = fig_ref.subplots()\n    im_ref = NonUniformImage(ax_ref, interpolation='bilinear')\n    im_ref.set_data(x, y, Z)\n    ax_ref.add_image(im_ref)\n    ax_ref.set_xlim(0, 10)\n    ax_ref.set_ylim(0, 10)\n", "def test_nonuniform_image(fig_test, fig_ref):\n    x = np.array([1, 2, 4, 7])\n    y = np.array([1, 3, 6])\n    z = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    ax_test = fig_test.subplots()\n    im_test = NonUniformImage(ax_test, interpolation='nearest')\n    im_test.set_data(x, y, z)\n    ax_test.add_image(im_test)\n\n    ax_ref = fig_ref.subplots()\n    ax_ref.pcolormesh(x[:, None], y[None, :], z, shading='nearest')\n\n    ax_test.set_xlim(ax_ref.get_xlim())\n    ax_test.set_ylim(ax_ref.get_ylim())\n", "def test_interpolation_stage(fig_test, fig_ref):\n    data = np.random.rand(10, 10)\n    ax1 = fig_test.add_subplot()\n    ax1.imshow(data, interpolation=\"nearest\", interpolation_stage=\"data\")\n    ax2 = fig_ref.add_subplot()\n    ax2.imshow(data, interpolation=\"nearest\", interpolation_stage=\"rgba\")\n", "def test_pcolorimage(fig_test, fig_ref):\n    x = np.linspace(0, 10, 5)\n    y = np.linspace(0, 10, 5)\n    data = np.random.rand(4, 4)\n    ax_test = fig_test.subplots()\n    ax_test.pcolor(x, y, data)\n    ax_ref = fig_ref.subplots()\n    ax_ref.add_artist(PcolorImage(ax_ref, x, y, data))\n    ax_ref.set(xlim=(0, 10), ylim=(0, 10))\n"], "sample_545": ["def test_subfigure_remove():\n    fig = plt.figure()\n    subfigs = fig.subfigures(2, 1)\n    axs0 = subfigs[0].subplots(2, 1)\n    axs1 = subfigs[1].subplots(2, 1)\n\n    # remove a subfigure\n    subfigs[0].remove()\n\n    assert len(fig.subfigs) == 1\n    assert len(fig.axes) == 2\n\n    # check that the remaining axes are correct\n    assert all(ax in fig.axes for ax in axs1)\n    assert all(ax not in fig.axes for ax in axs0)\n", "def test_set_layout_engine(fig_test, fig_ref):\n    fig_test.set_layout_engine('constrained')\n    assert isinstance(fig_test.get_layout_engine(), ConstrainedLayoutEngine)\n    fig_ref.set_layout_engine(TightLayoutEngine())\n    assert isinstance(fig_ref.get_layout_engine(), TightLayoutEngine)\n", "def test_sup_xlabel_position(fig_test, fig_ref):\n    fig_test.supxlabel(\"xlabel\", y=0.05)\n    fig_ref.supxlabel(\"xlabel\")\n    fig_ref.subplots_adjust(bottom=0.95)\n", "def test_sup_xlabel_y_label(fig_test, fig_ref):\n    fig_test.supxlabel(\"X Label\")\n    fig_test.supylabel(\"Y Label\")\n    ax = fig_ref.add_subplot(111)\n    ax.set_xlabel(\"X Label\")\n    ax.set_ylabel(\"Y Label\")\n    ax.xaxis.tick_bottom()\n    ax.yaxis.tick_left()\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    fig_ref.subplots_adjust(left=0.1, bottom=0.15)\n", "def test_subfigure_set_dpi():\n    fig = plt.figure(dpi=100)\n    subfig = fig.subfigures(1, 1)[0]\n    subfig.set_dpi(200)\n    assert fig.dpi == 200\n    assert subfig.dpi == 200\n"], "sample_546": ["def test_figure_suptitle():\n    fig = Figure()\n    assert fig._suptitle is None\n\n    title_text = \"This is a suptitle\"\n    title = fig.suptitle(title_text)\n\n    assert fig._suptitle is not None\n    assert title.get_text() == title_text\n    assert title.get_position() == (0.5, 0.98)\n\n    # Test update\n    new_title_text = \"New suptitle text\"\n    new_title = fig.suptitle(new_title_text)\n\n    assert fig._suptitle is not None\n    assert new_title.get_text() == new_title_text\n    assert new_title.get_position() == (0.5, 0.98)\n", "def test_figure_repr():\n    fig = plt.figure()\n    assert repr(fig) == f\"<{type(fig).__name__} size {fig.get_size_inches()[0]:g}x{fig.get_size_inches()[1]:g} with 0 Axes>\"\n    ax = fig.add_subplot(111)\n    assert repr(fig) == f\"<{type(fig).__name__} size {fig.get_size_inches()[0]:g}x{fig.get_size_inches()[1]:g} with 1 Axes>\"\n", "def test_autofmt_xdate():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    fig.autofmt_xdate()\n    assert ax.get_xticklabels()[0].get_ha() == 'right'\n    assert ax.get_xticklabels()[0].get_rotation() == 30\n\n    fig, axs = plt.subplots(2, 1)\n    axs[0].plot([1, 2, 3])\n    axs[1].plot([4, 5, 6])\n    fig.autofmt_xdate()\n    for label in axs[0].get_xticklabels():\n        assert label.get_visible() is False\n    assert axs[1].get_xticklabels()[0].get_ha() == 'right'\n    assert axs[1].get_xticklabels()[0].get_rotation() == 30\n", "def test_figure_repr():\n    fig = plt.figure()\n    assert re.match(\n        r\"<Figure size \\d+\\.\\d+x\\d+\\.\\d+ with 0 Axes>\",\n        repr(fig))\n    ax = fig.add_subplot(111)\n    assert re.match(\n        r\"<Figure size \\d+\\.\\d+x\\d+\\.\\d+ with 1 Axes>\",\n        repr(fig))\n    fig.add_subplot(122)\n    assert re.match(\n        r\"<Figure size \\d+\\.\\d+x\\d+\\.\\d+ with 2 Axes>\",\n        repr(fig))\n", "def test_figure_repr():\n    fig = Figure()\n    assert repr(fig) == \"<Figure size 640x480 with 0 Axes>\"\n    ax = fig.add_subplot(111)\n    assert repr(fig) == \"<Figure size 640x480 with 1 Axes>\"\n"], "sample_547": ["def test_drawingarea_add_artist():\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    r = mpatches.Rectangle((0, 0), 10, 10, fill=True)\n    da.add_artist(r)\n    assert len(da._children) == 1\n    assert da._children[0] is r\n    assert r.axes is None\n    fig, ax = plt.subplots()\n    da.set_figure(fig)\n    assert r.get_figure() is fig\n", "def test_drawing_area_clip():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100, clip=True)\n    patch = mpatches.Rectangle((0, 0), 50, 50, facecolor='blue')\n    da.add_artist(patch)\n    ax.add_artist(da)\n    fig.canvas.draw()\n\n    assert patch.get_clip_path() is not None\n    assert patch.clipbox is not None\n\n    da.clip_children = False\n    fig.canvas.draw()\n\n    assert patch.get_clip_path() is None\n    assert patch.clipbox is None\n", "def test_offsetbox_repr():\n    ob = OffsetBox()\n    assert repr(ob) == \"OffsetBox()\"\n", "def test_offsetbox_set_width_height(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    box0 = OffsetBox(width=100, height=50)\n    ax_ref.add_artist(box0)\n\n    ax_test = fig_test.add_subplot()\n    box1 = OffsetBox()\n    box1.set_width(100)\n    box1.set_height(50)\n    ax_test.add_artist(box1)\n", "def test_auxtransformbox():\n    fig, ax = plt.subplots()\n    atb = AuxTransformBox(ax.transData)\n    text = TextArea(\"Hello\")\n    atb.add_artist(text)\n    ax.add_artist(atb)\n    fig.canvas.draw()\n    assert not fig.stale\n    assert isinstance(atb.get_transform(), mtransforms.Transform)\n"], "sample_548": ["def test_colorbar_with_no_ticks():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n    cs = ax.contourf(data, levels=levels)\n    cbar = fig.colorbar(cs, ticks=[])\n    assert len(cbar.ax.yaxis.get_ticklocs()) == 0\n", "def test_colorbar_with_empty_data():\n    fig, ax = plt.subplots()\n    data = np.empty((0, 0))\n    im = ax.imshow(data)\n    with pytest.warns(UserWarning, match=\"No data for colormapping\"):\n        fig.colorbar(im)\n", "def test_colorbar_orientation_from_location():\n    fig, ax = plt.subplots(figsize=(6, 1))\n    im = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    fig.colorbar(im, location='top', fraction=0.2)\n    fig.colorbar(im, location='bottom', fraction=0.2)\n    fig.colorbar(im, location='left', fraction=0.2)\n    fig.colorbar(im, location='right', fraction=0.2)\n", "compilation error", "def test_colorbar_no_ticks():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    cb.set_ticks([])\n"], "sample_549": ["def test_is_math_text():\n    assert cbook.is_math_text('$a$')\n    assert not cbook.is_math_text('a')\n    assert not cbook.is_math_text('$a')\n    assert not cbook.is_math_text('a$')\n    assert cbook.is_math_text('$$a$$')\n    assert not cbook.is_math_text('$$$a$$$')\n", "def test_unikey_or_keysym_to_mplkey():\n    assert cbook._unikey_or_keysym_to_mplkey('a', None) == 'a'\n    assert cbook._unikey_or_keysym_to_mplkey(None, 'KP_Enter') == 'enter'\n    assert cbook._unikey_or_keysym_to_mplkey(None, 'Shift_L') == 'shift'\n    assert cbook._unikey_or_keysym_to_mplkey(None, 'Meta_L') == 'cmd'  # on darwin\n    assert cbook._unikey_or_keysym_to_mplkey(None, 'Control_L') == 'ctrl'\n    assert cbook._unikey_or_keysym_to_mplkey(None, 'Prior') == 'pageup'\n    assert cbook._unikey_or_keysym_to_mplkey(None, 'Next') == 'pagedown'\n", "def test__g_sig_digits():\n    assert cbook._g_sig_digits(45.67, 0.02) == 4\n    assert cbook._g_sig_digits(0, 0.02) == 1\n    assert cbook._g_sig_digits(45.67, 0) == 6  # Spacing(45.67)\n", "def test_is_math_text():\n    assert cbook.is_math_text(\"$1+1$\") is True\n    assert cbook.is_math_text(\"hello world\") is False\n    assert cbook.is_math_text(\"$hello world$\") is True\n    assert cbook.is_math_text(r\"\\$hello world\\$\") is False\n", "def test_flatten():\n    nested_list = [[1, 2], [3, [4, 5]], 6, [7, [8, [9]]]]\n    expected = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert list(cbook.flatten(nested_list)) == expected\n\n    nested_list = [(1, 2), (3, (4, 5)), 6, (7, (8, (9)))]\n    expected = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert list(cbook.flatten(nested_list)) == expected\n\n    nested_list = [np.array([1, 2]), np.array([3, [4, 5]])]\n    with pytest.raises(TypeError):\n        list(cbook.flatten(nested_list))\n"], "sample_550": ["def test_set_position():\n    fig, ax = plt.subplots()\n    pos1 = [0.1, 0.1, 0.5, 0.5]\n    ax.set_position(pos1)\n    assert np.allclose(ax.get_position().bounds, pos1)\n\n    pos2 = [0.2, 0.2, 0.6, 0.6]\n    ax._set_position(pos2)\n    assert np.allclose(ax.get_position().bounds, pos2)\n    assert np.allclose(ax.get_position(original=True).bounds, pos1)\n", "def test_axes_facecolor():\n    fig, ax = plt.subplots()\n    ax.set_facecolor('red')\n    assert ax.get_facecolor() == (1.0, 0.0, 0.0, 1.0)\n    ax.set_facecolor((0, 1, 0))\n    assert ax.get_facecolor() == (0.0, 1.0, 0.0, 1.0)\n    ax.set_facecolor('#0000ff')\n    assert ax.get_facecolor() == (0.0, 0.0, 1.0, 1.0)\n    ax.set_facecolor((0, 0, 1, 0.5))\n    assert ax.get_facecolor() == (0.0, 0.0, 1.0, 0.5)\n", "def test_axes_get_window_extent():\n    fig, ax = plt.subplots()\n    extent = ax.get_window_extent()\n    assert isinstance(extent, transforms.Bbox)\n    assert extent.x0 == 0\n    assert extent.y0 == 0\n    assert extent.width > 0\n    assert extent.height > 0\n", "def test_format_coord():\n    fig, ax = plt.subplots()\n    event = MouseEvent(\"motion_notify_event\", fig.canvas, 0, 0)\n    with pytest.raises(AttributeError):\n        ax.format_coord(event.xdata, event.ydata)\n\n    event.xdata, event.ydata = 0.5, 0.5\n    assert re.match(r\"x=0\\.5 +y=0\\.5\", ax.format_coord(event.xdata, event.ydata))\n\n    ax.fmt_xdata = lambda x: \"foo\"\n    ax.fmt_ydata = lambda y: \"bar\"\n    assert re.match(r\"x=foo +y=bar\", ax.format_coord(event.xdata, event.ydata))\n", "def test_axes_artists_lists():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2])\n    line2, = ax.plot([2, 3])\n    ax.add_patch(plt.Rectangle((0, 0), 1, 1))\n    ax.add_patch(plt.Circle((0.5, 0.5), 0.4))\n    ax.add_collection(plt.collections.LineCollection([[(-0.5, -0.5), (1.5, 1.5)]]))\n\n    assert len(ax.artists) == 0\n    assert len(ax.lines) == 2\n    assert len(ax.patches) == 2\n    assert len(ax.collections) == 1\n\n    with pytest.raises(TypeError):\n        ax.artists += \"foo\"\n\n    with pytest.raises(TypeError):\n        ax.lines += \"bar\"\n\n    with pytest.raises(TypeError):\n        ax.patches += \"baz\"\n\n    with pytest.raises(TypeError):\n        ax.collections += \"qux\"\n\n    new_lines = [line1, line2]\n    new_patches = ax.patches.copy()\n    new_collections = ax.collections.copy()\n\n    ax.lines.clear()\n    ax.lines.extend(new_lines)\n    ax.patches.clear()\n    ax.patches.extend(new_patches)\n    ax.collections.clear()\n    ax.collections.extend(new_collections)\n\n    assert len(ax.lines) == 2\n    assert len(ax.patches) == 2\n    assert len(ax.collections) == 1\n"], "sample_551": ["def test_text3d_properties():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    text = art3d.Text3D(0.5, 0.5, 0.5, 'text')\n    ax.add_artist(text)\n\n    assert text.get_position_3d() == (0.5, 0.5, 0.5)\n    text.set_position_3d((1, 1, 1))\n    assert text.get_position_3d() == (1, 1, 1)\n\n    text.set_z(2)\n    assert text.get_position_3d() == (1, 1, 2)\n\n    text.set_3d_properties(z=3, zdir='x')\n    assert text.get_position_3d() == (3, 1, 2)\n", "def test_pathpatch3d():\n    # Test PathPatch3D initialization and methods.\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    path = plt.Circle((0, 0), 1)\n    patch = art3d.PathPatch3D(path, zs=0, zdir='z')\n    ax.add_patch(patch)\n\n    assert isinstance(patch, art3d.PathPatch3D)\n    assert patch._segment3d is not None\n    assert patch._codes3d is not None\n\n    # Test do_3d_projection method.\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n    patch.do_3d_projection()\n    assert patch._path2d is not None\n", "def test_text3d_properties():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    text = art3d.Text3D(0.5, 0.5, 0.5, 'Hello, world!')\n    ax.add_artist(text)\n\n    # Test position properties\n    assert np.array_equal(text.get_position_3d(), (0.5, 0.5, 0.5))\n    text.set_position_3d((1.0, 1.0, 1.0))\n    assert np.array_equal(text.get_position_3d(), (1.0, 1.0, 1.0))\n\n    # Test zdir property\n    assert text._dir_vec == (0, 0, 1)\n    text.set_3d_properties(zdir='x')\n    assert text._dir_vec == (1, 0, 0)\n", "def test_line3d_collection_zsort():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    xs, ys, zs = np.random.rand(10), np.random.rand(10), np.random.rand(10)\n    segments = list(zip(zip(xs, ys, zs), zip(xs+1, ys+1, zs+1)))\n    collection = art3d.Line3DCollection(segments, zsort='average')\n    ax.add_collection(collection)\n    assert collection.do_3d_projection() is not None\n    collection.set_zsort('min')\n    assert collection.do_3d_projection() is not None\n    collection.set_zsort('max')\n    assert collection.do_3d_projection() is not None\n", "def test_poly3dcollection_shading():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    vertices = np.array([[[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]]])\n    poly = art3d.Poly3DCollection(vertices, shade=True)\n    ax.add_collection(poly)\n\n    # Check that the shading is applied correctly\n    assert poly.get_facecolor().shape == (1, 4)\n    assert poly.get_edgecolor().shape == (1, 4)\n"], "sample_552": ["def test_subfigure_label():\n    # Test that subfigure labels work correctly.\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1)\n    assert sub_fig.get_label() == ''\n    sub_fig.set_label('Subfigure label')\n    assert sub_fig.get_label() == 'Subfigure label'\n", "def test_add_subplot_by_spec(fig_test, fig_ref):\n    gs = fig_ref.add_gridspec(2, 2)\n    fig_ref.add_subplot(gs[0, :])\n    fig_ref.add_subplot(gs[1, 0])\n    fig_ref.add_subplot(gs[1, 1])\n\n    gs = fig_test.add_gridspec(2, 2)\n    fig_test.add_subplot(gs[0, :])\n    fig_test.add_subplot(gs[1, 0])\n    fig_test.add_subplot(gs[1, 1])\n    fig_test.add_subplot(gs[1, 1])  # this should be ignored\n", "def test_supxlabel_y():\n    fig, ax = plt.subplots()\n    fig.supxlabel('x')\n    fig.supylabel('y')\n\n    assert fig._supxlabel.get_text() == 'x'\n    assert fig._supylabel.get_text() == 'y'\n\n    # Check the positions are reasonable\n    fig.draw_without_rendering()\n    assert fig._supxlabel.get_position()[1] < 0.1\n    assert fig._supylabel.get_position()[0] < 0.1\n\n    # Check that removing them works.\n    fig.supxlabel(None)\n    fig.supylabel(None)\n\n    assert fig._supxlabel is None\n    assert fig._supylabel is None\n", "def test_set_layout_engine_place_holder():\n    fig = Figure()\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    fig.set_layout_engine('none')\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n    assert fig.get_layout_engine().colorbar_gridspec == False\n", "def test_add_subfigure_at():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    sub_fig = fig.add_subfigure(gs[0, 0])\n    assert sub_fig._subplotspec is gs[0, 0]\n"], "sample_553": ["def test_animation_pause_resume():\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                   frames=5, blit=True)\n\n    # Check pause and resume functionality\n    anim.pause()\n    assert anim.event_source.interval == 0\n\n    anim.resume()\n    assert anim.event_source.interval != 0\n", "def test_embed_frames_false_html_writer(tmpdir, anim):\n    writer = animation.HTMLWriter(embed_frames=False)\n    with tmpdir.as_cwd():\n        anim.save('movie.html', writer=writer)\n\n    # Check that there are frame files saved.\n    frame_files = list(tmpdir.glob('movie_frames/frame*.png'))\n    assert len(frame_files) > 0\n", "def test_html_writer_with_invalid_embed_limit(anim, tmpdir):\n    writer = animation.HTMLWriter(fps=30, embed_limit='foo')\n    with pytest.raises(ValueError):\n        anim.save(\"test.html\", writer=writer)\n", "def test_animation_to_jshtml():\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=5)\n\n    # Test that the to_jshtml method returns a string containing the animation\n    jshtml = anim.to_jshtml()\n    assert isinstance(jshtml, str)\n    assert '<script>' in jshtml\n", "def test_to_html5_video_errors(anim):\n    # Test errors for to_html5_video method\n\n    # Test that embed_limit must be a positive number\n    with pytest.raises(ValueError):\n        anim.to_html5_video(embed_limit=-1)\n\n    # Test that embed_limit must be a number\n    with pytest.raises(TypeError):\n        anim.to_html5_video(embed_limit='foo')\n"], "sample_554": ["def test_rotation_mode():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test', rotation=45, ha='left')\n    text.set_rotation_mode('anchor')\n    assert text.get_rotation_mode() == 'anchor'\n    text.set_rotation_mode('default')\n    assert text.get_rotation_mode() is None\n    with pytest.raises(ValueError):\n        text.set_rotation_mode('invalid')\n", "def test_text_position_xy():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    assert text._x == 0.5\n    assert text._y == 0.5\n\n    text.set_position((0.6, 0.6))\n    assert text._x == 0.6\n    assert text._y == 0.6\n\n    text.set_x(0.7)\n    assert text._x == 0.7\n    assert text._y == 0.6\n\n    text.set_y(0.8)\n    assert text._x == 0.7\n    assert text._y == 0.8\n", "def test_text_antialiased_on_default_vs_manual(fig_test, fig_ref):\n    fig_test.text(0.5, 0.5, '6 inches x 2 inches',\n                             antialiased=True)\n\n    mpl.rcParams['text.antialiased'] = True\n    fig_ref.text(0.5, 0.5, '6 inches x 2 inches')\n", "def test_set_rotation_mode():\n    txt = Text(.5, .5, \"foo\\nbar\")\n    txt.set_rotation_mode('default')\n    assert txt.get_rotation_mode() == 'default'\n\n    txt.set_rotation_mode('anchor')\n    assert txt.get_rotation_mode() == 'anchor'\n\n    with pytest.raises(ValueError):\n        txt.set_rotation_mode('invalid')\n", "def test_set_text():\n    txt = Text(.5, .5, \"foo\")\n    assert txt.get_text() == \"foo\"\n\n    txt.set_text(\"bar\")\n    assert txt.get_text() == \"bar\"\n\n    txt.set_text(None)\n    assert txt.get_text() == \"\"\n\n    txt.set_text(\"\")\n    assert txt.get_text() == \"\"\n"], "sample_555": ["def test_autoscale_ellipse():\n    fig, axs = plt.subplots(1, 3, figsize=(4, 1))\n    ellipse_lists = (\n        [Ellipse((0, 0), 1, 1)],\n        [Ellipse((0.5, 0.5), 1.5, 0.5)],\n        [Ellipse((0.5, 0.5), 1.5, 0.5),\n         Ellipse((0.5, 0.5), 2.5, 0.5),\n         Ellipse((0.5, 0.5), 3.5, 0.5),\n         Ellipse((0.5, 0.5), 4.5, 0.5)])\n\n    for ax, ellipses in zip(axs, ellipse_lists):\n        for ellipse in ellipses:\n            ax.add_patch(ellipse)\n        ax.autoscale()\n", "def test_patch_alpha_masking():\n    fig, ax = plt.subplots()\n    circle = mpath.Path.unit_circle()\n    cut_star1 = mpath.PathPatch(circle, facecolor=(0.5, 0.5, 0.5), edgecolor='none', alpha=0.7)\n    cut_star2 = mpath.PathPatch(circle, facecolor=(0.9, 0.9, 0.9), edgecolor='none', alpha=0.4)\n    ax.add_patch(cut_star1)\n    ax.add_patch(cut_star2)\n    ax.set_xlim(-1.1, 1.1)\n    ax.set_ylim(-1.1, 1.1)\n", "def test_connectionstyle():\n    fig, ax = plt.subplots()\n\n    for i, style in enumerate([\"arc\", \"arc,angleA=90,armA=30,rad=10\",\n                               \"angle\", \"angle,angleA=-90,angleB=180,rad=5\",\n                               \"angle3\", \"angle3,angleA=45,angleB=-45\",\n                               \"bar\", \"bar,fraction=0.3,angle=90\"]):\n        x1, y1 = 0.1, i / 8.0\n        x2, y2 = 0.9, (i + 1) / 8.0\n        ax.add_patch(ConnectionPatch((x1, y1), (x2, y2),\n                                     coordsA=\"data\", coordsB=\"data\",\n                                     axesA=ax, axesB=ax,\n                                     connectionstyle=style))\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_ellipse_aspect_ratio():\n    # Test that the aspect ratio of an ellipse is correctly handled.\n    fig, ax = plt.subplots()\n    ell = Ellipse((0.5, 0.5), 1, 2, fill=False)\n    ax.add_patch(ell)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect('equal')\n    fig.canvas.draw()\n    assert ell._aspect_ratio_correction == 1.0\n", "def test_annulus_aspect():\n    fig, ax = plt.subplots()\n    cir = Annulus((0.5, 0.5), 0.2, 0.05, fc='g')\n    ell = Annulus((0.5, 0.5), (0.5, 0.3), 0.1, 45, fc='m', ec='b', alpha=0.5)\n    ax.add_patch(cir)\n    ax.add_patch(ell)\n    ax.set_aspect('auto')  # Test with non-equal aspect ratio\n"], "sample_556": ["def test_set_size_inches():\n    fig = plt.figure(figsize=(8, 6))\n    assert fig.get_size_inches() == (8, 6)\n    fig.set_size_inches(10, 7)\n    assert fig.get_size_inches() == (10, 7)\n    fig.set_size_inches((12, 8))\n    assert fig.get_size_inches() == (12, 8)\n\n    # Test setting with forward=False\n    fig.set_size_inches(14, 9, forward=False)\n    assert fig.get_size_inches() == (14, 9)\n\n    # Test setting with dpi=100\n    fig.set_dpi(100)\n    fig.set_size_inches(16, 10)\n    assert fig.get_size_inches() == (16, 10)\n", "def test_figure_get_axes():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    axes = fig.get_axes()\n    assert len(axes) == 2\n    assert ax1 in axes\n    assert ax2 in axes\n    assert fig.axes == axes\n", "def test_add_subplot_multiple_calls(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot(111)\n    ax_test1 = fig_test.add_subplot(111)\n    ax_test2 = fig_test.add_subplot(ax_test1)\n    assert ax_test1 is not ax_test2\n\n    ax_ref.plot([0, 1], [0, 1])\n    ax_test2.plot([0, 1], [0, 1])\n", "def test_set_layout_engine(fig_test, fig_ref):\n    fig_test.set_layout_engine(\"constrained\")\n    assert isinstance(fig_test.get_layout_engine(), ConstrainedLayoutEngine)\n    fig_ref.set_layout_engine(ConstrainedLayoutEngine())\n    assert isinstance(fig_ref.get_layout_engine(), ConstrainedLayoutEngine)\n", "def test_subfigure_repr():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1)\n    assert repr(sub_fig[0]).startswith(\"<SubFigure\")\n"], "sample_557": ["def test_subfigure_edge_color():\n    fig = plt.figure()\n    sub_fig = fig.subfigures(1, 1, edgecolor='red')\n    assert sub_fig.patch.get_edgecolor() == (1.0, 0.0, 0.0, 1.0)\n", "def test_subfigure_repr():\n    fig = plt.figure(figsize=(10, 20), dpi=10)\n    subfig = fig.subfigures(1, 1)[0]\n    assert repr(subfig) == \"<SubFigure size 100x200 with 0 Axes>\"\n", "def test_figure_subfigures():\n    fig = Figure()\n    subfigs = fig.subfigures(2, 2)\n    assert len(subfigs) == 4\n    assert all(isinstance(subfig, FigureBase) for subfig in subfigs)\n    assert all(subfig.get_parent() is fig for subfig in subfigs)\n    assert fig.subfigs == list(subfigs)\n\n    # Test that subfigures are properly removed when cleared.\n    fig.clear()\n    assert fig.subfigs == []\n", "def test_figure_colorbar_gridspec():\n    fig = plt.figure(layout='constrained')\n    gs = fig.add_gridspec(1, 2)\n    ax1 = fig.add_subplot(gs[0])\n    ax2 = fig.add_subplot(gs[1])\n\n    pc1 = ax1.pcolormesh(np.random.randn(10, 10))\n    pc2 = ax2.pcolormesh(np.random.randn(10, 10))\n\n    cbar1 = fig.colorbar(pc1, ax=ax1)\n    cbar2 = fig.colorbar(pc2, ax=ax2)\n\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    assert fig.get_layout_engine().colorbar_gridspec\n\n    fig.set_layout_engine('tight')\n    assert not isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    assert not fig.get_layout_engine().colorbar_gridspec\n", "def test_get_constrained_layout_pads():\n    fig = plt.figure(constrained_layout=True)\n    w_pad, h_pad, wspace, hspace = fig.get_constrained_layout_pads()\n    assert isinstance(w_pad, float)\n    assert isinstance(h_pad, float)\n    assert isinstance(wspace, float)\n    assert isinstance(hspace, float)\n\n    # test relative option\n    renderer = fig.canvas.get_renderer()\n    dpi = renderer.dpi\n    width, height = fig.get_size_inches()\n\n    w_pad_rel, h_pad_rel, wspace, hspace = fig.get_constrained_layout_pads(relative=True)\n    np.testing.assert_allclose(w_pad_rel, w_pad * dpi / width)\n    np.testing.assert_allclose(h_pad_rel, h_pad * dpi / height)\n"], "sample_558": ["def test_grid_getitem():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 3))\n    assert len(grid) == 6\n    for i in range(6):\n        assert isinstance(grid[i], Axes)\n", "def test_grid_label_mode():\n    fig = plt.figure(figsize=(8, 6))\n    grid = Grid(fig, 111, nrows_ncols=(3, 3), label_mode='L')\n\n    for ax in grid:\n        ax.plot([0, 1], [0, 1])\n\n    grid = Grid(fig, 122, nrows_ncols=(3, 3), label_mode='1')\n\n    for ax in grid:\n        ax.plot([0, 1], [0, 1])\n", "def test_grid_set_aspect():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 3))\n    assert grid.get_aspect() is False\n    grid.set_aspect(True)\n    assert grid.get_aspect() is True\n    grid.set_aspect(False)\n    assert grid.get_aspect() is False\n    grid.set_aspect(\"equal\")\n    assert grid.get_aspect() == \"equal\"\n    with pytest.raises(ValueError):\n        grid.set_aspect(\"invalid\")\n", "def test_grid_label_mode():\n    fig = plt.figure(figsize=(6, 4))\n    grid = Grid(fig, 111, nrows_ncols=(2, 2), label_mode=\"L\")\n    for i in range(4):\n        grid[i].plot([0, 1], [0, 1])\n    grid.set_label_mode(\"all\")\n    grid.set_label_mode(\"1\")\n", "def test_grid_set_axes_locator():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 3), direction=\"row\")\n    locator = grid.get_axes_locator()\n    new_locator = Divider(fig, [0, 0, 1, 1], [Size.Fixed(1)], [Size.Fixed(1)])\n    grid.set_axes_locator(new_locator)\n    assert grid.get_axes_locator() is not locator\n    assert grid.get_axes_locator() is new_locator\n"], "sample_560": ["def test_legend_remove_all():\n    fig, ax = plt.subplots()\n    lines = ax.plot(range(10))\n    leg = fig.legend(lines, \"test\")\n    leg.remove()\n    assert fig.legends == []\n    leg = ax.legend(\"test\")\n    leg.remove()\n    assert ax.get_legend() is None\n    ax.legend([])\n    assert ax.get_legend() is None\n", "def test_legend_handles_and_labels_with_invalid_label():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='valid')\n    ax.plot([1, 2, 3], label='_invalid')  # starts with underscore\n    handles, labels = ax.get_legend_handles_labels()\n    assert len(handles) == 1\n    assert len(labels) == 1\n    assert labels[0] == 'valid'\n", "def test_legend_label_order_with_invalid_labels():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='valid')\n    ax.plot([4, 5, 6], label='_invalid')  # will be ignored\n    ax.plot([7, 8, 9], label='another_valid')\n\n    handles, labels = ax.get_legend_handles_labels()\n    assert len(handles) == 2\n    assert len(labels) == 2\n    assert labels == ['valid', 'another_valid']\n", "def test_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n\n    # Test setting bbox_to_anchor with a tuple\n    leg.set_bbox_to_anchor((0.5, 0.5))\n    assert isinstance(leg.get_bbox_to_anchor(), mtransforms.BboxBase)\n\n    # Test setting bbox_to_anchor with a BboxBase instance\n    bbox = mtransforms.Bbox.from_bounds(0, 0, 1, 1)\n    leg.set_bbox_to_anchor(bbox)\n    assert leg.get_bbox_to_anchor() is bbox\n\n    # Test setting bbox_to_anchor to None\n    leg.set_bbox_to_anchor(None)\n    assert leg.get_bbox_to_anchor() is None\n", "def test_legend_title_fontproperties():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    leg = ax.legend(title=\"Legend Title\", title_fontproperties={'family': 'serif', 'size': 20})\n    assert leg.get_title().get_family()[0] == 'serif'\n    assert leg.get_title().get_size() == 20\n"], "sample_561": ["def test_marker_fillstyle():\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='full')\n    assert marker_style.get_fillstyle() == 'full'\n    assert marker_style.is_filled()\n\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='none')\n    assert marker_style.get_fillstyle() == 'none'\n    assert not marker_style.is_filled()\n\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='top')\n    assert marker_style.get_fillstyle() == 'top'\n    assert marker_style.is_filled()\n\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='bottom')\n    assert marker_style.get_fillstyle() == 'bottom'\n    assert marker_style.is_filled()\n\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='left')\n    assert marker_style.get_fillstyle() == 'left'\n    assert marker_style.is_filled()\n\n    marker_style = markers.MarkerStyle(marker='o', fillstyle='right')\n    assert marker_style.get_fillstyle() == 'right'\n    assert marker_style.is_filled()\n", "def test_marker_half_fillstyles():\n    \"\"\"Test that half fillstyles are correctly implemented.\"\"\"\n    marker = markers.MarkerStyle('o')\n    for fillstyle in ['left', 'right', 'bottom', 'top']:\n        marker._set_fillstyle(fillstyle)\n        assert marker.is_filled()\n        assert marker.get_alt_path() is not None\n        assert marker.get_alt_transform() is not None\n", "def test_marker_scaled_invalid():\n    marker = markers.MarkerStyle(\"1\")\n    with pytest.raises(TypeError):\n        new_marker = marker.scaled()\n    with pytest.raises(TypeError):\n        new_marker = marker.scaled('a')\n    with pytest.raises(TypeError):\n        new_marker = marker.scaled(2, 'b')\n", "def test_marker_fillstyles():\n    \"\"\"Test that fillstyles are applied correctly.\"\"\"\n    marker = markers.MarkerStyle(\"o\")\n    for fillstyle in markers.MarkerStyle.fillstyles:\n        styled_marker = markers.MarkerStyle(\"o\", fillstyle=fillstyle)\n        assert styled_marker.get_fillstyle() == fillstyle\n        if fillstyle == 'none':\n            assert not styled_marker.is_filled()\n        else:\n            assert styled_marker.is_filled()\n\n    # Check default fillstyle\n    assert marker.get_fillstyle() == mpl.rcParams['markers.fillstyle']\n", "def test_marker_init_fillstyle():\n    marker = markers.MarkerStyle(\"o\")\n    styled_marker = markers.MarkerStyle(\"o\", fillstyle=\"none\")\n    assert styled_marker.get_fillstyle() == \"none\"\n    assert marker.get_fillstyle() != \"none\"\n"], "sample_562": ["def test_markevery_float(fig_test, fig_ref):\n    x = np.linspace(0, 10)\n    y = np.sin(x)\n\n    ax1 = fig_test.add_subplot()\n    ax1.plot(x, y, 'o-', markevery=0.1)\n\n    ax2 = fig_ref.add_subplot()\n    ax2.plot(x, y, 'o-')\n    markers, = ax2.plot(x[::10], y[::10], 'o')\n    markers.set_markerfacecolor('none')\n    markers.set_markeredgewidth(0)\n", "def test_get_dash_pattern():\n    offset, dashes = mlines._get_dash_pattern('dashed')\n    assert offset == 0\n    assert dashes == tuple(mpl.rcParams['lines.dashed_pattern'])\n\n    offset, dashes = mlines._get_dash_pattern((1, (2, 3)))\n    assert offset == 1\n    assert dashes == (2, 3)\n\n    with pytest.raises(ValueError):\n        mlines._get_dash_pattern('invalid')\n\n    with pytest.raises(ValueError):\n        mlines._get_dash_pattern((None, (2, 3)))\n", "def test_axline():\n    fig, ax = plt.subplots()\n    line = mlines.AxLine((0.5, 0.5), (1.0, 1.0))\n    ax.add_artist(line)\n\n    # Test slope computation.\n    assert line.get_slope() is None\n\n    # Test xy1 and xy2 properties.\n    assert line.get_xy1() == (0.5, 0.5)\n    assert line.get_xy2() == (1.0, 1.0)\n\n    # Test set_xy1 and set_xy2 methods.\n    line.set_xy1((0.6, 0.6))\n    assert line.get_xy1() == (0.6, 0.6)\n    line.set_xy2((1.1, 1.1))\n    assert line.get_xy2() == (1.1, 1.1)\n\n    # Test slope property when setting xy2.\n    line.set_xy2(None)\n    assert line.get_xy2() is None\n    assert line.get_slope() == 1.0\n", "def test_line2d_xy_data(fig_test, fig_ref):\n    x = np.array([1, 2, 3])\n    y = np.array([4, 5, 6])\n\n    fig_test.add_subplot().plot(x, y)\n\n    line = mlines.Line2D(x, y)\n    fig_ref.add_subplot().add_line(line)\n", "def test_axline(fig_test, fig_ref):\n    ax = fig_ref.add_subplot()\n    ax.axline((1, 2), (3, 4))\n    ax = fig_test.add_subplot()\n    line = mlines.AxLine((1, 2), (3, 4))\n    ax.add_line(line)\n"], "sample_563": ["def test_draggable_offsetbox():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    ab = AnnotationBbox(da, (0.5, 0.5), xycoords='axes fraction')\n    ax.add_artist(ab)\n\n    # Check that the DraggableOffsetBox is initialized correctly\n    dob = DraggableOffsetBox(ab, ab.offsetbox)\n    assert dob.ref_artist == ab\n    assert dob.offsetbox == ab.offsetbox\n\n    # Check that the DraggableOffsetBox is picking the correct artist\n        assert event.artist == ab\n\n    fig.canvas.mpl_connect('pick_event', on_pick)\n    dob.on_pick(MouseEvent(\"button_press_event\", fig.canvas, 10, 10, MouseButton.LEFT))\n", "def test_auxtransformbox():\n    # Test AuxTransformBox\n    fig, ax = plt.subplots()\n    auxbox = AuxTransformBox(ax.transData)\n    ab = AnnotationBbox(auxbox, (0.5, 0.5), xycoords='axes fraction')\n    ax.add_artist(ab)\n    \n    text = TextArea(\"Test\")\n    auxbox.add_artist(text)\n\n    fig.canvas.draw()\n\n    assert text.get_transform() == auxbox.get_transform()\n", "def test_auxtransformbox_get_bbox():\n    aux_transform = mtransforms.Affine2D()\n    ab = AuxTransformBox(aux_transform)\n    renderer = plt.gcf().canvas.get_renderer()\n\n    # No children, default bbox should be (0, 0, 0, 0) with some padding\n    bbox = ab.get_bbox(renderer)\n    assert_allclose(bbox.bounds, (-1, -1, 2, 2))\n\n    # Add a child with a specific bbox\n    child = OffsetBox()\n    child.get_bbox = lambda renderer: Bbox.from_bounds(10, 20, 30, 40)\n    ab.add_artist(child)\n\n    # The AuxTransformBox's bbox should now match the child's bbox\n    bbox = ab.get_bbox(renderer)\n    assert_allclose(bbox.bounds, (10, 20, 30, 40))\n", "def test_offsetbox_bbox():\n    # Test that OffsetBox.get_bbox returns the correct bounding box.\n    fig, ax = plt.subplots()\n    renderer = fig.canvas.get_renderer()\n\n    offsetbox = OffsetBox()\n    bbox = offsetbox.get_bbox(renderer)\n\n    assert_allclose(bbox.bounds, (0, 0, 0, 0))\n\n    child = DrawingArea(10, 20)\n    offsetbox._children = [child]\n    bbox = offsetbox.get_bbox(renderer)\n\n    assert_allclose(bbox.bounds, (0, 0, 10, 20))\n", "def test_offsetbox_alignment():\n    fig, ax = plt.subplots()\n\n    offsetbox1 = TextArea(\"Left-aligned\", textprops={'ha': 'left'})\n    ab1 = AnchoredOffsetbox('upper left', child=offsetbox1)\n    ax.add_artist(ab1)\n\n    offsetbox2 = TextArea(\"Centered\", textprops={'ha': 'center'})\n    ab2 = AnchoredOffsetbox('upper center', child=offsetbox2)\n    ax.add_artist(ab2)\n\n    offsetbox3 = TextArea(\"Right-aligned\", textprops={'ha': 'right'})\n    ab3 = AnchoredOffsetbox('upper right', child=offsetbox3)\n    ax.add_artist(ab3)\n"], "sample_565": ["def test_inset_locator_invalid_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError):\n        inset_axes(ax, width=\"40%\", height=\"30%\",\n                   bbox_to_anchor=(0.4, 0.5),\n                   bbox_transform=ax.transAxes)\n\n    with pytest.warns(UserWarning):\n        inset_axes(ax, width=\"40%\", height=\"30%\",\n                   bbox_transform=ax.transAxes)\n", "def test_inset_axes_locator():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, width=\"30%\", height=\"40%\", loc='lower left')\n    assert isinstance(inset_ax.get_axes_locator(), AnchoredSizeLocator)\n", "def test_inset_locator_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    inset_axes(ax, width=\"40%\", height=\"30%\", bbox_to_anchor=(0.4, 0.5, 0.6, 0.7))\n    fig.canvas.draw()\n\n    # Check that the inset axes is correctly positioned.\n    inset_ax = fig.axes[1]\n    assert inset_ax.get_position().x0 == pytest.approx(0.4 * fig.bbox.width)\n    assert inset_ax.get_position().y0 == pytest.approx(0.5 * fig.bbox.height)\n    assert inset_ax.get_position().width == pytest.approx(0.24 * fig.bbox.width)\n    assert inset_ax.get_position().height == pytest.approx(0.21 * fig.bbox.height)\n\n    # Check that the inset axes is correctly sized.\n    assert inset_ax.get_window_extent().width == pytest.approx(0.24 * fig.bbox.width)\n    assert inset_ax.get_window_extent().height == pytest.approx(0.21 * fig.bbox.height)\n", "def test_bboxconnector_patch():\n    fig, ax = plt.subplots()\n    ax.set(aspect=1, xlim=(0, 10), ylim=(0, 10))\n\n    bbox1 = Bbox.from_extents(1, 1, 4, 4)\n    bbox2 = Bbox.from_extents(6, 1, 9, 4)\n\n    p = BboxConnectorPatch(\n        bbox1, bbox2, loc1a=1, loc2a=2, loc1b=4, loc2b=3,\n        ec=\"r\", fc=\"b\")\n    p.set_clip_on(False)\n    ax.add_patch(p)\n\n    bbox3 = Bbox.from_extents(1, 6, 4, 9)\n    bbox4 = Bbox.from_extents(6, 6, 9, 9)\n\n    p = BboxConnectorPatch(\n        bbox3, bbox4, loc1a=1, loc2a=2, loc1b=4, loc2b=3,\n        ec=\"g\", fc=\"y\")\n    p.set_clip_on(False)\n    ax.add_patch(p)\n", "def test_bboxconnector():\n    fig, ax = plt.subplots()\n    bbox1 = Bbox.from_extents(0.1, 0.1, 0.4, 0.4)\n    bbox2 = Bbox.from_extents(0.6, 0.1, 0.9, 0.4)\n    connector = BboxConnector(bbox1, bbox2, loc1=2, loc2=1)\n    ax.add_patch(connector)\n    fig.canvas.draw()\n    assert connector.get_path().vertices.shape == (2, 2)\n    assert_array_almost_equal(connector.get_path().vertices,\n                              [[0.4, 0.1], [0.6, 0.1]])\n"], "sample_566": ["def test_add_artist_non_artist(fig_test, fig_ref):\n    fig_test.subplots()\n    line = plt.Line2D([.2, .7], [.7, .7])\n    with pytest.raises(TypeError, match=\"not an Artist\"):\n        fig_test.add_artist(line.__dict__)\n    fig_ref.subplots()\n    fig_ref.add_artist(line)\n", "def test_subfigure_get_axes(fig_test, fig_ref):\n    sub_fig = fig_test.subfigures()\n    ax1 = sub_fig.add_subplot(121)\n    ax2 = sub_fig.add_subplot(122)\n\n    ax3 = fig_ref.add_subplot(121)\n    ax4 = fig_ref.add_subplot(122)\n\n    assert len(sub_fig.get_axes()) == 2\n    assert len(fig_test.get_axes()) == 2\n\n    assert len(fig_ref.get_axes()) == 2\n\n    # check that axes are in the correct order...\n    for a1, a2 in zip([ax1, ax2], sub_fig.get_axes()):\n        assert a1 is a2\n    for a1, a2 in zip([ax1, ax2], fig_test.get_axes()):\n        assert a1 is a2\n    for a1, a2 in zip([ax3, ax4], fig_ref.get_axes()):\n        assert a1 is a2\n", "def test_sup_xlabel_ylabel_title_setter():\n    fig = Figure()\n    xlabel = \"X Axis\"\n    ylabel = \"Y Axis\"\n    title = \"Title\"\n\n    fig.supxlabel(xlabel)\n    fig.supylabel(ylabel)\n    fig.suptitle(title)\n\n    assert fig.get_supxlabel() == xlabel\n    assert fig.get_supylabel() == ylabel\n    assert fig.get_suptitle() == title\n", "def test_suppress_composite():\n    fig, ax = plt.subplots()\n    fig.suppressComposite = True\n    assert fig.suppressComposite\n    fig.suppressComposite = False\n    assert not fig.suppressComposite\n    with pytest.raises(TypeError):\n        fig.suppressComposite = 'maybe'\n", "def test_subfigure_bbox(fig_test, fig_ref):\n    # test that subfigure bbox is properly updated\n    fig_test.subplots()\n    subfig = fig_test.subfigures(1, 2)[0]\n    ax = subfig.subplots()[0]\n    ax.plot([1, 2, 3])\n\n    fig_ref.subplots()\n    ax_ref = fig_ref.subplots()[0]\n    ax_ref.plot([1, 2, 3])\n    ax_ref.set_position(subfig.bbox.bounds)\n"], "sample_567": ["def test_text_set_math_fontfamily(fig_test, fig_ref):\n    fig_test.text(0.5, 0.5, 'foo', math_fontfamily='dejavusans')\n\n    mpl.rcParams['mathtext.fontset'] = 'dejavusans'\n    fig_ref.text(0.5, 0.5, 'foo')\n", "def test_set_rotation_mode():\n    txt = Text(.5, .5, \"foo\\nbar\")\n    assert txt._rotation_mode == \"default\"\n\n    txt.set_rotation_mode(\"anchor\")\n    assert txt._rotation_mode == \"anchor\"\n\n    txt.set_rotation_mode(\"default\")\n    assert txt._rotation_mode == \"default\"\n\n    with pytest.raises(ValueError):\n        txt.set_rotation_mode(\"invalid\")\n\n    txt.set_rotation_mode(None)\n    assert txt._rotation_mode == \"default\"\n", "def test_text_get_rotation_mode():\n    txt = Text(.5, .5, \"foo\\nbar\")\n    assert txt.get_rotation_mode() == 'default'\n\n    txt.set_rotation_mode('anchor')\n    assert txt.get_rotation_mode() == 'anchor'\n\n    txt.set_rotation_mode('default')\n    assert txt.get_rotation_mode() == 'default'\n\n    with pytest.raises(ValueError):\n        txt.set_rotation_mode('invalid')\n", "def test_text_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"test\", bbox=dict(facecolor='red'))\n    fig.canvas.draw()\n    assert isinstance(text.get_bbox_patch(), mpatches.FancyBboxPatch)\n", "def test_text_contains():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Hello World\")\n    event1 = MouseEvent('button_press_event', fig.canvas, 400, 300, 1, None)\n    event2 = MouseEvent('button_press_event', fig.canvas, 100, 100, 1, None)\n\n    fig.canvas.draw()\n    assert text.contains(event1)[0]\n    assert not text.contains(event2)[0]\n"], "sample_568": ["def test_line_collection_2d_to_3d_modification(fig_test, fig_ref):\n    # Test that modifying Line3DCollection properties after creation works.\n    segments = np.random.rand(10, 2, 3)\n    lc = art3d.Line3DCollection(segments, linewidths=3)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(lc)\n    lc.set_edgecolor('C2')\n    lc.set_alpha(0.7)\n    assert lc.get_depthshade()\n    lc.set_depthshade(False)\n    assert not lc.get_depthshade()\n\n    lc = art3d.Line3DCollection(segments, edgecolor='C2', alpha=0.7, depthshade=False)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.add_collection3d(lc)\n", "def test_line3d_collection_modification(fig_test, fig_ref):\n    # Test that modifying Line3DCollection properties after creation works.\n    x = [1, 2, 3]\n    y = [4, 5, 6]\n    z = [7, 8, 9]\n    c = art3d.Line3DCollection([list(zip(x, y, z))], linewidths=3)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(c)\n    c.set_edgecolor('C2')\n    c.set_alpha(0.7)\n\n    c = art3d.Line3DCollection([list(zip(x, y, z))], linewidths=3,\n                               edgecolor='C2', alpha=0.7)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.add_collection3d(c)\n", "def test_line_collection_2d_to_3d_modification(fig_test, fig_ref):\n    # Test that modifying Line3DCollection properties after creation works.\n    lc = LineCollection([[(0, 0), (1, 1)], [(1, 1), (2, 2)]], colors='r')\n    art3d.line_collection_2d_to_3d(lc, zs=0)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    ax_test.add_collection3d(lc)\n    lc.set_edgecolor('C2')\n    lc.set_alpha(0.7)\n    assert lc.get_depthshade()\n    lc.set_depthshade(False)\n    assert not lc.get_depthshade()\n\n    lc = LineCollection([[(0, 0), (1, 1)], [(1, 1), (2, 2)]], colors='r')\n    art3d.line_collection_2d_to_3d(lc, zs=0)\n    lc.set_edgecolor('C2')\n    lc.set_alpha(0.7)\n    lc.set_depthshade(False)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    ax_ref.add_collection3d(lc)\n", "def test_scatter_masked_color_array():\n    \"\"\"\n    Test color parameter usage with non-finite coordinate arrays and color array.\n\n    GH#26236\n    \"\"\"\n\n    x = [np.nan, 1, 2,  1]\n    y = [0, np.inf, 2,  1]\n    z = [0, 1, -np.inf, 1]\n    colors = np.array([\n        [0.0, 0.0, 0.0, 1],\n        [0.0, 0.0, 0.0, 1],\n        [0.0, 0.0, 0.0, 1],\n        [0.0, 0.0, 0.0, 1]\n    ])\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    path3d = ax.scatter(x, y, z, c=colors)\n\n    # Assert sizes' equality\n    assert len(path3d.get_offsets()) ==\\\n           len(super(type(path3d), path3d).get_facecolors())\n", "def test_scatter3d_color_cycle(fig_test, fig_ref):\n    x = np.arange(10)\n    y = np.arange(10)\n    z = np.arange(10)\n\n    ax_test = fig_test.add_subplot(projection='3d')\n    for i in range(5):\n        ax_test.scatter(x, y, z)\n\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n    for i in range(5):\n        ax_ref.scatter(x, y, z, color=colors[i % len(colors)])\n"], "sample_569": ["def test_lmplot_logistic_regression(self):\n\n    g = lm.lmplot(x=\"x\", y=\"z\", data=self.df, logistic=True)\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, self.df.x)\n    npt.assert_array_equal(y, self.df.z)\n", "def test_lmplot_line_kws(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", hue=\"h\", data=self.df, ci=None,\n                  line_kws={\"linewidth\": 5})\n    red_line, blue_line = g.axes[0, 0].lines\n\n    assert red_line.get_linewidth() == 5\n    assert blue_line.get_linewidth() == 5\n", "def test_lmplot_x_estimator(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, x_estimator=np.mean)\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(np.unique(x), np.sort(np.unique(self.df.x)))\n    npt.assert_array_almost_equal(y, self.df.groupby(\"x\").y.mean())\n", "def test_lmplot_x_estimator(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, x_estimator=np.mean)\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(np.unique(x), np.sort(np.unique(self.df.x)))\n    npt.assert_array_almost_equal(y, self.df.groupby(\"x\")[\"y\"].mean())\n", "def test_lmplot_line_kws(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", hue=\"h\", data=self.df, line_kws={'lw': 3})\n    ax = g.axes[0, 0]\n\n    for line in ax.lines:\n        assert line.get_lw() == 3\n"], "sample_570": ["def test_validate_errorbar_arg(self):\n\n    # Test with None\n    method, level = _validate_errorbar_arg(None)\n    assert method is None\n    assert level is None\n\n    # Test with callable\n    func = lambda x: (x.min(), x.max())  # noqa: E731\n    method, level = _validate_errorbar_arg(func)\n    assert method is func\n    assert level is None\n\n    # Test with string\n    method, level = _validate_errorbar_arg(\"ci\")\n    assert method == \"ci\"\n    assert level == 95\n\n    # Test with tuple of string and number\n    method, level = _validate_errorbar_arg((\"ci\", 50))\n    assert method == \"ci\"\n    assert level == 50\n\n    # Test with invalid string\n    with pytest.raises(ValueError):\n        _validate_errorbar_arg(\"invalid\")\n\n    # Test with tuple of invalid string and number\n    with pytest.raises(ValueError):\n        _validate_errorbar_arg((\"invalid\", 50))\n\n    # Test with tuple of string and non-number\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg((\"ci\", \"50\"))\n", "def test_validate_errorbar_arg(self):\n\n    # Test None errorbar argument\n    assert _validate_errorbar_arg(None) == (None, None)\n\n    # Test string errorbar arguments\n    for method in [\"ci\", \"pi\", \"se\", \"sd\"]:\n        result = _validate_errorbar_arg(method)\n        assert result[0] == method\n        assert result[1] == {\"ci\": 95, \"pi\": 95, \"se\": 1, \"sd\": 1}[method]\n\n    # Test tuple errorbar arguments\n    for method, level in [(\"ci\", 90), (\"pi\", 99), (\"se\", 2), (\"sd\", 3)]:\n        result = _validate_errorbar_arg((method, level))\n        assert result[0] == method\n        assert result[1] == level\n\n    # Test callable errorbar argument\n        return x.min(), x.max()\n\n    result = _validate_errorbar_arg(custom_func)\n    assert result[0] is custom_func\n    assert result[1] is None\n\n    # Test invalid errorbar arguments\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg(123)\n\n    with pytest.raises(ValueError):\n        _validate_errorbar_arg((\"invalid_method\", 95))\n\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg((\"ci\", \"invalid_level\"))\n", "def test_validate_errorbar_arg():\n    # Test valid errorbar arguments\n    assert _validate_errorbar_arg(None) == (None, None)\n    assert _validate_errorbar_arg(\"ci\") == (\"ci\", 95)\n    assert _validate_errorbar_arg((\"ci\", 90)) == (\"ci\", 90)\n    assert _validate_errorbar_arg(\"pi\") == (\"pi\", 95)\n    assert _validate_errorbar_arg((\"pi\", 90)) == (\"pi\", 90)\n    assert _validate_errorbar_arg(\"se\") == (\"se\", 1)\n    assert _validate_errorbar_arg((\"se\", 2)) == (\"se\", 2)\n    assert _validate_errorbar_arg(\"sd\") == (\"sd\", 1)\n    assert _validate_errorbar_arg((\"sd\", 2)) == (\"sd\", 2)\n\n    # Test invalid errorbar arguments\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg(123)\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg((\"ci\", \"abc\"))\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg((\"invalid\", 90))\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg(\"invalid\")\n", "    def test_none(self):\n        method, level = _validate_errorbar_arg(None)\n        assert method is None\n        assert level is None\n", "def test_bad_errorbar_arg(self):\n\n    with pytest.raises(TypeError, match=\"`errorbar` must be a callable, string, or\"):\n        EstimateAggregator(\"mean\", errorbar=(\"invalid\",))\n\n    with pytest.raises(TypeError, match=\"`errorbar` must be a callable, string, or\"):\n        EstimateAggregator(\"mean\", errorbar=(1, 2, 3))\n\n    with pytest.raises(TypeError, match=\"`errorbar` must be a callable, string, or\"):\n        EstimateAggregator(\"mean\", errorbar=1)\n\n    with pytest.raises(ValueError, match=\"`errorbar` must be one of\"):\n        EstimateAggregator(\"mean\", errorbar=(\"ci\", \"invalid\"))\n\n    with pytest.raises(TypeError, match=\"`errorbar` must be a callable, string, or\"):\n        EstimateAggregator(\"mean\", errorbar=(\"ci\", \"95\", \"extra\"))\n"], "sample_571": ["def test_lmplot_hue_order(self):\n\n    hue_order = [\"y\", \"x\"]\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", hue_order=hue_order)\n    assert g.hue_names == hue_order\n", "def test_lmplot_x_partial(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, x_partial=\"z\")\n    ax = g.axes[0, 0]\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 2\n\n    x, y = ax.collections[0].get_offsets().T\n    npt.assert_array_equal(x, self.df.x)\n    npt.assert_array_equal(y, self.df.y)\n", "def test_lmplot_legend_out(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend_out=True)\n    assert g._legend is not None\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend_out=False)\n    assert g._legend is None\n", "def test_lmplot_line_kws(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\",\n                  line_kws={\"lw\": 3, \"ls\": \"--\"})\n\n    for ax in g.axes.flat:\n        for line in ax.lines:\n            assert line.get_linewidth() == 3\n            assert line.get_linestyle() == \"--\"\n", "def test_lmplot_palette(self):\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", palette=\"Set1\")\n    red_scatter, blue_scatter = g.axes[0, 0].collections\n\n    palette = color_palette(\"Set1\", n_colors=2)\n    npt.assert_array_equal(palette[0], red_scatter.get_facecolors()[0, :3])\n    npt.assert_array_equal(palette[1], blue_scatter.get_facecolors()[0, :3])\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", palette=[\"red\", \"blue\"])\n    red_scatter, blue_scatter = g.axes[0, 0].collections\n\n    npt.assert_array_equal((1, 0, 0), red_scatter.get_facecolors()[0, :3])\n    npt.assert_array_equal((0, 0, 1), blue_scatter.get_facecolors()[0, :3])\n"], "sample_572": ["    def test_string_arg(self):\n        arg = \"ci\"\n        method, level = _validate_errorbar_arg(arg)\n        assert method == \"ci\"\n        assert level == 95\n", "def test_validate_errorbar_arg():\n    # Test that _validate_errorbar_arg correctly validates errorbar arguments\n\n    # Test that None is returned when input is None\n    assert _validate_errorbar_arg(None) == (None, None)\n\n    # Test that a callable is returned when input is a callable\n        return x\n    assert _validate_errorbar_arg(custom_func) == (custom_func, None)\n\n    # Test that a string method with default level is returned when input is a string\n    assert _validate_errorbar_arg(\"ci\") == (\"ci\", 95)\n    assert _validate_errorbar_arg(\"pi\") == (\"pi\", 95)\n    assert _validate_errorbar_arg(\"se\") == (\"se\", 1)\n    assert _validate_errorbar_arg(\"sd\") == (\"sd\", 1)\n\n    # Test that a string method with custom level is returned when input is a tuple\n    assert _validate_errorbar_arg((\"ci\", 50)) == (\"ci\", 50)\n    assert _validate_errorbar_arg((\"pi\", 75)) == (\"pi\", 75)\n    assert _validate_errorbar_arg((\"se\", 2)) == (\"se\", 2)\n    assert _validate_errorbar_arg((\"sd\", 3)) == (\"sd\", 3)\n\n    # Test that an invalid input raises a TypeError\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg(123)\n\n    # Test that an invalid string method raises a ValueError\n    with pytest.raises(ValueError):\n        _validate_errorbar_arg(\"invalid\")\n\n    # Test that an invalid level raises a TypeError\n    with pytest.raises(TypeError):\n        _validate_errorbar_arg((\"ci\", \"invalid\"))\n", "    def test_valid_string_args(self):\n        valid_args = [\"ci\", \"pi\", \"se\", \"sd\"]\n        for arg in valid_args:\n            method, level = _validate_errorbar_arg(arg)\n            assert method == arg\n            assert level is None or isinstance(level, (int, float))\n", "    def test_none(self):\n        assert _validate_errorbar_arg(None) == (None, None)\n", "    def test_none(self):\n        method, level = _validate_errorbar_arg(None)\n        assert method is None\n        assert level is None\n"], "sample_573": ["def test_insufficient_unique_values(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = df[\"group\"].map({\"x\": 1, \"y\": 2})\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert_frame_equal(res, pd.DataFrame(columns=[\"x\", \"y\", \"group\"], index=pd.RangeIndex(0)))\n", "def test_too_few_unique_x(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = np.repeat([1, 2], len(df) // 2)\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert_frame_equal(res, pd.DataFrame(columns=[\"x\", \"y\"]))\n", "def test_insufficient_data(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = np.repeat([1, 2], len(df) // 2)\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n    ngroups = df[\"group\"].nunique()\n    assert_array_equal(res.index, np.arange(ngroups * 0))\n\n    for _, part in res.groupby(\"group\"):\n        assert_array_equal(part[\"x\"], [])\n        assert_array_equal(part[\"y\"], [])\n", "def test_too_few_unique_x(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = np.repeat([1, 2], len(df) // 2)\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\", \"group\"])\n    assert res.empty\n", "def test_constant_x(self, df):\n\n    df[\"x\"] = 1\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=1, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n    assert res.empty\n"], "sample_574": ["    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n", "def test_tick_at(self, t):\n\n    dates = pd.to_datetime([\"1972-09-27\", \"1975-06-24\", \"1980-12-14\"])\n    locator = mpl.dates.FixedLocator(mpl.dates.date2num(dates))\n    s = Temporal().tick(locator=locator)\n    a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n    assert_array_equal(a.major.locator(), mpl.dates.date2num(dates))\n", "    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n", "    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n", "    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n"], "sample_575": ["    def test_convert_units_numeric(self):\n\n        ax = PseudoAxis(mpl.scale.LinearScale())\n        x = pd.Series([1, 2, 3])\n        assert_array_equal(ax.convert_units(x), x)\n", "    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n", "    def x(self):\n        return pd.Series([\"a\", \"c\", \"b\", \"c\"], name=\"x\")\n", "    def test_set_view_interval(self):\n        ax = PseudoAxis(mpl.scale.LinearScale())\n        ax.set_view_interval(10, 20)\n        assert ax.get_view_interval() == (10, 20)\n", "    def test_scale_priority(self):\n\n        assert Nominal._priority == 3\n        assert Temporal._priority == 2\n        assert Continuous._priority == 1\n"], "sample_576": ["def test_default_legend_title_from_mark(self, long_df):\n\n    class MockMarkTitle(MockMark):\n        _legend_title = \"Mock\"\n\n    p = Plot(long_df, x=\"x\", y=\"y\", color=\"a\").add(MockMarkTitle()).plot()\n    legend, = p._figure.legends\n    assert legend.get_title().get_text() == \"Mock\"\n", "    def test_without_pyplot(self):\n\n        p = Plot().plot()\n        assert not hasattr(p, \"_pyplot\")\n        assert isinstance(p._figure, mpl.figure.Figure)\n", "def test_pairwise_xy_variables(self, long_df):\n\n    p = Plot(long_df).pair(x=[\"x\", \"y\"], y=[\"f\", \"z\"])\n    assert p._variables == [\"x0\", \"x1\", \"y0\", \"y1\"]\n\n    p = Plot(long_df).pair(x=[\"x\", \"y\"])\n    assert p._variables == [\"x0\", \"x1\", \"y\"]\n\n    p = Plot(long_df).pair(y=[\"f\", \"z\"])\n    assert p._variables == [\"x\", \"y0\", \"y1\"]\n", "def test_facet_order_with_pairing(self, long_df):\n\n    x_vars = [\"x\", \"y\"]\n    facet_var = \"a\"\n    order = [\"b\", \"a\"]\n\n    p = Plot(long_df).pair(x=x_vars).facet(col=facet_var, order=order).plot()\n\n    expected_levels = categorical_order(long_df[facet_var], order)\n    for ax in p._figure.axes:\n        title = ax.get_title()\n        assert title in expected_levels\n", "def test_pair_cross_mismatched_lengths(self, long_df):\n\n    p = Plot(long_df)\n    with pytest.raises(ValueError, match=\"Lengths of the `x` and `y`\"):\n        p.pair(x=[\"a\", \"b\"], y=[\"x\", \"y\", \"z\"], cross=True)\n"], "sample_577": ["    def test_scale_without_artist(self, xy):\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy).add(MockMark(), color=s, size=s).scale(size=None).plot()\n        legend, = p._figure.legends\n        labels = [t.get_text() for t in legend.get_texts()]\n        assert labels == categorical_order(s)\n", "    def test_complete_theme(self, long_df):\n\n        theme = {\n            \"axes.facecolor\": \".888\",\n            \"axes.edgecolor\": \".2\",\n            \"axes.labelcolor\": \".1\",\n            \"xtick.color\": \".1\",\n            \"ytick.color\": \".1\",\n        }\n        p = Plot(long_df).theme(theme).plot()\n        for key, val in theme.items():\n            assert mpl.rcParams[key] == val\n", "    def test_default_size(self):\n\n        p = Plot().plot()\n        assert p._figure.get_size_inches() == (6, 4)\n", "    def test_default_tight(self):\n\n        p = Plot().plot()\n        assert p._figure.get_layout_engine().__class__.__name__ == \"TightLayoutEngine\"\n", "def test_plot_with_layer_specific_data_variable(self, long_df):\n\n    x = long_df[\"x\"]\n    y = long_df[\"y\"]\n    m = MockMark()\n    Plot(long_df).add(m, x=x, y=y).plot()\n    for var, col in zip(\"xy\", (x, y)):\n        assert_vector_equal(m.passed_data[0][var], col)\n"], "sample_578": ["def test_baseline(self, x, y):\n\n    baseline = 5\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == y[i] + baseline\n", "def test_baseline(self, x, y):\n\n    baseline = 5\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    assert len(paths) == len(x)\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 0] == pytest.approx(x[i] - .5)\n        assert verts[1, 0] == pytest.approx(x[i] + .5)\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == y[i] + baseline\n", "def test_baseline(self, x, y):\n\n    baseline = 5\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    assert len(paths) == len(x)\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == y[i] + baseline\n", "def test_baseline(self, x, y):\n\n    baseline = 5\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    assert len(paths) == len(x)\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 0] == pytest.approx(x[i] - .5)\n        assert verts[1, 0] == pytest.approx(x[i] + .5)\n        assert verts[0, 1] == baseline\n        assert verts[3, 1] == y[i] + baseline\n", "def test_baseline(self, x, y):\n\n    baseline = 5\n    p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n    ax = p._figure.axes[0]\n    paths = ax.collections[0].get_paths()\n    for i, path in enumerate(paths):\n        verts = path.vertices\n        assert verts[0, 1] == pytest.approx(baseline)\n        assert verts[3, 1] == pytest.approx(y[i] + baseline)\n"], "sample_579": ["def test_clustermap_annotation_with_mask(self):\n    df = pd.DataFrame(data={'a': [1, 1, 1],\n                            'b': [2, np.nan, 2],\n                            'c': [3, 3, np.nan]})\n    mask = np.isnan(df.values)\n\n    g = mat.clustermap(df, annot=True, fmt=\".1f\", mask=mask)\n    for val, text in zip(np.ma.masked_array(g.data2d, mask).compressed(), g.ax_heatmap.texts):\n        assert text.get_text() == f\"{val:.1f}\"\n", "def test_clustermap_savefig_tight(self):\n    # Test if clustermap can be saved with tight layout.\n    g = mat.clustermap(self.df_norm)\n    with tempfile.NamedTemporaryFile(suffix=\".png\") as f:\n        g.savefig(f.name, bbox_inches=\"tight\")\n", "def test_clustermap_cbar_kws(self):\n    kws = self.default_kws.copy()\n    cbar_kws = dict(shrink=0.5, aspect=10, orientation='horizontal')\n    g = mat.clustermap(self.df_norm, cbar_kws=cbar_kws)\n\n    for key, val in cbar_kws.items():\n        assert getattr(g.ax_cbar, f'get_{key}')() == val\n", "def test_clustermap_linecolor(self):\n    g = mat.clustermap(self.df_norm, linecolor='blue', linewidths=0.5)\n    mesh = g.ax_heatmap.collections[0]\n    assert tuple(mesh.get_edgecolor().squeeze())[:3] == (0, 0, 1)\n    assert mesh.get_linewidths()[0] == 0.5\n", "def test_clustermap_colorbar_kws(self):\n    kws = self.default_kws.copy()\n    cbar_kws = dict(shrink=0.5, orientation='horizontal')\n    g = mat.clustermap(self.df_norm, cbar_kws=cbar_kws)\n\n    assert g.ax_cbar.get_position().height < 0.1\n    assert g.ax_cbar.get_orientation() == 'horizontal'\n"], "sample_580": ["def test_categorical_order():\n\n    s = pd.Series([\"a\", \"c\", \"b\", \"a\"])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([1, 3, 2, 1])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([1, 3, 2, np.nan])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series(pd.Categorical([\"a\", \"c\", \"b\"], categories=[\"b\", \"a\", \"c\"]))\n    assert categorical_order(s) == [\"b\", \"a\", \"c\"]\n\n    order = [\"c\", \"b\", \"a\"]\n    assert categorical_order(s, order) == order\n", "def test_categorical_order():\n\n    s = pd.Series([\"a\", \"c\", \"b\"])\n    order = categorical_order(s)\n    assert order == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([1, 3, 2])\n    order = categorical_order(s)\n    assert order == [1, 2, 3]\n\n    s = pd.Series([1, 3, 2], dtype=\"category\")\n    s = s.cat.reorder_categories([3, 2, 1])\n    order = categorical_order(s)\n    assert order == [3, 2, 1]\n\n    s = pd.Series([1., np.nan, 3., 2.])\n    order = categorical_order(s)\n    assert order == [1., 2., 3.]\n\n    s = pd.Series([1, 3, 2])\n    order = categorical_order(s, order=[3, 2, 1])\n    assert order == [3, 2, 1]\n", "def test_variable_type_strict_boolean():\n\n    s = pd.Series([True, False, False])\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n\n    s = pd.Series([1, 0, 0], dtype='int64')\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) != \"boolean\"\n\n    s = pd.Series([True, False, pd.NA], dtype=\"boolean\")\n    assert variable_type(s, boolean_type=\"boolean\", strict_boolean=True) == \"boolean\"\n", "def test_categorical_order():\n\n    s = pd.Series([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([1, 2, 3, 1, 2, 3])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([1.0, 2.0, 3.0, 1.0, 2.0, 3.0])\n    assert categorical_order(s) == [1.0, 2.0, 3.0]\n\n    s = pd.Series([\"a\", \"b\", np.nan, \"a\", \"b\", np.nan])\n    assert categorical_order(s) == [\"a\", \"b\"]\n\n    s = pd.Series([1, 2, np.nan, 1, 2, np.nan])\n    assert categorical_order(s) == [1, 2]\n\n    s_cat = pd.Series([\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"], dtype=\"category\")\n    assert categorical_order(s_cat) == [\"a\", \"b\", \"c\"]\n\n    order = [\"c\", \"b\", \"a\"]\n    assert categorical_order(s, order=order) == order\n\n    s = pd.Series([pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)])\n    assert categorical_order(s) == [pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)]\n", "def test_categorical_order():\n\n    s = pd.Series([\"a\", \"c\", \"b\"])\n    order = categorical_order(s)\n    assert order == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([1, 3, 2])\n    order = categorical_order(s)\n    assert order == [1, 2, 3]\n\n    s = pd.Series([1, 3, 2], dtype=\"category\")\n    s = s.cat.reorder_categories([3, 2, 1])\n    order = categorical_order(s)\n    assert order == [3, 2, 1]\n\n    s = pd.Series([pd.Timestamp(3), pd.Timestamp(1), pd.Timestamp(2)])\n    order = categorical_order(s)\n    assert order == [pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)]\n\n    order = [\"b\", \"a\", \"c\"]\n    s = pd.Series([\"a\", \"c\", \"b\"])\n    assert categorical_order(s, order) == order\n\n    s = pd.Series([1, np.nan, 2])\n    order = categorical_order(s)\n    assert order == [1, 2]\n"], "sample_581": ["def test_blueprint_name_prefix(app, client) -> None:\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.get(\"/\")\n        return \"index\"\n\n    app.register_blueprint(bp, name_prefix=\"prefix.\")\n\n    assert client.get(\"/\").data == b\"index\"\n    assert bp.name == \"bp\"\n    assert \"prefix.bp\" in app.blueprints\n", "def test_blueprint_url_value_preprocessor(app, client) -> None:\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.url_value_preprocessor\n        if \"lang\" in values:\n            values[\"lang\"] = values[\"lang\"].upper()\n\n    @bp.route(\"/<lang>/\")\n        return lang\n\n    app.register_blueprint(bp)\n\n    response = client.get(\"/en/\")\n    assert response.data == b\"EN\"\n", "def test_blueprint_cligroup(app):\n    bp = flask.Blueprint(\"bp\", __name__, cli_group=None)\n\n    @bp.cli.command(\"cmd\")\n        pass\n\n    app.register_blueprint(bp)\n    assert \"cmd\" in app.cli.commands\n\n    bp = flask.Blueprint(\"bp\", __name__, cli_group=\"group\")\n\n    @bp.cli.command(\"cmd\")\n        pass\n\n    app.register_blueprint(bp)\n    assert \"group\" in app.cli.commands\n    assert \"cmd\" in app.cli.commands[\"group\"].commands\n\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.cli.command(\"cmd\")\n        pass\n\n    app.register_blueprint(bp, cli_group=\"group2\")\n    assert \"group2\" in app.cli.commands\n    assert \"cmd\" in app.cli.commands[\"group2\"].commands\n", "def test_blueprint_registering_with_app_context_processor(app, client) -> None:\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.app_context_processor\n        return dict(title=\"Blueprint\")\n\n    @bp.route(\"/\")\n        return flask.render_template_string(\"{{ title }}\")\n\n    app.register_blueprint(bp)\n\n    response = client.get(\"/\")\n    assert response.data == b\"Blueprint\"\n", "def test_blueprint_cloning(app, client) -> None:\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/\")\n        return \"index\"\n\n    @bp.errorhandler(403)\n        return \"forbidden\", 403\n\n    @bp.app_template_filter()\n        return s[::-1]\n\n    @bp.app_context_processor\n        return {\"value\": 42}\n\n    bp2 = bp\n\n    app.register_blueprint(bp)\n    app.register_blueprint(bp2, name=\"bp2\")\n\n    assert client.get(\"/\").data == b\"index\"\n    assert client.get(\"/bp2/\").status_code == 404\n\n    @app.route(\"/bp2/\")\n        flask.abort(403)\n\n    assert client.get(\"/bp2/\").data == b\"forbidden\"\n\n    with app.app_context():\n        assert app.jinja_env.filters[\"my_reverse\"](\"hello\") == \"olleh\"\n        assert app.jinja_env.globals[\"value\"] == 42\n"], "sample_582": ["def test_cli_blueprint_registration(app, runner):\n    \"\"\"Test blueprints with custom cli_group register commands correctly\"\"\"\n    custom = Blueprint(\"custom\", __name__, cli_group=\"customized\")\n    merged = Blueprint(\"merged\", __name__, cli_group=None)\n\n    @custom.cli.command(\"custom\")\n        click.echo(\"custom_result\")\n\n    @merged.cli.command(\"merged\")\n        click.echo(\"merged_result\")\n\n    app.register_blueprint(custom)\n    app.register_blueprint(merged)\n\n    result = runner.invoke(app.cli, [\"custom\", \"custom\"])\n    assert result.output == \"custom_result\\n\"\n\n    result = runner.invoke(app.cli, [\"merged\"])\n    assert result.output == \"merged_result\\n\"\n", "def test_cli_blueprint_group_merge(app, runner):\n    \"\"\"Test blueprint commands merged into a single group\"\"\"\n    bp = Blueprint(\"bp\", __name__, cli_group=\"merged\")\n    app.register_blueprint(bp)\n\n    @click.group(\"cli\", cls=AppGroup)\n        pass\n\n    @bp.cli.command(\"cmd1\")\n        click.echo(\"cmd1\")\n\n    @bp.cli.command(\"cmd2\")\n        click.echo(\"cmd2\")\n\n    cli.add_command(bp.cli)\n    result = runner.invoke(cli, [\"cmd1\"])\n    assert result.output == \"cmd1\\n\"\n\n    result = runner.invoke(cli, [\"cmd2\"])\n    assert result.output == \"cmd2\\n\"\n", "def test_cli_custom_command_groups(app, runner):\n    \"\"\"Test custom command groups can be used with the application\"\"\"\n    custom = Blueprint(\"custom\", __name__, cli_group=\"customized\")\n\n    @custom.cli.command(\"custom\")\n        click.echo(\"custom_result\")\n\n    app.register_blueprint(custom)\n    result = runner.invoke(app.cli, [\"custom\"])\n    assert result.exit_code == 0\n    assert result.output.strip() == \"custom_result\"\n", "def test_cli_blueprint_groups(app, runner):\n    \"\"\"Test blueprint groups are registered correctly to the application\"\"\"\n    bp = Blueprint(\"bp\", __name__, cli_group=\"custom\")\n\n    @bp.cli.command(\"cmd\")\n        click.echo(\"result\")\n\n    app.register_blueprint(bp)\n\n    result = runner.invoke(app.cli, [\"custom\", \"cmd\"])\n    assert result.exit_code == 0\n    assert result.output == \"result\\n\"\n", "def test_cli_runner_invoke(app):\n    \"\"\"Test that the CLI runner can invoke a command.\"\"\"\n    @click.command()\n        click.echo(\"test_result\")\n\n    result = app.test_cli_runner().invoke(test_command)\n    assert result.exit_code == 0\n    assert result.output == \"test_result\\n\"\n"], "sample_583": ["def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = np.array([0, 0, 2])\n    actual = indexing.posify_mask_indexer(indexer).tuple[0]\n    np.testing.assert_array_equal(expected, actual)\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = (np.array([0, 0, 2]), np.array([0, 1, 0]))\n    actual = indexing.posify_mask_indexer(indexer).tuple\n    assert len(actual) == 2\n    np.testing.assert_array_equal(expected[0], actual[0])\n    np.testing.assert_array_equal(expected[1], actual[1])\n\n    indexer = indexing.BasicIndexer((-1,))\n    actual = indexing.posify_mask_indexer(indexer).tuple[0]\n    assert actual == 0\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = np.array([0, 0, 2])\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected, actual.tuple[0])\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = np.array([0, 0, 2])\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected, actual.tuple[0])\n\n    indexer = indexing.BasicIndexer((-1,))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert actual.tuple == (0,)\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = np.array([0, 0, 2])\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected, actual.tuple[0])\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = np.array([0, 0, 2])\n    actual = indexing.posify_mask_indexer(indexer)\n    np.testing.assert_array_equal(expected, actual.tuple[0])\n\n    indexer = indexing.BasicIndexer((-1,))\n    expected = 0\n    actual = indexing.posify_mask_indexer(indexer)\n    assert expected == actual.tuple[0]\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = indexing.OuterIndexer((np.array([0, 0, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, type(expected))\n    np.testing.assert_array_equal(expected.tuple[0], actual.tuple[0])\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = indexing.VectorizedIndexer(\n        (np.array([0, 0, 2]), np.array([0, 1, 1])))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, type(expected))\n    for e, a in zip(expected.tuple, actual.tuple):\n        np.testing.assert_array_equal(e, a)\n", "def test_posify_mask_indexer():\n    indexer = indexing.OuterIndexer((np.array([0, -1, 2]),))\n    expected = indexing.OuterIndexer((np.array([0, 2, 2]),))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, type(expected))\n    np.testing.assert_array_equal(actual.tuple[0], expected.tuple[0])\n\n    indexer = indexing.VectorizedIndexer(\n        (np.array([0, -1, 2]), np.array([0, 1, -1])))\n    expected = indexing.VectorizedIndexer(\n        (np.array([0, 2, 2]), np.array([0, 1, 1])))\n    actual = indexing.posify_mask_indexer(indexer)\n    assert isinstance(actual, type(expected))\n    np.testing.assert_array_equal(actual.tuple[0], expected.tuple[0])\n    np.testing.assert_array_equal(actual.tuple[1], expected.tuple[1])\n"], "sample_584": ["    def test_combine_nested_uneven_depth(self):\n        ds = create_test_data\n        input = [[ds(0), ds(1)], [ds(2)], [ds(3), ds(4), ds(5)]]\n        with raises_regex(ValueError, 'sub-lists do not have consistent depths'):\n            combine_nested(input, concat_dim=['dim1', 'dim2'])\n", "    def test_combine_nd_data_vars_coords(self):\n        ds1 = Dataset({'a': ('x', [0, 1])}, coords={'x': [0, 1]})\n        ds2 = Dataset({'a': ('x', [2, 3])}, coords={'x': [2, 3]})\n\n        expected_all = Dataset({'a': ('x', [0, 1, 2, 3])}, coords={'x': [0, 1, 2, 3]})\n        actual_all = _combine_nd(OrderedDict([(0, ds1), (1, ds2)]), concat_dims=['x'],\n                                 data_vars='all', coords='all')\n        assert_identical(expected_all, actual_all)\n\n        expected_minimal = Dataset({'a': ('x', [0, 1, 2, 3])})\n        actual_minimal = _combine_nd(OrderedDict([(0, ds1), (1, ds2)]), concat_dims=['x'],\n                                     data_vars='minimal', coords='minimal')\n        assert_identical(expected_minimal, actual_minimal)\n\n        expected_different = Dataset({'a': ('x', [0, 1, 2, 3])}, coords={'x': [0, 1, 2, 3]})\n        actual_different = _combine_nd(OrderedDict([(0, ds1), (1, ds2)]), concat_dims=['x'],\n                                       data_vars='different', coords='different')\n        assert_identical(expected_different, actual_different)\n", "    def test_combine_nested_empty_input(self):\n        assert_identical(Dataset(), combine_nested([], concat_dim='x'))\n", "    def test_manual_combine_empty_input(self):\n        assert_identical(Dataset(), combine_nested([], concat_dim='x'))\n", "    def test_combine_with_none(self):\n        ds1 = Dataset({'x': [0]})\n        ds2 = Dataset({'y': [1]})\n        combined_ids = {(0, 0): ds1, (0, 1): ds2}\n        result = _combine_nd(combined_ids, concat_dims=['x', None])\n        expected = merge([ds1, ds2], compat='no_conflicts')\n        assert_identical(result, expected)\n"], "sample_585": ["def test_da_groupby_quantile():\n\n    array = xr.DataArray([1, 2, 3, 4, 5], dims='x')\n    expected = xr.DataArray([3], dims=[])\n    actual = array.groupby(xr.DataArray([0]*5, dims='x')).quantile(0.5)\n    assert_identical(expected, actual)\n", "def test_groupby_bins():\n    array = xr.DataArray([1, 2, 3, 4, 5], dims='x')\n    bins = [0, 2, 4, 6]\n    expected = xr.DataArray([2, 7, 5], [('x_bins', pd.IntervalIndex.from_breaks(bins))])\n    actual = array.groupby_bins('x', bins).sum()\n    assert_identical(expected, actual)\n", "def test_groupby_bins():\n    array = xr.DataArray([1, 2, 3, 4, 5], dims='x')\n    bins = [0, 2, 4, 6]\n    expected = xr.DataArray([2, 7, 5], dims='x_bins', coords={'x_bins': [1, 3, 5]})\n    actual = array.groupby_bins('x', bins).sum()\n    assert_identical(expected, actual)\n", "def test_da_groupby_quantile():\n\n    array = xr.DataArray([1, 2, 3, 4, 5], dims='x')\n    expected = xr.DataArray([2.5], dims=['quantile'])\n    actual = array.groupby('x').quantile(0.5)\n    assert_identical(expected, actual)\n\n    array = xr.DataArray([1, 2, 3, 4, 5], dims='x')\n    expected = xr.DataArray([3], dims=['quantile'])\n    actual = array.groupby('x').quantile(0.6)\n    assert_identical(expected, actual)\n\n    array = xr.DataArray([1, 2, 3, 4, 5], dims='x')\n    expected = xr.DataArray([2, 4], dims=['quantile'])\n    actual = array.groupby('x').quantile([0.25, 0.75])\n    assert_identical(expected, actual)\n", "def test_groupby_quantile():\n    array = xr.DataArray([1, 2, 3, 4, 5], dims='x')\n    expected = xr.DataArray([3], dims=[])\n    actual = array.groupby(xr.DataArray([0, 0, 0, 0, 0], dims='x')).quantile(0.5)\n    assert_identical(actual, expected)\n\n    # Test with multiple quantiles\n    expected = xr.DataArray([1, 3, 5], dims=['quantile'],\n                            coords={'quantile': [0, 0.5, 1]})\n    actual = array.groupby(xr.DataArray([0, 0, 0, 0, 0], dims='x')).quantile([0, 0.5, 1])\n    assert_identical(actual, expected)\n"], "sample_586": ["def test_concat_positions(self):\n    # Test that the `positions` argument works as expected\n    foo = DataArray([1, 2], coords=[(\"x\", [1, 2])])\n    bar = DataArray([3, 4], coords=[(\"x\", [3, 4])])\n\n    # Test with integer positions\n    actual = concat((foo, bar), dim=\"x\", positions=[0, 1])\n    expected = DataArray([1, 2, 3, 4], coords=[(\"x\", [1, 2, 3, 4])])\n    assert_identical(actual, expected)\n\n    # Test with array-like positions\n    actual = concat((foo, bar), dim=\"x\", positions=np.array([0, 1]))\n    assert_identical(actual, expected)\n\n    # Test with list of arrays as positions\n    actual = concat((foo, bar), dim=\"x\", positions=[[0], [1]])\n    assert_identical(actual, expected)\n\n    # Test with invalid positions\n    with raises_regex(ValueError, \"positions must be\"):\n        concat((foo, bar), dim=\"x\", positions=\"invalid\")\n\n    # Test with positions that are not the same length as the datasets\n    with raises_regex(ValueError, \"Length of positions must match\"):\n        concat((foo, bar), dim=\"x\", positions=[0])\n", "def test_concat_positions(self):\n    data = create_test_data()\n    split_data = [data.isel(dim1=slice(0, 3)), data.isel(dim1=slice(3, None))]\n    positions = [[0, 2, 4], [1, 3, 5]]\n    actual = concat(split_data, \"dim1\", positions=positions)\n    expected = Dataset()\n    for var in data.data_vars:\n        if \"dim1\" in data[var].dims:\n            expected[var] = (data[var].dims, np.concatenate([split_data[0][var].values, split_data[1][var].values], axis=data[var].dims.index(\"dim1\")))\n        else:\n            expected[var] = (data[var].dims, data[var].values)\n    expected.coords[\"dim1\"] = (\"dim1\", [0, 2, 4, 1, 3, 5])\n    assert_identical(expected, actual)\n", "def test_concat_dim_with_different_dtype(self):\n    ds1 = Dataset({\"x\": [1, 2]}, coords={\"y\": (\"x\", [1.0, 2.0])})\n    ds2 = Dataset({\"x\": [3, 4]}, coords={\"y\": (\"x\", [3, 4])})\n\n    expected = Dataset(\n        {\"x\": (\"z\", [1, 2, 3, 4])},\n        coords={\"y\": (\"z\", [1.0, 2.0, 3.0, 4.0])},\n    )\n\n    actual = concat([ds1, ds2], dim=\"z\")\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    # Test that the `positions` argument works as expected\n    foo = DataArray([1, 2], coords=[(\"x\", [1, 2])])\n    bar = DataArray([1, 2], coords=[(\"x\", [1, 3])])\n    baz = DataArray([1, 2], coords=[(\"x\", [1, 4])])\n\n    actual = concat((foo, bar, baz), dim=\"y\", positions=[0, 1, 2])\n    expected = DataArray(\n        [[1, 2, np.nan, np.nan], [1, np.nan, 2, np.nan], [1, np.nan, np.nan, 2]],\n        dims=[\"y\", \"x\"],\n        coords={\"x\": [1, 2, 3, 4]},\n    )\n    assert_identical(actual, expected)\n\n    actual = concat((foo, bar, baz), dim=\"y\", positions=[0, 0, 1])\n    expected = DataArray(\n        [[1, 2, np.nan, np.nan], [1, np.nan, 2, np.nan], [1, np.nan, np.nan, 2]],\n        dims=[\"y\", \"x\"],\n        coords={\"x\": [1, 2, 3, 4]},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    # Test that the `positions` argument is used correctly.\n    ds1 = Dataset({\"x\": [0, 1]})\n    ds2 = Dataset({\"x\": [2, 3]})\n    positions = [1, 0]\n    actual = concat([ds1, ds2], dim=\"y\", positions=positions)\n    expected = Dataset({\"x\": ((\"y\", \"x\"), [[2, 3], [0, 1]])})\n    assert_identical(actual, expected)\n"], "sample_587": ["def test_merge_no_conflicts_multi_dim_coords(self):\n    ds1 = xr.Dataset(\n        {\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]])},\n        coords={\"x\": [0, 1], \"y\": [10, 20]},\n    )\n    ds2 = xr.Dataset(\n        {\"a\": ((\"x\", \"y\"), [[5, 6], [7, 8]])},\n        coords={\"x\": [1, 2], \"y\": [20, 30]},\n    )\n    expected = xr.Dataset(\n        {\"a\": ((\"x\", \"y\"), [[1, 2, np.nan], [3, 4, np.nan], [np.nan, 5, 6]])},\n        coords={\"x\": [0, 1, 2], \"y\": [10, 20, 30]},\n    )\n    assert expected.identical(xr.merge([ds1, ds2], compat=\"no_conflicts\"))\n", "def test_merge_coords_no_conflicts(self):\n    ds1 = xr.Dataset(coords={\"x\": [1, 2]})\n    ds2 = xr.Dataset(coords={\"y\": [3, 4]})\n    expected = xr.Dataset(coords={\"x\": [1, 2], \"y\": [3, 4]})\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(xr.merge([ds1, ds2], compat=\"no_conflicts\"))\n", "def test_merge_no_conflicts_coords(self):\n    ds1 = xr.Dataset(coords={\"x\": [1, 2]})\n    ds2 = xr.Dataset(coords={\"x\": [2, 3]})\n    expected = xr.Dataset(coords={\"x\": [1, 2, 3]})\n    assert expected.identical(xr.merge([ds1, ds2], compat=\"no_conflicts\"))\n", "def test_merge_update_no_conflicts_multi_var(self):\n    data = create_test_data()\n    data1 = data.copy(deep=True)\n    data2 = data.copy(deep=True)\n\n    data1[\"var1\"][:, :5] = np.nan\n    data2[\"var1\"][:, 5:] = np.nan\n    data1[\"var2\"][:4, :] = np.nan\n    data2[\"var2\"][4:, :] = np.nan\n    del data2[\"var3\"]\n\n    actual = data1.merge(data2, compat=\"no_conflicts\")\n    assert data.equals(actual)\n\n    # make sure it works with the update method too\n    data1.update(data2)\n    assert data.equals(data1)\n", "def test_merge_no_conflicts_dtype_promotion(self):\n    ds1 = xr.Dataset({\"x\": (\"y\", np.array([1, 2], dtype=\"int32\"))})\n    ds2 = xr.Dataset({\"x\": (\"y\", np.array([2, 3], dtype=\"float32\"))})\n\n    expected = xr.Dataset(\n        {\"x\": (\"y\", np.array([1, 2, 3], dtype=\"float64\"))}, coords={\"y\": [0, 1, 2]}\n    )\n\n    assert expected.identical(xr.merge([ds1, ds2], compat=\"no_conflicts\"))\n"], "sample_588": ["    def test_empty_input_with_concat_dim(self):\n        assert_identical(Dataset(), combine_nested([], concat_dim=\"x\"))\n", "    def test_combine_nd_with_none(self):\n        ds1 = Dataset({\"x\": [0, 1]})\n        ds2 = Dataset({\"y\": [2, 3]})\n        combined_ids = {(0,): ds1, (1,): ds2}\n        result = _combine_nd(combined_ids, concat_dims=[None])\n        expected = merge([ds1, ds2], compat=\"no_conflicts\")\n        assert_identical(result, expected)\n", "    def test_combine_nested_empty_input(self):\n        assert_identical(Dataset(), combine_nested([], concat_dim=\"x\"))\n", "    def test_combine_nested_empty_input_with_concat_dim(self):\n        with raises_regex(ValueError, \"concat_dims has length\"):\n            combine_nested([], concat_dim=\"x\")\n", "def test_combine_nested_with_data_vars_and_coords(self):\n    ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])}, coords={\"x\": [0, 1]})\n    ds2 = Dataset({\"a\": (\"x\", [5, 6]), \"c\": (\"x\", [7, 8])}, coords={\"x\": [2, 3]})\n\n    expected = Dataset(\n        {\"a\": (\"x\", [1, 2, 5, 6]), \"b\": (\"x\", [3, 4, np.nan, np.nan]), \"c\": (\"x\", [np.nan, np.nan, 7, 8])},\n        coords={\"x\": [0, 1, 2, 3]},\n    )\n\n    actual = combine_nested([ds1, ds2], concat_dim=\"x\", data_vars=\"all\", coords=\"different\")\n    assert_identical(expected, actual)\n"], "sample_589": ["def test_interpolate_na_limit_and_max_gap():\n    da = xr.DataArray(\n        [np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10],\n        dims=[\"t\"],\n        coords={\"t\": pd.date_range(\"2001-01-01\", freq=\"H\", periods=11)},\n    )\n    expected = da.copy(data=[np.nan, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    actual = da.interpolate_na(\"t\", limit=3, max_gap=\"9H\")\n    assert_equal(actual, expected)\n\n    expected = da.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da.interpolate_na(\"t\", limit=2, max_gap=\"9H\")\n    assert_equal(actual, expected)\n", "def test_interpolate_na_limit():\n    da = xr.DataArray(\n        [np.nan, 1, np.nan, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10],\n        dims=[\"t\"],\n    )\n    expected = xr.DataArray(\n        [np.nan, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dims=[\"t\"]\n    )\n    actual = da.interpolate_na(dim=\"t\", limit=5)\n    assert_equal(actual, expected)\n\n    expected = xr.DataArray(\n        [np.nan, 1, 2, 3, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10], dims=[\"t\"]\n    )\n    actual = da.interpolate_na(dim=\"t\", limit=2)\n    assert_equal(actual, expected)\n", "def test_interpolate_nd():\n    # Create a 2D array with some missing values\n    da = xr.DataArray(\n        np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]]),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": [1, 2, 3], \"y\": [4, 5, 6]},\n    )\n\n    # Interpolate using linear method\n    actual = da.interpolate_na(dim=[\"x\", \"y\"], method=\"linear\")\n\n    # Check that there are no missing values left\n    assert not actual.isnull().any()\n", "def test_interpolate_na_use_coordinate_errors(da_time):\n    da_time[\"t\"] = pd.date_range(\"2001-01-01\", freq=\"H\", periods=11)\n    with raises_regex(ValueError, \"Coordinates used for interpolation must be 1D\"):\n        da_time.interpolate_na(\"t\", use_coordinate=(\"t\", \"x\"))\n\n    with raises_regex(ValueError, \"Index 't' has duplicate values\"):\n        da_time[\"t\"] = [1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        da_time.interpolate_na(\"t\", use_coordinate=True)\n", "def test_bfill_limit():\n    da = xr.DataArray(\n        [0, np.nan, np.nan, np.nan, np.nan, 3, 4, 5, np.nan, 6, 7], dims=\"time\"\n    )\n    result = da.bfill(\"time\")\n    expected = xr.DataArray([0, 3, 3, 3, 3, 3, 4, 5, 6, 6, 7], dims=\"time\")\n    assert_array_equal(result, expected)\n\n    result = da.bfill(\"time\", limit=1)\n    expected = xr.DataArray(\n        [0, np.nan, np.nan, np.nan, 3, 3, 4, 5, 6, 6, 7], dims=\"time\"\n    )\n    assert_array_equal(result, expected)\n"], "sample_590": ["def test_concat_positions(self):\n    # Test that the `positions` argument works as expected.\n    # Create two DataArrays with different coordinates.\n    da1 = DataArray([1, 2], coords=[(\"x\", [0, 1])])\n    da2 = DataArray([3, 4], coords=[(\"x\", [2, 3])])\n\n    # Concatenate them along a new dimension, using the `positions` argument.\n    actual = concat((da1, da2), dim=\"y\", positions=[0, 1])\n\n    # Expected result: a DataArray with shape (2, 2) and the correct coordinates.\n    expected_coords = {\"x\": [0, 1, 2, 3], \"y\": [0, 1]}\n    expected_data = np.array([[1, 2, np.nan, np.nan], [np.nan, np.nan, 3, 4]])\n    expected = DataArray(expected_data, dims=[\"y\", \"x\"], coords=expected_coords)\n\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    # Test that the positions argument is handled correctly\n    ds1 = Dataset({\"x\": [1, 2]}, coords={\"y\": [0, 1]})\n    ds2 = Dataset({\"x\": [3, 4]}, coords={\"y\": [2, 3]})\n\n    expected = Dataset(\n        {\"x\": (\"y\", [1, 2, np.nan, np.nan, 3, 4])},\n        coords={\"y\": [0, 1, 2, 3, 4, 5]},\n    )\n\n    actual = concat([ds1, ds2], dim=\"y\", positions=[0, 4])\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    data = create_test_data()\n    split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n    positions = [[0, 1, 2], [3, 4, 5]]\n    actual = concat(split_data, dim=\"dim1\", positions=positions)\n    expected = data.assign_coords(dim1=[0, 1, 2, 3, 4, 5])\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    # Test that the `positions` argument works correctly\n    ds1 = Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n    ds2 = Dataset({\"foo\": (\"x\", [3, 4])}, coords={\"x\": [2, 3]})\n    ds3 = Dataset({\"foo\": (\"x\", [5, 6])}, coords={\"x\": [4, 5]})\n\n    actual = concat([ds1, ds2, ds3], dim=\"x\", positions=[0, 2, 4])\n    expected = Dataset(\n        {\"foo\": (\"x\", [1, 2, np.nan, 3, 4, np.nan, 5, 6])},\n        coords={\"x\": [0, 1, 2, 3, 4, 5, 6, 7]},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    data = create_test_data()\n    split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n    positions = [0, 2]\n    expected = Dataset()\n    for var in data.data_vars:\n        if 'dim1' in data[var].dims:\n            expected[var] = Variable(('dim1',) + data[var].dims[1:], \n                                     np.concatenate([split_data[0][var].values, \n                                                     np.full((positions[1] - positions[0] - split_data[0][var].shape[0],) + split_data[0][var].shape[1:], np.nan), \n                                                     split_data[1][var].values]))\n        else:\n            expected[var] = data[var]\n    actual = concat(split_data, dim='dim1', positions=positions)\n    assert_identical(actual, expected)\n"], "sample_592": ["def test_short_data_repr(self):\n    array = xr.DataArray(np.random.randn(100, 5, 1), dims=(\"x\", \"y\", \"z\"))\n    expected_max_lines = 40\n    actual = formatting.short_data_repr(array)\n    num_lines = actual.count(\"\\n\") + 1\n    assert num_lines <= expected_max_lines\n\n    # test for arrays that don't fit in memory\n    class DummyArray:\n            self.size = 10**6\n            self.dtype = np.float64\n\n            return \"DummyArray\"\n\n    array = xr.DataArray(DummyArray(), dims=(\"x\",))\n    actual = formatting.short_data_repr(array)\n    expected = \"[1000000 values with dtype=float64]\"\n    assert actual == expected\n\n    # test for arrays with __array_function__ but no shape or dtype\n    class DummyArray2:\n            return NotImplemented\n\n    array = xr.DataArray(DummyArray2(), dims=(\"x\",))\n    actual = formatting.short_data_repr(array)\n    expected = repr(array.data)\n    assert actual == expected\n", "def test_limit_lines():\n    string = \"\\n\".join([str(i) for i in range(100)])\n    expected = \"\\n\".join([\"0\", \"1\", \"2\", \"...\", \"97\", \"98\", \"99\"])\n    actual = formatting.limit_lines(string, limit=7)\n    assert actual == expected\n\n    string = \"\\n\".join([str(i) for i in range(5)])\n    expected = string\n    actual = formatting.limit_lines(string, limit=10)\n    assert actual == expected\n", "def test_inline_dask_repr():\n    import dask.array as da\n\n    array = da.from_array(np.arange(500), chunks=100)\n    expected = \"dask.array<chunksize=(100,), meta=np.ndarray>\"\n    assert formatting.inline_dask_repr(array) == expected\n\n    array = da.from_array(np.arange(500), chunks=(100, 10))\n    expected = \"dask.array<chunksize=(100, 10), meta=np.ndarray>\"\n    assert formatting.inline_dask_repr(array) == expected\n", "def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n\n    # Test with 'date' format\n    actual = formatting.format_timedelta(pd.Timedelta(\"10 days 1 hour\"), \"date\")\n    expected = \"10 days\"\n    assert expected == actual\n\n    # Test with 'time' format\n    actual = formatting.format_timedelta(pd.Timedelta(\"10 days 1 hour\"), \"time\")\n    expected = \"01:00:00\"\n    assert expected == actual\n", "def test_inline_variable_array_repr():\n    # Test with numpy array\n    var = xr.Variable(\"x\", np.array([1, 2, 3]))\n    expected = \"[1 2 3]\"\n    actual = formatting.inline_variable_array_repr(var, 80)\n    assert expected == actual\n\n    # Test with dask array\n    import dask.array as da\n    var = xr.Variable(\"x\", da.from_array(np.array([1, 2, 3])))\n    expected = \"dask.array<chunksize=(3,), meta='ndarray'>\"\n    actual = formatting.inline_variable_array_repr(var, 80)\n    assert expected == actual\n\n    # Test with sparse array\n    import sparse\n    var = xr.Variable(\"x\", sparse.COO(np.array([0, 1, 2]), np.array([1, 2, 3])))\n    expected = \"<COO: shape=(3,), dtype='int64', nnz=3, fill_value=0>\"\n    actual = formatting.inline_variable_array_repr(var, 80)\n    assert expected == actual\n\n    # Test with xarray internal array type\n    var = xr.Variable(\"x\", xr.core.variable.IndexVariable(\"x\", [1, 2, 3]))\n    expected = \"[1 2 3]\"\n    actual = formatting.inline_variable_array_repr(var, 80)\n    assert expected == actual\n"], "sample_593": ["def test_collapsible_section_enabled():\n    section = fh.collapsible_section(\n        \"Test Section\", details=\"Test details\", n_items=5, enabled=True\n    )\n    assert \"disabled\" not in section\n    assert \"checked\" not in section\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Detailed information\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n\n    assert f\"<label for='section-\" in section\n    assert f\"class='xr-section-summary' title='Expand/collapse section'>{name}:\" in section\n    assert f\"<span>({n_items})</span>\" in section\n    assert f\"<div class='xr-section-inline-details'>{inline_details}</div>\" in section\n    assert f\"<div class='xr-section-details'>{details}</div>\" in section\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Detailed text\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n\n    assert f\"<input id='section-\" in section\n    assert f\"class='xr-section-summary-in' type='checkbox' {'checked' if not collapsed else ''}\" in section\n    assert f\"<label for='section-\" in section\n    assert f\"class='xr-section-summary' title='Expand/collapse section'>{name}:\" in section\n    assert f\"<span>({n_items})</span>\" in section\n    assert f\"<div class='xr-section-inline-details'>{inline_details}</div>\" in section\n    assert f\"<div class='xr-section-details'>{details}</div>\" in section\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Detailed information\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details, details, n_items, enabled, collapsed\n    )\n\n    assert f\"<input id='section-\" in section\n    assert f\"class='xr-section-summary-in' type='checkbox' {'checked' if not collapsed else ''}\" in section\n    assert f\"<label for='section-\" in section\n    assert f\"class='xr-section-summary' title='Expand/collapse section'>{name}:\" in section\n    assert f\"<span>({n_items})</span>\" in section\n    assert f\"<div class='xr-section-inline-details'>{inline_details}</div>\" in section\n    assert f\"<div class='xr-section-details'>{details}</div>\" in section\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Expanded details\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details, details, n_items, enabled, collapsed\n    )\n\n    assert name in section\n    assert inline_details in section\n    assert details in section\n    assert str(n_items) in section\n    assert \"disabled\" not in section\n    assert \"checked\" not in section\n"], "sample_594": ["def test_format_timestamp_edge_cases(self):\n    # Test format_timestamp with edge cases\n    assert formatting.format_timestamp(None) == \"NaT\"\n    assert formatting.format_timestamp(pd.NaT) == \"NaT\"\n    assert formatting.format_timestamp(np.nan) == \"nan\"\n    assert formatting.format_timestamp(pd.Timestamp(\"1970-01-01T00:00:00\")) == \"1970-01-01T00:00:00\"\n", "def test_limit_lines():\n    lines = [\"line1\", \"line2\", \"line3\", \"line4\", \"line5\"]\n    expected = \"\\n\".join([\"line1\", \"line2\", \"...\", \"line4\", \"line5\"])\n    actual = formatting.limit_lines(\"\\n\".join(lines), limit=5)\n    assert actual == expected\n\n    lines = [\"line1\", \"line2\", \"line3\"]\n    expected = \"\\n\".join(lines)\n    actual = formatting.limit_lines(\"\\n\".join(lines), limit=5)\n    assert actual == expected\n\n    lines = []\n    expected = \"\"\n    actual = formatting.limit_lines(\"\\n\".join(lines), limit=5)\n    assert actual == expected\n", "def test_inline_variable_array_repr_sparse_array(self):\n    import sparse\n\n    value = sparse.COO(np.array([[0, 1], [0, 2]]))\n    variable = xr.Variable((\"x\", \"y\"), value)\n\n    actual = formatting.inline_variable_array_repr(variable, max_width=100)\n    expected = \"<COO: nnz=2, fill_value=nan>\"\n    assert actual == expected\n", "def test_short_data_repr(self):\n    array = xr.DataArray(np.arange(100))\n    expected = \"[0 1 2 ... 97 98 99]\"\n    actual = formatting.short_data_repr(array)\n    assert expected == actual\n\n    array = xr.DataArray(np.arange(5))\n    expected = \"[0 1 2 3 4]\"\n    actual = formatting.short_data_repr(array)\n    assert expected == actual\n\n    array = xr.DataArray(np.arange(100).reshape(10, 10))\n    expected = \"[[ 0  1  2 ...  7  8  9]\\n [10 11 12 ... 17 18 19]\\n [20 21 22 ... 27 28 29]\\n ...\\n [70 71 72 ... 77 78 79]\\n [80 81 82 ... 87 88 89]\\n [90 91 92 ... 97 98 99]]\"\n    actual = formatting.short_data_repr(array)\n    assert expected == actual\n\n    array = xr.DataArray(np.arange(100, dtype='float64'))\n    expected = \"[ 0.  1.  2. ... 97. 98. 99.]\"\n    actual = formatting.short_data_repr(array)\n    assert expected == actual\n", "def test_short_data_repr(self):\n    # Test for DataArray with numpy array data\n    da = xr.DataArray(np.array([1, 2, 3]))\n    expected = \"array([1, 2, 3], dtype='int64)\"\n    actual = formatting.short_data_repr(da)\n    assert expected == actual\n\n    # Test for DataArray with dask array data\n    da = xr.DataArray(dask.array.from_array(np.array([1, 2, 3])))\n    expected = \"dask.array<array, shape=(3,), dtype='int64, chunksize=(3,), meta=np.ndarray>\"\n    actual = formatting.short_data_repr(da)\n    assert expected == actual\n\n    # Test for DataArray with non-numerical data\n    da = xr.DataArray([\"a\", \"b\", \"c\"])\n    expected = \"['a', 'b', 'c]\"\n    actual = formatting.short_data_repr(da)\n    assert expected == actual\n"], "sample_595": ["def test_split():\n    # TODO: this is only a simple sanity check\n    values = xr.DataArray([\"hello world\", \"a b c\", \"foo\"])\n    result = values.str.split()\n    expected = xr.DataArray([[\"hello\", \"world\"], [\"a\", \"b\", \"c\"], [\"foo\"]])\n    assert_equal(result, expected)\n", "def test_split(dtype):\n    values = xr.DataArray([\"a_b_c\", \"c_d_e\", \"f_g_h\"]).astype(dtype)\n\n    result = values.str.split(\"_\")\n    expected = xr.DataArray([[\"a\", \"b\", \"c\"], [\"c\", \"d\", \"e\"], [\"f\", \"g\", \"h\"]])\n    assert_equal(result, expected)\n", "def test_encode_decode_errors():\n    data = xr.DataArray([\"a\", \"b\", \"a\\xe4\"])\n\n    # Test encoding with errors\n    encoded_replace = data.str.encode(\"ascii\", errors=\"replace\")\n    encoded_ignore = data.str.encode(\"ascii\", errors=\"ignore\")\n    assert_equal(encoded_replace, xr.DataArray([b\"a\", b\"b\", b\"a?\"]))\n    assert_equal(encoded_ignore, xr.DataArray([b\"a\", b\"b\", b\"a\"]))\n\n    # Test decoding with errors\n    decoded_replace = encoded_replace.str.decode(\"ascii\", errors=\"replace\")\n    decoded_ignore = encoded_ignore.str.decode(\"ascii\", errors=\"ignore\")\n    assert_equal(decoded_replace, xr.DataArray([\"a\", \"b\", \"a?\"]))\n    assert_equal(decoded_ignore, xr.DataArray([\"a\", \"b\", \"a\"]))\n", "def test_getitem_slice(dtype):\n    values = xr.DataArray([\"abcdef\", \"ab\", \"a\", \"\"]).astype(dtype)\n\n    result = values.str[1:3]\n    expected = xr.DataArray([\"bc\", \"\", \"\", \"\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str[:3]\n    expected = xr.DataArray([\"abc\", \"ab\", \"a\", \"\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str[1:]\n    expected = xr.DataArray([\"bcdef\", \"b\", \"\", \"\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str[:]\n    expected = values\n    assert_equal(result, expected)\n\n    result = values.str[::-1]\n    expected = xr.DataArray([\"fedcba\", \"ba\", \"a\", \"\"]).astype(dtype)\n    assert_equal(result, expected)\n", "def test_decode_encode_non_utf8_encoding(dtype):\n    data = xr.DataArray([\"a\", \"b\", \"a\\xe4\"]).astype(dtype)\n    encoded = data.str.encode(\"latin1\")\n    decoded = encoded.str.decode(\"latin1\")\n    assert_equal(data, decoded)\n"], "sample_596": ["def test_concat_attrs_merge():\n    ds1 = Dataset({}, attrs={\"a\": 1, \"b\": 2})\n    ds2 = Dataset({}, attrs={\"b\": 2, \"c\": 3})\n\n    actual = concat([ds1, ds2], dim=\"x\", combine_attrs=\"no_conflicts\")\n    expected = Dataset({}, attrs={\"a\": 1, \"b\": 2, \"c\": 3})\n    assert_identical(actual, expected)\n\n    with raises_regex(ValueError, \"combine_attrs='no_conflicts'\"):\n        ds3 = Dataset({}, attrs={\"b\": 3, \"c\": 3})\n        concat([ds1, ds3], dim=\"x\", combine_attrs=\"no_conflicts\")\n", "def test_concat_no_conflicts_with_nan():\n    # GH 4899\n    ds1 = Dataset(\n        {\"a\": ((\"x\", \"y\"), [[1, 2], [3, 4]])}, coords={\"x\": [0, 1], \"y\": [0, 1]}\n    )\n    ds2 = Dataset(\n        {\"a\": ((\"x\", \"y\"), [[np.nan, np.nan], [np.nan, np.nan]])},\n        coords={\"x\": [0, 1], \"y\": [0, 1]},\n    )\n\n    actual = concat([ds1, ds2], dim=\"new_dim\", compat=\"no_conflicts\")\n    expected = Dataset(\n        {\n            \"a\": (\n                (\"new_dim\", \"x\", \"y\"),\n                [[[1, 2], [3, 4]], [[np.nan, np.nan], [np.nan, np.nan]]],\n            )\n        },\n        coords={\"x\": [0, 1], \"y\": [0, 1], \"new_dim\": [0, 1]},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_dataarray_with_conflicting_attrs():\n    da1 = DataArray([1, 2], dims=\"x\", attrs={\"units\": \"m\"})\n    da2 = DataArray([3, 4], dims=\"x\", attrs={\"units\": \"cm\"})\n\n    with raises_regex(ValueError, \"conflicting values\"):\n        concat([da1, da2], dim=\"y\", compat=\"identical\")\n\n    actual = concat([da1, da2], dim=\"y\", compat=\"override\")\n    expected = DataArray([[1, 2], [3, 4]], dims=[\"y\", \"x\"], attrs={\"units\": \"m\"})\n    assert_identical(actual, expected)\n", "def test_concat_dim_not_in_coords():\n    da1 = DataArray([1, 2], dims=\"x\", coords={\"y\": 1})\n    da2 = DataArray([3, 4], dims=\"x\", coords={\"y\": 1})\n\n    expected = DataArray([[1, 2], [3, 4]], dims=[\"z\", \"x\"], coords={\"y\": 1, \"z\": [0, 1]})\n\n    actual = concat([da1, da2], dim=\"z\")\n    assert_identical(actual, expected)\n", "def test_concat_merge_multiple_non_dim_coords():\n    da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1, \"z\": 2})\n    da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6], \"y\": 1})\n\n    expected_minimal = DataArray(\n        range(1, 7), dims=\"x\", coords={\"x\": range(1, 7), \"y\": 1}\n    )\n    expected_all = DataArray(\n        range(1, 7),\n        dims=\"x\",\n        coords={\"x\": range(1, 7), \"y\": 1, \"z\": [2, np.nan, np.nan, np.nan, np.nan, np.nan]},\n    )\n\n    actual_minimal = concat([da1, da2], \"x\", coords=\"minimal\")\n    actual_all = concat([da1, da2], \"x\", coords=\"all\")\n\n    assert_identical(actual_minimal, expected_minimal)\n    assert_identical(actual_all, expected_all)\n"], "sample_597": ["def test_merge_no_conflicts_fill_value(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, 3])}, coords={\"x\": [0, 1, 2]}, attrs={\"filled\": True}\n    )\n    actual = ds1.merge(ds2, compat=\"no_conflicts\", fill_value=dtypes.NA)\n    assert expected.identical(actual)\n\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, -999])},\n        coords={\"x\": [0, 1, 2]},\n        attrs={\"filled\": True},\n    )\n    actual = ds1.merge(ds2, compat=\"no_conflicts\", fill_value=-999)\n    assert expected.identical(actual)\n\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, np.nan])},\n        coords={\"x\": [0, 1, 2]},\n        attrs={\"filled\": True},\n    )\n    actual = ds1.merge(ds2, compat=\"no_conflicts\", fill_value=np.nan)\n    assert expected.identical(actual)\n", "def test_merge_no_conflicts_with_nan(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, np.nan]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [np.nan, 3]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, np.nan, 3]), \"x\": [0, 1, 2]})\n\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\"))\n\n    assert ds1.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"left\"))\n\n    assert ds2.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"right\"))\n\n    expected2 = xr.Dataset({\"a\": (\"x\", [np.nan]), \"x\": [1]})\n    assert expected2.identical(ds1.merge(ds2, compat=\"no_conflicts\", join=\"inner\"))\n", "def test_merge_no_conflicts_multi_vars(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [10, 20]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"c\": (\"x\", [100, 200]), \"x\": [1, 2]})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", [10, 20, np.nan]), \"c\": (\"x\", [np.nan, 100, 200])},\n        {\"x\": [0, 1, 2]},\n    )\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\"))\n", "def test_merge_update_no_conflicts(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"a\": (\"x\", [2, np.nan]), \"x\": [1, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, np.nan]), \"x\": [0, 1, 2]})\n\n    actual = ds1.merge(ds2, compat=\"no_conflicts\")\n    assert expected.identical(actual)\n\n    actual = ds1.copy()\n    actual.update(ds2)\n    assert expected.identical(actual)\n", "def test_merge_attrs_override_with_nans(self):\n    var1_attrs = {\"a\": 1, \"b\": np.nan}\n    var2_attrs = {\"a\": 1, \"b\": 2}\n    expected_attrs = {\"a\": 1, \"b\": np.nan}\n\n    data = create_test_data()\n    data.var1.attrs = var1_attrs\n    data.var2.attrs = var2_attrs\n    actual = xr.merge([data.var1, data.var2], combine_attrs=\"override\")\n    expected = data[[\"var1\", \"var2\"]]\n    expected.attrs = expected_attrs\n    assert actual.identical(expected)\n"], "sample_598": ["def test_short_data_repr_large_array():\n    array = xr.DataArray(np.random.randn(1000, 5, 1))\n    repr_str = formatting.short_data_repr(array)\n    assert len(repr_str.splitlines()) < 50\n", "def test_summarize_coord_levels():\n    coord = xr.DataArray(\n        [1, 2, 3], dims=\"x\", coords={\"x\": (\"x\", [\"a\", \"b\", \"c\"], {\"level_names\": [\"x\"]})}\n    )\n    actual = formatting._summarize_coord_levels(coord, col_width=10)\n    expected = dedent(\n        \"\"\"\\\n        * x        (x) object 'a' 'b' 'c'\n        \"\"\"\n    )\n    assert actual == expected\n", "def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n\n    # Test with different formats\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"-3 days\"), \"date\", \"-3 days\"),\n        (pd.Timedelta(\"3 hours\"), \"time\", \"03:00:00\"),\n        (pd.Timedelta(\"NaT\"), \"time\", \"NaT\"),\n    ]\n    for item, timedelta_format, expected in cases:\n        actual = formatting.format_timedelta(item, timedelta_format=timedelta_format)\n        assert expected == actual\n", "def test_short_data_repr(self):\n    array = xr.DataArray(np.arange(100))\n    actual = formatting.short_data_repr(array)\n    expected = \"[0 1 2 ... 97 98 99]\"\n    assert actual == expected\n\n    array = xr.DataArray(np.random.randn(3, 4))\n    actual = formatting.short_data_repr(array)\n    expected = \"[[...]]\"\n    assert actual.startswith(expected)\n\n    array = xr.DataArray(np.random.randn(10, 10))\n    actual = formatting.short_data_repr(array)\n    expected = \"[[...]]\"\n    assert actual.startswith(expected)\n\n    array = xr.DataArray(np.random.randn(5))\n    actual = formatting.short_data_repr(array)\n    expected = \"[...]\"\n    assert actual.startswith(expected)\n", "def test_limit_lines():\n    lines = [\"Line 1\", \"Line 2\", \"Line 3\", \"Line 4\", \"Line 5\"]\n    expected = \"\\n\".join(lines[:2] + [\"...\"] + lines[-2:])\n    actual = formatting.limit_lines(\"\\n\".join(lines), limit=4)\n    assert actual == expected\n\n    lines = [\"Line 1\", \"Line 2\", \"Line 3\"]\n    expected = \"\\n\".join(lines)\n    actual = formatting.limit_lines(\"\\n\".join(lines), limit=4)\n    assert actual == expected\n"], "sample_599": ["def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), np.array([0, 255], dtype=\"uint8))\n    expected = xr.Variable((\"x\",), np.array([0, -1], dtype=\"int8\"))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), [0, 255], dtype=\"uint8)\n    expected = xr.Variable((\"x\",), [0, -1], dtype=\"int8\")\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), [0, 255], {\"_Unsigned\": \"true\"}, dtype=np.int8)\n    expected = xr.Variable((\"x\",), [0, -1], dtype=np.int8)\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_UnsignedIntegerCoder_decode():\n    original = xr.Variable((\"x\",), [-1, 0, 1], {\"_Unsigned\": \"true\"})\n    expected = xr.Variable((\"x\",), [255, 0, 1], dtype=np.uint8)\n    coder = variables.UnsignedIntegerCoder()\n    decoded = coder.decode(original)\n    assert_identical(expected, decoded)\n", "def test_UnsignedIntegerCoder_roundtrip():\n    original = xr.Variable((\"x\",), np.array([0, 1, 2], dtype=\"uint8\"))\n    original.encoding[\"_Unsigned\"] = \"true\"\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n"], "sample_600": ["def test_unsigned_coder_roundtrip(dtype):\n    original = xr.Variable((\"x\",), np.arange(10, dtype=dtype))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n    assert original.dtype == roundtripped.dtype\n", "def test_unsigned_integer_coder_roundtrip():\n    original = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=\"u2\"))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype.kind == \"i\"\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n", "def test_UnsignedIntegerCoder_encode():\n    original = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=np.uint8))\n    original.attrs[\"_Unsigned\"] = \"true\"\n    expected = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=np.int8))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert_identical(expected, encoded)\n", "def test_decode_signed_from_unsigned(dtype):\n    unsigned_dtype = np.dtype(dtype.replace('i', 'u'))\n    original_values = np.array([np.iinfo(np.dtype(dtype)).max], dtype=np.dtype(dtype))\n    encoded = xr.Variable(\n        (\"x\",), original_values.astype(unsigned_dtype), attrs={\"_Unsigned\": \"false\"}\n    )\n    coder = variables.UnsignedIntegerCoder()\n    decoded = coder.decode(encoded)\n    assert decoded.dtype == np.dtype(dtype)\n    assert decoded.values == original_values\n", "def test_UnsignedIntegerCoder_roundtrip():\n    original = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=\"u2\"))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n"], "sample_601": ["def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type):\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n    result = cftime_rounding_dataarray.dt.round(freq)\n    assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            [cftime_date_type(1, 1, 2, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n"], "sample_602": ["def test_open_dataset():\n    # create a netCDF file\n    with xr.Dataset() as ds:\n        ds.to_netcdf(\"example.nc\", engine=\"netcdf4\")\n\n    # check that the default engine is used\n    with xr.open_dataset(\"example.nc\") as ds:\n        assert isinstance(ds, xr.Dataset)\n\n    # check that the netcdf4 engine is used\n    with xr.open_dataset(\"example.nc\", engine=\"netcdf4\") as ds:\n        assert isinstance(ds, xr.Dataset)\n\n    # check that invalid engines raise an error\n    try:\n        xr.open_dataset(\"example.nc\", engine=\"invalid\")\n        assert False\n    except ValueError as e:\n        assert \"unrecognized engine\" in str(e)\n", "def test_open_dataset_with_pathlib_path():\n    import pathlib\n\n    expected = xr.Dataset()\n    filename = pathlib.Path(\"example.nc\")\n    with xr.backends.NetCDF4DataStore.open(filename, mode=\"w\") as store:\n        expected.to_netcdf(store)\n\n    actual = xr.open_dataset(filename)\n    assert_identical(expected, actual)\n", "def test_open_dataset_bad_engine():\n    with pytest.raises(ValueError):\n        xr.open_dataset(\"example.nc\", engine=\"invalid_engine\")\n", "def test_open_dataset():\n    # Create a simple dataset\n    ds = xr.Dataset(\n        dict(a=(\"x\", np.arange(5)), b=(\"y\", np.arange(3))),\n        coords=dict(x=(\"x\", np.arange(5)), y=(\"y\", np.arange(3))),\n    )\n\n    # Write the dataset to a netCDF file\n    with open(\"test.nc\", \"wb\") as f:\n        ds.to_netcdf(f)\n\n    # Open the dataset with open_dataset\n    with xr.open_dataset(\"test.nc\") as actual:\n        assert_identical(ds, actual)\n", "def test_open_dataset_invalid_engine():\n    with pytest.raises(ValueError) as excinfo:\n        xr.open_dataset(\"example.nc\", engine=\"invalid\")\n    assert \"unrecognized engine for open_dataset\" in str(excinfo.value)\n"], "sample_603": ["def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Detailed information\"\n    n_items = 5\n\n    section = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, n_items=n_items\n    )\n\n    assert name in section\n    assert inline_details in section\n    assert details in section\n    assert str(n_items) in section\n    assert \"disabled\" not in section\n    assert \"checked\" not in section\n\n    section = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, n_items=n_items, enabled=False\n    )\n    assert \"disabled\" in section\n\n    section = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, n_items=n_items, collapsed=True\n    )\n    assert \"checked\" in section\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    details = \"<p>This is a test section.</p>\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    formatted = fh.collapsible_section(\n        name, details=details, n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n\n    assert f\"<label for='section-\" in formatted\n    assert f\"class='xr-section-summary' title='Expand/collapse section'>{name}:</label>\" in formatted\n    assert f\"<div class='xr-section-details'>{details}</div>\" in formatted\n    assert f\"checked\" not in formatted\n", "def test_collapsible_section():\n    name = \"Test Section\"\n    inline_details = \"Inline details\"\n    details = \"Detailed text\"\n    n_items = 5\n    enabled = True\n    collapsed = False\n\n    section = fh.collapsible_section(\n        name, inline_details=inline_details, details=details, n_items=n_items, enabled=enabled, collapsed=collapsed\n    )\n\n    assert f\"<input id='section-\" in section\n    assert f\"class='xr-section-summary-in' type='checkbox' {'checked' if not collapsed else ''}>\" in section\n    assert f\"<label for='section-\" in section\n    assert f\"class='xr-section-summary' title='Expand/collapse section'>{name}: <span>({n_items})</span></label>\" in section\n    assert f\"<div class='xr-section-inline-details'>{inline_details}</div>\" in section\n    assert f\"<div class='xr-section-details'>{details}</div>\" in section\n", "def test_collapsible_section_enabled():\n    section = fh.collapsible_section(\n        name=\"Test Section\", details=\"Test details\", enabled=True\n    )\n    assert \"disabled\" not in section\n\n    section = fh.collapsible_section(\n        name=\"Test Section\", details=\"Test details\", enabled=False\n    )\n    assert \"disabled\" in section\n", "def test_summarize_coord_with_unsafe_name_and_value():\n    var = xr.Variable(\"x\", np.array([1, 2]), attrs={\"units\": \"<m>\"})\n    formatted = fh.summarize_coord(\"<x>\", var)\n    assert \"&lt;x&gt;\" in formatted\n    assert \"&lt;m&gt;\" in formatted\n"], "sample_604": ["def test_short_data_repr():\n    da = xr.DataArray(np.array([1, 2, 3]))\n    expected = \"[1 2 3]\"\n    actual = formatting.short_data_repr(da)\n    assert actual == expected\n\n    da = xr.DataArray(np.random.randn(100))\n    assert len(formatting.short_data_repr(da)) < 80\n\n    # Test with duck array\n    class DuckArray:\n            return \"Duck Array\"\n\n    da = xr.DataArray(DuckArray())\n    expected = \"Duck Array\"\n    actual = formatting.short_data_repr(da)\n    assert actual == expected\n", "def test_short_data_repr_for_dask_array():\n    import dask.array as da\n\n    # Create a large dask array\n    x = da.random.random((1000, 1000), chunks=(100, 100))\n\n    # Create an xarray DataArray from the dask array\n    xr_da = xr.DataArray(x)\n\n    # Test that the short data repr is used for dask arrays\n    repr_str = formatting.short_data_repr(xr_da)\n    assert \"xarray.DataArray\" in repr_str\n    assert \"dask.array\" in repr_str\n    assert \"chunksize\" in repr_str\n", "def test_inline_dask_repr():\n    import dask.array as da\n\n    x = da.from_array(np.random.randn(100, 5, 1), chunks=(10, 5, 1))\n    var = xr.Variable((\"x\", \"y\", \"z\"), x)\n    actual = formatting.inline_dask_repr(var.data)\n    expected = \"dask.array<chunksize=(10, 5, 1), meta=np.ndarray>\"\n    assert actual == expected\n", "def test_short_data_repr_for_non_numpy_array():\n    array = [1, 2, 3, 4, 5]\n    result = formatting.short_data_repr(array)\n    assert result == \"[1, 2, 3, 4, 5]\"\n\n    array = {\"a\": 1, \"b\": 2}\n    result = formatting.short_data_repr(array)\n    assert result == \"{'a': 1, 'b': 2}\"\n\n    array = \"Hello World\"\n    result = formatting.short_data_repr(array)\n    assert result == \"'Hello World'\"\n", "def test_short_data_repr_numpy_array():\n    array = np.random.randn(100, 5, 1)\n    expected = formatting.short_numpy_repr(array)\n    actual = formatting.short_data_repr(array)\n    assert expected == actual\n\n    with xr.set_options(display_expand_data=False):\n        expected = formatting.inline_variable_array_repr(xr.Variable((\"x\", \"y\", \"z\"), array), 80)\n        actual = formatting.short_data_repr(array)\n        assert expected == actual\n"], "sample_605": ["def test_groupby_quantile_dtype():\n    # GH3419\n    da = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    quantiles = [0.5]\n    result = da.groupby(\"x\").quantile(quantiles)\n    assert result.dtype == da.dtype\n    quantiles = [0.5, 0.75]\n    result = da.groupby(\"x\").quantile(quantiles)\n    assert result.dtype == da.dtype\n", "def test_groupby_map_func_args_with_kwargs():\n        return arg1 + arg2 if arg3 is None else arg1 + arg2 + arg3\n\n    array = xr.DataArray([1, 1, 1], [(\"x\", [1, 2, 3])])\n    expected = xr.DataArray([4, 4, 4], [(\"x\", [1, 2, 3])])\n    actual = array.groupby(\"x\").map(func, 1, arg3=2)\n    assert_identical(expected, actual)\n", "def test_groupby_quantile_empty():\n    # Test quantile on an empty DataArray\n    array = xr.DataArray([], dims=\"x\")\n    with pytest.raises(ValueError):\n        array.groupby(\"x\").quantile(0.5)\n", "def test_groupby_bins_dtype():\n    # GH3424\n    da = xr.DataArray([1, 2, 3], dims=[\"x\"], coords={\"x\": [0.1, 0.2, 0.3]})\n    bins = np.array([0.0, 0.15, 0.25], dtype=np.float32)\n    result = da.groupby_bins(\"x\", bins).sum()\n    expected = xr.DataArray(\n        [1, 2],\n        dims=[\"x_bins\"],\n        coords={\"x_bins\": pd.cut(bins, bins, include_lowest=True, right=False).categories},\n    )\n    assert_identical(result, expected)\n", "def test_groupby_bins_range_bins():\n    # Test that groupby_bins works with bins generated by np.arange\n    array = xr.DataArray([1, 2, 3, 4, 5], dims=\"x\")\n    bins = np.arange(0, 6, 2)\n    actual = array.groupby_bins(\"x\", bins).sum()\n    expected = xr.DataArray(\n        [3, 7, 5],\n        dims=[\"x_bins\"],\n        coords={\"x_bins\": pd.cut(bins, bins).categories},\n    )\n    assert_identical(actual, expected)\n"], "sample_607": ["def test_guess_engine():\n    engine = plugins.guess_engine(\"example.nc\")\n    assert engine == \"netcdf4\"\n", "def test_guess_engine_explicit_engine():\n    store_spec = \"foo.nc\"\n    engine = \"dummy\"\n    assert plugins.guess_engine(store_spec) == engine\n\n    with pytest.raises(ValueError, match=r\"unrecognized engine\"):\n        plugins.guess_engine(store_spec, engine=\"unknown\")\n", "def test_guess_engine_runtime_warning():\n    with pytest.warns(RuntimeWarning):\n        plugins.guess_engine(\"foo.nc\")\n", "def test_guess_engine_runtime_warning():\n    with pytest.warns(RuntimeWarning):\n        plugins.guess_engine(\"foo.nc\")\n", "def test_guess_engine_runtime_warning():\n    with pytest.warns(RuntimeWarning):\n        with mock.patch.object(DummyBackendEntrypointArgs, \"guess_can_open\", side_effect=Exception()):\n            plugins.guess_engine(\"not-valid\")\n"], "sample_608": ["def test__element_formatter() -> None:\n    elements = [\"a\", \"b\", \"c\", \"d\"]\n    col_width = 10\n    max_rows = 2\n    delimiter = \", \"\n\n    expected = \"\"\"\\\n    a, b, \n     ... , \n    c, d\"\"\"\n    actual = formatting._element_formatter(\n        elements, col_width, max_rows=max_rows, delimiter=delimiter\n    )\n    assert actual == expected\n\n    # Test with no maximum rows.\n    max_rows = None\n    expected = \"a, b, c, d\"\n    actual = formatting._element_formatter(\n        elements, col_width, max_rows=max_rows, delimiter=delimiter\n    )\n    assert actual == expected\n", "def test_short_data_repr_for_numpy_array() -> None:\n    # Test case for short data repr when the numpy array has more than 200 elements\n    array = np.random.rand(300)\n\n    actual = formatting.short_data_repr(array)\n    expected = \"[{} values with dtype=float64]\".format(len(array))\n\n    assert actual == expected\n\n    # Test case for short data repr when the numpy array has less than or equal to 200 elements\n    array = np.random.rand(100)\n\n    actual = formatting.short_data_repr(array)\n    expected = short_numpy_repr(array)\n\n    assert actual == expected\n", "def test__element_formatter() -> None:\n    elements = [f\"dim_{i}\" for i in range(100)]\n    col_width = 10\n    max_rows = 5\n\n    result = formatting._element_formatter(\n        elements, col_width=col_width, max_rows=max_rows\n    )\n    lines = result.splitlines()\n\n    # Check number of rows\n    assert len(lines) == max_rows\n\n    # Check column width\n    for line in lines:\n        assert len(line) <= col_width + len(\", \")\n\n    # Check that ellipsis is added when too many elements\n    assert \"...\" in result\n\n    # Check no ellipsis when not too many elements\n    elements = [f\"dim_{i}\" for i in range(max_rows)]\n    result = formatting._element_formatter(elements, col_width=col_width)\n    assert \"...\" not in result\n", "def test__element_formatter() -> None:\n    elements = [\"dim1\", \"dim2\", \"dim3\"]\n    col_width = 10\n\n    # Test that _element_formatter formats elements correctly\n    actual = formatting._element_formatter(elements, col_width)\n    expected = \"dim1, dim2, dim3\"\n    assert actual == expected\n\n    # Test that _element_formatter handles large number of elements correctly\n    elements = [f\"dim{i}\" for i in range(100)]\n    actual = formatting._element_formatter(elements, col_width)\n    assert len(actual) <= formatting.OPTIONS[\"display_width\"]\n\n    # Test that _element_formatter handles max_rows argument correctly\n    elements = [f\"dim{i}\" for i in range(100)]\n    max_rows = 5\n    actual = formatting._element_formatter(elements, col_width, max_rows=max_rows)\n    lines = actual.splitlines()\n    assert len(lines) <= max_rows\n", "def test__element_formatter() -> None:\n    elements = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n    col_width = 10\n    delimiter = \", \"\n\n    # Test with no max_rows\n    result = formatting._element_formatter(elements, col_width, delimiter=delimiter)\n    expected = \"a, b, c, d, e\"\n    assert result == expected\n\n    # Test with max_rows\n    max_rows = 3\n    result = formatting._element_formatter(\n        elements, col_width, max_rows=max_rows, delimiter=delimiter\n    )\n    expected = \"a, b, ...\\n\" + \" \" * col_width + \"d, e\"\n    assert result == expected\n\n    # Test with max_rows equal to 1\n    max_rows = 1\n    result = formatting._element_formatter(\n        elements, col_width, max_rows=max_rows, delimiter=delimiter\n    )\n    expected = \"a, ...\"\n    assert result == expected\n\n    # Test with max_rows greater than the number of elements\n    max_rows = 10\n    result = formatting._element_formatter(\n        elements, col_width, max_rows=max_rows, delimiter=delimiter\n    )\n    expected = \"a, b, c, d, e\"\n    assert result == expected\n"], "sample_609": ["def test_where_broadcasting() -> None:\n    cond = xr.DataArray([True, False], dims=\"x\")\n    x = xr.DataArray([1, 2], dims=\"y\")\n    y = xr.DataArray([3, 4], dims=\"y\")\n    actual = xr.where(cond, x, y)\n    expected = xr.DataArray([[1, 2], [3, 4]], dims=(\"x\", \"y\"))\n    assert_identical(expected, actual)\n\n    cond = xr.DataArray([True, False], dims=\"x\")\n    x = xr.DataArray([1, 2], dims=\"x\")\n    y = xr.DataArray(0)\n    actual = xr.where(cond, x, y)\n    expected = xr.DataArray([1, 0], dims=\"x\")\n    assert_identical(expected, actual)\n", "def test_where_dask() -> None:\n    import dask.array as da\n\n    cond = xr.DataArray(da.from_array([True, False], chunks=1), dims=\"x\")\n    x = xr.DataArray(da.from_array([1, 2], chunks=1), dims=\"x\")\n    y = xr.DataArray(da.from_array([3, 4], chunks=1), dims=\"x\")\n\n    actual = xr.where(cond, x, y)\n    expected = xr.DataArray(da.from_array([1, 4], chunks=1), dims=\"x\")\n    assert_identical(expected, actual)\n", "def test_where_broadcasting() -> None:\n    cond = xr.DataArray([True, False], dims=\"x\")\n    x = xr.DataArray([[1, 2], [3, 4]], dims=(\"x\", \"y\"))\n    y = xr.DataArray([[5, 6], [7, 8]], dims=(\"x\", \"y\"))\n    actual = xr.where(cond, x, y)\n    expected = xr.DataArray([[1, 2], [7, 8]], dims=(\"x\", \"y\"))\n    assert_identical(expected, actual)\n", "def test_apply_ufunc_dtype() -> None:\n    # Test that apply_ufunc preserves dtype of input when ufunc returns float\n    da = xr.DataArray([1, 2], dims=\"x\", dtype=np.int64)\n    result = xr.apply_ufunc(np.exp, da)\n    assert result.dtype == np.float64\n\n    # Test that apply_ufunc preserves dtype of input when ufunc returns complex\n    da = xr.DataArray([1, 2], dims=\"x\", dtype=np.int64)\n    result = xr.apply_ufunc(lambda x: x + 1j*x, da)\n    assert result.dtype == np.complex128\n\n    # Test that apply_ufunc raises an error if output_dtype is not provided for dask='parallelized'\n    da = xr.DataArray([1, 2], dims=\"x\", dtype=np.int64).chunk()\n    with pytest.raises(TypeError):\n        xr.apply_ufunc(np.exp, da, dask='parallelized')\n", "def test_where_coord() -> None:\n    cond = xr.DataArray([True, False], dims=\"x\", coords={\"x\": [\"a\", \"b\"]})\n    actual = xr.where(cond, 1, 0)\n    expected = xr.DataArray([1, 0], dims=\"x\", coords={\"x\": [\"a\", \"b\"]})\n    assert_identical(expected, actual)\n\n    cond = xr.DataArray([True, False], dims=\"x\", coords={\"x\": [1, 2]})\n    actual = xr.where(cond, 1, 0)\n    expected = xr.DataArray([1, 0], dims=\"x\", coords={\"x\": [1, 2]})\n    assert_identical(expected, actual)\n"], "sample_610": ["def test_strftime(date_type, index):\n    date_format = \"%Y-%m-%d %H:%M:%S\"\n    expected = pd.Index([date.strftime(date_format) for date in index])\n    result = index.strftime(date_format)\n    assert result.equals(expected)\n", "def test_to_datetimeindex_unsafe_with_standard_calendar():\n    index = xr.cftime_range(\"2000\", periods=5, calendar=\"standard\")\n    expected = pd.date_range(\"2000\", periods=5)\n    with pytest.warns(None) as record:\n        result = index.to_datetimeindex(unsafe=True)\n    assert result.equals(expected)\n    assert len(record) == 0\n", "def test_cftimeindex_name_property(index):\n    assert index.name is None\n\n    named_index = CFTimeIndex(index, name=\"foo\")\n    assert named_index.name == \"foo\"\n", "def test_cftimeindex_freq_property():\n    index = xr.cftime_range(\"2000-01-01\", periods=10, freq=\"D\")\n    assert index.freq == \"D\"\n\n    index = xr.cftime_range(\"2000-01-01\", periods=10, freq=\"2D\")\n    assert index.freq == \"2D\"\n\n    index = xr.cftime_range(\"2000-01-01\", periods=10, freq=\"MS\")\n    assert index.freq == \"MS\"\n", "def test_to_datetimeindex_unsafe_allows_invalid_dates():\n    index = xr.cftime_range(\"0001\", periods=5, calendar=\"standard\")\n    expected = pd.date_range(\"1970-01-01\", periods=5)\n    result = index.to_datetimeindex(unsafe=True)\n    assert result.equals(expected)\n"], "sample_611": ["def test_date_range_like_dataarray_input(start, freq, cal_src, cal_tgt, use_cftime):\n    source = DataArray(date_range(start, periods=12, freq=freq, calendar=cal_src))\n    out = date_range_like(source, cal_tgt, use_cftime=use_cftime)\n    assert len(out) == 12\n    assert infer_freq(out) == freq\n", "def test_cftime_range_end_of_month_with_various_calendars(freq, calendar):\n    result = cftime_range(\"2000-02\", periods=6, freq=freq, calendar=calendar)\n    assert len(result) == 6\n    assert result[0].month == 2\n", "def test_invalid_date_range_inputs(start, end, periods, freq, closed):\n    with pytest.raises(ValueError):\n        date_range(start, end, periods, freq, closed=closed)\n", "def test_date_range_like_invalid_source():\n    with pytest.raises(ValueError):\n        date_range_like([1, 2, 3], \"standard\")\n\n    with pytest.raises(ValueError):\n        date_range_like(DataArray([1, 2, 3]), \"standard\")\n\n    with pytest.raises(ValueError):\n        date_range_like(DataArray([1, 2, 3], dims=(\"x\", \"y\")), \"standard\")\n", "def test_date_range_with_tz(start, calendar, use_cftime, expected_type):\n    dr = date_range(\n        start, periods=14, freq=\"D\", calendar=calendar, use_cftime=use_cftime, tz=\"UTC\"\n    )\n\n    assert isinstance(dr, expected_type)\n"], "sample_612": ["def test_groupby_map_non_dimension_coordinate():\n    # Test mapping a function over a groupby object where the grouped variable\n    # is not a dimension coordinate.\n    array = DataArray(\n        np.arange(10),\n        dims=\"x\",\n        coords={\"x\": np.arange(10), \"y\": (\"x\", np.arange(10))},\n    )\n    expected = DataArray(np.arange(10) + 1, dims=\"x\", coords=array.coords)\n    actual = array.groupby(\"y\").map(lambda x: x + 1)\n    assert_identical(actual, expected)\n", "def test_groupby_dataset_map_dataarray_func_with_kwargs():\n    # regression GH6379\n    ds = xr.Dataset({\"foo\": (\"x\", [1, 2, 3, 4])}, coords={\"x\": [0, 0, 1, 1]})\n    actual = ds.groupby(\"x\").map(lambda grp: grp.foo.mean(dim=\"x\"), dim=\"x\")\n    expected = xr.DataArray([1.5, 3.5], coords={\"x\": [0, 1]}, dims=\"x\", name=\"foo\")\n    assert_identical(actual, expected)\n", "def test_groupby_dataset_reduce_to_empty_dim():\n    # regression test for GH4271\n    data = Dataset(\n        {\n            \"xy\": ([\"x\", \"y\"], np.random.randn(3, 4)),\n            \"xonly\": (\"x\", np.random.randn(3)),\n            \"yonly\": (\"y\", np.random.randn(4)),\n            \"letters\": (\"y\", [\"a\", \"a\", \"b\", \"b\"]),\n        }\n    )\n\n    actual = data.groupby(\"x\").mean(\"x\")\n    expected = Dataset(\n        {\n            \"xy\": ((\"y\",), data[\"xy\"].mean(axis=0)),\n            \"xonly\": ((), data[\"xonly\"].mean()),\n            \"yonly\": (\"y\", data[\"yonly\"]),\n            \"letters\": (\"y\", data[\"letters\"]),\n        }\n    )\n    assert_allclose(actual, expected)\n", "def test_groupby_bins_no_groups():\n    array = DataArray(np.arange(4), dims=\"dim_0\")\n    bins = [5, 6, 7]\n    bin_coords = pd.cut(array[\"dim_0\"], bins).categories\n    expected = DataArray([], dims=\"dim_0_bins\", coords={\"dim_0_bins\": bin_coords})\n    actual = array.groupby_bins(\"dim_0\", bins).sum()\n    assert_identical(expected, actual)\n", "def test_groupby_multiple_dims():\n    da = xr.DataArray(\n        np.random.rand(4, 3, 2),\n        dims=(\"x\", \"y\", \"z\"),\n        coords={\"x\": [1, 1, 2, 2], \"y\": [1, 2, 3], \"z\": [1, 2]},\n    )\n    grouped = da.groupby((\"x\", \"y\"))\n    actual = grouped.sum(dim=\"z\")\n    expected = da.sum(\"z\").groupby((\"x\", \"y\")).sum()\n    assert_identical(actual, expected)\n"], "sample_613": ["def test_groupby_apply_func_with_args_kwargs(dataset):\n        return x + arg1 + arg2 + kwarg1 + kwarg2\n\n    result = dataset.groupby(\"x\").apply(func, 1, 2, kwarg1=3, kwarg2=4)\n    expected = dataset + 10\n    assert_identical(result, expected)\n", "def test_groupby_map_uses_coords(self):\n    # regression test for GH3464\n\n    array = xr.DataArray(\n        [1, 2, 3],\n        dims=[\"x\"],\n        coords={\"x\": [10, 20, 30], \"y\": (\"x\", [100, 200, 300])},\n    )\n\n        return x.coords[\"y\"]\n\n    expected = xr.DataArray([100, 200, 300], dims=[\"x\"], coords=array.coords)\n    actual = array.groupby(\"x\").map(func)\n    assert_identical(expected, actual)\n", "def test_groupby_dataset_fillna_no_coords():\n    # GH4490: groupby should not add new coords when fillna is used\n    ds = Dataset(\n        {\"a\": (\"x\", [np.nan, 1, np.nan, 3])},\n        {\"x\": [0, 0, 1, 1]},\n    )\n    filled_ds = ds.groupby(\"x\").fillna(0)\n    assert_identical(ds.coords, filled_ds.coords)\n", "def test_groupby_reduce_dimension_error_multiple_dims(array) -> None:\n    grouped = array.groupby(\"y\")\n    with pytest.raises(ValueError, match=r\"cannot reduce over dimensions\"):\n        grouped.mean(dim=[\"x\", \"z\"])\n", "def test_groupby_bins_no_coord_change(self):\n    array = DataArray(np.arange(4), dims=\"dim_0\")\n    bins = [0, 1.5, 5]\n    actual = array.groupby_bins(\"dim_0\", bins).map(lambda x: x.sum())\n    expected = DataArray(\n        [1, 7], dims=\"dim_0_bins\", coords={\"dim_0_bins\": pd.cut(array[\"dim_0\"], bins).categories}\n    )\n    assert_identical(expected, actual)\n    # make sure original array dims are unchanged\n    assert len(array.dim_0) == 4\n"], "sample_614": ["def test__element_formatter() -> None:\n    elements = [\"a\", \"b\", \"c\", \"d\"]\n    delimiter = \", \"\n    col_width = 5\n\n    expected = \"a, b, c, d\"\n    actual = formatting._element_formatter(elements, col_width, delimiter=delimiter)\n    assert actual == expected\n\n    # Test with max_rows\n    max_rows = 2\n    expected = \"a, b, ...\\n     d\"\n    actual = formatting._element_formatter(\n        elements, col_width, max_rows=max_rows, delimiter=delimiter\n    )\n    assert actual == expected\n\n    # Test with long string\n    elements = [\"very_long_string\" * 10]\n    expected = \"very_long_string...\" * 3 + \"\\n\" + \"     ...\" * 2\n    actual = formatting._element_formatter(elements, col_width, delimiter=delimiter)\n    assert len(actual) <= 80  # check it doesn't exceed display width\n\n    # Test with max_rows and long string\n    max_rows = 1\n    expected = \"very_long_string...\" * 2 + \"\\n\" + \"     ...\"\n    actual = formatting._element_formatter(\n        elements, col_width, max_rows=max_rows, delimiter=delimiter\n    )\n    assert len(actual) <= 80  # check it doesn't exceed display width\n", "def test__element_formatter(self) -> None:\n    elements = [f\"dim_{i}\" for i in range(10)]\n    col_width = 5\n\n    # test with delimiter\n    expected = \"dim_0, dim_1, dim_2, ...\\n      dim_7, dim_8, dim_9\"\n    actual = formatting._element_formatter(elements, col_width, max_rows=3)\n    assert actual == expected\n\n    # test without delimiter\n    expected = \"dim_0 dim_1 dim_2 ...\\n      dim_7 dim_8 dim_9\"\n    actual = formatting._element_formatter(elements, col_width, max_rows=3, delimiter=\" \")\n    assert actual == expected\n\n    # test with no max_rows\n    expected = (\n        \"dim_0, dim_1, dim_2, dim_3, dim_4, dim_5, dim_6, dim_7,\\n\"\n        \"      dim_8, dim_9\"\n    )\n    actual = formatting._element_formatter(elements, col_width)\n    assert actual == expected\n\n    # test with max_rows of 1\n    expected = \"dim_0, dim_1, dim_2, ...\\n      dim_9\"\n    actual = formatting._element_formatter(elements, col_width, max_rows=1)\n    assert actual == expected\n", "def test__element_formatter() -> None:\n    elements = [\"dim1\", \"dim2\", \"dim3\"]\n    col_width = 10\n\n    # Test no max rows\n    result = formatting._element_formatter(elements, col_width)\n    expected = \"dim1, dim2, dim3\"\n    assert result == expected\n\n    # Test with max rows\n    result = formatting._element_formatter(elements, col_width, max_rows=2)\n    expected = \"dim1, ...\\ndim3\"\n    assert result == expected\n\n    # Test with single element and max rows\n    result = formatting._element_formatter([\"dim1\"], col_width, max_rows=1)\n    expected = \"dim1\"\n    assert result == expected\n\n    # Test with long elements\n    elements = [\"dim1_with_long_name\", \"dim2_with_long_name\", \"dim3_with_long_name\"]\n    result = formatting._element_formatter(elements, col_width)\n    expected = \"dim1_with_long_name, ...\\ndim3_with_long_name\"\n    assert result == expected\n\n    # Test with delimiter\n    result = formatting._element_formatter(elements, col_width, delimiter=\" | \")\n    expected = \"dim1_with_long_name | ...\\ndim3_with_long_name\"\n    assert result == expected\n", "def test__element_formatter() -> None:\n    elements = [\"dim1\", \"dim2\", \"dim3\"]\n    col_width = 10\n    delimiter = \", \"\n    expected = \"dim1, dim2, dim3\"\n    actual = formatting._element_formatter(elements, col_width, delimiter=delimiter)\n    assert actual == expected\n\n    elements = [\"dim1\", \"dim2\", \"dim3\", \"dim4\", \"dim5\", \"dim6\"]\n    max_rows = 2\n    expected = \"dim1, dim2, ...\\n          dim5, dim6\"\n    actual = formatting._element_formatter(\n        elements, col_width, max_rows=max_rows, delimiter=delimiter\n    )\n    assert actual == expected\n", "def test_dim_summary_limited() -> None:\n    cases = [\n        ((10,), 5, \"x: 10\"),\n        ((10, 20), 5, \"x: 10, y: 20\"),\n        ((10, 20, 30), 5, \"x: 10, y: 20, ...\"),\n        ((10, 20, 30, 40), 7, \"x: 10, y: 20, z: 30, ...\"),\n        ((10, 20, 30, 40, 50), 10, \"x: 10, y: 20, z: 30, t: 40, ...\"),\n    ]\n    for dims, max_rows, expected in cases:\n        ds = xr.Dataset()\n        ds.attrs[\"dimensions\"] = dims\n        actual = formatting.dim_summary_limited(ds, max_rows=max_rows)\n        assert actual == expected\n\n    # Test with unlimited max_rows\n    dims = (10, 20, 30, 40, 50)\n    ds = xr.Dataset()\n    ds.attrs[\"dimensions\"] = dims\n    actual = formatting.dim_summary_limited(ds)\n    expected = \"x: 10, y: 20, z: 30, t: 40, u: 50\"\n    assert actual == expected\n"], "sample_615": ["def test_cross_3d_vectors() -> None:\n    # Define two 3D vectors\n    a = xr.DataArray([1, 2, 3], dims=(\"cartesian\",))\n    b = xr.DataArray([4, 5, 6], dims=(\"cartesian\",))\n\n    # Compute the cross product\n    c = xr.cross(a, b, dim=\"cartesian\")\n\n    # Expected result\n    expected = xr.DataArray([-3, 6, -3], dims=(\"cartesian\",))\n\n    assert_identical(c, expected)\n", "def test_cross() -> None:\n    a = xr.DataArray([1, 2, 3], dims=\"cartesian\")\n    b = xr.DataArray([4, 5, 6], dims=\"cartesian\")\n\n    expected = xr.DataArray([-3, 6, -3], dims=\"cartesian\")\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n\n    # Test with broadcast\n    a = xr.DataArray([[1, 2, 3], [4, 5, 6]], dims=(\"time\", \"cartesian\"))\n    b = xr.DataArray([4, 5, 6], dims=\"cartesian\")\n\n    expected = xr.DataArray([[-3, 6, -3], [-6, 12, -6]], dims=(\"time\", \"cartesian\"))\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n\n    # Test with missing values\n    a = xr.DataArray([1, np.nan, 3], dims=\"cartesian\")\n    b = xr.DataArray([4, 5, 6], dims=\"cartesian\")\n\n    expected = xr.DataArray([np.nan, np.nan, np.nan], dims=\"cartesian\")\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n", "def test_cross_3d() -> None:\n    # Ensure that the cross product function works as expected with 3D vectors.\n    a = xr.DataArray([1, 2, 3], dims=[\"cartesian\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"])\n\n    result = xr.cross(a, b, dim=\"cartesian\")\n    expected_result = np.cross([1, 2, 3], [4, 5, 6])\n    assert_array_equal(result.values, expected_result)\n", "def test_cross() -> None:\n    # 1D vectors\n    a = xr.DataArray([1, 2, 3], dims=\"cartesian\")\n    b = xr.DataArray([4, 5, 6], dims=\"cartesian\")\n    expected = np.cross(a.data, b.data)\n    result = xr.cross(a, b, dim=\"cartesian\")\n    assert_array_equal(result.data, expected)\n\n    # 2D vectors\n    a = xr.DataArray([[1, 2], [3, 4]], dims=(\"time\", \"cartesian\"))\n    b = xr.DataArray([[4, 5], [6, 7]], dims=(\"time\", \"cartesian\"))\n    expected = np.cross(a.data, b.data)\n    result = xr.cross(a, b, dim=\"cartesian\")\n    assert_array_equal(result.data, expected)\n\n    # 2D vectors with different sizes\n    a = xr.DataArray([[1, 2], [3, 4]], dims=(\"time\", \"cartesian\"))\n    b = xr.DataArray([[4, 5, 6], [6, 7, 8]], dims=(\"time\", \"cartesian\"))\n    expected = np.cross(np.pad(a.data, ((0, 0), (0, 1))), b.data)\n    result = xr.cross(a, b, dim=\"cartesian\")\n    assert_array_equal(result.data, expected)\n\n    # Vectors with different coordinates\n    a = xr.DataArray(\n        [[1, 2], [3, 4]],\n        dims=(\"time\", \"cartesian\"),\n        coords={\"cartesian\": (\"cartesian\", [\"x\", \"y\"])},\n    )\n    b = xr.DataArray(\n        [[4, 5, 6], [6, 7, 8]],\n        dims=(\"time\", \"cartesian\"),\n        coords={\"cartesian\": (\"cartesian\", [\"x\", \"y\", \"z\"])},\n    )\n    expected = np.cross(np.pad(a.data, ((0, 0), (0, 1))), b.data)\n    result = xr.cross(a, b, dim=\"cartesian\")\n    assert_array_equal(result.data, expected)\n", "def test_cross() -> None:\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n\n    actual = xr.cross(a, b, dim=\"x\")\n    expected = xr.DataArray([-3, 6, -3], dims=\"x\")\n    assert_identical(actual, expected)\n\n    # test with broadcasting\n    a = xr.DataArray([1, 2], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n\n    actual = xr.cross(a, b, dim=\"x\")\n    expected = xr.DataArray([0, 0, -3], dims=\"x\")\n    assert_identical(actual, expected)\n\n    # test with different dimension names\n    a = xr.DataArray([1, 2, 3], dims=\"u\")\n    b = xr.DataArray([4, 5, 6], dims=\"u\")\n\n    actual = xr.cross(a, b, dim=\"u\")\n    expected = xr.DataArray([-3, 6, -3], dims=\"u\")\n    assert_identical(actual, expected)\n"], "sample_616": ["def test_cross_3d() -> None:\n    a = xr.DataArray(\n        [[1, 2], [3, 4]],\n        dims=[\"x\", \"cartesian\"],\n        coords={\"cartesian\": [\"x\", \"y\"]},\n    )\n    b = xr.DataArray(\n        [[0, 4], [5, 6]],\n        dims=[\"x\", \"cartesian\"],\n        coords={\"cartesian\": [\"x\", \"z\"]},\n    )\n    expected = xr.DataArray([[-8, -10]], dims=[\"cartesian\"], coords={\"cartesian\": [\"y\"]})\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(actual, expected)\n", "def test_cross():\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n    expected = np.cross(a.values, b.values)\n    result = xr.cross(a, b, dim=\"x\")\n    assert_identical(result, xr.DataArray(expected, dims=\"x\"))\n\n    # Test with 2D arrays\n    a = xr.DataArray([[1, 2, 3], [4, 5, 6]], dims=(\"y\", \"x\"))\n    b = xr.DataArray([[7, 8, 9], [10, 11, 12]], dims=(\"y\", \"x\"))\n    expected = np.cross(a.values, b.values)\n    result = xr.cross(a, b, dim=\"x\")\n    assert_identical(result, xr.DataArray(expected, dims=(\"y\", \"x\")))\n\n    # Test with different dimension lengths\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([4, 5], dims=\"x\")\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"x\")\n\n    # Test with non-numeric data\n    a = xr.DataArray([\"a\", \"b\", \"c\"], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n    with pytest.raises(TypeError):\n        xr.cross(a, b, dim=\"x\")\n\n    # Test with missing values\n    a = xr.DataArray([1, 2, np.nan], dims=\"x\")\n    b = xr.DataArray([4, 5, 6], dims=\"x\")\n    expected = np.cross(a.values, b.values)\n    result = xr.cross(a, b, dim=\"x\")\n    assert_identical(result, xr.DataArray(expected, dims=\"x\"))\n", "def test_polyval_degree_dim_size_mismatch():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 2, 4]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs)\n", "def test_polyval_degree_coord_checks():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.assign_coords(degree=[1, 2, 3]))\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.assign_coords(degree=[-1, 0, 1]))\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.assign_coords(degree=[0, 0, 0]))\n", "def test_cross_2d():\n    # Cross product of 2D vectors\n    a = xr.DataArray([[1, 2], [3, 4]], dims=(\"x\", \"y\"))\n    b = xr.DataArray([[5, 6], [7, 8]], dims=(\"x\", \"y\"))\n    expected = xr.DataArray([-4, -4], dims=(\"x\"))\n    actual = xr.cross(a, b, dim=\"y\")\n    assert_identical(actual, expected)\n\n    # Cross product with one vector as a scalar\n    a = xr.DataArray([1, 2], dims=(\"x\"))\n    b = xr.DataArray([[5, 6], [7, 8]], dims=(\"x\", \"y\"))\n    expected = xr.DataArray([[-6, 5], [-8, 7]], dims=(\"x\", \"y\"))\n    actual = xr.cross(a, b, dim=\"y\")\n    assert_identical(actual, expected)\n"], "sample_618": ["def test_polyval_degree_dim_not_integer():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1.5, 2]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs)\n", "def test_cross_1d() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"])\n\n    expected = xr.DataArray([-3, 6, -3], dims=[\"x\"])\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_identical(expected, actual)\n\n    # Test with different dimension names\n    c = xr.DataArray([7, 8, 9], dims=[\"y\"])\n    expected = xr.DataArray([-15, 24, -21], dims=[\"x\"])\n    actual = xr.cross(a, c, dim=\"x\")\n    assert_identical(expected, actual)\n", "def test_cross_2d_vectors() -> None:\n    a = xr.DataArray(\n        [[1, 2], [3, 4]],\n        dims=(\"x\", \"cartesian\"),\n        coords={\"cartesian\": ([\"cartesian\"], [\"x\", \"y\"])},\n    )\n    b = xr.DataArray(\n        [[5, 6], [7, 8]],\n        dims=(\"x\", \"cartesian\"),\n        coords={\"cartesian\": ([\"cartesian\"], [\"x\", \"y\"])},\n    )\n\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    expected = xr.DataArray([(-4, 4)], dims=(\"x\",))\n    assert_identical(actual, expected)\n", "def test_cross_3d():\n    a = xr.DataArray([1, 2, 3], dims=\"cartesian\")\n    b = xr.DataArray([4, 5, 6], dims=\"cartesian\")\n\n    expected = xr.DataArray([-3, 6, -3], dims=\"cartesian\")\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(expected, actual)\n", "def test_polyval_degree_dim_non_integer():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0.0, 1.0, 2.0]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs)\n"], "sample_619": ["def test_encode_cf_datetime_nonstandard_calendar_fallback() -> None:\n    import cftime\n\n    calendar = \"360_day\"\n    times = cftime.num2date([0, 1], \"hours since 2000-01-01\", calendar)\n    units = \"days since 2000-01-01\"\n\n    # Intentionally pass a bad value for use_cftime to force a fallback.\n    encoded, _, _ = encode_cf_datetime(times, units, calendar, use_cftime=False)\n\n    expected = cftime.date2num(times, units, calendar)\n    np.testing.assert_equal(encoded, expected)\n", "def test_decode_cf_datetime_int32_overflow_error():\n    units = \"microseconds since 1700-01-01\"\n    calendar = \"360_day\"\n    num_dates = np.int32(1_000_000 * 86_400 * 360 * 500)\n    with pytest.raises(OverflowError):\n        decode_cf_datetime(num_dates, units, calendar)\n", "def test_cftime_to_nptime_with_raise_on_invalid() -> None:\n    import cftime\n\n    # Create a cftime object that will be invalid when converted to nptime\n    cftime_obj = cftime.DatetimeNoLeap(2000, 2, 30)\n\n    with pytest.raises(ValueError):\n        cftime_to_nptime(cftime_obj, raise_on_invalid=True)\n", "def test_encode_cf_datetime_with_invalid_units() -> None:\n    dates = pd.date_range(\"2000\", periods=3)\n    with pytest.raises(ValueError, match=\"invalid time units\"):\n        encode_cf_datetime(dates, units=\"days after 2000-01-01\")\n    with pytest.raises(ValueError, match=\"invalid reference date\"):\n        encode_cf_datetime(dates, units=\"days since NO_YEAR\")\n", "def test_encode_cf_datetime_with_cftime_roundtrip() -> None:\n    import cftime\n\n    calendar = \"360_day\"\n    times = cftime.num2date([0, 1], \"hours since 2000-01-01\", calendar)\n\n    encoded, units, _ = encode_cf_datetime(times, calendar=calendar)\n    decoded = decode_cf_datetime(encoded, units, calendar=calendar)\n\n    np.testing.assert_array_equal(decoded, times)\n"], "sample_620": ["def test_concat_along_new_dim_with_scalar_coordinate() -> None:\n    ds1 = Dataset(coords={\"x\": 0})\n    ds2 = Dataset(coords={\"x\": 1})\n    actual = concat([ds1, ds2], dim=\"y\")\n    expected = Dataset(coords={\"x\": (\"y\", [0, 1])})\n    assert_identical(actual, expected)\n", "def test_concat_invalid_combine_attrs() -> None:\n    ds1 = Dataset({}, attrs={\"a\": 1})\n    ds2 = Dataset({}, attrs={\"a\": 2})\n\n    with pytest.raises(ValueError, match=\"combine_attrs must be one of\"):\n        concat([ds1, ds2], dim=\"x\", combine_attrs=\"invalid\")\n\n    with pytest.raises(TypeError, match=\"combine_attrs must be a callable or a string\"):\n        concat([ds1, ds2], dim=\"x\", combine_attrs=42)\n", "def test_concat_conflicting_coords() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [3, 4])})\n    ds3 = Dataset(coords={\"x\": (\"y\", [5, 6])})\n\n    # conflicting coords should raise an error\n    with pytest.raises(ValueError, match=r\"'x' must have either an index or no index in all datasets\"):\n        concat([ds1, ds2, ds3], dim=\"x\")\n\n    # conflicting coords should raise an error even if they are not concatenated\n    with pytest.raises(ValueError, match=r\"'x' is a coordinate in some datasets but not others\"):\n        concat([ds1, ds3], dim=\"y\")\n", "def test_concat_coord_dtype_promotion() -> None:\n    # Test that concatenating DataArrays with coords of different dtypes results in a promoted dtype.\n    da1 = DataArray([1, 2], dims=\"x\", coords={\"x\": [1.0, 2.0]})\n    da2 = DataArray([3, 4], dims=\"x\", coords={\"x\": [3, 4]})\n\n    actual = concat([da1, da2], dim=\"x\")\n    expected = DataArray([1, 2, 3, 4], dims=\"x\", coords={\"x\": [1.0, 2.0, 3.0, 4.0]})\n\n    assert_identical(actual, expected)\n", "def test_concat_positions() -> None:\n    da1 = DataArray([1, 2, 3], dims=\"x\")\n    da2 = DataArray([4, 5, 6], dims=\"x\")\n\n    positions = [[0, 2, 4], [1, 3, 5]]\n    actual = concat([da1, da2], dim=\"new_dim\", positions=positions)\n    expected = DataArray(\n        [1, 4, 2, 5, 3, 6],\n        dims=[\"new_dim\"],\n        coords={\"x\": (\"new_dim\", [0, 1, 2, 3, 4, 5])},\n    )\n    assert_identical(actual, expected)\n"], "sample_621": ["    def indexes_and_vars(self) -> tuple[list[PandasIndex], dict[Hashable, Variable]]:\n        x_idx = PandasIndex(pd.Index([1, 2, 3], name=\"x\"), \"x\")\n        y_idx = PandasIndex(pd.Index([4, 5, 6], name=\"y\"), \"y\")\n        z_pd_midx = pd.MultiIndex.from_product(\n            [[\"a\", \"b\"], [1, 2]], names=[\"one\", \"two\"]\n        )\n        z_midx = PandasMultiIndex(z_pd_midx, \"z\")\n\n        indexes = [x_idx, y_idx, z_midx]\n\n        variables = {}\n        for idx in indexes:\n            variables.update(idx.create_variables())\n\n        return indexes, variables\n", "def test_copy_indexes(self, indexes) -> None:\n    new_indexes, new_variables = indexes.copy_indexes(deep=True)\n    assert new_indexes.keys() == indexes.keys()\n    for k in new_indexes:\n        assert new_indexes[k].equals(indexes[k])\n        assert new_indexes[k] is not indexes[k]\n\n    for k in new_variables:\n        assert_identical(new_variables[k], indexes.variables[k])\n        assert new_variables[k] is not indexes.variables[k]\n\n    new_indexes, new_variables = indexes.copy_indexes(deep=False)\n    for k in new_indexes:\n        assert new_indexes[k] is indexes[k]\n    for k in new_variables:\n        assert new_variables[k] is indexes.variables[k]\n", "    def test_copy_indexes(self, indexes_and_vars):\n        indexes, variables = indexes_and_vars\n        index_dict = {idx.index.name: idx for idx in indexes}\n        copied_indexes, copied_variables = Indexes(index_dict, variables).copy_indexes()\n\n        assert len(copied_indexes) == len(index_dict)\n        for k, v in copied_indexes.items():\n            assert isinstance(v, PandasIndex)\n            assert v is not index_dict[k]\n            assert v.equals(index_dict[k])\n\n        assert len(copied_variables) == len(variables)\n        for k, v in copied_variables.items():\n            assert isinstance(v, Variable)\n            assert v is not variables[k]\n            assert_identical(v, variables[k])\n", "    def indexes_and_vars(self) -> tuple[list[PandasIndex], dict[Hashable, Variable]]:\n        x_idx = PandasIndex(pd.Index([1, 2, 3], name=\"x\"), \"x\")\n        y_idx = PandasIndex(pd.Index([4, 5, 6], name=\"y\"), \"y\")\n        z_pd_midx = pd.MultiIndex.from_product(\n            [[\"a\", \"b\"], [1, 2]], names=[\"one\", \"two\"]\n        )\n        z_midx = PandasMultiIndex(z_pd_midx, \"z\")\n\n        indexes = [x_idx, y_idx, z_midx]\n\n        variables = {}\n        for idx in indexes:\n            variables.update(idx.create_variables())\n\n        return indexes, variables\n", "def test_copy_indexes(self, indexes) -> None:\n    new_indexes, new_variables = indexes.copy_indexes()\n\n    assert len(new_indexes) == len(indexes)\n    for k, idx in new_indexes.items():\n        assert type(idx) is type(indexes[k])\n        assert idx.equals(indexes[k])\n\n    assert len(new_variables) == len(indexes.variables)\n    for k, var in new_variables.items():\n        assert_identical(var, indexes.variables[k])\n"], "sample_622": ["def test_decode_cf_variables_with_bounds() -> None:\n    # Test that decode_cf_variables properly updates bounds variables when decoding times.\n    original = Dataset(\n        {\n            \"time\": (\"time\", [0, 1, 2], {\"units\": \"days since 2000-01-01\"}),\n            \"time_bounds\": (\n                (\"time\", \"bnds\"),\n                [[-0.5, 0.5], [0.5, 1.5], [1.5, 2.5]],\n                {\"units\": \"days since 2000-01-01\"},\n            ),\n        }\n    )\n    expected = Dataset(\n        {\n            \"time\": pd.date_range(\"2000-01-01\", periods=3),\n            \"time_bounds\": (\n                (\"time\", \"bnds\"),\n                [\n                    [pd.Timestamp(\"1999-12-31\"), pd.Timestamp(\"2000-01-01\")],\n                    [pd.Timestamp(\"2000-01-01\"), pd.Timestamp(\"2000-01-02\")],\n                    [pd.Timestamp(\"2000-01-02\"), pd.Timestamp(\"2000-01-03\")],\n                ],\n            ),\n        }\n    )\n    actual = conventions.decode_cf_variables(\n        original._variables, original.attrs, decode_times=True\n    )[0]\n    assert_identical(actual, expected._variables)\n", "def test_decode_cf_variables_bounds() -> None:\n    # Test that decode_cf_variables updates bounds variables correctly\n    original = Dataset(\n        {\n            \"t\": (\"t\", [0, 1, 2], {\"units\": \"days since 2000-01-01\", \"bounds\": \"tbnds\"}),\n            \"tbnds\": ((\"t\", \"bnds\"), [[-1, 1], [1, 3], [3, 5]]),\n            \"foo\": (\"t\", [0, 0, 0], {\"coordinates\": \"y\", \"units\": \"bar\"}),\n            \"y\": (\"t\", [5, 10, -999], {\"_FillValue\": -999}),\n        }\n    )\n    expected = Dataset(\n        {\n            \"foo\": (\"t\", [0, 0, 0], {\"units\": \"bar\"}),\n        },\n        coords={\n            \"t\": pd.date_range(\"1999-12-31\", periods=3),\n            \"tbnds\": ((\"t\", \"bnds\"), pd.to_datetime([[\"1999-12-30\", \"2000-01-02\"], [\"2000-01-02\", \"2000-01-04\"], [\"2000-01-04\", \"2000-01-06\"]]).values),\n            \"y\": (\"t\", [5.0, 10.0, np.nan]),\n        },\n    )\n    actual = conventions.decode_cf(original)\n    assert_identical(expected, actual)\n", "def test_decode_cf_with_conflicting_fill_missing_value_and_scale() -> None:\n    expected = Variable([\"t\"], [np.nan, np.nan, 2], {\"scale_factor\": 1.0})\n    var = Variable(\n        [\"t\"],\n        np.arange(3),\n        {\n            \"missing_value\": 0,\n            \"_FillValue\": 1,\n            \"add_offset\": 0,\n            \"scale_factor\": 1.0,\n        },\n    )\n    with warnings.catch_warnings(record=True) as w:\n        actual = conventions.decode_cf_variable(\"t\", var)\n        assert_identical(actual, expected)\n        assert \"has multiple fill\" in str(w[0].message)\n", "def test_decode_cf_variables_stack_char_dim() -> None:\n    # Test that we can decode variables when the last dimension is a character\n    # dimension and stack_char_dim is True.\n\n    # Create a variable with a character dimension as the last dimension\n    var = Variable(\n        [\"time\", \"string10\"],\n        [[\"h\", \"e\", \"l\", \"l\", \"o\", \"\", \"\", \"\", \"\", \"\"], [\"w\", \"o\", \"r\", \"l\", \"d\", \"\", \"\", \"\", \"\", \"\"]],\n    )\n\n    # Decode the variable with stack_char_dim=True\n    decoded_var = conventions.decode_cf_variable(\"test\", var, stack_char_dim=True)\n\n    # Check that the decoded variable has the correct values\n    expected_values = np.array([\"hello\", \"world\"], dtype=object)\n    assert_array_equal(decoded_var.values, expected_values)\n", "def test_decode_cf_variable_with_bytes_dtype() -> None:\n    v = Variable([\"t\"], [b\"a\", b\"b\", b\"c\"])\n    v_decoded = conventions.decode_cf_variable(\"test2\", v)\n    assert_identical(v, v_decoded)\n"], "sample_623": ["    def test_chunking_roundtrip(self):\n        # Create a sample dataset\n        ds = xr.Dataset(\n            {\n                \"temperature\": ((\"x\", \"y\"), np.random.rand(10, 10)),\n                \"precipitation\": ((\"x\", \"y\"), np.random.rand(10, 10)),\n            },\n            coords={\"x\": np.arange(10), \"y\": np.arange(10)},\n        )\n\n        # Chunk the dataset\n        chunks = {\"x\": 2, \"y\": 2}\n        ds_chunked = ds.chunk(chunks)\n\n        # Save to netCDF and load back\n        with xr.backends.NetCDF4DataStore() as store:\n            ds_chunked.to_netcdf(store)\n            loaded_ds = xr.open_dataset(store, chunks=chunks)\n\n        # Check that chunking is preserved\n        assert loaded_ds.chunks == ds_chunked.chunks\n", "def test_open_dataset_invalid_netcdf():\n    # Create a sample dataset with invalid NetCDF attributes\n    ds = xr.Dataset()\n    ds.attrs[\"invalid_attribute\"] = np.bool_(True)\n\n    # Attempt to open the dataset with the 'h5netcdf' engine and invalid_netcdf=True\n    with pytest.raises(ValueError):\n        xr.open_dataset(ds, engine=\"h5netcdf\", invalid_netcdf=False)\n", "def test_open_dataset_chunking() -> None:\n    \"\"\"Test that open_dataset correctly handles chunking.\"\"\"\n    import dask.array as da\n\n    # Create a sample dataset\n    data = da.random.random((10, 10), chunks=(5, 5))\n    dataset = xr.Dataset()\n    dataset[\"data\"] = ((\"x\", \"y\"), data)\n\n    # Write the dataset to a netCDF file\n    with create_tmp_file() as tmp_file:\n        dataset.to_netcdf(tmp_file)\n\n        # Open the dataset with different chunking options\n        for chunks in [None, {\"x\": 3, \"y\": 3}, \"auto\"]:\n            opened_dataset = xr.open_dataset(tmp_file, chunks=chunks)\n            assert isinstance(opened_dataset[\"data\"].data, da.Array)\n\n        # Check that chunking along a dimension not in the dataset raises an error\n        with pytest.raises(ValueError):\n            xr.open_dataset(tmp_file, chunks={\"z\": 3})\n", "def test_open_dataset_chunks_with_none() -> None:\n    \"\"\"Test that chunks=None returns a Dataset with lazy dask arrays.\"\"\"\n    import dask.array\n\n    # Create a dataset with small, eager arrays\n    ds = xr.Dataset()\n    ds[\"x\"] = xr.Variable(\"x\", np.arange(10))\n\n    # Open the dataset with chunks=None\n    with assert_no_warnings():\n        opened_ds = xr.open_dataset(ds, engine=PassThroughBackendEntrypoint, chunks=None)\n\n    # Check that the resulting dataset has lazy dask arrays\n    assert isinstance(opened_ds[\"x\"].data, dask.array.core.Array)\n", "def test_open_dataset_chunks_auto():\n    \"\"\"Test that open_dataset chunks='auto' uses the engine preferred chunks.\"\"\"\n    shape = (10, 10)\n    pref_chunks = ((5, 5), (2, 2, 2, 2, 2))\n    initial = xr.Dataset(\n        {\n            \"data\": xr.Variable(\n                (\"x\", \"y\"),\n                np.empty(shape, dtype=np.dtype(\"V1\")),\n                encoding={\"preferred_chunks\": dict(zip((\"x\", \"y\"), pref_chunks))},\n            )\n        }\n    )\n\n    with assert_no_warnings():\n        final = xr.open_dataset(initial, engine=PassThroughBackendEntrypoint, chunks=\"auto\")\n\n    assert_identical(initial, final)\n    assert final[\"data\"].chunks == explicit_chunks(pref_chunks, shape)\n"], "sample_624": ["def test_format_timestamp_with_pandas_format() -> None:\n    # Test formatting of pandas Timestamp with a specific format\n    timestamp = pd.Timestamp(\"2022-01-01T12:00:00\")\n    formatted = formatting.format_timestamp(timestamp)\n    assert formatted == \"2022-01-01T12:00:00\"\n\n    timestamp = pd.Timestamp(\"2022-01-01\")\n    formatted = formatting.format_timestamp(timestamp)\n    assert formatted == \"2022-01-01\"\n", "def test_format_timedelta() -> None:\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\"),\n        (np.timedelta64(1, \"D\"), \"1 days 00:00:00\"),\n        (np.timedelta64(-1, \"D\"), \"-1 days +00:00:00\"),\n        (np.timedelta64(2, \"h\"), \"0 days 02:00:00\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n\n    # Test with different formats\n    timedelta_str = formatting.format_timedelta(pd.Timedelta(\"10 days 1 hour\"))\n    assert \"10 days\" in timedelta_str\n    assert \"01:00:00\" in timedelta_str\n\n    timedelta_str = formatting.format_timedelta(\n        pd.Timedelta(\"10 days 1 hour\"), timedelta_format=\"date\"\n    )\n    assert \"10 days\" in timedelta_str\n    assert \"01:00:00\" not in timedelta_str\n\n    timedelta_str = formatting.format_timedelta(\n        pd.Timedelta(\"10 days 1 hour\"), timedelta_format=\"time\"\n    )\n    assert \"10 days\" not in timedelta_str\n    assert \"01:00:00\" in timedelta_str\n", "def test_format_timedelta() -> None:\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\"),\n        (np.timedelta64(10, \"D\"), \"10 days\"),\n        (np.timedelta64(-3, \"D\"), \"-3 days\"),\n        (np.timedelta64(3, \"h\"), \"0 days 03:00:00\"),\n        (np.timedelta64(\"NaT\", \"ns\"), \"NaT\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n\n    # Test with different format options\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"date\", \"10 days\"),\n        (pd.Timedelta(\"-3 days\"), \"date\", \"-3 days\"),\n        (pd.Timedelta(\"3 hours\"), \"date\", \"0 days\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"time\", \"01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"time\", \"+00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"time\", \"03:00:00\"),\n    ]\n    for item, timedelta_format, expected in cases:\n        actual = formatting.format_timedelta(item, timedelta_format=timedelta_format)\n        assert expected == actual\n", "def test_short_data_repr_lazy_array() -> None:\n    from xarray.core.indexing import LazilyIndexedArray\n\n    class LazilyIndexedArrayNotComputable(LazilyIndexedArray):\n            raise NotImplementedError(\"Computing this array is not possible.\")\n\n    arr = LazilyIndexedArrayNotComputable(np.array([1, 2]))\n    var = xr.DataArray(arr)\n\n    # Test that short_data_repr does not crash with a lazy array\n    formatting.short_data_repr(var)\n", "def test_format_timedelta() -> None:\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\", None),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\", None),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\", None),\n        (pd.Timedelta(\"NaT\"), \"NaT\", None),\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days\", \"date\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days\", \"date\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days\", \"date\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\", \"date\"),\n        (pd.Timedelta(\"10 days 1 hour\"), \"01:00:00\", \"time\"),\n        (pd.Timedelta(\"-3 days\"), \"+00:00:00\", \"time\"),\n        (pd.Timedelta(\"3 hours\"), \"03:00:00\", \"time\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\", \"time\"),\n    ]\n    for item, expected, timedelta_format in cases:\n        actual = formatting.format_timedelta(item, timedelta_format=timedelta_format)\n        assert expected == actual\n"], "sample_625": ["def test_apply_ufunc_kwargs() -> None:\n        return x + y + z\n\n    array = np.array([1, 2, 3])\n    variable = xr.Variable(\"x\", array)\n    data_array = xr.DataArray(variable, dims=[\"x\"])\n    dataset = xr.Dataset({\"y\": variable})\n\n    actual_array = apply_ufunc(my_func, array, array, z=1)\n    expected_array = np.array([3, 6, 9])\n    assert_identical(actual_array, expected_array)\n\n    actual_variable = apply_ufunc(my_func, variable, variable, z=1)\n    expected_variable = xr.Variable(\"x\", np.array([3, 6, 9]))\n    assert_identical(actual_variable, expected_variable)\n\n    actual_data_array = apply_ufunc(my_func, data_array, data_array, z=1)\n    expected_data_array = xr.DataArray(\n        xr.Variable(\"x\", np.array([3, 6, 9])), dims=[\"x\"]\n    )\n    assert_identical(actual_data_array, expected_data_array)\n\n    actual_dataset = apply_ufunc(my_func, dataset, dataset, z=1)\n    expected_dataset = xr.Dataset({\"y\": xr.Variable(\"x\", np.array([3, 6, 9]))})\n    assert_identical(actual_dataset, expected_dataset)\n", "def test_unify_chunks() -> None:\n    # Test with DataArray\n    da1 = xr.DataArray(np.random.rand(10), dims=\"x\", chunks=5)\n    da2 = xr.DataArray(np.random.rand(10), dims=\"x\", chunks=2)\n    da3 = xr.DataArray(np.random.rand(10), dims=\"x\")\n\n    unified_da1, unified_da2, unified_da3 = unify_chunks(da1, da2, da3)\n\n    assert unified_da1.chunks == ((10,),)\n    assert unified_da2.chunks == ((10,),)\n    assert unified_da3.chunks == ((10,),)\n\n    # Test with Dataset\n    ds1 = xr.Dataset({\"foo\": da1})\n    ds2 = xr.Dataset({\"foo\": da2})\n    ds3 = xr.Dataset({\"foo\": da3})\n\n    unified_ds1, unified_ds2, unified_ds3 = unify_chunks(ds1, ds2, ds3)\n\n    assert unified_ds1.foo.chunks == ((10,),)\n    assert unified_ds2.foo.chunks == ((10,),)\n    assert unified_ds3.foo.chunks == ((10,),)\n", "def test_unify_chunks() -> None:\n    # Create DataArrays with different chunk sizes\n    da1 = xr.DataArray(np.random.rand(10), dims=\"x\", chunks=5)\n    da2 = xr.DataArray(np.random.rand(10), dims=\"x\", chunks=2)\n\n    # Unify chunks\n    unified_da1, unified_da2 = unify_chunks(da1, da2)\n\n    # Check if chunks are unified\n    assert unified_da1.chunks == ((2, 2, 2, 2, 2),)\n    assert unified_da2.chunks == ((2, 2, 2, 2, 2),)\n\n    # Create Datasets with different chunk sizes\n    ds1 = xr.Dataset()\n    ds1[\"var1\"] = da1\n    ds2 = xr.Dataset()\n    ds2[\"var1\"] = da2\n\n    # Unify chunks\n    unified_ds1, unified_ds2 = unify_chunks(ds1, ds2)\n\n    # Check if chunks are unified\n    assert unified_ds1[\"var1\"].chunks == ((2, 2, 2, 2, 2),)\n    assert unified_ds2[\"var1\"].chunks == ((2, 2, 2, 2, 2),)\n", "def test_cross_2d() -> None:\n    da_a = xr.DataArray(\n        [[1, 2], [0.1, 0.2]],\n        dims=(\"space\", \"cartesian\"),\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n    da_b = xr.DataArray(\n        [[4, 5], [15, 10]],\n        dims=(\"space\", \"cartesian\"),\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\"])),\n    )\n\n    cross_product = xr.cross(da_a, da_b, dim=\"cartesian\")\n    expected_result = xr.DataArray([0, 0], dims=(\"space\"))\n    assert_identical(cross_product, expected_result)\n", "def test_polyval_degree_dim_not_integer_dtype() -> None:\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.assign_coords(degree=coeffs.degree.astype(float)))\n"], "sample_626": ["def test_expanded_indexer() -> None:\n    key = (1, slice(None), Ellipsis, 0)\n    ndim = 5\n\n    expected = (1, slice(None), slice(None), 0, slice(None))\n    actual = expanded_indexer(key, ndim)\n\n    assert actual == expected\n\n    with pytest.raises(IndexError):\n        expanded_indexer((1, 2, 3, 4, 5, 6), ndim)\n", "def test_expanded_indexer():\n    key = slice(None)\n    ndim = 3\n    expected = (slice(None), slice(None), slice(None))\n    assert expanded_indexer(key, ndim) == expected\n\n    key = (1, slice(None))\n    ndim = 3\n    expected = (1, slice(None), slice(None))\n    assert expanded_indexer(key, ndim) == expected\n\n    key = (Ellipsis, 1)\n    ndim = 3\n    expected = (slice(None), slice(None), 1)\n    assert expanded_indexer(key, ndim) == expected\n\n    key = (1, Ellipsis, 2)\n    ndim = 4\n    expected = (1, slice(None), slice(None), 2)\n    assert expanded_indexer(key, ndim) == expected\n", "def test_posify_mask_indexer():\n    key = VectorizedIndexer(\n        (\n            np.array([0, 3, -1]),\n            np.array([2, 3, -1]),\n        )\n    )\n    shape = (5, 5)\n\n    posified_key = posify_mask_indexer(key)\n\n    expected_key = VectorizedIndexer(\n        (\n            np.array([0, 3, 0]),\n            np.array([2, 3, 0]),\n        )\n    )\n\n    assert isinstance(posified_key, VectorizedIndexer)\n    assert_array_equal(posified_key.tuple, expected_key.tuple)\n", "def test_safe_cast_to_index_with_nans():\n    array = np.array([1, 2, np.nan, 4], dtype=object)\n    expected = pd.Index(array)\n    actual = safe_cast_to_index(array)\n    assert_array_equal(expected, actual)\n    assert isinstance(actual, pd.Index)\n", "def test_create_mask():\n    shape = (3, 4)\n    indexer1 = VectorizedIndexer((np.array([0, 1]), np.array([2, 3])))\n    mask1 = create_mask(indexer1, shape)\n    expected_mask1 = np.array([[False, False, True, False], [False, False, False, True], [False, False, False, False]])\n    assert_array_equal(mask1, expected_mask1)\n\n    indexer2 = OuterIndexer((np.array([0, 1]), slice(None)))\n    mask2 = create_mask(indexer2, shape)\n    expected_mask2 = np.array([[True, True, True, True], [True, True, True, True], [False, False, False, False]])\n    assert_array_equal(mask2, expected_mask2)\n\n    indexer3 = BasicIndexer((1, slice(None)))\n    mask3 = create_mask(indexer3, shape)\n    expected_mask3 = np.array([[False, False, False, False], [True, True, True, True], [False, False, False, False]])\n    assert_array_equal(mask3, expected_mask3)\n"], "sample_627": ["def test_concat_dataarray_with_empty() -> None:\n    da1 = DataArray([1, 2], dims=\"x\")\n    da2 = DataArray([], dims=\"x\")\n\n    expected = DataArray([1, 2], dims=\"x\")\n    actual = concat([da1, da2], dim=\"x\")\n    assert_identical(actual, expected)\n\n    expected = DataArray([1, 2], dims=\"x\")\n    actual = concat([da2, da1], dim=\"x\")\n    assert_identical(actual, expected)\n", "def test_concat_invalid_dim() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [3, 4])})\n\n    with pytest.raises(ValueError, match=r\"dimension 'y' not present in all datasets\"):\n        concat([ds1, ds2], dim=\"y\")\n", "def test_concat_positions() -> None:\n    da1 = DataArray([0, 1], dims=\"x\", coords={\"x\": [0, 1]})\n    da2 = DataArray([2, 3], dims=\"x\", coords={\"x\": [2, 3]})\n\n    actual = concat([da1, da2], dim=\"x\", positions=[[0, 1], [2, 4]])\n    expected = DataArray([0, 1, np.nan, 2, 3], dims=\"x\", coords={\"x\": [0, 1, 2, 3, 4]})\n    assert_identical(actual, expected)\n", "def test_concat_coords_kwarg_with_dim_coordinate() -> None:\n    data = Dataset(\n        {\n            \"foo\": ([\"x\", \"y\"], np.random.random((2, 3))),\n            \"bar\": ([\"x\", \"y\"], np.random.random((2, 3))),\n        },\n        {\"x\": [0, 1]},\n    )\n    foo = data[\"foo\"]\n    bar = data[\"bar\"]\n\n    # from dataset array:\n    expected = DataArray(\n        np.array([foo.values, bar.values]),\n        dims=[\"w\", \"x\", \"y\"],\n        coords={\"x\": [0, 1], \"w\": [0, 1]},\n    )\n    actual = concat([foo, bar], \"w\", coords=\"all\")\n    assert_equal(expected, actual)\n\n    # with an index as the 'dim' argument\n    stacked = concat([foo, bar], pd.Index(data[\"x\"], name=\"x\"), coords=\"all\")\n    assert_identical(stacked, actual)\n", "def test_concat_invalid_combine_attrs() -> None:\n    ds1 = Dataset({\"foo\": 1}, {\"bar\": 2})\n    ds2 = Dataset({\"foo\": 3}, {\"bar\": 4})\n\n    # test invalid combine_attrs argument\n    with pytest.raises(ValueError, match=r\"combine_attrs must be one of\"):\n        concat([ds1, ds2], dim=\"foo\", combine_attrs=\"invalid\")\n"], "sample_628": ["def test_words_with_digits_in_docstring(self):\n    stmt = astroid.extract_node(\n        'class ComentAbc(object):\\n   \"\"\"This is a test123 with a bad coment\"\"\"\\n   pass'\n    )\n    with self.assertAddsMessages(\n        Message(\n            \"wrong-spelling-in-docstring\",\n            line=2,\n            args=(\n                \"coment\",\n                \"This is a test123 with a bad coment\",\n                \"                         ^^^^^^\",\n                self._get_msg_suggestions(\"coment\"),\n            ),\n        )\n    ):\n        self.checker.visit_classdef(stmt)\n", "def test_store_unknown_words(self):\n    self.checker.config.spelling_store_unknown_words = True\n    self.checker.open()\n    self.checker.process_tokens(_tokenize_str(\"# bad coment\"))\n    self.checker.close()\n    with open(self.checker.config.spelling_private_dict_file, \"r\") as f:\n        stored_words = [word.strip() for word in f.readlines()]\n    assert \"coment\" in stored_words\n", "def test_spelling_store_unknown_words(self):\n    with tempfile.NamedTemporaryFile(mode=\"w\") as tmp_file:\n        tmp_file.write(\"correct\\n\")\n        tmp_file.flush()\n        set_config(spelling_private_dict_file=tmp_file.name)\n        self.checker.open()\n        stmt = astroid.extract_node(\n            'class ComentAbc(object):\\n   \"\"\"Check teh dummy comment\"\"\"\\n   pass'\n        )\n        with self.assertAddsMessages():\n            self.checker.visit_classdef(stmt)\n        with open(tmp_file.name) as file:\n            lines = file.readlines()\n            assert len(lines) == 2\n            assert lines[1] == \"teh\\n\"\n", "def test_words_separated_by_multiple_spaces(self):\n    stmt = astroid.extract_node(\n        'class ComentAbc(object):\\n   \"\"\"This is  comment  Abcz with a bad comment\"\"\"\\n   pass'\n    )\n    with self.assertAddsMessages(\n        Message(\n            \"wrong-spelling-in-docstring\",\n            line=2,\n            args=(\n                \"Abcz\",\n                \"This is  comment  Abcz with a bad comment\",\n                \"                   ^^^^\",\n                self._get_msg_suggestions(\"Abcz\"),\n            ),\n        )\n    ):\n        self.checker.visit_classdef(stmt)\n", "def test_words_with_apostrophes(self):\n    stmt = astroid.extract_node(\n        '''def f():\n    \"\"\"\n    it's a test\n    \"\"\"'''\n    )\n    with self.assertAddsMessages():\n        self.checker.visit_functiondef(stmt)\n\n    stmt = astroid.extract_node(\n        '''def f():\n    \"\"\"\n    'its a test'\n    \"\"\"'''\n    )\n    with self.assertAddsMessages(\n        Message(\n            \"wrong-spelling-in-docstring\",\n            line=3,\n            args=(\n                \"its\",\n                \"'its a test'\",\n                \"    ^^^\",\n                self._get_msg_suggestions(\"its\"),\n            ),\n        )\n    ):\n        self.checker.visit_functiondef(stmt)\n"], "sample_629": ["def test_expand_modules_file():\n    files_or_modules = [\"test.py\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert len(result) == 1\n    assert result[0][\"path\"] == str(Path(\"test.py\").resolve())\n    assert result[0][\"name\"] == \"test\"\n    assert len(errors) == 0\n", "def test_expand_modules_file_ignored_by_basename():\n    ignore_list = [\"__init__.py\"]\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, _ = expand_modules([\"tests/__init__.py\"], ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert not result\n", "def test_expand_modules_file():\n    files_or_modules = [\"tests/test_expand_modules.py\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert len(result) == 1\n    assert result[0][\"path\"] == str(Path(\"tests/test_expand_modules.py\").resolve())\n    assert result[0][\"name\"] == \"tests.test_expand_modules\"\n    assert not errors\n", "def test_expand_modules_file():\n    files_or_modules = [\"path/to/file.py\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    assert len(result) == 1\n    assert result[0][\"path\"] == \"path/to/file.py\"\n    assert result[0][\"name\"] == \"file\"\n    assert len(errors) == 0\n", "def test_expand_modules_file():\n    files_or_modules = [\"path/to/file.py\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    assert len(result) == 1\n    assert result[0][\"path\"] == \"path/to/file.py\"\n    assert result[0][\"name\"] == \"file\"\n    assert not errors\n"], "sample_630": ["def test_get_annotation_label(func, expected):\n    \"\"\"Test get_annotation_label function\"\"\"\n    node = astroid.extract_node(func)\n    got = get_annotation_label(node.returns)\n    assert got == expected, f\"got {got} instead of {expected} for value {node}\"\n", "def test_get_title(assign, expected):\n    node = astroid.extract_node(assign)\n    writer = DotWriter(Config())\n    got = writer.get_title(node)\n    assert got == expected, f\"got {got} instead of {expected} for value {node}\"\n", "def test_diagram_writer(output_format, expected_writer_class):\n    config = Config()\n    config.output_format = output_format\n    writer = DiagramWriter(config, [])\n    if output_format == \"dot\":\n        assert isinstance(writer.set_printer(\"file_name\", \"basename\"), DotBackend)\n    else:\n        assert isinstance(writer.set_printer(\"file_name\", \"basename\"), VCGPrinter)\n", "def test_get_annotation_label(returns, expected):\n    got = get_annotation_label(returns)\n    assert got == expected, f\"got {got} instead of {expected} for value {returns}\"\n", "def test_get_annotation_label(returns, expected_label):\n    \"\"\"Test get_annotation_label function\"\"\"\n    got = get_annotation_label(returns)\n    assert got == expected_label, f\"got {got} instead of {expected_label}\"\n"], "sample_631": ["def test_nested_lambda_with_unused_variables(self):\n    \"\"\"Make sure variables from parent lambdas are properly marked as unused\"\"\"\n    node = astroid.parse(\n        \"\"\"\n    lambda x: lambda y: x + 1\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"unused-variable\", node=node.body[0].body[0].args.args[1], args=\"y\")\n    ):\n        self.walk(node)\n", "    def test_self_cls_assign(self):\n        \"\"\"Make sure assigning to self or cls in a class method is forbidden\"\"\"\n        node = astroid.parse(\n            \"\"\"\n        class MyClass:\n                self = None\n\n            @classmethod\n                cls = None\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"self-cls-assignment\", node=node.body[0].body[0], args=(\"self\",)),\n            Message(\n                \"self-cls-assignment\",\n                node=node.body[0].body[1].body[0],\n                args=(\"cls\",),\n            ),\n        ):\n            self.walk(node)\n", "def test_unpacking_non_sequence(self):\n    \"\"\"Make sure unpacking non-sequence raises a message\"\"\"\n    node = astroid.parse(\n        \"\"\"\n    a, b = 1\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"unpacking-non-sequence\", node=node)\n    ):\n        self.walk(node)\n", "    def test_self_cls_assign(self):\n        \"\"\"Make sure self/cls don't get assigned.\"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            class Test:\n                    self = 1\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"self-cls-assignment\", node=node, args=(\"self\",))\n        ):\n            self.walk(node)\n", "def test_loopvar_name(self):\n    \"\"\"Make sure loop variables are properly handled\"\"\"\n    node = astroid.parse(\n        \"\"\"\n        for i in range(10):\n            pass\n        print(i)\n        \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"undefined-loop-variable\", node=node.body[1], args=\"i\")\n    ):\n        self.walk(node)\n\n    node = astroid.parse(\n        \"\"\"\n        i = 42\n        for i in range(10):\n            pass\n        print(i)\n        \"\"\"\n    )\n    with self.assertNoMessages():\n        self.walk(node)\n\n    node = astroid.parse(\n        \"\"\"\n        for j in range(10):\n            i = 42\n        print(i)\n        \"\"\"\n    )\n    with self.assertAddsMessages(\n        Message(\"undefined-loop-variable\", node=node.body[1], args=\"i\")\n    ):\n        self.walk(node)\n"], "sample_632": ["def test_get_similarity_report():\n    sim = similar.Similar()\n    sim.linesets = [\n        similar.LineSet(\"file1\", [\"line1\", \"line2\", \"line3\"]),\n        similar.LineSet(\"file2\", [\"line1\", \"line2\", \"line3\"]),\n    ]\n    similarities = [(3, {(sim.linesets[0], 0), (sim.linesets[1], 0)})]\n    report = sim._get_similarity_report(similarities)\n    expected_report = (\n        \"\\n3 similar lines in 2 files\\n\"\n        \"==file1:0\\n\"\n        \"==file2:0\\n\"\n        \"   line1\\n\"\n        \"   line2\\n\"\n        \"   line3\\n\"\n        \"TOTAL lines=6 duplicates=3 percent=50.00\\n\"\n    )\n    assert report == expected_report\n", "def test_set_option():\n    linter = PyLinter()\n    checker = similar.SimilarChecker(linter)\n    checker.set_option(\"min-similarity-lines\", 10, None, None)\n    assert checker.min_lines == 10\n    checker.set_option(\"ignore-comments\", True, None, None)\n    assert checker.ignore_comments is True\n    checker.set_option(\"ignore-docstrings\", True, None, None)\n    assert checker.ignore_docstrings is True\n    checker.set_option(\"ignore-imports\", True, None, None)\n    assert checker.ignore_imports is True\n    checker.set_option(\"ignore-signatures\", True, None, None)\n    assert checker.ignore_signatures is True\n", "def test_get_similarity_report():\n    sim = similar.Similar(min_lines=4)\n    lineset1 = similar.LineSet(\"file1\", [\"line1\", \"line2\", \"line3\", \"line4\"])\n    lineset2 = similar.LineSet(\"file2\", [\"line1\", \"line2\", \"line3\", \"line4\"])\n    sim.linesets = [lineset1, lineset2]\n    similarities = [(4, {(lineset1, 0), (lineset2, 0)})]\n    report = sim._get_similarity_report(similarities)\n    assert (\n        report\n        == \"\"\"", "def test_get_similarity_report():\n    sim = similar.Similar(min_lines=4)\n    lineset1 = similar.LineSet(\"file1\", [\"line1\", \"line2\", \"line3\", \"line4\"])\n    lineset2 = similar.LineSet(\"file2\", [\"line1\", \"line2\", \"line3\", \"line4\"])\n    sim.linesets = [lineset1, lineset2]\n    similarities = [(4, {(lineset1, 0), (lineset2, 0)})]\n    report = sim._get_similarity_report(similarities)\n    expected_report = (\n        \"\\n4 similar lines in 2 files\\n\"\n        \"==file1:0\\n\"\n        \"==file2:0\\n\"\n        \"   line1\\n\"\n        \"   line2\\n\"\n        \"   line3\\n\"\n        \"   line4\\n\"\n        \"TOTAL lines=8 duplicates=4 percent=50.00\\n\"\n    )\n    assert report == expected_report\n", "def test_combine_mapreduce_data():\n    sim = similar.Similar()\n    linesets_collection = [[similar.LineSet(\"file1\", [\"line1\", \"line2\"]), similar.LineSet(\"file2\", [\"line3\", \"line4\"])]]\n    sim.combine_mapreduce_data(linesets_collection)\n    assert len(sim.linesets) == 2\n    assert sim.linesets[0].name == \"file1\"\n    assert sim.linesets[1].name == \"file2\"\n"], "sample_633": ["def test_remove_successives() -> None:\n    all_couples: similar.CplIndexToCplLines_T = {\n        similar.LineSetStartCouple(Index(11), Index(34)): similar.CplSuccessiveLinesLimits(\n            similar.SuccessiveLinesLimits(LineNumber(5), LineNumber(9)),\n            similar.SuccessiveLinesLimits(LineNumber(27), LineNumber(31)),\n            effective_cmn_lines_nb=4,\n        ),\n        similar.LineSetStartCouple(Index(23), Index(79)): similar.CplSuccessiveLinesLimits(\n            similar.SuccessiveLinesLimits(LineNumber(15), LineNumber(19)),\n            similar.SuccessiveLinesLimits(LineNumber(45), LineNumber(49)),\n            effective_cmn_lines_nb=4,\n        ),\n        similar.LineSetStartCouple(Index(12), Index(35)): similar.CplSuccessiveLinesLimits(\n            similar.SuccessiveLinesLimits(LineNumber(6), LineNumber(10)),\n            similar.SuccessiveLinesLimits(LineNumber(28), LineNumber(32)),\n            effective_cmn_lines_nb=4,\n        ),\n    }\n    similar.remove_successives(all_couples)\n    assert len(all_couples) == 2\n    assert (\n        all_couples[\n            similar.LineSetStartCouple(Index(11), Index(34))\n        ].first_file.end\n        == LineNumber(10)\n    )\n    assert (\n        all_couples[\n            similar.LineSetStartCouple(Index(11), Index(34))\n        ].second_file.end\n        == LineNumber(32)\n    )\n    assert (\n        all_couples[\n            similar.LineSetStartCouple(Index(11), Index(34))\n        ].effective_cmn_lines_nb\n        == 5\n    )\n", "def test_get_similarity_report() -> None:\n    \"\"\"Tests that the get_similarity_report method returns a string\"\"\"\n    sim = similar.Similar()\n    sim.linesets = [\n        similar.LineSet(\"file1\", [\"line1\", \"line2\", \"line3\"], True, True, True, True),\n        similar.LineSet(\"file2\", [\"line1\", \"line2\", \"line3\"], True, True, True, True),\n    ]\n    report = sim._get_similarity_report([(3, {(sim.linesets[0], 0, 3), (sim.linesets[1], 0, 3)})])\n    assert isinstance(report, str)\n    assert \"TOTAL lines=6 duplicates=3 percent=50.00\" in report\n", "def test_get_similarity_report() -> None:\n    \"\"\"Tests that the get_similarity_report function returns the expected report.\"\"\"\n    sim = similar.Similar(min_lines=5)\n    sim.linesets = [\n        similar.LineSet(\"file1.py\", [\"import os\", \"print('Hello World')\"]),\n        similar.LineSet(\"file2.py\", [\"import os\", \"print('Hello World')\"]),\n    ]\n    sims = sim._compute_sims()\n    report = sim._get_similarity_report(sims)\n    assert \"TOTAL lines=4 duplicates=2 percent=50.00\" in report\n    assert \"2 similar lines in 2 files\" in report\n    assert \"==file1.py:[0:2]\" in report\n    assert \"==file2.py:[0:2]\" in report\n", "def test_compute_sims() -> None:\n    sim = similar.Similar(min_lines=4)\n    sim.append_stream(\"file1\", [\"import os\\n\", \"print('Hello World')\\n\"] * 5)\n    sim.append_stream(\"file2\", [\"import os\\n\", \"print('Hello World')\\n\"] * 5)\n    result = sim._compute_sims()\n    assert len(result) == 1\n    num, couples = result[0]\n    assert num == 10\n    assert len(couples) == 1\n    lineset, start_line, end_line = next(iter(couples))\n    assert lineset.name == \"file1\"\n    assert start_line == 0\n    assert end_line == 10\n", "def test_remove_successives() -> None:\n    all_couples: similar.CplIndexToCplLines_T = {\n        similar.LineSetStartCouple(similar.Index(11), similar.Index(34)): similar.CplSuccessiveLinesLimits(\n            similar.SuccessiveLinesLimits(similar.LineNumber(5), similar.LineNumber(9)),\n            similar.SuccessiveLinesLimits(similar.LineNumber(27), similar.LineNumber(31)),\n            effective_cmn_lines_nb=4,\n        ),\n        similar.LineSetStartCouple(similar.Index(23), similar.Index(79)): similar.CplSuccessiveLinesLimits(\n            similar.SuccessiveLinesLimits(similar.LineNumber(15), similar.LineNumber(19)),\n            similar.SuccessiveLinesLimits(similar.LineNumber(45), similar.LineNumber(49)),\n            effective_cmn_lines_nb=4,\n        ),\n        similar.LineSetStartCouple(similar.Index(12), similar.Index(35)): similar.CplSuccessiveLinesLimits(\n            similar.SuccessiveLinesLimits(similar.LineNumber(6), similar.LineNumber(10)),\n            similar.SuccessiveLinesLimits(similar.LineNumber(28), similar.LineNumber(32)),\n            effective_cmn_lines_nb=4,\n        ),\n    }\n\n    expected_result: similar.CplIndexToCplLines_T = {\n        similar.LineSetStartCouple(similar.Index(11), similar.Index(34)): similar.CplSuccessiveLinesLimits(\n            similar.SuccessiveLinesLimits(similar.LineNumber(5), similar.LineNumber(10)),\n            similar.SuccessiveLinesLimits(similar.LineNumber(27), similar.LineNumber(32)),\n            effective_cmn_lines_nb=5,\n        ),\n        similar.LineSetStartCouple(similar.Index(23), similar.Index(79)): similar.CplSuccessiveLinesLimits(\n            similar.SuccessiveLinesLimits(similar.LineNumber(15), similar.LineNumber(19)),\n            similar.SuccessiveLinesLimits(similar.LineNumber(45), similar.LineNumber(49)),\n            effective_cmn_lines_nb=4,\n        ),\n    }\n\n    similar.remove_successives(all_couples)\n    assert all_couples == expected_result\n"], "sample_634": ["    def test_expand_modules_errors(self, files_or_modules, expected):\n        \"\"\"Test expand_modules handles non-existent modules correctly\"\"\"\n        ignore_list, ignore_list_re = [], []\n        _, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        for error in errors:\n            error[\"ex\"] = type(error[\"ex\"])  # compare only exception types\n        assert errors == expected\n", "    def test_expand_modules_with_ignore_list_re(self, files_or_modules, ignore_list_re, expected):\n        \"\"\"Test expand_modules with an ignore_list_re\"\"\"\n        ignore_list = []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "    def test_expand_modules_with_module_name(self, files_or_modules, expected):\n        \"\"\"Test expand_modules with a module name instead of a file path\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "    def test_expand_modules_with_ignore_list_re(\n        self, files_or_modules, ignore_list_re, expected", "    def test_expand_modules_with_ignore_list_re(\n        self, files_or_modules, ignore_list_re, expected"], "sample_635": ["def test_property_return_type_google(self) -> None:\n    \"\"\"Example of a property having return documentation in\n    a Google style docstring\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    class Foo(object):\n        @property\n            '''int: docstring ...\n\n            Raises:\n                RuntimeError: Always\n            '''\n            raise RuntimeError()\n            return 10\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_numpy_docstring_with_multiple_params_in_one_line(self) -> None:\n    \"\"\"Example of a function with multiple parameters in one line (Numpy style)\n\n    Everything is completely analogous to functions.\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''docstring foo\n\n        Parameters\n        ----------\n        x: int, y: str\n            bla x and y\n        '''\n        pass\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_ignores_raise_notimplementederror_google(self) -> None:\n    \"\"\"Example of an abstract\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    class Foo(object):\n            '''docstring ...\n\n            Args:\n                arg (int): An argument.\n            '''\n            raise NotImplementedError()\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_numpy_docstring_with_type_annotations(self) -> None:\n    \"\"\"Example of a function with correct NumPy style parameter documentation\n    and type annotations.\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        '''The docstring\n\n        Parameters\n        ----------\n        named_arg : int\n            Returned\n        typed_arg : bool\n            Other argument\n        untyped_arg :\n            Another argument without type annotation\n\n        Returns\n        -------\n        int\n            Maybe named_arg\n        '''\n        return named_arg\n    \"\"\"\n    )\n    with self.assertNoMessages():\n        self.checker.visit_functiondef(node)\n", "def test_useless_docs_ignored_argument_names_numpy_with_type(self) -> None:\n    \"\"\"Example of a method documenting the return type that an\n    implementation should return.\n    \"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n    class Foo(object):\n            '''docstring ...\n\n            Parameters\n            ----------\n            arg : int\n                An argument.\n\n            _ :\n                Another argument.\n\n            _ignored :\n                Ignored Argument\n            '''\n            pass\n    \"\"\"\n    )\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"useless-type-doc\", node=node, args=(\"_\",)),\n        MessageTest(msg_id=\"useless-param-doc\", node=node, args=(\"_, _ignored\",)),\n    ):\n        self.checker.visit_functiondef(node)\n"], "sample_636": ["def test_duplicate_code_ignore_imports(self) -> None:\n    \"\"\"Tests ignoring imports when checking for duplicate code.\"\"\"\n    path = join(DATA, \"ignore_imports\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-imports=y\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_with_non_ascii_characters(self) -> None:\n    \"\"\"Test similar lines with non-ASCII characters.\"\"\"\n    path = join(DATA, \"non_ascii_characters\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_with_min_similarity_lines(self) -> None:\n    \"\"\"Tests duplicate code detection with a custom min similarity lines.\"\"\"\n    path = join(DATA, \"duplicate_code\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--min-similarity-lines=3\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_ignore_signatures(self) -> None:\n    \"\"\"Tests ignoring function signatures in similar code.\"\"\"\n    path = join(DATA, \"ignore_signatures\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-signatures=y\"],\n        expected_output=expected_output,\n    )\n", "def test_duplicate_code_with_ignored_comments(self) -> None:\n    \"\"\"Tests duplicate code detection with ignored comments.\"\"\"\n    path = join(DATA, \"duplicate_code_with_ignored_comments\")\n    expected_output = \"Similar lines in 2 files\"\n    self._test_output(\n        [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-comments=y\"],\n        expected_output=expected_output,\n    )\n"], "sample_637": ["def test_fixme_pattern_with_custom_notes_rgx(self) -> None:\n    set_config(self.checker, notes_rgx=\"BUG\")\n    code = \"\"\"a = 1\n            # BUG: This is a bug\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"# BUG: This is a bug\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_issue_2321_should_trigger_with_custom_notes_rgx(self) -> None:\n    code = \"# MYNOTE this should trigger a fixme\"\n    set_config(notes_rgx=\"MYNOTE\")\n    with self.assertAddsMessages(\n        MessageTest(\n            msg_id=\"fixme\",\n            line=1,\n            args=\"MYNOTE this should trigger a fixme\",\n            col_offset=1,\n        )\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_fixme_pattern_with_notes_rgx(self) -> None:\n    set_config(self.checker, notes=[], notes_rgx=\"FIXME|TODO\")\n    code = \"\"\"a = 1\n            # FIXME message\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"FIXME message\", col_offset=17)\n    ):\n        self.checker.open()\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_fixme_pattern_with_notes_rgx(self) -> None:\n    code = \"\"\"a = 1\n            # BUG1234 message\n            \"\"\"\n    set_config(self.checker, notes=[], notes_rgx=\"BUG\\\\d+\")\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"BUG1234 message\", col_offset=17)\n    ):\n        self.checker.open()\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_disable_option_with_notes(self) -> None:\n    code = \"\"\"# pylint: disable=fixme\n                # FIXME\n                \"\"\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(code))\n"], "sample_638": ["def test_invalid_output_format(mock_writer, capsys):\n    \"\"\"Test that an invalid output format is handled correctly.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([\"-o\", \"invalid\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert (\n        \"Format invalid is not supported natively. Pyreverse will try to generate it using Graphviz...\"\n        in capsys.readouterr().out\n    )\n    # Check that pyreverse does not call DiagramWriter and exits with a non-zero code\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code != 0\n", "def test_unsupported_output_format(mock_writer, capsys):\n    \"\"\"Test that an unsupported output format raises the right error.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"unsupported_format\", TEST_DATA_DIR])\n    # Check that the right info message is shown to the user\n    assert (\n        f\"Format unsupported_format is not supported natively. Pyreverse will try to generate it using Graphviz...\"\n        in capsys.readouterr().out\n    )\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n", "def test_invalid_output_format(mock_writer, capsys):\n    \"\"\"Test that pyreverse handles an invalid output format.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([\"-o\", \"invalid\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert (\n        f\"Format invalid is not supported natively. Pyreverse will try to generate it using Graphviz...\"\n        in capsys.readouterr().out\n    )\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n", "def test_only_classnames_option(mock_writer):\n    \"\"\"Test that the 'only-classnames' option is correctly passed to DiagramWriter.\"\"\"\n    main.Run([\"-k\", \"-o\", \"dot\", TEST_DATA_DIR])\n    mock_writer.DiagramWriter.assert_called_once()\n    diagram_writer = mock_writer.DiagramWriter.return_value\n    assert diagram_writer.write.call_args[0][0].show_ancestors == False\n    assert diagram_writer.write.call_args[0][0].show_associated == False\n    assert diagram_writer.write.call_args[0][0].show_builtin == False\n    assert diagram_writer.write.call_args[0][0].module_names == None\n    assert diagram_writer.write.call_args[0][0].only_classnames == True\n", "def test_unsupported_output_format(mock_writer, capsys):\n    \"\"\"Test that an unsupported output format results in a SystemExit.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([\"-o\", \"unsupported\", TEST_DATA_DIR])\n    # Check that the right error message is shown to the user\n    assert (\n        \"Format unsupported is not supported natively. Pyreverse will try to generate it using Graphviz...\"\n        in capsys.readouterr().out\n    )\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert wrapped_sysexit.value.code == 0\n"], "sample_639": ["def test_base_checker_check_consistency() -> None:\n    class InconsistentChecker(BaseChecker):\n        name = \"inconsistent\"\n        msgs = {\n            \"W0001\": (\n                \"Inconsistent checker has an example.\",\n                \"inconsistent-checker-example\",\n                \"Used nowhere and serves no purpose.\",\n            ),\n            \"E0002\": (\n                \"Another message with inconsistent checker id.\",\n                \"another-message\",\n                \"This one will cause a consistency check failure.\",\n            ),\n        }\n\n    checker = InconsistentChecker()\n    try:\n        checker.check_consistency()\n        assert False, \"Expected InvalidMessageError to be raised.\"\n    except InvalidMessageError as e:\n        assert \"Inconsistent checker part in message id\" in str(e)\n\n    class ConsistentChecker(BaseChecker):\n        name = \"consistent\"\n        msgs = {\n            \"W0001\": (\n                \"Consistent checker has an example.\",\n                \"consistent-checker-example\",\n                \"Used nowhere and serves no purpose.\",\n            ),\n            \"W0002\": (\n                \"Another message with consistent checker id.\",\n                \"another-message\",\n                \"This one will not cause a consistency check failure.\",\n            ),\n        }\n\n    checker = ConsistentChecker()\n    try:\n        checker.check_consistency()\n    except InvalidMessageError:\n        assert False, \"Expected no exception to be raised.\"\n", "def test_base_checker_get_full_documentation() -> None:\n    basic = OtherBasicChecker()\n    expected_doc = \"\"\"\\", "def test_base_checker_messages() -> None:\n    checker = OtherBasicChecker()\n    messages = checker.messages\n    assert len(messages) == 1\n    message = messages[0]\n    assert message.msgid == \"W0001\"\n    assert message.symbol == \"basic-checker-example\"\n    assert message.msg == \"Basic checker has an example.\"\n    assert message.help == \"Used nowhere and serves no purpose.\"\n", "def test_base_checker_ordering() -> None:\n    basic = OtherBasicChecker()\n    less_basic = LessBasicChecker()\n    different = DifferentBasicChecker()\n\n    assert basic == less_basic\n    assert basic != different\n    assert basic < different\n    assert different > basic\n    assert hash(basic) == hash(less_basic)\n    assert hash(basic) != hash(different)\n", "def test_base_checker_get_full_documentation() -> None:\n    checker = OtherBasicChecker()\n    expected_doc = \"\"\"\\"], "sample_640": ["def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import overload\n\n    @overload\n        ...\n\n    @overload\n        ...\n    \"\"\"\n    )\n    assert isinstance(code, list) and len(code) == 2\n\n    assert utils.is_overload_stub(code[0])\n    assert utils.is_overload_stub(code[1])\n\n    code = astroid.extract_node(\n        \"\"\"\n        pass\n    \"\"\"\n    )\n    assert not utils.is_overload_stub(code)\n", "def test_is_call_of_name() -> None:\n    node = astroid.extract_node(\"foo()\")\n    assert utils.is_call_of_name(node, \"foo\") is True\n\n    node = astroid.extract_node(\"bar.foo()\")\n    assert utils.is_call_of_name(node, \"foo\") is False\n\n    node = astroid.extract_node(\"foo('arg')\")\n    assert utils.is_call_of_name(node, \"foo\") is True\n\n    node = astroid.extract_node(\"some_other_function()\")\n    assert utils.is_call_of_name(node, \"foo\") is False\n", "def test_is_node_in_type_annotation_context() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import TypeVar\n\n    T = TypeVar('T')\n\n        pass\n\n    y: int = 5  #@\n    \"\"\"\n    )\n    assert isinstance(code, list) and len(code) == 2\n\n    assert utils.is_node_in_type_annotation_context(code[0].args.annotations[0])\n    assert utils.is_node_in_type_annotation_context(code[1].annotation)\n", "def test_get_node_first_ancestor_of_type() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    assert isinstance(code, nodes.FunctionDef)\n    ancestor = utils.get_node_first_ancestor_of_type(code, nodes.FunctionDef)\n    assert ancestor is not None\n    assert ancestor.name == \"outer_function\"\n", "def test_is_subclass_of_self() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    class MyClass: #@\n        pass\n    \"\"\"\n    )\n    assert utils.is_subclass_of(code, code)\n"], "sample_641": ["def test_load_result_file_does_not_exist(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n", "def test_load_result_file_does_not_exist(path: str) -> None:\n    assert load_results(path) is None\n", "def test_load_result_file_does_not_exist(path: str) -> None:\n    assert load_results(path) is None\n", "def test_load_result_file_does_not_exist(path: str) -> None:\n    assert load_results(path) is None\n\n", "def test_load_result_non_existent_file(path: str) -> None:\n    assert load_results(path) is None\n"], "sample_642": ["def test_preprocess_options() -> None:\n    \"\"\"Test that options are pre-processed correctly.\"\"\"\n    run = Run([\"--init-hook\", \"import os\"])\n    with mock.patch.object(run, \"_rcfile\") as rcfile_mock:\n        _preprocess_options(run, [\"--init-hook\", \"import os\"])\n        assert rcfile_mock not in run._plugins\n\n    run = Run([\"--load-plugins\", \"foo,bar\"])\n    with mock.patch.object(run, \"_plugins\") as plugins_mock:\n        _preprocess_options(run, [\"--load-plugins\", \"foo,bar\"])\n        plugins_mock.extend.assert_called_once_with([\"foo\", \"bar\"])\n\n    run = Run([\"--verbose\"])\n    with mock.patch.object(run, \"verbose\") as verbose_mock:\n        _preprocess_options(run, [\"--verbose\"])\n        assert verbose_mock == True\n\n    run = Run([\"--enable-all-extensions\"])\n    with mock.patch.object(run, \"_plugins\") as plugins_mock:\n        _preprocess_options(run, [\"--enable-all-extensions\"])\n        assert len(plugins_mock) > 0\n\n    with pytest.raises(ArgumentPreprocessingError):\n        _preprocess_options(Run([]), [\"--init-hook\"])\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that pre-process options are correctly handled.\"\"\"\n    run = Run()\n    args = [\"--init-hook\", \"import os\", \"--rcfile\", \"path/to/rcfile\"]\n    processed_args = _preprocess_options(run, args)\n    assert len(processed_args) == 0\n    assert run._rcfile == \"path/to/rcfile\"\n\n    args = [\"--verbose\", \"--load-plugins\", \"plugin1, plugin2\"]\n    processed_args = _preprocess_options(run, args)\n    assert len(processed_args) == 0\n    assert run.verbose\n    assert run._plugins == [\"plugin1\", \"plugin2\"]\n\n    args = [\"--enable-all-extensions\"]\n    processed_args = _preprocess_options(run, args)\n    assert len(processed_args) == 0\n    assert any(plugin.startswith(\"pylint.extensions\") for plugin in run._plugins)\n\n    args = [\"--invalid-option\"]\n    processed_args = _preprocess_options(run, args)\n    assert len(processed_args) == 1\n    assert processed_args[0] == \"--invalid-option\"\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that pre-processable options are correctly executed.\"\"\"\n    run = Run([\"--init-hook\", \"import os; os.environ['PYLINTRC'] = 'test'\"])\n    processed_args = _preprocess_options(run, [\"--init-hook\", \"import os; os.environ['PYLINTRC'] = 'test'\"])\n    assert processed_args == []\n    assert os.environ[\"PYLINTRC\"] == \"test\"\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that pre-processable options are correctly handled.\"\"\"\n    run = Run([])\n    args = [\"--init-hook\", \"import os\", \"--rcfile\", \"path/to/rcfile\"]\n    processed_args = _preprocess_options(run, args)\n    assert processed_args == []\n    assert run._rcfile == \"path/to/rcfile\"\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that pre-processable options are correctly handled.\"\"\"\n    run = Run([])\n    args = [\"--init-hook\", \"import os\", \"--rcfile\", \"path/to/rcfile\", \"--load-plugins\", \"plugin1,plugin2\"]\n    processed_args = _preprocess_options(run, args)\n    assert processed_args == []\n    assert run._rcfile == \"path/to/rcfile\"\n    assert run._plugins == [\"plugin1\", \"plugin2\"]\n"], "sample_643": ["def test_colorized_text_reporter_deprecation() -> None:\n    \"\"\"Test the deprecation warning for ColorizedTextReporter's color_mapping parameter.\"\"\"\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        color_mapping = {\"I\": (\"green\", \"bold\")}\n        ColorizedTextReporter(color_mapping=color_mapping)\n\n    assert len(cm) == 1\n    assert isinstance(cm[0].message, DeprecationWarning)\n", "def test_colorized_text_reporter_output(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter = ColorizedTextReporter(output)\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\n        \"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4)\n    )\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert \"\\033[\" in out_lines[1]  # Check for ANSI escape code\n    assert \"Line too long (1/2)\" in out_lines[1]\n    assert \"Line too long (3/4)\" in out_lines[2]\n", "def test_colorized_text_reporter_color_mapping_deprecation() -> None:\n    \"\"\"TODO remove in 3.0.\"\"\"\n    color_mapping = {\n        \"I\": (\"green\", \"\"),\n        \"C\": (None, \"bold\"),\n        \"R\": (\"magenta\", \"bold italic\"),\n        \"W\": (\"magenta\", \"\"),\n        \"E\": (\"red\", \"bold\"),\n        \"F\": (\"red\", \"bold underline\"),\n        \"S\": (\"yellow\", \"inverse\"),  # S stands for module Separator\n    }\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        ColorizedTextReporter(color_mapping=color_mapping)\n\n    assert len(cm) == 1\n    assert isinstance(cm[0].message, DeprecationWarning)\n", "def test_colorized_text_reporter_deprecated_color_mapping() -> None:\n    \"\"\"Test that ColorizedTextReporter with deprecated color mapping raises a warning.\"\"\"\n    with warnings.catch_warnings(record=True) as w:\n        ColorizedTextReporter(color_mapping={\"C\": (\"red\", \"bold\")})\n        assert len(w) == 1\n        assert issubclass(w[0].category, DeprecationWarning)\n", "def test_colorized_text_reporter_message_style() -> None:\n    \"\"\"Test the ColorizedTextReporter's message style.\"\"\"\n    reporter = ColorizedTextReporter()\n    msg_style = reporter._get_decoration(\"I\")\n    assert msg_style.color == \"green\"\n    assert not msg_style.style\n\n    msg_style = reporter._get_decoration(\"C\")\n    assert msg_style.color is None\n    assert msg_style.style == (\"bold\",)\n\n    msg_style = reporter._get_decoration(\"R\")\n    assert msg_style.color == \"magenta\"\n    assert msg_style.style == (\"bold\", \"italic\")\n\n    msg_style = reporter._get_decoration(\"X\")  # Non-existent message type\n    assert msg_style.color is None\n    assert not msg_style.style\n"], "sample_644": ["def test_import_as_rename(self) -> None:\n    \"\"\"Tests that import aliases are correctly checked.\"\"\"\n    module = astroid.parse(\n        \"\"\"\n        from module import function as function\n        import package.submodule as package_submodule\n        from module import Class as Class\n        \"\"\"\n    )\n    messages = [\n        MessageTest(\n            msg_id=\"useless-import-alias\",\n            node=module.body[0],\n            line=2,\n            col_offset=0,\n            end_line=2,\n            end_col_offset=37,\n        ),\n        MessageTest(\n            msg_id=\"consider-using-from-import\",\n            node=module.body[1],\n            line=3,\n            col_offset=0,\n            end_line=3,\n            end_col_offset=41,\n            args=(\"package\", \"submodule\"),\n        ),\n    ]\n    with self.assertAddsMessages(*messages):\n        for node in module.body:\n            self.checker.visit_importfrom(node)\n", "def test_import_outside_toplevel(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel\", REGR_DATA)\n    function_def = module.body[0]\n    import_node = function_def.body[0]\n\n    msg = MessageTest(\n        msg_id=\"import-outside-toplevel\",\n        node=import_node,\n        args=\"os\",\n        confidence=UNDEFINED,\n        line=3,\n        col_offset=4,\n        end_line=3,\n        end_col_offset=14,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n", "def test_import_as_rename(self) -> None:\n    \"\"\"Test that import-as is flagged when the alias is the same as the original module.\"\"\"\n    module = astroid.parse(\n        \"\"\"\n        import concurrent.futures as futures\n        from concurrent import futures as futures\n        \"\"\"\n    )\n    messages = [\n        MessageTest(\n            msg_id=\"consider-using-from-import\",\n            node=module.body[0],\n            args=(\"concurrent\", \"futures\"),\n            confidence=UNDEFINED,\n            line=2,\n            col_offset=0,\n            end_line=2,\n            end_col_offset=36,\n        ),\n        MessageTest(\n            msg_id=\"useless-import-alias\",\n            node=module.body[1],\n            confidence=UNDEFINED,\n            line=3,\n            col_offset=0,\n            end_line=3,\n            end_col_offset=34,\n        ),\n    ]\n    with self.assertAddsMessages(*messages):\n        self.checker.visit_import(module.body[0])\n        self.checker.visit_importfrom(module.body[1])\n", "def test_useless_import_alias(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"useless_import_alias\", REGR_DATA)\n    import_from = module.body[0]\n\n    msg = MessageTest(\n        msg_id=\"useless-import-alias\",\n        node=import_from,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=29,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_importfrom(import_from)\n", "def test_check_import_as_rename(self) -> None:\n    module = astroid.MANAGER.ast_from_module_name(\"import_rename\", REGR_DATA)\n    import_node = module.body[0]\n    importfrom_node = module.body[1]\n\n    msg = MessageTest(\n        msg_id=\"useless-import-alias\",\n        node=import_node,\n        line=1,\n        col_offset=0,\n        end_line=1,\n        end_col_offset=28,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_import(import_node)\n\n    msg = MessageTest(\n        msg_id=\"consider-using-from-import\",\n        node=importfrom_node,\n        args=(\"package\", \"module\"),\n        line=2,\n        col_offset=0,\n        end_line=2,\n        end_col_offset=37,\n    )\n    with self.assertAddsMessages(msg):\n        self.checker.visit_importfrom(importfrom_node)\n"], "sample_645": ["def test_percent_style_multiline(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"Hello\\nWorld\")\n    assert \"Hello\" in caplog.text\n    assert \"World\" in caplog.text\n\n    logger.info(\"Hello\\nWorld\", extra={\"auto_indent\": True})\n    lines = caplog.text.splitlines()[-2:]\n    indent = len(lines[0]) - len(lines[0].lstrip())\n    assert lines[1].startswith(\" \" * indent)\n\n    logger.info(\"Hello\\nWorld\", extra={\"auto_indent\": 10})\n    lines = caplog.text.splitlines()[-2:]\n    assert lines[1].startswith(\" \" * 10)\n\n    logger.info(\"Hello\\nWorld\", extra={\"auto_indent\": \"on\"})\n    lines = caplog.text.splitlines()[-2:]\n    indent = len(lines[0]) - len(lines[0].lstrip())\n    assert lines[1].startswith(\" \" * indent)\n", "def test_log_auto_indent(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"This is a\\nmultiline log message.\")\n    assert \"  multiline\" in caplog.text\n\n    with caplog.at_level(logging.INFO):\n        logger.info(\"This is another\\nmultiline log message.\", extra={\"auto_indent\": True})\n    assert \"  multiline\" in caplog.text\n\n    with caplog.at_level(logging.INFO):\n        logger.info(\"This is another\\nmultiline log message.\", extra={\"auto_indent\": 4})\n    assert \"    multiline\" in caplog.text\n\n    with caplog.at_level(logging.INFO):\n        logger.info(\"This is another\\nmultiline log message.\", extra={\"auto_indent\": False})\n    assert \"\\nmultiline\" in caplog.text\n", "def test_percent_style_multiline_formatter(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"Hello\\nWorld\")\n    assert \"Hello\" in caplog.text\n    assert \"World\" in caplog.text\n\n    caplog.set_level(logging.INFO)\n    logger.info(\"Hello\\n  World\", extra={\"auto_indent\": True})\n    lines = caplog.text.splitlines()\n    assert \"Hello\" in lines[0]\n    assert \"  World\" in lines[1]\n\n    caplog.set_level(logging.INFO)\n    logger.info(\"Hello\\n  World\", extra={\"auto_indent\": 4})\n    lines = caplog.text.splitlines()\n    assert \"Hello\" in lines[0]\n    assert \"    World\" in lines[1]\n", "def test_percent_style_multiline_auto_indent(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\")\n    assert \"boo arg1\\n    baz arg2\" in caplog.text\n\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\", extra={\"auto_indent\": 4})\n    assert \"boo arg1\\n    baz arg2\" not in caplog.text\n    assert \"boo arg1\\n        baz arg2\" in caplog.text\n", "def test_log_cli_enabled(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger()\n            logger.info(\"INFO message will be shown\")\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=True\n        log_level=INFO\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*INFO message will be shown*\"])\n"], "sample_646": ["def test_unittest_skip_decorator_with_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @unittest.skip(\"my reason\")\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"-rs\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*SKIP*[1]*my reason*\",\n            \"*1 skipped*\",\n        ]\n    )\n", "def test_pytest_unittest_interoperability_with_fixtures(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        @pytest.fixture\n            return 42\n\n        class TestIt(unittest.TestCase):\n                assert some_fixture == 42\n\n            @pytest.mark.usefixtures(\"some_fixture\")\n                assert some_fixture == 42\n\n                pass\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=3)\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                for i in range(5):\n                    with self.subTest(i=i):\n                        self.assertEqual(i, i)\n\n                for i in range(5):\n                    with self.subTest(i=i):\n                        self.assertEqual(i, 0)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*MyTestCase.test_one PASSED*\",\n            \"*MyTestCase.test_two FAILED*\",\n            \"*AssertionError: 1 != 0*\",\n            \"*AssertionError: 2 != 0*\",\n            \"*AssertionError: 3 != 0*\",\n            \"*AssertionError: 4 != 0*\",\n        ]\n    )\n", "def test_is_skipped_decorator(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        @unittest.skip(\"skipped class\")\n        class MyTestCase(unittest.TestCase):\n                pass\n                pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(skipped=1)\n", "def test_unittest_expected_failure_for_passing_test_is_fail_with_reason(\n    pytester: Pytester,"], "sample_647": ["def test_unformatted_warning() -> None:\n    \"\"\"Test the UnformattedWarning class.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"Some {adjective} warning\"\n    )\n    formatted_warning = warning.format(adjective=\"terrible\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"Some terrible warning\"\n", "def test_warning_subclassing(warning_class: type) -> None:\n    \"\"\"Ensure that our custom warnings properly subclass built-in warnings.\"\"\"\n    assert issubclass(warning_class, UserWarning)\n    assert issubclass(warning_class, warning_types.PytestWarning)\n", "def test_unformatted_warning() -> None:\n    \"\"\"Test that UnformattedWarning formats the message correctly.\"\"\"\n    warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"This is a {adjective} warning\"\n    )\n    formatted_warning = warning.format(adjective=\"test\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"This is a test warning\"\n", "def test_unformatted_warning_formatting() -> None:\n    \"\"\"Test that UnformattedWarning.format properly formats the message.\"\"\"\n    category = warning_types.PytestWarning\n    template = \"Something went wrong with {thing}\"\n    warning = warning_types.UnformattedWarning(category, template)\n    formatted_warning = warning.format(thing=\"something\")\n    assert isinstance(formatted_warning, category)\n    assert str(formatted_warning) == template.format(thing=\"something\")\n", "def test_warning_types_messages(warning_class: type, expected_message: str) -> None:\n    \"\"\"Test the messages of some warnings.\"\"\"\n    if warning_class == warning_types.PytestExperimentalApiWarning:\n        warning = warning_class.simple(\"some_api\")\n    else:\n        warning = warning_class(\"some warning\")\n    assert str(warning) == expected_message\n"], "sample_648": ["def test_get_empty_parameterset_mark_reason(pytester: Pytester) -> None:\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    mark = get_empty_parameterset_mark(config, [\"a\", \"b\"], all)\n    assert mark.kwargs[\"reason\"] == \"got empty parameter set ['a', 'b'], function all at builtins.py:0\"\n", "def test_get_unpacked_marks_class_with_mro(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class BaseClass:\n            pytestmark = pytest.mark.base\n\n        class SubClass(BaseClass):\n            pytestmark = pytest.mark.sub\n\n            marks = pytest._mark.get_unpacked_marks(SubClass)\n            mark_names = [m.name for m in marks]\n            assert mark_names == [\"sub\", \"base\"]\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_normalize_mark_list() -> None:\n    from _pytest.mark import normalize_mark_list\n\n    mark1 = pytest.mark.foo()\n    mark2 = pytest.mark.bar()\n\n    # Test list of MarkDecorator objects\n    assert list(normalize_mark_list([mark1, mark2])) == [mark1.mark, mark2.mark]\n\n    # Test list of Mark objects\n    assert list(normalize_mark_list([mark1.mark, mark2.mark])) == [mark1.mark, mark2.mark]\n\n    # Test mixed list of Mark and MarkDecorator objects\n    assert list(normalize_mark_list([mark1, mark2.mark])) == [mark1.mark, mark2.mark]\n\n    # Test non-list input\n    with pytest.raises(TypeError):\n        list(normalize_mark_list(mark1))\n", "def test_get_empty_parameterset_mark_reason(pytester: Pytester) -> None:\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    mark = get_empty_parameterset_mark(config, [\"a\", \"b\"], lambda: None)\n    assert mark.kwargs[\"reason\"] == \"got empty parameter set ['a', 'b'], function <lambda> at ?:?\"\n", "def test_get_empty_parameterset_mark_with_reason(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        empty_parameter_set_mark = skip\n    \"\"\"\n    )\n    config = pytester.parseconfig()\n    from _pytest.mark import get_empty_parameterset_mark\n\n    mark = get_empty_parameterset_mark(config, [\"a\", \"b\"], lambda x: x)\n    assert mark.name == \"skip\"\n    assert mark.kwargs[\"reason\"].startswith(\"got empty parameter set ('a', 'b')\")\n"], "sample_649": ["def test_caplog_at_level_with_logger(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n            with caplog.at_level(logging.DEBUG, logger=__name__):\n                logger.debug(\"Visible text!\")\n                assert len(caplog.records) == 1\n                assert caplog.records[0].message == \"Visible text!\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.OK\n", "def test_log_auto_indent(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger\\\\nfrom multiple lines')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\", \"--log-auto-indent\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"*INFO    *text going to logger\",\n            \"          from multiple lines\",\n        ]\n    )\n", "def test_log_cli_format(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"log message from test_foo\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-cli-level=WARNING\", \"--log-cli-format=%(message)s\")\n    result.stdout.fnmatch_lines([\"*log message from test_foo\"])\n    assert \"WARNING\" not in result.stdout.str()\n", "def test_percent_multiline_style(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('line1\\\\n  line2')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\", \"--log-auto-indent\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"INFO    *test_percent_multiline_style.py*line1\",\n            \"  line2\",\n        ]\n    )\n", "def test_percent_multiline_formatter(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('line1\\\\nline2')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\", \"--log-format=%(message)s\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*- Captured log call -*\",\n            \"line1\",\n            \"line2\",\n        ]\n    )\n"], "sample_650": ["def test_log_format_with_auto_indent(pytester: Pytester) -> None:\n    \"\"\"Check that log messages with multiple lines are indented correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text\\\\nwith multiple lines')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(levelname)s; %(message)s\n        log_auto_indent=True\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"WARNING; text\",\n            \"         with multiple lines\"\n        ]\n    )\n", "def test_catch_logs_context_manager(pytester: Pytester) -> None:\n    \"\"\"Test the context manager for catch_logs.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            with catch_logs(level=logging.DEBUG) as log_handler:\n                logger.debug('debug message')\n                logger.info('info message')\n                assert len(log_handler.records) == 2\n            logger.warning('warning message')\n            assert len(log_handler.records) == 2\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_log_auto_indent_enabled(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('Line 1\\\\n  Line 2')\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-auto-indent\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\"WARNING  *test_log_auto_indent_enabled.py:6 Line 1\", \"  Line 2\"]\n    )\n", "def test_log_file_level_set_to_info_by_default(pytester: Pytester) -> None:\n    \"\"\"Check that log_file_level defaults to INFO.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('info text')\n            logger.warning('warning text')\n            logger.error('error text')\n            assert False\n        \"\"\"\n    )\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n    result = pytester.runpytest(f\"--log-file={log_file}\")\n    assert result.ret == 1\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"info text\" in contents\n        assert \"warning text\" in contents\n        assert \"error text\" in contents\n", "def test_colored_level_formatter_add_color_level(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            caplog.handler.formatter.add_color_level(logging.INFO, \"blue\")\n            logger.info('text going to logger from call')\n            assert '\\x1b[34m' in caplog.text  # blue color code\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=INFO\", \"--color=yes\")\n    assert result.ret == 0\n"], "sample_651": ["def test_warns_context_manager_with_multiple_expected_warnings(self) -> None:\n    with pytest.warns((UserWarning, DeprecationWarning)) as record:\n        warnings.warn(\"user warning\", UserWarning)\n        warnings.warn(\"some deprecation warning\", DeprecationWarning)\n\n    assert len(record) == 2\n    assert str(record[0].message) == \"user warning\"\n    assert str(record[1].message) == \"some deprecation warning\"\n", "def test_re_emit_non_match_multiple(self) -> None:\n    with pytest.warns(UserWarning, match=\"v2 warning\"):\n        with pytest.warns(UserWarning, match=\"v1 warning\"):\n            warnings.warn(\"v1 warning\", UserWarning)\n            warnings.warn(\"non-matching v2 warning\", UserWarning)\n            warnings.warn(\"another non-matching v3 warning\", UserWarning)\n", "def test_warnings_recorder_list_methods(recwarn: WarningsRecorder) -> None:\n    warnings.warn(\"hello\")\n    warnings.warn(\"world\")\n    assert len(recwarn.list) == 2\n    assert recwarn.pop() is not None\n    assert len(recwarn.list) == 1\n    recwarn.clear()\n    assert len(recwarn.list) == 0\n    with pytest.raises(AssertionError):\n        recwarn.pop()\n", "def test_warns_context_manager_with_match_regex_pattern(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n        import pytest\n\n            with pytest.warns(UserWarning, match=\".*regex.*\"):\n                warnings.warn(\"some regex warning\", UserWarning)\n\n            with pytest.raises(pytest.fail.Exception):\n                with pytest.warns(UserWarning, match=\".*regex.*\"):\n                    warnings.warn(\"non-matching warning\", UserWarning)\n\n            import re\n            regex = re.compile(\".*regex.*\")\n            with pytest.warns(UserWarning, match=regex):\n                warnings.warn(\"some regex warning\", UserWarning)\n\n            with pytest.raises(pytest.fail.Exception):\n                with pytest.warns(UserWarning, match=regex):\n                    warnings.warn(\"non-matching warning\", UserWarning)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*2 passed in*\"])\n", "def test_warningschecker_matches_method(self) -> None:\n    checker = WarningsChecker(UserWarning, match_expr=\"foo\")\n    with checker:\n        warnings.warn(\"foo\", UserWarning)\n        warnings.warn(\"bar\", UserWarning)\n\n    assert len(checker.list) == 2\n    assert checker.matches(checker.list[0])\n    assert not checker.matches(checker.list[1])\n"], "sample_652": ["def test_fixture_parametrize_mark(testdir):\n    \"\"\"Test that a fixture can be parametrized using the @pytest.mark.parametrize marker.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return request.param\n\n        @pytest.mark.parametrize('fix', ['value1', 'value2'], indirect=True)\n            assert fix in ['value1', 'value2']\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=2)\n", "def test_fixture_function_with_params_and_ids():\n    \"\"\"Check if a fixture function with params and ids works as expected\"\"\"\n\n    @pytest.fixture(params=[1, 2], ids=[\"one\", \"two\"])\n        return request.param\n\n        assert fix in [1, 2]\n", "def test_fixture_request_object_access_after_yield(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            request.addfinalizer(lambda: None)\n            yield\n            assert request is not None\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_fixture_function_with_yield(tmp_path):\n    \"\"\"Check if a fixture function with yield works as expected (#4545)\"\"\"\n    pytest_file = tmp_path / \"test_fixture_function_with_yield.py\"\n    pytest_file.write_text(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            yield 1\n\n            assert fix == 1\n        \"\"\"\n    )\n    result = pytest.run([str(pytest_file)])\n    result.assert_outcomes(passed=1)\n", "def test_fixture_parametrize_with_fixtures(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=[1, 2])\n            return request.param\n\n        @pytest.mark.parametrize(\"arg\", [3, 4])\n            assert fix in [1, 2]\n            assert arg in [3, 4]\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*4 passed*\"])\n"], "sample_653": ["def test_set_log_path(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            plugin.set_log_path(\"pytest.log\")\n            assert plugin.log_file_handler.baseFilename == \"pytest.log\"\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\", \"--log-file-level=INFO\")\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n", "def test_log_in_runtest_protocol(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"protocol\")\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"protocol\" in contents\n", "def test_log_file_path_traversal(testdir):\n    log_file = \"../pytest.log\"\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger().info(\"Normal message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n    assert os.path.isfile(os.path.join(testdir.tmpdir.dirname, \"pytest.log\"))\n    with open(os.path.join(testdir.tmpdir.dirname, \"pytest.log\"), encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"Normal message\" in contents\n", "def test_catchlog_records(testdir):\n    \"\"\"Test the LogCaptureFixture.records attribute\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info('info text')\n            logging.warning('warning text')\n            logging.error('error text')\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    records = result.getfixturevalue(\"caplog\").records\n    assert len(records) == 3\n    for record in records:\n        assert isinstance(record, logging.LogRecord)\n", "def test_colored_log_formatting(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"Warning message\")\n            logging.info(\"Info message\")\n            logging.debug(\"Debug message\")\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=DEBUG\", \"--color=yes\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- live log call --*\",\n            \"*WARNING*Warning message*\",\n            \"*INFO*Info message*\",\n            \"*DEBUG*Debug message*\",\n        ]\n    )\n"], "sample_654": ["def test_call_fixture_function_error():\n    \"\"\"Check if an error is raised if a fixture function is called directly (#4545)\"\"\"\n\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(pytest.fail.Exception):\n        fix()\n", "def test_fixture_function_called_directly_raises_error():\n    \"\"\"Check if an error is raised if a fixture function is called directly (#4545)\"\"\"\n    with pytest.raises(pytest.FixtureLookupError):\n        fix()\n", "def test_call_fixture_function_error():\n    \"\"\"Check if an error is raised if a fixture function is called directly (#4545)\"\"\"\n\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(pytest.FixtureLookupError):\n        fix()\n", "def test_fixture_parametrize_ids_function(self, testdir):\n    \"\"\"Test that parametrize ids can be a function (#3151).\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            return \"id-{}\".format(fixture_value)\n\n        @pytest.fixture(params=[1, 2], ids=ids_func)\n            return request.param\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--collect-only\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*<Function test_foo[id-1]>*\",\n            \"*<Function test_foo[id-2]>*\",\n        ]\n    )\n", "def test_call_fixture_function_error():\n    \"\"\"Check if an error is raised if a fixture function is called directly (#4545)\"\"\"\n\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(pytest.FixtureLookupError):\n        fix()\n"], "sample_655": ["def test_captured_output_with_long_lines(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            print('a' * 200)\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_long_line*\",\n            \"*assert 0*\",\n            \"*Captured stdout*\",\n            \"a\" * 200,\n        ]\n    )\n", "def test_readouterr_with_unencodable_chars(testdir):\n    p = testdir.makepyfile(\n        r\"\"\"\n            print('\\xff')\n            captured = capsys.readouterr()\n            assert captured.out == '\\ufffd\\n'\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.assert_outcomes(passed=1)\n", "def test_capture_fixture_with_disabled_capturing(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            with capsys.disabled():\n                print('this should not be captured')\n            print('this should be captured')\n            out, err = capsys.readouterr()\n            assert out == 'this should be captured\\\\n'\n            assert err == ''\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert \"this should not be captured\" in result.stdout.str()\n", "def test_capturing_with_nested_context_managers(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import contextlib\n        import sys\n\n        @contextlib.contextmanager\n            old_stdout = sys.stdout\n            try:\n                sys.stdout = sys.stderr\n                yield\n            finally:\n                sys.stdout = old_stdout\n\n            with capture_stdout_context():\n                print(\"nested context manager\")\n            captured = capsys.readouterr()\n            assert captured.out == \"\"\n            assert captured.err == \"nested context manager\\\\n\"\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n", "def test_capture_manager_is_globally_capturing():\n    capman = CaptureManager(\"fd\")\n    assert not capman.is_globally_capturing()\n    capman.start_global_capturing()\n    assert capman.is_globally_capturing()\n    capman.stop_global_capturing()\n    assert not capman.is_globally_capturing()\n"], "sample_656": ["def test_capturemanager_resume_suspend(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            print(\"hello\")\n            capsys.suspend()\n            sys.stdout.write(\"world\\\\n\")\n            capsys.resume()\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\\\\n\"\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_captured_outerr_unicode(tmpdir, capfd):\n    \"\"\"Test captured outerr with unicode characters.\"\"\"\n    tmpfile = tmpdir.join(\"tmpfile.txt\")\n    with open(str(tmpfile), \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\u00f6\")\n    with open(str(tmpfile), \"r\", encoding=\"utf-8\") as f:\n        print(f.read())\n    captured = capfd.readouterr()\n    assert captured.out == \"\\u00f6\\n\"\n", "def test_encodedfile_repr(tmpdir):\n    ef = capture.EncodedFile(open(str(tmpdir.join(\"example.txt\")), \"wb\"), \"utf-8\")\n    assert repr(ef) == \"<EncodedFile %s>\" % repr(ef.buffer)\n", "def test_capture_manager_is_globally_capturing(capman):\n    capman.start_global_capturing()\n    assert capman.is_globally_capturing() is True\n    capman.stop_global_capturing()\n    assert capman.is_globally_capturing() is False\n", "def test_capturing_binary_output_with_captured_temporary_file(testdir):\n    testdir.makepyfile(\n        r\"\"\"\n            with open(tmp_path / 'temp.txt', 'wb') as f:\n                f.write(b'\\xc3')\n\n            with open(tmp_path / 'temp.txt', 'rb') as f:\n                sys.stdout.buffer.write(f.read())\n\n            captured = capfd.readouterr()\n            assert captured.out == b'\\xc3'\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_657": ["def test_mark_decorator_with_invalid_name(testdir):\n    \"\"\"Test that a MarkDecorator with an invalid name raises an AttributeError.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark._invalid\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"AttributeError: Marker name must NOT start with underscore\"])\n", "def test_pytest_param_marks_keyword_only():\n    with pytest.raises(TypeError) as excinfo:\n        pytest.param(1, 2, marks=\"foo\")\n    msg, = excinfo.value.args\n    assert msg == \"got multiple values for argument 'marks'\"\n", "def test_alias_warning():\n    class Test:\n            self.mark = \"test\"\n\n    alias_name = alias(\"mark\", warning=\"This is a warning\")\n    with pytest.warns(UserWarning, match=\"This is a warning\"):\n        assert alias_name.fget(Test()) == \"test\"\n", "def test_mark_decorator_with_args():\n    mark = pytest.mark.foo(\"arg1\", \"arg2\")\n    assert mark.name == \"foo\"\n    assert mark.args == (\"arg1\", \"arg2\")\n    assert mark.kwargs == {}\n\n    mark = pytest.mark.foo(\"arg1\", \"arg2\", id=\"some_id\")\n    assert mark.name == \"foo\"\n    assert mark.args == (\"arg1\", \"arg2\")\n    assert mark.kwargs == {\"id\": \"some_id\"}\n", "def test_mark_decorator_with_single_callable_argument(testdir):\n    \"\"\"Test that a mark decorator with a single callable argument is applied correctly\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo\n            pass\n\n        @pytest.mark.foo(some_function)\n            pass\n        \"\"\"\n    )\n    items, rec = testdir.inline_genitems()\n    for item in items:\n        if item.name == \"test_something\":\n            assert \"foo\" in [mark.name for mark in item.iter_markers()]\n            break\n    else:\n        assert False, \"test_something not found\"\n"], "sample_658": ["def test_doctest_unwrap_mock_aware(testdir):\n    \"\"\"Test that mock-aware unwrap is used when collecting doctests (#5080)\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import unittest.mock\n\n        m = unittest.mock.Mock()\n        m.__wrapped__ = \"some value\"\n\n            '''\n            >>> m.__wrapped__\n            'some value'\n            '''\n\n            '''\n            >>> m.__wrapped__.__cause__\n            Traceback (most recent call last):\n            ...\n            AttributeError...\n            '''\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"* 1 passed *\"])\n", "def test_doctest_module_raises_import_error(self, testdir):\n    \"\"\"\n    Test that doctest module raises an import error when the module cannot be imported.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        raise ImportError(\"Cannot import this module\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"*ImportError: Cannot import this module*\"])\n", "def test_is_mocked(self):\n    \"\"\"Test that _is_mocked correctly identifies mock objects\"\"\"\n    import unittest.mock\n\n    mock_obj = unittest.mock.Mock()\n    assert _is_mocked(mock_obj)\n    assert not _is_mocked(object())\n", "def test_is_mocked(self):\n    class Example:\n            self.pytest_mock_example_attribute_that_shouldnt_exist = True\n\n    assert _is_mocked(Example()) is True\n\n    class NotMocked:\n        pass\n\n    assert _is_mocked(NotMocked()) is False\n", "def test_is_mocked(self):\n    \"\"\"Test that _is_mocked correctly identifies mock objects\"\"\"\n    import unittest.mock as mock\n\n    mock_obj = mock.Mock()\n    assert _is_mocked(mock_obj)\n\n    class NotMock:\n            raise AttributeError(\"Not a mock\")\n\n    not_mock_obj = NotMock()\n    assert not _is_mocked(not_mock_obj)\n"], "sample_659": ["def test_traceback_entry_getsource(self):\n    try:\n        int(\"asdf\")\n    except ValueError as e:\n        excinfo = pytest._code.ExceptionInfo.from_current()\n        entry = excinfo.traceback[0]\n        source = entry.getsource()\n        assert 'int(\"asdf\")' in str(source)\n", "def test_traceback_entry_repr():\n    \"\"\"Test that a TracebackEntry has a correct repr.\"\"\"\n    try:\n        raise ValueError(\"test\")\n    except ValueError:\n        exc_info = sys.exc_info()\n        tb_entry = TracebackEntry(exc_info[2])\n        assert isinstance(repr(tb_entry), str)\n        assert \"File\" in repr(tb_entry)\n        assert \"line\" in repr(tb_entry)\n", "def test_traceback_entry_getsource(self):\n    import _pytest._code\n\n    excinfo = pytest.raises(ZeroDivisionError, lambda: 1 / 0)\n    entry = excinfo.traceback[-1]\n    source = entry.getsource()\n    assert len(source) > 0\n    assert \"1 / 0\" in str(source)\n", "def test_repr_traceback_native(self):\n    excinfo = pytest.raises(ValueError, lambda: int(\"s\"))\n    repr_traceback_native = ReprTracebackNative(\n        traceback.format_exception(*excinfo.excinfo)\n    )\n    assert repr_traceback_native.style == \"native\"\n    assert isinstance(repr_traceback_native.reprentries[0], ReprEntryNative)\n    assert repr_traceback_native.extraline is None\n", "def test_traceback_entry_is_hidden(self):\n    \"\"\"Test that TracebackEntry.is_hidden() correctly identifies entries with __tracebackhide__.\"\"\"\n    import pytest\n\n        __tracebackhide__ = True\n        raise Exception()\n\n    try:\n        foo()\n    except Exception as exc:\n        exc_info = pytest._code.ExceptionInfo.from_current(exc)\n        assert exc_info.traceback[0].ishidden()\n"], "sample_660": ["def test_xunit1_family(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_family = xunit1\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(name=\"pytest\", errors=0, failures=0, skipped=0, tests=1)\n    tnode = node.find_first_by_tag(\"testcase\")\n    tnode.assert_attr(classname=\"test_xunit1_family\", name=\"test_func\")\n    assert \"file\" in tnode.attrib\n    assert \"line\" in tnode.attrib\n", "def test_log_passing_tests_disabled(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            import sys\n            print('hello-stdout')\n            sys.stderr.write('hello-stderr\\\\n')\n            pass\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests = False\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    assert len(tnode.find_by_tag(\"system-out\")) == 0\n    assert len(tnode.find_by_tag(\"system-err\")) == 0\n", "def test_mangle_test_address_with_brackets():\n    from _pytest.junitxml import mangle_test_address\n\n    address = \"tests/test_file.py::TestClass::test_method[1-2-3]\"\n    newnames = mangle_test_address(address)\n    assert newnames == [\"tests.test_file\", \"TestClass\", \"test_method[1-2-3]\"]\n", "def test_set_logging(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.info('info msg')\n            logging.warning('warning msg')\n            assert 0\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    systemout = tnode.find_first_by_tag(\"system-out\")\n    assert \"info msg\" not in systemout.toxml()\n    assert \"warning msg\" in systemout.toxml()\n", "def test_mangle_test_address_with_nested_dirs(testdir):\n    from _pytest.junitxml import mangle_test_address\n\n    address = \"::\".join([\"a/b/c/my.py\", \"Class\", \"()\", \"method\"])\n    newnames = mangle_test_address(address)\n    assert newnames == [\"a.b.c.my\", \"Class\", \"method\"]\n"], "sample_661": ["def test_add_stats(testdir):\n    path = testdir.tmpdir.join(\"test_add_stats.xml\")\n    log = LogXML(str(path), None)\n\n    class Report(BaseReport):\n        longrepr = \"FooBarBaz\"\n        sections = []\n        nodeid = \"something\"\n        location = \"tests/filename.py\", 42, \"TestClass.method\"\n\n    test_report = Report()\n\n    log.pytest_sessionstart()\n    node_reporter = log._opentestcase(test_report)\n    node_reporter.append_failure(test_report)\n    log.add_stats(\"failure\")\n    log.add_stats(\"skipped\")\n    log.pytest_sessionfinish()\n\n    dom = minidom.parse(str(path))\n    testsuite_node = dom.getElementsByTagName(\"testsuite\")[0]\n    assert testsuite_node.getAttribute(\"failures\") == \"1\"\n    assert testsuite_node.getAttribute(\"skipped\") == \"1\"\n", "def test_mangle_test_address_with_parameterized_tests():\n    from _pytest.junitxml import mangle_test_address\n\n    address = \"tests/test_module.py::TestClass::test_method[param1]\"\n    newnames = mangle_test_address(address)\n    assert newnames == [\"tests.test_module\", \"TestClass\", \"test_method[param1]\"]\n", "def test_log_passing_tests(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-o\", \"junit_log_passing_tests=False\")\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    testcase = node.find_first_by_tag(\"testcase\")\n    systemout = testcase.find_first_by_tag(\"system-out\")\n    assert systemout is None\n\n    result, dom = runandparse(testdir, \"-o\", \"junit_log_passing_tests=True\")\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    testcase = node.find_first_by_tag(\"testcase\")\n    systemout = testcase.find_first_by_tag(\"system-out\")\n    assert systemout is not None\n", "def test_log_passing_tests(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=False\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            print('hello-stdout')\n            import sys\n            sys.stderr.write('hello-stderr')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    pnode = node.find_first_by_tag(\"testcase\")\n    assert not pnode.find_first_by_tag(\"system-out\")\n    assert not pnode.find_first_by_tag(\"system-err\")\n", "def test_log_passing_tests(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            print('hello-stdout')\n            pytest.log('hello-log')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-o\", \"junit_log_passing_tests=True\")\n    node = dom.find_first_by_tag(\"testcase\")\n    systemout = node.find_first_by_tag(\"system-out\")\n    assert \"hello-stdout\" in systemout.toxml()\n    assert \"hello-log\" in systemout.toxml()\n\n    result, dom = runandparse(testdir, \"-o\", \"junit_log_passing_tests=False\")\n    node = dom.find_first_by_tag(\"testcase\")\n    systemout = node.find_first_by_tag(\"system-out\")\n    assert systemout is None\n"], "sample_662": ["def test_getslaveinfoline(self, testdir):\n    \"\"\"Test that getslaveinfoline returns the correct string.\"\"\"\n    node = testdir.spawn_pytest_node()\n    node.slaveinfo = {\n        \"id\": \"gw1\",\n        \"sysplatform\": \"linux\",\n        \"version_info\": (3, 8, 0),\n        \"executable\": \"/usr/bin/python3.8\",\n    }\n    expected = \"[gw1] linux -- Python 3.8.0 /usr/bin/python3.8\"\n    assert getslaveinfoline(node) == expected\n", "def test_report_head_line(self, testdir):\n    \"\"\"Test that the head_line property returns the correct domain.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    assert len(reports) == 3\n    report = reports[1]\n    assert report.head_line == \"test_a\"\n", "def test_base_report_properties(self, testdir):\n    \"\"\"Test that the properties of BaseReport are working correctly\"\"\"\n    reprec = testdir.inline_runsource(\n        \"\"\"\n        @pytest.mark.skipif(\"True\")\n    \"\"\"\n    )\n    reports = reprec.getreports(\"pytest_runtest_logreport\")\n    for rep in reports:\n        assert isinstance(rep.caplog, str)\n        assert isinstance(rep.capstdout, str)\n        assert isinstance(rep.capstderr, str)\n        assert isinstance(rep.fspath, str)\n        assert isinstance(rep.head_line, (str, type(None)))\n        assert isinstance(rep.count_towards_summary, bool)\n", "def test_base_report_properties(self, testdir):\n    \"\"\"Test properties of BaseReport.\"\"\"\n    report = TestReport(\n        \"nodeid\",\n        (\"fspath\", 1, \"domain\"),\n        {\"keyword\": \"value\"},\n        \"passed\",\n        None,\n        \"call\",\n        duration=0.5,\n    )\n\n    assert report.fspath == \"fspath\"\n    assert report.count_towards_summary\n    assert report.head_line == \"domain\"\n    assert report.caplog == \"\"\n    assert report.capstdout == \"\"\n    assert report.capstderr == \"\"\n\n    report.sections.append((\"Captured log\", \"log message\"))\n    report.sections.append((\"Captured stdout\", \"stdout message\"))\n    report.sections.append((\"Captured stderr\", \"stderr message\"))\n\n    assert report.caplog == \"log message\"\n    assert report.capstdout == \"stdout message\"\n    assert report.capstderr == \"stderr message\"\n\n    report.longrepr = \"longrepr\"\n    tw = py.io.TerminalWriter(stringio=True)\n    report.toterminal(tw)\n    assert tw.stringio.getvalue().strip() == \"longrepr\"\n", "def test_base_report_to_terminal(self, testdir):\n    \"\"\"\n    Test that BaseReport.to_terminal() method properly handles different types of longrepr.\n    \"\"\"\n    class DummyLongRepr:\n            out.line(\"DummyLongRepr\")\n\n    report = BaseReport(longrepr=DummyLongRepr())\n    tw = testdir.make_tw_mock()\n    report.toterminal(tw)\n    assert tw.lines == [\"DummyLongRepr\"]\n\n    report.longrepr = \"Simple string\"\n    report.toterminal(tw)\n    assert tw.lines == [\"DummyLongRepr\", \"Simple string\"]\n\n    report.longrepr = None\n    report.toterminal(tw)\n    assert tw.lines == [\"DummyLongRepr\", \"Simple string\"]  # no new lines added\n\n    report.longrepr = Exception(\"Test exception\")\n    report.toterminal(tw)\n    assert tw.lines[-1] == \"<unprintable longrepr>\"  # check the last line only\n"], "sample_663": ["def test_collect_pkg_init_and_submodule_in_args(testdir):\n    subdir = testdir.mkdir(\"sub\")\n    init = subdir.ensure(\"__init__.py\")\n    init.write(\"def test_init(): pass\")\n    submodule = subdir.mkdir(\"submodule\")\n    submodule.ensure(\"__init__.py\")\n    p = submodule.ensure(\"test_file.py\")\n    p.write(\"def test_file(): pass\")\n\n    result = testdir.runpytest(\"-v\", str(init), str(p))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub/__init__.py::test_init PASSED*\",\n            \"sub/submodule/test_file.py::test_file PASSED*\",\n            \"*2 passed in*\",\n        ]\n    )\n", "def test_collect_sub_with_broken_symlinks(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Create a broken symlink.\n    sub.join(\"test_broken.py\").mksymlinkto(\"test_doesnotexist.py\")\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines([\"sub/test_file.py::test_file PASSED*\"])\n    result.stdout.no_fnmatch_line(\"*test_broken.py*\")\n", "def test_collect_package_by_full_path(testdir):\n    sub = testdir.mkdir(\"sub\")\n    init = sub.ensure(\"__init__.py\")\n    init.write(\"def test_init(): pass\")\n    p = sub.ensure(\"test_file.py\")\n    p.write(\"def test_file(): pass\")\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub/__init__.py::test_init PASSED*\",\n            \"sub/test_file.py::test_file PASSED*\",\n            \"*2 passed in*\",\n        ]\n    )\n\n    # Run pytest from the package directory itself.\n    sub.chdir()\n    result = testdir.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"__init__.py::test_init PASSED*\",\n            \"test_file.py::test_file PASSED*\",\n            \"*2 passed in*\",\n        ]\n    )\n", "def test_collect_error_with_module_level_fixture(testdir):\n    \"\"\"Verify that collection error with module level fixture doesn't cause internal error (#5088).\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture(scope=\"module\", autouse=True)\n            assert 0, \"collection error\"\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*ERROR collecting*\", \"*1 error in*\"])\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_collect_ignore_symlink_loop(testdir):\n    \"\"\"Test that a symlink pointing to its parent directory does not cause an infinite recursion.\"\"\"\n    sub = testdir.mkdir(\"sub\")\n    sub.join(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Create a symlink that points to the parent directory.\n    sub.join(\"symlink_to_parent\").mksymlinkto(sub.dirpath())\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub/test_file.py::test_file PASSED*\",\n            \"*1 passed in*\",\n        ]\n    )\n"], "sample_664": ["def test_fixture_positional_arguments_warning():\n    \"\"\"Check that passing arguments to pytest.fixture() as positional arguments raises a warning\"\"\"\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        pytest.fixture(autouse=True)\n", "def test_deprecated_funcargnames_attribute(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 42\n\n            assert fixturenames == [\"my_fixture\"]\n            assert funcargnames == fixturenames\n            with pytest.warns(pytest.PytestDeprecationWarning):\n                print(funcargnames)\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.*\"]\n    )\n", "def test_fixture_positional_arguments_warns():\n    \"\"\"Check that passing arguments to pytest.fixture() as positional arguments warns\"\"\"\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        pytest.fixture(autouse=True)\n", "def test_fixture_positional_arguments_warning():\n    \"\"\"Check that passing arguments to pytest.fixture() as positional arguments raises a warning.\"\"\"\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        pytest.fixture(autouse=True)\n", "def test_funcargnames_attr(pytestconfig):\n    \"\"\"Check that the `funcargnames` attribute is deprecated in favor of `fixturenames`\"\"\"\n    with pytest.warns(deprecated.FUNCARGNAMES):\n        pytestconfig.pluginmanager.get_plugin(\"funcmanage\").funcargnames\n"], "sample_665": ["def test_collect_with_broken_import(testdir):\n    \"\"\"Test that pytest can collect tests when a broken import is present in the test file\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import this_is_a_broken_import\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--collect-only\")\n    result.stdout.fnmatch_lines([\"*ERROR collecting*\", \"*No module named *this_is_a_broken_import*\"])\n", "def test_collector_repr_failure(testdir):\n    p = testdir.makepyfile(\"assert 0\")\n    result = testdir.runpytest(p, \"--collect-only\")\n    assert result.ret == ExitCode.INTERRUPTED\n    result.stdout.fnmatch_lines(\n        [\n            \"*_ ERROR collecting test_collector_repr_failure.py _*\",\n            \"test_collector_repr_failure.py:1: in <module>\",\n            \"    assert 0\",\n            \"E   AssertionError: assert 0\",\n            \"*! Interrupted: 1 errors during collection !*\",\n            \"*= 1 error in *\",\n        ]\n    )\n", "def test_pytest_make_parametrize_id(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.hookimpl\n            if val == \"custom\":\n                return \"custom-id\"\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [\"custom\", \"not-custom\"])\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"--collect-only\")\n    result.stdout.fnmatch_lines([\"*test_foo[custom-id]*\", \"*test_foo[not-custom]*\"])\n", "def test_pytest_collect_file_hook_called_for_packages(testdir):\n    \"\"\"Verify pytest_collect_file is called for packages (#5418).\"\"\"\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n            if path.basename == \"example\":\n                return MyFile(path, parent)\n\n        class MyFile(pytest.File):\n                return [Item(\"hello\", parent=self)]\n\n        class Item(pytest.Item):\n                pass\n    \"\"\"\n    )\n    example = testdir.mkdir(\"example\")\n    example.ensure(\"__init__.py\")\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n", "def test_collect_package_init_with_broken_import(testdir):\n    pkg = testdir.mkpydir(\"pkg\")\n    pkg.join(\"__init__.py\").write(\"from . import doesnotexist\")\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n    result.stdout.fnmatch_lines([\"*no tests ran in*\"])\n"], "sample_666": ["def test_capture_manager_is_restored_after_test_item(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            print(\"in func1\")\n            assert 0\n\n            print(\"in func2\")\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    s = result.stdout.str()\n    assert \"in func1\" in s\n    assert \"in func2\" in s\n    assert \" Captured stdout call \" in s\n", "def test_logcapturefixture_messages(caplog):\n    \"\"\"Test that LogCaptureFixture.messages returns a list of format-interpolated log messages.\"\"\"\n    logging.warning(\"hello %s\", \"world\")\n    assert caplog.messages == [\"hello world\"]\n", "def test_colored_level_formatter():\n    formatter = ColoredLevelFormatter(create_terminal_writer(None), \"%(levelname)s\", \"%H:%M:%S\")\n    record = logging.LogRecord(\"name\", logging.INFO, \"pathname\", 1, \"message\", None, None)\n    formatted_record = formatter.format(record)\n    assert \"\\x1b[\" in formatted_record  # ANSI escape sequence for color\n", "def test_captured_log_messages(capsys, caplog):\n    \"\"\"Verify captured log messages are not duplicated in the stdout capture.\"\"\"\n    print(\"printed to stdout\")\n    logging.warning(\"warning message\")\n    out, err = capsys.readouterr()\n    assert \"printed to stdout\" in out\n    assert \"warning message\" not in out\n    assert \"warning message\" in caplog.text\n", "def test_catch_log_handler_set_level():\n    log = logging.getLogger(\"test_catch_log_handler\")\n    log.setLevel(logging.DEBUG)\n    handler = LogCaptureHandler()\n    handler.set_level(logging.INFO, logger=\"test_catch_log_handler\")\n    assert log.level == logging.DEBUG\n    assert handler.level == logging.INFO\n"], "sample_667": ["def test_getbasetemp_custom_removes_old(tmp_path, monkeypatch):\n    from _pytest.tmpdir import TempPathFactory\n\n    mytemp = tmp_path / \"custom_basetemp\"\n    mytemp.mkdir()\n\n    # create a file in the custom basetemp directory\n    (mytemp / \"hello\").touch()\n\n    config = FakeConfig(mytemp)\n    t = TempPathFactory.from_config(config)\n\n    # ensure getbasetemp returns the custom basetemp directory\n    assert t.getbasetemp() == mytemp\n\n    # recreate TempPathFactory instance to simulate pytest re-run\n    t = TempPathFactory.from_config(config)\n\n    # ensure the old file is removed\n    assert not (mytemp / \"hello\").exists()\n", "def test_tmpdir_factory_cleanup_lock(tmp_path_factory):\n    \"\"\"Test that the tmpdir_factory properly cleans up the lock file.\"\"\"\n    from _pytest.pathlib import create_cleanup_lock\n\n    lock_file = create_cleanup_lock(tmp_path_factory.getbasetemp())\n    assert lock_file.is_file()\n\n    # Simulate a test run\n    tmp_path_factory.mktemp(\"test\")\n\n    # Check that the lock file is still present\n    assert lock_file.is_file()\n\n    # Clean up the tmpdir_factory\n    tmp_path_factory._ensure_relative_to_basetemp(\"\")\n    tmp_path_factory.getbasetemp().rmdir()\n\n    # Check that the lock file is gone\n    assert not lock_file.is_file()\n", "def test_getbasetemp_absolute_path(tmp_path, monkeypatch):\n    \"\"\"Test that getbasetemp returns an absolute path\"\"\"\n    from _pytest.tmpdir import TempPathFactory\n\n    config = FakeConfig(tmp_path)\n    t = TempPathFactory.from_config(config)\n    basetemp = t.getbasetemp()\n    assert basetemp.is_absolute()\n\n    # Test with a relative path\n    monkeypatch.chdir(tmp_path)\n    config = FakeConfig(\"relative-path\")\n    t = TempPathFactory.from_config(config)\n    basetemp = t.getbasetemp()\n    assert basetemp.is_absolute()\n", "def test_tmp_path_factory_cleanup(tmp_path_factory, tmp_path):\n    \"\"\"Test that tmp_path_factory cleans up the created temporary directory.\"\"\"\n    p = tmp_path_factory.mktemp(\"test\")\n    assert p.exists()\n    tmp_path_factory._basetemp = None\n    tmp_path_factory.getbasetemp()\n    assert not p.exists()\n", "def test_tmp_path_factory_cleanup(tmp_path_factory, tmp_path):\n    # Create a temporary directory using the factory\n    temp_dir = tmp_path_factory.mktemp(\"my_temp_dir\")\n\n    # Create some files and directories inside the temporary directory\n    (temp_dir / \"file1.txt\").touch()\n    (temp_dir / \"dir1\").mkdir()\n    (temp_dir / \"dir1\" / \"file2.txt\").touch()\n\n    # Check that the temporary directory and its contents exist\n    assert temp_dir.exists()\n    assert (temp_dir / \"file1.txt\").exists()\n    assert (temp_dir / \"dir1\").exists()\n    assert (temp_dir / \"dir1\" / \"file2.txt\").exists()\n\n    # Clean up the temporary directory using the factory\n    tmp_path_factory._basetemp = None  # Force the factory to re-create the basetemp directory\n    new_temp_dir = tmp_path_factory.getbasetemp()\n\n    # Check that the original temporary directory and its contents have been removed\n    assert not temp_dir.exists()\n    assert not (temp_dir / \"file1.txt\").exists()\n    assert not (temp_dir / \"dir1\").exists()\n    assert not (temp_dir / \"dir1\" / \"file2.txt\").exists()\n\n    # Check that the new basetemp directory is different from the original one\n    assert new_temp_dir != temp_dir\n"], "sample_668": ["def test_funcargnames_attribute_deprecation():\n    with pytest.warns(pytest.PytestDeprecationWarning) as w:\n        deprecated.FUNCARGNAMES\n    assert \"The `funcargnames` attribute was an alias for `fixturenames`\" in str(w[0].message)\n", "def test_fixture_positional_arguments_warning():\n    with pytest.warns(pytest.PytestDeprecationWarning) as w:\n        pytest.fixture(\"scope\")  # Pass argument as positional instead of keyword\n    assert len(w) == 1\n    assert \"Passing arguments to pytest.fixture() as positional arguments is deprecated\" in str(w[0].message)\n", "def test_fixture_positional_arguments_warning():\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture\\\\(\\\\) as positional arguments is deprecated\",\n    ) as w:\n        pytest.fixture(autouse=True)\n    assert w[0].lineno == inspect.currentframe().f_lineno - 1\n    assert w[0].filename == __file__\n", "def test_fixture_positional_arguments_warning():\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture\\\\(\\\\) as positional arguments is deprecated - pass them as a keyword argument instead.\",\n    ):\n        pytest.fixture(\"function\")\n", "def test_fixture_positional_arguments_warning():\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.\",\n    ):\n        pytest.fixture(autouse=True)\n"], "sample_669": ["def test_capturing_readouterr_after_done(capsys):\n    print(\"hello\")\n    capsys.readouterr()\n    capsys.done()\n    with pytest.raises(ValueError, match=\"Capture is not started\"):\n        capsys.readouterr()\n", "def test_encodedfile_flush(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.write(\"Hello\")\n    ef.flush()\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"Hello\"\n    tmpfile.close()\n    with pytest.raises(ValueError):\n        ef.flush()\n", "def test_capturing_system_exit(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\\\n        import sys\n\n            print(\"hello\")\n            sys.exit(42)\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*SystemExit: 42*\", \"*1 failed in*\"])\n    assert \"hello\" not in result.stdout.str()\n", "def test_captured_file_mode(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert ef.mode == \"w\"\n    ef.close()\n    tmpfile.close()\n\n    with open(tmpfile.name, \"rb\") as f:\n        ef = capture.EncodedFile(f, encoding=\"utf-8\")\n    assert ef.mode == \"r\"\n    ef.close()\n", "def test_capturing_disabled_fixture(capsys):\n    \"\"\"Test that the capsys fixture can be disabled and re-enabled within a test.\"\"\"\n    print(\"before disable\")\n    with capsys.disabled():\n        print(\"while disabled\")\n    print(\"after disable\")\n    captured = capsys.readouterr()\n    assert captured.out == \"before disable\\nafter disable\\n\"\n    assert captured.err == \"\"\n"], "sample_670": ["def test_matcher_errors(expr: str, matcher: Callable[[str], bool]) -> None:\n    with pytest.raises(ZeroDivisionError):\n        evaluate(expr, matcher)\n", "def test_matcher(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_matcher_independence(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_matcher(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_syntax_errors_with_parentheses(expr: str, column: int, message: str) -> None:\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(expr, lambda ident: True)\n    assert excinfo.value.column == column\n    assert excinfo.value.message == message\n"], "sample_671": ["def test_xfail_strict_with_raises(testdir):\n    \"\"\"Test that xfail(strict=True) with raises works as expected\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(strict=True, raises=TypeError)\n            raise TypeError(\"expected\")\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n", "def test_xfail_strict_with_multiple_markers(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason='first', strict=True)\n        @pytest.mark.xfail(reason='second')\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS*test_func*\", \"*first*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\", \"*1 failed*\"])\n", "def test_xfail_strict_from_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_strict_with_multiple_markers(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(reason='first', strict=True)\n        @pytest.mark.xfail(reason='second', strict=False)\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*first*\", \"*1 failed*\"])\n"], "sample_672": ["def test_saferepr_recursive():\n    \"\"\"Test saferepr() with recursive objects.\"\"\"\n\n    class Recursive:\n            self.name = name\n            self.children = []\n\n            return \"<%s>\" % (self.name)\n\n    obj1 = Recursive(\"obj1\")\n    obj2 = Recursive(\"obj2\")\n    obj1.children.append(obj2)\n    obj2.children.append(obj1)\n\n    assert \"Recursion\" not in saferepr(obj1)\n    assert \"Recursion\" not in saferepr(obj2)\n", "def test_saferepr_with_recursive_object():\n    class RecursiveObject:\n            self.name = name\n            self.children = []\n\n            return f\"RecursiveObject({self.name})\"\n\n            self.children.append(child)\n\n    obj1 = RecursiveObject(\"obj1\")\n    obj2 = RecursiveObject(\"obj2\")\n    obj3 = RecursiveObject(\"obj3\")\n\n    obj1.add_child(obj2)\n    obj2.add_child(obj3)\n    obj3.add_child(obj1)  # create a cycle\n\n    saferepr_output = saferepr(obj1)\n    assert \"RecursiveObject\" in saferepr_output\n    assert \"...\" in saferepr_output  # check for ellipsis\n", "def test_saferepr_recursive():\n    class Recursive:\n            return saferepr(self)\n\n    assert \"RecursionError\" in saferepr(Recursive())\n", "def test_saferepr_recursive():\n    \"\"\"Test saferepr() with recursive objects.\"\"\"\n    class Recursive:\n            self.name = name\n            self.children = []\n\n            return f\"Recursive({self.name})\"\n\n    r1 = Recursive(\"r1\")\n    r2 = Recursive(\"r2\")\n    r3 = Recursive(\"r3\")\n\n    r1.children.append(r2)\n    r2.children.append(r3)\n    r3.children.append(r1)\n\n    assert \"RecursionError\" in saferepr(r1)\n", "def test_saferepr_recursive():\n    class Recursive:\n            return saferepr(self)\n\n    assert \"RecursionError\" in saferepr(Recursive())\n"], "sample_673": ["def test_doctest_continue_on_failure_with_pdb(testdir):\n    testdir.maketxtfile(\n        test_something=\"\"\"\n        >>> i = 5\n        >>> def foo():\n        ...     raise ValueError('error1')\n        >>> foo()\n        >>> i\n        >>> i + 2\n        7\n        >>> i + 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\", \"--pdb\")\n    result.stdout.fnmatch_lines(\n        [\"*4: UnexpectedException*\", \"*5: DocTestFailure*\", \"*8: DocTestFailure*\"]\n    )\n    result.stdout.fnmatch_lines([\"*entering PDB*\"])\n", "def test_doctest_namespace_fixture_autouse_false(testdir):\n    \"\"\"Test that doctest_namespace fixture is not executed when autouse=False.\"\"\"\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=False)\n            raise Exception(\"This should not be executed\")\n    \"\"\"\n    )\n    testdir.maketxtfile(\n        test_doc=\"\"\"\n        >>> 1 + 1\n        2\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"* 1 passed *\"])\n", "def test_doctest_continue_on_failure_with_pdb(self, testdir):\n    testdir.maketxtfile(\n        test_something=\"\"\"\n        >>> i = 5\n        >>> def foo():\n        ...     raise ValueError('error1')\n        >>> foo()\n        >>> i\n        >>> i + 2\n        7\n        >>> i + 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--doctest-modules\", \"--doctest-continue-on-failure\", \"--pdb\")\n    result.stdout.fnmatch_lines([\"*4: UnexpectedException*\", \"*5: DocTestFailure*\"])\n    result.stdout.no_fnmatch_line(\"*8: DocTestFailure*\")\n", "def test_is_setup_py_empty_file(tmpdir):\n    setup_py = tmpdir.join(\"setup.py\")\n    setup_py.write(\"\")\n    assert not _is_setup_py(setup_py)\n", "def test_doctest_namespace_fixture_autouse_false(testdir):\n    \"\"\"\n    Check that inserting something into the namespace does not work when\n    autouse is False.\n    \"\"\"\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=False)\n            doctest_namespace['cl'] = 42\n    \"\"\"\n    )\n    p = testdir.maketxtfile(\n        \"\"\"\n        >>> print(cl)\n        Traceback (most recent call last):\n        NameError: name 'cl' is not defined\n    \"\"\"\n    )\n    reprec = testdir.inline_run(p)\n    reprec.assertoutcome(passed=1)\n"], "sample_674": ["def test_item_location():\n    class DummySession:\n            return fspath\n\n    parent = nodes.Node(\"parent\", nodeid=\"parent\")\n    parent.session = DummySession()\n    item = nodes.Item(\"item\", parent=parent)\n    item.fspath = py.path.local(\"/path/to/item\")\n\n    location = item.location\n    assert location == (py.path.local(\"/path/to/item\"), None, \"\")\n", "def test_node_keywords():\n    node = nodes.Node.from_parent(None, name=\"test_node\")\n    assert isinstance(node.keywords, nodes.NodeKeywords)\n    assert not node.keywords\n\n    node.add_marker(\"slow\")\n    assert \"slow\" in node.keywords\n", "def test_node_repr():\n    node = nodes.Node.from_parent(None, name=\"test_node\")\n    assert repr(node) == \"<Node test_node>\"\n", "def test_get_fslocation_from_item():\n    class DummyItem:\n            self.fspath = fspath\n\n    item = DummyItem(py.path.local(\"/path/to/item\"))\n    assert nodes.get_fslocation_from_item(item) == (\"/path/to/item\", -1)\n\n    class DummyItemWithLocation:\n            self.location = location\n\n    item = DummyItemWithLocation((\"/path/to/item\", 10))\n    assert nodes.get_fslocation_from_item(item) == (\"/path/to/item\", 10)\n", "def test_node_keywords():\n    node = nodes.Node.from_parent(None, name=\"test_node\")\n    assert isinstance(node.keywords, nodes.NodeKeywords)\n    assert not node.keywords\n\n    node.keywords[\"my_keyword\"] = \"value\"\n    assert \"my_keyword\" in node.keywords\n    assert node.keywords[\"my_keyword\"] == \"value\"\n"], "sample_675": ["def test_percent_multiline_style(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger\\\\nfrom multiple lines')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-format=%(message)s\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"text going to logger\",\n            \"from multiple lines\",\n        ]\n    )\n", "def test_catch_log_handler_get_records(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger from call')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"*- Captured log call -*\", \"*text going to logger from call*\"])\n\n    # Test get_records method\n    plugin = result.retconfig.pluginmanager.getplugin(\"logging-plugin\")\n    records = plugin.log_handler.get_records(\"call\")\n    assert len(records) == 1\n    record = records[0]\n    assert record.getMessage() == \"text going to logger from call\"\n    assert record.levelname == \"INFO\"\n", "def test_percent_style_multiline(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('Line 1\\\\nLine 2')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-format=%(message)s\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"Line 1\",\n            \"Line 2\",\n        ]\n    )\n", "def test_log_auto_indent(testdir):\n    \"\"\"\n    Test that the log-auto-indent option works correctly.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('message from test_foo\\\\nwith multiple lines')\n            assert True\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=INFO\", \"--log-auto-indent\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- live log call --*\",\n            \"INFO    *message from test_foo\",\n            \"        with multiple lines\",\n        ]\n    )\n", "def test_log_file_format(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format = %(asctime)s %(levelname)s %(message)s\n        \"\"\".format(\n            log_file\n        )\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info('info message')\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert re.match(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3} INFO info message\", contents)\n"], "sample_676": ["def test_get_line_with_reprcrash_message_width_truncation(monkeypatch):\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n\n    mocked_pos = \"some::nodeid\"\n\n        return mocked_pos\n\n    monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n    class config(object):\n        pass\n\n    class rep(object):\n            return mocked_verbose_word\n\n        longrepr = type(\"\", (), {\"reprcrash\": type(\"\", (), {\"message\": \"x\" * 200})})\n\n    termwidth = 80\n    result = _get_line_with_reprcrash_message(config, rep, termwidth)\n    assert wcswidth(result) <= termwidth\n", "def test_line_with_reprcrash_message_truncated(monkeypatch):\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n\n    mocked_pos = \"some::nodeid\"\n\n        return mocked_pos\n\n    monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n    class config(object):\n        option = object()\n        option.verbosity = 0\n        option.showcapture = \"all\"\n\n    class rep(object):\n            return mocked_verbose_word\n\n        longrepr = object()\n        longrepr.reprcrash = object()\n        longrepr.reprcrash.message = \"a\" * 100\n        nodeid = \"some_node_id\"\n\n    term_width = 50\n    line = _get_line_with_reprcrash_message(config, rep, term_width)\n    assert len(line) <= term_width\n    assert line.startswith(mocked_verbose_word + \" \" + mocked_pos)\n    assert \"...\" in line\n", "def test_line_with_reprcrash_width_handling(monkeypatch):\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n\n    mocked_pos = \"some::nodeid\"\n\n        return mocked_pos\n\n    monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n    class config(object):\n        pass\n\n    class rep(object):\n            return mocked_verbose_word\n\n        @property\n            class longrepr:\n                reprcrash = type(\"ReprCrash\", (), {\"message\": \"very long message \" * 10})\n\n            return longrepr()\n\n    termwidth = 80\n    result = _get_line_with_reprcrash_message(config, rep, termwidth)\n    assert len(result) <= termwidth + 2  # the +2 accounts for potential ansi escape codes\n", "def test_get_line_with_reprcrash_message_longrepr_missing(self, testdir):\n    \"\"\"Test that get_line_with_reprcrash_message handles missing longrepr.\"\"\"\n    config = testdir.parseconfig()\n    report = testdir.getitem(\"def test_func(): pass\")._report\n    del report.longrepr\n    result = _get_line_with_reprcrash_message(config, report, 80)\n    assert result.startswith(\"FAILED some::nodeid - \")\n", "def test_line_with_reprcrash_message_width(monkeypatch):\n    import _pytest.terminal\n    from wcwidth import wcswidth\n\n    mocked_verbose_word = \"FAILED\"\n\n    mocked_pos = \"some::nodeid\"\n\n        return mocked_pos\n\n    monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n    class config(object):\n        pass\n\n    class rep(object):\n            return mocked_verbose_word\n\n    termwidth = 80\n    line = _get_line_with_reprcrash_message(config, rep, termwidth)\n    assert wcswidth(line) <= termwidth\n    assert mocked_verbose_word in line\n    assert mocked_pos in line\n"], "sample_677": ["def test_invalid_idents() -> None:\n    invalid_idents = (\n        \"(\",\n        \")\",\n        \" or \",\n        \" and \",\n        \" not \",\n        '\"',\n        \"'\",\n        \"=\",\n        \"<\",\n        \">\",\n        \",\",\n        \";\",\n        \":\",\n        \"\\\\\",\n        \"/\",\n        \"|\",\n        \"*\",\n        \"^\",\n        \"%\",\n        \"$\",\n        \"#\",\n        \"~\",\n        \"`\",\n    )\n    for ident in invalid_idents:\n        with pytest.raises(ParseError):\n            evaluate(ident, lambda ident: True)\n", "def test_precedence(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_reserved_words(ident: str) -> None:\n    with pytest.raises(ParseError):\n        evaluate(ident, {ident: True}.__getitem__)\n", "def test_matcher(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n", "def test_invalid_idents(ident: str) -> None:\n    with pytest.raises(ParseError):\n        evaluate(ident, {ident: True}.__getitem__)\n"], "sample_678": ["def test_ensure_deletable(tmp_path):\n    \"\"\"Ensure that ensure_deletable works correctly.\"\"\"\n    path = tmp_path / \"temp\"\n    path.mkdir()\n\n    # Create a lock file to prevent deletion\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n\n    # Try to delete the path, it should fail because of the lock\n    assert not ensure_deletable(path, 0)\n\n    # Try to delete the path with an old lock, it should succeed\n    lock_path.utime((0, 0))  # Set the lock file's modification time to 0\n    assert ensure_deletable(path, 1)\n\n    # Try to delete the path without a lock, it should succeed\n    lock_path.unlink()\n    assert ensure_deletable(path, 0)\n", "def test_ensure_deletable(tmp_path):\n    \"\"\"Test that ensure_deletable returns True for a directory without a lock file.\"\"\"\n    path = tmp_path / \"temp\"\n    path.mkdir()\n    assert ensure_deletable(path, 0) is True\n\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n    assert ensure_deletable(path, 0) is False\n\n    lock_path.write_text(\"pid\")\n    assert ensure_deletable(path, 0) is False\n\n    lock_path.unlink()\n    assert ensure_deletable(path, 0) is True\n", "def test_ensure_deletable(tmp_path):\n    \"\"\"Test that ensure_deletable works as expected.\"\"\"\n    path = tmp_path / \"test\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    with open(lock_path, \"w\") as f:\n        f.write(\"locked\")\n    assert not ensure_deletable(path, 0)\n    os.utime(lock_path, (0, 0))  # Set atime and mtime to 0\n    assert ensure_deletable(path, 1)\n", "def test_ensure_deletable(tmp_path):\n    \"\"\"Test that ensure_deletable returns True when the path is deletable and False otherwise.\"\"\"\n    path = tmp_path / \"temp-1\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    assert ensure_deletable(path, 0) == True\n    with open(lock_path, \"w\") as f:\n        f.write(\"locked\")\n    assert ensure_deletable(path, 0) == False\n    lock_path.unlink()\n    assert ensure_deletable(path, 0) == True\n", "def test_ensure_deletable(tmp_path):\n    \"\"\"Test that ensure_deletable returns True when there is no lock file and False when there is a lock file.\"\"\"\n    path = tmp_path / \"temp\"\n    path.mkdir()\n    assert ensure_deletable(path, 0) == True\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n    assert ensure_deletable(path, 0) == False\n"], "sample_679": ["def test_mark_evaluator_istrue() -> None:\n    class MockItem:\n            self.keywords = []\n            self.own_markers = []\n\n            return self.own_markers\n\n    item = MockItem()\n    mark_evaluator = MarkEvaluator(item, \"foo\")\n\n    # Test with no marks\n    assert not mark_evaluator.istrue()\n\n    # Test with a mark but no condition\n    mark = Mark(name=\"foo\")\n    item.own_markers.append(mark)\n    assert mark_evaluator.istrue()\n\n    # Test with a mark and a condition\n    mark.kwargs[\"condition\"] = \"False\"\n    assert not mark_evaluator.istrue()\n\n    # Test with multiple marks and conditions\n    mark2 = Mark(name=\"foo\", kwargs={\"condition\": \"True\"})\n    item.own_markers.append(mark2)\n    assert mark_evaluator.istrue()\n", "def test_MarkEvaluator_getexplanation():\n    item = mock.Mock()\n    item.nodeid = \"test_node\"\n    mark_evaluator = MarkEvaluator(item, \"skip\")\n    mark_evaluator._mark = Mark(\"skip\", (\"reason\",), {\"reason\": \"some reason\"})\n    assert mark_evaluator.getexplanation() == \"some reason\"\n\n    mark_evaluator._mark = Mark(\"skip\", (\"condition\",), {\"condition\": \"some condition\"})\n    assert mark_evaluator.getexplanation() == \"condition: some condition\"\n\n    mark_evaluator._mark = None\n    assert mark_evaluator.getexplanation() == \"\"\n", "def test_mark_evaluator_invalidraise(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"obj_type\", [\"template\", \"disk\"])\n            print(\"obj_type is:\", obj_type)\n            raise Exception(\"Test exception\")\n    \"\"\"\n    )\n\n    item = testdir.getitem(\"test_custom_mark_parametrized\")\n    mark_evaluator = MarkEvaluator(item, \"parametrize\")\n\n    assert not mark_evaluator.invalidraise(Exception(\"Test exception\"))\n    assert mark_evaluator.invalidraise(ValueError(\"Test exception\"))\n", "def test_mark_evaluator_with_multiple_marks(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo\n        @pytest.mark.bar\n            pass\n    \"\"\"\n    )\n    item = testdir.getitem(\"test_func\")\n    evaluator = MarkEvaluator(item, \"foo\")\n    assert evaluator._get_marks() == [Mark(name=\"foo\", args=(), kwargs={})]\n    assert evaluator.istrue()\n    evaluator = MarkEvaluator(item, \"bar\")\n    assert evaluator._get_marks() == [Mark(name=\"bar\", args=(), kwargs={})]\n    assert evaluator.istrue()\n", "def test_mark_evaluator_invalidraise():\n    item = mock.Mock(spec=Item)\n    mark_eval = MarkEvaluator(item, \"test_mark\")\n    mark_eval._mark = Mark(name=\"test_mark\", args=(), kwargs={\"raises\": ValueError})\n    assert mark_eval.invalidraise(ZeroDivisionError()) is None\n    assert mark_eval.invalidraise(ValueError()) is False\n"], "sample_680": ["def test_xfail_strict_in_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_from_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_ini_setting(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_module_level(testdir):\n    \"\"\"\n    Verify that using pytest.xfail(strict=True) at module level causes a collection error\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.xfail(\"xfail_module_level\", strict=True)\n\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*Using pytest.xfail outside of a test is not allowed*\"]\n    )\n", "def test_xfail_with_multiple_reasons(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason=\"reason1\")\n        @pytest.mark.xfail(reason=\"reason2\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_func*\", \"*reason1*reason2*\"])\n"], "sample_681": ["def test_log_file_format(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format = %(asctime)s %(levelname)s %(message)s\n        log_file_date_format = %Y-%m-%d %H:%M:%S\n        \"\"\".format(\n            log_file\n        )\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"test_log_file_format.py PASSED\"])\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert re.match(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} INFO This log message will be shown\", contents)\n", "def test_log_file_format(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level=INFO\n        log_file_format=%(asctime)s %(levelname)s %(message)s\n        \"\"\".format(\n            log_file\n        )\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.getLogger().info(\"Normal message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert re.match(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3} INFO    Normal message\", contents)\n", "def test_percent_style_multiline(testdir):\n    \"\"\"Test that PercentStyleMultiline formats multiline messages correctly.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('text going to logger\\\\nfrom multiple lines')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-format=%(message)s\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"text going to logger\",\n            \"from multiple lines\",\n        ]\n    )\n", "def test_get_option_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_format = %(asctime)s %(levelname)s %(message)s\n        \"\"\"\n    )\n    config = testdir.parseconfig()\n    assert get_option_ini(config, \"log_format\") == \"%(asctime)s %(levelname)s %(message)s\"\n", "def test_log_auto_indent_on(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.info('This is a\\\\n  multiline message')\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-auto-indent\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\n            \"*-- Captured log call --*\",\n            \"INFO    *:test_log_auto_indent_on.py:3 This is a\",\n            \"  multiline message\",\n        ]\n    )\n"], "sample_682": ["def test_xfail_strict_from_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_importorskip_with_minversion():\n    with pytest.raises(\n        pytest.skip.Exception,\n        match=\"^module 'sys' has no attribute '__version__'$\",\n    ):\n        pytest.importorskip(\"sys\", minversion=\"1.0\")\n", "def test_xfail_strict_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = true\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_strict_true_from_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\", \"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_from_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\", \"*1 failed*\"])\n"], "sample_683": ["def test_encodedfile_mode(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert ef.mode == \"wb\"\n    tmpfile.close()\n    with pytest.raises(ValueError):\n        ef.read()\n", "def test_capture_manager_is_globally_capturing():\n    capman = CaptureManager(\"fd\")\n    assert capman.is_globally_capturing()\n    capman = CaptureManager(\"no\")\n    assert not capman.is_globally_capturing()\n", "def test_capture_fixture_with_keyboard_interrupt(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            print(\"hello\")\n            raise KeyboardInterrupt()\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n    assert result.ret == 2\n    result.stdout.fnmatch_lines([\"*KeyboardInterrupt*\"])\n    result.stdout.fnmatch_lines([\"*hello*\"])\n", "def test_capturing_error_recursive_with_suspend_resume(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            print(\"cap1\")\n            with capsys.disabled():\n                print(\"not captured\")\n            print(\"cap2\")\n            capsys.suspend()\n            print(\"not captured\")\n            capsys.resume()\n            print(\"cap3\")\n            out, err = capsys.readouterr()\n            assert out == \"cap1\\\\ncap2\\\\ncap3\\\\n\"\n            assert err == \"\"\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_capturing_with_keyboard_interrupt(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n            raise KeyboardInterrupt()\n            print('should not be printed')\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines(['*KeyboardInterrupt*'])\n    assert 'should not be printed' not in result.stdout.str()\n"], "sample_684": ["def test_traceback_recursion_index() -> None:\n        recursive_function()\n\n    try:\n        recursive_function()\n    except RecursionError:\n        exci = ExceptionInfo.from_current()\n\n    assert exci.traceback.recursionindex() is not None\n", "    def test_repr_traceback(self, tw_mock) -> None:\n        try:\n            x = 1 / 0\n        except ZeroDivisionError:\n            excinfo = ExceptionInfo.from_current()\n        reprinfo = ReprExceptionInfo(reprtraceback=excinfo.getrepr(style=\"short\"), reprcrash=excinfo._getreprcrash())\n        reprinfo.toterminal(tw_mock)\n        assert len(tw_mock.lines) > 1\n        assert \"def test_repr_traceback\" in tw_mock.lines[0]\n", "    def test_repr_chained_exceptions(self, tw_mock) -> None:\n        try:\n            try:\n                raise ValueError(\"inner\")\n            except ValueError as e:\n                raise RuntimeError(\"outer\") from e\n        except RuntimeError:\n            exci = ExceptionInfo.from_current()\n\n        repr_chain = exci.getrepr(style=\"long\", chain=True)\n        assert isinstance(repr_chain, ExceptionChainRepr)\n\n        repr_chain.toterminal(tw_mock)\n        assert len(tw_mock.lines) > 5\n        assert \"ValueError: inner\" in \"\".join(tw_mock.lines)\n        assert \"RuntimeError: outer\" in \"\".join(tw_mock.lines)\n", "def test_traceback_entry_is_hidden(self) -> None:\n        __tracebackhide__ = True\n        raise Exception()\n\n    try:\n        foo()\n    except Exception:\n        exci = ExceptionInfo.from_current()\n\n    entry = exci.traceback[0]\n    assert entry.ishidden()\n", "def test_traceback_entry_is_hidden():\n        __tracebackhide__ = True\n        raise Exception\n\n    try:\n        f()\n    except Exception:\n        exci = ExceptionInfo.from_current()\n\n    assert exci.traceback[0].ishidden()\n"], "sample_685": ["def test_percent_style_multiline(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\\nbaz %s\", \"arg1\", \"arg2\")\n\n    assert \"boo arg1\" in caplog.text\n    assert \"baz arg2\" in caplog.text\n    assert len(caplog.records) == 1\n    assert len(caplog.messages) == 1\n    assert len(caplog.text.splitlines()) > 1\n", "def test_colored_log_formatter(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"INFO message\")\n    logger.warning(\"WARNING message\")\n    logger.error(\"ERROR message\")\n    logger.critical(\"CRITICAL message\")\n\n    log_text = caplog.text\n    assert \"\\033[32m\" in log_text  # Green color code for INFO level\n    assert \"\\033[33m\" in log_text  # Yellow color code for WARNING level\n    assert \"\\033[31m\" in log_text  # Red color code for ERROR and CRITICAL levels\n", "def test_log_cli_enabled(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logger = logging.getLogger('catchlog')\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin._log_cli_enabled() == True\n            logger.info(\"INFO message will be shown\")\n            assert \"INFO message will be shown\" in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n", "def test_log_cli_enabled(pytestconfig):\n    # Ensure live logging is enabled by default.\n    assert pytestconfig.getini(\"log_cli\") == \"False\"\n    # Ensure log_cli_level is None when log_cli is disabled.\n    assert pytestconfig.getoption(\"--log-cli-level\") is None\n", "def test_log_file_handler(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file=log.txt\n        log_file_level=INFO\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logger = logging.getLogger('catchlog')\n            logger.info(\"INFO message will be shown\")\n            logger.debug(\"DEBUG message won't be shown\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n\n    with open(\"log.txt\", \"r\") as f:\n        log_content = f.read()\n        assert \"INFO message will be shown\" in log_content\n        assert \"DEBUG message won't be shown\" not in log_content\n"], "sample_686": ["def test_fixture_positional_arguments_warning() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture\\\\(\\\\) as positional arguments is deprecated\",\n    ):\n        pytest.fixture(1, 2, 3)\n", "def test_fixture_positional_arguments_warning() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture\\\\(\\\\) as positional arguments is deprecated\",\n    ):\n        pytest.fixture(autouse=True)\n", "def test_fixture_positional_arguments_warning() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"Passing arguments to pytest.fixture\\\\(\\\\) as positional arguments is deprecated\",\n    ):\n        pytest.fixture(\"module\", \"some_fixture\")  # type: ignore\n", "def test_warning_captured_hook_is_deprecated(pytestconfig: Config) -> None:\n    \"\"\"Check that pytest_warning_captured hook is deprecated\"\"\"\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=\"The pytest_warning_captured is deprecated and will be removed in a future release\",\n    ):\n        pytestconfig.pluginmanager.add_hookcall_monitor(\"pytest_warning_captured\")\n", "def test_fixture_positional_arguments_warn(testdir) -> None:\n    conftest = testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True, scope=\"function\")\n            pass\n    \"\"\"\n    )\n    with pytest.warns(pytest.PytestDeprecationWarning) as w:\n        testdir.parseconftest(conftest)\n    assert len(w) == 1\n    assert \"Passing arguments to pytest.fixture() as positional arguments is deprecated\" in str(w[0].message)\n"], "sample_687": ["def test_percent_style_multiline(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo\\n%s\", \"arg\")\n    assert caplog.records[0].levelname == \"INFO\"\n    assert caplog.records[0].msg == \"boo\\n%s\"\n    assert \"boo\\narg\" in caplog.text\n\n    caplog.clear()\n    logger.info(\"bar\\n%s\\nbaz %s\", \"arg1\", \"arg2\")\n    assert caplog.records[0].levelname == \"INFO\"\n    assert caplog.records[0].msg == \"bar\\n%s\\nbaz %s\"\n    assert \"bar\\narg1\\nbaz arg2\" in caplog.text\n", "def test_caplog_get_records_unknown_when(caplog):\n    with pytest.raises(ValueError):\n        caplog.get_records(\"unknown\")\n", "def test_percent_style_multiline_with_fixed_indentation(caplog):\n    caplog.set_level(logging.INFO)\n    formatter = PercentStyleMultiline(DEFAULT_LOG_FORMAT, auto_indent=4)\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.handlers = [handler]\n\n    logger.info(\"message with\\nmultiple lines\")\n\n    expected_output = \"INFO     test_logging:         message with\\n            multiple lines\\n\"\n    assert expected_output in caplog.text\n", "def test_percent_style_multiline_formatter(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"Hello\\nWorld\")\n    assert \"Hello\" in caplog.text\n    assert \"World\" in caplog.text\n\n    logger.info(\"Hello\\n  World\", extra={\"auto_indent\": True})\n    lines = caplog.text.splitlines()\n    assert len(lines) > 2\n    assert lines[-2].startswith(\"  World\")\n\n    logger.info(\"Hello\\n  World\", extra={\"auto_indent\": 4})\n    lines = caplog.text.splitlines()\n    assert len(lines) > 2\n    assert lines[-2].startswith(\"    World\")\n\n    logger.info(\"Hello\\n  World\", extra={\"auto_indent\": False})\n    lines = caplog.text.splitlines()\n    assert len(lines) > 2\n    assert lines[-2] == \"World\"\n\n    logger.info(\"Hello\\n  World\", extra={\"auto_indent\": 0})\n    lines = caplog.text.splitlines()\n    assert len(lines) > 2\n    assert lines[-2] == \"World\"\n", "def test_log_file_cli_option(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logger = logging.getLogger('catchlog')\n            logger.error(\"ERROR message will be shown\")\n    \"\"\"\n    )\n\n    log_file = testdir.tmpdir.join(\"log.txt\")\n    result = testdir.runpytest(f\"--log-file={log_file}\")\n\n    assert result.ret == 0\n    assert \"ERROR message will be shown\" in log_file.read()\n"], "sample_688": ["def test_import_path_with_importlib_mode(testdir):\n    \"\"\"Test import_path function with importlib mode.\"\"\"\n    testdir.makepyfile(\n        **{\n            \"tests/test_foo.py\": \"def test_foo(): pass\",\n        }\n    )\n    pytest_config = testdir.parseconfig()\n    import_path = pytest._path.import_path\n    module = import_path(\"tests/test_foo.py\", mode=\"importlib\")\n    assert module.__name__ == \"test_foo\"\n    assert hasattr(module, \"test_foo\")\n", "def test_absolutepath(tmpdir):\n    \"\"\"Test absolutepath function.\"\"\"\n    path = tmpdir.join(\"test.txt\")\n    assert absolutepath(path) == Path(os.path.abspath(str(path)))\n", "def test_absolutepath(testdir):\n    \"\"\"Test that absolutepath returns the absolute path.\"\"\"\n    tmpdir = testdir.tmpdir\n    rel_path = tmpdir.join(\"rel_path\")\n    abs_path = absolutepath(rel_path)\n    assert abs_path == Path(os.path.abspath(str(rel_path)))\n", "def test_absolutepath(tmpdir):\n    \"\"\"Test absolutepath function with different types of paths.\"\"\"\n    path = tmpdir.join(\"test.txt\")\n    path.write(\"test\")\n\n    # Test with Path object\n    assert absolutepath(path) == Path(os.path.abspath(str(path)))\n\n    # Test with string\n    assert absolutepath(str(path)) == Path(os.path.abspath(str(path)))\n\n    # Test with relative path\n    rel_path = Path(\"test.txt\")\n    assert absolutepath(rel_path) != Path(os.path.abspath(str(rel_path)))\n", "def test_import_mode_importlib_with_relative_imports(testdir):\n    \"\"\"--import-mode=importlib works with relative imports.\"\"\"\n    testdir.makepyfile(\n        **{\n            \"tests/__init__.py\": \"\",\n            \"tests/test_foo.py\": \"\"\"\n                from . import foo\n                    assert foo.foo() == 42\n            \"\"\",\n            \"tests/foo.py\": \"\"\"\n            \"\"\",\n        }\n    )\n    result = testdir.runpytest(\"-v\", \"--import-mode=importlib\")\n    result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n"], "sample_689": ["def test_strict_option_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The --strict option is deprecated, use --strict-markers instead.*\"])\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makeconftest(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    testdir.makepyfile(\"def test_foo(): pass\")\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*pytest_warning_captured is deprecated*\"])\n", "def test_warning_captured_hook_is_deprecated(testdir) -> None:\n    conftest = testdir.makeconftest(\n        \"\"\"\n        import warnings\n\n            warnings.warn(pytest.PytestDeprecationWarning(\"Test warning captured\"))\n        \"\"\"\n    )\n    test_file = testdir.makepyfile(\n        \"\"\"\n        import warnings\n\n            warnings.warn(UserWarning(\"Test warning\"))\n        \"\"\"\n    )\n    result = testdir.runpytest(test_file, conftest)\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated and will be removed in a future release.*\"])\n", "def test_warning_captured_hook_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            \"The pytest_warning_captured is deprecated and will be removed in a future release.\\n\"\n            \"Please use pytest_warning_recorded instead.\"\n        ),\n    ):\n        warnings.warn(deprecated.WARNING_CAPTURED_HOOK, category=pytest.PytestDeprecationWarning)\n", "def test_warning_captured_hook_is_deprecated(testdir) -> None:\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated*\"])\n"], "sample_690": ["def test_xfail_strict_ini(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_strict_default(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_strict_ini(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_strict_ini(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_strict_ini(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n"], "sample_691": ["def test_faulthandler_stderr_key(pytester: Pytester) -> None:\n    \"\"\"Test that fault_handler_stderr_key is properly stored and cleaned up.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n\n    # Check that fault_handler_stderr_key is not present after the test run.\n    from _pytest.faulthandler import fault_handler_stderr_key\n    from _pytest.config import get_config\n\n    config = get_config(pytester.path)\n    assert fault_handler_stderr_key not in config._store\n", "def test_unconfigure(pytester: Pytester) -> None:\n    \"\"\"Test that faulthandler is properly unconfigured after pytest finishes.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n\n    import faulthandler\n\n    # After pytest has finished, faulthandler should be enabled and attached to sys.stderr.\n    assert faulthandler.is_enabled()\n    assert faulthandler._get_stderr_fileno() == sys.stderr.fileno()\n", "def test_fault_handler_stderr_key(pytester: Pytester) -> None:\n    \"\"\"Test that the fault handler stderr key is properly set and cleaned up.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            import faulthandler\n            assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n\n    # Check that the fault handler stderr key is set\n    config = pytester.parseconfigure()\n    assert fault_handler_stderr_key in config._store\n\n    # Check that the fault handler stderr key is cleaned up\n    class FaultHandlerHooksMock:\n            pass\n\n    hooks = FaultHandlerHooksMock()\n    hooks.pytest_unconfigure(config)\n    assert fault_handler_stderr_key not in config._store\n", "def test_fault_handler_stderr_key(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler stderr key is properly stored and cleaned up.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    with pytest.MonkeyPatch.context() as mp:\n        import faulthandler\n\n        class FaultHandlerHooksMock:\n                pass\n\n                pass\n\n        mp.setattr(\"_pytest.faulthandler.FaultHandlerHooks\", FaultHandlerHooksMock)\n\n        result = pytester.runpytest_subprocess()\n\n        # Ensure the test passed\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n        assert result.ret == 0\n\n        # Ensure fault_handler_stderr_key was stored and cleaned up\n        assert fault_handler_stderr_key not in pytester.config._store\n", "def test_faulthandler_stderr_key(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler stderr key is properly stored and cleaned up.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            import faulthandler\n            assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n\n    # Check that the fault handler stderr key is removed after the test\n    from _pytest.faulthandler import fault_handler_stderr_key\n    from _pytest.config import get_config\n    config = get_config()\n    assert fault_handler_stderr_key not in config._store\n"], "sample_692": ["def test_tmp_path_factory_basetemp_given_absolute(tmp_path_factory: TempPathFactory) -> None:\n    \"\"\"Test that an absolute path given as the basetemp is respected.\"\"\"\n    tmppath = tmp_path_factory.getbasetemp()\n    abs_path = tmppath.resolve()\n    assert tmppath == abs_path\n    assert tmppath.is_dir()\n", "def test_tmp_path_factory_uses_absolute_path(pytester: Pytester, tmp_path) -> None:\n    \"\"\"Test that tmp_path_factory uses absolute paths (#4427)\"\"\"\n    rel_path = \"relative/path\"\n    abs_path = pytester.path.joinpath(rel_path)\n    tmppath_handler = TempPathFactory(abs_path, pytester.config.trace.get(\"tmpdir\"))\n    assert tmppath_handler.getbasetemp().resolve() == abs_path.resolve()\n", "def test_tmp_path_factory_creates_basetemp_if_none_given(tmp_path_factory: TempPathFactory) -> None:\n    \"\"\"Test that tmp_path_factory creates a basetemp directory if none is given.\"\"\"\n    assert tmp_path_factory.getbasetemp().exists()\n    assert tmp_path_factory.getbasetemp().is_dir()\n", "def test_temp_path_factory_lock_timeout(pytester: Pytester, monkeypatch) -> None:\n    \"\"\"Test that the TempPathFactory uses the LOCK_TIMEOUT when creating numbered directories\"\"\"\n    import _pytest.pathlib\n\n    original_lock_timeout = _pytest.pathlib.LOCK_TIMEOUT\n    monkeypatch.setattr(_pytest.pathlib, \"LOCK_TIMEOUT\", 0.1)\n\n    pytester.makepyfile(\n        \"\"\"\n            tmp_path_factory.mktemp(\"hello\")\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n\n    monkeypatch.setattr(_pytest.pathlib, \"LOCK_TIMEOUT\", original_lock_timeout)\n    reprec.assertoutcome(passed=1)\n", "def test_tmp_path_factory_getbasetemp_with_given_basetemp(tmp_path):\n    given_basetemp = tmp_path / \"given-basetemp\"\n    given_basetemp.mkdir()\n    factory = TempPathFactory(given_basetemp, trace=None)\n    assert factory.getbasetemp() == given_basetemp.resolve()\n"], "sample_693": ["def test_unittest_skipif_mark(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        @pytest.mark.skipif(True, reason=\"skipping due to reasons\")\n        class MyTestCase(unittest.TestCase):\n                assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"-rs\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *SKIP*[1]*skipping due to reasons*\n        *1 skipped*\n    \"\"\"\n    )\n", "def test_is_skipped_mark(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            @unittest.skip(\"hello\")\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\", \"-rs\")\n    result.stdout.fnmatch_lines([\"*SKIP*[1]*hello*\", \"*1 skipped*\"])\n    assert result.ret == 0\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestExample(unittest.TestCase):\n\n                for i in range(5):\n                    with self.subTest(i=i):\n                        self.assertEqual(i % 2, 0)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*FAIL TestExample.test_example[i=1]*\",\n            \"*FAIL TestExample.test_example[i=3]*\",\n            \"*= 3 failed, 2 passed in *\",\n        ]\n    )\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytest.importorskip(\"unittest\", minversion=\"4.0\")\n\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                for i in range(5):\n                    with self.subTest(i=i):\n                        self.assertEqual(i % 2, 0)\n        \"\"\"\n    )\n    reprec = pytester.inline_run()\n    reprec.assertoutcome(passed=1, failed=1)\n", "def test_unittest_expected_failure_for_passing_test_is_fail_with_reason(\n    pytester: Pytester,"], "sample_694": ["def test_argument_percent_default_deprecation(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", default=\"bar\", help=\"%%default == bar\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: pytest now uses argparse. \\\"%%default\\\" should be changed to \\\"%%(default)s\\\"\",\n        ]\n    )\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        PytestRemovedIn8Warning,\n        match=r\"pytest now uses argparse. %default should be changed to %(default)s\",\n    ):\n        pytest.deprecated.ARGUMENT_PERCENT_DEFAULT\n", "def test_argument_type_str_is_deprecated() -> None:\n    with pytest.warns(\n        PytestRemovedIn8Warning,\n        match=r\"`type` argument to addoption\\(\\) is the string 'str', but when supplied should be a type \\(for example `str` or `int`\\).\",\n    ):\n        parser = pytest.Parser()\n        group = parser.getgroup(\"test group\")\n        group.addoption(\"--test-option\", type=\"str\")\n", "def test_addoption_type_str_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"str\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'str', \"\n            \"but when supplied should be a type (for example `str` or `int`).\",\n        ]\n    )\n", "def test_argument_type_str_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n            parser.addoption(\"--foo\", type=\"string\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: `type` argument to addoption() is the string 'string', \"\n            \"but when supplied should be a type (for example `str` or `int`).\",\n        ]\n    )\n"], "sample_695": ["def test_node_repr_failure() -> None:\n    class DummyNode(nodes.Node):\n            super().__init__(\"dummy\", nodeid=\"dummy\")\n\n    excinfo = pytest.raises(Exception, match=\".*\").excinfo\n    dummy_node = DummyNode()\n    with pytest.raises(ValueError):\n        dummy_node._repr_failure_py(excinfo, style=\"unknown_style\")\n", "def test_node_get_fslocation_from_item(tmp_path: Path) -> None:\n    class FakeNode(nodes.Node):\n            super().__init__(\"fake\", nodeid=\"fake\", fspath=legacy_path(path))\n            self.obj = obj\n            self._location = location\n\n        @property\n            return self._location\n\n    # Test with location attribute.\n    node = FakeNode(tmp_path / \"test.txt\", location=(\"test.txt\", 10, \"test\"))\n    assert nodes.get_fslocation_from_item(node) == (tmp_path / \"test.txt\", 10)\n\n    # Test with obj attribute.\n    obj = object()\n    node = FakeNode(tmp_path / \"test.txt\", obj=obj)\n    assert nodes.get_fslocation_from_item(node) == (tmp_path / \"test.txt\", -1)\n\n    # Test without location or obj attributes.\n    node = FakeNode(tmp_path / \"test.txt\")\n    assert nodes.get_fslocation_from_item(node) == (tmp_path / \"test.txt\", -1)\n", "def test_node_repr_failure_with_fspath(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    item.path = Path(\"/path/to/test.py\")\n    excinfo = pytest.raises(Exception, lambda: item.runtest())\n    assert str(item.path) in str(item.repr_failure(excinfo))\n", "def test_node_repr_failure_with_explicit_tbstyle(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    excinfo = pytest.raises(AssertionError, items[0].runtest)\n    rep = items[0].repr_failure(excinfo)\n    assert isinstance(rep, str)\n\n    # explicit tbstyle should be respected\n    pytester.makeconftest(\n        \"\"\"\n            config.option.tbstyle = \"short\"\n    \"\"\"\n    )\n    items = pytester.getitems(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    excinfo = pytest.raises(AssertionError, items[0].runtest)\n    rep = items[0].repr_failure(excinfo)\n    assert isinstance(rep, str)\n", "def test_item_add_report_section(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    item.add_report_section(\"call\", \"my_section\", \"Hello World!\")\n    assert item._report_sections == [(\"call\", \"my_section\", \"Hello World!\")]\n"], "sample_696": ["def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\"pytest now uses argparse. %default should be changed to %(default)s\"),\n    ):\n        import _pytest.deprecated\n        warnings.warn(_pytest.deprecated.ARGUMENT_PERCENT_DEFAULT)\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"'\n        ),\n    ):\n        pytest.deprecated.ARGUMENT_PERCENT_DEFAULT.warn()\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=r\"pytest now uses argparse. %default should be changed to %(default)s\",\n    ):\n        pytest.deprecated.ARGUMENT_PERCENT_DEFAULT.warn()\n", "def test_argument_percent_default_is_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\"pytest now uses argparse. %default should be changed to %(default)s\"),\n    ):\n        warn(deprecated.ARGUMENT_PERCENT_DEFAULT, stacklevel=2)\n", "def test_node_fspath_deprecated() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            \"Node.fspath is deprecated and will be replaced by Node.path.\\n\"\n            \"see https://docs.pytest.org/en/latest/deprecations.html#node-fspath-in-favor-of-pathlib-and-node-path\"\n        ),\n    ):\n        pytest.Node().fspath\n"], "sample_697": ["def test_tmp_path_factory_handles_lock_timeout(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_basetemp_exists(tmp_path_factory: TempPathFactory, tmp_path: Path) -> None:\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp.exists()\n    assert basetemp.is_dir()\n\n    # Create a new TempPathFactory with the same basetemp.\n    new_factory = TempPathFactory(basetemp, lambda *args: None, _ispytest=True)\n\n    # Verify that the new factory doesn't try to create the basetemp again.\n    assert new_factory.getbasetemp() == basetemp\n    assert new_factory._basetemp == basetemp\n", "def test_tmp_path_factory_uses_existing_basetemp_if_valid(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_uses_given_basetemp(tmp_path_factory: TempPathFactory, tmp_path: Path) -> None:\n    given_basetemp = tmp_path / \"given\"\n    tmp_path_factory = TempPathFactory(given_basetemp, lambda *args: None, _ispytest=True)\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp == given_basetemp\n    assert basetemp.is_dir()\n", "def test_tmp_path_factory_create_directory_with_owner_as_current_user(\n    tmp_path: Path, monkeypatch: MonkeyPatch"], "sample_698": ["def test_coloredlogformatter_non_string_levelname() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=42,  # custom level\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 Level 42  Test Message\"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 Level 42  Test Message\")\n", "def test_coloredlogformatter_with_invalid_loglevel() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=42,  # invalid log level\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 Level 42 Test Message\"\n    )\n", "def test_percent_style_multiline_auto_indent_on() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record: Any = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message line1\\nline2\",\n        args=(),\n        exc_info=None,\n    )\n    record.message = record.getMessage()\n\n    style = PercentStyleMultiline(logfmt, auto_indent=True)\n    output = style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\n\"\n        \"                                        line2\"\n    )\n\n    # Multiple lines.\n    record.msg = \"Test Message line1\\nline2\\nline3\"\n    record.message = record.getMessage()\n    output = style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\n\"\n        \"                                        line2\\n\"\n        \"                                        line3\"\n    )\n", "def test_percent_style_multiline_with_custom_indentation() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record: Any = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message line1\\nline2\\nline3\",\n        args=(),\n        exc_info=None,\n    )\n    # this is called by logging.Formatter.format\n    record.message = record.getMessage()\n\n    ai_on_style = PercentStyleMultiline(logfmt, auto_indent=3)\n    output = ai_on_style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\n\"\n        \"                                      line2\\n\"\n        \"                                      line3\"\n    )\n\n    ai_off_style = PercentStyleMultiline(logfmt, auto_indent=False)\n    output = ai_off_style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\nline2\\nline3\"\n    )\n", "def test_percent_style_multiline_with_exception() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record: Any = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message line1\\nline2\",\n        args=(),\n        exc_info=None,\n    )\n    # this is called by logging.Formatter.format\n    record.message = record.getMessage()\n    record.exc_text = \"Traceback (most recent call last):\\n  File \\\"test.py\\\", line 1, in <module>\\n    raise Exception('Test')\\nException: Test\"\n\n    ai_on_style = PercentStyleMultiline(logfmt, True)\n    output = ai_on_style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\n\"\n        \"                                        line2\\n\"\n        \"Traceback (most recent call last):\\n\"\n        \"  File \\\"test.py\\\", line 1, in <module>\\n\"\n        \"    raise Exception('Test')\\n\"\n        \"Exception: Test\"\n    )\n\n    ai_off_style = PercentStyleMultiline(logfmt, False)\n    output = ai_off_style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\n\"\n        \"line2\\n\"\n        \"Traceback (most recent call last):\\n\"\n        \"  File \\\"test.py\\\", line 1, in <module>\\n\"\n        \"    raise Exception('Test')\\n\"\n        \"Exception: Test\"\n    )\n"], "sample_699": ["def test_is_setup_py_with_contents(tmp_path: Path) -> None:\n    setup_py = tmp_path.joinpath(\"setup.py\")\n    setup_py.write_text('import os', \"utf-8\")\n    assert not _is_setup_py(setup_py)\n", "def test_doctest_item_nodeid(pytester: Pytester) -> None:\n    pytester.maketxtfile(\n        test_something=\"\"\"\n        >>> i = 5\n        >>> i-1\n        4\n    \"\"\"\n    )\n    items, _ = pytester.inline_genitems()\n    assert len(items) == 1\n    item = items[0]\n    assert isinstance(item, DoctestItem)\n    assert item.nodeid.endswith(\"test_something.txt\")\n", "def test_is_setup_py_with_different_content(tmp_path: Path) -> None:\n    setup_py = tmp_path.joinpath(\"setup.py\")\n    setup_py.write_text('print(\"Hello, World!\")')\n    assert not _is_setup_py(setup_py)\n", "def test_is_setup_py_no_contents(tmp_path: Path) -> None:\n    setup_py = tmp_path.joinpath(\"setup.py\")\n    setup_py.write_text(\"\", \"utf-8\")\n    assert not _is_setup_py(setup_py)\n", "def test_doctest_continue_on_failure_with_usepdb(self, pytester: Pytester):\n    pytester.maketxtfile(\n        test_something=\"\"\"\n        >>> i = 5\n        >>> def foo():\n        ...     raise ValueError('error1')\n        >>> foo()\n        >>> i\n        >>> i + 2\n        7\n        >>> i + 1\n    \"\"\"\n    )\n    result = pytester.runpytest(\n        \"--doctest-modules\", \"--doctest-continue-on-failure\", \"--pdb\"\n    )\n    result.stdout.fnmatch_lines([\"*4: UnexpectedException*\", \"*5: DocTestFailure*\"])\n    # pdb should stop at the first failure\n    result.stdout.no_fnmatch_line(\"*8: DocTestFailure*\")\n"], "sample_700": ["def test_xfail_markeval_namespace_override(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            return {\"color\": \"green\"}\n        \"\"\"\n    )\n    pytester.makepyfile(\n        conftest=\"\"\"\n        import pytest\n\n            return {\"color\": \"red\"}\n        \"\"\",\n        test_one=\"\"\"\n        import pytest\n\n        @pytest.mark.xfail(\"color == 'red'\")\n            assert False\n        \"\"\",\n    )\n    res = pytester.runpytest()\n    assert res.ret == 1\n    res.stdout.fnmatch_lines([\"*1 failed*\"])\n    res.stdout.fnmatch_lines([\"*1 xfailed*\"])\n", "def test_xfail_item_with_run_false(pytester: Pytester) -> None:\n    # Ensure pytest.xfail with run=False works with non-Python Item\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        class MyItem(pytest.Item):\n            nodeid = 'foo'\n                pytest.xfail(\"Expected Failure\", run=False)\n\n            return MyItem.from_parent(name=\"foo\", parent=parent)\n    \"\"\"\n    )\n    result = pytester.inline_run()\n    passed, skipped, failed = result.listoutcomes()\n    assert not failed\n    xfailed = [r for r in skipped if hasattr(r, \"wasxfail\")]\n    assert xfailed\n", "def test_pytest_pyfunc_call_async_warn_and_skip(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import asyncio\n\n        async def test_func():\n            assert 1\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*async def functions are not natively supported and have been skipped.\",\n            \"*You need to install a suitable plugin for your async framework, for example:\",\n            \"*- anyio\",\n            \"*- pytest-asyncio\",\n            \"*- pytest-tornasync\",\n            \"*- pytest-trio\",\n            \"*- pytest-twisted\",\n        ]\n    )\n", "def test_xfail_strict_keyword(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(True, reason=\"Expected failure - true\", strict=True)\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n", "def test_skipif_with_invalid_condition(pytester: Pytester) -> None:\n    \"\"\"Verify that using pytest.mark.skipif with invalid condition raises an error.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"invalid condition\")\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*Error evaluating 'skipif' condition as a boolean*\"])\n"], "sample_701": ["def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            parser.addoption(\"--foo\", default=\"bar\", help=\"foo option (default: %default)\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--help\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%(default)s\\\"\",\n        ]\n    )\n", "def test_argument_type_str_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            \"`type` argument to addoption() is the string 'str', \"\n            \"but when supplied should be a type (for example `str` or `int`).\"\n        ),\n    ):\n        class DummyParser:\n                pass\n\n        parser = DummyParser()\n        parser.addoption(\"--dummy\", type=\"str\")\n", "def test_argument_type_str_is_deprecated() -> None:\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            \"`type` argument to addoption() is the string 'str', \"\n            \"but when supplied should be a type (for example `str` or `int`).\"\n        ),\n    ):\n        class MyPlugin:\n                parser.addoption(\"--myopt\", type=\"str\")\n        MyPlugin().pytest_addoption(pytest.config.parser)\n", "def test_check_ispytest_stacklevel() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning, match=\"private pytest class or function\"\n    ) as record:\n        deprecated.check_ispytest(False)\n    (warning,) = record\n    assert warning.lineno == sys._getframe().f_lineno - 2\n", "def test_check_ispytest_warns_with_false_argument() -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning, match=\"private pytest class or function\"\n    ):\n        deprecated.check_ispytest(False)\n\n    # Doesn't warn.\n    deprecated.check_ispytest(True)\n"], "sample_702": ["def test_pytester_getpublicnames(pytester: Pytester) -> None:\n    assert pytester_mod.get_public_names([\"hello\", \"_private\", \"world\"]) == [\"hello\", \"world\"]\n    assert pytester_mod.get_public_names([]) == []\n", "def test_testdir_getitem(testdir: Testdir) -> None:\n    \"\"\"Check that getitem works with testdir.\"\"\"\n    item = testdir.getitem(\"def test_func(): pass\")\n    assert item.name == \"test_func\"\n", "def test_pytester_run_with_close_stdin(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\n        \"\"\"\n        import sys\n\n            data = sys.stdin.read()\n            assert data == \"\"\n    \"\"\"\n    )\n    result = pytester.runpytest(testfile, stdin=Pytester.CLOSE_STDIN)\n    result.assert_outcomes(passed=1)\n", "def test_pytester_copy_example(pytester: Pytester) -> None:\n    pytester.copy_example(\"conftest.py\")\n    assert (pytester.path / \"conftest.py\").exists()\n", "def test_testdir_makeconftest(testdir: Testdir) -> None:\n    \"\"\"Test that the testdir.makeconftest method correctly creates a conftest.py file.\"\"\"\n    testdir.makeconftest(\"pytest_plugins = ['pytester']\")\n    assert (testdir.tmpdir / \"conftest.py\").isfile()\n    with open(str(testdir.tmpdir / \"conftest.py\")) as f:\n        assert f.read() == \"pytest_plugins = ['pytester']\\n\"\n"], "sample_703": ["def test_matcher_adapter() -> None:\n    class TestMatcher:\n            self.prefix = prefix\n\n            return ident.startswith(self.prefix)\n\n    adapter = MatcherAdapter(TestMatcher(IDENT_PREFIX))\n    assert adapter[IDENT_PREFIX + \"foo\"]\n    assert not adapter[IDENT_PREFIX + \"barbaz\"]\n\n    with pytest.raises(KeyError):\n        adapter[\"foo\"]\n\n    with pytest.raises(NotImplementedError):\n        list(adapter)\n\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n", "def test_matcher_adapter() -> None:\n    class Matcher:\n            self.idents = idents\n\n            return ident in self.idents\n\n    adapter = MatcherAdapter(Matcher([\"foo\", \"bar\"]))\n    assert adapter[\"$foo\"]\n    assert adapter[\"$bar\"]\n    assert not adapter[\"$baz\"]\n\n    with pytest.raises(NotImplementedError):\n        list(adapter)\n\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n", "def test_reserved_words(expr: str, ident: str, expected: bool) -> None:\n    matcher = {ident: True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_matcher_adapter() -> None:\n    matcher = {\"foo\": True, \"bar\": False}.__getitem__\n\n    adapter = MatcherAdapter(matcher)\n\n    assert adapter[\"$foo\"]\n    assert not adapter[\"$bar\"]\n\n    with pytest.raises(KeyError):\n        adapter[\"baz\"]\n\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n\n    with pytest.raises(NotImplementedError):\n        iter(adapter)\n", "def test_reserved_words_as_idents(ident: str) -> None:\n    with pytest.raises(ParseError):\n        evaluate(ident, {ident: True}.__getitem__)\n"], "sample_704": ["def test_node_listchain() -> None:\n    root = nodes.Node(\"root\", nodeid=\"root\")\n    child = nodes.Node(\"child\", parent=root, nodeid=\"root::child\")\n    grandchild = nodes.Node(\"grandchild\", parent=child, nodeid=\"root::child::grandchild\")\n\n    expected_chain = [root, child, grandchild]\n    assert grandchild.listchain() == expected_chain\n", "def test_node_repr_failure_py() -> None:\n    class FakeNode(nodes.Node):\n            super().__init__(\"fake\", parent=None, config=None, session=None)\n\n    node = FakeNode()\n    excinfo = pytest.raises(ZeroDivisionError, match=\"division by zero\")\n    with pytest.raises(OutcomeException):\n        node._repr_failure_py(excinfo, style=\"short\")\n", "def test_node_repr_failure_path_mismatch(tmp_path: Path) -> None:\n    class FakeSession:\n        _initialpaths = frozenset({tmp_path})\n\n    session = cast(pytest.Session, FakeSession)\n\n    class FakeNode(nodes.Node):\n            super().__init__(\"test\", nodeid=\"test\", session=session)\n            self.path = tmp_path / \"mismatch\"\n\n    node = FakeNode()\n    excinfo = pytest.raises(FileNotFoundError)\n    with excinfo:\n        node.repr_failure(excinfo)\n    assert \"FileNotFoundError\" in str(excinfo.value)\n", "def test_node_get_fslocation_from_item() -> None:\n    class FakeNode(nodes.Node):\n            super().__init__(\"name\", parent=None, config=None, session=None)\n            self.path = path\n            self.obj = obj\n            self.location = location\n\n    node = FakeNode(Path(\"/path/to/node\"))\n    assert nodes.get_fslocation_from_item(node) == (str(node.path), -1)\n\n    node = FakeNode(None, obj={\"__file__\": \"/path/to/obj\"})\n    assert nodes.get_fslocation_from_item(node) == (\"/path/to/obj\", -1)\n\n    node = FakeNode(None, location=(\"path\", 10))\n    assert nodes.get_fslocation_from_item(node) == (\"path\", 10)\n", "def test_node_get_closest_marker() -> None:\n    class FakeNode(nodes.Node):\n            super().__init__(name, nodeid=nodeid)\n            if own_markers is not None:\n                self.own_markers = own_markers\n\n    marker1 = nodes.Mark(name=\"marker1\", args=(), kwargs={})\n    marker2 = nodes.Mark(name=\"marker2\", args=(), kwargs={})\n\n    parent = FakeNode(\"parent\", \"parent\")\n    parent.own_markers = [marker1]\n\n    child = FakeNode(\"child\", \"parent::child\", own_markers=[marker2])\n\n    assert child.get_closest_marker(\"marker1\") == marker1\n    assert child.get_closest_marker(\"marker2\") == marker2\n    assert child.get_closest_marker(\"marker3\") is None\n"], "sample_705": ["def test_pytester_runpytest_subprocess_env(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    monkeypatch.setenv(\"PYTEST_ADDOPTS\", \"--unknown-option\")\n    testfile = pytester.makepyfile(\"def test_one(): pass\")\n    result = pytester.runpytest_subprocess(testfile)\n    assert result.ret != 0\n", "def test_runresult_assert_outcomes_skipped(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1)\n    assert result.parseoutcomes() == {\"skipped\": 1}\n", "def test_pytester_with_nested_test_items(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert True\n\n        class TestClass:\n                assert True\n\n            @pytest.mark.parametrize(\"arg\", [1, 2])\n                assert arg in [1, 2]\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=4)\n", "def test_testdir_parseconfig(testdir: Testdir) -> None:\n    config1 = testdir.parseconfig()\n    config2 = testdir.parseconfig()\n    assert config2 is not config1\n", "def test_pytester_parse_summary_nouns_empty(pytester: Pytester) -> None:\n    lines = [\"some output 1\", \"done.\"]\n    with pytest.raises(ValueError, match=\"Pytest terminal summary report not found\"):\n        pytester_mod.RunResult.parse_summary_nouns(lines)\n"], "sample_706": ["def test_matcher_adapter() -> None:\n    matcher = {\"foo\": True, \"bar\": False}.__getitem__\n\n    adapter = MatcherAdapter(matcher)\n    assert adapter[\"$foo\"]\n    assert not adapter[\"$bar\"]\n\n    with pytest.raises(KeyError):\n        adapter[\"baz\"]\n\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n\n    with pytest.raises(NotImplementedError):\n        iter(adapter)\n", "def test_python_keywords_as_idents(expr: str, ident: str, expected: bool) -> None:\n    matcher = {ident: True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_matcher_adapter() -> None:\n    matcher = {\"foo\": True, \"bar\": False}.__getitem__\n    adapter = MatcherAdapter(matcher)\n\n    assert adapter[\"$foo\"]\n    assert not adapter[\"$bar\"]\n\n    with pytest.raises(KeyError):\n        adapter[\"$baz\"]\n\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n\n    with pytest.raises(NotImplementedError):\n        iter(adapter)\n", "def test_matcher_adapter() -> None:\n    class TestMatcher:\n            self.prefix = prefix\n\n            return ident.startswith(self.prefix)\n\n    adapter = MatcherAdapter(TestMatcher(IDENT_PREFIX))\n    assert adapter[IDENT_PREFIX + \"test\"]\n    assert not adapter[IDENT_PREFIX + \"other\"]\n    with pytest.raises(KeyError):\n        adapter[\"invalid_key\"]\n\n    # Ensure that the adapter raises NotImplementedError for other Mapping methods\n    with pytest.raises(NotImplementedError):\n        len(adapter)\n    with pytest.raises(NotImplementedError):\n        iter(adapter)\n", "def test_expression_with_trailing_whitespace() -> None:\n    assert not evaluate(\"   \", lambda ident: False)\n    assert not evaluate(\"\\t\", lambda ident: False)\n    assert not evaluate(\"true  \", {\"true\": True}.__getitem__)\n    assert not evaluate(\"false\\t\", {\"false\": False}.__getitem__)\n"], "sample_707": ["def test_node_repr_failure_with_non_pytest_warning(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    try:\n        raise UserWarning(\"some warning\")\n    except UserWarning as exc:\n        exc_info = pytest._code.ExceptionInfo.from_exc_info((type(exc), exc, exc.__traceback__))\n        with pytest.warns(UserWarning, match=\"some warning\"):\n            items[0].repr_failure(exc_info)\n", "def test_get_fslocation_from_item(tmp_path: Path) -> None:\n    \"\"\"Test that get_fslocation_from_item works correctly for different node types.\"\"\"\n    class FakeNode(nodes.Node):\n            self.path = path\n            self.location = location\n\n    class FakeItem(nodes.Item):\n            super().__init__(\"test\", parent=None)\n            self.path = path\n            self.location = location\n\n    fake_node = FakeNode(tmp_path / \"node.py\", (\"node.py\", 10))\n    fake_item = FakeItem(tmp_path / \"item.py\", (\"item.py\", 20))\n\n    assert nodes.get_fslocation_from_item(fake_node) == (str(tmp_path / \"node.py\"), 10)\n    assert nodes.get_fslocation_from_item(fake_item) == (str(tmp_path / \"item.py\"), 20)\n\n    # Test with obj attribute.\n    class FakeObj:\n        __file__ = str(tmp_path / \"obj.py\")\n        __code__ = type(compile(\"pass\", \"obj.py\", mode=\"exec\"))(\n            compile(\"pass\", \"obj.py\", mode=\"exec\"), \"obj.py\", \"exec\"\n        )\n\n    fake_node.obj = FakeObj()\n    assert nodes.get_fslocation_from_item(fake_node) == (str(tmp_path / \"obj.py\"), 1)\n\n    # Test with fspath attribute.\n    fake_node.fspath = tmp_path / \"fspath.py\"\n    assert nodes.get_fslocation_from_item(fake_node) == (str(tmp_path / \"fspath.py\"), -1)\n", "def test_node_repr_failure_with_traceback_style() -> None:\n    class FakeExcInfo:\n            self.value = Exception(\"Test\")\n            self.traceback = []\n\n            return f\"Repr with style {style}\"\n\n    node = nodes.Node.from_parent(None)\n    excinfo = FakeExcInfo()\n\n    # Test that the traceback style is passed to getrepr\n    repr_failure = node._repr_failure_py(excinfo, style=\"short\")\n    assert repr_failure == \"Repr with style short\"\n\n    # Test that the traceback style defaults to \"long\" if not provided\n    repr_failure = node._repr_failure_py(excinfo)\n    assert repr_failure == \"Repr with style long\"\n", "def test_node_repr_failure_with_full_trace(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            raise Exception(\"foo\")\n    \"\"\"\n    )\n    items = pytester.getitems()\n    excinfo = pytest.raises(Exception, items[0].runtest)\n    with pytest.raises(OutcomeException):\n        items[0].repr_failure(excinfo, style=\"short\")\n", "def test_node_keywords_contains_own_markers() -> None:\n    node = nodes.Item.from_parent(None, name=\"test_node\")\n    marker = pytest.mark.skip(reason=\"example reason\")\n    node.add_marker(marker)\n    assert \"skip\" in node.keywords\n    assert node.keywords[\"skip\"] == marker\n"], "sample_708": ["def test_getstatementrange_with_empty_lines() -> None:\n    source = Source(\n        \"\"\"\\\n            pass\n\n\n            pass\n        \"\"\"\n    )\n    assert source.getstatementrange(0) == (0, 2)\n    assert source.getstatementrange(3) == (3, 5)\n", "def test_getstatementrange_ast_with_nested_constructs() -> None:\n    source = Source(\n        \"\"\"\\\n            try:\n                if x > 0:\n                    raise ValueError\n                else:\n                    return x\n            except ValueError:\n                return -1\n        \"\"\"\n    )\n    assert len(source) == 9\n    # check all lineno's that could occur in a traceback\n    assert source.getstatementrange(2) == (2, 4)\n    assert source.getstatementrange(3) == (3, 4)\n    assert source.getstatementrange(4) == (4, 5)\n    assert source.getstatementrange(6) == (6, 7)\n", "def test_getstatementrange_with_nested_blocks() -> None:\n    source = Source(\n        \"\"\"\\", "def test_getstatementrange_ast_with_nested_functions() -> None:\n    source = Source(\n        \"\"\"\\", "def test_getstatementrange_ast_with_nested_constructs() -> None:\n    source = Source(\n        \"\"\"\\\n                try:\n                    x = 1\n                except Exception:\n                    pass\n        \"\"\"\n    )\n    ast, start, end = getstatementrange_ast(3, source)\n    assert start == 2\n    assert end == 6\n"], "sample_709": ["def test_pytester_assert_outcomes_deselected(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-k\", \"test_one or test_two\")\n    result.assert_outcomes(passed=2, deselected=1)\n    # If deselected is not passed, it is not checked at all.\n    result.assert_outcomes(passed=2)\n", "def test_pytester_spawn_use_custom_env(pytester: Pytester) -> None:\n    pytester._monkeypatch.setenv(\"CUSTOMENV\", \"42\")\n\n    p1 = pytester.makepyfile(\n        \"\"\"\n        import os\n\n            assert os.environ[\"HOME\"] != \"/home/user\"\n            assert os.environ[\"USERPROFILE\"] != \"/Users/user\"\n            assert os.environ[\"CUSTOMENV\"] == \"42\"\n    \"\"\"\n    )\n    child = pytester.spawn_pytest(str(p1))\n    out = child.read()\n    assert child.wait() == 0, out.decode(\"utf8\")\n", "def test_pytester_runpytest_subprocess_with_plugin(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        pytest_plugins = \"pytester\"\n            assert 1\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.assert_outcomes(passed=1)\n", "def test_pytester_runpytest_args(pytester: Pytester) -> None:\n    pytester.makepyfile(\"def test_pass(): assert True\")\n    result = pytester.runpytest(\"-k\", \"test_pass\")\n    result.assert_outcomes(passed=1)\n", "def test_pytester_run_result_parseoutcomes_with_no_summary(pytester: Pytester) -> None:\n    pytester.makepyfile(\"def test_foo(): pass\")\n    result = pytester.runpytest(\"-q\")\n    with pytest.raises(ValueError, match=\"Pytest terminal summary report not found\"):\n        result.parseoutcomes()\n"], "sample_710": ["def test_unittest_subtest(pytester: Pytester) -> None:\n    pytest.importorskip(\"unittest\", minversion=\"3.4\")\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class T(unittest.TestCase):\n                for x in range(5):\n                    with self.subTest(x=x):\n                        assert x < 3\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*2 failed, 3 passed*\"])\n", "def test_unittest_addCleanup(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class Test(unittest.TestCase):\n                self.cleanup_called = False\n\n                self.addCleanup(self.cleanup)\n                assert not self.cleanup_called\n\n                self.cleanup_called = True\n\n            assert Test().cleanup_called\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 0\n    assert passed == 2\n", "def test_unittest_TestCase_with_abstract_method(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        from abc import ABC, abstractmethod\n\n        class AbstractTestCase(ABC, unittest.TestCase):\n            @abstractmethod\n                pass\n\n        class MyTestCase(AbstractTestCase):\n                pass\n\n                pass\n\n        class MyOtherTestCase(AbstractTestCase):\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*TypeError: Can't instantiate abstract class *\"])\n    assert result.ret == 1\n", "def test_unittest_subtest(pytester: Pytester) -> None:\n    pytest.importorskip(\"unittest\", minversion=\"4.0\")\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                for i in range(5):\n                    with self.subTest(i=i):\n                        self.assertEqual(i % 2, 0)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*3 failed*\"])\n", "def test_unittest_subtest_context(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                with self.subTest(i=1):\n                    assert 1 == 1\n                with self.subTest(i=2):\n                    assert 2 == 2\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*::MyTestCase::test_one[i-1] PASSED *\",\n            \"*::MyTestCase::test_one[i-2] PASSED *\",\n        ]\n    )\n"], "sample_711": ["def test_node_get_closest_marker() -> None:\n    class DummyNode(nodes.Node):\n            super().__init__(\"dummy\", parent)\n            self.own_markers = own_markers or []\n\n    root = DummyNode(None)\n    child = DummyNode(root)\n    grandchild = DummyNode(child)\n\n    marker1 = pytest.mark.foo()\n    marker2 = pytest.mark.bar()\n\n    root.own_markers = [marker1]\n    child.own_markers = [marker2]\n\n    assert grandchild.get_closest_marker(\"foo\") == marker1\n    assert grandchild.get_closest_marker(\"bar\") == marker2\n    assert grandchild.get_closest_marker(\"baz\") is None\n", "def test_node_repr_failure_respects_explicit_tbstyle(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--tb=short\")\n    assert \"short test summary info\" in result.stdout.str()\n\n    class FakeSession:\n        _setupstate = None\n\n            return self.config.pluginmanager.gethookproxy(fspath)\n\n        @property\n            class FakeConfig:\n                option = {\"fulltrace\": False, \"tbstyle\": \"auto\"}\n\n                    return self.option.get(name, default)\n\n                @property\n                    class FakePluginManager:\n                            return None\n\n                    return FakePluginManager()\n\n            return FakeConfig()\n\n    session = cast(pytest.Session, FakeSession())\n    item = pytester.getitems()[0]\n    excinfo = pytest.raises(AssertionError, item.runtest)\n    item.session = session\n    repr_traceback = item.repr_failure(excinfo)\n    assert \"short test summary info\" in str(repr_traceback)\n", "def test__check_initialpaths_for_relpath_multiple_initial_paths() -> None:\n    \"\"\"Ensure that it handles multiple initial paths.\"\"\"\n    cwd = Path.cwd()\n    sub_dir = cwd / \"sub\"\n\n    class FakeSession:\n        _initialpaths = frozenset({cwd, sub_dir})\n\n    session = cast(pytest.Session, FakeSession)\n\n    assert nodes._check_initialpaths_for_relpath(session, cwd) == \"\"\n    assert nodes._check_initialpaths_for_relpath(session, sub_dir) == \"\"\n    assert nodes._check_initialpaths_for_relpath(session, sub_dir / \"file\") == \"file\"\n", "def test_node_get_closest_marker() -> None:\n    class FakeNode(nodes.Node):\n            super().__init__(name)\n            if own_markers is not None:\n                self.own_markers = own_markers\n\n    node1 = FakeNode(\"node1\", own_markers=[nodes.Mark(name=\"mark1\")])\n    node2 = FakeNode(\"node2\", parent=node1, own_markers=[nodes.Mark(name=\"mark2\")])\n    node3 = FakeNode(\"node3\", parent=node2)\n\n    assert node3.get_closest_marker(\"mark2\").name == \"mark2\"\n    assert node3.get_closest_marker(\"mark1\").name == \"mark1\"\n    assert node3.get_closest_marker(\"mark3\") is None\n", "def test_node_get_closest_marker() -> None:\n    class FakeNode(nodes.Node):\n            super().__init__(name, parent)\n            if own_markers is not None:\n                self.own_markers = own_markers\n\n    marker1 = nodes.Mark(\"marker1\", (), {})\n    marker2 = nodes.Mark(\"marker2\", (), {})\n\n    grandparent = FakeNode(\"grandparent\", own_markers=[marker1])\n    parent = FakeNode(\"parent\", parent=grandparent, own_markers=[marker2])\n    node = FakeNode(\"node\", parent=parent)\n\n    assert node.get_closest_marker(\"marker2\") == marker2\n    assert node.get_closest_marker(\"marker1\") == marker1\n    assert node.get_closest_marker(\"marker3\") is None\n"], "sample_712": ["def test_ordinal_encoder_handle_unknown():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that ordinal encoder raises error for unknown features\n    # present during transform.\n    oh = OrdinalEncoder()\n    oh.fit(X)\n    assert_raises(ValueError, oh.transform, X2)\n\n    # Raise error if handle_unknown is neither ignore or error.\n    oh = OrdinalEncoder(handle_unknown='42')\n    assert_raises(ValueError, oh.fit, X)\n", "def test_ordinal_encoder_handle_unknown():\n    X = np.array([['a', 2], ['b', 1]])\n    enc = OrdinalEncoder()\n\n    # Test that an error is raised when an unknown categorical feature\n    # is present during transform.\n    enc.fit(X)\n    X2 = np.array([['c', 1]])\n    assert_raises(ValueError, enc.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    enc = OrdinalEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(enc.transform(X2_passed), [[0, 0]])\n    # ensure transformed data was not modified in place\n    assert_allclose(X2, X2_passed)\n\n    # Raise error if handle_unknown is neither ignore or error.\n    enc = OrdinalEncoder(handle_unknown='42')\n    assert_raises(ValueError, enc.fit, X)\n", "def test_ordinal_encoder_get_params():\n    enc = OrdinalEncoder()\n    params = enc.get_params()\n    assert 'categories' in params\n    assert 'dtype' in params\n\n    enc = OrdinalEncoder(categories='auto', dtype='int64')\n    params = enc.get_params()\n    assert params['categories'] == 'auto'\n    assert params['dtype'] == 'int64'\n", "def test_ordinal_encoder_set_params():\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[[0, 1, 2, 3]])\n    assert enc.get_params()['categories'] == [[0, 1, 2, 3]]\n    assert_array_equal(enc.fit_transform(X), np.array([[1.], [2.]]))\n    # set params on already fitted object\n    enc.set_params(categories=[[0, 1, 2, 3, 4]])\n    assert_array_equal(enc.fit_transform(X), np.array([[1.], [2.]]))\n", "def test_ordinal_encoder_dtypes():\n    # check that dtypes are preserved when determining categories\n    enc = OrdinalEncoder()\n    exp = np.array([[0, 1], [1, 0]], dtype='float64')\n\n    for X in [np.array([[1, 2], [3, 4]], dtype='int64'),\n              np.array([[1, 2], [3, 4]], dtype='float64'),\n              np.array([['a', 'b'], ['c', 'd']]),  # string dtype\n              np.array([[1, 'a'], [3, 'b']], dtype='object')]:\n        enc.fit(X)\n        assert all([enc.categories_[i].dtype == X.dtype for i in range(2)])\n        assert_array_equal(enc.transform(X), exp)\n\n    X = [[1, 2], [3, 4]]\n    enc.fit(X)\n    assert all([np.issubdtype(enc.categories_[i].dtype, np.integer)\n                for i in range(2)])\n    assert_array_equal(enc.transform(X), exp)\n\n    X = [[1, 'a'], [3, 'b']]\n    enc.fit(X)\n    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])\n    assert_array_equal(enc.transform(X), exp)\n"], "sample_713": ["def test_ridge_classifier_multiclass():\n    # Test multiclass classification using RidgeClassifier\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_classes=4, random_state=42)\n\n    clf = RidgeClassifier()\n    clf.fit(X, y)\n    assert_equal(clf.coef_.shape, (4, 10))\n    assert_equal(clf.intercept_.shape, (4,))\n    y_pred = clf.predict(X)\n    assert_equal(y_pred.shape, (100,))\n", "def test_ridge_classifier_multiclass():\n    # Test multiclass classification\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, n_classes=4,\n                               random_state=42)\n\n    for solver in ['sag', 'saga']:\n        ridge = RidgeClassifier(solver=solver)\n        ridge.fit(X, y)\n        assert_equal(len(ridge.coef_), 4)\n        assert_equal(len(ridge.classes_), 4)\n        y_pred = ridge.predict(X)\n        assert_array_equal(np.unique(y_pred), np.array([0, 1, 2, 3]))\n", "def test_ridge_saga_coordinate_descent_convergence():\n    # Convergence check with SAGA solver for both dense and sparse matrices.\n\n    rng = np.random.RandomState(42)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Create sparse matrix and ensure that the 'sparse' version has more\n    # zeros than the dense version.\n    X_sparse = sp.csr_matrix(X)\n    X_sparse.data[0] = 0.  # Set a few values to zero\n    assert (X_sparse.getnnz() < X.size)\n\n    max_iter = 1000\n    tol = 1e-8\n    params = dict(alpha=alpha, max_iter=max_iter, tol=tol, solver='saga')\n\n    # Make sure that we converge to some similar solution for both sparse\n    # and dense matrices when applicable.\n    ridge_dense = Ridge(**params).fit(X, y)\n    ridge_sparse = Ridge(**params).fit(X_sparse, y)\n    assert_array_almost_equal(ridge_dense.coef_, ridge_sparse.coef_)\n", "def test_ridge_saga_solver():\n    # Test that the saga solver converges to the same solution as other solvers\n    X, y = make_regression(n_samples=1000, n_features=2, n_informative=2,\n                           random_state=42)\n\n    # Compare saga solver with svd solver\n    saga = Ridge(solver='saga', tol=1e-8, max_iter=10000, random_state=42)\n    svd = Ridge(solver='svd')\n\n    saga.fit(X, y)\n    svd.fit(X, y)\n    assert_array_almost_equal(saga.coef_, svd.coef_)\n    assert_almost_equal(saga.intercept_, svd.intercept_)\n\n    # Compare saga solver with sparse_cg solver\n    sparse_cg = Ridge(solver='sparse_cg', tol=1e-8, max_iter=10000)\n    sparse_cg.fit(X, y)\n    assert_array_almost_equal(saga.coef_, sparse_cg.coef_)\n    assert_almost_equal(saga.intercept_, sparse_cg.intercept_)\n", "def test_ridgecv_scorer():\n    # Test that RidgeCV works with a custom scorer\n    from sklearn.metrics import mean_squared_error, make_scorer\n\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n\n    ridge_cv = RidgeCV(scorer=scorer)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'best_score_')\n"], "sample_714": ["def test_balanced_accuracy_score():\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 0, 1]\n\n    # true positives: 2, false positives: 1, true negatives: 3, false negatives: 0\n    # sensitivity = recall = TP / (TP + FN) = 2 / (2 + 0) = 1\n    # specificity = TN / (TN + FP) = 3 / (3 + 1) = 0.75\n    # balanced accuracy = (sensitivity + specificity) / 2 = (1 + 0.75) / 2 = 0.875\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.875)\n", "def test_brier_score_loss():\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.3, 0.4, 0.7, 0.2]\n\n    # for true labels, lower is better\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.2375)\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label=1), 0.2375)\n\n    # for predicted probabilities, closer to true label is better\n    assert_almost_equal(brier_score_loss(y_true, [0, 0, 1, 0]), 0)\n    assert_almost_equal(brier_score_loss(y_true, [0.5, 0.5, 0.5, 0.5]), 0.25)\n\n    # assert that the function handles sample weights correctly\n    y_true = [0, 0, 1, 1, 1]\n    y_prob = [0.1, 0.2, 0.6, 0.8, 0.9]\n    sample_weight = [0.1, 0.2, 0.3, 0.2, 0.2]\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight),\n                        np.average((np.array(y_true) - np.array(y_prob)) ** 2,\n                                   weights=sample_weight))\n\n    # check that y_true can be a string array\n    y_true = ['a', 'b', 'b']\n    y_prob = [0.3, 0.4, 0.7]\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label='b'), 0.13)\n", "def test_log_loss_labels_mismatch():\n    y_true = [1, 2, 3]\n    y_pred = [[0.2, 0.7, 0.1], [0.6, 0.5, 0.3], [0.4, 0.1, 0.5]]\n    labels = [1, 2]\n\n    error_str = (\"The number of classes in labels is different from that \"\n                 \"in y_pred. Classes found in labels: {0}\".format(labels))\n    assert_raise_message(ValueError, error_str, log_loss, y_true, y_pred,\n                         labels=labels)\n", "def test_brier_score_loss():\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    sample_weight = [0.5, 1., 0.5, 1.]\n\n    # without sample weights\n    assert_almost_equal(brier_score_loss(y_true, y_prob),\n                        np.mean(np.array(y_prob) - np.array(y_true)) ** 2)\n\n    # with sample weights\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight),\n                        np.average((np.array(y_prob) - np.array(y_true)) ** 2,\n                                   weights=sample_weight))\n\n    # check perfect score\n    y_prob = [0., 1., 1., 0.]\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.)\n\n    # check 0 and 1 probabilities\n    y_prob = [0., 1., 0., 1.]\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.25)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.037, decimal=3)\n    assert_almost_equal(brier_score_loss(y_true, 1 - y_prob, pos_label=0),\n                        0.037, decimal=3)\n\n    y_true = np.array([0, 2, 1, 1])\n    y_prob = np.array([[0.1, 0.5, 0.4], [0.2, 0.7, 0.1], [0.9, 0.05, 0.05],\n                       [0.15, 0.75, 0.1]])\n    error_str = ('The number of classes in labels is different '\n                 'from that in y_pred. Classes found in '\n                 'labels: [0 1 2]')\n    assert_raise_message(ValueError, error_str, brier_score_loss, y_true,\n                         y_prob[:, :2])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.508, decimal=3)\n\n    # Test with a single-element array-like for y_true\n    y_true = [0]\n    y_prob = [[0.1, 0.9]]\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.01, decimal=3)\n\n    # Test with a pandas DataFrame and Series\n    try:\n        from pandas import DataFrame, Series\n    except ImportError:\n        return\n\n    y_true = Series(y_true)\n    y_prob = DataFrame(y_prob)\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.01, decimal=3)\n"], "sample_715": ["def test_cross_val_score_multimetric():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    # Test multimetric scoring\n    scoring = ['accuracy', 'precision', 'recall']\n    scores = cross_val_score(SVC(), X, y, scoring=scoring, cv=5)\n    assert_equal(len(scores), 3)\n    for score in scores:\n        assert_equal(len(score), 5)\n\n    # Test multimetric scoring with dict\n    scoring = {'accuracy': 'accuracy', 'precision': 'precision'}\n    scores = cross_val_score(SVC(), X, y, scoring=scoring, cv=5)\n    assert_equal(len(scores), 2)\n    for score in scores.values():\n        assert_equal(len(score), 5)\n", "def test_cross_validate_return_estimator():\n    # Test that cross_validate returns the estimator when return_estimator=True\n    X, y = make_classification(random_state=0)\n    estimator = MockClassifier()\n    cv_results = cross_validate(estimator, X, y, return_estimator=True)\n    assert 'estimator' in cv_results\n    assert len(cv_results['estimator']) == 5  # default cv is 5-fold\n", "def test_cross_val_score_fit_params_callback():\n    clf = MockClassifier()\n    n_samples = X.shape[0]\n    n_classes = len(np.unique(y))\n\n        estimator.callback_called_ = True\n\n    fit_params = {'sample_weight': np.ones(n_samples),\n                  'class_prior': np.ones(n_classes) / n_classes,\n                  'callback': callback}\n\n    cross_val_score(clf, X, y, fit_params=fit_params)\n    assert hasattr(clf, 'callback_called_')\n    assert clf.callback_called_\n", "def test_cross_val_score_sparse_scoring():\n    # check that cross_val_score works with sparse scoring\n    from sklearn.metrics import f1_score\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    scoring = make_scorer(f1_score, average='macro')\n    cv = KFold(n_splits=5)\n    scores = cross_val_score(clf, X, y, scoring=scoring, cv=cv)\n    assert_array_almost_equal(scores, [0.88, 0.88, 0.88, 0.87, 0.86], 2)\n", "def test_cross_val_score_fit_params_callback():\n    X, y = make_classification(n_samples=10, n_features=1, random_state=0)\n    clf = MockClassifier()\n\n        estimator.callback_called = True\n\n    scores = cross_val_score(clf, X, y, fit_params={'callback': callback})\n    assert_true(hasattr(clf, 'callback_called'))\n    assert_true(clf.callback_called)\n"], "sample_716": ["def test_ridge_intercept_scaling():\n    # Test that the intercept in Ridge does not change when scaling the data.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 10\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    ridge = Ridge(fit_intercept=True)\n    ridge.fit(X, y)\n    intercept = ridge.intercept_\n\n    X_scaled = 2 * X\n    ridge.fit(X_scaled, y)\n    assert_almost_equal(intercept, ridge.intercept_)\n", "def test_ridgecv_alphas_per_target():\n    # Test that RidgeCV can accept a 2D array of alphas, with each row corresponding to a target variable\n    X, y = make_regression(n_samples=100, n_features=10, n_targets=3)\n    alphas = np.array([[0.1, 1, 10], [0.01, 0.1, 1], [0.001, 0.01, 0.1]])\n    ridgecv = RidgeCV(alphas=alphas, cv=5)\n    ridgecv.fit(X, y)\n    assert_equal(len(ridgecv.alpha_), 3)\n", "def test_ridge_cv_with_scoring():\n    # Test that RidgeCV works with scoring parameter.\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n\n    # Define custom scorer with higher is better\n        y_pred = estimator.predict(X)\n        return np.mean(np.abs(y - y_pred))\n\n    scoring = make_scorer(custom_scorer, greater_is_better=False)\n\n    ridge_cv = RidgeCV(scoring=scoring, cv=5)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'best_score_')\n", "def test_ridgecv_class_weight():\n    # Test class weights in RidgeCV.\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               random_state=42)\n\n    # balanced class weights\n    ridge = RidgeClassifierCV(class_weight='balanced')\n    ridge.fit(X, y)\n    assert_equal(len(ridge.classes_), 2)\n\n    # custom class weights\n    class_weight = {0: 1., 1: 10.}\n    ridge = RidgeClassifierCV(class_weight=class_weight)\n    ridge.fit(X, y)\n    assert_equal(len(ridge.classes_), 2)\n", "def test_ridge_cv_return_n_iter():\n    # Test that RidgeCV returns the correct n_iter_\n    rng = np.random.RandomState(42)\n    X, y = make_regression(n_samples=1000, n_features=10, random_state=rng)\n    ridge_cv = RidgeCV(solver='sag', cv=5, return_n_iter=True)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'n_iter_')\n    assert isinstance(ridge_cv.n_iter_, np.ndarray)\n    assert ridge_cv.n_iter_.shape == (len(ridge_cv.alphas),)\n"], "sample_717": ["def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                subset='train',\n                                download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (5, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (5, 5828))\n\n    # the target is array of boolean values: same or different persons\n    assert_array_equal(lfw_pairs.target, [True, False, True, False, True])\n\n    # names of the classes can be found using the target_names array\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs.target_names, expected_classes)\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, subset='train',\n                                download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (5, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (5, 5828))\n\n    # the target is array of booleans indicating whether each pair is the same\n    # person or not\n    assert_array_equal(lfw_pairs.target, [1, 0, 1, 0, 1])\n\n    # names of the target can be found using the target_names array\n    expected_target_names = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs.target_names, expected_target_names)\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                subset='train',\n                                download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (5, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (5, 5828))\n\n    # the target is array of booleans indicating whether the pair of pictures\n    # are of the same person or not\n    assert_array_equal(lfw_pairs.target, [True, False, True, False, True])\n\n    # names of the targets can be found using the target_names array\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs.target_names, expected_classes)\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (5, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (5, 5828))\n\n    # the target is array of booleans: same or not same person\n    assert_array_equal(lfw_pairs.target, [1, 0, 1, 0, 0])\n\n    # names of the persons can be found using the target_names array\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs.target_names, expected_classes)\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, subset='train',\n                                download_if_missing=False)\n\n    # The data is croped around the center as a rectangular bounding box\n    # around the face. Colors are converted to gray levels:\n    assert_equal(lfw_pairs.pairs.shape, (5, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (5, 5828))\n\n    # the target is array of boolean values indicating whether the pair of\n    # pictures are from the same person\n    assert_array_equal(lfw_pairs.target, [1, 0, 1, 0, 1])\n\n    # names of the target can be found using the target_names array\n    expected_classes = ['Different persons', 'Same person']\n    assert_array_equal(lfw_pairs.target_names, expected_classes)\n"], "sample_718": ["def test_check_estimator_n_iter():\n    # check that estimators with a parameter max_iter return the attribute of\n    # n_iter_ at least 1.\n\n    # tests that the estimator actually fails on \"bad\" estimators.\n    # not a complete test of all checks, which are very extensive.\n\n    class NonConformantEstimatorNoIter(object):\n            self.max_iter = max_iter\n\n            return self\n\n    assert_raises_regex(AssertionError,\n                        \"Estimator estimator_name should have a\"\n                        \" 'n_iter_' attribute after fitting.\",\n                        check_non_transformer_estimators_n_iter,\n                        'estimator_name',\n                        NonConformantEstimatorNoIter(max_iter=100))\n\n    class NonConformantEstimatorNoIterAttr(object):\n            self.max_iter = max_iter\n\n            self.n_iter_ = 0\n            return self\n\n    assert_raises_regex(AssertionError,\n                        \"n_iter_ must be greater than zero\",\n                        check_non_transformer_estimators_n_iter,\n                        'estimator_name',\n                        NonConformantEstimatorNoIterAttr(max_iter=100))\n", "def test_check_estimator_sparse_data():\n    # check that estimators raise an exception on providing sparse data\n    X = np.random.sample(10) + 1j * np.random.sample(10)\n    X = X.reshape(-1, 1)\n    y = np.random.sample(10) + 1j * np.random.sample(10)\n    estimator = SVC()\n    assert_raises_regex(ValueError, \"Complex data not supported\",\n                        estimator.fit, X, y)\n\n    X = np.random.sample(10).reshape(-1, 1)\n    y = np.random.sample(10).reshape(-1, 1)\n    estimator = SVC()\n    set_random_state(estimator)\n    estimator.fit(X, y)\n    for method in [\"predict\", \"decision_function\", \"score\"]:\n        if hasattr(estimator, method):\n            assert_raise_message(ValueError, \"Reshape your data\",\n                                 getattr(estimator, method), X[0])\n", "def test_check_estimators_overwrite_params():\n    # check that estimators do not change or mutate the parameter array\n    # during fit\n\n    class OverwritesParam(BaseEstimator):\n            self.param = param\n\n            self.param[0] = 0\n            return self\n\n    msg = ('Estimator OverwritesParam should not change or mutate  '\n           'the parameter param from [1 2 3] to [0 2 3] during fit.')\n    assert_raises_regex(AssertionError, msg,\n                        check_estimator, OverwritesParam())\n", "def test_check_estimator_sparse_data():\n    # check that estimators can handle sparse data correctly\n\n    # check that sparse data is passed correctly\n    X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y = np.array([1, 2, 3])\n    X_sparse = sp.csr_matrix(X)\n\n    for Estimator in [SVC, KNeighborsRegressor]:\n        est = Estimator()\n        set_random_state(est)\n        est.fit(X, y)\n        est_sparse = Estimator()\n        set_random_state(est_sparse)\n        est_sparse.fit(X_sparse, y)\n\n        assert_array_equal(est.predict(X), est_sparse.predict(X_sparse))\n", "def test_check_estimator_pairwise_distances():\n    # check that check_estimator() works on estimator with _pairwise_distances\n\n    est = SVC(kernel='rbf', gamma=0.1)\n    check_estimator(est)\n\n    est = KNeighborsRegressor(metric='minkowski')\n    check_estimator(est)\n"], "sample_719": ["def test_vectorizer_input_validation():\n    # Test that invalid input types raise an error\n    vectorizer = CountVectorizer()\n    \n    # Test that a list of integers raises an error\n    with pytest.raises(ValueError):\n        vectorizer.fit_transform([1, 2, 3])\n        \n    # Test that a numpy array of integers raises an error\n    with pytest.raises(ValueError):\n        vectorizer.fit_transform(np.array([1, 2, 3]))\n        \n    # Test that a pandas DataFrame with integer columns raises an error\n    import pandas as pd\n    df = pd.DataFrame({'a': [1, 2, 3]})\n    with pytest.raises(ValueError):\n        vectorizer.fit_transform(df)\n        \n    # Test that a list of floats raises an error\n    with pytest.raises(ValueError):\n        vectorizer.fit_transform([1.0, 2.0, 3.0])\n", "def test_countvectorizer_get_stop_words():\n    cv = CountVectorizer()\n    stop_words = cv.get_stop_words()\n    assert_equal(stop_words, ENGLISH_STOP_WORDS)\n    cv.set_params(stop_words='english')\n    stop_words = cv.get_stop_words()\n    assert_equal(stop_words, ENGLISH_STOP_WORDS)\n", "def test_vectorizer_input_type():\n    # Test input type validation for fit and transform methods\n\n    class InvalidInput:\n            yield \"This is a test string\"\n\n    vectorizers = [\n        CountVectorizer(),\n        TfidfVectorizer(),\n        HashingVectorizer()\n    ]\n\n    invalid_inputs = [\n        123,\n        456.789,\n        None,\n        InvalidInput(),\n        [\"string\", 123, \"another string\"],\n        (\"string\", 123, \"another string\"),\n        {\"key\": \"value\"}\n    ]\n\n    for vectorizer in vectorizers:\n        for invalid_input in invalid_inputs:\n            assert_raises(ValueError, vectorizer.fit, invalid_input)\n            assert_raises(ValueError, vectorizer.transform, invalid_input)\n", "def test_tfidf_vectorizer_with_empty_string():\n    # Non-regression test: TfidfVectorizer used to raise an exception when\n    # encountering an empty string.\n    v = TfidfVectorizer()\n    X = v.fit_transform(['hello world', '', 'hello hello'])\n    assert_array_equal(X.toarray().shape, (3, 2))\n", "def test_vectorizer_dtype():\n    X = [\"numpy\", \"scipy\", \"sklearn\"]\n    vectorizer = CountVectorizer(dtype=np.float32)\n    X_count = vectorizer.fit_transform(X)\n    assert X_count.dtype == np.float32\n\n    vectorizer = TfidfVectorizer(dtype=np.float64)\n    X_tfidf = vectorizer.fit_transform(X)\n    assert X_tfidf.dtype == np.float64\n\n    # test that setting dtype on TfidfTransformer also works\n    transformer = TfidfTransformer(dtype=np.float32)\n    X_tfidf = transformer.fit_transform(X_count)\n    assert X_tfidf.dtype == np.float32\n"], "sample_721": ["def test_check_memory():\n    # Test that check_memory returns a Memory object when given a string or None\n    memory = check_memory('cachedir')\n    assert isinstance(memory, Memory)\n    memory = check_memory(None)\n    assert isinstance(memory, Memory)\n\n    # Test that check_memory raises an error when given an invalid input\n    assert_raises_regex(ValueError, \"memory should be None, a string or have\",\n                        check_memory, 1)\n    assert_raises_regex(ValueError, \"memory should be None, a string or have\",\n                        check_memory, [1])\n\n    # Test that check_memory returns the input when given a valid Memory object\n    class DummyMemory:\n            return func\n    dummy_memory = DummyMemory()\n    memory = check_memory(dummy_memory)\n    assert memory is dummy_memory\n", "def test_check_memory():\n    # Test that check_memory returns a joblib.Memory instance\n    memory = check_memory(None)\n    assert isinstance(memory, Memory)\n\n    # Test that check_memory raises an error for invalid input\n    assert_raises_regex(ValueError, \"should be None, a string or have the same\",\n                        check_memory, 1)\n\n    # Test that check_memory accepts a joblib.Memory instance\n    memory = Memory(cachedir=None, verbose=0)\n    checked_memory = check_memory(memory)\n    assert checked_memory is memory\n\n    # Test that check_memory accepts a string\n    memory = check_memory(\"cachedir\")\n    assert isinstance(memory, Memory)\n    assert memory.cachedir == \"cachedir\"\n", "def test_check_memory():\n    # Test that check_memory returns a Memory object with the correct location.\n    memory = check_memory('test_cache')\n    assert isinstance(memory, Memory)\n    assert memory.location == 'test_cache'\n\n    # Test that check_memory returns the same Memory object when given the same\n    # location.\n    memory2 = check_memory('test_cache')\n    assert memory is memory2\n\n    # Test that check_memory raises an error when given an invalid location.\n    assert_raises_regex(ValueError, \"Memory location must be a string or None\",\n                        check_memory, 123)\n\n    # Test that check_memory warns when given a non-existent location.\n    assert_warns_message(UserWarning, \"Path 'non_existent_location' does not exist.\",\n                         check_memory, 'non_existent_location')\n\n    # Test that check_memory accepts a Memory object as input.\n    memory3 = Memory(location='test_cache')\n    memory4 = check_memory(memory3)\n    assert memory3 is memory4\n\n    # Test that check_memory returns the same Memory object when given None as\n    # input.\n    memory5 = check_memory(None)\n    assert isinstance(memory5, Memory)\n    memory6 = check_memory(None)\n    assert memory5 is memory6\n", "def test_check_memory():\n    # Test that check_memory returns a joblib.Memory instance\n    memory = check_memory(None)\n    assert isinstance(memory, Memory)\n\n    # Test that check_memory raises an error for invalid input\n    assert_raises_regex(ValueError, 'should be None, a string or have the same',\n                        check_memory, 1)\n\n    # Test that check_memory warns for deprecated cache dir\n    with warnings.catch_warnings(record=True) as w:\n        check_memory('/tmp/sklearn')\n        assert len(w) == 1\n        assert 'DeprecationWarning' in str(w[0])\n\n    # Test that check_memory accepts joblib.Memory instances\n    memory = Memory(location='/tmp/sklearn', verbose=0)\n    checked_memory = check_memory(memory)\n    assert checked_memory is memory\n\n    # Test that check_memory raises an error for objects without a cache method\n    class DummyMemory(object):\n        pass\n    assert_raises_regex(ValueError, 'should be None, a string or have the same',\n                        check_memory, DummyMemory())\n", "def test_check_memory():\n    # Test that check_memory returns a joblib.Memory instance\n    memory = check_memory(None)\n    assert isinstance(memory, Memory)\n\n    # Test that check_memory raises an error when memory is not None or str\n    assert_raises_regex(ValueError, \"should be None, a string or have the same\",\n                        check_memory, 1)\n\n    # Test that check_memory returns the input if it's already a Memory instance\n    memory = Memory(location='~/cachedir')\n    result = check_memory(memory)\n    assert result is memory\n\n    # Test that check_memory creates a new Memory instance from a string\n    result = check_memory('~/cachedir')\n    assert isinstance(result, Memory)\n    assert result.location == os.path.expanduser('~/cachedir')\n\n    # Test that check_memory raises an error when memory has no 'cache' method\n    class Dummy:\n        pass\n    assert_raises_regex(ValueError, \"'memory' should be None, a string or have the same\",\n                        check_memory, Dummy())\n"], "sample_722": ["def test_k_means_init_callable():\n    # This test is used to check KMeans with a callable init parameter\n\n        # Custom initialization for the centroids\n        return X[:k]\n\n    km = KMeans(init=custom_init, n_clusters=3, n_init=1)\n    km.fit(X)\n\n    # Check that the number of clusters centers and distinct labels match\n    # the expectation\n    assert_equal(km.cluster_centers_.shape, (3, n_features))\n    assert_equal(np.unique(km.labels_).shape[0], 3)\n", "def test_k_means_n_init_no_warning():\n    # check that using n_init > 1 does not raise a warning when init is an array\n    X_small = np.array([[1.1, 1.1], [-7.5, -7.5], [-1.1, -1.1], [7.5, 7.5]])\n    init_centers = np.array([[0.0, 0.0], [5.0, 5.0]])\n    km = KMeans(init=init_centers, n_clusters=2, n_init=10)\n    with warnings.catch_warnings(record=True) as w:\n        km.fit(X_small)\n        assert len(w) == 0\n", "def test_k_means_n_clusters_attr():\n    # Check that the n_clusters attribute is set after fitting\n    km = KMeans(n_clusters=n_clusters, random_state=42)\n    assert not hasattr(km, 'n_clusters_')\n    km.fit(X)\n    assert hasattr(km, 'n_clusters_')\n    assert_equal(km.n_clusters_, n_clusters)\n", "def test_k_means_warning_convergence():\n    # Test that a warning is raised when convergence is not reached after max_iter\n    km = KMeans(max_iter=1, n_clusters=n_clusters)\n    assert_warns(ConvergenceWarning, km.fit, X)\n", "def test_k_means_n_jobs():\n    # Check that using n_jobs > 1 works correctly\n    X, _ = make_blobs(n_samples=1000, centers=4, cluster_std=0.8,\n                      random_state=0)\n    for algorithm in ['full', 'elkan']:\n        km = KMeans(n_clusters=4, n_init=10, random_state=42, algorithm=algorithm)\n        km.fit(X)\n        labels_single = km.labels_\n\n        km = KMeans(n_clusters=4, n_init=10, random_state=42, n_jobs=2, algorithm=algorithm)\n        km.fit(X)\n        labels_multi = km.labels_\n\n        assert_array_equal(labels_single, labels_multi)\n"], "sample_723": ["def test_imputation_copy():\n    # Test that imputation doesn't modify the original array when copy=True.\n    X = np.array([\n        [1, 2],\n        [np.nan, 3],\n        [7, 6]\n    ])\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=True)\n        X_imputed = imputer.fit_transform(X)\n\n        assert_false(np.may_share_memory(X, X_imputed))\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=False)\n        X_imputed = imputer.fit_transform(X)\n\n        assert_array_equal(X, X_imputed)\n", "def test_imputation_copy():\n    # Test that copy was done correctly\n    X = sparse_random_matrix(100, 100, density=0.10).toarray()\n    X[:, 0] = np.nan\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=True)\n        X_imputed = imputer.fit_transform(X)\n\n        assert_false(np.isnan(X_imputed).any())\n\n        # Set a new value for the original matrix\n        X[0, 0] = 0\n\n        # Check that the imputed matrix was not modified\n        assert_false(np.isnan(X_imputed).any())\n        assert_array_almost_equal(imputer.transform(X),\n                                  X_imputed,\n                                  err_msg=\"Data modified: \"\n                                  \"(strategy = %s)\" % (strategy))\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=False)\n        X_imputed = imputer.fit_transform(X.copy())\n\n        assert_false(np.isnan(X_imputed).any())\n\n        # Set a new value for the original matrix\n        X[0, 0] = 0\n\n        # Check that the imputed matrix was modified\n        assert_array_almost_equal(imputer.transform(X),\n                                  X_imputed,\n                                  err_msg=\"Data not modified: \"\n                                  \"(strategy = %s)\" % (strategy))\n", "def test_imputation_copy():\n    # Test that copy was done correctly\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=True)\n        X_imputed = imputer.fit_transform(X)\n\n        assert_false(np.may_share_memory(X, X_imputed))\n\n        imputer = SimpleImputer(strategy=strategy, copy=False)\n        X_imputed = imputer.fit_transform(X)\n\n        assert np.may_share_memory(X, X_imputed)\n", "def test_imputation_copy():\n    # Test that imputation doesn't modify the original array when copy=True.\n    X = np.array([\n        [np.nan, 0, 0],\n        [1, np.nan, -1],\n        [1, 1, np.nan]\n    ])\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=True)\n        X_imputed = imputer.fit_transform(X)\n\n        assert_false(np.array_equal(X, X_imputed))\n        assert_array_equal(X, np.array([\n            [np.nan, 0, 0],\n            [1, np.nan, -1],\n            [1, 1, np.nan]\n        ]))\n\n    # Test that imputation modifies the original array when copy=False.\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=False)\n        X_imputed = imputer.fit_transform(X)\n\n        assert_array_equal(X, X_imputed)\n", "def test_imputation_copy():\n    # Test that copy was done correctly\n    X = np.array([[1, 2], [np.nan, 6]])\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=True)\n        X_imputed = imputer.fit_transform(X)\n\n        assert_false(np.may_share_memory(X, X_imputed))\n\n        X_imputed_again = imputer.transform(X)\n        assert_array_equal(X_imputed, X_imputed_again)\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = SimpleImputer(strategy=strategy, copy=False)\n        X_imputed = imputer.fit_transform(X)\n\n        assert_true(np.may_share_memory(X, X_imputed))\n\n        X_imputed_again = imputer.transform(X)\n        assert_array_equal(X_imputed, X_imputed_again)\n"], "sample_724": ["def test_imputation_copy():\n    # Test that imputation doesn't modify the original array when copy=True.\n    X = np.array([\n        [np.nan, 0, 0, 0, 5],\n        [np.nan, 1, 0, np.nan, 3],\n        [np.nan, 2, 0, 0, 0],\n        [np.nan, 6, 0, 5, 13],\n    ])\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(strategy=strategy)\n        X_imputed = imputer.fit_transform(X.copy())\n        assert_false(np.isnan(X_imputed).any())\n        assert_array_equal(X, np.array([\n            [np.nan, 0, 0, 0, 5],\n            [np.nan, 1, 0, np.nan, 3],\n            [np.nan, 2, 0, 0, 0],\n            [np.nan, 6, 0, 5, 13],\n        ]))\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(strategy=strategy)\n        X_imputed = imputer.fit_transform(sparse.csr_matrix(X))\n        assert_false(np.isnan(X_imputed.toarray()).any())\n        assert_array_equal(X, np.array([\n            [np.nan, 0, 0, 0, 5],\n            [np.nan, 1, 0, np.nan, 3],\n            [np.nan, 2, 0, 0, 0],\n            [np.nan, 6, 0, 5, 13],\n        ]))\n", "def test_imputation_copy():\n    # Test that imputation modifies or does not modify the input depending on copy.\n    X = np.array([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ])\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(missing_values=-1, strategy=strategy)\n        X_transformed = imputer.fit_transform(X.copy())\n\n        imputer_copy = Imputer(missing_values=-1, strategy=strategy, copy=False)\n        X_copy_transformed = imputer_copy.fit_transform(X)\n\n        assert_array_almost_equal(\n            X_transformed,\n            X_copy_transformed,\n            err_msg=\"Copy and non-copy transform should have same output \"\n            \"(strategy = %s)\" % (strategy)\n        )\n\n        assert_false(np.allclose(X, X_copy_transformed),\n                     msg=\"Non-copy transform should modify the input \"\n                         \"(strategy = %s)\" % (strategy))\n\n        assert_array_almost_equal(\n            X,\n            imputer.transform(X.copy()),\n            err_msg=\"Second transform should be the same as the first \"\n            \"(strategy = %s)\" % (strategy)\n        )\n", "def test_imputation_copy():\n    # Test that transform can modify the input when copy=False.\n    X = np.array([\n        [1, 2],\n        [np.nan, 3],\n        [7, 6]\n    ])\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(strategy=strategy, axis=0, copy=False)\n        X_transformed = imputer.fit_transform(X)\n        assert_false(np.isnan(X).any())\n\n    # compare X_transformed and modified X to be sure they are equal\n    assert_array_equal(X, X_transformed)\n", "def test_imputation_copy():\n    # Test that transform does not modify the input when copy=True.\n    X = np.array([[1, 2], [np.nan, 3]])\n    imputer = Imputer(missing_values=np.nan, strategy='mean')\n    imputer.fit(X)\n\n    X_transformed = imputer.transform(X.copy())\n    assert_false(np.isnan(X_transformed).any())\n\n    # check that the original array was not modified\n    assert_array_equal(X, np.array([[1, 2], [np.nan, 3]]))\n", "def test_imputation_axis_parameter():\n    # Test imputation with different axis parameters.\n    X = np.array([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [np.nan, 8, 9]\n    ])\n\n    X_imputed_axis0 = np.array([\n        [1, 5, 7.5],\n        [4, 5, 6],\n        [2.5, 8, 9]\n    ])\n\n    X_imputed_axis1 = np.array([\n        [1, 2, 1.5],\n        [4, 5, 6],\n        [8.5, 8, 9]\n    ])\n\n    assert_array_almost_equal(Imputer(missing_values=np.nan, strategy=\"mean\", axis=0).fit_transform(X), X_imputed_axis0)\n    assert_array_almost_equal(Imputer(missing_values=np.nan, strategy=\"mean\", axis=1).fit_transform(X), X_imputed_axis1)\n\n    X_sparse = sparse.csr_matrix(X)\n    assert_array_almost_equal(Imputer(missing_values=np.nan, strategy=\"mean\", axis=0).fit_transform(X_sparse).toarray(), X_imputed_axis0)\n    assert_array_almost_equal(Imputer(missing_values=np.nan, strategy=\"mean\", axis=1).fit_transform(X_sparse).toarray(), X_imputed_axis1)\n"], "sample_725": ["def test_check_memory():\n    memory = DummyMemory()\n    assert_true(isinstance(check_memory(memory), DummyMemory))\n    memory2 = check_memory(memory)\n    assert_true(memory is memory2)\n\n    memory = \"path/to/cache\"\n    assert_true(isinstance(check_memory(memory), Memory))\n\n    memory = None\n    assert_true(isinstance(check_memory(memory), Memory))\n\n    class WrongMemory(object):\n        pass\n\n    memory = WrongMemory()\n    assert_raises_regex(ValueError, 'memory should be', check_memory, memory)\n", "def test_check_memory():\n    # Test that check_memory returns a joblib.Memory instance\n    memory = check_memory(None)\n    assert isinstance(memory, Memory)\n\n    # Test that check_memory raises an error for invalid input\n    assert_raises_regex(ValueError, \".*should be None.*\", check_memory, 1)\n\n    # Test that check_memory returns the input if it's already a joblib.Memory instance\n    memory = Memory(location='/tmp/joblib', verbose=0)\n    assert check_memory(memory) is memory\n\n    # Test that check_memory returns a joblib.Memory instance when given a string\n    memory = check_memory('/tmp/joblib')\n    assert isinstance(memory, Memory)\n    assert memory.location == '/tmp/joblib'\n\n    # Test that check_memory raises an error for an object without a cache method\n    class NoCache:\n        pass\n    assert_raises_regex(ValueError, \".*should have the same interface.*\", check_memory, NoCache())\n\n    # Test that check_memory raises an error for an object with a cache method but not the right interface\n    class WrongCache:\n            pass\n    assert_raises_regex(ValueError, \".*should have the same interface.*\", check_memory, WrongCache())\n", "def test_check_memory():\n    # Test that check_memory returns the input unchanged if it's None or has a\n    # cache method\n    assert check_memory(None) is None\n\n    class CacheMock:\n            return func\n\n    cache_mock = CacheMock()\n    assert check_memory(cache_mock) is cache_mock\n\n    # Test that check_memory raises an error if the input is not None and does\n    # not have a cache method\n    assert_raises_regex(ValueError,\n                        \"'memory' should be None, a string or have the same \"\n                        \"interface as sklearn.externals.joblib.Memory.\",\n                        check_memory, object())\n\n    # Test that check_memory creates a Memory instance from a string\n    memory = check_memory(\"cachedir\")\n    assert isinstance(memory, Memory)\n    assert memory.location == \"cachedir\"\n", "def test_check_memory():\n    # Test that check_memory returns a Memory object\n    memory = check_memory(None)\n    assert isinstance(memory, Memory)\n\n    # Test that check_memory raises an error for invalid input\n    assert_raises_regex(ValueError, \".*should be.*\", check_memory, 1)\n\n    # Test that check_memory returns the input if it's already a Memory object\n    dummy_memory = DummyMemory()\n    assert check_memory(dummy_memory) is dummy_memory\n\n    # Test that check_memory accepts string as input\n    temp_dir = os.path.join(os.getcwd(), 'temp_cache')\n    try:\n        memory = check_memory(temp_dir)\n        assert isinstance(memory, Memory)\n    finally:\n        # Clean up the temporary cache directory\n        import shutil\n        shutil.rmtree(temp_dir, ignore_errors=True)\n", "def test_check_memory():\n    # Test that check_memory returns the input if it's already a Memory instance\n    memory = Memory(cachedir=None, verbose=0)\n    assert_true(check_memory(memory) is memory)\n\n    # Test that check_memory returns a Memory instance when given a string\n    cache_dir = \"test_cache\"\n    memory = check_memory(cache_dir)\n    assert_true(isinstance(memory, Memory))\n    assert_equal(memory.cachedir, cache_dir)\n\n    # Test that check_memory raises an error when given an invalid input\n    assert_raises_regex(ValueError, \"memory should be None, a string or have the same interface as sklearn.externals.joblib.Memory\",\n                        check_memory, 1)\n"], "sample_726": ["def test_label_binarize_sparse_input_empty_row():\n    # Test that label_binarize handles sparse input with empty rows correctly\n    y = csr_matrix(np.array([[0, 1], [0, 0]]))\n    classes = [0, 1]\n    expected = np.array([[0, 1], [0, 0]])\n\n    binarized = label_binarize(y, classes)\n    assert_array_equal(binarized.toarray(), expected)\n\n    # Check inverse\n    y_type = type_of_target(y)\n    inversed = _inverse_binarize_thresholding(binarized,\n                                              output_type=y_type,\n                                              classes=classes,\n                                              threshold=0.5)\n    assert_array_equal(inversed, toarray(y))\n", "def test_label_binarize_empty_input():\n    y = []\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n\n    expected = np.array([]).reshape(0, len(classes))\n\n    binarized = label_binarize(y, classes, neg_label=neg_label,\n                               pos_label=pos_label)\n    assert_array_equal(binarized, expected)\n\n    lb = LabelBinarizer(neg_label=neg_label, pos_label=pos_label)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(binarized, expected)\n", "def test_label_binarize_empty_input():\n    y = []\n    classes = [0, 1, 2]\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([]).reshape(0, len(classes))\n\n    binarized = label_binarize(y, classes, neg_label=neg_label,\n                               pos_label=pos_label)\n    assert_array_equal(binarized, expected)\n\n    lb = LabelBinarizer(neg_label=neg_label, pos_label=pos_label)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(binarized, expected)\n", "def test_label_encoder_with_numpy_array():\n    # Test LabelEncoder's fit, transform and inverse_transform methods with numpy arrays\n    le = LabelEncoder()\n    y = np.array([1, 1, 4, 5, -1, 0])\n    le.fit(y)\n    assert_array_equal(le.classes_, [-1, 0, 1, 4, 5])\n    assert_array_equal(le.transform(y), [2, 2, 3, 4, 0, 1])\n    assert_array_equal(le.inverse_transform(le.transform(y)), y)\n\n    # Test fit_transform\n    le = LabelEncoder()\n    ret = le.fit_transform(y)\n    assert_array_equal(ret, [2, 2, 3, 4, 0, 1])\n\n    # Test with non-integer values\n    y = np.array(['a', 'b', 'c'])\n    le = LabelEncoder()\n    le.fit(y)\n    assert_array_equal(le.classes_, ['a', 'b', 'c'])\n    assert_array_equal(le.transform(y), [0, 1, 2])\n    assert_array_equal(le.inverse_transform(le.transform(y)), y)\n", "def test_label_binarize_unknown_type():\n    y = np.array([[1, 2], [3, 4]])\n    classes = [1, 2, 3, 4]\n    assert_raises(ValueError, label_binarize, y, classes)\n"], "sample_727": ["def test_imputation_axis_deprecated():\n    # Test deprecation of axis parameter in Imputer\n    X = np.array([[1, 2], [np.nan, 3]])\n\n    imputer = Imputer(strategy='mean', axis=0)\n    assert_warns_message(DeprecationWarning,\n                         \"Parameter 'axis' has been deprecated\",\n                         imputer.fit, X)\n\n    imputer = Imputer(strategy='mean', axis=1)\n    assert_warns_message(DeprecationWarning,\n                         \"Parameter 'axis' has been deprecated\",\n                         imputer.fit, X)\n\n    imputer = Imputer(strategy='mean')\n    imputer.fit(X)\n    assert_equal(imputer.axis, None)\n    assert_equal(imputer._axis, 0)\n", "def test_imputation_deprecated_axis():\n    # Test imputation with deprecated axis parameter.\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n\n    imputer = Imputer(strategy=\"mean\", axis=0)\n    assert_warns_message(DeprecationWarning, \"Parameter 'axis' has been \"\n                         \"deprecated in 0.20 and will be removed in 0.22.\",\n                         imputer.fit, X)\n\n    imputer = Imputer(strategy=\"mean\", axis=None)\n    imputer.fit(X)\n    assert_equal(imputer._axis, 0)\n\n    imputer = Imputer(strategy=\"mean\")\n    imputer.fit(X)\n    assert_equal(imputer._axis, 0)\n", "def test_imputation_axis_deprecation():\n    # Test that axis parameter is deprecated\n    X = np.array([\n        [1, 2],\n        [np.nan, 3],\n        [7, 6]\n    ])\n\n    imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0)\n    assert_warns_message(DeprecationWarning,\n                         \"Parameter 'axis' has been deprecated\",\n                         imputer.fit, X)\n\n    imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=1)\n    assert_warns_message(DeprecationWarning,\n                         \"Parameter 'axis' has been deprecated\",\n                         imputer.fit, X)\n", "def test_imputation_axis_deprecation():\n    # Test that axis deprecation warning is raised.\n    X = np.array([\n        [np.nan, 0, 0],\n        [5, np.nan, 0],\n        [0, 5, np.nan]\n    ])\n\n    imputer = Imputer(strategy=\"mean\", axis=1)\n    assert_warns_message(DeprecationWarning,\n                         \"Parameter 'axis' has been deprecated in 0.20 and \"\n                         \"will be removed in 0.22.\",\n                         imputer.fit, X)\n", "def test_imputation_axis_parameter_deprecation():\n    # Test that the axis parameter is deprecated.\n    X = np.array([\n        [0, np.nan, np.nan],  \n        [5, np.nan, np.nan],  \n        [0, 0, np.nan],    \n        [-5, 0, np.nan],   \n        [0, 5, np.nan],    \n        [4, 5, np.nan],    \n        [-4, -5, np.nan],  \n        [-1, 2, np.nan],   \n    ]).transpose()\n\n    imputer = Imputer(missing_values='NaN', strategy=\"mean\", axis=1)\n\n    with assert_warns_message(DeprecationWarning,\n                             \"Parameter 'axis' has been deprecated in 0.20\"):\n        imputer.fit(X)\n"], "sample_728": ["def test_make_circles():\n    X, y = make_circles(3, shuffle=False)\n    for x, label in zip(X, y):\n        dist_sqr = ((x - [0.0, 0.0]) ** 2).sum()\n        if label == 0:\n            assert_almost_equal(dist_sqr, 1.0,\n                                err_msg=\"Point is not on expected unit circle\")\n        else:\n            assert_almost_equal(dist_sqr, 0.64,\n                                err_msg=\"Point is not on expected smaller circle\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3,\n                                   shuffle=True, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=1., n_samples=100,\n                                   n_features=2, n_classes=3,\n                                   shuffle=True, random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n    assert_equal(np.unique(y).shape, (3,), \"Unexpected number of classes\")\n\n    # Test that the samples are ordered by density (i.e. quantile)\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=4,\n                                   shuffle=False, random_state=0)\n\n    densities = np.linalg.norm(X, axis=1)\n    for i in range(1, 4):\n        assert_true((densities[y == i] >= densities[y == i-1]).all())\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n\n    # Ensure no samples are too far away from the nearest mean vector\n    centers = np.array([[4, -4], [-4, -4], [0, -4]])\n    distances = []\n    for x in X:\n        distance_to_center = np.linalg.norm(x - centers[0])\n        for center in centers[1:]:\n            distance_to_center = min(distance_to_center,\n                                     np.linalg.norm(x - center))\n        distances.append(distance_to_center)\n    max_distance = max(distances)\n    assert_less(max_distance, 5.0, \"Sample is too far away from mean\")\n", "def test_make_circles():\n    X, y = make_circles(3, shuffle=False)\n    for x, label in zip(X, y):\n        center = [0.0, 0.0] if label == 0 else [0.0, 0.0]\n        dist_sqr = ((x - center) ** 2).sum()\n        if label == 0:\n            assert_almost_equal(dist_sqr, 1.0,\n                                err_msg=\"Point is not on expected unit circle\")\n        else:\n            assert_almost_equal(dist_sqr, 0.64,\n                                err_msg=\"Point is not on expected smaller circle\")\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=0)\n\n    assert_equal(X.shape, (100, 2), \"X shape mismatch\")\n    assert_equal(y.shape, (100,), \"y shape mismatch\")\n\n    # Ensure the class counts are approximately equal\n    counts, _ = np.histogram(y, bins=3)\n    assert_less(np.std(counts), 10, \"Class counts are not approximately equal\")\n\n    # Ensure the quantiles are correct\n    for i in range(3):\n        X_i = X[y == i]\n        dist_sq = np.sum(X_i ** 2, axis=1)\n        quantiles = np.percentile(dist_sq, [0, 33.33, 66.67, 100])\n        assert_less(quantiles[0], 1.0, \"Min distance to origin greater than 1\")\n        assert_less(quantiles[-1], 4.0, \"Max distance to origin greater than 4\")\n        assert_less(np.abs(np.diff(quantiles) - 1.0), 0.5,\n                    \"Quantiles are not approximately 1 apart\")\n"], "sample_729": ["def test_enet_non_float_X():\n    X = [[0, 0], [1, 1], [-1, -1]]\n    y = [0.0, 1.0, 2.0]\n    X_float = [[0.0, 0.0], [1.0, 1.0], [-1.0, -1.0]]\n\n    for model in [ElasticNet, Lasso]:\n        clf = model(fit_intercept=False)\n        clf.fit(X, y)\n        clf_float = model(fit_intercept=False)\n        clf_float.fit(X_float, y)\n        assert_array_equal(clf.coef_, clf_float.coef_)\n", "def test_enet_non_fortran_arrays():\n    # Generate dataset\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n\n    # Ensure arrays are not in fortran order\n    X = check_array(X, order='C')\n    y = check_array(y, order='C')\n\n    clf = ElasticNet(max_iter=100)\n    ignore_warnings(clf.fit)(X, y)\n\n    # Make sure the fit is correct by checking against the original order\n    X_original, y_original = build_dataset(n_samples=20, n_features=10)\n    clf_original = ElasticNet(max_iter=100)\n    ignore_warnings(clf_original.fit)(X_original, y_original)\n\n    assert_array_almost_equal(clf.coef_, clf_original.coef_)\n    assert_array_almost_equal(clf.intercept_, clf_original.intercept_)\n", "def test_enet_path_precompute():\n    # Test enet_path with precomputed Gram matrix.\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    Gram = np.dot(X.T, X)\n\n    # With precompute='auto'\n    alphas, coefs, dual_gaps = enet_path(X, y, precompute='auto', max_iter=100)\n    # With precompute=True\n    alphas2, coefs2, dual_gaps2 = enet_path(X, y, precompute=True, max_iter=100)\n    # With precompute=Gram\n    alphas3, coefs3, dual_gaps3 = enet_path(X, y, precompute=Gram, max_iter=100)\n\n    assert_array_almost_equal(alphas, alphas2)\n    assert_array_almost_equal(coefs, coefs2)\n    assert_array_almost_equal(dual_gaps, dual_gaps2)\n\n    assert_array_almost_equal(alphas, alphas3)\n    assert_array_almost_equal(coefs, coefs3)\n    assert_array_almost_equal(dual_gaps, dual_gaps3)\n", "def test_sparse_lasso_with_copy():\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    X_sparse = sparse.csr_matrix(X)\n\n    lasso = Lasso()\n    lasso.fit(X_sparse, y)\n\n    lasso_copy = Lasso(copy_X=True)\n    lasso_copy.fit(X_sparse, y)\n\n    assert_array_almost_equal(lasso.coef_, lasso_copy.coef_)\n    assert_array_almost_equal(lasso.intercept_, lasso_copy.intercept_)\n", "def test_lasso_path_return_models_vs_new_return_gives_same_coefficients_with_sparse():\n    # Test that lasso_path with lars_path style output gives the\n    # same result with sparse data\n\n    # Some toy data\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    y = np.array([1, 2, 3.1])\n    alphas = [5., 1., .5]\n\n    # Use lars_path and lasso_path(new output) with 1D linear interpolation\n    # to compute the same path\n    alphas_lars, _, coef_path_lars = lars_path(X, y, method='lasso')\n    coef_path_cont_lars = interpolate.interp1d(alphas_lars[::-1],\n                                               coef_path_lars[:, ::-1])\n    alphas_lasso2, coef_path_lasso2, _ = lasso_path(sparse.csr_matrix(X), y,\n                                                    alphas=alphas,\n                                                    return_models=False)\n    coef_path_cont_lasso = interpolate.interp1d(alphas_lasso2[::-1],\n                                                coef_path_lasso2[:, ::-1])\n\n    assert_array_almost_equal(\n        coef_path_cont_lasso(alphas), coef_path_cont_lars(alphas),\n        decimal=1)\n"], "sample_730": ["def test_enet_path_return_models_vs_new_return_gives_same_coefficients():\n    # Test that enet_path with lars_path style output gives the\n    # same result\n\n    # Some toy data\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    y = np.array([1, 2, 3.1])\n    alphas = [5., 1., .5]\n    l1_ratio = 0.7\n\n    # Use enet_path and lasso_path(new output) with 1D linear interpolation\n    # to compute the same path\n    alphas_enet, coef_path_enet, _ = enet_path(X, y, l1_ratio=l1_ratio,\n                                               alphas=alphas,\n                                               return_models=False)\n    coef_path_cont_enet = interpolate.interp1d(alphas_enet[::-1],\n                                               coef_path_enet[:, ::-1])\n\n    alphas_lasso, coef_path_lasso, _ = lasso_path(X, y, alphas=alphas,\n                                                  return_models=False)\n    coef_path_cont_lasso = interpolate.interp1d(alphas_lasso[::-1],\n                                                coef_path_lasso[:, ::-1])\n\n    assert_array_almost_equal(\n        coef_path_cont_enet(alphas), coef_path_cont_lasso(alphas),\n        decimal=1)\n", "def test_lasso_enet_coordinate_descent_convergence_warning():\n    # Test that a warning is raised when the coordinate descent algorithm does\n    # not converge within the specified number of iterations.\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    max_iter = 1  # too few iterations to converge\n\n    # Lasso\n    clf = Lasso(max_iter=max_iter)\n    with pytest.warns(ConvergenceWarning):\n        clf.fit(X, y)\n\n    # ElasticNet\n    clf = ElasticNet(max_iter=max_iter)\n    with pytest.warns(ConvergenceWarning):\n        clf.fit(X, y)\n\n    # MultiTaskLasso\n    clf = MultiTaskLasso(max_iter=max_iter)\n    with pytest.warns(ConvergenceWarning):\n        clf.fit(X, np.c_[y, y])\n\n    # MultiTaskElasticNet\n    clf = MultiTaskElasticNet(max_iter=max_iter)\n    with pytest.warns(ConvergenceWarning):\n        clf.fit(X, np.c_[y, y])\n", "def test_enet_l1_ratio_warning():\n    # Test that a warning message is raised if l1_ratio is outside the range [0, 1]\n    X = np.array([[1, 2, 4, 5, 8], [3, 5, 7, 7, 8]]).T\n    y = np.array([12, 10, 11, 21, 5])\n\n    msg = \"l1_ratio must be between 0 and 1; got l1_ratio=\"\n    for l1_ratio in [-1, 2]:\n        est = ElasticNetCV(l1_ratio=l1_ratio, random_state=42)\n        assert_warns_message(UserWarning, msg + str(l1_ratio), est.fit, X, y)\n\n    for l1_ratio in [-1, 2]:\n        est = MultiTaskElasticNetCV(l1_ratio=l1_ratio, random_state=42)\n        assert_warns_message(UserWarning, msg + str(l1_ratio), est.fit, X, y[:, None])\n", "def test_enet_multitask_path():\n    # Test the multitask elastic net path using coordinate descent.\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10, n_targets=3)\n    max_iter = 100\n\n    alphas, coefs, dual_gaps = enet_path(X, y, l1_ratio=0.8, max_iter=max_iter)\n\n    for coef in coefs:\n        assert_equal(coef.shape, (y.shape[1], X.shape[1]))\n\n    # Check that all coefficients are zero when alpha is large enough\n    assert_array_almost_equal(coefs[:, -1], np.zeros((y.shape[1], X.shape[1])))\n", "def test_lasso_non_finite_input():\n    X = [[0, 0], [1, np.nan], [-1, -1]]\n    y = [0, 1, 2]\n    clf = Lasso()\n\n    with pytest.raises(ValueError):\n        clf.fit(X, y)\n"], "sample_731": ["def test_california_housing_not_downloaded():\n    # Test that an exception is raised when the data is not downloaded\n    data_home = \"/tmp/sklearn_data_not_existing\"\n    try:\n        fetch_california_housing(data_home=data_home, download_if_missing=False)\n        raise AssertionError(\"fetch_california_housing did not raise an error\"\n                             \" as expected.\")\n    except IOError as e:\n        assert \"Data not found\" in str(e)\n", "def test_california_housing_fetch():\n    try:\n        data = fetch_california_housing(download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"California housing dataset can not be loaded.\")\n    \n    assert data.data.shape == (20640, 8)\n    assert data.target.shape == (20640,)\n    assert len(data.feature_names) == 8\n    assert data.DESCR.startswith(\"California housing dataset.\")\n", "def test_california_housing_fetch():\n    try:\n        data = fetch()\n    except IOError as e:\n        raise SkipTest(\"California housing dataset can not be loaded\")\n\n    assert (20640, 8) == data.data.shape\n    assert (20640,) == data.target.shape\n    assert len(data.feature_names) == 8\n    assert data.DESCR.startswith(\"California housing dataset.\")\n", "def test_california_housing_fetch():\n    data = fetch()\n    assert data.data.shape == (20640, 8)\n    assert data.target.shape == (20640,)\n    assert len(data.feature_names) == 8\n    assert data.DESCR.startswith(\"California housing dataset.\")\n", "def test_california_housing_not_downloaded_error():\n    # Test that an error is raised when trying to load the dataset without\n    # downloading it first.\n    try:\n        fetch_california_housing(download_if_missing=False)\n        assert False, \"Expected an IOError\"\n    except IOError as e:\n        assert \"Data not found and `download_if_missing` is False\" in str(e)\n"], "sample_732": ["def test_download_if_missing():\n    try:\n        data = fetch_kddcup99(download_if_missing=False, percent10=False)\n    except IOError:\n        # if the full dataset is not downloaded, fetch it and then continue test\n        fetch_kddcup99(download_if_missing=True, percent10=False)\n        \n    # Now we should be able to load the full dataset\n    data = fetch_kddcup99(percent10=False)\n    assert_equal(data.data.shape[1], 41)\n", "def test_kddcup99_return_X_y():\n    try:\n        data = fetch_kddcup99(download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    X, y = fetch_kddcup99(return_X_y=True)\n    assert_equal(X.shape, (494021, 41))\n    assert_equal(y.shape, (494021,))\n", "def test_kddcup99_return_X_y():\n    try:\n        data = fetch_kddcup99(download_if_missing=False, return_X_y=True)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(len(data), 2)\n    X, y = data\n    assert_equal(X.shape, (494021, 41))\n    assert_equal(y.shape, (494021,))\n", "def test_return_X_y():\n    try:\n        data = fetch_kddcup99(download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    X, y = fetch_kddcup99(return_X_y=True)\n    assert_equal(X.shape, data.data.shape)\n    assert_equal(y.shape, data.target.shape)\n\n    X, y = fetch_kddcup99('SA', return_X_y=True)\n    assert_equal(X.shape, (100655, 41))\n    assert_equal(y.shape, (100655,))\n\n    X, y = fetch_kddcup99('SF', return_X_y=True)\n    assert_equal(X.shape, (73237, 4))\n    assert_equal(y.shape, (73237,))\n\n    X, y = fetch_kddcup99('http', return_X_y=True)\n    assert_equal(X.shape, (58725, 3))\n    assert_equal(y.shape, (58725,))\n\n    X, y = fetch_kddcup99('smtp', return_X_y=True)\n    assert_equal(X.shape, (9571, 3))\n    assert_equal(y.shape, (9571,))\n", "def test_return_X_y():\n    try:\n        data = fetch_kddcup99(download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    X, y = fetch_kddcup99(return_X_y=True)\n    assert_equal(X.shape, (494021, 41))\n    assert_equal(y.shape, (494021,))\n    assert_equal(data.data, X)\n    assert_equal(data.target, y)\n\n    X, y = fetch_kddcup99('SA', return_X_y=True)\n    assert_equal(X.shape, (100655, 41))\n    assert_equal(y.shape, (100655,))\n    assert_equal(fetch_kddcup99('SA').data, X)\n    assert_equal(fetch_kddcup99('SA').target, y)\n"], "sample_733": ["def test_vectorizer_dtype():\n    # Ensure that the vectorizer's dtype is propagated to the resulting matrix\n    vect = CountVectorizer(dtype=np.float32)\n    X = vect.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.dtype, np.float32)\n\n    vect = TfidfVectorizer(dtype=np.float64)\n    X = vect.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.dtype, np.float64)\n\n    vect = HashingVectorizer(dtype=np.int32, norm=None, non_negative=True)\n    X = vect.transform(ALL_FOOD_DOCS)\n    assert_equal(X.dtype, np.int32)\n", "def test_vectorizer_empty_input():\n    # Test that vectorizers can handle empty input lists or arrays\n\n    # Create an empty list and an empty numpy array\n    empty_list = []\n    empty_array = np.array([])\n\n    # Create a CountVectorizer, TfidfVectorizer, and HashingVectorizer\n    cv = CountVectorizer()\n    tv = TfidfVectorizer()\n    hv = HashingVectorizer()\n\n    # Check that fit_transform returns an empty matrix for each vectorizer\n    assert_equal(cv.fit_transform(empty_list).shape, (0, 0))\n    assert_equal(tv.fit_transform(empty_list).shape, (0, 0))\n    assert_equal(hv.fit_transform(empty_list).shape, (0, hv.n_features))\n\n    # Check that fit_transform raises an error for empty numpy arrays\n    message = \"Iterable over raw text documents expected, string object received.\"\n    exception = ValueError\n\n        cv.fit_transform(empty_array)\n\n        tv.fit_transform(empty_array)\n\n        hv.fit_transform(empty_array)\n\n    assert_raise_message(exception, message, func_cv)\n    assert_raise_message(exception, message, func_tv)\n    assert_raise_message(exception, message, func_hv)\n", "def test_tfidfvectorizer_use_idf_false():\n    # Test that TfidfVectorizer with use_idf=False is equivalent to\n    # CountVectorizer followed by TfidfTransformer with use_idf=False.\n    cv = CountVectorizer()\n    tfidf_transformer = TfidfTransformer(use_idf=False)\n    tfidf_vectorizer = TfidfVectorizer(use_idf=False)\n\n    X_cv = cv.fit_transform(JUNK_FOOD_DOCS)\n    X_tfidf_transformer = tfidf_transformer.fit_transform(X_cv)\n    X_tfidf_vectorizer = tfidf_vectorizer.fit_transform(JUNK_FOOD_DOCS)\n\n    assert_array_almost_equal(X_tfidf_transformer.toarray(),\n                              X_tfidf_vectorizer.toarray())\n", "def test_vectorizer_dtype():\n    # Test that the dtype parameter is propagated to the matrix.\n    X = CountVectorizer(dtype=np.float32).fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.dtype, np.float32)\n\n    X = TfidfVectorizer(dtype=np.float64).fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.dtype, np.float64)\n\n    X = HashingVectorizer(dtype=np.float32, non_negative=True,\n                          norm=None).transform(ALL_FOOD_DOCS)\n    assert_equal(X.dtype, np.float32)\n", "def test_vectorizer_input_type():\n    message = (\"Iterable over raw text documents expected, \"\n               \"string object received.\")\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        assert_raise_message(\n            ValueError, message, vec.fit_transform, 123)\n        assert_raise_message(\n            ValueError, message, vec.fit, 123)\n        assert_raise_message(\n            ValueError, message, vec.transform, 123)\n\n    message = (\"Iterable over raw text documents expected, \"\n               \"int object received.\")\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        assert_raise_message(\n            ValueError, message, vec.fit_transform, [1, 2, 3])\n        assert_raise_message(\n            ValueError, message, vec.fit, [1, 2, 3])\n        assert_raise_message(\n            ValueError, message, vec.transform, [1, 2, 3])\n\n    # Ensure valid input types are processed correctly\n    docs = [\"hello world!\", \"another doc\"]\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        X = vec.fit_transform(docs)\n        assert_equal(X.shape[0], len(docs))\n"], "sample_734": ["def test_fowlkes_mallows_score_with_sparse_contingency_matrix():\n    # Test that Fowlkes-Mallows score is the same with sparse and dense contingency matrices\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n\n    contingency_dense = contingency_matrix(labels_a, labels_b)\n    contingency_sparse = contingency_matrix(labels_a, labels_b, sparse=True)\n\n    score_dense = fowlkes_mallows_score(labels_a, labels_b)\n    score_sparse = mutual_info_score(None, None, contingency=contingency_sparse)  # hack to compute FM score from contingency\n\n    tk = np.dot(contingency_sparse.data, contingency_sparse.data) - len(labels_a)\n    pk = np.sum(np.asarray(contingency_sparse.sum(axis=0)).ravel() ** 2) - len(labels_a)\n    qk = np.sum(np.asarray(contingency_sparse.sum(axis=1)).ravel() ** 2) - len(labels_a)\n    score_sparse = np.sqrt(tk / pk) * np.sqrt(tk / qk) if tk != 0. else 0.\n\n    assert_almost_equal(score_dense, score_sparse)\n", "def test_contingency_matrix_eps():\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    C = contingency_matrix(labels_a, labels_b)\n    C_eps = contingency_matrix(labels_a, labels_b, eps=0.5)\n    assert_array_almost_equal(C_eps, C + 0.5)\n", "def test_fowlkes_mallows_score_sparse():\n    # Test that sparse and dense contingency matrices give the same result\n    labels_a = np.array([0, 0, 0, 1, 1, 1])\n    labels_b = np.array([0, 0, 1, 1, 2, 2])\n    score_dense = fowlkes_mallows_score(labels_a, labels_b)\n    score_sparse = fowlkes_mallows_score(labels_a, labels_b, sparse=True)\n    assert_almost_equal(score_dense, score_sparse)\n", "def test_normalized_mutual_info_score():\n    # Test for normalized mutual information score\n    labels_a = np.array([1, 1, 1, 1, 2, 2, 2, 2])\n    labels_b = np.array([1, 1, 1, 2, 2, 2, 2, 2])\n    score = normalized_mutual_info_score(labels_a, labels_b)\n    assert_almost_equal(score, 0.6825, 4)\n\n    # Test with a very large array\n    a110 = np.array([list(labels_a) * 110]).flatten()\n    b110 = np.array([list(labels_b) * 110]).flatten()\n    score = normalized_mutual_info_score(a110, b110)\n    assert_almost_equal(score, 0.6825, 4)\n", "def test_homogeneity_completeness_v_measure_perfect_labeling():\n    # Perfect labeling\n    labels_true = np.array([0, 0, 1, 1, 2, 2])\n    labels_pred = np.array([0, 0, 1, 1, 2, 2])\n\n    h, c, v = homogeneity_completeness_v_measure(labels_true, labels_pred)\n    assert_almost_equal(h, 1.0, 5)\n    assert_almost_equal(c, 1.0, 5)\n    assert_almost_equal(v, 1.0, 5)\n\n    # Check that the scores are the same when the labels are reversed\n    labels_pred_reversed = np.array([2, 2, 1, 1, 0, 0])\n    h, c, v = homogeneity_completeness_v_measure(labels_true, labels_pred_reversed)\n    assert_almost_equal(h, 1.0, 5)\n    assert_almost_equal(c, 1.0, 5)\n    assert_almost_equal(v, 1.0, 5)\n"], "sample_735": ["def test_get_parameters():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n        \n        # Get the parameters\n        weights, means, covariances, precisions_cholesky = gmm._get_parameters()\n        \n        # Check if they are equal to the corresponding attributes\n        assert_array_equal(weights, gmm.weights_)\n        assert_array_equal(means, gmm.means_)\n        assert_array_equal(covariances, gmm.covariances_)\n        assert_array_equal(precisions_cholesky, gmm.precisions_cholesky_)\n", "def test_get_parameters():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        params = gmm._get_parameters()\n        assert_equal(len(params), 4)\n        assert_array_equal(params[0], gmm.weights_)\n        assert_array_equal(params[1], gmm.means_)\n        assert_array_equal(params[2], gmm.covariances_)\n        assert_array_equal(params[3], gmm.precisions_cholesky_)\n", "def test_get_parameters():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        weights, means, covariances, precisions_cholesky = gmm._get_parameters()\n        assert_array_almost_equal(weights, gmm.weights_)\n        assert_array_almost_equal(means, gmm.means_)\n        assert_array_almost_equal(covariances, gmm.covariances_)\n        assert_array_almost_equal(precisions_cholesky, gmm.precisions_cholesky_)\n", "def test_get_parameters():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n        weights, means, covariances, precisions_cholesky = gmm._get_parameters()\n        assert_array_equal(weights, gmm.weights_)\n        assert_array_equal(means, gmm.means_)\n        if covar_type == 'full':\n            assert_array_equal(covariances, gmm.covariances_)\n        elif covar_type == 'tied':\n            assert_array_equal(covariances, gmm.covariances_)\n        else:\n            assert_array_equal(covariances, gmm.covariances_)\n        assert_array_equal(precisions_cholesky, gmm.precisions_cholesky_)\n", "def test_gaussian_mixture_get_parameters():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n\n        params = gmm._get_parameters()\n        assert_array_equal(params[0], gmm.weights_)\n        assert_array_equal(params[1], gmm.means_)\n        assert_array_equal(params[2], gmm.covariances_)\n        assert_array_equal(params[3], gmm.precisions_cholesky_)\n"], "sample_736": ["def test_logistic_regressioncv_max_iter():\n    # Test that the maximum number of iterations is reached\n    X, y = make_classification(n_samples=50, n_features=5,\n                               random_state=0)\n    lr = LogisticRegressionCV(max_iter=1, tol=1e-15)\n    assert_warns(ConvergenceWarning, lr.fit, X, y)\n    assert_equal(lr.n_iter_.max(), 1)\n", "def test_logreg_multiclass_predict_proba_consistency():\n    # Test that predict_proba is consistent with predict for multiclass case.\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n\n    clf = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n                             random_state=42, max_iter=2000)\n    clf.fit(X, y)\n\n    y_pred = clf.predict(X)\n    y_pred_proba = clf.predict_proba(X).argmax(axis=1)\n\n    assert_array_equal(y_pred, y_pred_proba)\n", "def test_logistic_regression_path_convergence_warning():\n    # Test that a convergence warning is raised when the logistic regression\n    # path does not converge.\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    Cs = [1e3]\n    max_iter = 2\n    with ignore_warnings(category=ConvergenceWarning):\n        logistic_regression_path(X, y, Cs=Cs, fit_intercept=False,\n                                 tol=1e-15, solver='lbfgs', max_iter=max_iter)\n    assert_warns_message(ConvergenceWarning, \"lbfgs failed to converge\",\n                         logistic_regression_path, X, y, Cs=Cs,\n                         fit_intercept=False, tol=1e-15, solver='lbfgs',\n                         max_iter=max_iter)\n", "def test_warm_start_with_changed_solver():\n    # Test that we raise a ValueError when warm_start is True\n    # but the solver is changed.\n\n    X, y = iris.data, iris.target\n\n    clf = LogisticRegression(tol=1e-4, warm_start=True,\n                             random_state=42, max_iter=100,\n                             fit_intercept=True)\n    clf.fit(X, y)\n\n    # Change the solver and check that a ValueError is raised.\n    for solver in ['newton-cg', 'sag', 'saga', 'lbfgs']:\n        clf.solver = solver\n        assert_raises(ValueError, clf.fit, X, y)\n", "def test_logistic_regression_with_non_convergence():\n    # Test that the LogisticRegression estimator still works even if the\n    # underlying solver fails to converge. In this case, we should get a\n    # ConvergenceWarning.\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    clf = LogisticRegression(max_iter=1, tol=1e-15)\n    assert_warns(ConvergenceWarning, clf.fit, X, y)\n    assert_array_equal(clf.classes_, np.unique(y))\n    assert_equal(clf.coef_.shape, (2, 10))\n    assert_equal(clf.intercept_.shape, (2,))\n"], "sample_737": ["def test_tfidfvectorizer_non_negative():\n    # Non-regression test: TfidfVectorizer should raise a deprecation warning\n    # when `non_negative` is passed as a parameter.\n    message = (\"Parameter non_negative is deprecated and will be removed \"\n               \"in version 0.21.\")\n    exception = DeprecationWarning\n\n        v = TfidfVectorizer(non_negative=True)\n\n    assert_warns_message(exception, message, func)\n", "def test_countvectorizer_filter_empty_strings():\n    # Test that empty strings are ignored when fed to CountVectorizer\n\n    X = ['hello world', '', 'hello again']\n    vect = CountVectorizer()\n    X_counted = vect.fit_transform(X)\n    assert_equal(X_counted.shape, (2, 3))\n    assert_equal(vect.get_feature_names(), ['again', 'hello', 'world'])\n", "def test_countvectorizer_analyzer_callable():\n    # Test that the analyzer can be a callable\n        return [t for t in doc.split() if len(t) > 2]\n\n    vect = CountVectorizer(analyzer=analyzer)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    assert_true('the' not in vect.vocabulary_)\n", "def test_tfidfvectorizer_use_idf_false():\n    # Non-regression test: TfidfVectorizer should behave like CountVectorizer\n    # when use_idf=False.\n    cv = CountVectorizer()\n    tv = TfidfVectorizer(use_idf=False)\n\n    X_cv = cv.fit_transform(JUNK_FOOD_DOCS)\n    X_tv = tv.fit_transform(JUNK_FOOD_DOCS)\n\n    assert_array_equal(X_cv.toarray(), X_tv.toarray())\n", "def test_tfidfvectorizer_invalid_norm():\n    vect = TfidfVectorizer(norm='invalid')\n    assert_raises(ValueError, vect.fit, JUNK_FOOD_DOCS)\n"], "sample_738": ["def test_tfidfvectorizer_invalid_norm():\n    # Test that an invalid norm raises a ValueError\n    v = TfidfVectorizer(norm='invalid')\n    message = \"Invalid norm: invalid\"\n    assert_raise_message(ValueError, message, v.fit_transform, ALL_FOOD_DOCS)\n", "def test_tfidfvectorizer_use_idf_deprecation():\n    # Non-regression test: 'use_idf' parameter was deprecated in 0.19.\n    message = (\"The parameter 'use_idf' is deprecated and will be removed \"\n               \"in version 0.21.\")\n    with pytest.warns_message(DeprecationWarning, message):\n        TfidfVectorizer(use_idf=False)\n", "def test_vectorizer_preprocessor_callable():\n    # Ensure that preprocessor parameter accepts callable\n        return x.replace('-', '')\n\n    vectorizers = (CountVectorizer, TfidfVectorizer)\n    for Vectorizer in vectorizers:\n        vect = Vectorizer(preprocessor=strip_hyphen)\n        X = vect.fit_transform(['hello-world', 'hello hello'])\n        assert_equal(len(vect.vocabulary_), 2)\n        assert_in('helloworld', vect.vocabulary_)\n        assert_in('hello', vect.vocabulary_)\n\n    # Ensure that preprocessor parameter also accepts lambda functions\n    vect = CountVectorizer(preprocessor=lambda x: x.replace('-', ''))\n    X = vect.fit_transform(['hello-world', 'hello hello'])\n    assert_equal(len(vect.vocabulary_), 2)\n    assert_in('helloworld', vect.vocabulary_)\n    assert_in('hello', vect.vocabulary_)\n", "def test_vectorizer_dtype():\n    # Test that the vectorizer's dtype is correctly propagated to the output\n    # matrix, even when the numpy default integer type is not the same as the\n    # vectorizer's dtype.\n    orig_dtype = np.get_default_integer()\n    np.set_default_integer('int16')\n    try:\n        vect = CountVectorizer(dtype='int32')\n        X = vect.fit_transform(JUNK_FOOD_DOCS)\n        assert_equal(X.dtype, 'int32')\n    finally:\n        np.set_default_integer(orig_dtype)\n", "def test_tfidfvectorizer_dtype():\n    # Non-regression test: TfidfVectorizer used to ignore its \"dtype\" param.\n    v = TfidfVectorizer(dtype=np.float32)\n    X = v.fit_transform(['hello world', 'hello hello']).toarray()\n    assert_equal(X.dtype, np.float32)\n\n    v = TfidfVectorizer(dtype=np.int64)\n    X = v.fit_transform(['hello world', 'hello hello']).toarray()\n    assert_equal(X.dtype, np.int64)\n"], "sample_739": ["def test_label_binarize_threshold():\n    y = [[0, 1], [1, 0], [0.5, 0.5]]\n    classes = [0, 1]\n    expected = np.array([[0, 1], [1, 0], [1, 1]])\n\n    binarized = label_binarize(y, classes)\n    assert_array_equal(binarized, expected)\n\n    inversed = _inverse_binarize_thresholding(binarized, 'multilabel-indicator',\n                                              classes=classes, threshold=0.5)\n    assert_array_equal(inversed, np.array([[0, 1], [1, 0], [1, 1]]))\n\n    # Test thresholding with different threshold values\n    binarized = label_binarize(y, classes, threshold=0.3)\n    assert_array_equal(binarized, expected)\n\n    inversed = _inverse_binarize_thresholding(binarized, 'multilabel-indicator',\n                                              classes=classes, threshold=0.3)\n    assert_array_equal(inversed, np.array([[0, 1], [1, 0], [1, 1]]))\n\n    binarized = label_binarize(y, classes, threshold=0.7)\n    assert_array_equal(binarized, np.array([[0, 1], [1, 0], [0, 0]]))\n\n    inversed = _inverse_binarize_thresholding(binarized, 'multilabel-indicator',\n                                              classes=classes, threshold=0.7)\n    assert_array_equal(inversed, np.array([[0, 1], [1, 0], [0, 0]]))\n", "def test_label_binarizer_fit_transform_empty_input():\n    lb = LabelBinarizer()\n    assert_raises(ValueError, lb.fit_transform, [])\n", "def test_label_binarize_unknown_type():\n    y = [[1, 2], [3, 4]]\n    classes = [1, 2, 3, 4]\n    assert_raises(ValueError, label_binarize, y, classes)\n", "def test_label_binarize_empty_input():\n    y = []\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n\n    expected = np.array([]).reshape(0, len(classes))\n\n    for sparse_output in [True, False]:\n        binarized = label_binarize(y, classes, neg_label=neg_label,\n                                   pos_label=pos_label,\n                                   sparse_output=sparse_output)\n        assert_array_equal(toarray(binarized), expected)\n        assert_equal(issparse(binarized), sparse_output)\n\n    lb = LabelBinarizer(neg_label=neg_label, pos_label=pos_label)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(toarray(binarized), expected)\n", "def test_label_binarize_threshold():\n    # Test label_binarize with a threshold\n    y = np.array([[0.8, 0.4], [0.3, 0.7]])\n    classes = [0, 1]\n    expected = np.array([[1, 0], [0, 1]])\n\n    binarized = label_binarize(y, classes, threshold=0.5)\n    assert_array_equal(binarized, expected)\n\n    # Test inverse\n    inversed = _inverse_binarize_thresholding(binarized,\n                                              output_type=\"multilabel-indicator\",\n                                              classes=classes,\n                                              threshold=0.5)\n    assert_array_equal(inversed, (y > 0.5).astype(int))\n\n    # Check LabelBinarizer\n    lb = LabelBinarizer(threshold=0.5)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(binarized, expected)\n    inverse_output = lb.inverse_transform(binarized)\n    assert_array_equal(inverse_output, (y > 0.5).astype(int))\n"], "sample_740": ["def test_indexable():\n    # Tests that indexable works as expected\n    # Case 1: array-like\n    X = np.array([1, 2, 3])\n    assert_array_equal(indexable(X), X)\n\n    # Case 2: sparse matrix\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    assert_array_equal(indexable(X).toarray(), X.toarray())\n\n    # Case 3: None\n    assert indexable(None) is None\n\n    # Case 4: non-array-like\n    X = 'hello'\n    assert_array_equal(indexable(X), np.array(X))\n\n    # Case 5: multiple inputs\n    X = np.array([1, 2, 3])\n    y = np.array([4, 5, 6])\n    X_indexable, y_indexable = indexable(X, y)\n    assert_array_equal(X_indexable, X)\n    assert_array_equal(y_indexable, y)\n", "def test_check_X_y():\n    # Test that X and y are checked correctly\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    # Test that X and y are checked correctly when X is sparse\n    X_sparse = sp.csr_matrix(X)\n    X_checked, y_checked = check_X_y(X_sparse, y, accept_sparse=True)\n    assert_array_equal(X_sparse.toarray(), X_checked.toarray())\n    assert_array_equal(y, y_checked)\n\n    # Test that an error is raised when X and y have different lengths\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1, 2])\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # Test that an error is raised when y is not a 1D array\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[0, 1], [2, 3]])\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # Test that an error is raised when X is not a 2D array\n    X = np.array([1, 2, 3, 4])\n    y = np.array([0, 1])\n    assert_raises(ValueError, check_X_y, X, y)\n", "def test_check_X_y_multilabel():\n    # Test that check_X_y doesn't squeeze multilabel arrays\n    X = np.random.random((10, 3))\n    y = np.random.randint(0, 2, (10, 3))\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_equal(y_checked.shape, (10, 3))\n    assert_array_equal(y, y_checked)\n\n    # check for pandas dataframe\n    pd = importorskip(\"pandas\")\n    X_df = pd.DataFrame(X)\n    y_df = pd.DataFrame(y)\n    X_checked, y_checked = check_X_y(X_df, y_df, multi_output=True)\n    assert_equal(y_checked.shape, (10, 3))\n    assert_array_equal(y, y_checked)\n", "def test_check_X_y_invalid_parameters():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    # Test invalid parameters for y_numeric\n    assert_raises_regex(ValueError, \"y should be a 1d array\",\n                        check_X_y, X, y, y_numeric='invalid')\n\n    # Test invalid parameters for multi_output\n    assert_raises_regex(ValueError, \"Invalid multi_output parameter\",\n                        check_X_y, X, y, multi_output='invalid')\n\n    # Test invalid parameters for dtype\n    assert_raises_regex(ValueError, \"Invalid dtype parameter\",\n                        check_X_y, X, y, dtype='invalid')\n", "def test_check_X_y_input_type():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    \n    # Test X and y are numpy arrays\n    X_checked, y_checked = check_X_y(X, y)\n    assert isinstance(X_checked, np.ndarray)\n    assert isinstance(y_checked, np.ndarray)\n\n    # Test X is a list and y is a numpy array\n    X_checked, y_checked = check_X_y(X.tolist(), y)\n    assert isinstance(X_checked, np.ndarray)\n    assert isinstance(y_checked, np.ndarray)\n\n    # Test X is a numpy array and y is a list\n    X_checked, y_checked = check_X_y(X, y.tolist())\n    assert isinstance(X_checked, np.ndarray)\n    assert isinstance(y_checked, np.ndarray)\n\n    # Test X and y are lists\n    X_checked, y_checked = check_X_y(X.tolist(), y.tolist())\n    assert isinstance(X_checked, np.ndarray)\n    assert isinstance(y_checked, np.ndarray)\n\n    # Test X and y are pandas DataFrame and Series\n    pd = importorskip(\"pandas\")\n    X_df = pd.DataFrame(X)\n    y_series = pd.Series(y)\n    X_checked, y_checked = check_X_y(X_df, y_series)\n    assert isinstance(X_checked, np.ndarray)\n    assert isinstance(y_checked, np.ndarray)\n"], "sample_741": ["def test_parameter_grid_distribution():\n    # Test that ParameterGrid can handle distributions as input\n    param_grid = {'foo': [1, 2], 'bar': expon()}\n    grid = ParameterGrid(param_grid)\n    assert_true(isinstance(grid, Iterable))\n    assert_true(isinstance(grid, Sized))\n    assert_equal(len(grid), 2)\n\n    for params in grid:\n        assert_true('foo' in params)\n        assert_true('bar' in params)\n        assert_true(params['foo'] in [1, 2])\n        assert_true(isinstance(params['bar'], float))\n", "def test_grid_search_cv_results_rank_tie_breaking_non_iid():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid, iid=False)\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_score'][0],\n                        cv_results['mean_test_score'][1])\n    assert_almost_equal(cv_results['mean_train_score'][0],\n                        cv_results['mean_train_score'][1])\n    assert_false(np.allclose(cv_results['mean_test_score'][1],\n                             cv_results['mean_test_score'][2]))\n    assert_false(np.allclose(cv_results['mean_train_score'][1],\n                             cv_results['mean_train_score'][2]))\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [1, 1, 3])\n", "def test_grid_search_cv_results_rank_tie_breaking_non_finite_scores():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid)\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n\n    # Force the scores of the first two candidates to be identical and\n    # non-finite.\n    cv_results['mean_test_score'][:2] = np.inf\n\n    # Check that the rank is assigned correctly even when there are ties with\n    # positive infinity.\n    assert_array_almost_equal(cv_results['rank_test_score'], [1, 1, 3])\n", "def test_grid_search_warns_on_deprecated_parameter():\n    # Test that GridSearchCV warns on deprecated parameter 'fit_params'\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    clf = LinearSVC(random_state=0)\n    grid_search = GridSearchCV(clf, {'C': [1]}, fit_params={'sample_weight': y})\n    assert_warns(DeprecationWarning, grid_search.fit, X, y)\n", "def test_grid_search_cv_results_rank_tie_breaking_regression():\n    # Test rank tie breaking for regressors where greater is better is False.\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SGDClassifier(random_state=42),\n                               param_grid=param_grid, scoring='neg_mean_squared_error')\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_score'][0],\n                        cv_results['mean_test_score'][1])\n    assert_almost_equal(cv_results['mean_train_score'][0],\n                        cv_results['mean_train_score'][1])\n    assert_false(np.allclose(cv_results['mean_test_score'][1],\n                             cv_results['mean_test_score'][2]))\n    assert_false(np.allclose(cv_results['mean_train_score'][1],\n                             cv_results['mean_train_score'][2]))\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_score'], [1, 1, 3])\n"], "sample_742": ["def test_logistic_regression_refit():\n    # Test that refit works as expected for LogisticRegressionCV\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    clf_refit = LogisticRegressionCV(Cs=[1.], fit_intercept=False,\n                                     solver='liblinear', refit=True)\n    clf_no_refit = LogisticRegressionCV(Cs=[1.], fit_intercept=False,\n                                        solver='liblinear', refit=False)\n    clf_refit.fit(X, y)\n    clf_no_refit.fit(X, y)\n    assert_array_almost_equal(clf_refit.coef_, clf_no_refit.coef_)\n    assert_array_almost_equal(clf_refit.intercept_, clf_no_refit.intercept_)\n    assert_array_almost_equal(clf_refit.C_, clf_no_refit.C_)\n", "def test_logistic_regression_multiclass_predict_proba():\n    # Tests for the multinomial option in logistic regression\n    # Predicted probabilities should add up to 1.\n\n    # Some basic attributes of Logistic Regression\n    n_samples, n_features, n_classes = 50, 20, 3\n    X, y = make_classification(n_samples=n_samples,\n                               n_features=n_features,\n                               n_informative=10,\n                               n_classes=n_classes, random_state=0)\n\n    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n    clf.fit(X, y)\n    y_pred_proba = clf.predict_proba(X)\n\n    assert_array_almost_equal(y_pred_proba.sum(axis=1), np.ones(n_samples))\n", "def test_saga_classifier_matches_sgd():\n    # Test that SAGA matches SGD in log loss mode on the training set.\n\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n\n    sgd_loss = 'log'\n    clf_sgd = SGDClassifier(loss=sgd_loss, max_iter=1000, tol=1e-6,\n                            shuffle=True, random_state=42)\n    clf_saga = LogisticRegression(solver='saga', max_iter=1000, tol=1e-6,\n                                  random_state=42)\n\n    clf_sgd.fit(X, y)\n    clf_saga.fit(X, y)\n\n    assert_array_almost_equal(clf_sgd.coef_, clf_saga.coef_)\n    assert_array_almost_equal(clf_sgd.intercept_, clf_saga.intercept_)\n", "def test_logistic_regressioncv_saga_sparse_data():\n    # Test that the LogisticRegressionCV with saga solver works with sparse data\n    X, y = make_classification(n_samples=100, n_features=10,\n                               random_state=0)\n    clf = LogisticRegressionCV(solver='saga', Cs=[1.0], fit_intercept=False,\n                               tol=1e-5, max_iter=10000, random_state=0)\n    clf.fit(sparse.csr_matrix(X), y)\n    assert_array_almost_equal(clf.coef_, clf.coef_)\n", "def test_logistic_regression_multiclass_predict_proba():\n    # Test that predict_proba works with all solvers for multiclass problems\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0,\n                               n_classes=3, n_informative=5)\n\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, multi_class='multinomial',\n                                 max_iter=1000, tol=1e-6)\n        clf.fit(X, y)\n        proba = clf.predict_proba(X)\n        assert_array_almost_equal(proba.sum(axis=1), np.ones(len(X)))\n        assert_equal(proba.shape, (len(X), 3))\n"], "sample_743": ["def test_radius_neighbors_outlier_label_warning():\n    X = np.array([[1.0, 1.0], [2.0, 2.0]])\n    y = np.array([1, 2])\n    radius = 0.1\n\n    z = np.array([[1.4, 1.4]])  # outlier\n\n    for algorithm in ALGORITHMS:\n        clf = neighbors.RadiusNeighborsClassifier(radius=radius,\n                                                  algorithm=algorithm)\n        clf.fit(X, y)\n\n        warning_msg = (\"One or more samples have no neighbors \"\n                       \"within specified radius; predicting NaN.\")\n\n        assert_warns_message(UserWarning, warning_msg, clf.predict, z)\n", "def test_radius_neighbors_outlier_label():\n    # Test radius neighbors outlier label.\n    X = np.array([[1.0, 1.0], [2.0, 2.0]])\n    y = np.array([1, 2])\n    radius = 0.1\n\n    z1 = np.array([[1.01, 1.01], [2.01, 2.01]])  # no outliers\n    z2 = np.array([[1.4, 1.4], [1.01, 1.01], [2.01, 2.01]])    # one outlier\n\n    for outlier_label in [0, -1, None]:\n        for algorithm in ALGORITHMS:\n            rnc = neighbors.RadiusNeighborsClassifier(\n                radius=radius, algorithm=algorithm,\n                outlier_label=outlier_label)\n            rnc.fit(X, y)\n            assert_array_equal(np.array([1, 2]),\n                               rnc.predict(z1))\n            if outlier_label is None:\n                assert_raises(ValueError, rnc.predict, z2)\n", "def test_radius_neighbors_classifier_sparse_input():\n    # Test radius-based classifier with sparse input\n    X = csr_matrix(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n    y = np.array([1, 2, 3])\n    clf = neighbors.RadiusNeighborsClassifier(radius=1.0)\n    clf.fit(X, y)\n\n    X_test = csr_matrix(np.array([[0, 0, 1]]))\n    assert_array_equal(clf.predict(X_test), np.array([3]))\n", "def test_kneighbors_graph_n_neighbors():\n    # Test kneighbors_graph with different values of n_neighbors\n    X = np.array([[0, 1], [1.01, 1.], [2, 0]])\n\n    # n_neighbors = 1\n    A = neighbors.kneighbors_graph(X, 1, mode='connectivity',\n                                   include_self=True)\n    assert_array_equal(A.toarray(), np.eye(A.shape[0]))\n\n    # n_neighbors = 2\n    A = neighbors.kneighbors_graph(X, 2, mode='connectivity',\n                                   include_self=True)\n    assert_array_equal(\n        A.toarray(),\n        [[1., 1., 0.],\n         [1., 1., 0.],\n         [0., 1., 1.]])\n\n    # n_neighbors = 3\n    A = neighbors.kneighbors_graph(X, 3, mode='connectivity',\n                                   include_self=True)\n    assert_array_almost_equal(\n        A.toarray(),\n        [[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n", "def test_radius_neighbors_predict_sparse():\n    # Test radius neighbors predict on sparse data\n    X = csr_matrix(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n    y = np.array([1, 2, 3])\n    clf = neighbors.RadiusNeighborsClassifier(radius=1.5)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), y)\n"], "sample_745": ["def test_function_transformer_deprecated_pass_y():\n    X = np.random.randn(100, 10)\n    y = np.random.randn(100)\n\n    transformer = FunctionTransformer(pass_y=True)\n    with pytest.warns(DeprecationWarning):\n        transformer.transform(X, y)\n\n    transformer = FunctionTransformer(pass_y=False)\n    assert_no_warnings(transformer.transform, X, y)\n\n    transformer = FunctionTransformer()\n    assert_no_warnings(transformer.transform, X, y)\n", "def test_function_transformer_inverse_func_kw_args():\n    X = np.linspace(0, 1, num=10).reshape((5, 2))\n\n    F = FunctionTransformer(\n        func=np.expm1,\n        inverse_func=np.log1p,\n        inv_kw_args=dict(out=None),\n    )\n\n    # Test that inverse function is called with the correct keyword arguments\n    assert_array_equal(F.inverse_transform(F.transform(X)), np.log1p(np.expm1(X)))\n\n    # Update inv_kw_args and check again\n    F.inv_kw_args['out'] = np.zeros_like(X)\n    assert_array_equal(F.inverse_transform(F.transform(X)), np.log1p(np.expm1(X)))\n", "def test_function_transformer_deprecated_pass_y():\n    # Test that pass_y is deprecated and will be removed in 0.21\n    X = np.arange(10).reshape((5, 2))\n    y = object()\n    \n    transformer = FunctionTransformer(pass_y=True)\n    assert_warns_message(DeprecationWarning, \"The parameter pass_y is deprecated\",\n                         transformer.transform, X, y)\n    \n    transformer = FunctionTransformer(pass_y=False)\n    assert_no_warnings(transformer.transform, X, y)\n    \n    # Test that setting pass_y to non-boolean raises an error\n    with pytest.raises(ValueError):\n        FunctionTransformer(pass_y='invalid')\n", "def test_function_transformer_validate_parameter():\n    # Test that the validate parameter is set correctly\n    transformer = FunctionTransformer(validate=True)\n    assert transformer._validate == True\n\n    transformer = FunctionTransformer(validate=False)\n    assert transformer._validate == False\n\n    transformer = FunctionTransformer(validate=None)\n    assert transformer._validate == True  # should be set to True by default\n\n    # Test that a warning is raised when validate is None\n    with pytest.warns(FutureWarning):\n        FunctionTransformer(validate=None)\n\n    # Test that an error is raised when validate is not a boolean or None\n    with pytest.raises(ValueError):\n        FunctionTransformer(validate=\"invalid\")\n", "def test_function_transformer_inverse_func_none():\n    # Test that FunctionTransformer works correctly when inverse_func is None\n    X = np.array([1, 4, 9, 16]).reshape((2, 2))\n    F = FunctionTransformer(func=np.sqrt, inverse_func=None)\n    assert_array_equal(F.inverse_transform(F.transform(X)), F.transform(X))\n"], "sample_746": ["def test_brier_score_loss():\n    y_true = [0, 1, 1, 0]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.037)\n\n    y_true = [0, 1, 1, 1]\n    y_prob = [0.1, 0.9, 0.8, 0.3]\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.1425)\n\n    y_true = np.array([[0, 1], [1, 0]])\n    y_prob = np.array([[0.2, 0.8], [0.7, 0.3]])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.09)\n\n    y_true = [0, 1]\n    y_prob = [0.2, 0.8]\n    sample_weight = [0.6, 0.4]\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight), 0.068)\n\n    y_true = [0, 1]\n    y_prob = [[0.2, 0.8], [0.7, 0.3]]\n    pos_label = 1\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label=pos_label), 0.17)\n\n    # Test for array-like input\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.037)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n\n    # for true binary labels\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.037)\n    # for arbitrary label values, errors should be raised\n    assert_raises(ValueError, brier_score_loss, y_true + 1, y_prob)\n\n    # for non-binary pos_label\n    y_true = np.array(['a', 'b', 'b', 'a'])\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label='b'), 0.037)\n\n    # check the average option\n    y_true = np.array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1])\n    y_prob = np.array([0.26, 0.73, 0.42, 0.07, 0.98, 0.14, 0.83, 0.23, 0.46,\n                      0.68, 0.35, 0.96])\n\n    # macro\n    assert_almost_equal(brier_score_loss(y_true, y_prob, average='macro'),\n                        (2 / 3) * brier_score_loss(y_true[:6], y_prob[:6]) +\n                        (1 / 3) * brier_score_loss(y_true[6:], y_prob[6:]))\n\n    # weighted\n    weights = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n    assert_almost_equal(brier_score_loss(y_true, y_prob, average='weighted',\n                                         sample_weight=weights),\n                        (1 / 3) * brier_score_loss(y_true[:6], y_prob[:6]) +\n                        (2 / 3) * brier_score_loss(y_true[6:], y_prob[6:]))\n", "def test_brier_score_loss():\n    # Test Brier score for binary case\n    y_true = [0, 1]\n    y_prob = [0.5, 0.5]\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.125)\n\n    # Test Brier score for multiclass case with list of lists input format\n    y_true = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    y_prob = [[0.3, 0.3, 0.4], [0.2, 0.7, 0.1], [0.1, 0.1, 0.8]]\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.107)\n\n    # Test Brier score for multiclass case with array input format\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y_prob = np.array([[0.3, 0.3, 0.4], [0.2, 0.7, 0.1], [0.1, 0.1, 0.8]])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.107)\n", "def test_brier_score_loss():\n    # Test Brier score loss function\n    y_true = [0, 1, 0, 1, 0]\n    y_prob = [0.3, 0.7, 0.4, 0.6, 0.2]\n\n    error = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(error, 0.16, decimal=2)\n\n    y_true = np.array([0, 1, 0, 1, 0])\n    y_prob = np.array([0.3, 0.7, 0.4, 0.6, 0.2])\n\n    error = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(error, 0.16, decimal=2)\n\n    y_true = [\"spam\", \"ham\", \"spam\", \"ham\", \"spam\"]\n    y_prob = [0.3, 0.7, 0.4, 0.6, 0.2]\n\n    error = brier_score_loss(y_true, y_prob, pos_label=\"ham\")\n    assert_almost_equal(error, 0.16, decimal=2)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.037)\n\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    loss = brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n    assert_almost_equal(loss, 0.037)\n\n    # Test with a single positive class\n    y_true = np.array([0, 0, 0, 1])\n    y_prob = np.array([0.0, 0.0, 0.0, 1.0])\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.0)\n\n    # Test with all negative classes\n    y_true = np.array([0, 0, 0, 0])\n    y_prob = np.array([0.5, 0.5, 0.5, 0.5])\n    loss = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(loss, 0.25)\n"], "sample_748": ["def test_grid_search_cv_results_rank_tie_breaking_non_finite():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid)\n    random_search = RandomizedSearchCV(SVC(gamma=\"scale\"), n_iter=3,\n                                       param_distributions=param_grid)\n\n    for search in (grid_search, random_search):\n        search.fit(X, y)\n        cv_results = search.cv_results_\n        # Check tie breaking strategy -\n        # Check that there is a tie in the mean scores between\n        # candidates 1 and 2 alone\n        cv_results['mean_test_score'][0] = np.nan\n        assert_true(np.isnan(cv_results['mean_test_score'][0]))\n        assert_almost_equal(cv_results['mean_test_score'][1],\n                            cv_results['mean_test_score'][2])\n        assert_false(np.allclose(cv_results['mean_test_score'][1],\n                                 cv_results['mean_test_score'][0]))\n        assert_false(np.allclose(cv_results['mean_train_score'][1],\n                                 cv_results['mean_train_score'][0]))\n        # 'min' rank should be assigned to the tied candidates\n        assert_almost_equal(search.cv_results_['rank_test_score'], [3, 1, 1])\n", "def test_grid_search_cv_results_rank_tie_breaking_multimetric():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid,\n                               scoring=['accuracy', 'precision'])\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_accuracy'][0],\n                        cv_results['mean_test_accuracy'][1])\n    assert_almost_equal(cv_results['mean_train_accuracy'][0],\n                        cv_results['mean_train_accuracy'][1])\n    assert_false(np.allclose(cv_results['mean_test_accuracy'][1],\n                             cv_results['mean_test_accuracy'][2]))\n    assert_false(np.allclose(cv_results['mean_train_accuracy'][1],\n                             cv_results['mean_train_accuracy'][2]))\n\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_accuracy'], [1, 1, 3])\n    assert_almost_equal(grid_search.cv_results_['rank_test_precision'], [1, 1, 3])\n", "def test_grid_search_cv_results_dtype():\n    # Check if the dtypes of the cv_results_ are correct\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    grid_search = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1, 2, 3]})\n    grid_search.fit(X, y)\n\n    result_keys = ['mean_fit_time', 'std_fit_time', 'mean_score_time',\n                   'std_score_time']\n    for key in result_keys:\n        assert_equal(grid_search.cv_results_[key].dtype, np.float64)\n\n    score_keys = ['mean_test_score', 'std_test_score']\n    for key in score_keys:\n        assert_equal(grid_search.cv_results_[key].dtype, np.float64)\n\n    param_keys = ['param_C']\n    for key in param_keys:\n        assert_equal(grid_search.cv_results_[key].dtype, object)\n", "def test_grid_search_cv_results_best_index():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    grid_search = GridSearchCV(SVC(gamma='scale'), cv=n_splits,\n                               param_grid=params)\n    grid_search.fit(X, y)\n\n    cv_results = grid_search.cv_results_\n    best_index = np.argmax(cv_results['mean_test_score'])\n    assert_equal(grid_search.best_index_, best_index)\n", "def test_grid_search_cv_results_rank_tie_breaking_multimetric():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    scoring = ('accuracy', 'precision')\n\n    grid_search = GridSearchCV(SVC(gamma=\"scale\"), param_grid=param_grid,\n                               scoring=scoring, refit='accuracy')\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_accuracy'][0],\n                        cv_results['mean_test_accuracy'][1])\n    assert_almost_equal(cv_results['mean_train_accuracy'][0],\n                        cv_results['mean_train_accuracy'][1])\n    assert_false(np.allclose(cv_results['mean_test_accuracy'][1],\n                             cv_results['mean_test_accuracy'][2]))\n    assert_false(np.allclose(cv_results['mean_train_accuracy'][1],\n                             cv_results['mean_train_accuracy'][2]))\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_accuracy'], [1, 1, 3])\n\n    # For precision, there might not be ties, but we still check ranks\n    assert_true(all(cv_results['rank_test_precision'][i] in (1, 2, 3)\n                   for i in range(3)))\n"], "sample_749": ["def test_column_transformer_get_feature_names_remainder_passthrough():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder='passthrough')\n    ct.fit(X_array)\n    assert_raise_message(NotImplementedError,\n                         'get_feature_names is not yet supported when using '\n                         \"a 'passthrough' transformer.\",\n                         ct.get_feature_names)\n", "def test_column_transformer_empty_dataframe():\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame()\n    ct = ColumnTransformer([('trans', Trans(), [0])])\n    assert_raise_message(ValueError, \"No valid specification\",\n                         ct.fit, X_df)\n    assert_raise_message(ValueError, \"No valid specification\",\n                         ct.fit_transform, X_df)\n", "def test_column_transformer_with_n_jobs():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # single column 1D / 2D\n    ct = ColumnTransformer([('trans', Trans(), [0])], n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), X_array[:, [0]])\n\n    # list-like\n    ct = ColumnTransformer([('trans', Trans(), [0, 1])], n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n\n    # slice\n    ct = ColumnTransformer([('trans', Trans(), slice(0, 1))], n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), X_array[:, [0]])\n\n    # boolean mask\n    ct = ColumnTransformer([('trans', Trans(), np.array([True, False]))], n_jobs=2)\n    assert_array_equal(ct.fit_transform(X_array), X_array[:, [0]])\n", "def test_column_transformer_remainder_with_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_second = np.array([2, 4, 6]).reshape(-1, 1)\n    X_res_both = X_array\n\n    # specify to drop remaining columns with weights\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder='drop',\n                           transformer_weights={'trans1': 0.5})\n    assert_array_equal(ct.fit_transform(X_array), 0.5 * X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), 0.5 * X_res_first)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'drop'\n    assert_array_equal(ct.transformers_[-1][2], [1])\n\n    # passthrough with weights\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder='passthrough',\n                           transformer_weights={'remainder': 2})\n    assert_array_equal(ct.fit_transform(X_array), np.hstack((X_res_first, 2 * X_res_second)))\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.hstack((X_res_first, 2 * X_res_second)))\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'passthrough'\n    assert_array_equal(ct.transformers_[-1][2], [1])\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0, 1])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert_raise_message(\n        NotImplementedError,\n        'get_feature_names is not yet supported when using '\n        \"a 'passthrough' or 'remainder' transformer.\",\n        ct.get_feature_names\n    )\n\n    class DummyTrans(BaseEstimator):\n            return self\n\n            return X\n\n            return ['dummy']\n\n    ct = ColumnTransformer([('trans', DummyTrans(), [0, 1])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    exp = ['trans__dummy', 'remainder__x0', 'remainder__x1']\n    assert_equal(ct.get_feature_names(), exp)\n\n    ct = ColumnTransformer([('trans', DummyTrans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    exp = ['trans__dummy', 'remainder__x0', 'remainder__x1']\n    assert_equal(ct.get_feature_names(), exp)\n"], "sample_750": ["def test_omp_cv_multiple_targets():\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y)\n    assert_equal(ompcv.coef_.shape, (n_targets, n_features))\n    assert_array_almost_equal(ompcv.coef_, gamma)\n    omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=False,\n                                    n_nonzero_coefs=ompcv.n_nonzero_coefs_)\n    omp.fit(X, y)\n    assert_array_almost_equal(ompcv.coef_, omp.coef_)\n", "def test_omp_cv_multiple_targets():\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y)\n    assert_equal(ompcv.n_nonzero_coefs_, n_nonzero_coefs)\n    assert_array_almost_equal(ompcv.coef_, gamma)\n    omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=False,\n                                    n_nonzero_coefs=ompcv.n_nonzero_coefs_)\n    omp.fit(X, y)\n    assert_array_almost_equal(ompcv.coef_, omp.coef_)\n", "def test_omp_cv_verbose():\n    y_ = y[:, 0]\n    gamma_ = gamma[:, 0]\n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5, verbose=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        ompcv.fit(X, y_)\n    assert_equal(ompcv.n_nonzero_coefs_, n_nonzero_coefs)\n    assert_array_almost_equal(ompcv.coef_, gamma_)\n    omp = OrthogonalMatchingPursuit(normalize=True, fit_intercept=False,\n                                    n_nonzero_coefs=ompcv.n_nonzero_coefs_)\n    with ignore_warnings(category=ConvergenceWarning):\n        omp.fit(X, y_)\n    assert_array_almost_equal(ompcv.coef_, omp.coef_)\n", "def test_omp_n_iter():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n    omp.fit(X, y[:, 0])\n    assert_equal(omp.n_iter_, n_nonzero_coefs)\n    \n    ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                        max_iter=10, cv=5)\n    ompcv.fit(X, y[:, 0])\n    assert_equal(len(ompcv.n_iter_), 1)\n", "def test_omp_n_iter():\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n    omp.fit(X, y[:, 0])\n    assert_equal(omp.n_iter_, n_nonzero_coefs)\n    omp.fit(X, y)\n    assert_equal(omp.n_iter_.shape, (n_targets,))\n    assert_array_equal(omp.n_iter_, np.repeat(n_nonzero_coefs, n_targets))\n"], "sample_751": ["def test_feature_importances():\n    # Check feature importances.\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=10,\n                                        n_informative=3,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert_equal(importances.shape[0], 10)\n    assert_equal(np.sum(importances), 1)\n\n    # Check feature importances with different number of features\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=20,\n                                        n_informative=5,\n                                        n_redundant=5,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y)\n    importances = clf.feature_importances_\n\n    assert_equal(importances.shape[0], 20)\n    assert_equal(np.sum(importances), 1)\n", "def test_feature_importances():\n    # Check feature importances.\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=10,\n                                        n_informative=3,\n                                        n_redundant=0,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y)\n\n    # check feature importances shape\n    assert_equal(clf.feature_importances_.shape[0], 10)\n\n    # check feature importances sum\n    assert_almost_equal(np.sum(clf.feature_importances_), 1)\n\n    # check feature importances are non-negative\n    assert_array_equal(clf.feature_importances_ >= 0, True)\n\n    # check that gini importance is an array with the correct shape\n    assert_equal(clf.feature_importances_.shape, (X.shape[1],))\n\n    # check that all importances are non-negative\n    assert_array_less(-1e-8, clf.feature_importances_)\n", "def test_base_estimator_has_no_sample_weight():\n    \"\"\"Test that the base estimator does not need a sample_weight parameter\"\"\"\n    class DummyEstimator(BaseEstimator):\n            return self\n\n            return np.zeros(X.shape[0])\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator())\n    clf.fit(np.array([[1, 2], [3, 4]]), np.array([0, 1]))\n", "def test_forest_regressor_attributes():\n    # Check ForestRegressor attributes.\n    X = iris.data\n    y = iris.target\n\n    clf = RandomForestRegressor(n_estimators=10)\n    clf.fit(X, y)\n\n    assert_equal(clf.n_features_, X.shape[1])\n    assert_equal(clf.n_outputs_, 1)\n\n    clf = ExtraTreesRegressor(n_estimators=10)\n    clf.fit(X, y)\n\n    assert_equal(clf.n_features_, X.shape[1])\n    assert_equal(clf.n_outputs_, 1)\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work without sample_weights in the base estimator\n\n    The random weighted sampling is done internally in the _boost method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator())\n    clf.fit(X, y_class)\n"], "sample_752": ["def test_threshold_deprecation_warning():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest(contamination=0.1).fit(X_train)\n\n    with pytest.warns(DeprecationWarning) as record:\n        threshold = clf.threshold_\n\n    assert len(record) == 1\n    assert 'threshold_ attribute is deprecated' in str(record[0].message)\n    assert threshold == clf.offset_\n", "def test_iforest_threshold():\n    # Test that threshold_ attribute is correctly set and used\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n    clf = IsolationForest(contamination=0.25).fit(X)\n    assert_almost_equal(clf.threshold_, clf.offset_)\n    assert_array_equal(clf.predict(X), (clf.decision_function(X) > clf.threshold_).astype(int) * 2 - 1)\n", "def test_threshold_deprecation():\n    # Make sure threshold_ is still accessible but deprecated\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest(contamination=0.1).fit(X_train)\n    assert_warns_message(DeprecationWarning,\n                         \"threshold_ attribute is deprecated\",\n                         getattr, clf, 'threshold_')\n    assert_almost_equal(clf.threshold_, clf.offset_)\n", "def test_iforest_fit_time():\n    \"\"\"Test Isolation Forest fit time\"\"\"\n    X = np.random.rand(1000, 10)\n    clf = IsolationForest(n_estimators=50)\n    start_time = time.time()\n    clf.fit(X)\n    end_time = time.time()\n    assert end_time - start_time < 5.0\n", "def test_threshold_deprecation():\n    X = iris.data\n    clf = IsolationForest(contamination='auto').fit(X)\n    with pytest.warns(DeprecationWarning):\n        _ = clf.threshold_\n"], "sample_753": ["def test_logistic_regression_path_liblinear():\n    # Test that logistic_regression_path returns same result as LogisticRegression\n    # with liblinear solver.\n    X, y = make_classification(n_samples=50, n_features=5,\n                               random_state=0)\n    Cs = [1.0]\n    tol = 1e-8\n\n    coefs, _, _ = logistic_regression_path(X, y, Cs=Cs, fit_intercept=False,\n                                           tol=tol, solver='liblinear',\n                                           max_iter=100)\n    clf = LogisticRegression(solver='liblinear', C=Cs[0], fit_intercept=False,\n                             tol=tol, max_iter=100)\n    clf.fit(X, y)\n\n    assert_array_almost_equal(coefs[0], clf.coef_, decimal=6)\n", "def test_logistic_regression_solvers_convergence_warnings():\n    X, y = make_classification(n_features=10, n_informative=5, random_state=0)\n\n    # Solvers should raise ConvergenceWarning when max_iter is low\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, max_iter=1)\n        assert_warns(ConvergenceWarning, clf.fit, X, y)\n", "def test_saga_sparse_binary():\n    # Test that SAGA solver on sparse data gives the same result as dense data\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    X[X < 1.0] = 0.0\n    csr = sp.csr_matrix(X)\n\n    clf_dense = LogisticRegression(solver='saga', random_state=42)\n    clf_sparse = LogisticRegression(solver='saga', random_state=42)\n    clf_dense.fit(X, y)\n    clf_sparse.fit(csr, y)\n    assert_array_almost_equal(clf_dense.coef_, clf_sparse.coef_)\n    assert_array_almost_equal(clf_dense.intercept_, clf_sparse.intercept_)\n", "def test_logistic_regressioncv_sparse_data():\n    # Test that LogisticRegressionCV works with sparse data\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    X_sparse = sp.csr_matrix(X)\n\n    for solver in ['liblinear', 'sag', 'saga']:\n        clf_sparse = LogisticRegressionCV(solver=solver, max_iter=1000,\n                                          random_state=42)\n        clf_dense = LogisticRegressionCV(solver=solver, max_iter=1000,\n                                         random_state=42)\n        clf_sparse.fit(X_sparse, y)\n        clf_dense.fit(X, y)\n        assert_array_almost_equal(clf_sparse.coef_, clf_dense.coef_)\n", "def test_logreg_l2_saga_sparse_data():\n    # Because liblinear penalizes the intercept and saga does not, we do not\n    # fit the intercept to make it possible to compare the coefficients of\n    # the two models at convergence.\n    rng = np.random.RandomState(42)\n    n_samples = 50\n    X, y = make_classification(n_samples=n_samples, n_features=20,\n                               random_state=0)\n    X_noise = rng.normal(scale=0.1, size=(n_samples, 3))\n    X_constant = np.zeros(shape=(n_samples, 2))\n    X = np.concatenate((X, X_noise, X_constant), axis=1)\n    X[X < 1] = 0\n    X = sparse.csr_matrix(X)\n\n    lr_liblinear = LogisticRegression(penalty=\"l2\", C=1.0, solver='liblinear',\n                                      fit_intercept=False,\n                                      tol=1e-10)\n    lr_liblinear.fit(X, y)\n\n    lr_saga = LogisticRegression(penalty=\"l2\", C=1.0, solver='saga',\n                                 fit_intercept=False,\n                                 max_iter=1000, tol=1e-10)\n    lr_saga.fit(X, y)\n    assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)\n\n    # Noise and constant features should be regularized to zero by the l2\n    # penalty\n    assert_array_almost_equal(lr_liblinear.coef_[0, -5:], np.zeros(5))\n    assert_array_almost_equal(lr_saga.coef_[0, -5:], np.zeros(5))\n\n    # Check that solving on the sparse and dense data yield the same results\n    lr_saga_dense = LogisticRegression(penalty=\"l2\", C=1.0, solver='saga',\n                                       fit_intercept=False,\n                                       max_iter=1000, tol=1e-10)\n    lr_saga_dense.fit(X.toarray(), y)\n    assert_array_almost_equal(lr_saga.coef_, lr_saga_dense.coef_)\n"], "sample_754": ["def test_sparse_pca_init(norm_comp):\n    n_components = 5\n    U_init = np.random.rand(10, n_components)\n    V_init = np.random.rand(n_components, 8)\n    spca = SparsePCA(n_components=n_components, U_init=U_init, V_init=V_init,\n                     normalize_components=norm_comp)\n    assert_equal(spca.U_init.shape, (10, n_components))\n    assert_equal(spca.V_init.shape, (n_components, 8))\n    assert_array_almost_equal(spca.U_init, U_init)\n    assert_array_almost_equal(spca.V_init, V_init)\n", "def test_sparse_pca_verbose(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng, verbose=10,\n                     normalize_components=norm_comp)\n    spca.fit(X)\n    assert_true(spca.n_iter_ > 0)\n", "def test_sparse_pca_verbose(norm_comp):\n    alpha = 1\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # wide array\n    spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,\n                          random_state=0, normalize_components=norm_comp,\n                          verbose=True)\n    spca_lars.fit(Y)\n    assert_true(spca_lars.n_iter_ > 0)\n", "def test_sparse_pca_warns(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng,\n                     normalize_components=norm_comp)\n    warning_msg = \"normalize_components=False is a\"\n    assert_warns_message(DeprecationWarning, warning_msg, spca.fit, X)\n", "def test_sparse_pca_init(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng,\n                     normalize_components=norm_comp)\n\n    # Test that U_init and V_init are used correctly\n    U_init = rng.randn(12, 8)\n    V_init = rng.randn(8, 10)\n    spca = SparsePCA(n_components=8, random_state=rng, U_init=U_init,\n                     V_init=V_init, normalize_components=norm_comp)\n    assert_array_almost_equal(spca.U_init, U_init)\n    assert_array_almost_equal(spca.V_init, V_init)\n\n    # Test that max_iter is set correctly\n    spca = SparsePCA(n_components=8, random_state=rng, max_iter=100,\n                     normalize_components=norm_comp)\n    assert_equal(spca.max_iter, 100)\n"], "sample_755": ["def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  0.4561403518613277)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert isinstance(score, float)\n    assert score >= 0.\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert score > 0.\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    assert davies_bouldin_score(X, labels) > 0.\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                      [0] * 5 + [1] * 5)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert score >= 0.\n"], "sample_756": ["def test_cluster_tree():\n    # Test building the cluster tree\n    reachability_plot = np.array([np.inf, 0.9, 0.9, 1.0, 0.89, 0.88, 10, .9, .9, .9, 10, 0.9,\n                                  0.9, 0.89, 0.88, 10, .9, .9, .9, .9])\n    ordering = np.r_[0:20]\n    local_maxima_points = _find_local_maxima(reachability_plot, 5)\n\n    root_node = _TreeNode(ordering, 0, 20, None)\n    _cluster_tree(root_node, None, local_maxima_points, reachability_plot, ordering, 5, .75, .7, .4, .3)\n\n    assert_equal(root_node.split_point, local_maxima_points[0])\n    assert_equal(len(root_node.children), 2)\n    assert_array_equal(root_node.children[0].points, ordering[:6])\n", "def test_optics_fit_raise_value_error_when_input_is_not_supported():\n    msg = (\"OPTICS does not support input with the given type/structure\")\n    unsupported_data_types = [\"string\", 1, None]\n    for data in unsupported_data_types:\n        clust = OPTICS(min_samples=5)\n        assert_raise_message(ValueError, msg, clust.fit, data)\n", "def test_optics_fit_predict():\n    # Test that fit_predict works and returns labels identical to those obtained\n    # by calling fit followed by predict.\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    clust1 = OPTICS(min_samples=10).fit(X)\n    labels1 = clust1.labels_\n\n    clust2 = OPTICS(min_samples=10)\n    labels2 = clust2.fit_predict(X)\n\n    assert_array_equal(labels1, labels2)\n", "def test_optics_fit_parameter_passing():\n    # Ensure that parameters passed to fit are properly propagated\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    # Compute OPTICS\n    clust1 = OPTICS(min_samples=10, metric='euclidean', p=2,\n                    max_eps=5.0 * 6.0, algorithm='ball_tree',\n                    leaf_size=30, n_jobs=None).fit(X)\n\n    # pass parameters to fit method\n    clust2 = OPTICS().fit(X, min_samples=10, metric='euclidean', p=2,\n                          max_eps=5.0 * 6.0, algorithm='ball_tree',\n                          leaf_size=30, n_jobs=None)\n\n    assert_array_equal(clust1.labels_, clust2.labels_)\n    assert_array_equal(clust1.core_sample_indices_, clust2.core_sample_indices_)\n    assert_array_equal(clust1.ordering_, clust2.ordering_)\n    assert_array_equal(clust1.core_distances_, clust2.core_distances_)\n    assert_array_equal(clust1.reachability_, clust2.reachability_)\n    assert_array_equal(clust1.predecessor_, clust2.predecessor_)\n", "def test_core_distances():\n    # Test that core distances are calculated correctly\n\n    # Parameters chosen specifically for this task\n    X = [[1, 2], [3, 4], [5, 6]]\n    neighbors = NearestNeighbors(n_neighbors=2).fit(X)\n\n    clust = OPTICS(min_samples=2)\n    core_distances = clust._compute_core_distances_(X, neighbors)\n\n    # Check that core distances are calculated correctly\n    assert_array_equal(core_distances, [np.sqrt(8), np.sqrt(8), np.inf])\n"], "sample_757": ["def test_encoder_get_feature_names_empty():\n    enc = OneHotEncoder()\n    X = np.array([[1, 2], [3, 4]], dtype='int64')\n    enc.fit(X)\n\n    # get feature names without input_features\n    with pytest.raises(NotFittedError):\n        enc.get_feature_names()\n\n    # get feature names with empty list as input_features\n    assert_array_equal(enc.get_feature_names([]), [])\n\n    # get feature names with input_features of length not matching n_features\n    msg = (\"input_features should have length equal to number of features \"\n           \"(2), got 1\")\n    with pytest.raises(ValueError, match=msg):\n        enc.get_feature_names(['A'])\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_encoder_get_feature_names_with_numpy_array_input_features():\n    enc = OneHotEncoder()\n    X = np.array([['a', 1, 'cat'], ['b', 2, 'dog']])\n    enc.fit(X)\n\n    feature_names = enc.get_feature_names(np.array(['A', 'B', 'C']))\n    assert isinstance(feature_names, np.ndarray)\n    assert_array_equal(['A_a', 'A_b', 'B_1', 'B_2', 'C_cat', 'C_dog'],\n                       feature_names)\n", "def test_ordinal_encoder_not_fitted():\n    X = np.array([['a'], ['b']])\n    enc = OrdinalEncoder()\n    msg = (\"This OrdinalEncoder instance is not fitted yet. \"\n           \"Call 'fit' with appropriate arguments before using this method.\")\n    with pytest.raises(NotFittedError, match=msg):\n        enc.transform(X)\n", "def test_ordinal_encoder_set_params():\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[[0, 1, 2, 3]])\n    assert enc.get_params()['categories'] == [[0, 1, 2, 3]]\n    assert enc.fit_transform(X).shape == (2, 1)\n    # set params on already fitted object\n    enc.set_params(categories=[[0, 1, 2, 3, 4]])\n    assert enc.fit_transform(X).shape == (2, 1)\n"], "sample_758": ["def test_check_array_allow_nd():\n    X = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n    check_array(X, allow_nd=True)  # doesn't raise\n\n    X = np.array([[[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]])\n    check_array(X, allow_nd=True)  # doesn't raise\n", "def test_check_X_y_multi_output():\n    X = np.ones((10, 2))\n    y = np.ones((10, 3))\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    # Test that y is checked for finiteness\n    y[0, 0] = np.nan\n    assert_raises_regex(ValueError, \"Input contains NaN\", check_X_y, X, y,\n                        multi_output=True)\n", "def test_check_X_y_with_invalid_dtype():\n    X = np.ones((2, 2))\n    y = np.array([1, 2], dtype=object)\n    msg = \"Unknown label type: 'object'\"\n    assert_raise_message(ValueError, msg, check_X_y, X, y)\n", "def test_check_X_y_with_multiple_samples_and_features():\n    # Test that check_X_y handles multiple samples and features correctly\n    X = np.random.rand(10, 5)\n    y = np.random.rand(10)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n\n    # Test with multiple outputs (y has shape (n_samples, n_outputs))\n    y = np.random.rand(10, 3)\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X, X_checked)\n    assert_array_equal(y, y_checked)\n", "def test_check_X_y_multilabel_sequence():\n    X = [[1, 2], [3, 4]]\n    y = [(1, 2), (3, 4)]\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # Test with a numpy array of lists\n    y = np.array([(1, 2), (3, 4)], dtype=object)\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n"], "sample_759": ["def test_one_hot_encoder_get_feature_names_without_input_features():\n    enc = OneHotEncoder()\n    X = [['Male', 1, 'girl', 2, 3],\n         ['Female', 41, 'girl', 1, 10],\n         ['Male', 51, 'boy', 12, 3],\n         ['Male', 91, 'girl', 21, 30]]\n\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert isinstance(feature_names, np.ndarray)\n\n    assert_array_equal(['x0_Female', 'x0_Male',\n                        'x1_1', 'x1_41', 'x1_51', 'x1_91',\n                        'x2_boy', 'x2_girl',\n                        'x3_1', 'x3_2', 'x3_12', 'x3_21',\n                        'x4_3',\n                        'x4_10', 'x4_30'], feature_names)\n", "def test_encoder_get_feature_names_dataframe():\n    # check get_feature_names with dataframe input\n    pd = pytest.importorskip('pandas')\n\n    X = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n    enc = OneHotEncoder()\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0_1', 'x0_2', 'x1_3', 'x1_4'], feature_names)\n\n    # custom feature names\n    feature_names_custom = enc.get_feature_names(['col1', 'col2'])\n    assert_array_equal(['col1_1', 'col1_2', 'col2_3', 'col2_4'],\n                       feature_names_custom)\n\n    # unicode in column names\n    X = pd.DataFrame({u'c\u2764t1': [1, 2], u'dat2': [3, 4]})\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal([u'x0_1', u'x0_2', u'x1_3', u'x1_4'], feature_names)\n\n    feature_names_custom = enc.get_feature_names([u'n\ud83d\udc4dme1', u'n\ud83d\udc4dme2'])\n    assert_array_equal([u'n\ud83d\udc4dme1_1', u'n\ud83d\udc4dme1_2', u'n\ud83d\udc4dme2_3', u'n\ud83d\udc4dme2_4'],\n                       feature_names_custom)\n", "def test_ordinal_encoder_handle_unknown_strings():\n    X = np.array(['11111111', '22', '333', '4444']).reshape((-1, 1))\n    X2 = np.array(['55555', '22']).reshape((-1, 1))\n    # Non Regression test for the issue #12470\n    # Test the ignore option, when categories are numpy string dtype\n    # particularly when the known category strings are larger\n    # than the unknown category strings\n    oh = OrdinalEncoder(handle_unknown='ignore',\n                        categories=[['11111111', '22', '333']])\n    oh.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(\n        oh.transform(X2_passed),\n        np.array([[np.nan], [1.]]))\n    # ensure transformed data was not modified in place\n    assert_array_equal(X2, X2_passed)\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert_array_equal(enc.fit(X).transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n", "def test_ordinal_encoder_unsorted_categories():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(categories=[['b', 'a', 'c']])\n    exp = np.array([[1.], [0.]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].tolist() == ['b', 'a', 'c']\n    assert np.issubdtype(enc.categories_[0].dtype, np.object_)\n\n    # unsorted passed categories still raise for numerical values\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder(categories=[[2, 1, 3]])\n    msg = 'Unsorted categories are not supported'\n    with pytest.raises(ValueError, match=msg):\n        enc.fit_transform(X)\n"], "sample_760": ["def test_get_scorer_raises_error():\n    # Test that get_scorer raises an error for unknown scorers\n    with pytest.raises(ValueError):\n        get_scorer('unknown_scorer')\n", "def test_get_scorer_error_message():\n    # Test that get_scorer raises a ValueError with a good error message.\n    with pytest.raises(ValueError, match=\".*valid scoring value.*\"):\n        get_scorer('not_a_scorer')\n", "def test_make_scorer_with_kwargs():\n    # Test that make_scorer can handle keyword arguments.\n    f = lambda y_true, y_pred, **kwargs: 0\n    scorer = make_scorer(f, greater_is_better=True, needs_proba=False,\n                         needs_threshold=False, some_kwarg='some_value')\n    assert scorer._kwargs == {'some_kwarg': 'some_value'}\n", "def test_make_scorer_with_kwargs():\n    # Test that make_scorer can handle keyword arguments.\n    scorer = make_scorer(fbeta_score, beta=2)\n    score1 = scorer(EstimatorWithFitAndPredict(), [[1]], [1])\n    score2 = fbeta_score([1], [1], beta=2)\n    assert_almost_equal(score1, score2)\n\n    # Test that make_scorer raises an error when kwargs are invalid.\n    assert_raises(TypeError, make_scorer, fbeta_score, invalid_kwarg='value')\n", "def test_make_scorer_with_kwargs():\n    # Test that make_scorer captures kwargs correctly.\n    f = lambda *args, **kwargs: 0\n    scorer = make_scorer(f, beta=2)\n    assert_equal(scorer._kwargs, {'beta': 2})\n    scorer = make_scorer(f, beta=2, average='weighted')\n    assert_equal(scorer._kwargs, {'beta': 2, 'average': 'weighted'})\n"], "sample_761": ["def test_iterative_imputer_min_value_parameter():\n    # Test that min_value parameter is used correctly in IterativeImputer\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10, random_state=rng).toarray()\n    missing_flag = X == 0\n    X[missing_flag] = np.nan\n\n    imputer = IterativeImputer(max_iter=1, min_value=5)\n    X_imputed = imputer.fit_transform(X)\n\n    assert np.min(X_imputed) >= 5\n", "def test_imputation_add_indicator_sparse():\n    X = sparse.csr_matrix(np.array([\n        [0, 1,      5,       0, 1],\n        [2,      0, 1,       0, 2],\n        [6,      3,      0,  0, 3],\n        [1,      2,      9,  0, 4]\n    ]))\n\n    X_true = np.array([\n        [3., 1., 5., 1., 1., 0., 0., 1.],\n        [2., 2., 1., 2., 0., 1., 0., 1.],\n        [6., 3., 5., 3., 0., 0., 1., 1.],\n        [1., 2., 9., 4., 0., 0., 0., 1.]\n    ])\n\n    imputer = SimpleImputer(missing_values=0, add_indicator=True)\n    X_trans = imputer.fit_transform(X)\n\n    assert_allclose(X_trans.toarray(), X_true)\n    assert_array_equal(imputer.indicator_.features_, np.array([0, 1, 2, 3]))\n", "def test_iterative_imputer_min_value_max_value():\n    rng = np.random.RandomState(0)\n    X = np.array([[1, 2], [np.nan, 6], [7, np.nan]])\n\n    imputer = IterativeImputer(min_value=5, max_value=10)\n    X_imputed = imputer.fit_transform(X)\n\n    assert_allclose(np.min(X_imputed), 5)\n    assert_allclose(np.max(X_imputed), 7)\n", "def test_iterative_imputer_feature_names_out():\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10,\n                             random_state=rng).toarray()\n\n    imputer = IterativeImputer(max_iter=1,\n                               random_state=rng)\n    imputer.fit(X)\n\n    feature_names = [f'feature_{i}' for i in range(d)]\n    assert_array_equal(imputer.get_feature_names_out(feature_names), feature_names)\n", "def test_iterative_imputer_parallel():\n    # Check that IterativeImputer can handle parallel jobs correctly\n    rng = np.random.RandomState(0)\n    n = 100\n    d = 10\n    X = sparse_random_matrix(n, d, density=0.10,\n                             random_state=rng).toarray()\n\n    imputer_single_core = IterativeImputer(max_iter=1,\n                                           n_nearest_features=5,\n                                           random_state=rng,\n                                           n_jobs=1)\n    imputer_multi_core = IterativeImputer(max_iter=1,\n                                          n_nearest_features=5,\n                                          random_state=rng,\n                                          n_jobs=-1)\n\n    X_imputed_single_core = imputer_single_core.fit_transform(X)\n    X_imputed_multi_core = imputer_multi_core.fit_transform(X)\n\n    assert_allclose(X_imputed_single_core, X_imputed_multi_core)\n"], "sample_762": ["def test_is_regressor():\n    svc = SVC()\n    assert_false(is_regressor(svc))\n    assert is_regressor(DecisionTreeRegressor())\n    assert is_regressor(GridSearchCV(DecisionTreeRegressor(), {'max_depth': [3]}))\n    assert is_regressor(Pipeline([('rgr', DecisionTreeRegressor())]))\n    assert_true(is_regressor(Pipeline(\n        [('rgr_cv', GridSearchCV(DecisionTreeRegressor(), {'max_depth': [3]}))])))\n", "def test_is_regressor():\n    regressor = DecisionTreeRegressor()\n    assert is_regressor(regressor)\n    assert is_regressor(GridSearchCV(regressor, {}))\n    assert is_regressor(Pipeline([('reg', regressor)]))\n    assert_true(is_regressor(Pipeline([('reg_cv', GridSearchCV(regressor, {}))])))\n    classifier = DecisionTreeClassifier()\n    assert_false(is_regressor(classifier))\n", "def test_set_params_with_typed_dict():\n    # Check that set_params works with a TypedDict\n    from typing import TypedDict\n\n    class MyParams(TypedDict):\n        max_depth: int\n        min_samples_leaf: int\n\n    params: MyParams = {'max_depth': 5, 'min_samples_leaf': 2}\n    est = DecisionTreeClassifier()\n    est.set_params(**params)\n    assert est.max_depth == 5\n    assert est.min_samples_leaf == 2\n", "def test_repr_works_with_numpy_array():\n    # Test that repr works when an estimator contains a numpy array\n    estimator = MyEstimator(empty=np.array([1, 2, 3]))\n    repr(estimator)\n", "def test_is_regressor():\n    svc = SVC()\n    assert_false(is_regressor(svc))\n    assert_true(is_regressor(DecisionTreeRegressor()))\n    assert_true(is_regressor(Pipeline([('reg', DecisionTreeRegressor())])))\n    assert_true(is_regressor(GridSearchCV(DecisionTreeRegressor(), {})))\n"], "sample_763": ["def test_num_samples():\n    # Test _num_samples function on various inputs.\n    assert_equal(_num_samples([[1, 2], [3, 4]]), 2)\n    assert_equal(_num_samples(np.array([[1, 2], [3, 4]])), 2)\n    assert_equal(_num_samples(sp.csr_matrix(np.array([[1, 2], [3, 4]]))), 2)\n\n    # Test on single sample\n    assert_equal(_num_samples([1]), 1)\n    assert_equal(_num_samples(np.array([1])), 1)\n    assert_equal(_num_samples(sp.csr_matrix(np.array([1]))), 1)\n\n    # Test on sparse matrix with no shape information\n    class NoShapeSparseMatrix:\n            self.data = np.array([1])\n            self.indptr = np.array([0, 1])\n            self.indices = np.array([0])\n\n    assert_equal(_num_samples(NoShapeSparseMatrix()), 1)\n", "def test_check_X_y_non_numeric_shape():\n    X = TestNonNumericShape()\n    y = [1, 2, 3]\n    assert_raises_regex(TypeError, \"Expected sequence or array-like\",\n                        check_X_y, X, y)\n", "def test__num_samples_raises_error_on_estimator():\n    # Test that _num_samples raises a TypeError when given an estimator.\n    X = KNeighborsClassifier()\n    assert_raises_regex(TypeError, \"Expected sequence or array-like, got \"\n                        \"estimator KNeighborsClassifier\", _num_samples, X)\n", "def test__num_samples():\n    # Test that _num_samples returns the correct number of samples for different\n    # types of inputs.\n    X = np.array([[1, 2], [3, 4]])\n    assert_equal(_num_samples(X), 2)\n\n    X = np.array([1, 2, 3])\n    assert_equal(_num_samples(X), 3)\n\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    assert_equal(_num_samples(X), 2)\n\n    X = [(1, 2), (3, 4)]\n    assert_equal(_num_samples(X), 2)\n\n    class CustomClass:\n            return 5\n\n    X = CustomClass()\n    assert_equal(_num_samples(X), 5)\n\n    # Check that _num_samples raises an error when the input is not supported.\n    X = \"not supported\"\n    assert_raises(TypeError, _num_samples, X)\n", "def test__num_samples_typeerror():\n    # Check that _num_samples raises a TypeError when given an estimator\n    class TestEstimator:\n            pass\n\n    assert_raises_regex(TypeError, 'estimator', _num_samples, TestEstimator())\n"], "sample_764": ["def test_column_transformer_with_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights={'trans1': 2, 'trans2': 3})\n\n    X_res_first = 2 * np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_second = 3 * np.array([2, 4, 6]).reshape(-1, 1)\n\n    assert_array_equal(ct.fit_transform(X_array), np.hstack((X_res_first, X_res_second)))\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.hstack((X_res_first, X_res_second)))\n", "def test_column_transformer_empty_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # if all columns are specified in transformers, remainder should be None\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])])\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n\n    # if all columns are specified in transformers, remainder should be None\n    ct = ColumnTransformer([('trans1', Trans(), [0, 1])])\n    assert_array_equal(ct.fit_transform(X_array), X_array)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_array)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] != 'remainder'\n", "def test_column_transformer_remainder_with_sparse_threshold():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_second = np.array([2, 4, 6]).reshape(-1, 1)\n\n    # test with sparse_threshold=0.8\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.8)\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert_array_equal(X_trans.toarray(), np.hstack((X_res_first, np.eye(3))))\n\n    # test with sparse_threshold=0.2\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.2)\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n    assert_array_equal(X_trans, np.hstack((X_res_first, np.eye(3))))\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n\n    # get_feature_names should include names from the remainder\n    assert_equal(ct.get_feature_names(), ['trans__x0', 'remainder__x1'])\n\n    # if remainder is 'passthrough' or 'drop', get_feature_names should\n    # still work\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder='passthrough')\n    ct.fit(X_array)\n    assert_raise_message(\n        NotImplementedError, 'get_feature_names is not yet supported',\n        ct.get_feature_names)\n\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder='drop')\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), ['trans__x0'])\n", "def test_column_transformer_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # weights are applied to the transformed columns\n    transformer_weights = {'trans1': .1, 'trans2': 10}\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights=transformer_weights)\n    exp = np.vstack([transformer_weights['trans1'] * X_array[:, 0],\n                     transformer_weights['trans2'] * X_array[:, 1]]).T\n    assert_array_equal(ct.fit_transform(X_array), exp)\n\n    # weights can be changed after fitting\n    transformer_weights = {'trans1': .5, 'trans2': 2}\n    ct.set_params(transformer_weights=transformer_weights)\n    exp = np.vstack([transformer_weights['trans1'] * X_array[:, 0],\n                     transformer_weights['trans2'] * X_array[:, 1]]).T\n    assert_array_equal(ct.transform(X_array), exp)\n\n    # unknown transformers in transformer_weights are ignored\n    transformer_weights = {'trans1': .1, 'unknown': 10}\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1])],\n                           transformer_weights=transformer_weights)\n    exp = np.vstack([transformer_weights['trans1'] * X_array[:, 0],\n                     X_array[:, 1]]).T\n    assert_array_equal(ct.fit_transform(X_array), exp)\n\n    # weights for remainder transformers are ignored\n    transformer_weights = {'remainder': .1}\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder='passthrough',\n                           transformer_weights=transformer_weights)\n    exp = X_array\n    assert_array_equal(ct.fit_transform(X_array), exp)\n"], "sample_765": ["def test_f1_score_pos_label_mismatch():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n\n    # providing pos_label when average is not 'binary' should raise an error\n    assert_raises(ValueError, f1_score, y_true, y_pred,\n                  average='macro', pos_label=1)\n    assert_raises(ValueError, f1_score, y_true, y_pred,\n                  average='weighted', pos_label=1)\n    assert_raises(ValueError, f1_score, y_true, y_pred,\n                  average='micro', pos_label=1)\n", "def test_balanced_accuracy_score_adjusted():\n    y_true = [0, 1, 1, 1, 0]\n    y_pred = [0, 0, 1, 1, 1]\n\n    # Calculate the chance for this case\n    n_classes = len(np.unique(y_true))\n    chance = 1 / n_classes\n\n    # Calculate the true balanced accuracy\n    true_ba = (recall_score(y_true, y_pred, pos_label=0) +\n               recall_score(y_true, y_pred, pos_label=1)) / 2\n\n    # Calculate the adjusted balanced accuracy\n    adjusted_ba = (true_ba - chance) / (1 - chance)\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred, adjusted=True),\n                        adjusted_ba)\n", "def test_balanced_accuracy_score_adjusted():\n    y_true = [0, 1, 1, 1, 1, 1, 1, 1]\n    y_pred = [0, 1, 1, 1, 1, 1, 1, 1]\n\n    # With adjustment\n    adjusted_score = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    assert_almost_equal(adjusted_score, 0)\n\n    # Without adjustment\n    score = balanced_accuracy_score(y_true, y_pred, adjusted=False)\n    assert_almost_equal(score, 1)\n", "def test_brier_score_loss_with_sample_weight():\n    # Check brier_score_loss function with sample weights\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    sample_weight = np.array([1., 2., 3., 4., 5., 6.])\n    true_score = np.average((y_true - y_pred) ** 2, weights=sample_weight)\n\n    assert_almost_equal(brier_score_loss(y_true, y_true, sample_weight), 0.0)\n    assert_almost_equal(brier_score_loss(y_true, y_pred, sample_weight),\n                        true_score)\n    assert_almost_equal(brier_score_loss(1. + y_true, y_pred, sample_weight),\n                        true_score)\n    assert_almost_equal(brier_score_loss(2 * y_true - 1, y_pred,\n                                         sample_weight), true_score)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred[1:],\n                  sample_weight)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred + 1.,\n                  sample_weight)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred - 1.,\n                  sample_weight)\n    assert_raises(ValueError, brier_score_loss, y_true, y_pred,\n                  sample_weight[:-1])\n", "def test_balanced_accuracy_score_binary():\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 2, 1]\n\n    # true balanced accuracy (non-adjusted)\n    recall_0 = 3 / 4\n    recall_1 = 1 / 2\n    true_ba = (recall_0 + recall_1) / 2\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), true_ba)\n\n    # true balanced accuracy (adjusted)\n    adjusted_ba = (recall_0 + recall_1 - 1) / 2 * 2\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred, adjusted=True),\n                        adjusted_ba)\n"], "sample_766": ["def test_dict_learning_online_init_inner_stats():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    A = rng.randn(n_components, n_components)\n    B = rng.randn(n_features, n_components)\n    dico = MiniBatchDictionaryLearning(n_components, n_iter=10,\n                                       dict_init=V, random_state=0,\n                                       inner_stats=(A, B)).fit(X)\n    assert_array_equal(dico.inner_stats_[0], A)\n    assert_array_equal(dico.inner_stats_[1], B)\n", "def test_sparse_coder_estimator_input():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    Xf = check_array(X, order='F')\n    for algo in ('lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'):\n        coder = SparseCoder(dictionary=V, transform_algorithm=algo)\n        a = coder.transform(X)\n        b = coder.transform(Xf)\n        assert_array_almost_equal(a, b)\n", "def test_minibatch_dictionary_learning_partial_fit_readonly():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X),\n                                        batch_size=1,\n                                        alpha=1, shuffle=False, dict_init=V,\n                                        random_state=0).fit(X)\n    dict2 = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                        n_iter=1, dict_init=V,\n                                        random_state=0)\n    for i in range(10):\n        for sample in X:\n            sample.flags.writeable = False\n            dict2.partial_fit(sample[np.newaxis, :])\n\n    assert not np.all(sparse_encode(X, dict1.components_, alpha=1) == 0)\n    assert_array_almost_equal(dict1.components_, dict2.components_,\n                              decimal=2)\n", "def test_dict_learning_online_n_components():\n    n_components = X.shape[1]\n    dico = MiniBatchDictionaryLearning(n_components=None, random_state=0)\n    dico.fit(X)\n    assert_equal(dico.components_.shape, (n_components, n_features))\n", "def test_minibatch_dictionary_learning_partial_fit_input():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    dict_ = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                        n_iter=1, dict_init=V,\n                                        random_state=0)\n\n    Xf = check_array(X, order='F')\n    dict_.partial_fit(X)\n    dict_f = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                         n_iter=1, dict_init=V,\n                                         random_state=0)\n    dict_f.partial_fit(Xf)\n    assert_array_almost_equal(dict_.components_, dict_f.components_)\n"], "sample_767": ["def test_column_transformer_remainder_with_sparse_threshold():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # remainder is a transformer with dense output and sparse_threshold=0.5\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=StandardScaler(),\n                           sparse_threshold=0.5)\n    assert not sparse.issparse(ct.fit_transform(X_array))\n\n    # remainder is a transformer with sparse output and sparse_threshold=0.5\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.5)\n    assert sparse.issparse(ct.fit_transform(X_array))\n", "def test_column_transformer_empty_input():\n    # Test that ColumnTransformer works with empty input\n    X_array = np.array([[]])\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder='drop')\n    assert_array_equal(ct.fit_transform(X_array), np.array([[]]))\n    assert_array_equal(ct.fit(X_array).transform(X_array), np.array([[]]))\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert ct.transformers_[-1][1] == 'drop'\n    assert_array_equal(ct.transformers_[-1][2], [])\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    feature_names = ct.get_feature_names()\n    assert_equal(feature_names, ['trans__x0', 'remainder__x1'])\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(), [1])],\n                           remainder='passthrough')\n    with pytest.raises(NotImplementedError):\n        ct.fit(X_array).get_feature_names()\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n    with pytest.raises(AttributeError):\n        ct.fit(X_array).get_feature_names()\n", "def test_column_transformer_sparse_remainder_empty():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0, 1, 2])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.8)\n\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3)\n\n    exp_array = X_array\n    assert_array_equal(X_trans, exp_array)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] != 'remainder'\n"], "sample_768": ["def test_repr():\n    # Test that repr works for all cross-validation classes.\n    cv_classes = [\n        KFold, StratifiedKFold, GroupKFold, TimeSeriesSplit,\n        LeaveOneOut, LeavePOut, ShuffleSplit, GroupShuffleSplit,\n        StratifiedShuffleSplit, LeaveOneGroupOut, LeavePGroupsOut,\n        PredefinedSplit, RepeatedKFold, RepeatedStratifiedKFold\n    ]\n\n    for cv_class in cv_classes:\n        cv = cv_class()\n        repr(cv)\n", "def test_validate_shuffle_split_init():\n    # Test that _validate_shuffle_split_init raises errors for invalid input\n    assert_raises(ValueError, _validate_shuffle_split_init, None, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, 0.6)\n    assert_raises(TypeError, _validate_shuffle_split_init, 'a', 0.2)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.1, -0.1)\n    assert_raises(ValueError, _validate_shuffle_split_init, 1.1, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, None, 1.1)\n", "def test_validate_shuffle_split_init():\n    # Test validation of test_size and train_size at init\n\n    # If neither is specified, test_size defaults to 0.25 (for now)\n    # this will change in version 0.21\n    with warnings.catch_warnings(record=True) as w:\n        ss = ShuffleSplit()\n        assert len(w) == 1\n        assert str(w[0].message) == \"You should specify a value for 'test_size'\"\n\n    # Both can be specified\n    ss = ShuffleSplit(test_size=2, train_size=3)\n\n    # Check that an error is raised if test_size or train_size are invalid\n    assert_raises(ValueError, ShuffleSplit, test_size=1.1)\n    assert_raises(ValueError, ShuffleSplit, test_size=0)\n    assert_raises(ValueError, ShuffleSplit, test_size=-1)\n    assert_raises(ValueError, ShuffleSplit, test_size=11)\n    assert_raises(ValueError, ShuffleSplit, test_size='a')\n    assert_raises(TypeError, ShuffleSplit, test_size=None, train_size=None)\n    assert_raises(ValueError, ShuffleSplit, train_size=1.1)\n    assert_raises(ValueError, ShuffleSplit, train_size=0)\n    assert_raises(ValueError, ShuffleSplit, train_size=-1)\n    assert_raises(ValueError, ShuffleSplit, train_size=11)\n    assert_raises(ValueError, ShuffleSplit, train_size='a')\n\n    # Check that an error is raised if test_size + train_size > 1\n    assert_raises(ValueError, ShuffleSplit, test_size=0.7, train_size=0.4)\n\n    # Check that an error is raised if test_size + train_size < 1\n    assert_raises(ValueError, ShuffleSplit, test_size=0.1, train_size=0.1)\n", "def test_validate_shuffle_split():\n    # Test _validate_shuffle_split\n    n_samples = 10\n    for test_size, train_size in [(3, None), (None, 7), (0.2, None), (None, 0.8)]:\n        n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n        assert n_train + n_test == n_samples\n\n    # Test invalid arguments\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, test_size='invalid')\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, test_size=11)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, test_size=1.1)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, test_size=-1)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, test_size=0)\n\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, train_size='invalid')\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, train_size=11)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, train_size=1.1)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, train_size=-1)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, train_size=0)\n\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, test_size=5, train_size=6)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, test_size=0.5, train_size=0.6)\n", "def test_validate_shuffle_split_init():\n    # Test _validate_shuffle_split_init function\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, 'a', None)\n    assert_raises(TypeError, _validate_shuffle_split_init, 0.5, 'a')\n    assert_raises(ValueError, _validate_shuffle_split_init, 1.2, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, None, 1.2)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, 0.4, 0.1)\n"], "sample_769": ["def test_balanced_accuracy_score_binary_averaged():\n    y_true = np.array([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1])\n    y_pred = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1])\n\n    # compute scores with default labels introspection\n    bas = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(bas, 0.5)\n", "def test_jaccard_similarity_score_multilabel_1():\n    # Test jaccard similarity score for multilabel case\n    y_true = np.array([[1, 0, 1], [0, 1, 1]])\n    y_pred = np.array([[1, 1, 0], [0, 0, 1]])\n\n    # jaccard similarity is 0.5 for first sample and 0.5 for second sample,\n    # which gives an average jaccard similarity of 0.5\n    assert_almost_equal(jaccard_similarity_score(y_true, y_pred), 0.5)\n    assert_almost_equal(jaccard_similarity_score(y_true, y_pred, normalize=False), 1.)\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n\n    # compute balanced accuracy with default sample weight\n    score = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(score, 0.3333, decimal=4)\n\n    # compute balanced accuracy with sample weight\n    sample_weight = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n    score = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n    assert_almost_equal(score, 0.3333, decimal=4)\n\n    # compute balanced accuracy with adjusted parameter\n    score = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    assert_almost_equal(score, -0.6666, decimal=4)\n", "def test_balanced_accuracy_score_binary_averaged():\n    y_true = np.array([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1])\n    y_pred = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1])\n\n    # compute scores with default labels introspection\n    bac = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(bac, 0.5)\n\n    bac = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    assert_almost_equal(bac, 0.0)\n", "def test_balanced_accuracy_score_binary():\n    # Test balanced accuracy score for binary classification problems.\n    y_true = [0, 1, 0, 0, 1, 0, 1, 1]\n    y_pred = [0, 1, 0, 0, 0, 1, 1, 1]\n\n    # Calculate support (number of actual occurrences of each class)\n    support = np.bincount(y_true)\n\n    # Calculate true positives and false negatives\n    tp = np.sum([p == t for p, t in zip(y_pred, y_true)])\n    fn = np.sum([p != t and t == 1 for p, t in zip(y_pred, y_true)])\n\n    # Calculate recall (sensitivity)\n    recall = tp / support[1]\n\n    # Calculate specificity\n    tn = len(y_true) - support[1] - fn\n    specificity = tn / support[0]\n\n    # Calculate balanced accuracy\n    expected_score = (recall + specificity) / 2\n\n    # Compare with the implementation\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), expected_score)\n"], "sample_770": ["def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                          [0] * 5 + [1] * 5))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert_greater(score, 0.)\n    assert_less(score, np.inf)\n\n    # Test with sparse input\n    X_sparse = csr_matrix(X)\n    score_sparse = davies_bouldin_score(X_sparse, labels)\n    pytest.approx(score, score_sparse)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                           [0] * 5 + [1] * 5))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert_greater(score, 0.)\n    assert_less(score, 1.)\n\n    # Test with sparse input\n    X_sparse = sp.csr_matrix(X)\n    score_sparse = davies_bouldin_score(X_sparse, labels)\n    pytest.approx(score, score_sparse)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                           [0] * 5 + [1] * 5))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels), 0.60, abs=0.01)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                          [0] * 5 + [1] * 5))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  (0.5 + 0.5 + 0.5) / 4)\n\n    # Test with sparse matrix\n    X_sparse = csr_matrix(X)\n    pytest.approx(davies_bouldin_score(X_sparse, labels),\n                  (0.5 + 0.5 + 0.5) / 4)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                          [0] * 5 + [1] * 5))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert_greater(score, 0)\n"], "sample_771": ["def test_power_transformer_input_type():\n    # Check that PowerTransformer supports different input types\n    X = np.abs(X_2d)\n    pt = PowerTransformer(method='box-cox', standardize=False)\n\n    # numpy array\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, np.ndarray)\n\n    # pandas DataFrame\n    import pandas as pd\n    X_df = pd.DataFrame(X)\n    X_trans_df = pt.fit_transform(X_df)\n    assert isinstance(X_trans_df, np.ndarray)\n\n    # list of lists\n    X_list = X.tolist()\n    X_trans_list = pt.fit_transform(X_list)\n    assert isinstance(X_trans_list, np.ndarray)\n\n    # sparse matrix\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix(X)\n    X_trans_sparse = pt.fit_transform(X_sparse)\n    assert isinstance(X_trans_sparse, np.ndarray)\n", "def test_power_transformer_dtype():\n    # Ensure PowerTransformer preserves float dtype\n\n    X = np.abs(X_2d).astype(np.float32)\n    pt = PowerTransformer(method='box-cox')\n\n    X_trans = pt.fit_transform(X)\n    assert X.dtype == X_trans.dtype\n    assert pt.lambdas_.dtype == np.float64\n", "def test_power_transformer_dtype():\n    # check that PowerTransformer preserves the input dtype\n\n    pt = PowerTransformer()\n    X_float64 = np.random.RandomState(0).randn(100, 3)\n    X_float32 = X_float64.astype(np.float32)\n\n    for X in [X_float64, X_float32]:\n        X_trans = pt.fit_transform(X)\n        assert X.dtype == X_trans.dtype\n        X_inv = pt.inverse_transform(X_trans)\n        assert X.dtype == X_inv.dtype\n", "def test_power_transformer_sparse_input():\n    # Check that PowerTransformer supports sparse input\n    X = sparse.csr_matrix(X_2d)\n    pt = PowerTransformer(method='yeo-johnson')\n    pt.fit(X)\n    assert pt.lambdas_.shape == (X.shape[1],)\n    X_trans = pt.transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n    assert X_trans.shape == X.shape\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert isinstance(X_inv_trans, sparse.csr_matrix)\n    assert X_inv_trans.shape == X.shape\n", "def test_power_transformer_dtype():\n    # check that PowerTransformer preserves dtype of input when possible\n    X = np.abs(X_1col).astype(np.float32)\n    pt = PowerTransformer()\n    assert pt.fit_transform(X).dtype == X.dtype\n\n    X = np.abs(X_1col).astype(np.float64)\n    pt = PowerTransformer()\n    assert pt.fit_transform(X).dtype == X.dtype\n"], "sample_772": ["def check_base_estimator_subsetting(name):\n    # Test that base estimators can be subsetted\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(n_estimators=5, random_state=0)\n    est.fit(X, y)\n\n    # Check that accessing the base estimators directly works\n    assert len(est.estimators_) == 5\n\n    # Check that slicing the base estimators works\n    sliced_estimators = est.estimators_[1:3]\n    assert len(sliced_estimators) == 2\n\n    # Check that indexing the base estimators works\n    single_estimator = est.estimators_[0]\n    assert isinstance(single_estimator, type(est.base_estimator))\n", "def test_random_state_consistency():\n    # Test that different methods of specifying the random state give\n    # consistent results.\n    X, y = datasets.make_classification(n_samples=100, n_features=4,\n                                        n_informative=3, n_redundant=0,\n                                        random_state=42)\n\n    # Using an integer as the random state\n    clf_int = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf_int.fit(X, y)\n\n    # Using a numpy RandomState object\n    rng = np.random.RandomState(42)\n    clf_rng = RandomForestClassifier(n_estimators=10, random_state=rng)\n    clf_rng.fit(X, y)\n\n    # Using a numpy random state with the same seed\n    clf_np = RandomForestClassifier(n_estimators=10, random_state=np.random.RandomState(42))\n    clf_np.fit(X, y)\n\n    assert_array_equal(clf_int.apply(X), clf_rng.apply(X))\n    assert_array_equal(clf_int.apply(X), clf_np.apply(X))\n\n    assert_array_equal(clf_int.predict(X), clf_rng.predict(X))\n    assert_array_equal(clf_int.predict(X), clf_np.predict(X))\n\n    assert_array_almost_equal(clf_int.feature_importances_, clf_rng.feature_importances_)\n    assert_array_almost_equal(clf_int.feature_importances_, clf_np.feature_importances_)\n", "def check_max_features(name):\n    # Test that an error is raised when max_features is invalid\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(max_features='invalid')\n    assert_raises(ValueError, est.fit, X, y)\n\n    # Test that an error is raised when max_features is not in (0, n_features]\n    est = ForestEstimator(max_features=0)\n    assert_raises(ValueError, est.fit, X, y)\n\n    est = ForestEstimator(max_features=X.shape[1] + 1)\n    assert_raises(ValueError, est.fit, X, y)\n\n", "def check_n_features_deprecated(name):\n    # Check that the old parameter name `n_features` is deprecated.\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(n_estimators=5, max_depth=1, random_state=1)\n    assert_warns_message(FutureWarning, \"The parameter 'n_features' is deprecated\",\n                         est.fit, X, y)\n", "def test_parallel_backend_change():\n    # Test that the parallel backend can be changed after initialization.\n\n    forest = RandomForestClassifier(n_estimators=10, n_jobs=2)\n    forest.fit(X, y)\n\n    register_parallel_backend('dask', MyBackend)\n    with parallel_backend('dask'):\n        forest.set_params(n_estimators=20)\n        forest.fit(X, y)\n"], "sample_773": ["def test_logistic_regression_path_multiclass():\n    # Test that logistic_regression_path works for multiclass problems.\n    X, y = make_classification(n_samples=200, n_features=20, n_informative=10,\n                               n_classes=3, random_state=0)\n    Cs = [1e-2, 1, 1e2]\n\n    coefs, Cs, _ = _logistic_regression_path(X, y, Cs=Cs, penalty='l2',\n                                             solver='lbfgs', max_iter=1000,\n                                             multi_class='multinomial')\n\n    assert len(coefs) == Cs.shape[0]\n    assert coefs[0].shape == (3, 20)\n\n    # check that the same results are obtained with LogisticRegression\n    lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n    lr.fit(X, y)\n    assert_array_almost_equal(coefs[-1], lr.coef_)\n", "def test_logistic_regression_multiclass_auto_non_array_like():\n    # check multi_class='auto' raises a ValueError when y is not array-like\n\n    X = iris.data[::10]\n    y_multi = [iris.target[::10][i] for i in range(len(iris.target[::10]))]\n\n    est_auto_multi = LogisticRegression(multi_class='auto', solver='lbfgs')\n\n    with pytest.raises(ValueError):\n        est_auto_multi.fit(X, y_multi)\n", "def test_logistic_regression_n_iter_attribute(solver):\n    # Test that self.n_iter_ has the correct format.\n    X, y = iris.data, iris.target\n    clf = LogisticRegression(solver=solver)\n    clf.fit(X, y)\n    if solver == 'liblinear':\n        assert_equal(clf.n_iter_.shape, ())\n    else:\n        assert_equal(clf.n_iter_.shape, (1,))\n", "def test_logistic_regression_penalty_none(est):\n    # check penalty='none' gives the same result as C=1e15\n\n        return clone(est).set_params(**kw).fit(X, y)\n\n    X = iris.data[::10]\n    y_bin = iris.target[::10] == 0\n    est_none = fit(X, y_bin, penalty='none')\n    est_large_C = fit(X, y_bin, penalty='l2', C=1e15)\n    assert np.allclose(est_none.coef_, est_large_C.coef_)\n    assert np.allclose(est_none.predict_proba(X), est_large_C.predict_proba(X))\n", "def test_logistic_regression_max_iter_deprecation():\n    # Test that the future deprecation warning is raised when max_iter is not\n    # set explicitly in LogisticRegression.\n\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n\n    with pytest.warns(FutureWarning,\n                      match=\"max_iter was deprecated in version 0.22\"):\n        LogisticRegression(solver='liblinear').fit(X, y)\n"], "sample_774": ["def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b'], [0, 2]], dtype=object).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[['a', 'b', 'c'], [0, 1, 2]])\n    assert enc.get_params()['categories'] == [['a', 'b', 'c'], [0, 1, 2]]\n    assert_array_equal(enc.fit_transform(X), np.array([[0.], [1.]]))\n    # set params on already fitted object\n    enc.set_params(categories=[['a', 'b', 'c', 'd'], [0, 1, 2, 3]])\n    assert_array_equal(enc.fit_transform(X), np.array([[0.], [1.]]))\n", "def test_ordinal_encoder_get_feature_names():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3]]\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert isinstance(feature_names, np.ndarray)\n    assert_array_equal(['x0_Male', 'x0_Female',\n                        'x1_1', 'x1_3'], feature_names)\n\n    feature_names2 = enc.get_feature_names(['one', 'two'])\n    assert_array_equal(['one_Male', 'one_Female',\n                        'two_1', 'two_3'], feature_names2)\n\n    with pytest.raises(ValueError, match=\"input_features should have length\"):\n        enc.get_feature_names(['one'])\n", "def test_one_hot_encoder_get_feature_names_with_drop():\n    enc = OneHotEncoder(drop='first')\n    X = [['Male', 1, 'girl', 2, 3],\n         ['Female', 41, 'girl', 1, 10],\n         ['Male', 51, 'boy', 12, 3],\n         ['Male', 91, 'girl', 21, 30]]\n\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert isinstance(feature_names, np.ndarray)\n\n    exp = ['x0_Female',\n           'x1_41', 'x1_51', 'x1_91',\n           'x2_boy',\n           'x3_1', 'x3_12', 'x3_21',\n           'x4_10', 'x4_30']\n    assert_array_equal(exp, feature_names)\n\n    feature_names2 = enc.get_feature_names(['one', 'two', 'three', 'four', 'five'])\n    exp = ['one_Female',\n           'two_41', 'two_51', 'two_91',\n           'three_boy',\n           'four_1', 'four_12', 'four_21',\n           'five_10', 'five_30']\n    assert_array_equal(exp, feature_names2)\n\n    with pytest.raises(ValueError, match=\"input_features should have length\"):\n        enc.get_feature_names(['one', 'two'])\n", "def test_ordinal_encoder_dtype():\n    # check for correct output dtype\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OrdinalEncoder(dtype=np.float32)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == np.float32\n\n    enc = OrdinalEncoder(dtype=np.int64)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == np.int64\n\n    # check that setting the dtype after construction has an effect\n    enc = OrdinalEncoder()\n    enc.set_params(dtype=np.float32)\n    X_tr = enc.fit_transform(X)\n    assert X_tr.dtype == np.float32\n", "def test_one_hot_encoder_get_feature_names_with_drop():\n    enc = OneHotEncoder(drop='first')\n    X = [['Male', 1], ['Female', 3]]\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert isinstance(feature_names, np.ndarray)\n    assert_array_equal(['x0_Female', 'x1_3'], feature_names)\n\n    feature_names2 = enc.get_feature_names(['one', 'two'])\n    assert_array_equal(['one_Female', 'two_3'], feature_names2)\n\n    with pytest.raises(ValueError, match=\"input_features should have length\"):\n        enc.get_feature_names(['one'])\n"], "sample_775": ["def test_indent_at_name():\n    # Render an estimator with indent_at_name=False\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=False)\n    lr = LogisticRegression()\n    expected = \"\"\"LogisticRegression(C=1.0, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n          max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n          random_state=None, solver='warn', tol=0.0001, verbose=0,\n          warm_start=False)\"\"\"\n\n    expected = expected[1:]  # remove first \\n\n    assert pp.pformat(lr) == expected\n", "def test_indent_at_name():\n    # Render an estimator with indent_at_name=False\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=False)\n    lr = LogisticRegression()\n    expected = \"\"\"LogisticRegression(C=1.0, class_weight=None, dual=False,\n      fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100,\n      multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n      solver='warn', tol=0.0001, verbose=0, warm_start=False)\"\"\"\n    assert pp.pformat(lr) == expected\n\n    # Render a pipeline object with indent_at_name=False\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression(C=999))\n    expected = \"\"\"Pipeline(memory=None,\n  steps=[('standardscaler',\n          StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('logisticregression',\n          LogisticRegression(C=999, class_weight=None, dual=False,\n                             fit_intercept=True, intercept_scaling=1,\n                             l1_ratio=None, max_iter=100, multi_class='warn',\n                             n_jobs=None, penalty='l2', random_state=None,\n                             solver='warn', tol=0.0001, verbose=0,\n                             warm_start=False))])\"\"\"\n    assert pp.pformat(pipeline) == expected\n", "def test_n_max_elements_to_show_with_tuples():\n    # Test n_max_elements_to_show with tuples\n    n_max_elements_to_show = 30\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    # No ellipsis\n    categories = tuple(range(n_max_elements_to_show))\n    classifier = SVC(classes_=categories)\n\n    expected = r\"\"\"", "def test_empty_dict():\n    # Render an estimator with an empty dict as a parameter\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    class TestEstimator(BaseEstimator):\n            self.param = param\n\n    estimator = TestEstimator(param={})\n    expected = \"\"\"TestEstimator(param={})\"\"\"\n    assert pp.pformat(estimator) == expected\n", "def test_length_constraint_with_lists():\n    # When repr is still too long, use bruteforce ellipsis for lists\n    param_grid = {'C': [0] * 1000}\n    gs = GridSearchCV(SVC(), param_grid)\n    repr_ = gs.__repr__()\n    assert '...' in repr_\n"], "sample_776": ["def test_lasso_lars_with_sparse_X():\n    # Test that LassoLars works with sparse X\n    from scipy import sparse\n    X_sparse = sparse.csr_matrix(X)\n    lars_sparse = linear_model.LassoLars()\n    lars_dense = linear_model.LassoLars()\n    lars_sparse.fit(X_sparse, y)\n    lars_dense.fit(X, y)\n    assert_array_almost_equal(lars_sparse.coef_, lars_dense.coef_)\n", "def test_lasso_larsRaiseError():\n    # Test that LassoLars raises an error when the input is invalid\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6])\n    \n    # Test invalid precompute\n    lasso_lars = linear_model.LassoLars(precompute=' invalid')\n    with pytest.raises(ValueError):\n        lasso_lars.fit(X, y)\n\n    # Test invalid max_iter\n    lasso_lars = linear_model.LassoLars(max_iter=-1)\n    with pytest.raises(ValueError):\n        lasso_lars.fit(X, y)\n\n    # Test invalid eps\n    lasso_lars = linear_model.LassoLars(eps=-1)\n    with pytest.raises(ValueError):\n        lasso_lars.fit(X, y)\n\n    # Test invalid copy_X\n    lasso_lars = linear_model.LassoLars(copy_X=' invalid')\n    with pytest.raises(TypeError):\n        lasso_lars.fit(X, y)\n\n    # Test invalid fit_intercept\n    lasso_lars = linear_model.LassoLars(fit_intercept=' invalid')\n    with pytest.raises(TypeError):\n        lasso_lars.fit(X, y)\n\n    # Test invalid verbose\n    lasso_lars = linear_model.LassoLars(verbose=' invalid')\n    with pytest.raises(TypeError):\n        lasso_lars.fit(X, y)\n\n    # Test invalid normalize\n    lasso_lars = linear_model.LassoLars(normalize=' invalid')\n    with pytest.raises(TypeError):\n        lasso_lars.fit(X, y)\n\n    # Test invalid positive\n    lasso_lars = linear_model.LassoLars(positive=' invalid')\n    with pytest.raises(TypeError):\n        lasso_lars.fit(X, y)\n", "def test_lars_path_with_non_float_data():\n    # Test that lars_path function works with non-float data.\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6], dtype='int64')\n    alphas, active, coefs = linear_model.lars_path(X, y)\n    assert_array_almost_equal(coefs.T[-1], [0.5, 1.5])\n", "def test_lars_path_return_n_iter():\n    # Test that return_n_iter returns the correct number of iterations\n\n    # Generate some data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    # Fit lars_path with return_n_iter=True\n    _, _, _, n_iter = linear_model.lars_path(X, y, return_n_iter=True)\n\n    # Check that n_iter is an integer\n    assert isinstance(n_iter, int)\n\n    # Check that n_iter is not negative\n    assert n_iter >= 0\n", "def test_lars_path_early_stopping():\n    # Test that lars_path stops early when all coefficients are zero\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 0])\n    alphas, active, coefs = linear_model.lars_path(X, y, method='lar')\n    assert len(alphas) == 1\n    assert len(active) == 0\n    assert np.all(coefs == 0)\n"], "sample_777": ["def test_gradient_boosting_n_iter_no_change_equals_n_estimators():\n    # Check that setting n_iter_no_change equal to n_estimators stops training\n\n    gbc = GradientBoostingClassifier(n_estimators=10, n_iter_no_change=10,\n                                     learning_rate=0.1, max_depth=3,\n                                     random_state=42)\n\n    gbr = GradientBoostingRegressor(n_estimators=10, n_iter_no_change=10,\n                                    learning_rate=0.1, max_depth=3,\n                                    random_state=42)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    gbc.fit(X_train, y_train)\n    assert gbc.n_estimators_ == 10\n\n    gbr.fit(X_train, y_train)\n    assert gbr.n_estimators_ == 10\n", "def test_gradient_boosting_init_none():\n    # Check that GradientBoostingRegressor works when init is None.\n    X, y = make_regression()\n    gb = GradientBoostingRegressor(init=None)\n    gb.fit(X, y)\n    assert isinstance(gb.init_, DummyRegressor)\n    assert gb.init_ == gb.loss_.init_estimator()\n", "def test_gradient_boosting_n_iter_no_change_with_small_tol():\n    # Check that the small tol does not affect n_iter_no_change when\n    # n_iter_no_change is specified.\n\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=100,\n                                     n_iter_no_change=10,\n                                     validation_fraction=0.1,\n                                     learning_rate=0.1, max_depth=3,\n                                     tol=1e-7, random_state=42)\n    gbc.fit(X, y)\n\n    gbc_control = GradientBoostingClassifier(n_estimators=100,\n                                             n_iter_no_change=10,\n                                             validation_fraction=0.1,\n                                             learning_rate=0.1, max_depth=3,\n                                             tol=1e-4, random_state=42)\n    gbc_control.fit(X, y)\n\n    assert_equal(gbc.n_estimators_, gbc_control.n_estimators_)\n", "def test_gradient_boosting_init_not_estimator():\n    # Make sure error is raised if init is not an estimator\n\n    message = (\"The init parameter must be an estimator or 'zero'. \"\n               \"Got init='invalid'\")\n\n    with pytest.raises(ValueError, match=message):\n        GradientBoostingClassifier(init='invalid').fit(X, y)\n", "def test_gradient_boosting_n_iter_no_change_none():\n    # Check that the default value of n_iter_no_change doesn't break anything\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=100,\n                                     validation_fraction=0.1,\n                                     learning_rate=0.1, max_depth=3,\n                                     random_state=42)\n    gbc.fit(X, y)\n\n    gbr = GradientBoostingRegressor(n_estimators=100,\n                                    validation_fraction=0.1,\n                                    learning_rate=0.1, max_depth=3,\n                                    random_state=42)\n    gbr.fit(X, y)\n\n    assert gbc.n_estimators_ == 100\n    assert gbr.n_estimators_ == 100\n"], "sample_778": ["def test_nmf_fit_transform_custom_init_dtype():\n    # Test that fit_transform works with custom initialization and float32 dtype\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(6, 5)).astype(np.float32)\n    n_components = 4\n    avg = np.sqrt(A.mean() / n_components)\n    H_init = np.abs(avg * rng.randn(n_components, 5).astype(np.float32))\n    W_init = np.abs(avg * rng.randn(6, n_components).astype(np.float32))\n\n    m = NMF(solver='cd', n_components=n_components, init='custom',\n            random_state=0)\n    m.fit_transform(A, W=W_init, H=H_init)\n    assert m.components_.dtype == np.float32\n    assert m.transform(A).dtype == np.float32\n", "def test_nmf_multiplicative_update_max_iter():\n    # Test that the multiplicative update solver stops after max_iter\n    n_samples = 20\n    n_features = 10\n    n_components = 5\n\n    # initialization\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    np.clip(X, 0, None, out=X)\n    W0, H0 = nmf._initialize_nmf(X, n_components, init='random',\n                                 random_state=42)\n\n        W, H, n_iter = non_negative_factorization(\n            X, W0, H0, n_components, init='custom', update_H=True,\n            solver='mu', beta_loss=2, max_iter=max_iter, random_state=42)\n        assert n_iter == max_iter\n\n    for max_iter in (1, 2, 5, 10):\n        _assert_nmf_max_iter(X, max_iter)\n", "def test_nmf_fit_with_custom_init():\n    # Test that the fit method works with a custom initialization\n    n_samples = 6\n    n_features = 5\n    n_components = 3\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(n_samples, n_features))\n    W_init = np.abs(rng.randn(n_samples, n_components))\n    H_init = np.abs(rng.randn(n_components, n_features))\n\n    for solver in ('cd', 'mu'):\n        model = nmf.NMF(n_components=n_components, solver=solver,\n                        init='custom', random_state=42)\n        model.fit(X, W=W_init, H=H_init)\n\n        assert_array_almost_equal(model.transform(X), W_init, decimal=2)\n        assert_array_almost_equal(model.components_, H_init, decimal=2)\n", "def test_nmf_solver_parameter():\n    # Test that the solver parameter is correctly set in NMF\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(6, 5))\n\n    # 'cd' is the default solver\n    nmf = NMF(n_components=2, random_state=0)\n    assert nmf.solver == 'cd'\n\n    # other solvers can be specified\n    nmf = NMF(n_components=2, solver='mu', random_state=0)\n    assert nmf.solver == 'mu'\n", "def test_nmf_fit_transform_warns_convergence():\n    # Test that a warning is raised when convergence is not reached\n    random_state = np.random.RandomState(0)\n    X = np.abs(random_state.randn(6, 5))\n    model = NMF(n_components=2, init='random', random_state=0, tol=1e-12,\n                max_iter=1)\n    assert_warns_message(ConvergenceWarning, \"Maximum number of iteration\",\n                         model.fit_transform, X)\n"], "sample_779": ["def test_check_estimator_sparse_data():\n    # check that estimators raise an informative error when passed sparse data\n    # without being able to handle it\n\n    class NonSparseEstimator(BaseEstimator):\n            return self\n\n    est = NonSparseEstimator()\n    X = np.array([[1, 2], [3, 4]])\n    X_sparse = sp.csr_matrix(X)\n\n    msg = (\"Estimator NonSparseEstimator doesn't seem to fail gracefully on \"\n           \"sparse data. Please check that it handles sparse data correctly, \"\n           \"or explicitly state that it does not support sparse data by \"\n           \"using `check_array(X, accept_sparse=False)`.\")\n\n    with assert_raises_regex(TypeError, msg):\n        check_estimator_sparse_data(\"est\", est)\n", "def test_check_estimator_with_correct_not_fitted_error():\n    # check that estimators with correct NotFittedError are passed\n    check_estimator(CorrectNotFittedErrorClassifier())\n", "def test_check_class_weight_balanced_linear_classifier():\n    # check that the class weights are correctly computed in\n    # LinearClassifierMixin when class_weight='balanced'\n    from sklearn.datasets import make_classification\n    X, y = make_classification(n_samples=200, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=3,\n                               n_clusters_per_class=1, random_state=0)\n    check_class_weight_balanced_linear_classifier(\"Classifier\", SVC, X, y)\n", "def test_check_estimator_sparse_data():\n    # check that check_estimator() works on estimator with sparse data\n\n    # test sparse matrix input\n    est = SVC()\n    check_estimator(est)\n\n    # test sparse matrix output\n    est = SparseTransformer()\n    check_estimator(est)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # check that check_class_weight_balanced_linear_classifier raises a\n    # ValueError when the classifier is not linear\n\n    class NonLinearClassifier(BaseEstimator):\n            return self\n\n    assert_raises_regex(ValueError,\n                        \"check_class_weight_balanced_linear_classifier only \"\n                        \"accepts linear classifiers, got \"\n                        \"NonLinearClassifier instead.\",\n                        check_class_weight_balanced_linear_classifier,\n                        \"estimator\", NonLinearClassifier)\n\n    # check that check_class_weight_balanced_linear_classifier raises a\n    # ValueError when the classifier does not support class weights\n\n    class NoClassWeightClassifier(BaseEstimator):\n            return self\n\n    assert_raises_regex(ValueError,\n                        \"check_class_weight_balanced_linear_classifier only \"\n                        \"accepts classifiers with a 'class_weight' parameter, \"\n                        \"got NoClassWeightClassifier instead.\",\n                        check_class_weight_balanced_linear_classifier,\n                        \"estimator\", NoClassWeightClassifier)\n"], "sample_780": ["def test_lda_perplexity_before_fit():\n    # Test that perplexity raises an error when called before fit\n    rng = np.random.RandomState(0)\n    X = rng.randint(4, size=(20, 10))\n    lda = LatentDirichletAllocation(n_components=5)\n    assert_raises_regexp(NotFittedError,\n                         \"This LatentDirichletAllocation instance is not fitted yet.\",\n                         lda.perplexity, X)\n", "def test_lda_random_state():\n    # Test that LDA results are the same with different calls to fit\n    # when random_state is fixed\n    n_components, X = _build_sparse_mtx()\n    lda_1 = LatentDirichletAllocation(n_components=n_components,\n                                      max_iter=10, learning_method='batch',\n                                      random_state=0)\n    lda_2 = LatentDirichletAllocation(n_components=n_components,\n                                      max_iter=10, learning_method='batch',\n                                      random_state=0)\n\n    lda_1.fit(X)\n    lda_2.fit(X)\n\n    assert_array_almost_equal(lda_1.components_, lda_2.components_)\n    assert_array_almost_equal(lda_1.transform(X), lda_2.transform(X))\n", "def test_lda_perplexity_before_fit():\n    # Test LDA perplexity before fit\n    # Should raise NotFittedError\n    n_components = 5\n    X = np.random.randint(4, size=(20, 10))\n    lda = LatentDirichletAllocation(n_components=n_components)\n    assert_raises_regexp(NotFittedError,\n                         \"This LatentDirichletAllocation instance is not fitted yet.\",\n                         lda.perplexity, X)\n", "def test_lda_partial_fit_update_component():\n    # Test that the components are updated correctly in partial_fit\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_offset=10., total_samples=100,\n                                    random_state=rng)\n\n    # Get initial component values\n    initial_components = lda.components_\n\n    # Call partial_fit with a small batch size to update the components\n    lda.partial_fit(X[:5, :])\n\n    # Check that the components have changed\n    assert_not_equal(initial_components, lda.components_)\n", "def test_lda_get_params():\n    # Test get_params method of LatentDirichletAllocation\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components)\n    params = lda.get_params()\n    assert 'n_components' in params\n    assert 'doc_topic_prior' in params\n    assert 'topic_word_prior' in params\n    assert 'learning_method' in params\n    assert 'learning_decay' in params\n    assert 'learning_offset' in params\n    assert 'max_iter' in params\n    assert 'batch_size' in params\n    assert 'evaluate_every' in params\n    assert 'total_samples' in params\n    assert 'perp_tol' in params\n    assert 'mean_change_tol' in params\n    assert 'max_doc_update_iter' in params\n    assert 'n_jobs' in params\n    assert 'verbose' in params\n    assert 'random_state' in params\n"], "sample_781": ["def test_forest_estimators_sample_weights():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    sample_weight = np.random.rand(len(X))\n    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                 n_estimators=200).fit(X, y, sample_weight=sample_weight)\n\n    # Check that all estimators have the same sample weights\n    for est in clf.estimators_:\n        assert_array_equal(est.sample_weight, sample_weight)\n", "def test_forest_min_impurity_decrease():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    clf = RandomForestClassifier(min_impurity_decrease=0.1, random_state=42,\n                                 n_estimators=200).fit(X, y)\n    for tree in clf.estimators_:\n        assert tree.min_impurity_decrease == 0.1\n\n    # Check that feature importances still sum up to 1\n    assert math.isclose(1, clf.feature_importances_.sum(), abs_tol=1e-7)\n", "def test_forest_estimators_sample_weights():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    sample_weight = np.random.rand(len(X))\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    for tree in clf.estimators_:\n        assert_array_almost_equal(tree.sample_weight, sample_weight)\n", "def test_forest_feature_importances_zero():\n    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,\n                               n_classes=3)\n    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,\n                                 n_estimators=200).fit(X, y)\n    assert np.all(clf.feature_importances_ >= 0)\n", "def check_estimators_pickleable(name):\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(n_estimators=5)\n    X, y = iris.data, iris.target\n\n    # fit then pickle\n    est.fit(X, y)\n    assert est.score(X, y) > 0.9\n    dump = pickle.dumps(est)\n    est2 = pickle.loads(dump)\n    assert est2.score(X, y) > 0.9\n\n    # pickle before fitting\n    dump = pickle.dumps(est)\n    est2 = pickle.loads(dump)\n    est2.fit(X, y)\n    assert est2.score(X, y) > 0.9\n\n"], "sample_782": ["def test_column_transformer_remainder_none():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # remainder='drop' is default\n    ct = ColumnTransformer([('trans1', Trans(), [0])])\n    assert ct.remainder == 'drop'\n\n    # remainder=None should raise an error\n    ct = ColumnTransformer([('trans1', Trans(), [0])], remainder=None)\n    assert_raise_message(\n        ValueError,\n        \"remainder keyword needs to be one of 'drop', 'passthrough', \"\n        \"or estimator.\",\n        ct.fit, X_array)\n", "def test_column_transformer_sparse_output():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # all transformers output sparse\n    ct = ColumnTransformer([('trans1', SparseMatrixTrans(), [0]),\n                            ('trans2', SparseMatrixTrans(), [1])],\n                           sparse_threshold=0.5)\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n    # one transformer outputs sparse and threshold is 0\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', SparseMatrixTrans(), [1])],\n                           sparse_threshold=0)\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), ['trans__x0', 'remainder__x1'])\n", "def test_column_transformer_get_feature_names_remainder_passthrough():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])],\n                           remainder='passthrough')\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), ['trans__x0', 'x1'])\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert_equal(ct.get_feature_names(), ['trans__x0', 'remainder__x0',\n                                          'remainder__x1'])\n"], "sample_783": ["def test_missing_indicator_features_param():\n    # Test the features parameter of MissingIndicator\n    X = np.array([[np.nan, np.nan, 1],\n                  [4, np.nan, 2]])\n\n    indicator_all = MissingIndicator(missing_values=np.nan, features='all')\n    indicator_missing_only = MissingIndicator(missing_values=np.nan,\n                                              features='missing-only')\n\n    X_trans_all = indicator_all.fit_transform(X)\n    X_trans_missing_only = indicator_missing_only.fit_transform(X)\n\n    assert_array_equal(indicator_all.features_, np.array([0, 1, 2]))\n    assert_array_equal(indicator_missing_only.features_, np.array([0, 1]))\n\n    assert X_trans_all.shape[1] == 3\n    assert X_trans_missing_only.shape[1] == 2\n\n    assert_array_equal(X_trans_all, np.array([[True, True, False],\n                                              [False, True, False]]))\n    assert_array_equal(X_trans_missing_only, np.array([[True, True],\n                                                       [False, True]]))\n", "def test_missing_indicator_with_imputer_sparse():\n    # Test that MissingIndicator works with SimpleImputer and sparse data\n    X = sparse.csr_matrix(np.array([[np.nan, 1.], [1., np.nan]]))\n    trans = make_union(\n        SimpleImputer(missing_values=np.nan, strategy='most_frequent'),\n        MissingIndicator(missing_values=np.nan)\n    )\n    X_trans = trans.fit_transform(X)\n    assert_array_equal(X_trans.toarray(), np.array([[1., 1., True, False],\n                                                    [1., 1., False, True]]))\n", "def test_imputerIndicator_feature_names_out():\n    X = np.array([['a', 'b'], ['b', 'a']], dtype=object)\n    imputer_indicator = MissingIndicator(missing_values='a').fit(X)\n    feature_names_out = imputer_indicator.get_feature_names_out()\n    assert_array_equal(feature_names_out, ['missingindicator_0', 'missingindicator_1'])\n", "def test_missing_indicator_with_empty_features():\n    X = np.array([[1, 2], [3, 4]])\n    indicator = MissingIndicator(missing_values=np.nan, features='missing-only')\n    X_trans = indicator.fit_transform(X)\n    assert_array_equal(X_trans, np.empty((2, 0), dtype=bool))\n", "def test_missing_indicator_get_mask():\n    # Test that MissingIndicator._get_mask works with different dtypes and\n    # missing_values.\n    X = np.array([[np.nan, 1], [2, np.nan]])\n\n    # NaN is the default for floating point dtypes\n    mask = MissingIndicator._get_mask(X, np.nan)\n    assert_array_equal(mask, np.array([[True, False], [False, True]]))\n\n    # integer dtype\n    X_int = X.astype(int)\n    X_int[X_int == -9223372036854775808] = 0  # pandas uses this to represent NaN\n    mask = MissingIndicator._get_mask(X_int, 0)\n    assert_array_equal(mask, np.array([[True, False], [False, True]]))\n\n    # object dtype\n    X_obj = X.astype(object)\n    mask = MissingIndicator._get_mask(X_obj, np.nan)\n    assert_array_equal(mask, np.array([[True, False], [False, True]]))\n\n    # string dtype\n    X_str = np.array([['a', 'b'], ['c', '']])\n    mask = MissingIndicator._get_mask(X_str, '')\n    assert_array_equal(mask, np.array([[False, False], [False, True]]))\n"], "sample_784": ["def test_calibration_curve_edge_cases():\n    \"\"\"Check edge cases for calibration_curve function\"\"\"\n    y_true = np.array([0, 1])\n    y_pred = np.array([0., 1.])\n\n    # Check that calibration_curve works with only one sample per class\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 2)\n    assert_almost_equal(prob_true, [0, 1])\n    assert_almost_equal(prob_pred, [0, 1])\n\n    # Check that calibration_curve works with only one bin\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=1)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n    assert_almost_equal(prob_true, [0.5])\n    assert_almost_equal(prob_pred, [0.5])\n\n    # Check that calibration_curve raises an error when y_true and y_pred\n    # have different lengths\n    y_true = np.array([0, 1])\n    y_pred = np.array([0.])\n    assert_raises(ValueError, calibration_curve, y_true, y_pred)\n", "def test_calibration_curve_multiclass():\n    \"\"\"Check calibration_curve function for multiclass case\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n    y_pred = np.array([\n        [0.8, 0.1, 0.1],\n        [0.7, 0.2, 0.1],\n        [0.6, 0.3, 0.1],\n        [0.1, 0.8, 0.1],\n        [0.1, 0.7, 0.2],\n        [0.1, 0.6, 0.3],\n        [0.1, 0.1, 0.8],\n        [0.2, 0.1, 0.7],\n        [0.3, 0.1, 0.6]\n    ])\n\n    prob_true, prob_pred = calibration_curve(y_true, y_pred[:, 1], n_bins=3)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 3)\n\n    # test that quantiles work as expected\n    prob_true_quantile, prob_pred_quantile = calibration_curve(\n        y_true, y_pred[:, 1], n_bins=3, strategy='quantile')\n\n    assert len(prob_true_quantile) == len(prob_pred_quantile)\n    assert len(prob_true_quantile) == 3\n\n    # Check that error is raised when invalid strategy is selected\n    assert_raises(ValueError, calibration_curve, y_true, y_pred[:, 1],\n                  strategy='percentile')\n", "def test_calibration_curve_input_validation():\n    \"\"\"Check calibration_curve function input validation\"\"\"\n    y_true = np.array([0, 1])\n    y_pred = np.array([0., 1.])\n\n    # Check that y_true and y_pred have the same length\n    assert_raises(ValueError, calibration_curve, y_true, np.array([0.]))\n\n    # Check that y_true is a binary array\n    assert_raises(ValueError, calibration_curve, np.array([0, 2]), y_pred)\n\n    # Check that y_pred is an array of floats\n    assert_raises(ValueError, calibration_curve, y_true, np.array([0, 1]))\n\n    # Check that n_bins is at least 2\n    assert_raises(ValueError, calibration_curve, y_true, y_pred, n_bins=1)\n", "def test_calibration_curve_edge_cases():\n    \"\"\"Check calibration_curve function for edge cases\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n\n    # Check that calibration_curve works with only one bin\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=1)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n\n    # Check that calibration_curve works with all samples in one bin\n    prob_true, prob_pred = calibration_curve(y_true, y_pred * 0, n_bins=5)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n\n    # Check that calibration_curve raises an error when there are no bins\n    assert_raises(ValueError, calibration_curve, y_true, y_pred, n_bins=0)\n", "def test_calibration_curve_edge_cases():\n    \"\"\"Check calibration_curve function for edge cases\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0., 0., 1., 1., 1.])\n\n    # Check that calibration_curve returns the correct number of bins\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 2)  # instead of 3 because only two unique values\n\n    # Check that calibration_curve raises an error when y_true and y_pred have different lengths\n    y_true_2 = np.array([0, 0, 0, 1, 1])\n    assert_raises(ValueError, calibration_curve, y_true_2, y_pred)\n\n    # Check that calibration_curve works with one bin\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=1)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 1)\n"], "sample_785": ["def test_splitter_reprs():\n    # Test that repr works for all splitters\n    splitters = [\n        KFold(n_splits=5, shuffle=True, random_state=42),\n        StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n        GroupKFold(n_splits=5),\n        TimeSeriesSplit(n_splits=5, max_train_size=3),\n        LeaveOneOut(),\n        LeavePOut(p=2),\n        LeaveOneGroupOut(),\n        LeavePGroupsOut(n_groups=2),\n        ShuffleSplit(n_splits=5, test_size=0.2, random_state=42),\n        StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42),\n        GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42),\n        PredefinedSplit(test_fold=[1, 2, 3]),\n        RepeatedKFold(n_splits=5, n_repeats=2, random_state=42),\n        RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42)\n    ]\n\n    for splitter in splitters:\n        repr(splitter)\n", "def test_validate_shuffle_split_init():\n    # Test _validate_shuffle_split_init function with different inputs\n    assert_raises(ValueError, _validate_shuffle_split_init, None, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.5, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, 8, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.1, 8)\n\n    # Should not raise any error for valid input\n    _validate_shuffle_split_init(test_size=2, train_size=None)\n    _validate_shuffle_split_init(test_size=0.2, train_size=None)\n    _validate_shuffle_split_init(test_size=None, train_size=5)\n    _validate_shuffle_split_init(test_size=None, train_size=0.5)\n", "def test_validate_shuffle_split_init():\n    # Test _validate_shuffle_split_init with different inputs\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=None,\n                  train_size=None)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=1.2,\n                  train_size=None)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=0.6,\n                  train_size=0.6)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=11,\n                  train_size=5)\n    assert_raises(TypeError, _validate_shuffle_split_init, test_size='a',\n                  train_size=None)\n\n    # Check that valid arguments pass without error\n    _validate_shuffle_split_init(test_size=0.5, train_size=None)\n    _validate_shuffle_split_init(test_size=5, train_size=10)\n    _validate_shuffle_split_init(test_size=None, train_size=0.8)\n", "def test_validate_shuffle_split_init():\n    # Test if _validate_shuffle_split_init raises errors for invalid inputs\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=None,\n                  train_size=None)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=1.2,\n                  train_size=0.8)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=0.8,\n                  train_size=1.2)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=0.8,\n                  train_size=-0.2)\n    assert_raises(ValueError, _validate_shuffle_split_init, test_size=-0.2,\n                  train_size=0.8)\n    assert_raises(TypeError, _validate_shuffle_split_init, test_size='a',\n                  train_size=0.8)\n    assert_raises(TypeError, _validate_shuffle_split_init, test_size=0.8,\n                  train_size='a')\n", "def test_check_cv_is_none():\n    # Test that check_cv raises a ValueError when cv is None and y is None\n    assert_raise_message(ValueError, \"y: array-like is not provided\",\n                         check_cv, cv=None, y=None)\n"], "sample_786": ["def test_fit_transform_constant_feature():\n    X = np.array([[1, 2, 3], [1, 4, 5], [1, 6, 7]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n    assert len(est.bin_edges_[0]) == 2\n", "def test_fit_transform_constant_feature(strategy):\n    X = np.array([[1, 2, 3], [1, 4, 5], [1, 6, 7]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n    assert est.bin_edges_[0].shape == (2,)\n", "def test_bin_edges_():\n    X = np.array([0, 1, 2, 3, 4])[:, None]\n    kbd = KBinsDiscretizer(n_bins=2, strategy='uniform', encode='ordinal')\n    kbd.fit(X)\n    assert len(kbd.bin_edges_) == 1\n    assert len(kbd.bin_edges_[0]) == kbd.n_bins_[0] + 1\n    assert kbd.bin_edges_[0][0] == X.min()\n    assert kbd.bin_edges_[0][-1] == X.max()\n\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    kbd = KBinsDiscretizer(n_bins=[2, 3], strategy='uniform', encode='ordinal')\n    kbd.fit(X)\n    assert len(kbd.bin_edges_) == 2\n    assert len(kbd.bin_edges_[0]) == kbd.n_bins_[0] + 1\n    assert len(kbd.bin_edges_[1]) == kbd.n_bins_[1] + 1\n    assert kbd.bin_edges_[0][0] == X[:, 0].min()\n    assert kbd.bin_edges_[0][-1] == X[:, 0].max()\n    assert kbd.bin_edges_[1][0] == X[:, 1].min()\n    assert kbd.bin_edges_[1][-1] == X[:, 1].max()\n", "def test_fit_transform_constant_feature():\n    X = np.array([[1, 2, 3], [1, 4, 5], [1, 6, 7]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n    assert len(est.bin_edges_[0]) == 2\n", "def test_bin_edges(strategy):\n    X = np.array([0, 1, 2, 3])[:, None]\n    kbd = KBinsDiscretizer(n_bins=4, strategy=strategy, encode='ordinal')\n    kbd.fit(X)\n\n    bin_edges = kbd.bin_edges_[0]\n    assert bin_edges.shape == (5,)  # 4 bins + 1 extra edge for clipping\n    assert np.all(bin_edges[:-1] <= bin_edges[1:])  # edges are sorted\n    assert np.all(kbd.transform(X) >= 0)  # transformed values are non-negative\n    assert np.all(kbd.transform(X) < kbd.n_bins_[0])  # transformed values are below n_bins\n"], "sample_787": ["def test_fbeta_score_equal():\n    y_true = [0, 1, 1, 0, 1, 1]\n    y_pred = [0.1, 0.8, 0.9, 0.3, 1., 0.95]\n\n    f1 = fbeta_score(y_true, np.round(y_pred), beta=1)\n    f05 = fbeta_score(y_true, np.round(y_pred), beta=0.5)\n    f2 = fbeta_score(y_true, np.round(y_pred), beta=2)\n\n    assert_not_equal(f1, f05)\n    assert_not_equal(f1, f2)\n    assert_not_equal(f05, f2)\n", "def test_balanced_accuracy_score_adjusted():\n    y_true = [1, 0, 1, 0]\n    y_pred = [1, 1, 0, 0]\n\n    # Calculate adjusted balanced accuracy score\n    adjusted_ba = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n\n    assert_almost_equal(adjusted_ba, 0.0)\n", "def test_balanced_accuracy_score_equal_to_accuracy():\n    # In the case where all classes have the same number of samples,\n    # balanced accuracy should be equal to accuracy.\n    y_true = [0, 0, 0, 1, 1, 1]\n    y_pred = [0, 1, 1, 0, 1, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred),\n                        accuracy_score(y_true, y_pred))\n", "def test_balanced_accuracy_score():\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 0, 1]\n\n    # the following tests are from metrics.classification._check_targets\n    assert_equal(balanced_accuracy_score(y_true, y_pred),\n                 balanced_accuracy_score([y_true], [y_pred]))\n    assert_equal(balanced_accuracy_score(y_true, y_pred),\n                 balanced_accuracy_score(np.array(y_true), np.array(y_pred)))\n    assert_equal(balanced_accuracy_score(y_true, y_pred),\n                 balanced_accuracy_score(pd.Series(y_true), pd.Series(y_pred)))\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.625)\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred, adjusted=True),\n                        0.25)\n", "def test_balanced_accuracy_score_adjusted():\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 0, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.625)\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred, adjusted=True),\n                        (0.625 - 0.5) / (1 - 0.5))\n"], "sample_788": ["def test_fit_transform_constant_feature():\n    X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    est.fit(X)\n    assert_array_equal(est.n_bins_, [1, 3])\n    assert_array_equal(est.bin_edges_[0], [-np.inf, np.inf])\n    assert_array_equal(est.transform(X)[:, 0], np.zeros(X.shape[0]))\n", "def test_encode_onehot_inverse_transform(encode):\n    kbd = KBinsDiscretizer(n_bins=3, encode=encode)\n    Xt = kbd.fit_transform(X)\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_almost_equal(kbd.inverse_transform(kbd._encoder.inverse_transform(Xt)), Xinv)\n", "def test_fit_transform_constant_feature():\n    X = np.array([[1, 2, 3], [1, 4, 5], [1, 6, 7]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert est.n_bins_[0] == 1\n    assert len(est.bin_edges_[0]) == 2\n", "def test_bin_edges():\n    X = np.array([[0, 1], [2, 3], [4, 5], [6, 7]])\n    kbd = KBinsDiscretizer(n_bins=2)\n    kbd.fit(X)\n    assert_array_equal(kbd.bin_edges_[0], [0, 3, 6])\n    assert_array_equal(kbd.bin_edges_[1], [1, 4, 7])\n\n    X = np.array([[0, 1], [2, 3], [4, 5]])\n    kbd = KBinsDiscretizer(n_bins=3)\n    kbd.fit(X)\n    assert_array_equal(kbd.bin_edges_[0], [0, 2/3, 4/3, 2, 8/3, 10/3, 4])\n    assert_array_equal(kbd.bin_edges_[1], [1, 5/3, 7/3, 3, 11/3, 13/3, 5])\n\n    X = np.array([[0], [0], [0], [1], [1], [1]])\n    kbd = KBinsDiscretizer(n_bins=2)\n    msg = (\"Bins whose width are too small (i.e., <= 1e-8) in feature 0 \"\n           \"are removed. Consider decreasing the number of bins.\")\n    assert_warns_message(UserWarning, msg, kbd.fit, X)\n    assert_array_equal(kbd.bin_edges_[0], [-np.inf, np.inf])\n", "def test_bin_edges():\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    est.fit(X)\n    assert len(est.bin_edges_) == X.shape[1]\n    for i in range(X.shape[1]):\n        assert len(est.bin_edges_[i]) == est.n_bins_[i] + 1\n"], "sample_789": ["def test_base_estimator_has_fit_parameter():\n    # Test that the base estimator has a fit parameter.\n\n    class DummyEstimator:\n            pass\n\n    class DummyEstimatorWithSampleWeight:\n            pass\n\n    # Should not raise an error.\n    AdaBoostClassifier(base_estimator=DummyEstimatorWithSampleWeight())\n\n    # Should raise an error.\n    msg = \"AdaBoostClassifier.base_estimator must be a \"\n    with pytest.raises(ValueError, match=msg):\n        AdaBoostClassifier(base_estimator=DummyEstimator())\n\n    # Should not raise an error.\n    AdaBoostRegressor(base_estimator=DummyEstimator())\n\n    # Should not raise an error.\n    AdaBoostRegressor(base_estimator=DummyEstimatorWithSampleWeight())\n", "def test_adaboost_regressor_with_loss():\n    # Check regression with different loss functions.\n    X, y = datasets.make_regression(n_samples=50, n_features=5, random_state=0)\n\n    for loss in ['linear', 'square', 'exponential']:\n        reg = AdaBoostRegressor(loss=loss, random_state=0)\n        reg.fit(X, y)\n        assert_greater(reg.score(X, y), 0.5)\n\n    # Check that an invalid loss function raises an error\n    reg = AdaBoostRegressor(loss='invalid')\n    assert_raises(ValueError, reg.fit, X, y)\n", "def test_early_stopping():\n    # Check that early stopping works as expected.\n    X, y = datasets.make_classification(n_samples=1000, n_features=10,\n                                        n_informative=3, n_redundant=0,\n                                        n_repeated=0, shuffle=False,\n                                        random_state=1)\n\n    clf = AdaBoostClassifier(n_estimators=50, random_state=0)\n    clf.fit(X, y)\n\n    # With early stopping, we should have less estimators\n    clf_early = AdaBoostClassifier(n_estimators=50, random_state=0)\n    clf_early.estimator_errors_ = np.zeros(5)\n    clf_early.fit(X, y)\n    assert len(clf_early.estimators_) < len(clf.estimators_)\n", "def test_adaboost_regressor_with_constant_target():\n    # Check that AdaBoostRegressor works with constant target.\n    X, _ = datasets.make_regression(n_samples=10, n_features=5)\n    y = np.ones(10)\n\n    reg = AdaBoostRegressor()\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(X), np.ones(10))\n", "def test_adaboost_regressor_with_constant_target():\n    # Check that AdaBoostRegressor works with constant target.\n    X, _ = datasets.make_regression(n_samples=10, n_features=5)\n    y = np.ones(10)\n\n    clf = AdaBoostRegressor()\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), y)\n"], "sample_790": ["def test_kernel_pca_duplicate_eigenvalues():\n    # Test if KernelPCA can handle duplicate eigenvalues.\n    # This test uses a matrix with two identical columns,\n    # which will result in duplicate eigenvalues.\n    X = np.array([[1, 1], [2, 2], [3, 3]])\n\n    kpca = KernelPCA(n_components=2)\n    Xt = kpca.fit_transform(X)\n\n    assert_equal(Xt.shape, (3, 2))\n", "def test_kernel_pca_n_jobs():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    for n_jobs in [1, -1]:\n        kpca = KernelPCA(n_components=2, n_jobs=n_jobs)\n        Xt = kpca.fit(X_fit).transform(X_pred)\n        assert_equal(Xt.shape, (2, 2))\n\n    # Check if an invalid value raises an error\n    kpca = KernelPCA(n_components=2, n_jobs=0)\n    with pytest.raises(ValueError):\n        kpca.fit(X_fit)\n", "def test_kernel_pca_n_jobs():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    for n_jobs in [1, -1]:\n        kpca = KernelPCA(n_components=2, n_jobs=n_jobs)\n        Xt = kpca.fit_transform(X_fit)\n        assert_equal(Xt.shape, (5, 2))\n\n        Xt_pred = kpca.transform(X_pred)\n        assert_equal(Xt_pred.shape, (2, 2))\n", "def test_kernel_pca_n_jobs():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    for n_jobs in [1, -1]:\n        kpca = KernelPCA(n_components=2, n_jobs=n_jobs)\n        Xt = kpca.fit(X_fit).transform(X_pred)\n        assert_equal(Xt.shape, (2, 2))\n", "def test_kernel_pca_n_jobs():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    for n_jobs in [1, -1]:\n        kpca = KernelPCA(n_components=2, n_jobs=n_jobs)\n        Xt = kpca.fit_transform(X_fit)\n        assert_equal(Xt.shape, (5, 2))\n\n        Xt_pred = kpca.transform(X_pred)\n        assert_equal(Xt_pred.shape, (2, 2))\n"], "sample_791": ["def test_one_hot_encoder_categories_as_numpy_arrays():\n    X = np.array([['a', 'b'], ['c', 'd']])\n    categories = [np.array(['a', 'c']), np.array(['b', 'd'])]\n    enc = OneHotEncoder(categories=categories)\n    exp = np.array([[1., 0., 1., 0.], [0., 1., 0., 1.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])\n    assert_array_equal(enc.inverse_transform(exp), X)\n", "def test_one_hot_encoder_get_feature_names_out_dtype():\n    X = [['Male', 1], ['Female', 3]]\n    enc = OneHotEncoder()\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert feature_names.dtype == object\n", "def test_one_hot_encoder_get_feature_names_raises():\n    enc = OneHotEncoder()\n    X = [['Male', 1], ['Female', 3]]\n    enc.fit(X)\n\n    # invalid length of input_features\n    with pytest.raises(ValueError, match=\"input_features should have length\"):\n        enc.get_feature_names(input_features=['one'])\n\n    # input_features not an array-like\n    with pytest.raises(TypeError, match=\"input_features must be a list of strings\"):\n        enc.get_feature_names(input_features='one')\n", "def test_one_hot_encoder_get_feature_names_with_drop():\n    enc = OneHotEncoder(drop='first')\n    X = [['Male', 1], ['Female', 3]]\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0_Female', 'x1_3'], feature_names)\n\n    enc = OneHotEncoder(drop=['Male', 1])\n    X = [['Male', 1], ['Female', 3]]\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0_Female', 'x1_3'], feature_names)\n", "def test_one_hot_encoder_get_feature_names_with_drop():\n    X = [['Male', 1], ['Female', 3]]\n    enc = OneHotEncoder(drop='first')\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(['x0_Female', 'x1_3'], feature_names)\n\n    enc = OneHotEncoder(drop=[0, 1])\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal([], feature_names)\n"], "sample_792": ["def test_complementnb_pickle():\n    # Test picklability of ComplementNB classifier\n\n    clf = ComplementNB().fit(X2, y2)\n    y_pred = clf.predict(X2)\n\n    store = BytesIO()\n    pickle.dump(clf, store)\n    clf = pickle.load(BytesIO(store.getvalue()))\n\n    assert_array_equal(y_pred, clf.predict(X2))\n", "def test_gnb_partial_fit_weighted_calls():\n    # Test that weighted calls to partial_fit are equivalent to a single call.\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 1, 2])\n    weights = np.array([0.5, 0.5, 1.0])\n\n    clf_single_call = GaussianNB()\n    clf_single_call.partial_fit(X, y, classes=[1, 2], sample_weight=weights)\n\n    clf_multiple_calls = GaussianNB()\n    clf_multiple_calls.partial_fit(X[:1], y[:1], classes=[1, 2],\n                                   sample_weight=weights[:1])\n    clf_multiple_calls.partial_fit(X[1:], y[1:], sample_weight=weights[1:])\n\n    assert_array_almost_equal(clf_single_call.theta_, clf_multiple_calls.theta_)\n    assert_array_almost_equal(clf_single_call.sigma_, clf_multiple_calls.sigma_)\n    assert_array_almost_equal(clf_single_call.class_prior_,\n                              clf_multiple_calls.class_prior_)\n", "def test_cnb_partial_fit():\n    # Test partial fit method for ComplementNB\n    X = np.array([[1, 0], [1, 1]])\n    y = np.array([0, 1])\n\n    clf = ComplementNB()\n    clf.partial_fit(X, y, classes=[0, 1])\n\n    # Check that the partial fit with the same data gives the same result as fit\n    clf2 = ComplementNB()\n    clf2.fit(X, y)\n\n    assert_array_almost_equal(clf.feature_log_prob_, clf2.feature_log_prob_)\n    assert_array_almost_equal(clf.class_count_, clf2.class_count_)\n    assert_array_almost_equal(clf.feature_all_, clf2.feature_all_)\n\n    # Check that partial fit updates correctly\n    X_new = np.array([[0, 1]])\n    y_new = np.array([0])\n    clf.partial_fit(X_new, y_new)\n\n    assert_array_almost_equal(clf.class_count_, np.array([3, 1]))\n    assert_array_almost_equal(clf.feature_all_, np.array([2, 2]))\n", "def test_cnb_partial_fit():\n    # Test ComplementNB when using partial_fit.\n    X = np.array([[1, 0], [1, 1]])\n    y = np.array([0, 1])\n\n    clf = ComplementNB()\n    clf.partial_fit(X, y, classes=[0, 1])\n    assert_array_equal(clf.class_count_, np.array([1, 1]))\n    assert_array_equal(clf.feature_all_, np.array([2, 1]))\n\n    clf.partial_fit(X, y)\n    assert_array_equal(clf.class_count_, np.array([2, 2]))\n    assert_array_equal(clf.feature_all_, np.array([4, 2]))\n", "def test_complementnb_partial_fit():\n    # Test ComplementNB's partial fit method\n    X = np.array([[1, 0], [1, 1]])\n    y = np.array([0, 1])\n\n    clf = ComplementNB()\n    clf.partial_fit(X, y, classes=[0, 1])\n    assert_array_equal(clf.feature_count_, np.array([[1, 0], [0, 1]]))\n    assert_array_equal(clf.class_count_, np.array([1, 1]))\n    assert_array_equal(clf.feature_all_, np.array([1, 1]))\n\n    clf.partial_fit(X, y)\n    assert_array_equal(clf.feature_count_, np.array([[2, 0], [0, 2]]))\n    assert_array_equal(clf.class_count_, np.array([2, 2]))\n    assert_array_equal(clf.feature_all_, np.array([2, 2]))\n"], "sample_793": ["def test_iforest_sparse_chunked():\n    \"\"\"Check IForest for various parameter settings on sparse input with chunking.\"\"\"\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                                                        boston.target[:50],\n                                                        random_state=rng)\n\n    # make a sparse matrix with 2 chunks of data\n    X_train_sparse = csr_matrix(np.vstack((X_train[:25], X_train[25:])))\n    X_test_sparse = csr_matrix(X_test)\n\n    grid = ParameterGrid({\"max_samples\": [0.5, 1.0],\n                          \"bootstrap\": [True, False]})\n\n    for params in grid:\n        # Trained on sparse format\n        sparse_classifier = IsolationForest(\n            n_estimators=10, random_state=1, **params).fit(X_train_sparse)\n        sparse_results = sparse_classifier.predict(X_test_sparse)\n\n        # Trained on dense format\n        dense_classifier = IsolationForest(\n            n_estimators=10, random_state=1, **params).fit(X_train)\n        dense_results = dense_classifier.predict(X_test)\n\n        assert_array_equal(sparse_results, dense_results)\n", "def test_iforest_offset_():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest(contamination=0.1).fit(X_train)\n    assert isinstance(clf.offset_, float)\n    assert clf.offset_ < 0\n    clf = IsolationForest(contamination='auto').fit(X_train)\n    assert clf.offset_ == -0.5\n    clf = IsolationForest(behaviour='old').fit(X_train)\n    assert clf.offset_ == -0.5\n", "def test_iforest_fit_with_sample_weight():\n    X = iris.data\n    sample_weight = np.random.rand(X.shape[0])\n    clf = IsolationForest(random_state=42)\n    clf.fit(X, sample_weight=sample_weight)\n    assert_array_equal(clf.score_samples(X).shape, (X.shape[0],))\n", "def test_iforest_n_features():\n    # Test that IsolationForest handles n_features=1 correctly\n    X = np.array([[1], [2], [3]])\n    clf = IsolationForest()\n    clf.fit(X)\n    assert_array_equal(clf.predict([[2]]), [1])\n    assert_array_equal(clf.predict([[5]]), [-1])\n", "def test_iforest_n_features():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest().fit(X_train)\n    assert clf.n_features_ == 2\n\n    with pytest.raises(ValueError):\n        clf.predict([[1]])\n"], "sample_794": ["def test_ridge_multitarget():\n    # Test that Ridge can handle multi-target regression\n    X, y = make_regression(n_samples=100, n_features=10, n_targets=3,\n                           random_state=42)\n    ridge = Ridge()\n    ridge.fit(X, y)\n    assert ridge.coef_.shape == (3, 10)\n    assert ridge.intercept_.shape == (3,)\n    y_pred = ridge.predict(X)\n    assert y_pred.shape == (100, 3)\n", "def test_ridge_classifier_multiclass():\n    # Test multiclass classification with RidgeClassifier\n\n    X, y = datasets.load_iris(return_X_y=True)\n    n_classes = np.unique(y).shape[0]\n    n_features = X.shape[1]\n\n    ridge = RidgeClassifier()\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (n_classes, n_features))\n    y_pred = ridge.predict(X)\n    assert_greater(np.mean(y == y_pred), .79)\n\n    # Check if the predict_proba method works\n    y_proba = ridge.predict_proba(X)\n    assert_array_almost_equal(y_proba.sum(axis=1), np.ones(len(X)))\n\n    # Check if the decision_function method works\n    dec_func = ridge.decision_function(X)\n    assert_array_almost_equal(dec_func.shape, (len(X), n_classes))\n", "def test_ridge_classifier_multiclass():\n    X, y = datasets.load_iris(return_X_y=True)\n    clf = RidgeClassifier()\n    clf.fit(X, y)\n    assert_array_equal(clf.classes_, np.unique(y))\n    y_pred = clf.predict(X)\n    assert accuracy_score(y, y_pred) > 0.7\n\n    # Test decision_function\n    dec_func = clf.decision_function(X)\n    assert dec_func.shape == (X.shape[0], len(clf.classes_))\n\n    # Test predict_proba\n    proba = clf.predict_proba(X)\n    assert proba.shape == (X.shape[0], len(clf.classes_))\n    assert np.allclose(proba.sum(axis=1), 1)\n", "def test_ridge_regression_multitarget():\n    # Test that Ridge regression works with multiple targets.\n    X, y = make_regression(n_samples=100, n_features=10, n_targets=3,\n                           random_state=0)\n    ridge = Ridge()\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (3, 10))\n    assert_equal(ridge.intercept_.shape, (3,))\n    y_pred = ridge.predict(X)\n    assert_equal(y_pred.shape, (100, 3))\n", "def test_ridge_multitarget():\n    # Test that Ridge can handle multiple targets.\n    X = np.random.randn(10, 5)\n    y = np.random.randn(10, 3)\n\n    ridge = Ridge()\n    ridge.fit(X, y)\n\n    assert ridge.coef_.shape == (3, 5)\n    assert ridge.intercept_.shape == (3,)\n\n    # Check the shape of the predictions.\n    y_pred = ridge.predict(X)\n    assert y_pred.shape == (10, 3)\n\n    # Check the shape of the score.\n    score = ridge.score(X, y)\n    assert isinstance(score, float)\n"], "sample_795": ["def test_check_estimator_sparse_data():\n    # check that estimators can handle sparse data correctly\n\n    class SparseDataEstimator(BaseEstimator):\n            X = check_array(X, accept_sparse='csr')\n            return self\n\n    est = SparseDataEstimator()\n    check_estimator(est)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # Test that check_class_weight_balanced_linear_classifier works.\n    estimator = SVC()\n    check_class_weight_balanced_linear_classifier(\"SVC\", estimator)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # check that check_class_weight_balanced_linear_classifier works as expected\n    class MockEstimator:\n            self.classes_ = [0, 1]\n            self.n_iter = 100\n\n            return self\n\n            return np.ones(X.shape[0])\n\n    msg = \"Classifier MockEstimator is not computing class_weight=balanced\"\n    assert_raises_regex(AssertionError, msg,\n                        check_class_weight_balanced_linear_classifier,\n                        \"MockEstimator\", MockEstimator)\n", "def test_check_estimator_works_with_64bit_sparse_indices():\n    # check that check_estimator() works with 64-bit sparse indices\n    X = sp.csr_matrix(np.random.rand(10, 5))\n    X.indices = X.indices.astype('int64')\n    X.indptr = X.indptr.astype('int64')\n\n    y = np.random.randint(0, 2, size=10)\n\n    estimator = SVC()\n    estimator.fit(X, y)\n\n    check_estimator(estimator)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # Test that check_class_weight_balanced_linear_classifier raises an error\n    # when the estimator's class weights are not balanced.\n\n    class DummyClassifier:\n            self.class_weight = class_weight\n\n            return self\n\n    with assert_raises_regex(AssertionError,\n                             \"Classifier DummyClassifier is not computing \"\n                             \"class_weight=balanced properly\"):\n        check_class_weight_balanced_linear_classifier(\"DummyClassifier\",\n                                                      DummyClassifier)\n"], "sample_796": ["def test_huber_raises_error_for_invalid_epsilon():\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True, epsilon=0.5)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n", "def test_huber_edge_cases():\n    # Test edge cases for HuberRegressor\n\n    # Test with X and y being empty\n    huber = HuberRegressor()\n    with pytest.raises(ValueError):\n        huber.fit(np.array([]), np.array([]))\n\n    # Test with epsilon being less than 1.0\n    huber = HuberRegressor(epsilon=0.5)\n    with pytest.raises(ValueError):\n        huber.fit(np.array([[1]]), np.array([1]))\n\n    # Test with alpha being negative\n    huber = HuberRegressor(alpha=-0.1)\n    with pytest.raises(ValueError):\n        huber.fit(np.array([[1]]), np.array([1]))\n\n    # Test with fit_intercept being None\n    huber = HuberRegressor(fit_intercept=None)\n    with pytest.raises(TypeError):\n        huber.fit(np.array([[1]]), np.array([1]))\n", "def test_huber_raises_error_for_low_epsilon():\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True, epsilon=0.5)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n", "def test_huber_edge_cases():\n    # Test edge cases such as X or y being empty, or having a single feature/samples.\n    X = np.array([[1]])\n    y = np.array([2])\n    huber = HuberRegressor(fit_intercept=True)\n    with pytest.raises(ValueError):\n        huber.fit(X, [])\n\n    with pytest.raises(ValueError):\n        huber.fit([], y)\n\n    huber.fit(X, y)\n    assert_array_almost_equal(huber.coef_, np.array([0.]), 2)\n    assert_array_almost_equal(huber.intercept_, np.array([2.]), 2)\n\n    X = np.array([[1], [2]])\n    y = np.array([2, 4])\n    huber.fit(X, y)\n    assert_array_almost_equal(huber.coef_, np.array([2.]), 2)\n    assert_array_almost_equal(huber.intercept_, np.array([0.]), 2)\n", "def test_huber_raise_error_for_epsilon():\n    # Test that HuberRegressor raises an error for epsilon < 1.0\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True, epsilon=0.5)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n"], "sample_797": ["def test_power_transformer_dtype():\n    # check that PowerTransformer handles different dtypes correctly\n    X = np.abs(X_2d)\n\n    for dtype in [np.float16, np.float32, np.float64]:\n        pt = PowerTransformer()\n        X_trans = pt.fit_transform(X.astype(dtype))\n        assert X_trans.dtype == dtype\n        assert pt.lambdas_.dtype == np.float64\n", "def test_power_transformer_dtype():\n    # check that PowerTransformer preserves the dtype of the input data\n    X = np.abs(X_1col).astype(np.float32)\n    pt = PowerTransformer()\n    assert pt.fit_transform(X).dtype == np.float32\n\n    X = np.abs(X_1col).astype(np.float64)\n    pt = PowerTransformer()\n    assert pt.fit_transform(X).dtype == np.float64\n", "def test_power_transformer_dtype():\n    # check that power_transformer supports various dtypes\n\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 3)\n\n    for dtype in [np.float16, np.float32, np.float64]:\n        XTyped = X.astype(dtype)\n        pt = PowerTransformer()\n        X_trans = pt.fit_transform(XTyped)\n        assert X_trans.dtype == XTyped.dtype\n", "def test_power_transformer_dtype():\n    # Test that power_transformer returns the correct dtype\n    X = np.abs(X_2d)\n    pt = PowerTransformer(method='box-cox')\n    X_trans = pt.fit_transform(X)\n    assert X.dtype == X_trans.dtype\n\n    X_float32 = X.astype(np.float32)\n    pt = PowerTransformer(method='box-cox')\n    X_trans_float32 = pt.fit_transform(X_float32)\n    assert X_float32.dtype == X_trans_float32.dtype\n", "def test_power_transformer_dtype():\n    # check that the output dtype is float64\n    X = np.abs(X_2d).astype(np.float32)\n    pt = PowerTransformer(method='box-cox')\n    X_trans = pt.fit_transform(X)\n    assert X_trans.dtype == np.float64\n\n    X = np.abs(X_2d).astype(np.int64)\n    pt = PowerTransformer(method='box-cox')\n    X_trans = pt.fit_transform(X)\n    assert X_trans.dtype == np.float64\n"], "sample_798": ["def test_ridgecv_scorer():\n    # Test that RidgeCV can take a custom scorer and raise an error when the\n    # scorer is not applicable.\n\n    rng = np.random.RandomState(0)\n    X, y = make_regression(n_samples=10, n_features=5, random_state=rng)\n\n    scorer = get_scorer('neg_mean_squared_error')\n    ridge = RidgeCV(scorer=scorer)\n    ridge.fit(X, y)\n\n    # Define a scorer that will throw an error when used with RidgeCV\n        raise ValueError(\"This scorer will always throw an error\")\n\n    scorer = make_scorer(bad_scorer)\n    ridge = RidgeCV(scorer=scorer)\n    assert_raises(ValueError, ridge.fit, X, y)\n", "def test_ridgecv_alphas_as_ndarray():\n    # Check that the alpha_ attribute of RidgeCV is set as a scalar\n    # when alphas is passed as an ndarray with one element.\n\n    X, y = make_regression(n_samples=6, n_features=5, random_state=42)\n\n    ridgecv = RidgeCV(alphas=np.array([1.0]))\n    ridgecv.fit(X, y)\n\n    assert isinstance(ridgecv.alpha_, float)\n    assert_equal(ridgecv.alpha_, 1.0)\n", "def test_ridge_regression_sparse_data():\n    # Test ridge regression on sparse data\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 50, 100\n    X = sp.csr_matrix(rng.randn(n_samples, n_features))\n    y = rng.randn(n_samples)\n\n    alphas = [1e-3, 1e-2, 1e-1]\n    for alpha in alphas:\n        ridge_dense = Ridge(alpha=alpha, fit_intercept=False)\n        ridge_dense.fit(X.toarray(), y)\n        ridge_sparse = Ridge(alpha=alpha, fit_intercept=False)\n        ridge_sparse.fit(X, y)\n        assert_array_almost_equal(ridge_dense.coef_, ridge_sparse.coef_)\n", "def test_ridge_classifier_multiclass():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=0, n_repeated=0, n_classes=3,\n                               random_state=42)\n    clf = RidgeClassifier()\n    clf.fit(X, y)\n\n    assert_array_equal(np.unique(y), clf.classes_)\n    assert_equal(clf.coef_.shape[0], len(clf.classes_))\n    assert_equal(clf.intercept_.shape[0], len(clf.classes_))\n", "def test_ridge_saga_solver_convergence():\n    # Test that the SAGA solver converges to the same solution as the\n    # Cholesky solver for a small dataset.\n\n    X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n    ridge_chol = Ridge(solver='cholesky', alpha=1.0)\n    ridge_saga = Ridge(solver='saga', alpha=1.0, max_iter=1000)\n\n    ridge_chol.fit(X, y)\n    ridge_saga.fit(X, y)\n\n    assert_array_almost_equal(ridge_chol.coef_, ridge_saga.coef_)\n    assert_almost_equal(ridge_chol.intercept_, ridge_saga.intercept_)\n"], "sample_799": ["def test_cross_validate_return_estimator():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    # Test that the returned estimator is the same as the input estimator\n    cv_results = cross_validate(clf, X, y, return_estimator=True)\n    assert_array_equal(cv_results['estimator'], [clf] * 5)\n\n    # Test that the returned estimator is a copy of the input estimator when\n    # the input estimator is modified during fitting\n    class ModifyingEstimator(SVC):\n            super().fit(X, y)\n            self.param = 'modified'\n\n    modifying_clf = ModifyingEstimator(kernel=\"linear\", random_state=0)\n    cv_results = cross_validate(modifying_clf, X, y, return_estimator=True)\n    for estimator in cv_results['estimator']:\n        assert hasattr(estimator, 'param')\n        assert estimator.param == 'modified'\n    assert not hasattr(modifying_clf, 'param')\n\n    # Test that the returned estimator is a copy of the input estimator when\n    # the input estimator has a public attribute\n    class PublicAttributeEstimator(SVC):\n            super().__init__(*args, **kwargs)\n            self.public_attribute = None\n\n    public_attribute_clf = PublicAttributeEstimator(kernel=\"linear\",\n                                                    random_state=0)\n    public_attribute_clf.public_attribute = 'public'\n    cv_results = cross_validate(public_attribute_clf, X, y,\n                                return_estimator=True)\n    for estimator in cv_results['estimator']:\n        assert hasattr(estimator, 'public_attribute')\n        assert estimator.public_attribute == 'public'\n", "def test_cross_validate_error_score_ndarray():\n    # Test cross_validate with error_score='raise' using ndarray as input\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)\n\n    with pytest.raises(ValueError):\n        cross_validate(clf, X, y, cv=2, error_score='raise')\n", "def test_fit_and_score_multimetric_scorer():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n\n    multimetric_scorer = {'accuracy': 'accuracy', 'f1': 'f1_macro'}\n    fit_and_score_args = [clf, X, y, multimetric_scorer, train, test, 0]\n    fit_and_score_kwargs = {'parameters': None, 'fit_params': None,\n                            'return_train_score': True}\n    result = _fit_and_score(*fit_and_score_args,\n                            **fit_and_score_kwargs)\n\n    assert isinstance(result[0], dict)\n    assert len(result[0]) == 2\n    assert 'accuracy' in result[0]\n    assert 'f1' in result[0]\n    assert isinstance(result[1], dict)\n    assert len(result[1]) == 2\n    assert 'accuracy' in result[1]\n    assert 'f1' in result[1]\n", "def test_cross_validate_fit_params():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n\n    # Test that an empty dictionary is passed when no fit_params is specified\n        assert fit_params == {}\n        return clf.fit(X, y)\n\n    clf.fit = fit_func\n    cross_validate(clf, X, y, cv=3)\n\n    # Test that fit_params are passed correctly\n        assert fit_params == {'sample_weight': np.ones(len(y))}\n        return clf.fit(X, y)\n\n    clf.fit = fit_func\n    cross_validate(clf, X, y, cv=3, fit_params={'sample_weight': np.ones(len(y))})\n", "def test_cross_val_score_warns_on_deprecated_parameter():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n\n    # Test that warning is raised when using deprecated parameter 'n_jobs' in cv\n    with warnings.catch_warnings(record=True) as w:\n        cross_val_score(clf, X, y, cv=3, n_jobs=2)\n    assert len(w) == 1\n    assert w[0].category == FutureWarning\n    assert \"The parameter 'n_jobs' in cross_val_score will be removed\" in str(w[0].message)\n\n    # Test that warning is raised when using deprecated parameter 'pre_dispatch' in cv\n    with warnings.catch_warnings(record=True) as w:\n        cross_val_score(clf, X, y, cv=3, pre_dispatch='all')\n    assert len(w) == 1\n    assert w[0].category == FutureWarning\n    assert \"The parameter 'pre_dispatch' in cross_val_score will be removed\" in str(w[0].message)\n"], "sample_800": ["def test_check_estimator_with_default_parameter():\n    class EstimatorWithDefaultParameter(BaseEstimator):\n            self.param = param\n\n    check_estimator(EstimatorWithDefaultParameter())\n", "def test_check_estimator_partial_fit_n_features():\n    # check that estimators raise an error when partial_fit is called with\n    # a different number of features than the first call\n\n    class DummyEstimator(BaseEstimator):\n            self.X_shape_ = None\n\n            if self.X_shape_ is not None and X.shape[1] != self.X_shape_[1]:\n                raise ValueError(\"Number of features has changed\")\n            self.X_shape_ = X.shape\n            return self\n\n    estimator = DummyEstimator()\n    check_estimator(estimator)\n", "def test_check_estimator_unfitted_with_fit():\n    # check that a NotFittedError is raised when calling predict\n    # on an unfitted estimator, even after fit was called but failed\n    class EstimatorWithFailingFit(BaseEstimator):\n            raise ValueError(\"Failed to fit\")\n        \n            raise CorrectNotFittedError(\"Estimator needs to be fitted first\")\n\n    check_estimators_unfitted(\"estimator\", EstimatorWithFailingFit())\n", "def test_check_estimator_unsupported_sparse_y():\n    # check that a ValueError is raised when y is sparse\n    class MyEstimator(BaseEstimator):\n            if sp.issparse(y):\n                raise ValueError(\"Sparse y not supported\")\n            return self\n\n            return np.zeros(X.shape[0])\n\n    check_estimator(MyEstimator())\n", "def test_check_class_weight_balanced_linear_classifier():\n    # check that the estimator raises a warning when setting\n    # class_weight='balanced' and max_iter is too low for convergence.\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    with warnings.catch_warnings(record=True) as w:\n        check_class_weight_balanced_linear_classifier(\n            \"SGDClassifier\", SGDClassifier(max_iter=100))\n        assert len(w) == 0\n\n    with warnings.catch_warnings(record=True) as w:\n        check_class_weight_balanced_linear_classifier(\n            \"SVC\", SVC())\n        assert len(w) == 0\n\n    with warnings.catch_warnings(record=True) as w:\n        check_class_weight_balanced_linear_classifier(\n            \"LogisticRegression\", LogisticRegression(max_iter=2))\n        assert len(w) > 0\n        assert issubclass(w[0].category, UserWarning)\n"], "sample_801": ["def test_repr_max_depth():\n    # Test the `max_depth` parameter of the repr\n\n    class RecursiveEstimator(BaseEstimator):\n            self.level = level\n            if level < 5:\n                self.child = RecursiveEstimator(level + 1)\n\n    est = RecursiveEstimator()\n\n    # Check that with default max_depth, we don't see the child estimator\n    assert \"child\" not in repr(est)\n\n    # Check that increasing max_depth, we can see the child estimator\n    set_config(print_changed_only=False)\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True,\n                                 max_depth=float('inf'))\n    expected = \"\"\"", "def test_clone():\n    # Test clone function\n    lr = LogisticRegression()\n    lr_cloned = clone(lr)\n    assert lr.__class__ == lr_cloned.__class__\n    assert lr.get_params() == lr_cloned.get_params()\n\n    # Test clone function with non-estimator object\n    try:\n        clone(\"not an estimator\")\n        assert False, \"Clone did not raise an error for a non-estimator object\"\n    except TypeError:\n        pass\n\n    # Test clone function with safe=False\n    try:\n        clone(\"not an estimator\", safe=False)\n        assert False, \"Clone did not raise an error for a non-estimator object with safe=False\"\n    except TypeError:\n        pass\n", "def test_get_params():\n    # Test that get_params returns all the parameters of an estimator\n    class MyEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    est = MyEstimator(param2='value2')\n    params = est.get_params()\n    assert 'param1' in params\n    assert 'param2' in params\n    assert params['param1'] == 'value1'\n    assert params['param2'] == 'value2'\n\n    # Test that get_params also works with nested objects\n    class NestedEstimator(BaseEstimator):\n            self.est1 = est1\n            self.est2 = est2\n\n    nested_est = NestedEstimator(est, est)\n    params = nested_est.get_params()\n    assert 'est1__param1' in params\n    assert 'est2__param2' in params\n    assert params['est1__param1'] == 'value1'\n    assert params['est2__param2'] == 'value2'\n", "def test_repr_with_np_array():\n    # Test that the repr of an estimator with a numpy array is correctly\n    # rendered.\n    lr = LogisticRegression()\n    lr.coef_ = np.array([[1, 2, 3]])\n    expected = \"\"\"", "def test_repr_of_estimator_with_array_parameter():\n    # Make sure estimator reprs work fine with array parameters\n    param_grid = {'C': np.array([1, 10])}\n    gs = GridSearchCV(SVC(), param_grid)\n    expected = \"\"\""], "sample_802": ["def test_pipeline_get_params():\n    # Test that get_params returns all the parameters for a pipeline\n    X = np.array([[1, 2]])\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n    mult5 = Mult(mult=5)\n\n    pipeline = Pipeline([('m2', mult2), ('m3', mult3), ('last', mult5)])\n    params = pipeline.get_params(deep=True)\n    assert 'steps' in params\n    assert 'memory' in params\n    assert 'm2' in params\n    assert 'm2__mult' in params\n    assert 'm3' in params\n    assert 'm3__mult' in params\n    assert 'last' in params\n    assert 'last__mult' in params\n\n    # check if we get the same params after fitting\n    pipeline.fit(X)\n    params2 = pipeline.get_params(deep=True)\n    assert params == params2\n", "def test_pipeline_transformer_get_params():\n    # Test that get_params on a transformer in a pipeline yields the same\n    # result as on the transformer directly.\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    transf = PCA(n_components=2, svd_solver='full')\n    pipe = Pipeline([('pca', transf)])\n    pipe.fit(X, y)\n    assert_equal(transf.get_params(), pipe.named_steps['pca'].get_params())\n", "def test_pipeline_inverse_transform():\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Mult(mult=2)), ('clf', Transf())])\n    pipe.fit(X, y=None)\n    assert_array_equal(pipe.inverse_transform([[4, 8]]), X)\n    pipe.set_params(transf='passthrough')\n    pipe.fit(X, y=None)\n    assert_array_equal(pipe.inverse_transform([[4, 8]]), [[4, 8]])\n", "def test_pipeline_memory_with_fit_params():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = TransfFitParams()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y, transf__should_get_this=True)\n        pipe.fit(X, y, transf__should_get_this=True)\n        # Get the time stamp of the transformer in the cached pipeline\n        ts = cached_pipe.named_steps['transf'].timestamp_\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].fit_params,\n                           cached_pipe.named_steps['transf'].fit_params)\n        # Check that we are reading the cache while fitting\n        # a second time\n        cached_pipe.fit(X, y, transf__should_get_this=True)\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].fit_params,\n                           cached_pipe.named_steps['transf'].fit_params)\n        assert_equal(ts, cached_pipe.named_steps['transf'].timestamp_)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_get_params():\n    # Test that get_params returns all parameters of the estimators\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n    pipeline = Pipeline([('m1', mult2), ('m2', mult3)])\n    params = pipeline.get_params()\n    assert 'm1__mult' in params\n    assert 'm2__mult' in params\n    assert params['m1__mult'] == 2\n    assert params['m2__mult'] == 3\n"], "sample_803": ["def test_roc_auc_score_average_options():\n    # Test different averaging options for multi-class roc_auc_score\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([[0.9, 0.05, 0.05], [0.1, 0.8, 0.1], [0.3, 0.4, 0.3]])\n\n    macro_auc = roc_auc_score(y_true, y_pred, average='macro')\n    weighted_auc = roc_auc_score(y_true, y_pred, average='weighted')\n    samples_auc = roc_auc_score(y_true, y_pred, average='samples')\n\n    assert macro_auc != weighted_auc\n    assert macro_auc != samples_auc\n    assert weighted_auc != samples_auc\n", "def test_roc_auc_score_partial():\n    # Test whether the roc_auc_score function returns half the area of the unit\n    # square when there is only one positive sample.\n    y_true = [0, 0, 1]\n    y_predict = [0.25, 0.75, 0.5]\n    assert_almost_equal(roc_auc_score(y_true, y_predict), 0.75)\n\n    # Also test the max_fpr parameter.\n    assert_almost_equal(_partial_roc_auc_score(y_true, y_predict, max_fpr=1),\n                        roc_auc_score(y_true, y_predict))\n    assert_almost_equal(_partial_roc_auc_score(y_true, y_predict, max_fpr=1),\n                        roc_auc_score(y_true, y_predict, max_fpr=1))\n", "def test_roc_auc_score_multiclass():\n    # Test Area under ROC curve for multiclass problems\n    y_true, _, probas_pred = make_prediction(binary=False)\n    n_classes = 3\n\n    # Compute ROC AUC using the one-vs-all approach\n    for average in ['macro', 'weighted', 'micro']:\n        roc_auc_ovr = roc_auc_score(y_true, probas_pred,\n                                    multi_class='ovr', average=average)\n\n        # We use the following formula to compute the average ROC AUC:\n        # 1. one-vs-all ROC AUC is computed for each class\n        # 2. if average='macro' or average='weighted', the average is taken\n        #    over the classes, with 'weighted' taking into account the number\n        #    of samples in each class.\n        if average == 'macro':\n            # macro-average (each class has the same weight)\n            roc_auc_macro = np.mean([roc_auc_score((y_true == c).astype(int),\n                                                  probas_pred[:, i])\n                                     for c, i in enumerate(range(n_classes))])\n            assert_almost_equal(roc_auc_ovr, roc_auc_macro)\n        elif average == 'weighted':\n            # weighted average (weighting by the number of samples in each class)\n            weights = np.array([np.sum(y_true == c) for c in range(n_classes)])\n            weights /= np.sum(weights)\n            roc_auc_weighted = np.average(\n                [roc_auc_score((y_true == c).astype(int), probas_pred[:, i])\n                 for c, i in enumerate(range(n_classes))],\n                weights=weights)\n            assert_almost_equal(roc_auc_ovr, roc_auc_weighted)\n        elif average == 'micro':\n            # micro-average (use the micro-average precision and recall of\n            # each label, as done for multilabel problems)\n            y_true_micro = label_binarize(y_true, np.unique(y_true))\n            roc_auc_micro = roc_auc_score(y_true_micro, probas_pred,\n                                          average='micro')\n            assert_almost_equal(roc_auc_ovr, roc_auc_micro)\n", "def test_label_ranking_average_precision_score_tie_handling():\n    # Test that the function handles ties in the scores correctly\n    y_true = np.array([[1, 0], [0, 1]])\n    y_score = np.array([[0.5, 0.5], [0.5, 0.5]])\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), \n                        0.5)\n", "def test_roc_auc_score_average_options():\n    # Test different average options for roc_auc_score\n    y_true = np.array([[0, 1], [0, 1]])\n    y_score = np.array([[0, 1], [1, 0]])\n\n    # 'macro' average\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='macro'), 0.5)\n\n    # 'weighted' average\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='weighted'), 0.5)\n\n    # 'samples' average\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='samples'), 0.5)\n\n    # 'micro' average\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='micro'), 0.5)\n"], "sample_804": ["def test_one_hot_encoder_drop_first_multiple_features():\n    X = [['Male', 1, 'cat'], ['Female', 2, 'dog']]\n    enc = OneHotEncoder(drop='first')\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 0, 0],\n           [1, 1, 1]]\n    assert_array_equal(trans, exp)\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_inverse_transform_object_dtype():\n    X = np.array([['a', 1], ['b', 2]], dtype=object)\n    enc = OneHotEncoder(sparse=False)\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(X_tr), X)\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['def', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 0, 1, 1],\n           [1, 0, 0, 1],\n           [1, 1, 0, 0]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 12, 2, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['def', 12, 3, 56]]\n    enc = OneHotEncoder(drop='first')\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 0, 1, 0, 0, 1],\n           [1, 0, 0, 1, 0, 0],\n           [0, 1, 0, 0, 1, 0]]\n    assert_array_equal(trans, exp)\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['def', 12, 3, 56]]\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 0, 1, 0],\n           [1, 0, 0, 0],\n           [0, 1, 0, 1]]\n    assert_array_equal(trans, exp)\n    dropped_cats = [cat[feature]\n                    for cat, feature in zip(enc.categories_,\n                                            enc.drop_idx_)]\n    assert_array_equal(dropped_cats, ['abc', 12, 1, 55])\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n"], "sample_805": ["def test_mean_tweedie_deviance_dtype():\n    y_true = np.array([1.0, 2.0, 3.0])\n    y_pred = np.array([1.5, 2.5, 3.5])\n\n    # check that it works with float32\n    y_true_32 = y_true.astype(np.float32)\n    y_pred_32 = y_pred.astype(np.float32)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=1),\n                        mean_tweedie_deviance(y_true_32, y_pred_32, p=1))\n\n    # check that it works with float64\n    y_true_64 = y_true.astype(np.float64)\n    y_pred_64 = y_pred.astype(np.float64)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=1),\n                        mean_tweedie_deviance(y_true_64, y_pred_64, p=1))\n", "def test_regression_metrics_with_sample_weights():\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([1.1, 1.9, 3.2, 4.1])\n    sample_weights = np.array([0.5, 0.5, 1, 1])\n\n    # Test mean squared error with sample weights\n    mse = mean_squared_error(y_true, y_pred, sample_weight=sample_weights)\n    assert_almost_equal(mse, 0.125, decimal=2)\n\n    # Test mean absolute error with sample weights\n    mae = mean_absolute_error(y_true, y_pred, sample_weight=sample_weights)\n    assert_almost_equal(mae, 0.225, decimal=2)\n\n    # Test mean squared log error with sample weights\n    msle = mean_squared_log_error(y_true, y_pred, sample_weight=sample_weights)\n    assert_almost_equal(msle, 0.007, decimal=2)\n\n    # Test r2 score with sample weights\n    r2 = r2_score(y_true, y_pred, sample_weight=sample_weights)\n    assert_almost_equal(r2, 0.98, decimal=2)\n\n    # Test explained variance score with sample weights\n    evs = explained_variance_score(y_true, y_pred, sample_weight=sample_weights)\n    assert_almost_equal(evs, 0.99, decimal=2)\n", "def test_mean_tweedie_deviance_input_validation():\n    y_true = [1, 2, 3]\n    y_pred = [4, 5, 6]\n\n    # Test p must be numeric\n    with pytest.raises(TypeError):\n        mean_tweedie_deviance(y_true, y_pred, p='string')\n\n    # Test p cannot be None\n    with pytest.raises(TypeError):\n        mean_tweedie_deviance(y_true, y_pred, p=None)\n\n    # Test p must be in the defined range\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=0.5)\n", "def test_regression_metrics_with_nan():\n    y_true = np.array([1, 2, np.nan, 4])\n    y_pred = np.array([1, 3, 3, 4])\n\n    assert np.isnan(mean_squared_error(y_true, y_pred))\n    assert np.isnan(mean_absolute_error(y_true, y_pred))\n    assert np.isnan(median_absolute_error(y_true, y_pred))\n    assert np.isnan(max_error(y_true, y_pred))\n    assert np.isnan(r2_score(y_true, y_pred))\n    assert np.isnan(explained_variance_score(y_true, y_pred))\n\n    # Check that nan is propagated to the output for multioutput\n    y_true = np.array([[1, 2], [2, np.nan], [4, 5]])\n    y_pred = np.array([[1, 3], [3, 3], [4, 5]])\n\n    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n    assert np.isnan(mse[1])\n    assert not np.isnan(mse[0])\n\n    mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n    assert np.isnan(mae[1])\n    assert not np.isnan(mae[0])\n\n    r2 = r2_score(y_true, y_pred, multioutput='raw_values')\n    assert np.isnan(r2[1])\n    assert not np.isnan(r2[0])\n\n    evs = explained_variance_score(y_true, y_pred, multioutput='raw_values')\n    assert np.isnan(evs[1])\n    assert not np.isnan(evs[0])\n", "def test_mean_tweedie_deviance_zero_powers():\n    y_true = [1, 2, 3]\n    y_pred = [1.1, 2.2, 3.3]\n\n    # check that p=0 corresponds to mean squared error\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=0),\n                        mean_squared_error(y_true, y_pred))\n\n    # check that p=1 corresponds to mean Poisson deviance\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=1),\n                        mean_poisson_deviance(y_true, y_pred))\n\n    # check that p=2 corresponds to mean Gamma deviance\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=2),\n                        mean_gamma_deviance(y_true, y_pred))\n"], "sample_806": ["def test_gradient_boosting_min_weight_fraction_leaf():\n    # Check if min_weight_fraction_leaf has an effect.\n    X, y = make_classification(n_samples=1000, random_state=0)\n    sample_weight = np.random.RandomState(42).rand(1000)\n\n    gbc = GradientBoostingClassifier(n_estimators=10,\n                                     learning_rate=0.1, max_depth=3,\n                                     random_state=42)\n    gbc.fit(X, y, sample_weight=sample_weight)\n\n    gbc_min_weight_fraction_leaf = GradientBoostingClassifier(\n        n_estimators=10,\n        learning_rate=0.1, max_depth=3,\n        min_weight_fraction_leaf=0.5,\n        random_state=42)\n    gbc_min_weight_fraction_leaf.fit(X, y, sample_weight=sample_weight)\n\n    assert gbc.estimators_[0, 0].tree_.node_count != \\\n        gbc_min_weight_fraction_leaf.estimators_[0, 0].tree_.node_count\n\n    gbr = GradientBoostingRegressor(n_estimators=10,\n                                    learning_rate=0.1, max_depth=3,\n                                    random_state=42)\n    gbr.fit(X, y, sample_weight=sample_weight)\n\n    gbr_min_weight_fraction_leaf = GradientBoostingRegressor(\n        n_estimators=10,\n        learning_rate=0.1, max_depth=3,\n        min_weight_fraction_leaf=0.5,\n        random_state=42)\n    gbr_min_weight_fraction_leaf.fit(X, y, sample_weight=sample_weight)\n\n    assert gbr.estimators_[0, 0].tree_.node_count != \\\n        gbr_min_weight_fraction_leaf.estimators_[0, 0].tree_.node_count\n", "def test_gradient_boosting_init_estimator_with_nans():\n    # Check that GradientBoostingRegressor works when init estimator predicts\n    # NaNs.\n\n    X, y = make_regression()\n    init_est = DummyRegressor(strategy='constant', constant=np.nan)\n    gb = GradientBoostingRegressor(init=init_est)\n\n    with pytest.raises(ValueError,\n                       match=\"The initial estimator .* predicted NaN\"):\n        gb.fit(X, y)\n", "def test_gradient_boosting_with_init_string():\n    # Check that GradientBoostingRegressor works when init is a string.\n    X, y = make_regression(random_state=0)\n\n    gb = GradientBoostingRegressor(init='zero')\n    gb.fit(X, y)\n\n    with pytest.raises(ValueError,\n                       match=\"init must be a valid estimator or 'zero'\"):\n        gb = GradientBoostingRegressor(init='invalid')\n        gb.fit(X, y)\n", "def test_gradient_boosting_with_init_not_fitted():\n    # Check that GradientBoostingRegressor raises an error if the init\n    # estimator is not fitted.\n\n    X, y = make_regression(random_state=0)\n    init = LinearRegression()\n    gb = GradientBoostingRegressor(init=init)\n\n    with pytest.raises(NotFittedError):\n        gb._raw_predict_init(X)\n", "def test_gradient_boosting_init_estimator_not_fitted():\n    # Check that an error is raised if trying to fit with an initial estimator\n    # that has not been fitted\n\n    X, y = make_classification()\n\n    init_est = DummyClassifier()\n    gb = GradientBoostingClassifier(init=init_est)\n\n    message = \"The initial estimator DummyClassifier has not been fitted.\"\n    with pytest.raises(ValueError, match=message):\n        gb.fit(X, y, init=init_est)\n"], "sample_807": ["def test_calibration_curve_pos_label():\n    \"\"\"Check calibration_curve function with pos_label\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n\n    # default pos_label=1 should work fine\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 2)\n    assert_almost_equal(prob_true, [0, 1])\n    assert_almost_equal(prob_pred, [0.1, 0.9])\n\n    # setting pos_label to 0 should give the same result as inverting y_true\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, pos_label=0,\n                                             n_bins=2)\n    prob_true_inv, prob_pred_inv = calibration_curve(1-y_true, y_pred,\n                                                      n_bins=2)\n    assert_array_almost_equal(prob_true, prob_true_inv)\n    assert_array_almost_equal(prob_pred, prob_pred_inv)\n\n    # setting pos_label to a value not present in y_true should raise an error\n    assert_raises(ValueError, calibration_curve, y_true, y_pred, pos_label=2,\n                  n_bins=2)\n", "def test_calibration_curve_pos_label():\n    \"\"\"Check calibration_curve function with pos_label\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    prob_true_unnormalized, prob_pred_unnormalized = \\\n        calibration_curve(y_true, y_pred * 2, n_bins=2, normalize=True)\n\n    # Check that results are the same when using pos_label\n    prob_true_pos_label, prob_pred_pos_label = calibration_curve(\n        y_true, y_pred, n_bins=2, pos_label=1)\n    assert_array_almost_equal(prob_true, prob_true_pos_label)\n    assert_array_almost_equal(prob_pred, prob_pred_pos_label)\n\n    # Check that results are the same when using pos_label with normalize\n    prob_true_pos_label_unnormalized, prob_pred_pos_label_unnormalized = \\\n        calibration_curve(y_true, y_pred * 2, n_bins=2, normalize=True,\n                          pos_label=1)\n    assert_array_almost_equal(prob_true_unnormalized,\n                              prob_true_pos_label_unnormalized)\n    assert_array_almost_equal(prob_pred_unnormalized,\n                              prob_pred_pos_label_unnormalized)\n", "def test_calibration_curve_warning():\n    \"\"\"Check calibration_curve function raises warning when only one class\"\"\"\n    y_true = np.array([0, 0, 0])\n    y_pred = np.array([0., 0.1, 0.2])\n\n    with pytest.warns(UserWarning):\n        calibration_curve(y_true, y_pred, n_bins=2)\n", "def test_calibration_curve_no_data_points():\n    \"\"\"Check calibration_curve function when no data points are in a bin\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n\n    # With only two bins and data points concentrated on the ends, we expect\n    # one of the bins to be empty\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    assert_equal(len(prob_true), 1)\n    assert_almost_equal(prob_true, [0.5])\n    assert_almost_equal(prob_pred, [0.5])\n", "def test_calibration_curve_pos_label():\n    \"\"\"Check calibration_curve function with pos_label argument\"\"\"\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_pred = np.array([0., 0.1, 0.2, 0.8, 0.9, 1.])\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    prob_true_pos_label, prob_pred_pos_label = calibration_curve(\n        y_true, y_pred, n_bins=2, pos_label=0)\n\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_equal(len(prob_true), 2)\n    assert_almost_equal(prob_true, [0, 1])\n    assert_almost_equal(prob_pred, [0.1, 0.9])\n    assert_almost_equal(prob_true_pos_label, [1, 0])\n    assert_almost_equal(prob_pred_pos_label, [0.9, 0.1])\n"], "sample_808": ["def test_iforest_predict_on_sparse():\n    # Test that predict works with sparse data when trained on dense data\n    X_train = np.array([[0, 1], [1, 2]])\n    X_test_sparse = csr_matrix(np.array([[2, 1], [1, 1]]))\n\n    clf = IsolationForest(random_state=rng).fit(X_train)\n    y_pred = clf.predict(X_test_sparse)\n\n    assert_array_equal(y_pred, [-1, -1])\n", "def test_offset_():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    clf2 = IsolationForest().fit(X_train)\n    assert_array_equal(clf1.offset_, -0.5)\n    assert_array_equal(clf2.offset_, -0.5)\n    clf3 = IsolationForest(behaviour='new', contamination=0.1).fit(X_train)\n    assert clf3.offset_ != -0.5\n", "def test_iforest_fit_time():\n    \"\"\"Test that fitting time of IsolationForest is not too high\"\"\"\n    X = np.random.randn(100, 5)\n    start_time = time.time()\n    IsolationForest(n_estimators=100).fit(X)\n    end_time = time.time()\n    assert end_time - start_time < 2.0\n", "def test_iforest_fit_time():\n    \"\"\"Test that fitting an IsolationForest is faster for `contamination='auto'`\n    than for `contamination=0.1`.\"\"\"\n\n    rng = check_random_state(0)\n    X = rng.randn(1000, 2)\n\n    start = time.time()\n    IsolationForest(contamination='auto').fit(X)\n    auto_time = time.time() - start\n\n    start = time.time()\n    IsolationForest(contamination=0.1).fit(X)\n    const_time = time.time() - start\n\n    assert auto_time < const_time\n", "def test_iforest_n_features_in():\n    X_train = np.array([[0, 1], [1, 2]])\n    clf = IsolationForest().fit(X_train)\n    assert hasattr(clf, 'n_features_in_')\n    assert clf.n_features_in_ == X_train.shape[1]\n"], "sample_809": ["def test_mutual_info_regression_sparse():\n    # Test that sparse and dense inputs give the same results for mutual_info_regression\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([0, 1])\n    \n    # Create a sparse version of X\n    X_sparse = csr_matrix(X)\n    \n    mi_dense = mutual_info_regression(X, y)\n    mi_sparse = mutual_info_regression(X_sparse, y)\n    \n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Test that MI is correctly computed on sparse data.\n    rng = check_random_state(0)\n    X = csr_matrix(rng.rand(100, 5))\n    y = (rng.rand(100) > 0.5).astype(int)\n\n    mi_dense = mutual_info_classif(X.toarray(), y, discrete_features='auto')\n    mi_sparse = mutual_info_classif(X, y, discrete_features='auto')\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Test that mutual_info_classif works with sparse matrices\n    rng = check_random_state(0)\n    X = csr_matrix(rng.rand(100, 5))\n    y = rng.randint(0, 2, size=100)\n\n    mi = mutual_info_classif(X, y, discrete_features='auto', random_state=0)\n    assert_array_equal(mi.shape, (X.shape[1],))\n", "def test_mutual_info_regression_sparse():\n    # Test that mutual_info_regression works with sparse matrices.\n    rng = check_random_state(0)\n    X = rng.rand(100, 5)\n    y = rng.rand(100)\n\n    # Make X sparse by replacing most of the values with zeros.\n    X[X < 0.9] = 0\n    X_sparse = csr_matrix(X)\n\n    mi_dense = mutual_info_regression(X, y, random_state=0)\n    mi_sparse = mutual_info_regression(X_sparse, y, random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_regression_sparse():\n    # We generate sample from multivariate normal distribution, using\n    # transformation from initially uncorrelated variables. The zero\n    # variables after transformation is selected as the target vector,\n    # it has the strongest correlation with the variable 2, and\n    # the weakest correlation with the variable 1.\n    T = np.array([\n        [1, 0.5, 2, 1],\n        [0, 1, 0.1, 0.0],\n        [0, 0.1, 1, 0.1],\n        [0, 0.1, 0.1, 1]\n    ])\n    cov = T.dot(T.T)\n    mean = np.zeros(4)\n\n    rng = check_random_state(0)\n    Z = rng.multivariate_normal(mean, cov, size=1000)\n    X = csr_matrix(Z[:, 1:])\n    y = Z[:, 0]\n\n    mi = mutual_info_regression(X, y, random_state=0)\n    assert_array_equal(np.argsort(-mi), np.array([1, 2, 0]))\n"], "sample_810": ["def test_pipeline_get_params():\n    # Test that get_params returns all parameters of the estimators\n    mult2 = Mult(2)\n    mult3 = Mult(3)\n    pipe = Pipeline([('m1', mult2), ('m2', mult3)])\n\n    params = pipe.get_params()\n    assert 'm1' in params\n    assert 'm2' in params\n    assert 'm1__mult' in params\n    assert 'm2__mult' in params\n\n    # Check that parameter values are correct\n    assert params['m1'] == mult2\n    assert params['m2'] == mult3\n    assert params['m1__mult'] == 2\n    assert params['m2__mult'] == 3\n", "def test_pipeline_repr():\n    # Test the repr of pipeline\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    transf = SelectKBest(f_classif)\n    pipe = Pipeline([('anova', transf), ('svc', clf)])\n    pipe.fit(X, y)\n\n    # Check repr when steps have been fitted\n    repr_step1 = (\"SelectKBest(k=10, score_func=<function f_classif at \"\n                  \"%s>)\" % hex(id(f_classif)))\n    repr_step2 = (\"SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\"\n                  \" coef0=0.0,\\n\"\n                  \"    decision_function_shape='ovr', degree=3, gamma='scale',\"\n                  \" kernel='rbf', max_iter=-1,\\n\"\n                  \"    probability=True, random_state=0, shrinking=True,\"\n                  \" tol=0.001,\\n\"\n                  \"    verbose=False)\")\n    expected_repr = (\"Pipeline(memory=None,\\n\"\n                     \"         steps=[('anova', %s), ('svc', %s)])\" %\n                     (repr_step1, repr_step2))\n    assert_equal(repr(pipe), expected_repr)\n\n    # Check repr when steps have not been fitted\n    pipe = Pipeline([('anova', transf), ('svc', clf)])\n    expected_repr = (\"Pipeline(memory=None,\\n\"\n                     \"         steps=[('anova', SelectKBest(k=10)),\"\n                     \" ('svc', SVC())])\")\n    assert_equal(repr(pipe), expected_repr)\n", "def test_pipeline_with_no_transform_method():\n    class NoTransform(BaseEstimator):\n            return self\n\n    pipe = Pipeline([('nt', NoTransform()), ('svc', SVC())])\n    assert_raises_regex(TypeError,\n                        'All intermediate steps should be transformers '\n                        'and implement fit and transform or be the string '\n                        \"'passthrough' 'NoTransform' (type <class \"\n                        \"'sklearn.utils._testing.test_pipeline.NoTransform'>) \"\n                        \"doesn't\", pipe.fit, [[1]], [1])\n", "def test_pipeline_with_none_estimator():\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', None)])\n    assert_raises_regex(TypeError,\n                        \"Last step of Pipeline should implement fit \"\n                        \"or be the string 'passthrough'.\",\n                        pipe.fit, X)\n", "def test_pipeline_with_none_step():\n    # Test that a pipeline with a None step raises an error when fitting\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Define a pipeline with a None step\n    pipe = Pipeline([('transf', DummyTransf()), ('None', None), ('svc', SVC())])\n\n    # Check that an error is raised when fitting the pipeline\n    assert_raises_regex(ValueError, \"All intermediate steps should be\"\n                        \" transformers and implement fit and transform or\"\n                        \" be the string 'passthrough' 'None' (type <class\"\n                        \"'NoneType'>) doesn't\", pipe.fit, X, y)\n"], "sample_811": ["def test_pairwise_distances_dtype():\n    # Test that pairwise distances returns a float64 array for dense input\n    X = np.random.RandomState(0).rand(5, 4)\n    Y = np.random.RandomState(0).rand(3, 4)\n    D = pairwise_distances(X, Y, metric='euclidean')\n    assert_equal(D.dtype, np.float64)\n\n    # Test that pairwise distances returns a float32 array for sparse input\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    D_sparse = pairwise_distances(X_sparse, Y_sparse, metric='euclidean')\n    assert_equal(D_sparse.dtype, np.float32)\n", "def test_check_pairwise_arrays_dtype():\n    # Ensures that checks return valid arrays with correct dtype.\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XB, XA)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, None)\n    assert_equal(XA_checked.dtype, np.float32)\n    assert_equal(XB_checked.dtype, np.float32)\n", "def test_check_pairwise_arrays_dtype():\n    # Test that check_pairwise_arrays returns arrays with correct dtype\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n\n    X_checked, Y_checked = check_pairwise_arrays(X, Y)\n    assert_equal(X_checked.dtype, np.float64)\n    assert_equal(Y_checked.dtype, np.float64)\n\n    X_checked, Y_checked = check_pairwise_arrays(X, None)\n    assert_equal(X_checked.dtype, np.float32)\n    assert_equal(Y_checked.dtype, np.float32)\n", "def test_check_pairwise_arrays_dtype():\n    # Test that the dtype parameter is correctly handled.\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype='float64')\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype=None)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, None, dtype=None)\n    assert_equal(XA_checked.dtype, np.float32)\n    assert_equal(XB_checked.dtype, np.float32)\n\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB, dtype='int32')\n", "def test_pairwise_distances_reduce_memory_usage():\n    # Test the pairwise_distance helper function.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    # Working memory is much less than required for a dense matrix\n    with config_context(working_memory=1):\n        gen = pairwise_distances_chunked(X, reduce_func=np.sum)\n        assert isinstance(gen, GeneratorType)\n        sums = list(gen)\n        assert len(sums) > 1\n        assert_array_almost_equal(np.sum(pairwise_distances(X)), np.sum(sums))\n"], "sample_812": ["def test_pprint_with_empty_dict():\n    # Test that an empty dict is correctly rendered\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    estimator = LogisticRegression()\n    estimator.param_grid = {}\n    expected = \"\"\"", "def test_deep_params():\n    # Render an estimator with deep parameters.\n    pca = PCA(n_components=5, whiten=True)\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    selector = SelectKBest(score_func=chi2, k=5)\n    pipeline = make_pipeline(pca, selector)\n    expected = \"\"\"", "def test__format_items():\n    # Test that _format_items adds ellipsis when needed\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=10\n    )\n\n    # No ellipsis\n    items = [i for i in range(10)]\n    expected = '[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]'\n    assert pp._format_items(items, stream=None, indent=1, allowance=1, context={}, level=1) == expected\n\n    # With ellipsis\n    items = [i for i in range(11)]\n    expected = '[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...]'\n    assert pp._format_items(items, stream=None, indent=1, allowance=1, context={}, level=1) == expected\n", "def test_estimator_with_long_name():\n    # Test estimator with a long name that exceeds the line length limit\n    class EstimatorWithLongName(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n            self.param3 = param3\n            self.param4 = param4\n            self.param5 = param5\n\n    estimator = EstimatorWithLongName()\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    repr_ = pp.pformat(estimator)\n    expected = \"\"\"", "def test_empty_dict():\n    # Render an estimator with an empty dict parameter\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    pipeline = Pipeline([\n        ('reduce_dim', PCA()),\n        ('classify', SVC())\n    ])\n    param_grid = {\n        'reduce_dim': [PCA()],\n        'reduce_dim__n_components': [],\n        'classify__C': []\n    }\n    gspipline = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid)\n    expected = \"\"\""], "sample_813": ["def test_bayesian_ridge_verbose():\n    # Test BayesianRidge with verbose output\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(compute_score=True, verbose=True)\n    clf.fit(X, y)\n\n    # Check that the model could approximately learn the identity function\n    test = [[1], [3], [4]]\n    assert_array_almost_equal(clf.predict(test), [1, 3, 4], 2)\n", "def test_bayesian_ridge_predict_std():\n    # Test BayesianRidge.predict with return_std option\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(compute_score=True)\n    clf.fit(X, Y)\n\n    # Check that the model could approximately learn the identity function\n    test = [[1], [3], [4]]\n    y_mean, y_std = clf.predict(test, return_std=True)\n    assert_array_almost_equal(y_mean, [1, 3, 4], 2)\n    assert_array_less(y_std, 1)  # std should be small for these data\n", "def test_bayesian_ridge_multi_output():\n    # Test BayesianRidge with multi-output targets\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([[1, 2], [2, 3], [6, 5], [8, 7], [10, 9]])\n    clf = BayesianRidge(compute_score=True)\n    msg = \"Multi-output regression is not supported for BayesianRidge\"\n    assert_raise_message(ValueError, msg, clf.fit, X, y)\n", "def test_bayesian_ridge_ard_intercept():\n    # Test BayesianRidge and ARDRegression with and without intercept\n    X = np.array([[1, 0],\n                  [0, 0]])\n    y = np.array([1, 1])\n\n    for clf in [BayesianRidge(), ARDRegression()]:\n        clf.fit(X, y)\n        assert_almost_equal(clf.intercept_, 1)\n\n        clf = clf.__class__(fit_intercept=False)\n        clf.fit(X, y)\n        assert_almost_equal(clf.intercept_, 0)\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with and without intercept.\n    X = np.array([[1], [2], [6], [8], [10]])\n    y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(fit_intercept=True)\n    clf.fit(X, y)\n\n    clf_no_intercept = BayesianRidge(fit_intercept=False)\n    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n    clf_no_intercept.fit(X_with_bias, y)\n\n    assert_array_almost_equal(clf.coef_, clf_no_intercept.coef_[1:])\n    assert_almost_equal(clf.intercept_, clf_no_intercept.coef_[0])\n"], "sample_814": ["def test_gradient_boosting_with_init_estimator_not_fitted():\n    # Check that GradientBoostingRegressor works when init is a sklearn\n    # estimator that has not been fitted.\n\n    X, y = make_regression()\n    init_est = DummyRegressor()\n    gb = GradientBoostingRegressor(init=init_est)\n    gb.fit(X, y)\n\n    # Check that an error is raised if trying to fit with sample weight but\n    # initial estimator does not support sample weight\n    sample_weight = np.random.RandomState(42).rand(100)\n    gb.fit(X, y, sample_weight=sample_weight)\n", "def test_gradient_boosting_with_init_estimator_with_nans():\n    # Check that GradientBoostingRegressor works when init estimator is a\n    # sklearn estimator and there are NaNs in the data.\n\n    X, y = make_regression(random_state=0)\n    X[0, 0] = np.nan\n\n    init_est = DummyRegressor()\n    gb = GradientBoostingRegressor(init=init_est)\n\n    with pytest.raises(ValueError,\n                       match=\"Input contains NaN\"):\n        gb.fit(X, y)\n", "def test_gradient_boosting_with_init_estimator():\n    # Check that GradientBoostingRegressor works when init is an estimator\n    X, y = make_regression(random_state=0)\n    init_est = DummyRegressor()\n    gb = GradientBoostingRegressor(init=init_est)\n    gb.fit(X, y)\n\n    # Check that the init estimator was fitted\n    assert hasattr(init_est, 'n_features_in_')\n\n    # Check that the predictions are not equal to the init estimator's\n    # predictions\n    assert not np.array_equal(gb.predict(X), init_est.predict(X))\n", "def test_gradient_boosting_alpha():\n    # Test that the `alpha` parameter is correctly handled.\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1,\n                                     max_depth=3, random_state=42)\n    gbc.fit(X, y)\n\n    gbr = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1,\n                                    max_depth=3, loss='quantile', alpha=0.5,\n                                    random_state=42)\n    gbr.fit(X, y)\n\n    assert gbc.alpha == 0.9  # default value for classification\n    assert gbr.alpha == 0.5  # specified value for regression\n\n    # Check that an error is raised if alpha is outside (0, 1) for quantile\n    with pytest.raises(ValueError,\n                       match=\"alpha must be in (0.0, 1.0) but was\"):\n        GradientBoostingRegressor(loss='quantile', alpha=-1).fit(X, y)\n", "def test_early_stopping_n_iter_no_change_none():\n    # when n_iter_no_change is None, no early stopping should occur\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=100, n_iter_no_change=None,\n                                     learning_rate=0.1, max_depth=3,\n                                     random_state=42)\n\n    gbc.fit(X, y)\n    assert gbc.n_estimators == gbc.n_estimators_\n"], "sample_815": ["def test_multilabel_confusion_matrix_sparse():\n    # Test multilabel confusion matrix with sparse input\n    from scipy.sparse import csr_matrix\n\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    cm_sparse = multilabel_confusion_matrix(csr_matrix(y_true), csr_matrix(y_pred))\n\n    assert_array_equal(cm, cm_sparse)\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n    sample_weight = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\n    with ignore_warnings():\n        # Warnings are tested in test_balanced_accuracy_score_unseen\n        balanced = balanced_accuracy_score(y_true, y_pred,\n                                           sample_weight=sample_weight)\n    C = confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    per_class = np.diag(C) / C.sum(axis=1)\n    assert_almost_equal(balanced, np.mean(per_class))\n", "def test_balanced_accuracy_score_sample_weight():\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 1, 0]\n    sample_weight = [1, 2, 3, 4]\n\n    macro_recall = recall_score(y_true, y_pred, average='macro',\n                                labels=np.unique(y_true),\n                                sample_weight=sample_weight)\n    balanced = balanced_accuracy_score(y_true, y_pred,\n                                       sample_weight=sample_weight)\n    assert balanced == pytest.approx(macro_recall)\n", "def test_multilabel_confusion_matrix_sparse_inputs():\n    # Test multilabel confusion matrix with sparse inputs\n    from scipy.sparse import csr_matrix\n\n    y_true = csr_matrix(np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]]))\n    y_pred = csr_matrix(np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]]))\n\n    # compute confusion matrix with default labels introspection\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, [[[1, 0], [1, 1]],\n                            [[1, 0], [1, 1]],\n                            [[0, 2], [1, 0]]])\n\n    # compute confusion matrix with explicit label ordering\n    labels = [0, 2, 1]\n    cm = multilabel_confusion_matrix(y_true, y_pred, labels=labels)\n    assert_array_equal(cm, [[[1, 0], [1, 1]],\n                            [[0, 2], [1, 0]],\n                            [[1, 0], [1, 1]]])\n", "def test_multilabel_confusion_matrix_sparse():\n    # Test multilabel confusion matrix with sparse input\n    from scipy.sparse import csr_matrix\n\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n\n    # compute with dense arrays\n    cm_dense = multilabel_confusion_matrix(y_true, y_pred)\n\n    # compute with sparse arrays\n    y_true_sparse = csr_matrix(y_true)\n    y_pred_sparse = csr_matrix(y_pred)\n    cm_sparse = multilabel_confusion_matrix(y_true_sparse, y_pred_sparse)\n\n    # compare results\n    assert_array_equal(cm_dense, cm_sparse)\n"], "sample_816": ["def test_countvectorizer_fit_transform_empty_strings():\n    # Test that CountVectorizer can handle empty strings and empty lists of strings\n    cv = CountVectorizer()\n    X = cv.fit_transform(['', 'hello world', '', 'world hello'])\n    assert_equal(X.shape[0], 4)\n    assert_equal(X.shape[1], 2)\n    assert_array_equal(cv.get_feature_names(), ['hello', 'world'])\n", "def test_vectorizer_empty_input():\n    # Test that an empty input list returns an empty matrix\n    cv = CountVectorizer()\n    X = cv.fit_transform([])\n    assert_equal(X.shape, (0, 0))\n    assert_equal(cv.vocabulary_, {})\n", "def test_tfidfvectorizer_get_feature_names_out():\n    \"\"\"Check that get_feature_names_out returns correct feature names.\"\"\"\n    vect = TfidfVectorizer()\n    X = vect.fit_transform(ALL_FOOD_DOCS)\n    feature_names_out = vect.get_feature_names_out()\n    assert_array_equal(feature_names_out, vect.get_feature_names())\n", "def test_vectorizer_input_validation():\n    # check that invalid input types raise an error\n    message = (\"Iterable over raw text documents expected, \"\n               \"string object received.\")\n    for input_type in ['filename', 'file', 'content']:\n        vectorizer = CountVectorizer(input=input_type)\n        assert_raise_message(ValueError, message, vectorizer.fit_transform,\n                             \"hello world\")\n        assert_raise_message(ValueError, message, vectorizer.fit,\n                             \"hello world\")\n        assert_raise_message(ValueError, message, vectorizer.transform,\n                             \"hello world\")\n\n    # check that valid input types do not raise an error\n    for input_type in ['filename', 'file', 'content']:\n        vectorizer = CountVectorizer(input=input_type)\n        if input_type == 'filename':\n            X = [\"file1.txt\", \"file2.txt\"]\n        elif input_type == 'file':\n            X = [StringIO(\"hello world\")]\n        else:\n            X = [\"hello world\"]\n        vectorizer.fit_transform(X)\n        vectorizer.fit(X)\n        vectorizer.transform(X)\n", "def test_countvectorizer_empty_input():\n    # Test that CountVectorizer raises an error on empty input\n    vect = CountVectorizer()\n    with pytest.raises(ValueError, match=\"empty vocabulary\"):\n        vect.fit([])\n    with pytest.raises(ValueError, match=\"Iterable over raw text documents expected\"):\n        vect.transform([])\n"], "sample_817": ["def test_variance_threshold_sparse():\n    # Test VarianceThreshold with sparse matrix and custom variance.\n\n    X_sparse = csr_matrix([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]])\n    sel = VarianceThreshold(threshold=.5).fit(X_sparse)\n    assert_array_equal([], sel.get_support(indices=True))\n", "def test_variance_threshold_sparse_zero_threshold():\n    # Test VarianceThreshold with zero threshold on sparse data.\n    X = csr_matrix([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])\n    sel = VarianceThreshold(threshold=0).fit(X)\n    assert_array_equal([1, 2, 3], sel.get_support(indices=True))\n", "def test_variance_threshold_sparse_with_threshold_zero():\n    # Test VarianceThreshold with threshold=0 on sparse data.\n    X = csr_matrix([[0, 1, 0, 3], [0, 1, 0, 3], [0, 1, 0, 3]])\n    sel = VarianceThreshold(threshold=0).fit(X)\n    assert_array_equal([1, 3], sel.get_support(indices=True))\n", "def test_variance_threshold_sparse():\n    # Test VarianceThreshold with sparse matrix and custom variance.\n\n    X = csr_matrix([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 4]])\n    sel = VarianceThreshold(threshold=.2)\n    X = sel.fit_transform(X)\n    assert_array_equal([3], sel.get_support(indices=True))\n    assert_equal((3, 1), X.shape)\n", "def test_variance_threshold_edge_cases():\n    # Test VarianceThreshold with edge cases (threshold=0 and threshold=inf)\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # threshold=0 should not remove any features\n    sel = VarianceThreshold(threshold=0).fit(X)\n    assert_array_equal([True, True, True], sel.get_support())\n\n    # threshold=inf should remove all features\n    sel = VarianceThreshold(threshold=np.inf).fit(X)\n    assert_array_equal([False, False, False], sel.get_support())\n"], "sample_818": ["def test_spectral_clustering_with_eigen_tol():\n    # Test that spectral_clustering works with eigen_tol parameter\n\n    # a small two coin image\n    x, y = np.indices((40, 40))\n\n    center1, center2 = (14, 12), (20, 25)\n    radius1, radius2 = 8, 7\n\n    circle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2\n    circle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2\n\n    circles = circle1 | circle2\n    mask = circles.copy()\n    img = circles.astype(float)\n\n    graph = img_to_graph(img, mask=mask)\n    graph.data = np.exp(-graph.data / graph.data.std())\n\n    labels_arpack = spectral_clustering(\n        graph, n_clusters=2, eigen_solver='arpack', random_state=0, eigen_tol=1e-3)\n\n    assert len(np.unique(labels_arpack)) == 2\n", "def test_spectral_clustering_with_n_init():\n    # Test that spectral_clustering works with different n_init values\n\n    # a small two coin image\n    x, y = np.indices((40, 40))\n\n    center1, center2 = (14, 12), (20, 25)\n    radius1, radius2 = 8, 7\n\n    circle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2\n    circle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2\n\n    circles = circle1 | circle2\n    mask = circles.copy()\n    img = circles.astype(float)\n\n    graph = img_to_graph(img, mask=mask)\n    graph.data = np.exp(-graph.data / graph.data.std())\n\n    labels_1 = spectral_clustering(\n        graph, n_clusters=2, eigen_solver='arpack', random_state=0, n_init=1)\n\n    labels_5 = spectral_clustering(\n        graph, n_clusters=2, eigen_solver='arpack', random_state=0, n_init=5)\n\n    labels_10 = spectral_clustering(\n        graph, n_clusters=2, eigen_solver='arpack', random_state=0, n_init=10)\n\n    assert adjusted_rand_score(labels_1, labels_5) == 1\n    assert adjusted_rand_score(labels_1, labels_10) == 1\n    assert adjusted_rand_score(labels_5, labels_10) == 1\n", "def test_spectral_clustering_with_invalid_n_clusters():\n    # Test that spectral_clustering raises an error when n_clusters is invalid\n    S = np.array([[1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                  [1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                  [1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0],\n                  [0.2, 0.2, 0.2, 1.0, 1.0, 1.0, 1.0],\n                  [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],\n                  [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],\n                  [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]])\n\n    assert_raises(ValueError, spectral_clustering, S, n_clusters=0)\n    assert_raises(ValueError, spectral_clustering, S, n_clusters=-1)\n    assert_raises(ValueError, spectral_clustering, S, n_clusters=8)\n", "def test_spectral_clustering_n_components():\n    # Test that the number of components in the spectral embedding is correct\n    X, y = make_blobs(n_samples=100, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    sp = SpectralClustering(n_clusters=2, n_components=10, random_state=0)\n    labels = sp.fit(X).labels_\n    assert sp.n_components == 10\n    assert adjusted_rand_score(y, labels) == 1\n", "def test_spectral_clustering_with_n_components():\n    # Test that spectral_clustering works with n_components parameter\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n\n    S = rbf_kernel(X, gamma=1)\n    S = np.maximum(S - 1e-4, 0)\n\n    labels = spectral_clustering(S, n_clusters=2, n_components=3,\n                                 random_state=0, eigen_solver='arpack')\n    assert adjusted_rand_score(y, labels) == 1\n\n    labels = spectral_clustering(S, n_clusters=2, n_components=5,\n                                 random_state=0, eigen_solver='lobpcg')\n    assert adjusted_rand_score(y, labels) == 1\n"], "sample_819": ["def test_get_params():\n    \"\"\"Check get_params method of VotingClassifier.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                voting='soft')\n\n    params = eclf.get_params()\n    assert 'estimators' in params\n    assert 'voting' in params\n    assert 'weights' in params\n    assert 'n_jobs' in params\n    assert 'flatten_transform' in params\n\n    eclf.set_params(voting='hard')\n    params = eclf.get_params()\n    assert params['voting'] == 'hard'\n\n    eclf.set_params(n_jobs=2)\n    params = eclf.get_params()\n    assert params['n_jobs'] == 2\n", "def test_voting_regressor_get_params():\n    \"\"\"Test get_params method of VotingRegressor.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor()\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)])\n    params = ereg.get_params()\n    assert 'estimators' in params\n    assert len(params['estimators']) == 2\n    assert 'lr' in dict(params['estimators'])\n    assert 'rf' in dict(params['estimators'])\n", "def test_voting_regressor():\n    \"\"\"Check VotingRegressor on a regression dataset.\"\"\"\n    X, y = datasets.make_regression(n_samples=100, n_features=10, random_state=42)\n\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=42)\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)])\n\n    ereg.fit(X, y)\n    assert_array_almost_equal(ereg.predict(X), (reg1.fit(X, y).predict(X) + \n                                                reg2.fit(X, y).predict(X)) / 2)\n", "def test_get_params():\n    \"\"\"Check get_params method of VotingClassifier.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                voting='soft')\n\n    params = eclf.get_params()\n    assert 'estimators' in params\n    assert len(params['estimators']) == 2\n    assert 'voting' in params\n    assert params['voting'] == 'soft'\n\n    params = eclf.get_params(deep=True)\n    assert 'lr__C' in params\n    assert 'rf__n_estimators' in params\n", "def test_voting_regressor_predict():\n    \"\"\"Check VotingRegressor prediction on a toy dataset.\"\"\"\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=10, random_state=123)\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)])\n\n    ereg.fit(X, y)\n    assert_array_almost_equal(ereg.predict(X), np.array([1.05, 1.09, 1.89, 1.94]))\n"], "sample_820": ["def test_get_params():\n    \"\"\"Check the get_params method of VotingClassifier.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2)],\n        voting='soft')\n\n    params = eclf.get_params()\n    assert 'estimators' in params\n    assert len(params['estimators']) == 2\n    assert params['voting'] == 'soft'\n", "def test_set_params_passing_estimator_instance():\n    \"\"\"Check passing an estimator instance to set_params\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1)], voting='soft')\n    eclf.set_params(lr=clf2)\n    assert eclf.estimators[0][1] is clf2\n", "def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)]).fit(X, y)\n\n    assert_array_equal(ereg.transform(X).shape, (4, 2))\n    assert_array_almost_equal(ereg.transform(X),\n                              np.column_stack((reg1.predict(X), reg2.predict(X))))\n", "def test_get_params():\n    \"\"\"Check get_params method of VotingClassifier.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                voting='soft')\n    params = eclf.get_params()\n    assert 'lr' in params\n    assert 'rf' in params\n    assert params['lr'] is clf1\n    assert params['rf'] is clf2\n    assert params['voting'] == 'soft'\n", "def test_voting_regressor_get_params():\n    \"\"\"Check that VotingRegressor.get_params() returns all parameters.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor()\n    ereg = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)])\n\n    params = ereg.get_params(deep=True)\n    assert 'estimators' in params\n    assert 'lr' in params\n    assert 'rf' in params\n\n    params = ereg.get_params(deep=False)\n    assert 'estimators' in params\n    assert 'lr' not in params\n    assert 'rf' not in params\n"], "sample_821": ["def test_affinity_propagation_sparse_input():\n    # Test that AffinityPropagation accepts sparse input\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_sparse = af.fit_predict(X_sparse)\n    labels_dense = af.fit_predict(X)\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_sparse_input():\n    # Test that sparse input is not supported for non-precomputed affinity\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    assert_raises(TypeError, af.fit, X_sparse)\n", "def test_affinity_propagation_return_n_iter():\n    # Test that the number of iterations is returned when return_n_iter=True\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n    cluster_centers_indices, labels, n_iter = affinity_propagation(\n        S, preference=preference, return_n_iter=True)\n\n    assert isinstance(n_iter, int)\n    assert n_iter > 0\n\n    af = AffinityPropagation(preference=preference, return_n_iter=True)\n    af.fit(X)\n    assert hasattr(af, 'n_iter_')\n    assert isinstance(af.n_iter_, int)\n    assert af.n_iter_ > 0\n", "def test_affinity_propagation_fit_return_n_iter():\n    # Test return_n_iter parameter of affinity_propagation function\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S) * 10\n\n    cluster_centers_indices, labels, n_iter_1 = affinity_propagation(\n        S, preference=preference, return_n_iter=True)\n\n    af = AffinityPropagation(preference=preference)\n    af.fit(X)\n\n    assert_equal(n_iter_1, af.n_iter_)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from sklearn.utils.testing import set_random_state\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix(X)\n\n    af = AffinityPropagation(affinity=\"euclidean\", random_state=0)\n    set_random_state(af)\n\n    labels_sparse = af.fit_predict(X_sparse)\n\n    af = AffinityPropagation(affinity=\"euclidean\", random_state=0)\n    set_random_state(af)\n\n    labels_dense = af.fit_predict(X)\n\n    assert_array_equal(labels_sparse, labels_dense)\n"], "sample_822": ["def test_pairwise_distances_chunked_dtype():\n    # Check that pairwise_distances_chunked returns the correct dtype\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 10))\n    Y = rng.random_sample((50, 10))\n\n    for metric in PAIRWISE_DISTANCE_FUNCTIONS:\n        if metric == 'precomputed':\n            continue\n\n        distances = pairwise_distances(X, Y, metric=metric)\n        distances_chunked = next(pairwise_distances_chunked(X, Y, metric=metric))\n\n        assert_equal(distances.dtype, distances_chunked.dtype)\n", "def test_pairwise_distances_chunked_dtype():\n    # Test pairwise distances with different dtypes\n    X = np.random.RandomState(0).randn(100, 5)\n    Y = np.random.RandomState(0).randn(50, 5)\n\n    for dtype in [np.float32, np.float64]:\n        XTyped = X.astype(dtype)\n        YTyped = Y.astype(dtype)\n        D1 = pairwise_distances(XTyped, Y=YTYPed)\n        gen = pairwise_distances_chunked(XTyped, Y=YTYPed,\n                                         working_memory=2 ** -16)\n        D2 = np.vstack(list(gen))\n        assert_array_almost_equal(D1, D2)\n", "def test_pairwise_distances_chunked_reduce_with_axis():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    Y = rng.random_sample((200, 4))\n\n        return D_chunk.argmin(axis=0), D_chunk.min(axis=0)\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func,\n                                     working_memory=2 ** -16)\n    assert isinstance(gen, GeneratorType)\n    argmins, mins = zip(*list(gen))\n    argmin = np.concatenate(argmins)\n    min_val = np.concatenate(mins)\n\n    D = pairwise_distances(X, Y)\n    assert_array_almost_equal(argmin, D.argmin(axis=0))\n    assert_array_almost_equal(min_val, D.min(axis=0))\n", "def test_pairwise_distances_dtype():\n    # Check that pairwise_distances returns the correct dtype\n    X = np.random.rand(10, 5)\n    Y = np.random.rand(10, 5)\n\n    # Check with default metric (euclidean)\n    D = pairwise_distances(X, Y)\n    assert_equal(D.dtype, np.float64)\n\n    # Check with other metrics\n    for metric in ['manhattan', 'cosine']:\n        D = pairwise_distances(X, Y, metric=metric)\n        assert_equal(D.dtype, np.float64)\n\n    # Check with sparse input\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    D = pairwise_distances(X_sparse, Y_sparse)\n    assert_equal(D.dtype, np.float64)\n", "def test_pairwise_distances_chunked_reduce_callable():\n    # Test that the reduce_func can be a callable in pairwise_distances_chunked\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    Y = rng.random_sample((200, 4))\n\n        return D_chunk.mean(axis=1)\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func,\n                                     working_memory=2 ** -16)\n    assert isinstance(gen, GeneratorType)\n    means = list(gen)\n    assert len(means) > 1\n\n    # compute reference result\n    D = pairwise_distances(X, Y)\n    ref_means = [D[i].mean() for i in range(D.shape[0])]\n    assert_allclose(np.concatenate(means), ref_means)\n"], "sample_823": ["def test_pairwise_distances_chunked_reduce_memory_error():\n    # Test that pairwise_distances_chunked raises a MemoryError when the input is too large\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100000, 10))\n    Y = rng.random_sample((100000, 10))\n\n    with pytest.raises(MemoryError):\n        list(pairwise_distances_chunked(X, Y, working_memory=1e-6))\n", "def test_pairwise_distances_argmin_min_reduce_func():\n    # Test that argmin and min are correctly returned when using reduce_func\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[1, 2], [3, 4], [7, 8]])\n\n        return D_chunk.argmin(axis=1), D_chunk.min(axis=1)\n\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y, metric='euclidean',\n                                                     reduce_func=reduce_func)\n    expected_argmin = np.array([0, 1, 2])\n    expected_min_dist = np.array([0., 0., np.sqrt(5**2 + 2**2)])\n    assert_array_almost_equal(argmin, expected_argmin)\n    assert_array_almost_equal(min_dist, expected_min_dist)\n", "def test_pairwise_distances_dtype():\n    # Test that pairwise_distances returns the correct dtype\n    X = np.random.rand(10, 5).astype(np.float32)\n    Y = np.random.rand(10, 5).astype(np.float32)\n\n    # Check with default metric (euclidean)\n    D = pairwise_distances(X, Y)\n    assert D.dtype == np.float32\n\n    # Check with other metrics\n    for metric in PAIRWISE_DISTANCE_FUNCTIONS:\n        D = pairwise_distances(X, Y, metric=metric)\n        assert D.dtype == np.float32\n\n    # Check with sparse input\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    D = pairwise_distances(X_sparse, Y_sparse)\n    assert D.dtype == np.float32\n", "def test_pairwise_distances_chunked_reduce_func():\n    # Test the reduce function of pairwise_distances_chunked with different\n    # types of output from the reduce function.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    \n    # Reduce function returning an array\n        return D_chunk[:, :100]\n    \n    gen = pairwise_distances_chunked(X, None, reduce_func=reduce_func_array,\n                                     working_memory=2 ** -16)\n    assert isinstance(gen, GeneratorType)\n    result_array = np.vstack(list(gen))\n    \n    # Reduce function returning a list\n        return [D_chunk[:, :100]]\n    \n    gen = pairwise_distances_chunked(X, None, reduce_func=reduce_func_list,\n                                     working_memory=2 ** -16)\n    assert isinstance(gen, GeneratorType)\n    result_list = np.vstack([x[0] for x in gen])\n    \n    # Reduce function returning a sparse matrix\n        return csr_matrix(D_chunk[:, :100])\n    \n    gen = pairwise_distances_chunked(X, None, reduce_func=reduce_func_sparse,\n                                     working_memory=2 ** -16)\n    assert isinstance(gen, GeneratorType)\n    result_sparse = vstack(list(gen))\n    \n    assert_array_almost_equal(result_array, result_list)\n    assert_array_almost_equal(result_array, result_sparse.toarray())\n", "def test_pairwise_distances_chunked_sparse():\n    # Test pairwise distances with sparse matrix and chunking\n    rng = np.random.RandomState(0)\n    X = csr_matrix(rng.random_sample((100, 4)))\n    Y = csr_matrix(rng.random_sample((200, 4)))\n\n    D = pairwise_distances(X, Y, metric='euclidean', n_jobs=1)\n    D_chunked = pairwise_distances_chunked(X, Y, metric='euclidean',\n                                          working_memory=1)\n\n    assert isinstance(D_chunked, GeneratorType)\n    D_chunked = np.vstack(list(D_chunked))\n\n    assert_array_almost_equal(D, D_chunked)\n"], "sample_824": ["def test_check_pairwise_arrays_dtype():\n    # Test that check_pairwise_arrays returns arrays with correct dtype\n    XA = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    XB = np.array([[5, 6], [7, 8]], dtype=np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XB, XA)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, None)\n    assert_equal(XA_checked.dtype, np.float32)\n    assert_equal(XB_checked.dtype, np.float32)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XB, None)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n", "def test_pairwise_distances_chunked_reduce_memory_error():\n    # Test that pairwise_distances_chunked with reduce_func raises MemoryError\n    # if the reduced chunk is too large to fit in memory\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n\n        return D_chunk[:, :100].astype(np.float64)\n\n    with pytest.raises(MemoryError):\n        with config_context(working_memory=0.01):\n            next(pairwise_distances_chunked(X, None, reduce_func=reduce_func))\n", "def test_pairwise_distances_chunked_reduce_memory_usage():\n    # Test that pairwise_distances_chunked reduce_func does not increase\n    # memory usage beyond working_memory.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((1000, 10))\n    Y = rng.random_sample((1000, 10))\n\n        return (D_chunk,)\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func,\n                                     working_memory=1)\n\n    mem_usage = []\n    for _ in gen:\n        mem_usage.append(psutil.Process().memory_info().rss / (1024 * 1024))\n\n    assert np.max(mem_usage) - np.min(mem_usage) < 2\n", "def test_check_pairwise_arrays_dtype():\n    # Ensures that checks return valid arrays with correct dtype.\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XB, XA)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, None)\n    assert_equal(XA_checked.dtype, np.float32)\n    assert_equal(XB_checked.dtype, np.float32)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XB, None)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n", "def test_pairwise_distances_chunked_dtype():\n    # Test pairwise_distances_chunked with different dtypes\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n    Y = rng.random_sample((200, 4))\n\n    for metric in ['euclidean', 'manhattan', 'cosine']:\n        for dtype in [np.float32, np.float64]:\n            X_cast = X.astype(dtype)\n            Y_cast = Y.astype(dtype)\n            D_chunks = pairwise_distances_chunked(X_cast, Y_cast,\n                                                  working_memory=1,\n                                                  metric=metric)\n            D = pairwise_distances(X_cast, Y_cast, metric=metric)\n            assert_array_almost_equal(np.vstack(D_chunks), D)\n"], "sample_825": ["def test_pls_predict_shape():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression()]:\n        clf.fit(X, Y)\n        y_pred = clf.predict(X)\n        assert_equal(y_pred.shape, Y.shape)\n", "def test_pls_score_decomposition():\n    # Test that the X and Y scores are correctly decomposed into their component parts\n\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression()]:\n        clf.fit(X, Y)\n\n        # Check that the X scores can be reconstructed from the x_loadings_ and x_weights_\n        assert_array_almost_equal(clf.x_scores_, np.dot(X - clf.x_mean_, clf.x_weights_))\n\n        # Check that the Y scores can be reconstructed from the y_loadings_ and y_weights_\n        assert_array_almost_equal(clf.y_scores_, np.dot(Y - clf.y_mean_, clf.y_weights_))\n", "def test_pls_canonical_zero_components():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls_ca = pls_.PLSCanonical(n_components=0)\n    with pytest.raises(ValueError):\n        pls_ca.fit(X, Y)\n", "def test_pls_invalid_algorithm():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    pls_bynipals = pls_.PLSCanonical(n_components=X.shape[1], algorithm=\"invalid\")\n    assert_raise_message(ValueError, \"Got algorithm invalid when only 'svd' \"\n                             \"and 'nipals' are known\", pls_bynipals.fit, X, Y)\n", "def test_pls_non_finite_inputs():\n    # check that the function raises a ValueError when given non-finite inputs\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # introduce non-finite values in X and Y\n    X[0, 0] = np.nan\n    Y[0, 0] = np.inf\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        assert_raise_message(ValueError, \"Input contains NaN, infinity or a value too large\",\n                             clf.fit, X, Y)\n"], "sample_826": ["def test_one_hot_encoder_categories_not_sorted():\n    X = [[1, 2], [3, 4]]\n    categories = [[3, 1, 2], [4, 5, 6]]\n    enc = OneHotEncoder(categories=categories)\n    msg = \"Unsorted categories are not supported\"\n    with pytest.raises(ValueError, match=msg):\n        enc.fit(X)\n\n    # should work fine if categories are sorted\n    categories = [[1, 2, 3], [4, 5, 6]]\n    enc = OneHotEncoder(categories=categories)\n    enc.fit(X)\n", "def test_one_hot_encoder_get_feature_names_unsorted_categories():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OneHotEncoder(categories=[['def', 'abc'], [2, 1], [55]])\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert isinstance(feature_names, np.ndarray)\n    assert_array_equal(['x0_def', 'x0_abc', 'x1_2', 'x1_1', 'x2_55'],\n                       feature_names)\n", "def test_one_hot_encoder_drop_first_multiple_features():\n    X = [['abc', 12, 2, 55],\n         ['def', 12, 1, 55],\n         ['def', 12, 3, 56]]\n    enc = OneHotEncoder(drop='first')\n    trans = enc.fit_transform(X).toarray()\n    exp = [[0, 0, 1, 0, 0],\n           [0, 0, 0, 1, 0],\n           [0, 1, 0, 0, 1]]\n    assert_array_equal(trans, exp)\n    assert_array_equal(np.array(X, dtype=object),\n                       enc.inverse_transform(trans))\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder()\n    enc.set_params(categories=[['a', 'b', 'c']])\n    assert enc.get_params()['categories'] == [['a', 'b', 'c']]\n    assert_array_equal(enc.fit_transform(X), [[0.], [1.]])\n", "def test_ordinal_encoder_set_params():\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder()\n    enc.fit(X)\n\n    # set params on already fitted object\n    enc.set_params(categories=[[0, 1, 2, 3]])\n    assert enc.get_params()['categories'] == [[0, 1, 2, 3]]\n    assert enc.fit_transform(X).shape == (2, 1)\n"], "sample_827": ["def test_csc_median_axis_0_empty_matrix():\n    # Test that csc_median_axis_0 raises an error when the matrix is empty.\n    X = sp.csc_matrix((0, 5))\n    with pytest.raises(ValueError):\n        csc_median_axis_0(X)\n", "def test_csc_median_axis_0_empty_rows():\n    # Test csc_median_axis_0 when some rows are empty.\n    X = sp.csc_matrix(np.array([[0, 2], [1, 0], [0, 0]]))\n    median = csc_median_axis_0(X)\n    assert_array_equal(median, np.array([0.5, 1]))\n", "def test_csc_median_axis_0_empty_matrix():\n    # Test that csc_median_axis_0 raises an error for an empty matrix.\n    X = sp.csc_matrix((0, 10))\n    assert_raises(ValueError, csc_median_axis_0, X)\n", "def test_csr_csc_median_axis_0_empty_rows():\n    # Test median computation on CSR and CSC matrices with empty rows.\n    X = sp.csr_matrix(np.array([[1, 2], [0, 0], [3, 4]]))\n    assert_array_equal(csc_median_axis_0(X.tocsc()), np.array([1., 2.]))\n\n    X = sp.csc_matrix(np.array([[1, 0, 2], [0, 0, 0], [3, 0, 4]]))\n    assert_array_equal(csc_median_axis_0(X), np.array([2., np.nan, 3.]))\n", "def test_csc_median_axis_0_empty():\n    # Test csc_median_axis_0 with an empty matrix.\n    X = sp.csc_matrix((5, 4))\n    median = csc_median_axis_0(X)\n    assert_array_equal(median, np.full(4, np.nan))\n\n    # Test csc_median_axis_0 with a matrix containing only zeros.\n    X = sp.csc_matrix(np.zeros((5, 4)))\n    median = csc_median_axis_0(X)\n    assert_array_equal(median, np.zeros(4))\n\n    # Test csc_median_axis_0 with a matrix containing only one non-zero element.\n    X = sp.csc_matrix(np.array([[0, 1, 0], [0, 0, 0]]))\n    median = csc_median_axis_0(X)\n    assert_array_equal(median, np.array([np.nan, 1., np.nan]))\n"], "sample_828": ["def test_pairwise_distances_chunked_reduce_func_tuple():\n    # Test that reduce_func can return a tuple of arrays\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((400, 4))\n        return (D_chunk[:, :100], D_chunk[:, 100:200])\n    S_chunks = pairwise_distances_chunked(X, None, reduce_func=reduce_func,\n                                          working_memory=2 ** -16)\n    S1, S2 = zip(*S_chunks)\n    S1 = np.vstack(S1)\n    S2 = np.vstack(S2)\n    S = pairwise_distances(X)\n    assert_array_almost_equal(S1, S[:, :100])\n    assert_array_almost_equal(S2, S[:, 100:200])\n", "def test_pairwise_distances_argmin_min_reduce_func():\n    # Check that argmin and min are correctly computed with a reduce function.\n    X = [[0], [1]]\n    Y = [[-2], [3]]\n\n        return np.argmin(D_chunk, axis=1), np.min(D_chunk, axis=1)\n\n    idx, vals = pairwise_distances_argmin_min(X, Y, metric=\"euclidean\",\n                                              reduce_func=reduce_func)\n    assert_array_almost_equal(idx, [0, 1])\n    assert_array_almost_equal(vals, [2, 2])\n\n    # Test with sparse matrix\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    idx, vals = pairwise_distances_argmin_min(X_sparse, Y_sparse,\n                                              metric=\"euclidean\",\n                                              reduce_func=reduce_func)\n    assert_array_almost_equal(idx, [0, 1])\n    assert_array_almost_equal(vals, [2, 2])\n", "def test_pairwise_distances_argmin_min_axis():\n    # Check pairwise minimum distances computation for any metric along axis=0\n    X = [[0], [1]]\n    Y = [[-2], [3]]\n\n    expected_idx = [0, 1]\n    expected_vals = [2, 2]\n\n    idx, vals = pairwise_distances_argmin_min(Y, X, axis=0, metric=\"euclidean\")\n    assert_array_almost_equal(idx, expected_idx)\n    assert_array_almost_equal(vals, expected_vals)\n", "def test_pairwise_distances_chunked_reduce_memory():\n    # Test the pairwise_distance helper function.\n    rng = np.random.RandomState(0)\n    # Euclidean distance should be equivalent to calling the function.\n    X = rng.random_sample((400, 4))\n    S = pairwise_distances(X, metric=\"euclidean\")\n    with config_context(working_memory=1):\n        S_chunks = pairwise_distances_chunked(X, None, reduce_func=np.sum,\n                                              working_memory=None)\n    assert isinstance(S_chunks, GeneratorType)\n    total = sum(S_chunks)\n    assert_allclose(total, np.sum(S))\n", "def test_check_pairwise_arrays_dtype():\n    # Ensures that the dtype parameter is handled correctly\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype='float64')\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB, dtype=None)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, None, dtype='float64')\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n"], "sample_829": ["def test_incremental_pca_n_components_mismatch():\n    # Test that calling partial_fit with a different n_components raises an error.\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2)\n    ipca.partial_fit(X)\n\n    # Try to change n_components and call partial_fit again\n    ipca.n_components = 3\n    assert_raises(ValueError, ipca.partial_fit, X)\n", "def test_incremental_pca_get_covariance():\n    # Test that get_covariance returns the covariance of the original data.\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    n_features = 3\n    X = rng.randn(n_samples, n_features) + 5 * rng.rand(1, n_features)\n    ipca = IncrementalPCA(n_components=3, batch_size=25).fit(X)\n    cov = ipca.get_covariance()\n    np.testing.assert_allclose(np.cov(X.T), cov, atol=1e-13)\n", "def test_incremental_pca_sparse_batch_size():\n    # Test that IncrementalPCA works with sparse data and different batch sizes.\n    X = sparse.csr_matrix(iris.data)\n    n_samples, n_features = X.shape\n\n    for batch_size in [None, 10, 50]:\n        ipca = IncrementalPCA(n_components=2, batch_size=batch_size)\n        Xt = ipca.fit_transform(X)\n\n        assert Xt.shape == (n_samples, 2)\n\n        # Check that the explained variance ratio is valid\n        assert np.all(ipca.explained_variance_ratio_ >= 0)\n        assert np.all(ipca.explained_variance_ratio_ <= 1)\n", "def test_incremental_pca_no_copy():\n    # Test that fit and partial_fit work correctly when copy=False.\n    X = iris.data\n    batch_size = X.shape[0] // 3\n    ipca = IncrementalPCA(n_components=2, batch_size=batch_size, copy=False)\n    pca = PCA(n_components=2)\n\n    X_transformed = ipca.fit_transform(X)\n    pca.fit_transform(X)\n\n    assert X_transformed.shape == (X.shape[0], 2)\n    np.testing.assert_allclose(ipca.explained_variance_ratio_.sum(),\n                               pca.explained_variance_ratio_.sum(), rtol=1e-3)\n\n    for n_components in [1, 2, X.shape[1]]:\n        ipca = IncrementalPCA(n_components, batch_size=batch_size, copy=False)\n        ipca.fit(X)\n        cov = ipca.get_covariance()\n        precision = ipca.get_precision()\n        np.testing.assert_allclose(np.dot(cov, precision),\n                                   np.eye(X.shape[1]), atol=1e-13)\n\n    # Check that the input data was modified\n    assert not np.allclose(X, iris.data)\n", "def test_incremental_pca_n_components_mismatch():\n    # Test that an error is raised when n_components does not match the\n    # number of components in the components_ attribute.\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2)\n    ipca.fit(X)\n\n    # Change n_components to a different value\n    ipca.n_components = 3\n\n    # Check that an error is raised when partial_fit is called\n    with pytest.raises(ValueError):\n        ipca.partial_fit(X)\n"], "sample_830": ["def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n"], "sample_831": ["def test_plot_tree_labels(pyplot):\n    # mostly smoke tests\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test label options\n    plot_tree(clf, label='root')\n    plot_tree(clf, label='none')\n\n    # Test plot options\n    plot_tree(clf, filled=True, impurity=False, proportion=True,\n              rounded=True, precision=1)\n    plot_tree(clf, feature_names=[\"feature0\", \"feature1\"], class_names=True)\n    plot_tree(clf, max_depth=0)\n", "def test_plot_tree_friedman_mse(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for criterion = friedman_mse\n    clf = DecisionTreeRegressor(max_depth=3,\n                                min_samples_split=2,\n                                criterion=\"friedman_mse\",\n                                random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\nfriedman_mse = 1.0\\n\"\n                                   \"samples = 6\\nvalue = [3.0]\")\n    assert nodes[1].get_text() == \"friedman_mse = 0.0\\nsamples = 3\\nvalue = [3.0]\"\n    assert nodes[2].get_text() == \"friedman_mse = 0.0\\nsamples = 3\\nvalue = [1.5]\"\n", "def test_plot_tree_max_depth(pyplot):\n    # mostly smoke tests\n    # Check correctness of plot_tree for different max_depth values\n    clf = DecisionTreeClassifier(max_depth=5,\n                                 min_samples_split=2,\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=3)\n    assert len(nodes) == 7\n\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=None)\n    assert len(nodes) == 15\n\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=0)\n    assert len(nodes) == 1\n", "def test_plot_tree_multiclass(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for multiclass classification\n    X_multiclass = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1],\n                    [-1, 1], [0, 0]]\n    y_multiclass = [0, 0, 0, 1, 1, 1, 2, 2]\n    clf = DecisionTreeClassifier(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"gini\",\n                                 random_state=2)\n    clf.fit(X_multiclass, y_multiclass)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 5\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.857\\n\"\n                                   \"samples = 8\\nvalue = [3, 3, 2]\")\n    assert nodes[1].get_text() == (\"sepal_width <= -1.0\\ngini = 0.0\\n\"\n                                   \"samples = 3\\nvalue = [3, 0, 0]\")\n    assert nodes[2].get_text() == (\"gini = 0.667\\nsamples = 5\\n\"\n                                   \"value = [0, 3, 2]\")\n    assert nodes[3].get_text() == (\"gini = 0.0\\nsamples = 3\\nvalue = [0, 3, 0]\")\n    assert nodes[4].get_text() == (\"gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 2]\")\n", "def test_plot_tree_mse(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for criterion = mse\n    clf = DecisionTreeRegressor(max_depth=3,\n                                 min_samples_split=2,\n                                 criterion=\"mse\",\n                                 random_state=2)\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = ['first feat', 'sepal_width']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == (\"first feat <= 0.0\\nmse = 1.0\\n\"\n                                   \"samples = 6\\nvalue = [3, 3]\")\n    assert nodes[1].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"mse = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n"], "sample_832": ["def test_bayesian_ridge_predict_std():\n    # Test BayesianRidge prediction with standard deviation\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(compute_score=True)\n    clf.fit(X, Y)\n\n    y_mean, y_std = clf.predict(X, return_std=True)\n\n    assert_array_almost_equal(y_mean, Y, 2)\n    assert_array_less(y_std, 1.0)\n", "def test_bayesian_ridge_predict_std():\n    # Test BayesianRidge.predict with return_std option\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(compute_score=True)\n    clf.fit(X, Y)\n\n    y_mean, y_std = clf.predict(X, return_std=True)\n    assert_array_almost_equal(y_mean, Y, 2)\n    assert_array_less(y_std, 1.0)  # weak bound but should be enough\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with and without intercept\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([3, 4, 5])\n\n    clf_with_intercept = BayesianRidge(fit_intercept=True)\n    clf_without_intercept = BayesianRidge(fit_intercept=False)\n\n    clf_with_intercept.fit(X, y)\n    clf_without_intercept.fit(X, y)\n\n    assert clf_with_intercept.intercept_ != 0\n    assert clf_without_intercept.intercept_ == 0\n\n    y_pred_with_intercept = clf_with_intercept.predict(X)\n    y_pred_without_intercept = clf_without_intercept.predict(X)\n\n    assert_array_almost_equal(y_pred_with_intercept, y, decimal=1)\n    assert_array_almost_equal(y_pred_without_intercept, y, decimal=1)\n", "def test_bayesian_ridge_lambda_init_alpha_init():\n    # Test BayesianRidge with initial values (alpha_init, lambda_init)\n    X = np.vander(np.linspace(0, 4, 5), 4)\n    y = np.array([0., 1., 0., -1., 0.])    # y = (x^3 - 6x^2 + 8x) / 3\n\n    # In this case, starting from the default initial values will increase\n    # the bias of the fitted curve. So, lambda_init should be small.\n    reg = BayesianRidge(alpha_init=1e-3, lambda_init=1.)\n    # Check the R2 score nearly equals to one.\n    r2 = reg.fit(X, y).score(X, y)\n    assert_almost_equal(r2, 1.)\n", "def test_bayesian_ridge_intercept():\n    # Test BayesianRidge with and without intercept.\n    X = np.array([[1], [2], [6], [8], [10]])\n    Y = np.array([1, 2, 6, 8, 10])\n    clf = BayesianRidge(fit_intercept=True)\n    clf.fit(X, Y)\n    assert hasattr(clf, 'intercept_')\n\n    clf = BayesianRidge(fit_intercept=False)\n    clf.fit(X, Y)\n    assert not hasattr(clf, 'intercept_')\n"], "sample_833": ["def test_logistic_regression_auto_solver():\n    # check solver='auto' => 'liblinear' iff n_samples <= 1000 and\n    # (n_classes = 2 or multi_class='ovr'), otherwise 'lbfgs'\n\n        return LogisticRegression(**kw).fit(X, y)\n\n    X = iris.data[::10]\n    y_bin = iris.target[::10] == 0\n    y_multi = iris.target[::10]\n\n    est_auto_bin = fit(X, y_bin, solver='auto')\n    est_liblinear_bin = fit(X, y_bin, solver='liblinear')\n    assert np.allclose(est_auto_bin.coef_, est_liblinear_bin.coef_)\n\n    est_auto_multi_ovr = fit(X, y_multi, solver='auto', multi_class='ovr')\n    est_liblinear_multi_ovr = fit(X, y_multi, solver='liblinear',\n                                  multi_class='ovr')\n    assert np.allclose(est_auto_multi_ovr.coef_, est_liblinear_multi_ovr.coef_)\n\n    est_auto_multi_multi = fit(X, y_multi, solver='auto',\n                               multi_class='multinomial')\n    est_lbfgs_multi_multi = fit(X, y_multi, solver='lbfgs',\n                                multi_class='multinomial')\n    assert np.allclose(est_auto_multi_multi.coef_, est_lbfgs_multi_multi.coef_)\n", "def test_logistic_regression_path_warns_convergence():\n    # Test that a warning is raised when logistic_regression_path fails to converge.\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    Cs = [1.]\n    max_iter = 1\n\n    assert_warns(ConvergenceWarning, _logistic_regression_path, X, y, Cs=Cs,\n                 fit_intercept=True, solver='sag', tol=1e-10, max_iter=max_iter)\n", "def test_logistic_regression_path_multiclass_iris():\n    # Test that logistic_regression_path gives the same result as\n    # LogisticRegression with multi_class='multinomial' for the iris dataset.\n    X, y = iris.data, iris.target\n    Cs = np.logspace(-4, 4, 10)\n    coefs, _, _ = _logistic_regression_path(\n        X, y, Cs=Cs, solver='saga', multi_class='multinomial',\n        penalty='l2', tol=1e-5, random_state=0, max_iter=1000)\n\n    for i, C in enumerate(Cs):\n        lr = LogisticRegression(C=C, solver='saga', multi_class='multinomial',\n                                penalty='l2', tol=1e-5, random_state=0,\n                                max_iter=1000)\n        lr.fit(X, y)\n        assert_array_almost_equal(coefs[i], lr.coef_, decimal=3)\n", "def test_logistic_regression_multinomial_with_sparse_X():\n    # Test logistic regression with multinomial and sparse X\n\n    X, y = make_classification(n_samples=200, n_features=20, n_informative=10,\n                               n_classes=3, random_state=0)\n    X[X < 1] = 0\n    X_sparse = sparse.csr_matrix(X)\n\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf_dense = LogisticRegression(solver=solver, multi_class='multinomial',\n                                       max_iter=1000, tol=1e-6)\n        clf_sparse = LogisticRegression(solver=solver, multi_class='multinomial',\n                                        max_iter=1000, tol=1e-6)\n\n        clf_dense.fit(X, y)\n        clf_sparse.fit(X_sparse, y)\n\n        assert_array_almost_equal(clf_dense.coef_, clf_sparse.coef_)\n", "def test_logistic_regression_n_jobs():\n    # Test that n_jobs is handled correctly for LogisticRegression and \n    # LogisticRegressionCV\n\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n\n    # Test that an error is raised when n_jobs is not supported\n    msg = \"'n_jobs' > 1 does not have any effect when 'solver' is set to \" \\\n          \"'liblinear'. Got 'n_jobs' = 2.\"\n    with pytest.raises(UserWarning, match=msg):\n        LogisticRegression(solver='liblinear', n_jobs=2).fit(X, y)\n\n    # Test that n_jobs is used correctly for other solvers\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, n_jobs=2)\n        clf.fit(X, y)\n        assert hasattr(clf, 'coef_')\n\n    # Test that n_jobs is used correctly for LogisticRegressionCV\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf = LogisticRegressionCV(solver=solver, n_jobs=2, cv=2)\n        clf.fit(X, y)\n        assert hasattr(clf, 'coef_')\n"], "sample_834": ["def test_components_shape():\n    \"\"\"Test that the components_ attribute has the expected shape.\"\"\"\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(n_components=2)\n    nca.fit(X, y)\n\n    assert nca.components_.shape == (2, X.shape[1])\n", "def test_components_shape():\n    \"\"\"Test that the components_ attribute has the expected shape.\"\"\"\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(n_components=2)\n    nca.fit(X, y)\n\n    assert nca.components_.shape == (2, X.shape[1])\n", "def test_components_shape():\n    \"\"\"Test that the learned components have the expected shape.\"\"\"\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(n_components=None)\n    nca.fit(X, y)\n    assert nca.components_.shape == (X.shape[1], X.shape[1])\n\n    nca = NeighborhoodComponentsAnalysis(n_components=X.shape[1] - 1)\n    nca.fit(X, y)\n    assert nca.components_.shape == (X.shape[1] - 1, X.shape[1])\n", "def test_components_init():\n    # Test that the components are initialized correctly.\n    rng = np.random.RandomState(42)\n    X, y = make_blobs(n_samples=30, centers=6, n_features=5, random_state=0)\n\n    # Initialize with PCA\n    nca_pca = NeighborhoodComponentsAnalysis(init='pca', n_components=2)\n    nca_pca.fit(X, y)\n    assert nca_pca.components_.shape == (2, X.shape[1])\n\n    # Initialize with LDA\n    nca_lda = NeighborhoodComponentsAnalysis(init='lda', n_components=2)\n    nca_lda.fit(X, y)\n    assert nca_lda.components_.shape == (2, X.shape[1])\n\n    # Initialize with identity\n    nca_identity = NeighborhoodComponentsAnalysis(init='identity')\n    nca_identity.fit(X, y)\n    assert nca_identity.components_.shape == (X.shape[1], X.shape[1])\n\n    # Initialize with random\n    nca_random = NeighborhoodComponentsAnalysis(init='random', random_state=rng)\n    nca_random.fit(X, y)\n    assert nca_random.components_.shape == (X.shape[1], X.shape[1])\n\n    # Initialize with numpy array\n    init = rng.rand(X.shape[1], X.shape[1])\n    nca_array = NeighborhoodComponentsAnalysis(init=init)\n    nca_array.fit(X, y)\n    assert_array_equal(nca_array.components_, init)\n", "def test_labels_validation():\n    # Test that invalid labels raise value error\n    X = np.arange(12).reshape(4, 3)\n    y = [1, 2, 3, 4]\n    nca = NeighborhoodComponentsAnalysis()\n\n    # Fail if labels are not an array-like\n    assert_raises(TypeError, nca.fit, X, \"string\")\n\n    # Fail if labels have wrong shape\n    assert_raise_message(ValueError,\n                         \"y should be a 1d array, got an array of shape {} \"\n                         \"instead.\".format((2, 2)),\n                         nca.fit, X, np.array([[1, 2], [3, 4]]))\n\n    # Fail if not all labels are integers\n    assert_raise_message(ValueError,\n                         \"Labels in y should be integers\",\n                         nca.fit, X, [1, 'b', 3, 4])\n\n    # Fail if there is less than 2 different labels\n    assert_raise_message(ValueError,\n                         \"The number of classes has to be greater than one; \"\n                         \"got 1 class\",\n                         nca.fit, X, [1, 1, 1, 1])\n"], "sample_835": ["def test_feature_importances_(self):\n    \"\"\"\n    Check feature importances calculation.\n    \"\"\"\n\n    X, y = datasets.make_classification(n_samples=2000,\n                                        n_features=10,\n                                        n_informative=3,\n                                        n_redundant=3,\n                                        n_repeated=0,\n                                        shuffle=False,\n                                        random_state=1)\n\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg)\n        clf.fit(X, y)\n\n        # check feature importances shape\n        assert clf.feature_importances_.shape == (X.shape[1],)\n\n        # check values are non-negative and sum to 1\n        assert np.all(clf.feature_importances_ >= 0)\n        assert np.isclose(np.sum(clf.feature_importances_), 1)\n\n        # check values are the same as base_estimator's if one estimator\n        clf.n_estimators = 1\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.feature_importances_,\n                                  clf.base_estimator_.feature_importances_)\n", "def test_adaboost_regressor_with_loss():\n    \"\"\"\n    Test AdaBoostRegressor with different loss functions.\n    \"\"\"\n    X, y = datasets.make_regression(n_samples=100, n_features=5, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    for loss in ['linear', 'square', 'exponential']:\n        reg = AdaBoostRegressor(loss=loss, random_state=1)\n        reg.fit(X_train, y_train)\n        assert reg.score(X_test, y_test) > 0.5\n", "def test_base_estimator_is_not_fitted():\n    # Test that the base estimator is not fitted before calling `fit`.\n    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n    assert not hasattr(clf.base_estimator, 'tree_')\n    clf.fit(X, y_class)\n    assert hasattr(clf.base_estimator, 'tree_')\n\n    reg = AdaBoostRegressor(base_estimator=DecisionTreeRegressor())\n    assert not hasattr(reg.base_estimator, 'tree_')\n    reg.fit(X, y_regr)\n    assert hasattr(reg.base_estimator, 'tree_')\n", "def test_adaboost_regressor_with_constant_target():\n    \"\"\"\n    Check that AdaBoostRegressor works correctly when the target variable is constant.\n    \"\"\"\n    X = np.random.randn(50, 5)\n    y = np.ones(50)\n\n    boost = AdaBoostRegressor(n_estimators=10)\n    boost.fit(X, y)\n    assert_array_almost_equal(boost.predict(X), y)\n", "def test_base_estimator_has_fit_parameter():\n    # Test that an error is raised if the base estimator does not have a fit \n    # method with a sample_weight parameter.\n\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0])\n\n    boost = AdaBoostClassifier(DummyEstimator())\n    with pytest.raises(ValueError):\n        boost.fit(X, y_class)\n\n    boost = AdaBoostRegressor(DummyEstimator())\n    with pytest.raises(ValueError):\n        boost.fit(X, y_regr)\n"], "sample_836": ["def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 0], [1, 0, 1]])\n    confidences = np.array([[0.2, 0.8, 0.4], [0.7, 0.3, 0.9]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision.shape == (2, 3)\n\n    # Check that the confidences are monotonically transformed\n    for i in range(2):\n        for j in range(3):\n            if predictions[i, j] == 1:\n                assert decision[i, j] > 0\n            else:\n                assert decision[i, j] < 0\n\n    # Check that the votes are added to the confidences\n    for i in range(2):\n        for j in range(3):\n            if predictions[i, j] == 1:\n                assert decision[i, j] > confidences[i, j]\n            else:\n                assert decision[i, j] < confidences[i, j]\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 1], [1, 0, 1]])\n    confidences = np.array([[0.3, 0.7, 0.9], [0.8, 0.2, 0.6]])\n    n_classes = 4\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision.shape == (2, 4)\n\n    # Check that the predicted class has the highest value\n    assert np.argmax(decision, axis=1) == np.array([2, 0])\n\n    # Check that the sum of confidences for each sample is equal to the number of classes\n    assert np.sum(decision, axis=1) == n_classes\n", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with various inputs\n    predictions = np.array([[0, 1, 1], [1, 0, 1]])\n    confidences = np.array([[0.2, 0.8, 0.7], [0.9, 0.1, 0.6]])\n\n    n_classes = 3\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision.shape == (2, 3)\n\n    # Test with more classes than predictions\n    n_classes = 5\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision.shape == (2, 5)\n\n    # Test with float predictions and confidences\n    predictions = np.array([[0.0, 1.0, 1.0], [1.0, 0.0, 1.0]])\n    confidences = np.array([[0.2, 0.8, 0.7], [0.9, 0.1, 0.6]])\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision.shape == (2, 5)\n\n    # Test with ties in votes\n    predictions = np.array([[0, 1, 1], [1, 1, 0]])\n    confidences = np.array([[0.2, 0.8, 0.7], [0.9, 0.8, 0.1]])\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision.shape == (2, 3)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 0], [1, 0, 1]])\n    confidences = np.array([[0.3, 0.7, 0.4], [0.8, 0.2, 0.6]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_decision = np.array([[0.06666667, 0.63333333, 0.3],\n                                  [0.46666667, 0.13333333, 0.4]])\n\n    assert_allclose(decision, expected_decision)\n", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with a simple example\n    predictions = np.array([[0, 1, 1], [1, 0, 1]])\n    confidences = np.array([[0.2, 0.8, 0.7], [0.9, 0.1, 0.8]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_decision = np.array([[0.06666667, 0.46666667, 0.46666667],\n                                  [0.63333333, 0.03333333, 0.33333333]])\n\n    assert_array_almost_equal(decision, expected_decision)\n"], "sample_837": ["def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n"], "sample_838": ["def test_column_transformer_sparse_remainder_transformer_multiple_columns():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.8)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    # SparseMatrixTrans creates 3 features for each column. There is\n    # one column in ``transformers``, thus:\n    assert X_trans.shape == (3, 3 + 2)\n\n    exp_array = np.hstack(\n        (X_array[:, 0].reshape(-1, 1), np.eye(3)[:, :2]))\n    assert_array_equal(X_trans.toarray(), exp_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_feature_names_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n\n    assert ct.get_feature_names() == ['trans__x0', 'remainder__x1']\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0., 1., 2.], [2., 4., 6.]]).T\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=StandardScaler())\n    ct.fit(X_array)\n    assert ct.get_feature_names() == ['trans__x0', 'remainder__x1', 'remainder__x2']\n", "def test_column_transformer_invalid_n_jobs():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans', Trans(), [0])], n_jobs='invalid')\n    assert_raise_message(\n        ValueError,\n        \"'n_jobs' should be an integer or None\",\n        ct.fit_transform, X_array\n    )\n\n    ct = ColumnTransformer([('trans', Trans(), [0])], n_jobs=-2)\n    assert_raise_message(\n        ValueError,\n        \"'n_jobs' should be an integer between 1 and the number of CPUs or -1\",\n        ct.fit_transform, X_array\n    )\n", "def test_column_transformer_get_feature_names_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans', Trans(), [0])],\n                           remainder=StandardScaler())\n\n    with pytest.raises(NotImplementedError):\n        ct.fit(X_array).get_feature_names()\n"], "sample_839": ["def test_tfidfvectorizer_dtype():\n    X = [\"This is a sample document.\", \"Another example document.\"]\n    vectorizer = TfidfVectorizer(dtype=np.float32)\n    X_tfidf = vectorizer.fit_transform(X)\n    assert X_tfidf.dtype == np.float32\n    vectorizer = TfidfVectorizer(dtype=np.float64)\n    X_tfidf = vectorizer.fit_transform(X)\n    assert X_tfidf.dtype == np.float64\n", "def test_vectorizer_empty_input():\n    # Test that passing empty input to a vectorizer raises an error\n    vectorizer = CountVectorizer()\n    with pytest.raises(ValueError, match=\"Iterable over raw text documents expected, string object received\"):\n        vectorizer.fit_transform(\"\")\n    with pytest.raises(ValueError, match=\"Iterable over raw text documents expected, string object received\"):\n        vectorizer.fit(\"\")\n    with pytest.raises(ValueError, match=\"Iterable over raw text documents expected, string object received\"):\n        vectorizer.transform(\"\")\n", "def test_vectorizer_build_analyzer():\n    # Check the behavior and sanity of build_analyzer method\n    cv = CountVectorizer()\n    analyzer = cv.build_analyzer()\n    text = \"This is a test, really.\\n\\n I met Harry yesterday.\"\n    expected = ['this', 'is', 'test', 'really', 'met', 'harry',\n                'yesterday']\n    assert analyzer(text) == expected\n\n    # Check that the preprocessor is called\n        return doc.upper()\n\n    cv.set_params(preprocessor=preprocessor)\n    analyzer = cv.build_analyzer()\n    expected = ['THIS', 'IS', 'TEST', 'REALLY', 'MET', 'HARRY',\n                'YESTERDAY']\n    assert analyzer(text) == expected\n\n    # Check that the tokenizer is called\n        return doc.split()\n\n    cv.set_params(tokenizer=tokenizer)\n    analyzer = cv.build_analyzer()\n    expected = ['This', 'is', 'a', 'test,', 'really.', 'I', 'met',\n                'Harry', 'yesterday.']\n    assert analyzer(text) == expected\n\n    # Check n-grams\n    cv.set_params(ngram_range=(1, 2))\n    analyzer = cv.build_analyzer()\n    expected = ['This', 'is', 'a', 'test,', 'really.', 'I', 'met',\n                'Harry', 'yesterday.', 'This is', 'is a', 'a test,',\n                'test, really.', 'really. I', 'I met', 'met Harry',\n                'Harry yesterday.']\n    assert analyzer(text) == expected\n\n    # Check stop words\n    cv.set_params(stop_words=['is', 'a'])\n    analyzer = cv.build_analyzer()\n    expected = ['This', 'test,', 'really.', 'I', 'met', 'Harry',\n                'yesterday.', 'This test,', 'test, really.', 'really. I',\n                'I met', 'met Harry', 'Harry yesterday.']\n    assert analyzer(text) == expected\n\n    # Check that the analyzer is used if provided\n        return ['ANALYZED']\n\n    cv.set_params(analyzer=analyzer)\n    new_analyzer = cv.build_analyzer()\n    assert new_analyzer(text) == ['ANALYZED']\n", "def test_countvectorizer_dtype():\n    # Test that the dtype parameter is correctly propagated to the output.\n    X = [\"This is a sample document.\", \"Another document for testing.\"]\n    vectorizer = CountVectorizer(dtype=np.float32)\n    X_trans = vectorizer.fit_transform(X)\n    assert X_trans.dtype == np.float32\n\n    # Test that the dtype parameter is correctly set when using the\n    # fit_transform method.\n    X_trans = CountVectorizer(dtype=np.float64).fit_transform(X)\n    assert X_trans.dtype == np.float64\n", "def test_countvectorizer_validate_vocabulary():\n    vocab = {\"pizza\": 0, \"beer\": 1}\n    cv = CountVectorizer(vocabulary=vocab)\n    invalid_vocab = {\"pizza\": 0, \"burger\": 2}\n    assert_raises(ValueError, cv._validate_vocabulary, invalid_vocab)\n"], "sample_840": ["def test_pls_with_zero_columns():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # Set a column of X and Y to zero\n    X[:, 0] = 0\n    Y[:, 0] = 0\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression()]:\n        clf.fit(X, Y)\n\n        # Check that the x_weights_ and y_weights_ are not NaN\n        assert not np.any(np.isnan(clf.x_weights_))\n        assert not np.any(np.isnan(clf.y_weights_))\n\n        # Check that the x_scores_ and y_scores_ are not NaN\n        assert not np.any(np.isnan(clf.x_scores_))\n        assert not np.any(np.isnan(clf.y_scores_))\n\n        # Check that the x_loadings_ and y_loadings_ are not NaN\n        assert not np.any(np.isnan(clf.x_loadings_))\n        assert not np.any(np.isnan(clf.y_loadings_))\n", "def test_pls_predict_shape():\n    # Test shape of prediction output\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        y_pred = clf.predict(X)\n        assert y_pred.shape == Y.shape\n        if isinstance(clf, pls_.PLSSVD):\n            continue\n        y_pred = clf.predict(X[:1])\n        assert y_pred.shape == (1, Y.shape[1])\n", "def test_pls_transform():\n    # Test that transform method works correctly\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        X_transformed = clf.transform(X)\n        assert_array_almost_equal(X_transformed, clf.x_scores_)\n        X_transformed, Y_transformed = clf.transform(X, Y)\n        assert_array_almost_equal(X_transformed, clf.x_scores_)\n        assert_array_almost_equal(Y_transformed, clf.y_scores_)\n", "def test_pls_input_validation():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # invalid n_components\n    pls = pls_.PLSCanonical(n_components='a')\n    assert_raise_message(ValueError, \"Invalid number of components\",\n                         pls.fit, X, Y)\n\n    pls = pls_.PLSRegression(n_components=-1)\n    assert_raise_message(ValueError, \"Invalid number of components\",\n                         pls.fit, X, Y)\n\n    # invalid scale parameter\n    pls = pls_.PLSCanonical(scale=None)\n    assert_raise_message(ValueError, \"Scale must be True or False\",\n                         pls.fit, X, Y)\n\n    # invalid algorithm\n    pls = pls_.PLSCanonical(algorithm='foo')\n    assert_raise_message(ValueError, \"Got algorithm foo when only 'svd' \"\n                         \"and 'nipals' are known\", pls.fit, X, Y)\n\n    # invalid deflation mode\n    pls = pls_.PLSCanonical(deflation_mode='foo')\n    assert_raise_message(ValueError, \"The deflation mode is unknown\",\n                         pls.fit, X, Y)\n\n    # invalid mode\n    pls = pls_.PLSCanonical(mode='foo')\n    assert_raise_message(ValueError, \"Got mode foo when only 'A' and 'B' \"\n                         \"are known\", pls.fit, X, Y)\n", "def test_pls_transformer():\n    # check that pls.transform returns x_scores or (x_scores, y_scores)\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        X_score = clf.transform(X)\n        X_score2, Y_score = clf.transform(X, Y)\n        assert_array_almost_equal(X_score, X_score2)\n        assert_array_almost_equal(Y_score, clf.transform(Y))\n"], "sample_841": ["def test_ridge_regression_dtype_stability_with_sample_weights():\n    random_state = np.random.RandomState(0)\n    n_samples, n_features = 6, 5\n    X = random_state.randn(n_samples, n_features)\n    coef = random_state.randn(n_features)\n    y = np.dot(X, coef) + 0.01 * random_state.randn(n_samples)\n    alpha = 1.0\n    sample_weight = random_state.rand(n_samples)\n\n    results = dict()\n    for current_dtype in (np.float32, np.float64):\n        results[current_dtype] = ridge_regression(\n            X.astype(current_dtype), y.astype(current_dtype),\n            alpha=alpha, solver='cholesky', sample_weight=sample_weight,\n            max_iter=500, tol=1e-10, return_n_iter=False,\n            return_intercept=False)\n\n    assert results[np.float32].dtype == np.float32\n    assert results[np.float64].dtype == np.float64\n    assert_allclose(results[np.float32], results[np.float64], atol=1e-5)\n", "def test_ridge_regression_sparse_input_float32():\n    # Test that ridge regression with sparse input and float32 dtype works.\n    X, y = _make_sparse_offset_regression(n_features=20, random_state=0)\n    X_csr = sp.csr_matrix(X).astype(np.float32)\n    y_float32 = y.astype(np.float32)\n\n    ridge = Ridge(alpha=1., solver='sparse_cg', fit_intercept=True)\n    ridge.fit(X_csr, y_float32)\n\n    assert np.allclose(ridge.coef_, np.array([2.32443394e-02, 8.12054426e-03,\n                                              9.73422819e-01, 6.39981525e-04,\n                                              -7.23566533e-02, -3.38432239e-02,\n                                              3.35996178e-01, 2.13584427e-01,\n                                              -5.39217936e-02, -4.98866186e-02,\n                                              3.39074149e-01, 4.59842505e-01,\n                                              -1.50242138e-01, 2.00485131e-01,\n                                              4.21564974e-01, 2.37731063e-01,\n                                              -2.79572252e-01, 3.14381809e-01,\n                                              2.68050318e-01, 1.13357641e-01]),\n                          atol=1e-3)\n    assert np.allclose(ridge.intercept_, np.array([30.18566434]), atol=1e-3)\n", "def test_ridgecv_scorer():\n    # Test RidgeCV with a custom scorer\n    X, y = make_regression(n_samples=10, n_features=5)\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n    ridge = RidgeCV(scorer=scorer)\n    ridge.fit(X, y)\n    assert hasattr(ridge, 'best_score_')\n", "def test_ridgecv_with_scoring():\n    # Test RidgeCV with a custom scoring function\n    X, y = make_regression(n_samples=100, n_features=10)\n    scoring = make_scorer(mean_squared_error, greater_is_better=False)\n    ridge_cv = RidgeCV(scoring=scoring)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'best_score_')\n    assert hasattr(ridge_cv, 'alpha_')\n", "def test_ridge_multitarget_dtype_match():\n    # Test that the `Ridge` estimator handles multitarget regression with\n    # different dtypes correctly.\n    rng = np.random.RandomState(0)\n    X_64 = rng.randn(10, 5)\n    y_64 = rng.randn(10, 3)\n    X_32 = X_64.astype(np.float32)\n    y_32 = y_64.astype(np.float32)\n\n    ridge_32 = Ridge()\n    ridge_32.fit(X_32, y_32)\n    assert ridge_32.coef_.dtype == X_32.dtype\n\n    ridge_64 = Ridge()\n    ridge_64.fit(X_64, y_64)\n    assert ridge_64.coef_.dtype == X_64.dtype\n\n    assert_allclose(ridge_32.coef_, ridge_64.coef_, rtol=1e-4, atol=5e-4)\n"], "sample_842": ["def test_kernel_str(kernel):\n    # Smoke-test for __str__ in kernels.\n\n    str(kernel)\n", "def test_clone_with_safe_false():\n    # Test that clone works correctly with safe=False.\n    kernel = RBF(length_scale=2.0)\n    kernel_cloned = clone(kernel, safe=False)\n\n    assert kernel == kernel_cloned\n    assert id(kernel) != id(kernel_cloned)\n\n    # Check that all constructor parameters are equal.\n    assert kernel.get_params() == kernel_cloned.get_params()\n\n    # Check that all hyperparameters are equal.\n    check_hyperparameters_equal(kernel, kernel_cloned)\n", "def test_clone_with_dynamic_params():\n    # Test that clone works with dynamic parameters.\n    kernel = RBF(length_scale=2.0)\n    kernel.length_scale_bounds = \"fixed\"\n    kernel_cloned = clone(kernel)\n    assert kernel_cloned.length_scale_bounds == \"fixed\"\n    assert id(kernel_cloned.length_scale_bounds) != id(kernel.length_scale_bounds)\n", "def test_kernel_str(kernel):\n    # Smoke-test for __str__ method in kernels.\n\n    str(kernel)\n", "def test_base_estimator_get_tags():\n    # Test that the _get_tags method of BaseEstimator returns a dict with all\n    # the default tags and any additional tags defined by the estimator.\n\n    class MyEstimator(BaseEstimator):\n            pass\n\n            return {'allow_nan': True}\n\n    estimator = MyEstimator()\n    tags = estimator._get_tags()\n    assert isinstance(tags, dict)\n    for key, value in _DEFAULT_TAGS.items():\n        assert key in tags\n        assert tags[key] == value\n    assert 'allow_nan' in tags\n    assert tags['allow_nan'] is True\n"], "sample_843": ["def test_kernel_hyperparameter_bounds():\n    # Test that kernel hyperparameter bounds are handled correctly.\n\n    # Test that setting bounds to \"fixed\" makes the hyperparameter fixed\n    kernel = RBF(length_scale=1.0, length_scale_bounds=\"fixed\")\n    assert kernel.hyperparameter_length_scale.fixed\n\n    # Test that setting bounds to a pair of floats sets the bounds correctly\n    kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-5, 1e5))\n    assert not kernel.hyperparameter_length_scale.fixed\n    assert kernel.hyperparameter_length_scale.bounds == (1e-5, 1e5)\n\n    # Test that an error is raised when trying to set invalid bounds\n    with pytest.raises(ValueError):\n        RBF(length_scale=1.0, length_scale_bounds=(1e5, 1e-5))\n", "def test_kernel_equals():\n    # Test that equality checks work correctly for kernels.\n\n    # Check equality of two identical kernels\n    kernel1 = RBF(length_scale=2.0)\n    kernel2 = RBF(length_scale=2.0)\n    assert kernel1 == kernel2\n\n    # Check inequality of two different kernels\n    kernel1 = RBF(length_scale=2.0)\n    kernel2 = RBF(length_scale=3.0)\n    assert kernel1 != kernel2\n\n    # Check equality of two kernels with different types but equal parameters\n    kernel1 = ConstantKernel(constant_value=2.0)\n    kernel2 = ConstantKernel(constant_value=2.0)\n    assert kernel1 == kernel2\n\n    # Check inequality of two kernels with different parameters\n    kernel1 = ConstantKernel(constant_value=2.0)\n    kernel2 = ConstantKernel(constant_value=3.0)\n    assert kernel1 != kernel2\n\n    # Check equality of composite kernels\n    kernel1 = RBF(length_scale=2.0) + WhiteKernel(noise_level=3.0)\n    kernel2 = RBF(length_scale=2.0) + WhiteKernel(noise_level=3.0)\n    assert kernel1 == kernel2\n\n    # Check inequality of composite kernels\n    kernel1 = RBF(length_scale=2.0) + WhiteKernel(noise_level=3.0)\n    kernel2 = RBF(length_scale=2.0) + WhiteKernel(noise_level=4.0)\n    assert kernel1 != kernel2\n", "def test_kernel_operator_inheritance():\n    # Check that KernelOperator instances inherit the properties of their\n    # operands. This is not explicitly tested elsewhere.\n    kernel1 = RBF()\n    kernel2 = Matern()\n\n    product = kernel1 * kernel2\n    sum_kernel = kernel1 + kernel2\n\n    assert isinstance(product, StationaryKernelMixin)\n    assert isinstance(sum_kernel, StationaryKernelMixin)\n\n    assert isinstance(product, NormalizedKernelMixin)\n    assert isinstance(sum_kernel, NormalizedKernelMixin)\n", "def test_kernel_equals():\n    # Test equality of kernels.\n    kernel1 = RBF(length_scale=2.0)\n    kernel2 = RBF(length_scale=2.0)\n    assert kernel1 == kernel2\n\n    kernel1 = RBF(length_scale=2.0)\n    kernel2 = RBF(length_scale=3.0)\n    assert not (kernel1 == kernel2)\n\n    kernel1 = 2.0 * RBF(length_scale=2.0)\n    kernel2 = 2.0 * RBF(length_scale=2.0)\n    assert kernel1 == kernel2\n\n    kernel1 = 2.0 * RBF(length_scale=2.0)\n    kernel2 = 3.0 * RBF(length_scale=2.0)\n    assert not (kernel1 == kernel2)\n\n    kernel1 = RBF(length_scale=2.0) + WhiteKernel(noise_level=3.0)\n    kernel2 = RBF(length_scale=2.0) + WhiteKernel(noise_level=3.0)\n    assert kernel1 == kernel2\n\n    kernel1 = RBF(length_scale=2.0) + WhiteKernel(noise_level=3.0)\n    kernel2 = RBF(length_scale=2.0) + WhiteKernel(noise_level=4.0)\n    assert not (kernel1 == kernel2)\n", "def test_clone_with_theta():\n    # Test that clone_with_theta clones a kernel and sets theta correctly.\n    kernel = RBF(length_scale=2.0)\n    new_theta = np.array([np.log(3.0)])\n    kernel_cloned = kernel.clone_with_theta(new_theta)\n    assert kernel_cloned.length_scale == 3.0\n    assert kernel.length_scale == 2.0\n"], "sample_844": ["def test_optics_fit_xi_no_predecessor_correction():\n    # testing with predecessor correction set to False\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 10\n    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)\n    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)\n    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3, C4, C5, np.array([[100, 100]]), C6))\n    expected_labels = np.r_[[2] * 10, [0] * 10, [1] * 10, [3] * 10, [1] * 10,\n                            -1, [4] * 10]\n    X, expected_labels = shuffle(X, expected_labels, random_state=rng)\n\n    clust = OPTICS(min_samples=3, min_cluster_size=2,\n                   max_eps=20, cluster_method='xi',\n                   xi=0.4, predecessor_correction=False).fit(X)\n    assert_array_equal(clust.labels_, expected_labels)\n", "def test_compute_optics_graph():\n    # Test that compute_optics_graph returns the correct types and shapes\n    X = np.random.rand(100, 2)\n    min_samples = 5\n    max_eps = np.inf\n    algorithm = 'ball_tree'\n    leaf_size = 30\n    metric = 'minkowski'\n    p = 2\n    n_jobs = None\n\n    (ordering, core_distances, reachability,\n     predecessor) = compute_optics_graph(X=X, min_samples=min_samples,\n                                         algorithm=algorithm,\n                                         leaf_size=leaf_size, metric=metric,\n                                         p=p, n_jobs=n_jobs, max_eps=max_eps)\n\n    assert ordering.shape == (X.shape[0],)\n    assert ordering.dtype.kind == 'i'\n    assert_array_equal(np.unique(ordering), np.arange(X.shape[0]))\n\n    assert core_distances.shape == (X.shape[0],)\n    assert core_distances.dtype.kind == 'f'\n\n    assert reachability.shape == (X.shape[0],)\n    assert reachability.dtype.kind == 'f'\n\n    assert predecessor.shape == (X.shape[0],)\n    assert predecessor.dtype.kind == 'i'\n", "def test_optics_xi_predecessor_correction():\n    # testing that predecessor correction works as expected.\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 20\n    C1 = [-5, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .2 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, 2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3, C4))\n\n    clust_corrected = OPTICS(cluster_method='xi', xi=.05,\n                             predecessor_correction=True).fit(X)\n    clust_not_corrected = OPTICS(cluster_method='xi', xi=.05,\n                                 predecessor_correction=False).fit(X)\n\n    # make sure there are differences between corrected and not corrected labels\n    assert not np.array_equal(clust_corrected.labels_, clust_not_corrected.labels_)\n", "def test_compute_optics_graph():\n    # Test the compute_optics_graph function\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    min_samples = 2\n    max_eps = np.inf\n    metric = 'minkowski'\n    p = 2\n    metric_params = None\n    algorithm = 'auto'\n    leaf_size = 30\n    n_jobs = None\n\n    (ordering, core_distances, reachability,\n     predecessor) = compute_optics_graph(X=X, min_samples=min_samples,\n                                         algorithm=algorithm,\n                                         leaf_size=leaf_size, metric=metric,\n                                         metric_params=metric_params, p=p,\n                                         n_jobs=n_jobs, max_eps=max_eps)\n\n    assert ordering.shape == (X.shape[0],)\n    assert core_distances.shape == (X.shape[0],)\n    assert reachability.shape == (X.shape[0],)\n    assert predecessor.shape == (X.shape[0],)\n", "def test_compute_optics_graph_min_samples():\n    # Test that compute_optics_graph handles different values of min_samples\n\n    X = np.array([[1, 2], [2, 2], [3, 2], [4, 2]])\n\n    # min_samples as an absolute number\n    optics = OPTICS(min_samples=3)\n    optics.fit(X)\n\n    # min_samples as a fraction of the total number of samples\n    optics_fraction = OPTICS(min_samples=0.75)\n    optics_fraction.fit(X)\n\n    assert_array_equal(optics.labels_, optics_fraction.labels_)\n"], "sample_845": ["def test_countvectorizer_vocabulary_update():\n    # Test that updating the vocabulary works as expected.\n    cv = CountVectorizer()\n    cv.fit([\"hello world\"])\n    vocab = cv.vocabulary_\n    assert \"hello\" in vocab\n    assert \"world\" in vocab\n\n    # Update the vocabulary with a new document\n    cv.fit([\"goodbye world\"])\n    vocab2 = cv.vocabulary_\n    assert \"hello\" not in vocab2\n    assert \"goodbye\" in vocab2\n    assert \"world\" in vocab2\n\n    # Check that the old vocabulary is still accessible\n    assert vocab == cv.vocabulary_\n", "def test_tfidf_transformer_dtype():\n    # Test that TfidfTransformer preserves the dtype of input data\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]), dtype=np.float32)\n    transformer = TfidfTransformer()\n    transformed_X = transformer.fit_transform(X)\n    assert transformed_X.dtype == np.float32\n\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]), dtype=np.float64)\n    transformer = TfidfTransformer()\n    transformed_X = transformer.fit_transform(X)\n    assert transformed_X.dtype == np.float64\n", "def test_tfidfvectorizer_dtype():\n    X = [\"This is a sample.\", \"Another example document.\"]\n    vectorizer = TfidfVectorizer(dtype=np.float32)\n    X_trans = vectorizer.fit_transform(X)\n    assert X_trans.dtype == np.float32\n\n    # Test that the dtype is propagated to the underlying transformer\n    assert vectorizer._tfidf.dtype == np.float32\n", "def test_tfidf_vectorizer_dtype():\n    # Test that TfidfVectorizer sets the correct dtype for the output matrix.\n    X = ['This is a sample document.', 'Another document for testing.']\n    vectorizer = TfidfVectorizer(dtype=np.float32)\n    X_tfidf = vectorizer.fit_transform(X)\n    assert X_tfidf.dtype == np.float32\n\n    vectorizer = TfidfVectorizer(dtype=np.float64)\n    X_tfidf = vectorizer.fit_transform(X)\n    assert X_tfidf.dtype == np.float64\n", "def test_vectorizer_dtype():\n    # Test that CountVectorizer and TfidfVectorizer return matrices with the\n    # correct dtype.\n    data = ['this is text', 'this is also text']\n\n    # Test with default dtype (int64 for CountVectorizer, float64 for TfidfVectorizer)\n    count_vect = CountVectorizer()\n    X_count = count_vect.fit_transform(data)\n    assert X_count.dtype == np.int64\n\n    tfidf_vect = TfidfVectorizer()\n    X_tfidf = tfidf_vect.fit_transform(data)\n    assert X_tfidf.dtype == np.float64\n\n    # Test with custom dtype\n    count_vect = CountVectorizer(dtype=np.float32)\n    X_count = count_vect.fit_transform(data)\n    assert X_count.dtype == np.float32\n\n    tfidf_vect = TfidfVectorizer(dtype=np.float32)\n    X_tfidf = tfidf_vect.fit_transform(data)\n    assert X_tfidf.dtype == np.float32\n"], "sample_846": ["def test_column_transformer_empty_transformers():\n    X = np.array([[1, 2], [3, 4]])\n    ct = ColumnTransformer([])\n    with pytest.raises(ValueError, match=\"No valid specification\"):\n        ct.fit(X)\n    with pytest.raises(ValueError, match=\"No valid specification\"):\n        ct.transform(X)\n    with pytest.raises(ValueError, match=\"No valid specification\"):\n        ct.fit_transform(X)\n", "def test_column_transformer_empty_transformers():\n    X = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([], remainder='passthrough')\n    assert_array_equal(ct.fit_transform(X), X)\n    assert_array_equal(ct.fit(X).transform(X), X)\n\n    ct = ColumnTransformer([], remainder='drop')\n    assert_array_equal(ct.fit_transform(X), np.zeros((3, 0)))\n    assert_array_equal(ct.fit(X).transform(X), np.zeros((3, 0)))\n", "def test_column_transformer_empty_dataframe():\n    \"\"\"Test that an empty DataFrame is handled correctly.\"\"\"\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(columns=['first', 'second'])\n\n    tf = ColumnTransformer([('bycol', Trans(), ['first'])])\n    tf.fit(X_df)\n\n    assert_array_equal(tf.transform(X_df), np.empty((0, 1)))\n", "def test_column_transformer_feature_names_with_drop():\n    \"\"\"Test `get_feature_names` with 'drop' estimator.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n\n    # no feature names available\n    ct = ColumnTransformer([('trans', 'drop', [0])])\n    ct.fit(X)\n    assert_raise_message(AttributeError,\n                         \"Transformer trans (type drop) does not provide \"\n                         \"get_feature_names\", ct.get_feature_names)\n\n    # other transformer has get_feature_names\n    ct = ColumnTransformer([('trans1', StandardScaler(), [1]),\n                            ('trans2', 'drop', [0])])\n    ct.fit(X)\n    assert ct.get_feature_names() == ['trans1__x0']\n\n    # multiple transformers have get_feature_names\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(), [1]),\n                            ('trans3', 'drop', [1])])\n    ct.fit(X)\n    assert ct.get_feature_names() == ['trans1__x0', 'trans2__x0']\n", "def test_column_transformer_remainder_with_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    X_res_first = np.array([0, 1, 2]).reshape(-1, 1)\n    X_res_second = np.array([2, 4, 6]).reshape(-1, 1)\n\n    transformer_weights = {'trans1': .1, 'remainder': 10}\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=DoubleTrans(),\n                           transformer_weights=transformer_weights)\n    exp = np.vstack([transformer_weights['trans1'] * X_res_first.T,\n                     transformer_weights['remainder'] * 2 * X_res_second.T]).T\n    assert_array_equal(ct.fit_transform(X_array), exp)\n    assert_array_equal(ct.fit(X_array).transform(X_array), exp)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], DoubleTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1])\n"], "sample_847": ["def test_enet_coordinate_descent_sparse_input_dtype():\n    \"\"\"Test that sparse input to enet_path results in float64 output\"\"\"\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    csr = sparse.csr_matrix(X)\n\n    for path in [enet_path, lasso_path]:\n        _, coefs, _ = path(csr, y, fit_intercept=False)\n        assert coefs.dtype == np.float64\n\n    # Test other supported dtypes\n    for dtype in [np.float32]:\n        csr = sparse.csr_matrix(X, dtype=dtype)\n        for path in [enet_path, lasso_path]:\n            _, coefs, _ = path(csr, y, fit_intercept=False)\n            assert coefs.dtype == dtype\n", "def test_lassoCV_positive_constraint():\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    model = LassoCV(positive=True, cv=2, random_state=42)\n    model.fit(X, y)\n    assert np.all(model.coef_ >= 0)\n", "def test_lasso_path_return_models_vs_new_return_gives_same_coefficients_sparse():\n    # Test that lasso_path with lars_path style output gives the\n    # same result with sparse matrices\n\n    # Some toy data\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    y = np.array([1, 2, 3.1])\n    alphas = [5., 1., .5]\n\n    # Use lars_path and lasso_path(new output) with 1D linear interpolation\n    # to compute the same path\n    alphas_lars, _, coef_path_lars = lars_path(X, y, method='lasso')\n    coef_path_cont_lars = interpolate.interp1d(alphas_lars[::-1],\n                                               coef_path_lars[:, ::-1])\n    alphas_lasso2, coef_path_lasso2, _ = lasso_path(X, y, alphas=alphas,\n                                                    return_models=False)\n    coef_path_cont_lasso = interpolate.interp1d(alphas_lasso2[::-1],\n                                                coef_path_lasso2[:, ::-1])\n\n    X_sparse = sparse.csr_matrix(X)\n    alphas_lasso_sparse, coef_path_lasso_sparse, _ = lasso_path(\n        X_sparse, y, alphas=alphas, return_models=False)\n\n    assert_array_almost_equal(\n        coef_path_cont_lasso(alphas), coef_path_cont_lars(alphas),\n        decimal=1)\n    assert_array_almost_equal(coef_path_lasso_sparse, coef_path_lasso2,\n                              decimal=1)\n", "def test_sparse_enet_coordinate_descent():\n    # Test coordinate descent for sparse elastic net\n    n_samples, n_features = 50, 100\n    X = sparse.csr_matrix(np.random.randn(n_samples, n_features))\n    y = np.random.randn(n_samples)\n\n    clf = ElasticNet(alpha=0.1, max_iter=1000, tol=1e-7)\n    clf.fit(X, y)\n\n    # Check that the prediction is consistent with the dense version\n    X_dense = X.toarray()\n    clf_dense = ElasticNet(alpha=0.1, max_iter=1000, tol=1e-7)\n    clf_dense.fit(X_dense, y)\n    assert_array_almost_equal(clf.coef_, clf_dense.coef_)\n    assert_array_almost_equal(clf.predict(X), clf_dense.predict(X_dense))\n", "def test_multitask_lasso_and_enet_cv_return_models():\n    X, y, _, _ = build_dataset(n_features=50, n_targets=3)\n    clf = MultiTaskLassoCV(cv=3, return_models=True).fit(X, y)\n    assert len(clf.models_) == 3\n\n    clf = MultiTaskElasticNetCV(cv=3, return_models=True).fit(X, y)\n    assert len(clf.models_) == 3\n"], "sample_848": ["def test_regressor_chain_crossval_fit_and_predict_with_sparse_data():\n    # Fit regressor chain with cross_val_predict and verify predict\n    # performance using sparse data\n    X, Y = generate_multilabel_dataset_with_correlations()\n    X_sparse = sp.csr_matrix(X)\n\n    chain = RegressorChain(Ridge())\n    chain.fit(X_sparse, Y)\n    chain_cv = clone(chain).set_params(cv=3)\n    chain_cv.fit(X_sparse, Y)\n    Y_pred_cv = chain_cv.predict(X_sparse)\n    Y_pred = chain.predict(X_sparse)\n\n    assert Y_pred_cv.shape == Y_pred.shape\n    assert not np.all(Y_pred == Y_pred_cv)\n    assert mean_squared_error(Y, Y_pred_cv) < .25\n", "def test_multi_output_classifier_chain_partial_fit():\n    # test if multi_target initializes correctly with base estimator and fit\n    # assert predictions work as expected for predict\n\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, n_classes=3,\n                               random_state=1)\n\n    Y = np.column_stack((y, y))\n\n    sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)\n    multi_target_linear = ClassifierChain(sgd_linear_clf, order=[0, 1])\n\n    # train the multi_target_linear and also get the predictions.\n    half_index = X.shape[0] // 2\n    multi_target_linear.partial_fit(\n        X[:half_index], Y[:half_index], classes=[np.unique(y), np.unique(y)])\n\n    first_predictions = multi_target_linear.predict(X)\n    assert (X.shape[0], 2) == first_predictions.shape\n\n    multi_target_linear.partial_fit(X[half_index:], Y[half_index:])\n    second_predictions = multi_target_linear.predict(X)\n    assert (X.shape[0], 2) == second_predictions.shape\n\n    # train the linear classification with each column and assert that\n    # predictions are equal after first partial_fit and second partial_fit\n    for i in range(2):\n        # create a clone with the same state\n        sgd_linear_clf = clone(sgd_linear_clf)\n        sgd_linear_clf.partial_fit(\n            X[:half_index], Y[:half_index, i], classes=np.unique(y))\n        assert_array_equal(sgd_linear_clf.predict(X), first_predictions[:, i])\n        sgd_linear_clf.partial_fit(X[half_index:], Y[half_index:, i])\n        assert_array_equal(sgd_linear_clf.predict(X), second_predictions[:, i])\n", "def test_multi_output_regressor_sample_weights_with_sparse_data():\n    # weighted regressor with sparse data\n    Xw = sp.csr_matrix([[1, 2, 3], [4, 5, 6]])\n    yw = [[3.141, 2.718], [2.718, 3.141]]\n    w = np.asarray([2., 1.])\n    rgr_w = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    rgr_w.fit(Xw, yw, w)\n\n    # unweighted, but with repeated samples\n    X = sp.csr_matrix([[1, 2, 3], [1, 2, 3], [4, 5, 6]])\n    y = [[3.141, 2.718], [3.141, 2.718], [2.718, 3.141]]\n    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n    rgr.fit(X, y)\n\n    X_test = sp.csr_matrix([[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]])\n    assert_almost_equal(rgr.predict(X_test).toarray(), rgr_w.predict(X_test).toarray())\n", "def test_regressor_chain_crossval_fit_and_predict_with_sparse_data():\n    # Fit regressor chain with cross_val_predict and verify predict\n    # performance using sparse data\n    X, Y = generate_multilabel_dataset_with_correlations()\n    X_sparse = sp.csr_matrix(X)\n\n    chain = RegressorChain(Ridge(), cv=3)\n    chain.fit(X_sparse, Y)\n    Y_pred_cv = chain.predict(X_sparse)\n\n    assert Y_pred_cv.shape == Y.shape\n    assert mean_squared_error(Y, Y_pred_cv) < .25\n", "def test_regressor_chain_fit_and_predict_with_linear_regression():\n    # Fit regressor chain and verify predict performance using LinearRegression\n    X, Y = generate_multilabel_dataset_with_correlations()\n    Y = np.random.rand(*Y.shape)  # convert to regression problem\n    regressor_chain = RegressorChain(Ridge())\n    regressor_chain.fit(X, Y)\n\n    Y_pred = regressor_chain.predict(X)\n    assert Y_pred.shape == Y.shape\n\n    # Verify that the predictions are not identical for each output\n    assert not np.allclose(Y_pred[:, 0], Y_pred[:, 1])\n"], "sample_849": ["def test_validate_shuffle_split():\n    # Test _validate_shuffle_split function\n    n_samples = 10\n    test_size = 0.5\n    train_size = None\n    expected_n_train, expected_n_test = 5, 5\n    actual_n_train, actual_n_test = _validate_shuffle_split(\n        n_samples, test_size, train_size)\n    assert actual_n_train == expected_n_train\n    assert actual_n_test == expected_n_test\n\n    # Test with integer values\n    test_size = 3\n    train_size = 7\n    expected_n_train, expected_n_test = 7, 3\n    actual_n_train, actual_n_test = _validate_shuffle_split(\n        n_samples, test_size, train_size)\n    assert actual_n_train == expected_n_train\n    assert actual_n_test == expected_n_test\n\n    # Test with None values\n    test_size = None\n    train_size = 0.5\n    expected_n_train, expected_n_test = 5, 5\n    actual_n_train, actual_n_test = _validate_shuffle_split(\n        n_samples, test_size, train_size)\n    assert actual_n_train == expected_n_train\n    assert actual_n_test == expected_n_test\n\n    # Test error handling\n    test_size = 0.8\n    train_size = 0.7\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size, train_size)\n\n    test_size = 11\n    train_size = None\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size, train_size)\n\n    test_size = None\n    train_size = 11\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size, train_size)\n\n    test_size = -1\n    train_size = None\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size, train_size)\n\n    test_size = None\n    train_size = -1\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size, train_size)\n", "def test_validate_shuffle_split():\n    # Test the _validate_shuffle_split function\n    n_samples = 10\n\n    # Test with test_size as an integer\n    test_size, train_size = _validate_shuffle_split(n_samples, test_size=3,\n                                                    train_size=None)\n    assert test_size == 3\n    assert train_size == 7\n\n    # Test with test_size as a float\n    test_size, train_size = _validate_shuffle_split(n_samples, test_size=0.3,\n                                                    train_size=None)\n    assert test_size == 3\n    assert train_size == 7\n\n    # Test with train_size as an integer\n    test_size, train_size = _validate_shuffle_split(n_samples, test_size=None,\n                                                    train_size=7)\n    assert test_size == 3\n    assert train_size == 7\n\n    # Test with train_size as a float\n    test_size, train_size = _validate_shuffle_split(n_samples, test_size=None,\n                                                    train_size=0.7)\n    assert test_size == 3\n    assert train_size == 7\n\n    # Test with both test_size and train_size as integers\n    test_size, train_size = _validate_shuffle_split(n_samples, test_size=3,\n                                                    train_size=7)\n    assert test_size == 3\n    assert train_size == 7\n\n    # Test with both test_size and train_size as floats\n    test_size, train_size = _validate_shuffle_split(n_samples, test_size=0.3,\n                                                    train_size=0.7)\n    assert test_size == 3\n    assert train_size == 7\n\n    # Test invalid values\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, test_size=11)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples,\n                  test_size=1.1)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples, train_size=11)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples,\n                  train_size=1.1)\n    assert_raises(ValueError, _validate_shuffle_split, n_samples,\n                  test_size=5, train_size=6)\n", "def test_validate_shuffle_split():\n    n_samples = 10\n    test_size = 0.2\n    train_size = 0.8\n\n    # Test that the validation function returns the correct train and test sizes\n    train_size_out, test_size_out = _validate_shuffle_split(\n        n_samples, test_size, train_size)\n    assert train_size_out == int(train_size * n_samples)\n    assert test_size_out == int(test_size * n_samples)\n\n    # Test that the validation function raises an error when the sum of train\n    # and test sizes is greater than the total number of samples\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=0.7, train_size=0.6)\n\n    # Test that the validation function raises an error when the test size is\n    # greater than the total number of samples\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=1.1, train_size=None)\n\n    # Test that the validation function raises an error when the train size is\n    # greater than the total number of samples\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=None, train_size=1.1)\n\n    # Test that the validation function raises an error when both train and test\n    # sizes are specified and their sum is not equal to 1\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=0.4, train_size=0.5)\n", "def test_validate_shuffle_split():\n    # Test the _validate_shuffle_split function\n    n_samples = 10\n\n    # Test with test_size as a proportion\n    test_size = 0.2\n    train_size, test_size_out = _validate_shuffle_split(n_samples, test_size)\n    assert train_size == 8\n    assert test_size_out == 2\n\n    # Test with test_size as an integer\n    test_size = 2\n    train_size, test_size_out = _validate_shuffle_split(n_samples, test_size)\n    assert train_size == 8\n    assert test_size_out == 2\n\n    # Test with train_size as a proportion\n    train_size = 0.8\n    train_size_out, test_size = _validate_shuffle_split(n_samples, None, train_size)\n    assert train_size_out == 8\n    assert test_size == 2\n\n    # Test with train_size as an integer\n    train_size = 8\n    train_size_out, test_size = _validate_shuffle_split(n_samples, None, train_size)\n    assert train_size_out == 8\n    assert test_size == 2\n\n    # Test when neither test_size nor train_size is specified\n    train_size, test_size = _validate_shuffle_split(n_samples)\n    assert train_size == 7\n    assert test_size == 3\n\n    # Test when both test_size and train_size are specified\n    test_size = 2\n    train_size = 8\n    train_size_out, test_size_out = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert train_size_out == 8\n    assert test_size_out == 2\n\n    # Test with invalid test_size\n    test_size = 1.2\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size)\n\n    # Test with invalid train_size\n    train_size = 1.2\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, None, train_size)\n\n    # Test when test_size is too large\n    test_size = 11\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size)\n\n    # Test when train_size is too large\n    train_size = 11\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, None, train_size)\n", "def test_split_with_sparse_matrices():\n    X_sparse = csr_matrix(np.array([[1, 2], [3, 4], [5, 6]]))\n    y = np.array([1, 2, 3])\n\n    # Test that StratifiedKFold and KFold work with sparse matrices\n    for cv in [StratifiedKFold(), KFold()]:\n        for train_index, test_index in cv.split(X_sparse, y):\n            assert len(train_index) + len(test_index) == X_sparse.shape[0]\n            assert len(set(train_index) & set(test_index)) == 0\n\n    # Test that LeaveOneOut and LeavePOut work with sparse matrices\n    for cv in [LeaveOneOut(), LeavePOut(2)]:\n        for train_index, test_index in cv.split(X_sparse):\n            assert len(train_index) + len(test_index) == X_sparse.shape[0]\n            assert len(set(train_index) & set(test_index)) == 0\n\n    # Test that ShuffleSplit works with sparse matrices\n    for cv in [ShuffleSplit(), StratifiedShuffleSplit()]:\n        for train_index, test_index in cv.split(X_sparse, y):\n            assert len(train_index) + len(test_index) == X_sparse.shape[0]\n            assert len(set(train_index) & set(test_index)) == 0\n"], "sample_850": ["def test_nystroem_feature_names_out():\n    # Test Nystroem feature names out.\n    rnd = np.random.RandomState(42)\n    n_samples = 10\n    X = rnd.uniform(size=(n_samples, 4))\n\n    nystroem = Nystroem(n_components=5)\n    X_transformed = nystroem.fit_transform(X)\n\n    assert nystroem.get_feature_names_out().shape == (5,)\n", "def test_nystroem_sparse():\n    # Test that Nystroem works with sparse matrices.\n    rnd = np.random.RandomState(42)\n    n_samples = 10\n    X = csr_matrix(rnd.uniform(size=(n_samples, 4)))\n\n    nystroem = Nystroem(n_components=5)\n    X_transformed = nystroem.fit_transform(X)\n\n    assert X_transformed.shape == (n_samples, 5)\n", "def test_nystroem_invalid_kernel():\n    # Test Nystroem with invalid kernel.\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    # Invalid kernel name\n    nystroem = Nystroem(kernel=\"invalid\", n_components=2)\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n\n    # Invalid kernel callable\n        return \"invalid\"\n    nystroem = Nystroem(kernel=invalid_kernel, n_components=2)\n    with pytest.raises(TypeError):\n        nystroem.fit(X)\n\n    # Invalid kernel None\n    nystroem = Nystroem(kernel=None, n_components=2)\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n", "def test_nystroem_invalid_kernel():\n    # Test that Nystroem raises an error for invalid kernels.\n    rnd = np.random.RandomState(42)\n    X = rnd.uniform(size=(10, 4))\n\n    # Invalid kernel name\n    with pytest.raises(ValueError):\n        Nystroem(kernel=\"invalid_kernel\", n_components=2).fit(X)\n\n    # Invalid kernel function (not callable)\n    with pytest.raises(ValueError):\n        Nystroem(kernel=\"rbf\", n_components=2, gamma=\"invalid_gamma\").fit(X)\n\n    # Invalid kernel parameter\n    with pytest.raises(ValueError):\n        Nystroem(kernel=\"poly\", n_components=2, degree=\"invalid_degree\").fit(X)\n\n    # Kernel not in available kernels\n    with pytest.raises(ValueError):\n        Nystroem(kernel=\"squared_sine\", n_components=2).fit(X)\n", "def test_nystroem_invalid_kernel():\n    # Test that Nystroem raises an error with an invalid kernel.\n    rnd = np.random.RandomState(42)\n    n_samples = 10\n    X = rnd.uniform(size=(n_samples, 4))\n\n    # Invalid kernel name\n    nystroem = Nystroem(kernel=\" invalid\", n_components=5)\n    with pytest.raises(ValueError):\n        nystroem.fit(X)\n\n    # Invalid kernel callable\n        return \"invalid\"\n\n    nystroem = Nystroem(kernel=invalid_kernel, n_components=5)\n    with pytest.raises(TypeError):\n        nystroem.fit(X)\n"], "sample_851": ["def test_regression_metrics_with_sample_weight():\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 3, 2, 4]\n    sample_weight = [0.5, 1., 0.5, 1.]\n\n    mse = mean_squared_error(y_true, y_pred, sample_weight=sample_weight)\n    mae = mean_absolute_error(y_true, y_pred, sample_weight=sample_weight)\n    r2 = r2_score(y_true, y_pred, sample_weight=sample_weight)\n\n    assert_almost_equal(mse, 0.5)\n    assert_almost_equal(mae, 0.5)\n    assert_almost_equal(r2, 0.75)\n", "def test_mean_poisson_deviance():\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n\n    # Expected result evaluated by hand\n    expected_result = np.mean([2 * (2 * np.log(2 / 0.5) - 2 + 0.5),\n                               2 * (0 * np.log(0 / 0.5) - 0 + 0.5),\n                               2 * (1 * np.log(1 / 2) - 1 + 2),\n                               2 * (4 * np.log(4 / 2) - 4 + 2)])\n\n    assert_almost_equal(mean_poisson_deviance(y_true, y_pred), expected_result)\n\n    # Check that function raises error when y_true or y_pred is negative\n    with pytest.raises(ValueError):\n        mean_poisson_deviance([2, -1, 1, 4], [0.5, 0.5, 2., 2.])\n\n    with pytest.raises(ValueError):\n        mean_poisson_deviance([2, 0, 1, 4], [-0.5, 0.5, 2., 2.])\n\n    with pytest.raises(ValueError):\n        mean_poisson_deviance([2, 0, 1, 4], [0, 0.5, 2., 2.])\n", "def test_mean_tweedie_deviance_sample_weight():\n    y_true = np.array([1.0, 2.0, 3.0])\n    y_pred = np.array([1.0, 2.0, 3.0])\n    sample_weight = np.array([0.5, 1.0, 1.5])\n\n    # Test with power=0 (same as mean squared error)\n    score = mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight,\n                                  power=0)\n    assert_almost_equal(score, 0.0)\n\n    # Test with power=1 (same as mean poisson deviance)\n    score = mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight,\n                                  power=1)\n    assert_almost_equal(score, 0.0)\n\n    # Test with power=2 (same as mean gamma deviance)\n    score = mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight,\n                                  power=2)\n    assert_almost_equal(score, 0.0)\n", "def test_regression_metrics_with_zero_weights():\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([1, 3, 2])\n\n    # Test with zero weights for all samples\n    sample_weight = np.array([0, 0, 0])\n    assert np.isnan(mean_squared_error(y_true, y_pred, sample_weight=sample_weight))\n    assert np.isnan(mean_absolute_error(y_true, y_pred, sample_weight=sample_weight))\n    assert np.isnan(r2_score(y_true, y_pred, sample_weight=sample_weight))\n\n    # Test with zero weights for some samples\n    sample_weight = np.array([1, 0, 1])\n    assert_almost_equal(mean_squared_error(y_true, y_pred, sample_weight=sample_weight), 1.0)\n    assert_almost_equal(mean_absolute_error(y_true, y_pred, sample_weight=sample_weight), 1.0)\n    assert_almost_equal(r2_score(y_true, y_pred, sample_weight=sample_weight), -0.5)\n", "def test_regression_metrics_with_zeros_and_negative_values():\n    y_true = [0, -1, 2, 3]\n    y_pred = [0, -1, 2, 4]\n\n    # Test mean_squared_error with zeros and negative values\n    assert_almost_equal(mean_squared_error(y_true, y_pred), 1.0, decimal=2)\n\n    # Test mean_absolute_error with zeros and negative values\n    assert_almost_equal(mean_absolute_error(y_true, y_pred), 1.0, decimal=2)\n\n    # Test median_absolute_error with zeros and negative values\n    assert_almost_equal(median_absolute_error(y_true, y_pred), 1.0, decimal=2)\n\n    # Test max_error with zeros and negative values\n    assert_almost_equal(max_error(y_true, y_pred), 1.0, decimal=2)\n\n    # Test r2_score with zeros and negative values\n    assert_almost_equal(r2_score(y_true, y_pred), 0.5, decimal=2)\n\n    # Test explained_variance_score with zeros and negative values\n    assert_almost_equal(explained_variance_score(y_true, y_pred), 0.5, decimal=2)\n"], "sample_852": ["def test_make_classification_return_X_y():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=1, n_repeated=1, n_classes=3,\n                               n_clusters_per_class=1, hypercube=False,\n                               shift=None, scale=None, weights=[0.1, 0.25],\n                               random_state=0)\n\n    assert isinstance(X, np.ndarray)\n    assert isinstance(y, np.ndarray)\n    assert X.shape == (100, 20)\n    assert y.shape == (100,)\n    assert np.unique(y).shape == (3,)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=None)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=None)\n\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(mean=None, cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=0)\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n\n    X, y = make_gaussian_quantiles(mean=[0, 0], cov=1., n_samples=100,\n                                   n_features=2, n_classes=3, shuffle=True,\n                                   random_state=0)\n    assert X.shape == (100, 2), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (3,), \"Unexpected number of classes\"\n", "def test_make_circles():\n    X, y = make_circles(3, shuffle=False)\n    for x, label in zip(X, y):\n        center = [0.0, 0.0] if label == 0 else [0.0, 0.0]\n        dist_sqr = ((x - center) ** 2).sum()\n        assert_almost_equal(dist_sqr, 1.0,\n                            err_msg=\"Point is not on expected unit circle\")\n", "def test_make_circles():\n    X, y = make_circles(3, shuffle=False)\n    for x, label in zip(X, y):\n        dist_sqr = ((x - [0.0, 0.0]) ** 2).sum()\n        if label == 0:\n            assert_almost_equal(dist_sqr, 1.0,\n                                err_msg=\"Point is not on expected unit circle\")\n        else:\n            assert_almost_equal(dist_sqr, 0.64,\n                                err_msg=\"Point is not on expected smaller circle\")\n"], "sample_853": ["def test_transform_target_regressor_inverse_func_not_provided():\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                      func=np.log)\n    with pytest.raises(ValueError, match=\"When 'func' is provided, \"\n                       \"'inverse_func' must also be provided\"):\n        regr.fit(X, y)\n", "def test_transform_target_regressor_pipeline():\n    # check that the TransformedTargetRegressor can be used in a pipeline\n    X, y = friedman\n    regr = Pipeline([\n        ('transformer', StandardScaler()),\n        ('regressor', TransformedTargetRegressor(\n            regressor=LinearRegression(),\n            func=np.log, inverse_func=np.exp))\n    ])\n    regr.fit(X, y)\n    y_pred = regr.predict(X)\n    assert y.shape == y_pred.shape\n    assert_allclose(y_pred, np.exp(regr.named_steps['regressor'].regressor_.predict(\n        regr.named_steps['transformer'].transform(X))))\n", "def test_transform_target_regressor_clone():\n    # Check that the regressor and transformer are properly cloned.\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                      transformer=StandardScaler())\n    regr.fit(X, y)\n\n    regr_cloned = clone(regr)\n    assert regr_cloned.regressor is not regr.regressor\n    assert regr_cloned.transformer is not regr.transformer\n\n    regr_cloned.fit(X, y)\n    assert_allclose(regr_cloned.regressor_.coef_, regr.regressor_.coef_)\n    assert_allclose(regr_cloned.transformer_.mean_, regr.transformer_.mean_)\n", "def test_transform_target_regressor_pipeline():\n    # Check that the TransformedTargetRegressor can be used in a pipeline\n    X, y = friedman\n    regr = Pipeline([\n        ('transformer', StandardScaler()),\n        ('regressor', TransformedTargetRegressor(regressor=LinearRegression(),\n                                                 func=np.log, inverse_func=np.exp))\n    ])\n    regr.fit(X, y)\n    y_pred = regr.predict(X)\n    assert y.shape == y_pred.shape\n    assert_allclose(y_pred, np.exp(regr.named_steps['regressor'].regressor_.predict(\n        regr.named_steps['transformer'].transform(X))))\n", "def test_transform_target_regressor_with_pipeline():\n    # check that the TransformedTargetRegressor works with a Pipeline\n    X, y = friedman\n    regr = TransformedTargetRegressor(\n        regressor=Pipeline([('scaler', StandardScaler()), ('lr', LinearRegression())]),\n        transformer=StandardScaler()\n    )\n    regr.fit(X, y)\n    y_pred = regr.predict(X)\n    assert y.shape == y_pred.shape\n"], "sample_854": ["def test_sparse_data_warning():\n    # Test that warning is raised when sparse data is passed to a classifier\n    # that does not support it.\n\n    X, y = make_classification(n_samples=10, n_features=10)\n    X_sparse = sparse.csr_matrix(X)\n\n    for clf in [svm.SVC(kernel='precomputed'), svm.NuSVC(kernel='precomputed')]:\n        with pytest.raises(TypeError):\n            clf.fit(X_sparse, y)\n\n    for clf in [svm.SVC(), svm.NuSVC()]:\n        assert_warns_message(UserWarning,\n                             \"The kernel 'linear' is not implemented for \"\n                             \"sparse data. Falling back to 'rbf'.\",\n                             clf.fit, X_sparse, y)\n\n    for clf in [svm.LinearSVC(), svm.LinearSVR()]:\n        with pytest.raises(ValueError):\n            clf.fit(X_sparse, y)\n", "def test_base_libsvm_sparse_predict_proba():\n    # Test that sparse predict_proba works as expected.\n    X, y = make_classification(n_samples=20, n_features=5)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf = svm.SVC(probability=True, kernel='linear')\n    clf.fit(X_sparse, y)\n\n    pred_dense = clf.predict_proba(X_sparse.toarray())\n    pred_sparse = clf.predict_proba(X_sparse)\n\n    assert_array_almost_equal(pred_dense, pred_sparse)\n", "def test_svc_proba_ovr():\n    # Test that predict_proba with ovr works and does not raise a warning\n    X, y = make_blobs(random_state=42)\n    clf = svm.SVC(kernel=\"linear\", decision_function_shape='ovr',\n                   probability=True, random_state=42).fit(X, y)\n    assert_no_warnings(clf.predict_proba, X)\n    assert_array_almost_equal(np.sum(clf.predict_proba(X), axis=1), np.ones(len(X)))\n", "def test_libsvm_sparse_input():\n    # Test libsvm with sparse input\n    X_sparse = sp.csr_matrix(X)\n    clf = svm.SVC(kernel='linear').fit(X_sparse, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # test with sparse data and kernel trick\n    clf = svm.SVC(kernel='rbf').fit(X_sparse, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # test with sparse data and custom kernel\n        return np.dot(x, y.T)\n    clf = svm.SVC(kernel=my_kernel).fit(X_sparse, Y)\n    assert_array_equal(clf.predict(T), true_result)\n", "def test_base_libsvm_sparse_predict():\n    # Test BaseLibSVM with sparse data and predict\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    clf = svm.SVC(kernel='linear', probability=True, random_state=0)\n    clf.fit(X, y)\n\n    X_sparse = sparse.csr_matrix(X)\n    y_pred_sparse = clf.predict(X_sparse)\n    y_pred_dense = clf.predict(X)\n    assert_array_equal(y_pred_sparse, y_pred_dense)\n"], "sample_855": ["def test_dummy_classifier_on_sparse_input():\n    X = sp.csr_matrix(np.array([[0, 1], [1, 0]]))\n    y = np.array([0, 1])\n\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    clf.fit(X, y)\n\n    assert_array_equal(clf.predict(X), np.array([0, 0]))\n    _check_predict_proba(clf, X, y)\n", "def test_dummy_classifier_predict_proba_unfitted():\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    X = [[0], [0], [0]]\n    with pytest.raises(NotFittedError):\n        clf.predict_proba(X)\n", "def test_dummy_classifier_constant_strategy_predict_proba():\n    X = [[0]] * 5\n    y = [1, 2, 1, 1, 2]\n    clf = DummyClassifier(strategy=\"constant\", constant=1, random_state=0)\n    clf.fit(X, y)\n\n    proba = clf.predict_proba(X)\n    assert_array_equal(proba[:, 0], np.ones(len(X)))\n    assert_array_equal(proba[:, 1], np.zeros(len(X)))\n\n    log_proba = clf.predict_log_proba(X)\n    assert_array_equal(log_proba[:, 0], np.zeros(len(X)))\n    assert_array_equal(log_proba[:, 1], -np.inf * np.ones(len(X)))\n", "def test_dummy_classifier_sparse_output():\n    X = [[0]] * 5  # ignored\n    y = np.array([[2, 1],\n                  [2, 2],\n                  [1, 4],\n                  [4, 2],\n                  [1, 1]])\n\n    clf = DummyClassifier(strategy=\"stratified\", random_state=0)\n    clf.fit(X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n    assert isinstance(y_pred, np.ndarray)\n\n    y_sparse = sp.csr_matrix(y)\n    clf = DummyClassifier(strategy=\"stratified\", random_state=0)\n    clf.fit(X, y_sparse)\n\n    X = [[0]] * 500\n    y_pred_sparse = clf.predict(X)\n    assert sp.issparse(y_pred_sparse)\n    assert_array_equal(y_pred_sparse.toarray(), y_pred)\n", "def test_dummy_classifier_output_2d():\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = np.array([1, 2, 1, 1])\n\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    clf.fit(X, y)\n\n    assert clf.output_2d_ is False\n\n    y_2d = np.array([[1], [2], [1], [1]])\n    clf.fit(X, y_2d)\n\n    assert clf.output_2d_ is True\n"], "sample_856": ["def test_validate_shuffle_split():\n    # Test if _validate_shuffle_split raises appropriate errors and warnings\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=0.6,\n                  train_size=0.6)\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=11,\n                  train_size=5)\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size='wrong',\n                  train_size=5)\n    assert_raises(ValueError, _validate_shuffle_split, 10, test_size=5,\n                  train_size='wrong')\n    assert_warns_message(UserWarning, \"The least populated class\",\n                         _validate_shuffle_split, 10, test_size=3,\n                         train_size=7, default_test_size=0.25)\n    assert_warns_message(UserWarning, \"Too few samples for each class\",\n                         _validate_shuffle_split, 10, test_size=8,\n                         train_size=2, default_test_size=0.25)\n    assert_warns_message(UserWarning, \"Too few samples for each class\",\n                         _validate_shuffle_split, 10, test_size=0.9,\n                         train_size=0.1, default_test_size=0.25)\n\n    # Ensure that test_size is not greater than n_samples when its a float\n    n_samples = 100\n    test_size = 0.99\n    train_size, test_size = _validate_shuffle_split(n_samples, test_size=test_size)\n    assert test_size < n_samples\n\n    # Ensure that test_size and train_size sum to n_samples\n    n_samples = 100\n    test_size = 20\n    train_size, test_size = _validate_shuffle_split(n_samples, test_size=test_size)\n    assert test_size + train_size == n_samples\n", "def test_predefined_split_init():\n    # Check that PredefinedSplit can be initialized with arrays of different types\n    test_fold = np.array([1, 2, 3, 4, 5])\n    ps = PredefinedSplit(test_fold)\n    assert_array_equal(ps.test_fold, test_fold)\n\n    test_fold = [1, 2, 3, 4, 5]\n    ps = PredefinedSplit(test_fold)\n    assert_array_equal(ps.test_fold, test_fold)\n\n    test_fold = pd.Series([1, 2, 3, 4, 5])\n    ps = PredefinedSplit(test_fold)\n    assert_array_equal(ps.test_fold, test_fold)\n", "def test_leave_p_out_edge_cases():\n    # Check edge cases for LeavePOut\n    assert_raises(ValueError, LeavePOut, 0)\n    assert_raises(ValueError, LeavePOut, -1)\n    assert_raises(TypeError, LeavePOut, 2.0)\n\n    X = np.ones(3)\n    cv = LeavePOut(p=2)\n    assert cv.get_n_splits(X) == comb(3, 2)\n\n    with pytest.raises(ValueError,\n                       match=r\"p=2 should be strictly less than the number\"):\n        next(cv.split(X))\n", "def test_time_series_cv_with_max_train_size():\n    X = np.zeros((10, 1))\n    tscv = TimeSeriesSplit(n_splits=3, max_train_size=5)\n    splits = list(tscv.split(X))\n\n    assert len(splits) == 3\n\n    expected_splits = [\n        ([0, 1, 2, 3], [4, 5]),\n        ([2, 3, 4, 5, 6], [7, 8]),\n        ([5, 6, 7, 8], [9])\n    ]\n\n    for actual_split, expected_split in zip(splits, expected_splits):\n        assert_array_equal(actual_split[0], expected_split[0])\n        assert_array_equal(actual_split[1], expected_split[1])\n", "def test_validate_shuffle_split():\n    # Test that _validate_shuffle_split raises errors for invalid inputs\n    assert_raises(ValueError, _validate_shuffle_split, 10, 1.2)\n    assert_raises(ValueError, _validate_shuffle_split, 10, -1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 11)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 1.2)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, -1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 11)\n\n    # Test that _validate_shuffle_split returns the correct values\n    n_samples = 10\n    test_size = 0.2\n    train_size = 0.8\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == int(train_size * n_samples)\n    assert n_test == int(test_size * n_samples)\n\n    # Test that _validate_shuffle_split returns the correct values when\n    # test_size is not specified\n    n_train, n_test = _validate_shuffle_split(n_samples, None, train_size)\n    assert n_train == int(train_size * n_samples)\n    assert n_test == n_samples - n_train\n\n    # Test that _validate_shuffle_split returns the correct values when\n    # train_size is not specified\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, None)\n    assert n_test == int(test_size * n_samples)\n    assert n_train == n_samples - n_test\n"], "sample_857": ["def test_ccp_pruning_path_tree_has_correct_number_of_nodes():\n    # Test that the number of nodes in the trees generated by\n    # `cost_complexity_pruning_path` is correct.\n\n    X, y = iris.data, iris.target\n\n    for tree_cls in [DecisionTreeClassifier, ExtraTreeClassifier]:\n        est = tree_cls(max_leaf_nodes=20, random_state=0)\n        info = est.cost_complexity_pruning_path(X, y)\n\n        num_unique_alphas = len(np.unique(info.ccp_alphas))\n        assert len(info.impurities) == num_unique_alphas\n\n        trees = info.pruned_trees\n        assert len(trees) == num_unique_alphas\n\n        prev_num_nodes = np.inf\n        for tree in trees:\n            num_nodes = tree.node_count\n            assert num_nodes <= prev_num_nodes\n            prev_num_nodes = num_nodes\n\n    for tree_cls in [DecisionTreeRegressor, ExtraTreeRegressor]:\n        est = tree_cls(max_leaf_nodes=20, random_state=0)\n        info = est.cost_complexity_pruning_path(X, y)\n\n        num_unique_alphas = len(np.unique(info.ccp_alphas))\n        assert len(info.impurities) == num_unique_alphas\n\n        trees = info.pruned_trees\n        assert len(trees) == num_unique_alphas\n\n        prev_num_nodes = np.inf\n        for tree in trees:\n            num_nodes = tree.node_count\n            assert num_nodes <= prev_num_nodes\n            prev_num_nodes = num_nodes\n", "def test_prune_tree_parameters_validation():\n    clf = DecisionTreeClassifier()\n\n    # ccp_alpha is not a number\n    with pytest.raises(ValueError, match=\"ccp_alpha must be a non-negative float\"):\n        clf.set_params(ccp_alpha='a')\n        clf.fit(X, y)\n\n    # ccp_alpha is a negative number\n    with pytest.raises(ValueError, match=\"ccp_alpha must be greater than or equal to 0\"):\n        clf.set_params(ccp_alpha=-1.0)\n        clf.fit(X, y)\n\n    # ccp_alpha is a positive number\n    clf.set_params(ccp_alpha=0.5)\n    clf.fit(X, y)\n", "def test_prune_tree_clone():\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    # clone should preserve the tree structure and parameters\n    clf_clone = clone(clf)\n    assert_is_subtree(clf.tree_, clf_clone.tree_)\n    assert clf_clone.get_params() == clf.get_params()\n\n    # clone should be independent from original tree\n    clf_clone.set_params(ccp_alpha=10)\n    clf_clone._prune_tree()\n    assert not np.allclose(clf.tree_.impurity, clf_clone.tree_.impurity)\n", "def test_prune_tree_ccp_alpha_not_affecting_fitting():\n    # ccp_alpha should not affect the structure of the initial tree before\n    # pruning. It only affects the pruning part.\n    X, y = iris.data, iris.target\n\n    clf1 = DecisionTreeClassifier(random_state=0)\n    clf1.fit(X, y)\n\n    clf2 = DecisionTreeClassifier(random_state=0, ccp_alpha=10)\n    clf2.fit(X, y)\n\n    assert_tree_equal(clf1.tree_, clf2.tree_)\n", "def test_prune_tree_ccp_alpha_not_affecting_other_params():\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    # save original tree parameters\n    original_max_depth = clf.tree_.max_depth\n    original_node_count = clf.tree_.node_count\n    original_capacity = clf.tree_.capacity\n\n    # set ccp_alpha to a large value and prune the tree\n    clf.set_params(ccp_alpha=10.0)\n    clf._prune_tree()\n\n    # check that max_depth, node_count and capacity are updated correctly\n    assert clf.tree_.max_depth < original_max_depth\n    assert clf.tree_.node_count < original_node_count\n    assert clf.tree_.capacity == original_capacity\n\n    # reset ccp_alpha to 0 and check that the tree is not pruned again\n    clf.set_params(ccp_alpha=0.0)\n    clf._prune_tree()\n    assert clf.tree_.max_depth < original_max_depth\n    assert clf.tree_.node_count < original_node_count\n    assert clf.tree_.capacity == original_capacity\n"], "sample_858": ["def test_voting_regressor_multidimensional_target():\n    \"\"\"Check VotingRegressor works with multidimensional target.\"\"\"\n    X = np.random.rand(10, 2)\n    y = np.random.rand(10, 3)\n    reg1 = LinearRegression()\n    reg2 = DecisionTreeRegressor()\n    ereg = VotingRegressor([('lr', reg1), ('dt', reg2)])\n    ereg.fit(X, y)\n    assert ereg.predict(X).shape == (10, 3)\n", "def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)],\n        weights=[1, 1]).fit(X, y)\n\n    assert_array_equal(ereg.transform(X).shape, (4, 2))\n    assert_array_almost_equal(ereg.transform(X),\n                              np.column_stack((reg1.fit(X, y).predict(X),\n                                               reg2.fit(X, y).predict(X))))\n", "def test_voting_regressor_multitarget():\n    \"\"\"Check VotingRegressor for multi-target regression.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[5, 6], [7, 8]])\n\n    reg1 = DecisionTreeRegressor()\n    reg2 = DecisionTreeRegressor()\n\n    ereg = VotingRegressor([('dt1', reg1), ('dt2', reg2)])\n\n    with pytest.raises(NotImplementedError):\n        ereg.fit(X, y)\n", "def test_voting_classifier_get_params():\n    \"\"\"Test that VotingClassifier get_params return expected dictionary\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='soft')\n    params = eclf.get_params()\n    assert 'estimators' in params\n    assert len(params['estimators']) == 2\n    assert 'voting' in params\n    assert params['voting'] == 'soft'\n    assert 'weights' in params\n    assert params['weights'] is None\n    assert 'n_jobs' in params\n    assert params['n_jobs'] is None\n    assert 'flatten_transform' in params\n    assert params['flatten_transform'] is True\n", "def test_check_is_fitted():\n    \"\"\"Test if check_is_fitted raises an error on unfitted VotingClassifier\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2)],\n                voting='soft')\n    msg = (\"This VotingClassifier instance is not fitted yet. Call 'fit' \"\n           \"with appropriate arguments before using this method.\")\n    assert_raise_message(NotFittedError, msg, eclf._check_is_fitted)\n"], "sample_859": ["def test_multitask_enet_coordinate_descent():\n    \"\"\"Test that a warning is issued if multitask model does not converge\"\"\"\n    clf = MultiTaskElasticNet(max_iter=2)\n    n_samples = 5\n    n_features = 2\n    n_tasks = 3\n    X = np.ones((n_samples, n_features)) * 1e50\n    y = np.ones((n_samples, n_tasks))\n    assert_warns(ConvergenceWarning, clf.fit, X, y)\n", "def test_multitask_enet_coordinate_descent():\n    \"\"\"Test that a warning is issued if multitask model does not converge\"\"\"\n    clf = MultiTaskElasticNet(max_iter=2)\n    n_samples = 5\n    n_features = 2\n    n_tasks = 3\n    X = np.ones((n_samples, n_features)) * 1e50\n    y = np.ones((n_samples, n_tasks))\n    assert_warns(ConvergenceWarning, clf.fit, X, y)\n", "def test_multitask_lasso_and_enet_float_precision():\n    # Generate dataset\n    X, y, X_test, y_test = build_dataset(n_samples=20, n_features=10)\n    y = np.hstack((y[:, np.newaxis], y[:, np.newaxis]))\n\n    for normalize in [True, False]:\n        for fit_intercept in [True, False]:\n            coef = {}\n            intercept = {}\n            for dtype in [np.float64, np.float32]:\n                clf = MultiTaskLasso(alpha=0.5, max_iter=100,\n                                     precompute=False, fit_intercept=fit_intercept,\n                                     normalize=normalize)\n\n                X = dtype(X)\n                y = dtype(y)\n                ignore_warnings(clf.fit)(X, y)\n\n                coef[('lasso', dtype)] = clf.coef_\n                intercept[('lasso', dtype)] = clf.intercept_\n\n                assert clf.coef_.dtype == dtype\n\n                clf = MultiTaskElasticNet(alpha=0.5, max_iter=100,\n                                          precompute=False, fit_intercept=fit_intercept,\n                                          normalize=normalize)\n\n                ignore_warnings(clf.fit)(X, y)\n\n                coef[('enet', dtype)] = clf.coef_\n                intercept[('enet', dtype)] = clf.intercept_\n\n                assert clf.coef_.dtype == dtype\n\n            for est in ['lasso', 'enet']:\n                assert_array_almost_equal(coef[(est, np.float32)],\n                                          coef[(est, np.float64)],\n                                          decimal=4)\n                assert_array_almost_equal(intercept[(est, np.float32)],\n                                          intercept[(est, np.float64)],\n                                          decimal=4)\n", "def test_multi_task_lasso_and_enet_sparse_input():\n    X, y, _, _ = build_dataset(n_samples=50, n_features=100)\n    Y = np.c_[y, y]\n    # Y_test = np.c_[y_test, y_test]\n    clf = MultiTaskLasso(alpha=1, tol=1e-8).fit(sparse.csr_matrix(X), Y)\n    assert 0 < clf.dual_gap_ < 1e-5\n    assert_array_almost_equal(clf.coef_[0], clf.coef_[1])\n\n    clf = MultiTaskElasticNet(alpha=1, tol=1e-8).fit(sparse.csr_matrix(X), Y)\n    assert 0 < clf.dual_gap_ < 1e-5\n    assert_array_almost_equal(clf.coef_[0], clf.coef_[1])\n", "def test_enet_coordinate_descent_multitarget():\n    \"\"\"Test that a warning is issued if model does not converge\"\"\"\n    clf = ElasticNet(max_iter=2)\n    n_samples = 5\n    n_features = 2\n    X = np.ones((n_samples, n_features)) * 1e50\n    y = np.ones((n_samples, 3))\n    assert_warns(ConvergenceWarning, clf.fit, X, y)\n"], "sample_860": ["def test_check_array_dtype_list():\n    # Test that check_array handles lists of dtypes correctly\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    X_checked = check_array(X, dtype=[np.float32, np.float64])\n    assert X_checked.dtype == np.float32\n\n    X_checked = check_array(X, dtype=['float32', 'float64'])\n    assert X_checked.dtype == np.float32\n\n    X = np.array([[1, 2], [3, 4]], dtype=np.int32)\n    X_checked = check_array(X, dtype=[np.float32, np.float64])\n    assert X_checked.dtype == np.float32\n", "def test_check_array_dtype_object_non_numeric():\n    X = np.array([['a', 'b'], ['c', 'd']], dtype=object)\n    X_checked = check_array(X, dtype='object')\n    assert_array_equal(X, X_checked)\n\n    # Check that we don't try to convert non-numeric data to numeric type\n    with pytest.raises(ValueError, match=\"could not convert string to float\"):\n        check_array(X, dtype='float64')\n", "def test_check_array_dtype_object_with_numeric_cols():\n    # Test that check_array with dtype 'object' converts numeric columns to\n    # object dtype.\n    X = np.array([[1, 2, 'a'], [3, 4, 'b']])\n    X_checked = check_array(X, dtype='object')\n    assert X_checked.dtype == 'object'\n    assert X_checked[0, 0] == 1\n    assert X_checked[0, 1] == 2\n    assert X_checked[0, 2] == 'a'\n    assert isinstance(X_checked[0, 0], int)\n    assert isinstance(X_checked[0, 1], int)\n    assert isinstance(X_checked[0, 2], str)\n", "def test_check_X_y_dtype_object_conversion():\n    # Test that data-frame like objects with dtype object get converted\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.object)\n    y = np.array([0, 1, 2])\n    X_df = MockDataFrame(X)\n    X_checked, y_checked = check_X_y(X_df, y)\n    assert X_checked.dtype.kind == \"f\"\n    assert_array_equal(X_checked, X.astype(np.float64))\n    assert_array_equal(y_checked, y)\n", "def test__num_samples_with_unknown_dtype():\n    # check that _num_samples raises an error for unknown dtype\n    class UnknownDtype:\n            self.shape = (3, 4)\n            self.dtype = 'unknown'\n\n    X = UnknownDtype()\n    with pytest.raises(TypeError, match=\"Expected sequence or array-like\"):\n        _num_samples(X)\n"], "sample_861": ["def test_parameter_sampler_seed_reproducibility():\n    # Test that ParameterSampler with a seed produces the same results on\n    # multiple calls\n    param_distributions = {\"kernel\": [\"rbf\", \"linear\"],\n                           \"C\": uniform(0, 1)}\n    sampler1 = ParameterSampler(param_distributions=param_distributions,\n                                n_iter=10, random_state=0)\n    sampler2 = ParameterSampler(param_distributions=param_distributions,\n                                n_iter=10, random_state=0)\n\n    samples1 = [x for x in sampler1]\n    samples2 = [x for x in sampler2]\n\n    assert samples1 == samples2\n", "def test_grid_search_cv_results_multimetric_with_refit():\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n\n    n_splits = 3\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    scoring = {'accuracy': make_scorer(accuracy_score),\n               'recall': make_scorer(recall_score)}\n\n    grid_search = GridSearchCV(SVC(), cv=n_splits, param_grid=params,\n                               scoring=scoring, refit='accuracy')\n    grid_search.fit(X, y)\n\n    assert grid_search.best_estimator_ is not None\n    assert grid_search.best_index_ is not None\n    assert grid_search.best_params_ is not None\n    assert grid_search.best_score_ is not None\n\n    # Check that best_score_ corresponds to the score of the best_estimator_\n    best_score = grid_search.cv_results_['mean_test_accuracy'][grid_search.best_index_]\n    assert grid_search.best_score_ == best_score\n", "def test_grid_search_cv_results_rank_tie_breaking_with_multiple_metrics():\n    X, y = make_blobs(n_samples=50, random_state=42)\n\n    # The two C values are close enough to give similar models\n    # which would result in a tie of their mean cv-scores\n    param_grid = {'C': [1, 1.001, 0.001]}\n\n    scoring = ('accuracy', 'precision')\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid,\n                               return_train_score=True, scoring=scoring,\n                               refit='accuracy')\n\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    # Check tie breaking strategy -\n    # Check that there is a tie in the mean scores between\n    # candidates 1 and 2 alone\n    assert_almost_equal(cv_results['mean_test_accuracy'][0],\n                        cv_results['mean_test_accuracy'][1])\n    assert_almost_equal(cv_results['mean_train_accuracy'][0],\n                        cv_results['mean_train_accuracy'][1])\n    assert not np.allclose(cv_results['mean_test_accuracy'][1],\n                           cv_results['mean_test_accuracy'][2])\n    assert not np.allclose(cv_results['mean_train_accuracy'][1],\n                           cv_results['mean_train_accuracy'][2])\n    # 'min' rank should be assigned to the tied candidates\n    assert_almost_equal(grid_search.cv_results_['rank_test_accuracy'], [1, 1, 3])\n", "def test_parameter_sampler_sparse_output():\n    # Test that ParameterSampler can handle sparse output from a distribution\n    class SparseDistribution:\n            return sp.csr_matrix([1, 2, 3])\n\n    param_distributions = {\"C\": [0.1, 1], \"sparse_param\": SparseDistribution()}\n    sampler = ParameterSampler(param_distributions, n_iter=10)\n    samples = list(sampler)\n\n    for sample in samples:\n        assert isinstance(sample[\"sparse_param\"], sp.csr_matrix)\n", "def test_grid_search_cv_results_masked_arrays():\n    X, y = make_classification(n_samples=50, n_features=4,\n                               random_state=42)\n\n    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),\n              dict(kernel=['poly', ], degree=[1, 2])]\n\n    grid_search = GridSearchCV(SVC(), cv=3,\n                               param_grid=params, return_train_score=True)\n    grid_search.fit(X, y)\n\n    cv_results = grid_search.cv_results_\n    for key in ['param_C', 'param_degree', 'param_gamma', 'param_kernel']:\n        assert isinstance(cv_results[key], np.ma.MaskedArray)\n\n    # Check that the masked arrays are properly masked\n    for cand_i in range(len(grid_search.cv_results_['params'])):\n        if grid_search.cv_results_['param_kernel'][cand_i] == 'rbf':\n            assert not grid_search.cv_results_['param_C'].mask[cand_i]\n            assert not grid_search.cv_results_['param_gamma'].mask[cand_i]\n            assert grid_search.cv_results_['param_degree'].mask[cand_i]\n        elif grid_search.cv_results_['param_kernel'][cand_i] == 'poly':\n            assert grid_search.cv_results_['param_C'].mask[cand_i]\n            assert grid_search.cv_results_['param_gamma'].mask[cand_i]\n            assert not grid_search.cv_results_['param_degree'].mask[cand_i]\n"], "sample_862": ["def test_vectorizer_stop_words_set(Vectorizer):\n    stop_words = {'the', 'and', 'a'}\n    vectorizer = Vectorizer(stop_words=stop_words)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    feature_names = vectorizer.get_feature_names()\n    assert not set(feature_names).intersection(stop_words)\n", "def test_vectorizer_strip_accents():\n    # Test that accents are stripped from words and characters\n\n    text = \"caf\u00e9, r\u00e9sum\u00e9, na\u00efve, \u00fcber-stra\u00dfe\"\n    expected_text_no_accents = \"cafe, resume, naive, uber-strasse\"\n\n    for analyzer in ['word', 'char']:\n        v = CountVectorizer(analyzer=analyzer, strip_accents='unicode')\n        result = v.build_preprocessor()(text)\n        assert result == expected_text_no_accents\n\n        v = CountVectorizer(analyzer=analyzer, strip_accents='ascii')\n        result = v.build_preprocessor()(text)\n        assert result == expected_text_no_accents\n", "def test_tfidf_transformer_idf_smoothing():\n    # Test that idf smoothing is applied correctly\n    X = [[1, 1, 1],\n         [1, 1, 0],\n         [1, 0, 0]]\n    tr = TfidfTransformer(smooth_idf=True)\n    tfidf = tr.fit_transform(X).toarray()\n    assert_array_almost_equal(tr.idf_, [1. + np.log(4. / (3. + 1.)),\n                                        1. + np.log(4. / (2. + 1.)),\n                                        1. + np.log(4. / (1. + 1.))])\n", "def test_tfidfvectorizer_l1_norm():\n    # Test that the l1 norm is properly applied in TfidfVectorizer\n    X = [\"hello world\", \"hello hello\"]\n    vectorizer = TfidfVectorizer(norm='l1')\n    X_idf = vectorizer.fit_transform(X)\n    assert_array_almost_equal(np.sum(np.abs(X_idf.toarray()), axis=1), [1., 1.])\n", "def test_tfidfvectorizer_vocabulary_update():\n    # Test if vocabulary_ is updated correctly when fit() is called multiple times\n    vectorizer = TfidfVectorizer()\n    docs1 = [\"This is a sample document.\", \"Another example document.\"]\n    docs2 = [\"A new document for updating vocabulary.\"]\n    \n    vectorizer.fit(docs1)\n    vocab_size_before_update = len(vectorizer.vocabulary_)\n    \n    vectorizer.fit(docs2)\n    vocab_size_after_update = len(vectorizer.vocabulary_)\n    \n    assert vocab_size_after_update != vocab_size_before_update\n"], "sample_863": ["def test_pipeline_get_params():\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    params = pipe.get_params(deep=True)\n    assert 'steps' in params\n    assert 'transf' in params\n    assert 'clf' in params\n    assert 'transf__a' in params\n    assert 'clf__should_succeed' in params\n", "def test_pipeline_score_with_sample_weight():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    sample_weight = np.array([0.5, 0.5])\n\n    clf = LogisticRegression()\n    pipe = Pipeline([('clf', clf)])\n\n    pipe.fit(X, y)\n    score_without_sample_weight = pipe.score(X, y)\n\n    pipe.fit(X, y, clf__sample_weight=sample_weight)\n    score_with_sample_weight = pipe.score(X, y, sample_weight=sample_weight)\n\n    assert score_without_sample_weight != score_with_sample_weight\n", "def test_pipeline_get_params_returns_parameter_names():\n    # Test that get_params returns parameter names for each step in the pipeline.\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n    params = pipe.get_params()\n    assert 'transf__a' in params\n    assert 'transf__b' in params\n    assert 'clf__should_succeed' in params\n", "def test_pipeline_fit_params_none():\n    # Test that the pipeline handles fit_params=None correctly\n    X = np.array([[1]])\n    y = np.array([1])\n    pipe = Pipeline([('transf', Transf()), ('clf', FitParamT())])\n\n    # fit_params=None should be handled as an empty dictionary\n    pipe.fit(X, y, **None)\n    assert not hasattr(pipe.named_steps['clf'], 'successful')\n", "def test_feature_union_get_params():\n    # Test that get_params returns all the parameters from all the transformers\n    mult1 = Mult(mult=2)\n    mult2 = Mult(mult=3)\n    fu = FeatureUnion([('mult1', mult1), ('mult2', mult2)])\n    params = fu.get_params(deep=True)\n    assert 'mult1__mult' in params\n    assert 'mult2__mult' in params\n    assert params['mult1__mult'] == 2\n    assert params['mult2__mult'] == 3\n"], "sample_864": ["def test_mean_shift_max_iter():\n    # Test MeanShift algorithm with max_iter parameter\n    ms = MeanShift(bandwidth=1.2, max_iter=5)\n    labels = ms.fit(X).labels_\n    assert hasattr(ms, \"n_iter_\")\n    assert ms.n_iter_ <= 5\n", "def test_mean_shift_max_iter():\n    # Test MeanShift algorithm with max_iter parameter\n    ms = MeanShift(bandwidth=1.2, max_iter=10)\n    labels = ms.fit(X).labels_\n    assert ms.n_iter_ <= 10\n", "def test_meanshift_max_iter():\n    # Test MeanShift with max_iter parameter\n    ms = MeanShift(bandwidth=1.2, max_iter=10)\n    labels = ms.fit(X).labels_\n    assert ms.n_iter_ <= 10\n", "def test_mean_shift_max_iter():\n    # Test MeanShift with max_iter parameter\n    ms = MeanShift(bandwidth=1.2, max_iter=1)\n    labels = ms.fit(X).labels_\n    assert ms.n_iter_ == 1\n", "def test_mean_shift_max_iter():\n    # Test MeanShift algorithm with max_iter parameter\n    ms = MeanShift(bandwidth=1.2, max_iter=1)\n    labels = ms.fit(X).labels_\n    assert ms.n_iter_ <= 1\n\n    ms = MeanShift(bandwidth=1.2, max_iter=1000)\n    labels = ms.fit(X).labels_\n    assert ms.n_iter_ > 1\n"], "sample_865": ["def test_prune_tree_default_ccp_alpha():\n    clf = DecisionTreeClassifier()\n    assert clf.ccp_alpha == 0.0\n\n    clf.set_params(ccp_alpha=0.1)\n    assert clf.ccp_alpha == 0.1\n\n    clf = DecisionTreeRegressor()\n    assert clf.ccp_alpha == 0.0\n\n    clf.set_params(ccp_alpha=0.1)\n    assert clf.ccp_alpha == 0.1\n", "def test_prune_tree_ccp_alpha_not_used_without_fitting():\n    clf = DecisionTreeClassifier(ccp_alpha=0.1)\n    with pytest.raises(NotFittedError):\n        clf._prune_tree()\n", "def test_prune_tree_with_sample_weight():\n    # Test pruning with sample weights.\n    X = np.array([[0, 0], [1, 1]])\n    y = np.array([0, 1])\n    sample_weight = np.array([0.5, 1.0])\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    ccp_path = clf.cost_complexity_pruning_path(X, y, sample_weight=sample_weight)\n\n    for ccp_alpha in ccp_path.ccp_alphas:\n        pruned_clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=0)\n        pruned_clf.fit(X, y, sample_weight=sample_weight)\n\n        assert_is_subtree(clf.tree_, pruned_clf.tree_)\n", "def test_prune_tree_input_dtype():\n    # check prune tree works with different input data types\n    X = np.array([[0, 1], [1, 0]], dtype=np.float32)\n    y = np.array([0, 1])\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    # check the data type of X doesn't change after pruning\n    X_pruned = clf._validate_X_predict(X)\n    assert X_pruned.dtype == X.dtype\n\n    # check prune tree works with different input data types\n    X_float64 = X.astype(np.float64)\n    clf_float64 = DecisionTreeClassifier(random_state=0)\n    clf_float64.fit(X_float64, y)\n\n    assert_array_equal(clf.tree_.apply(X), clf_float64.tree_.apply(X_float64))\n\n    # check prune tree raises an error if the input data type is not supported\n    X_int32 = X.astype(np.int32)\n    clf_int32 = DecisionTreeClassifier(random_state=0)\n    with pytest.raises(ValueError):\n        clf_int32.fit(X_int32, y)\n", "def test_prune_tree_raises_on_unfitted_estimator():\n    clf = DecisionTreeClassifier(ccp_alpha=0.1)\n    msg = \"Estimator needs to be fit before pruning\"\n\n    with pytest.raises(NotFittedError, match=msg):\n        clf._prune_tree()\n"], "sample_866": ["def test_affinity_propagation_sparse_input():\n    # Test that sparse input raises an error when affinity is 'precomputed'\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity='precomputed')\n    with pytest.raises(TypeError):\n        af.fit(X_sparse)\n\n    # Test that sparse input works when affinity is not 'precomputed'\n    af = AffinityPropagation(affinity='euclidean')\n    af.fit(X_sparse)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation()\n    labels_sparse = af.fit_predict(X_sparse)\n\n    af = AffinityPropagation()\n    labels_dense = af.fit_predict(X)\n\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_sparse = af.fit_predict(X_sparse)\n    af_dense = AffinityPropagation(affinity=\"euclidean\")\n    labels_dense = af_dense.fit_predict(X)\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_sparse = af.fit_predict(X_sparse)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X)\n    assert_array_equal(labels, labels_sparse)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation()\n    labels_sparse = af.fit_predict(X_sparse)\n\n    af_dense = AffinityPropagation()\n    labels_dense = af_dense.fit_predict(X)\n\n    assert_array_equal(labels_sparse, labels_dense)\n"], "sample_867": ["def test_parameter_sampler_sparse_matrices():\n    # Test ParameterSampler with sparse matrices in the parameter grid\n    param_grid = {\"kernel\": [\"rbf\", \"linear\"],\n                  \"C\": [sp.csr_matrix(np.array([1, 2, 3]))],\n                  \"gamma\": sp.csr_matrix(np.array([0.1, 1]))}\n\n    sampler = ParameterSampler(param_grid, n_iter=10, random_state=0)\n    samples = [x for x in sampler]\n    assert len(samples) == 10\n    for sample in samples:\n        assert sample[\"kernel\"] in [\"rbf\", \"linear\"]\n        assert np.allclose(sample[\"C\"].toarray(), [1, 2, 3])\n        assert np.allclose(sample[\"gamma\"].toarray(), [0.1, 1])\n", "def test_parameter_sampler_distribution():\n    # Test ParameterSampler with different distributions\n    param_distributions = {\"kernel\": [\"rbf\", \"linear\"],\n                           \"C\": uniform(0, 1),\n                           \"gamma\": expon(scale=0.1)}\n\n    sampler = ParameterSampler(param_distributions, n_iter=10, random_state=0)\n    samples = [x for x in sampler]\n    assert len(samples) == 10\n    for sample in samples:\n        assert sample[\"kernel\"] in [\"rbf\", \"linear\"]\n        assert 0 <= sample[\"C\"] <= 1\n        assert sample[\"gamma\"] >= 0\n\n    param_distributions = {\"kernel\": [\"rbf\", \"linear\"],\n                           \"C\": uniform(0, 1),\n                           \"gamma\": \"scale\"}\n    with pytest.raises(TypeError):\n        ParameterSampler(param_distributions, n_iter=10, random_state=0)\n", "def test_search_cv__estimator_type_property_delegated_to_base_estimator():\n    \"\"\"\n    Test implementation of BaseSearchCV has the _estimator_type property\n    which matches the _estimator_type property of its estimator.\n    This test make sure _estimator_type is delegated to the base estimator.\n\n    Non-regression test for issue #13920.\n    \"\"\"\n    est = BaseEstimator()\n    attr_message = \"BaseSearchCV _estimator_type property must match estimator\"\n\n    for _estimator_type_setting in ['classifier', 'regressor']:\n        setattr(est, '_estimator_type', _estimator_type_setting)\n        cv = GridSearchCV(est, {'n_neighbors': [10]})\n        assert _estimator_type_setting == cv._estimator_type, attr_message\n", "def test_grid_search_cv_results_dtype():\n    # Test that the dtypes of the cv_results_ are correct\n    X, y = make_classification(n_samples=50, n_features=4, random_state=42)\n    param_grid = {'C': [1, 2, 3]}\n    grid_search = GridSearchCV(SVC(), param_grid=param_grid, cv=3,\n                               return_train_score=True)\n    grid_search.fit(X, y)\n    cv_results = grid_search.cv_results_\n    assert all(cv_results[key].dtype == np.float64\n               for key in ('mean_fit_time', 'std_fit_time',\n                           'mean_score_time', 'std_score_time'))\n    assert all(cv_results[key].dtype == np.int32\n               for key in ('rank_test_score',))\n    assert all(cv_results[key].dtype == np.float64\n               for key in ('mean_test_score', 'std_test_score',\n                           'mean_train_score', 'std_train_score'))\n    assert all(cv_results[key].dtype == object\n               for key in ('params', 'param_C'))\n", "def test_grid_search_with_masked_array_target():\n    # Test GridSearchCV with masked array as target\n    X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    y = np.ma.array([1, 1, 2, 2], mask=[False, False, False, True])\n    clf = MockClassifier()\n    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=3)\n    grid_search.fit(X, y)\n    assert hasattr(grid_search, \"cv_results_\")\n"], "sample_868": ["def test_empty_labels(metric_name):\n    # All clustering metrics should be able to handle empty labels arrays\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0\n    if metric_name in SYMMETRIC_METRICS:\n        assert metric([1, 2, 3], []) == 0.0\n        assert metric([], [1, 2, 3]) == 0.0\n    else:\n        try:\n            metric([1, 2, 3], [])\n        except ValueError:\n            pass\n        else:\n            assert False, f\"{metric_name} should raise ValueError on empty labels\"\n        try:\n            metric([], [1, 2, 3])\n        except ValueError:\n            pass\n        else:\n            assert False, f\"{metric_name} should raise ValueError on empty labels\"\n", "def test_empty_input(metric_name):\n    # All supervised clustering metrics should be able to handle empty input\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0\n", "def test_empty_input(metric_name):\n    # Test that all supervised metrics can handle empty input arrays\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0\n    with pytest.raises(ValueError):\n        metric([1], [])\n    with pytest.raises(ValueError):\n        metric([], [1])\n", "def test_empty_input(metric_name):\n    # Test that all supervised metrics can handle empty inputs\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0\n    assert metric([1, 2, 3], []) == 1.0\n    assert metric([], [1, 2, 3]) == 1.0\n", "def test_empty_labels(metric_name):\n    # All supervised clustering metrics should be able to handle empty labels\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0\n    with pytest.raises(ValueError):\n        metric([0, 0, 1], [])\n    with pytest.raises(ValueError):\n        metric([], [0, 0, 1])\n"], "sample_870": ["def test_predict_with_prior_multioutput():\n    \"\"\"Check the output shape of `predict` with prior distribution for multi-output.\"\"\"\n    rng = np.random.RandomState(1024)\n\n    n_sample = 10\n    n_features = 3\n    n_targets = 2\n\n    X = rng.randn(n_sample, n_features)\n\n    model = GaussianProcessRegressor(n_targets=n_targets)\n    mean_prior, cov_prior = model.predict(X, return_cov=True)\n    _, std_prior = model.predict(X, return_std=True)\n\n    assert mean_prior.shape == (n_sample, n_targets)\n    assert cov_prior.shape == (n_sample, n_sample, n_targets)\n    assert std_prior.shape == (n_sample, n_targets)\n", "def test_custom_kernel_diag():\n    # Test that a custom kernel with a diag method does not mutate the input\n    X = np.array([[1, 2], [3, 4]])\n    kernel = CustomKernel()\n    diag_values = kernel.diag(X)\n    assert_array_almost_equal(diag_values, X[:, 0])\n    assert_array_almost_equal(X, np.array([[1, 2], [3, 4]]))\n", "def test_kernel_diag_not_mutating_input():\n    \"\"\"Check that the diag method of the kernel does not mutate the input.\"\"\"\n    X = np.array([[1.0, 2.0], [3.0, 4.0]])\n    kernel = CustomKernel()\n    kernel.diag(X)\n    assert_array_almost_equal(X, np.array([[1.0, 2.0], [3.0, 4.0]]))\n", "def test_kernel_property_is_not_mutated():\n    \"\"\"Check that the input matrix X is not mutated when passed to a custom kernel.\"\"\"\n    X = np.array([[1.0, 2.0], [3.0, 4.0]])\n    X_original = X.copy()\n\n    gpr = GaussianProcessRegressor(kernel=CustomKernel())\n    gpr.fit(X, np.array([1.0, 2.0]))\n\n    assert_array_almost_equal(X, X_original)\n", "def test_copy_X_train():\n    \"\"\"Check that the input data X is not mutated when copy_X_train=False.\"\"\"\n    kernel = RBF()\n    X = np.array([[1, 2], [3, 4]])\n\n    # Create a wrapper around the CustomKernel to check if the diag method\n    # receives the original or a modified version of X.\n    class CustomRBF(RBF):\n            assert_array_almost_equal(X, np.array([[1, 2], [3, 4]]))\n            return super().diag(X)\n\n    gpr = GaussianProcessRegressor(kernel=CustomRBF(), copy_X_train=False)\n    gpr.fit(X, np.array([1, 2]))\n\n    X[0, 0] += 10\n\n    # If copy_X_train=False, then diag should receive the mutated version of X\n    with pytest.raises(AssertionError):\n        gpr.predict(np.array([[1, 2]]), return_std=True)\n\n    gpr = GaussianProcessRegressor(kernel=CustomRBF(), copy_X_train=True)\n    gpr.fit(X, np.array([1, 2]))\n\n    X[0, 0] += 10\n\n    # If copy_X_train=True, then diag should receive the original version of X\n    gpr.predict(np.array([[1, 2]]), return_std=True)\n"], "sample_871": ["def test_calinski_harabasz_score_sparse():\n    # Test Calinski-Harabasz score with sparse matrices\n    from scipy.sparse import csr_matrix\n\n    X = csr_matrix(np.array([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5))\n    labels = [0] * 10 + [1] * 10\n    pytest.approx(calinski_harabasz_score(X, labels), 45 * (40 - 4) / (5 * (4 - 1)))\n", "def test_calinski_harabasz_score_sparse():\n    # Assert the function works with sparse matrices\n    from scipy.sparse import csr_matrix\n\n    X = csr_matrix(np.array([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5))\n    labels = [0] * 10 + [1] * 10\n    pytest.approx(calinski_harabasz_score(X, labels), 45 * (40 - 4) / (5 * (4 - 1)))\n", "def test_calinski_harabasz_score_sparse_input():\n    # Assert the function works with sparse matrices\n    from scipy.sparse import csr_matrix\n\n    X = csr_matrix(np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]))\n    labels = [0, 0, 0, 1, 1, 1]\n    score = calinski_harabasz_score(X, labels)\n    assert score > 0\n", "def test_calinski_harabasz_score_sparse_input():\n    # Test the function with sparse input\n    from scipy.sparse import csr_matrix\n\n    X = np.array([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5)\n    labels = [0] * 10 + [1] * 10\n    X_sparse = csr_matrix(X)\n\n    score_dense = calinski_harabasz_score(X, labels)\n    score_sparse = calinski_harabasz_score(X_sparse, labels)\n\n    pytest.approx(score_dense, score_sparse)\n", "def test_davies_bouldin_score_non_finite_values():\n    # Test that davies_bouldin_score function handles non-finite values correctly.\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n    labels = np.array([0, 0, 1, 1])\n\n    # Introduce a NaN value in the array\n    X[0, 0] = np.nan\n\n    with pytest.raises(ValueError, match=\"Array contains non-finite values\"):\n        davies_bouldin_score(X, labels)\n\n    # Introduce an infinite value in the array\n    X[0, 0] = np.inf\n\n    with pytest.raises(ValueError, match=\"Array contains non-finite values\"):\n        davies_bouldin_score(X, labels)\n"], "sample_872": ["def test_roc_auc_score_average_binary():\n    # Test using average with binary classification\n    y_true = [0, 1]\n    y_score = [[0.5, 0.5], [0, 1]]\n    assert_almost_equal(roc_auc_score(y_true, y_score[:, 1]), 1)\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_score, average=\"macro\")\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_score, average=\"weighted\")\n", "def test_top_k_accuracy_score_multilabel_error():\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_score = np.array([[0.4, 0.3, 0.2], [0.1, 0.4, 0.3]])\n\n    msg = r\"`y_type` must be 'binary' or 'multiclass', got 'multilabel-indicator'.\"\n    with pytest.raises(ValueError, match=msg):\n        top_k_accuracy_score(y_true, y_score, k=2)\n", "def test_roc_curve_pos_label_errors():\n    # Raise an error when pos_label is not in y_true for binary classification\n    y_true = np.array([0, 1])\n    y_pred = np.array([0, 1])\n    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\"\n    with pytest.raises(ValueError, match=err_msg):\n        roc_curve(y_true, y_pred, pos_label=2)\n\n    # Raise an error when pos_label is not specified for non-binary classification\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([0, 1, 2])\n    err_msg = (\n        \"y_true takes value in {0, 1, 2} and pos_label is \"\n        \"not specified: either make y_true take \"\n        \"value in {0, 1} or {-1, 1} or pass pos_label \"\n        \"explicitly.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        roc_curve(y_true, y_pred)\n", "def test_top_k_accuracy_score_binary_average_parameter():\n    y_true = [0, 0, 1, 1]\n    y_score = np.array([0.2, 0.3, 0.7, 0.5])\n\n    # Binary classification is not affected by the average parameter\n    assert_almost_equal(\n        top_k_accuracy_score(y_true, y_score, k=2, average=\"macro\"),\n        top_k_accuracy_score(y_true, y_score, k=2, average=None),\n    )\n    assert_almost_equal(\n        top_k_accuracy_score(y_true, y_score, k=2, average=\"weighted\"),\n        top_k_accuracy_score(y_true, y_score, k=2, average=None),\n    )\n    assert_almost_equal(\n        top_k_accuracy_score(y_true, y_score, k=2, average=\"micro\"),\n        top_k_accuracy_score(y_true, y_score, k=2, average=None),\n    )\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function returns an error when the 'average'\n    # parameter is not one of [None, 'macro', 'weighted']\n    rng = check_random_state(404)\n    y_score = rng.rand(20, 3)\n    y_prob = softmax(y_score)\n    y_true = rng.randint(0, 3, size=20)\n\n    # average should be one of ['macro', 'weighted', None]\n    msg = (\n        \"average must be one of ('macro', 'weighted', None) for multiclass \"\n        \"problems\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        roc_auc_score(y_true, y_prob, average=\"micro\")\n    with pytest.raises(ValueError, match=msg):\n        roc_auc_score(y_true, y_prob, average=\"samples\")\n    with pytest.raises(ValueError, match=msg):\n        roc_auc_score(y_true, y_prob, average=\"unknown\")\n"], "sample_873": ["def test_get_support_empty():\n    sel = StepSelector(step=0)\n    sel.fit(X, y)\n    assert_array_equal(np.zeros(X.shape[1], dtype=bool), sel.get_support())\n    assert_array_equal(np.array([]), sel.get_support(indices=True))\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[\"bad\", \"feature\", \"names\"])\n", "def test_get_support_mask_not_fitted():\n    sel = StepSelector()\n    with pytest.raises(ValueError):\n        sel.get_support()\n    with pytest.raises(ValueError):\n        sel.get_support(indices=True)\n", "def test_get_support_empty():\n    sel = StepSelector(step=0)\n    sel.fit(X, y)\n    assert_array_equal(np.zeros(X.shape[1], dtype='bool), sel.get_support())\n    assert_array_equal([], sel.get_support(indices=True))\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[\"wrong\", \"names\"])\n"], "sample_874": ["def test_clone():\n    sel = StepSelector(step=3)\n    clone_sel = sel.clone()\n    assert isinstance(clone_sel, StepSelector)\n    assert clone_sel.step == 3\n\n    # Check clone works with fit object\n    sel.fit(X, y)\n    clone_sel_fit = sel.clone()\n    assert isinstance(clone_sel_fit, StepSelector)\n    assert clone_sel_fit.step == 3\n    assert hasattr(clone_sel_fit, 'n_features_in_')\n", "def test_clone_selector():\n    sel = StepSelector()\n    sel.fit(X, y)\n    cloned_sel = sel.__sklearn_clone__()\n    assert_array_equal(sel.get_support(), cloned_sel.get_support())\n    assert_array_equal(sel.get_support(indices=True), cloned_sel.get_support(indices=True))\n", "def test_validate_data():\n    class DummyEstimator(BaseEstimator):\n            X = self._validate_data(X, y)\n            return self\n\n    estimator = DummyEstimator()\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    # Test with X and y\n    estimator.fit(X, y)\n\n    # Test with X only\n    estimator.fit(X)\n\n    # Test with y only\n    with pytest.raises(ValueError):\n        estimator.fit(y=y)\n\n    # Test with neither X nor y\n    with pytest.raises(ValueError):\n        estimator.fit()\n\n    # Test with invalid input type for X\n    with pytest.raises(TypeError):\n        estimator.fit(X=\"invalid\", y=y)\n\n    # Test with invalid input type for y\n    with pytest.raises(TypeError):\n        estimator.fit(X=X, y=\"invalid\")\n", "def test_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check default output when input_features is None\n    feature_names_out_actual = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n    # Check invalid input_features raises error\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(feature_names[:5])\n", "def test_clone_selector():\n    sel = StepSelector(step=3)\n    clone_sel = sel.__sklearn_clone__()\n    assert isinstance(clone_sel, StepSelector)\n    assert clone_sel.step == sel.step\n    assert clone_sel.get_support() == sel.get_support()\n"], "sample_876": ["def test_mlp_classifier_multiclass():\n    # Test that MLPClassifier works as expected on multiclass classification.\n    X, y = load_iris(return_X_y=True)\n    clf = MLPClassifier(hidden_layer_sizes=(10,), random_state=0)\n    clf.fit(X, y)\n    assert clf.score(X, y) > 0.95\n", "def test_mlp_n_features_in():\n    # Test that n_features_in_ is set correctly\n    X = np.array([[0.6, 0.8, 0.7]])\n    y = np.array([0])\n    mlp = MLPClassifier(hidden_layer_sizes=(10,))\n    mlp.fit(X, y)\n    assert mlp.n_features_in_ == X.shape[1]\n", "def test_mlp_multiclass_n_outputs():\n    \"\"\"Check that multiclass multi-output problems are handled correctly\"\"\"\n    X, y = make_classification(n_samples=200, n_features=10, n_informative=5, n_redundant=0,\n                               n_repeated=0, n_classes=3, random_state=1)\n    y = np.column_stack((y, np.mod(y + 1, 3)))\n\n    mlp = MLPClassifier(hidden_layer_sizes=(20,), random_state=1, max_iter=1000)\n    mlp.fit(X, y)\n\n    assert mlp.coefs_[0].shape[0] == 10\n    assert mlp.coefs_[0].shape[1] == 20\n    assert mlp.coefs_[1].shape[0] == 20\n    assert mlp.coefs_[1].shape[1] == 6\n\n    y_pred = mlp.predict(X)\n    assert y_pred.shape[1] == 2\n", "def test_mlp_classifier_predict_proba_unseen_labels():\n    # Test that predict_proba raises an error when the model is presented with unseen labels.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    clf = MLPClassifier(hidden_layer_sizes=5, activation=\"logistic\", random_state=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n\n    y_new = np.array([2] * 10)  # unseen label\n    X_new = np.random.rand(10, X.shape[1])\n\n    msg = (\n        \"The model was fitted on the following classes: [0, 1], but the data contains the class: 2\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        clf.predict_proba(np.vstack((X, X_new)), np.hstack((y, y_new)))\n", "def test_mlp_n_iter_no_change_with_early_stopping():\n    \"\"\"Check that n_iter_no_change works with early stopping.\"\"\"\n    mlp = MLPClassifier(\n        max_iter=1000,\n        random_state=0,\n        early_stopping=True,\n        validation_fraction=0.2,\n        n_iter_no_change=5,\n    )\n    mlp.fit(X_iris, y_iris)\n    assert mlp.n_iter_ < mlp.max_iter\n    assert len(mlp.validation_scores_) == mlp.n_iter_\n"], "sample_877": ["def test_isotonic_regression_feature_names_out_dtype():\n    \"\"\"Check `get_feature_names_out` returns numpy array of object dtype.\"\"\"\n    X = np.arange(10)\n    y = np.arange(10)\n\n    iso = IsotonicRegression().fit(X, y)\n    names = iso.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n", "def test_isotonic_regression_get_params():\n    \"\"\"Check `get_params` method of `IsotonicRegression`.\"\"\"\n    ir = IsotonicRegression(y_min=0.0, y_max=1.0, increasing=True, out_of_bounds=\"clip\")\n    params = ir.get_params()\n    expected_params = {\n        \"y_min\": 0.0,\n        \"y_max\": 1.0,\n        \"increasing\": True,\n        \"out_of_bounds\": \"clip\",\n    }\n    assert params == expected_params\n", "def test_isotonic_regression_feature_names_out_no_fit():\n    \"\"\"Check `get_feature_names_out` for `IsotonicRegression` without fit.\"\"\"\n    iso = IsotonicRegression()\n    msg = \"This IsotonicRegression instance is not fitted yet\"\n    with pytest.raises(NotFittedError, match=msg):\n        iso.get_feature_names_out()\n", "def test_isotonic_regression_output_dtype():\n    \"\"\"Check the output dtype of IsotonicRegression.\"\"\"\n    X = np.arange(10, dtype=np.float32)\n    y = np.arange(10, dtype=np.float32)\n\n    iso = IsotonicRegression()\n    iso.fit(X, y)\n    assert iso.predict(X).dtype == np.float32\n\n    X = np.arange(10, dtype=np.float64)\n    y = np.arange(10, dtype=np.float64)\n\n    iso = IsotonicRegression()\n    iso.fit(X, y)\n    assert iso.predict(X).dtype == np.float64\n", "def test_isotonic_regression_increasing_auto_with_constant_values():\n    # Test isotonic regression with constant values and increasing=\"auto\"\n    X = np.arange(10)\n    y = np.repeat(5, 10)\n\n    iso_reg = IsotonicRegression(increasing=\"auto\")\n    iso_reg.fit(X, y)\n\n    assert iso_reg.increasing_ == True\n    assert_array_equal(iso_reg.predict(X), np.repeat(5, 10))\n"], "sample_878": ["def test_column_transformer_set_output_with_dataframe_feature_names():\n    \"\"\"Check that set_output(transform='pandas') uses dataframe feature names.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        [(\"first\", TransWithNames([\"new_a\", \"new_b\"]), [\"a\", \"b\"])]\n    ).set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(df)\n\n    expected_columns = [\"first__new_a\", \"first__new_b\", \"c\", \"d\"]\n    assert_array_equal(X_trans.columns, expected_columns)\n", "def test_column_transformer_set_output_with_dataframe_columns():\n    \"\"\"Check that set_output(transform=\"pandas\") works with DataFrame columns.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n        }\n    )\n\n    ct = ColumnTransformer(\n        [\n            (\n                \"encode\",\n                OneHotEncoder(sparse_output=False),\n                [\"pet\"],\n            ),\n            (\"scale\", StandardScaler(), [\"age\"]),\n        ],\n        remainder=\"passthrough\",\n    )\n\n    ct.fit(df)\n    X_trans_df = ct.set_output(transform=\"pandas\").transform(df)\n\n    assert isinstance(X_trans_df, pd.DataFrame)\n    assert_array_equal(X_trans_df.columns, ct.get_feature_names_out())\n", "def test_column_transformer_feature_names_out_non_fitted():\n    \"\"\"Check get_feature_names_out raises an error if not fitted.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n\n    ct = ColumnTransformer(\n        [(\"first\", TransWithNames(), [\"a\", \"c\"]), (\"second\", TransWithNames(), [\"d\"])],\n    )\n\n    msg = re.escape(\n        \"This ColumnTransformer instance is not fitted yet. Call 'fit' with \"\n        \"appropriate arguments before using this estimator.\"\n    )\n    with pytest.raises(NotFittedError, match=msg):\n        ct.get_feature_names_out()\n", "def test_column_transformer_set_output_with_function_transformer():\n    \"\"\"Check column transformer behavior with FunctionTransformer and set_output.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"], index=[10])\n\n        return X\n\n    ct = ColumnTransformer(\n        [(\"first\", FunctionTransformer(func), [\"a\", \"c\"]), (\"second\", TransWithNames(), [\"d\"])],\n        remainder=\"passthrough\",\n    )\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, np.ndarray)\n\n    ct.set_output(transform=\"pandas\")\n\n    df_test = pd.DataFrame([[1, 2, 3, 4]], columns=df.columns, index=[20])\n    X_trans = ct.transform(df_test)\n    assert isinstance(X_trans, pd.DataFrame)\n\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(X_trans.columns, feature_names_out)\n    assert_array_equal(X_trans.index, df_test.index)\n", "def test_column_transformer_get_feature_names_out_with_numpy_input():\n    \"\"\"Check get_feature_names_out with numpy input.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    ct = ColumnTransformer(\n        [(\"trans\", TransWithNames([\"a\", \"b\"]), [0, 1])],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X)\n    assert_array_equal(ct.get_feature_names_out(), [\"a\", \"b\"])\n"], "sample_879": ["def test_ordinal_encoder_drop():\n    \"\"\"Check drop parameter of OrdinalEncoder\"\"\"\n    X = np.array([[\"a\"], [\"b\"], [\"c\"]])\n    oe = OrdinalEncoder(drop=\"first\").fit(X)\n    assert_array_equal(oe.categories_, [[\"a\", \"b\", \"c\"]])\n    assert_array_equal(oe.transform(X), [[0], [1], [2]])\n\n    oe = OrdinalEncoder(drop=[\"a\"]).fit(X)\n    assert_array_equal(oe.categories_, [[\"a\", \"b\", \"c\"]])\n    assert_array_equal(oe.transform(X), [[0], [1], [2]])\n\n    oe = OrdinalEncoder(drop=\"if_binary\").fit(X)\n    assert_array_equal(oe.categories_, [[\"a\", \"b\", \"c\"]])\n    assert_array_equal(oe.transform(X), [[0], [1], [2]])\n", "def test_one_hot_encoder_feature_names_with_drop():\n    enc = OneHotEncoder(drop=\"first\")\n    X = [\n        [\"Male\", 1, \"girl\", 2, 3],\n        [\"Female\", 41, \"girl\", 1, 10],\n        [\"Male\", 51, \"boy\", 12, 3],\n        [\"Male\", 91, \"girl\", 21, 30],\n    ]\n\n    enc.fit(X)\n    feature_names = enc.get_feature_names_out()\n\n    assert_array_equal(\n        [\n            \"x0_Female\",\n            \"x1_41\",\n            \"x1_51\",\n            \"x1_91\",\n            \"x2_boy\",\n            \"x3_1\",\n            \"x3_12\",\n            \"x3_21\",\n            \"x4_10\",\n            \"x4_30\",\n        ],\n        feature_names,\n    )\n\n    feature_names2 = enc.get_feature_names_out([\"one\", \"two\", \"three\", \"four\", \"five\"])\n\n    assert_array_equal(\n        [\n            \"one_Female\",\n            \"two_41\",\n            \"two_51\",\n            \"two_91\",\n            \"three_boy\",\n            \"four_1\",\n            \"four_12\",\n            \"four_21\",\n            \"five_10\",\n            \"five_30\",\n        ],\n        feature_names2,\n    )\n", "def test_ordinal_encoder_get_feature_names_out_numpy():\n    \"\"\"Check feature names out is same as the input.\"\"\"\n    X = np.array([[1, 2, 3]])\n    enc = OrdinalEncoder().fit(X)\n\n    feature_names_out = enc.get_feature_names_out()\n    assert_array_equal([\"x0\", \"x1\", \"x2\"], feature_names_out)\n", "def test_one_hot_encoder_output_dtype():\n    \"\"\"Check that the output dtype of OneHotEncoder is correct.\"\"\"\n    X = np.array([[\"a\", \"b\"], [\"c\", \"d\"]])\n    ohe = OneHotEncoder(dtype=np.float32)\n    X_trans = ohe.fit_transform(X)\n    assert X_trans.dtype == np.float32\n\n    ohe = OneHotEncoder(dtype=np.int64)\n    X_trans = ohe.fit_transform(X)\n    assert X_trans.dtype == np.int64\n", "def test_one_hot_encoder_infrequent_categories_none():\n    \"\"\"Check infrequent categories are None when no infrequent categories.\"\"\"\n    X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 6]).T\n    ohe = OneHotEncoder(\n        handle_unknown=\"infrequent_if_exist\", sparse_output=False, max_categories=4\n    ).fit(X)\n    assert_array_equal(ohe.infrequent_categories_, [None])\n"], "sample_880": ["def test_ovr_decision_function():\n    # Test _ovr_decision_function with a simple example\n    predictions = np.array([[0, 1, 0], [1, 0, 1]])\n    confidences = np.array([[0.2, 0.8, 0.4], [0.7, 0.3, 0.9]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    expected_decision = np.array([[0.06666667, 0.73333333, 0.2], [0.56666667, 0.13333333, 0.3]])\n\n    assert_allclose(decision, expected_decision)\n\n    # Test with another example where all predictions are correct\n    predictions = np.array([[0, 1, 1], [1, 0, 0]])\n    confidences = np.array([[0.1, 0.9, 0.8], [0.8, 0.2, 0.1]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    expected_decision = np.array([[0.03333333, 0.86666667, 0.1], [0.7, 0.06666667, -0.23333333]])\n\n    assert_allclose(decision, expected_decision)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 1], [1, 0, 1]])\n    confidences = np.array([[0.2, 0.8, 0.7], [0.9, 0.1, 0.6]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    expected_decision = np.array([[0.06666667, 0.86666667, 0.56666667],\n                                  [0.63333333, 0.13333333, 0.36666667]])\n\n    assert_allclose(decision, expected_decision)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 0], [1, 0, 1]])\n    confidences = np.array([[0.2, 0.8, 0.3], [0.7, 0.4, 0.9]])\n    n_classes = 3\n\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_decision_function = np.array(\n        [[-0.6, 0.5, -0.2], [0.3, -0.3, 0.6]]\n    )\n\n    assert_allclose(decision_function, expected_decision_function)\n", "def test_ovr_decision_function():\n    # Test _ovr_decision_function with a simple example\n\n    predictions = np.array([[0, 1, 0], [1, 0, 1]])\n    confidences = np.array([[0.2, 0.8, 0.3], [0.7, 0.4, 0.9]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_decision = np.array([[0.06666667, 0.83333333, 0.1], [0.63333333, 0.36666667, 0.76666667]])\n\n    assert_allclose(decision, expected_decision)\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 0], [1, 0, 1]])\n    confidences = np.array([[0.3, 0.7, 0.2], [0.6, 0.4, 0.8]])\n    n_classes = 3\n\n    decision = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision.shape == (2, 3)\n\n    # Test that the decision function is a continuous, tie-breaking OvR decision function\n    for i in range(2):\n        for j in range(3):\n            assert -1 <= decision[i, j] <= 1\n"], "sample_881": ["def test_dcg_score_log_base():\n    # Test dcg_score with different log bases\n    _, y_true = make_multilabel_classification(random_state=0, n_classes=10)\n    y_score = -y_true + 1\n\n    dcg_2 = dcg_score(y_true, y_score, log_base=2)\n    dcg_e = dcg_score(y_true, y_score, log_base=np.e)\n    dcg_10 = dcg_score(y_true, y_score, log_base=10)\n\n    assert dcg_2 != dcg_e\n    assert dcg_2 != dcg_10\n    assert dcg_e != dcg_10\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function handles the average parameter correctly.\n    y_true = np.array([0, 1, 2, 2])\n    y_pred = np.array([[0.1, 0.8, 0.1], [0.3, 0.4, 0.3], [0.35, 0.5, 0.15], [0, 0.2, 0.8]])\n\n    # Test that average='macro' is the default behavior\n    macro_roc_auc = roc_auc_score(y_true, y_pred, multi_class=\"ovr\", average=\"macro\")\n    assert_almost_equal(roc_auc_score(y_true, y_pred, multi_class=\"ovr\"), macro_roc_auc)\n\n    # Test that average='weighted' works as expected\n    weighted_roc_auc = roc_auc_score(y_true, y_pred, multi_class=\"ovr\", average=\"weighted\")\n    assert_not_almost_equal(weighted_roc_auc, macro_roc_auc)\n", "def test_label_ranking_avg_precision_score_with_tied_values():\n    # Test that label_ranking_average_precision_score handles tied values correctly.\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.5, 0.6], [0, 0, 1]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(2 / 3)\n", "def test_average_precision_score_pos_label():\n    # Test average_precision_score with pos_label\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_score = np.array([0.1, 0.8, 0.4, 0.3, 0.9, 0.6])\n\n    # binary classification\n    y_true_binary = np.array([y in [1] for y in y_true])\n    assert_almost_equal(\n        average_precision_score(y_true, y_score, pos_label=1),\n        average_precision_score(y_true_binary, y_score),\n    )\n\n    # multiclass classification with label indicator matrix\n    y_true_multiclass = label_binarize(y_true, classes=[0, 1, 2])\n    assert_array_almost_equal(\n        average_precision_score(y_true, y_score, pos_label=1, average=None),\n        average_precision_score(y_true_multiclass[:, 1], y_score, average=None),\n    )\n\n    # same as above with different pos_label\n    assert_array_almost_equal(\n        average_precision_score(y_true, y_score, pos_label=2, average=None),\n        average_precision_score(y_true_multiclass[:, 2], y_score, average=None),\n    )\n", "def test_top_k_accuracy_score_sample_weight():\n    y_true = np.array([0, 1, 2, 3])\n    y_score = np.array(\n        [\n            [0.4, 0.3, 0.2, 0.1],\n            [0.1, 0.4, 0.3, 0.2],\n            [0.2, 0.1, 0.4, 0.3],\n            [0.3, 0.2, 0.1, 0.4],\n        ]\n    )\n    sample_weight = np.array([0.5, 1.0, 1.0, 0.5])\n    score = top_k_accuracy_score(y_true, y_score, k=2, sample_weight=sample_weight)\n    assert score == pytest.approx(0.6)\n"], "sample_882": ["def test_mlp_classifier_get_params():\n    # Test that get_params returns all parameters and they are correctly set.\n    mlp = MLPClassifier()\n    params = mlp.get_params()\n\n    for param_name, value in params.items():\n        assert hasattr(mlp, param_name)\n        assert getattr(mlp, param_name) == value\n\n    # Ensure that `warm_start` parameter is included\n    assert \"warm_start\" in params\n", "def test_mlp_n_outputs_attribute():\n    # Check that the `n_outputs_` attribute is set correctly for different\n    # problem types (classification, regression, multi-output regression)\n\n    # Classification\n    mlp = MLPClassifier()\n    mlp.fit(X_digits_binary, y_digits_binary)\n    assert mlp.n_outputs_ == 1\n\n    # Regression\n    mlp = MLPRegressor()\n    mlp.fit(X_reg, y_reg)\n    assert mlp.n_outputs_ == 1\n\n    # Multi-output regression\n    X, y = make_regression(n_samples=100, n_features=10, n_targets=3)\n    mlp = MLPRegressor()\n    mlp.fit(X, y)\n    assert mlp.n_outputs_ == 3\n", "def test_mlp_n_iter_no_change_reset_on_warm_start():\n    \"\"\"Check that `n_iter_no_change` counter is reset on warm start.\"\"\"\n    mlp = MLPClassifier(max_iter=10, random_state=0, warm_start=True)\n    mlp.fit(X_iris, y_iris)\n    n_iter_no_change = mlp._no_improvement_count\n\n    # change max_iter to trigger another fit iteration\n    mlp.set_params(max_iter=20)\n    mlp.fit(X_iris, y_iris)\n\n    assert mlp._no_improvement_count < n_iter_no_change\n", "def test_mlp_classifier_multiclass_n_outputs():\n    \"\"\"Check that MLPClassifier can handle multi-class classification with more than 2 classes.\"\"\"\n    X, y = load_iris(return_X_y=True)\n    clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)\n    clf.fit(X, y)\n    assert clf.score(X, y) > 0.95\n    assert clf.n_outputs_ == 3\n    assert clf.classes_.shape[0] == 3\n    assert clf.coefs_[1].shape[1] == 3\n", "def test_mlp_classifier_multiclass_loss():\n    # Test that the multiclass log loss is computed correctly\n    X, y = make_classification(n_samples=50, n_features=10, n_informative=5, n_redundant=0,\n                               n_classes=3, random_state=0)\n    mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, random_state=0)\n    mlp.fit(X, y)\n    y_pred = mlp.predict_proba(X)\n    assert_allclose(mlp.loss_, -np.mean(np.log(y_pred[np.arange(len(y)), y])))\n"], "sample_883": ["def test_bayesian_ridge_ard_max_iter():\n    \"\"\"Check that the 'max_iter' parameter is respected.\"\"\"\n    X, y = diabetes.data, diabetes.target\n\n    # Check that max_iter is not exceeded\n    model = BayesianRidge(max_iter=5)\n    model.fit(X, y)\n    assert model.n_iter_ <= 5\n\n    model = ARDRegression(max_iter=5)\n    model.fit(X, y)\n    assert model.n_iter_ <= 5\n\n    # Check that max_iter does not cause an infinite loop\n    model = BayesianRidge(max_iter=0)\n    with pytest.raises(ValueError):\n        model.fit(X, y)\n\n    model = ARDRegression(max_iter=0)\n    with pytest.raises(ValueError):\n        model.fit(X, y)\n", "def test_bayesian_ridge_ard_verbose():\n    \"\"\"Check that the verbose mode does not crash.\"\"\"\n    X, y = diabetes.data, diabetes.target\n\n    BayesianRidge(verbose=True).fit(X, y)\n    ARDRegression(verbose=True).fit(X, y)\n", "def test_bayesian_ridge_ard_attributes():\n    # Test that all attributes are set after fitting the model\n    X, y = diabetes.data, diabetes.target\n\n    for Estimator in [BayesianRidge, ARDRegression]:\n        model = Estimator()\n        model.fit(X, y)\n\n        assert hasattr(model, \"coef_\")\n        assert hasattr(model, \"intercept_\")\n        assert hasattr(model, \"alpha_\")\n        assert hasattr(model, \"lambda_\")\n        assert hasattr(model, \"sigma_\")\n        assert hasattr(model, \"scores_\")\n        assert hasattr(model, \"X_offset_\")\n        assert hasattr(model, \"X_scale_\")\n        assert hasattr(model, \"n_features_in_\")\n", "def test_bayesian_ridge_ard_fit_intercept():\n    \"\"\"Check that fit_intercept works correctly for BayesianRidge and ARDRegression\"\"\"\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n\n    for Estimator in [BayesianRidge, ARDRegression]:\n        model = Estimator(fit_intercept=False)\n        model.fit(X, y)\n        assert model.intercept_ == 0.0\n\n        model = Estimator(fit_intercept=True)\n        model.fit(X, y)\n        assert model.intercept_ != 0.0\n", "def test_bayesian_ridge_ard_feature_names():\n    \"\"\"Check the feature names are correctly set for BayesianRidge and ARDRegression.\"\"\"\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n\n    model = BayesianRidge()\n    model.fit(X, y)\n    assert model.feature_names_in_ is None\n\n    model = ARDRegression()\n    model.fit(X, y)\n    assert model.feature_names_in_ is None\n\n    # check with pandas dataframe\n    import pandas as pd\n\n    df = pd.DataFrame(X, columns=[\"feature1\", \"feature2\"])\n    model = BayesianRidge()\n    model.fit(df, y)\n    assert_array_almost_equal(model.feature_names_in_, [\"feature1\", \"feature2\"])\n\n    model = ARDRegression()\n    model.fit(df, y)\n    assert_array_almost_equal(model.feature_names_in_, [\"feature1\", \"feature2\"])\n"], "sample_884": ["def test_pickle_deprecated_class():\n    # Test that a deprecated class can be pickled and unpickled correctly\n    with pytest.warns(FutureWarning, match=\"qwerty\"):\n        obj = MockClass1()\n    pickled_obj = pickle.dumps(obj)\n    with pytest.warns(FutureWarning, match=\"qwerty\"):\n        unpickled_obj = pickle.loads(pickled_obj)\n    assert isinstance(unpickled_obj, MockClass1)\n", "def test_pickle_deprecated():\n    # Test if a deprecated class or function can be pickled and unpickled.\n    with pytest.warns(FutureWarning, match=\"qwerty\"):\n        obj = MockClass1()\n\n    # Pickle the object\n    pickled_obj = pickle.dumps(obj)\n\n    # Unpickle the object\n    unpickled_obj = pickle.loads(pickled_obj)\n\n    # Check if the unpickled object is an instance of the original class\n    assert isinstance(unpickled_obj, MockClass1)\n\n    # Check if the deprecated warning is still raised when calling a method\n    # on the unpickled object\n    with pytest.warns(FutureWarning, match=\"qwerty\"):\n        MockClass1.__new__(MockClass1)\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n    assert MockClass2().n_features_ == 10\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        MockClass2().n_features_\n    assert MockClass2().n_features_ == 10\n", "def test_pickle_deprecated():\n    # Test that deprecated functions and classes can be pickled\n    with pytest.warns(FutureWarning, match=\"deprecated\"):\n        mock_obj = MockClass3()\n    \n    # Check that we can pickle and unpickle the object without issue\n    pickled_obj = pickle.dumps(mock_obj)\n    unpickled_obj = pickle.loads(pickled_obj)\n\n    assert isinstance(unpickled_obj, MockClass3)\n\n    # Repeat the process for a decorated function\n    with pytest.warns(FutureWarning, match=\"deprecated\"):\n        pickled_func = pickle.dumps(mock_function)\n    unpickled_func = pickle.loads(pickled_func)\n\n    with pytest.warns(FutureWarning, match=\"deprecated\"):\n        result = unpickled_func()\n\n    assert result == 10\n"], "sample_885": ["def test_validate_params_invalid_constraint():\n    \"\"\"Check that an informative error is raised when a parameter constraint is not valid.\"\"\"\n    with pytest.raises(ValueError, match=\"Unknown constraint type\"):\n        @validate_params({\"param\": [\"not a valid constraint\"]})\n            pass\n\n    # check that the function is not created\n    assert \"f\" not in locals()\n", "def test_validate_params_multiple_constraints():\n    \"\"\"Check that validate_params works with multiple constraints per parameter\"\"\"\n\n    @validate_params({\"param\": [int, str]})\n        pass\n\n    # int and str are valid params\n    f(1)\n    f(\"a\")\n\n    with pytest.raises(\n        InvalidParameterError, match=\"The 'param' parameter of f must be\"\n    ) as exc_info:\n        f(param=1.5)\n\n    err_msg = str(exc_info.value)\n    assert \"an instance of 'int'\" in err_msg\n    assert \"an instance of 'str'\" in err_msg\n", "def test_validate_params_no_constraints():\n    \"\"\"Check that validate_params does not raise when there are no constraints\"\"\"\n    @validate_params({})\n        pass\n\n    func(1, 2)\n", "def test_validate_params_positional_only():\n    \"\"\"Check that validate_params works with positional-only parameters.\"\"\"\n    # In Python 3.8+, we can define positional-only parameters using the / syntax.\n    # We want to make sure that validate_params works correctly with these types of\n    # parameters.\n\n    @validate_params({\"a\": [int]})\n        pass\n\n    with pytest.raises(\n        InvalidParameterError, match=\"The 'a' parameter of func must be\"\n    ):\n        func(\"wrong\", 2)\n\n    func(1, 2)\n", "def test_validate_params_replaces_estimator_name_in_error_message():\n    \"\"\"Check that validate_params replaces the estimator name in the error message\n    when it is called from an estimator method.\n    \"\"\"\n\n    class Estimator(BaseEstimator):\n        _parameter_constraints: dict = {\"param\": [int]}\n\n            self.param = param\n\n            # calling validate_params with wrong param type to get an error message\n            with pytest.raises(InvalidParameterError) as exc_info:\n                validate_parameter_constraints(\n                    self._parameter_constraints,\n                    {\"param\": \"wrong\"},\n                    caller_name=\"Estimator\",\n                )\n            err_msg = str(exc_info.value)\n\n            # calling validate_params through fit\n            with pytest.raises(InvalidParameterError) as exc_info:\n                super().fit(X, y)\n            err_msg_through_fit = str(exc_info.value)\n\n            assert err_msg == err_msg_through_fit\n"], "sample_886": ["def test__wrap_data_with_container_dense():\n    \"\"\"Check _wrap_data_with_container for dense data.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    columns = np.asarray([\"f0\", \"f1\", \"f2\"], dtype=object)\n    index = np.asarray([0, 1])\n\n    class Estimator:\n            return columns\n\n    estimator = Estimator()\n    output_config = _get_output_config(\"transform\", estimator)\n\n    data_to_wrap = X\n    wrapped_data = _wrap_data_with_container(\n        method=\"transform\", data_to_wrap=data_to_wrap, original_input=X, estimator=estimator\n    )\n\n    assert isinstance(wrapped_data, pd.DataFrame)\n    assert_array_equal(wrapped_data.columns, columns)\n    assert_array_equal(wrapped_data.index, index)\n", "def test_set_output_mixin_fit_transform():\n    \"\"\"Check that the output is pandas when using fit_transform.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput()\n\n    # By default, output should be numpy array\n    X_trans_np = est.fit_transform(X)\n    assert isinstance(X_trans_np, np.ndarray)\n\n    est.set_output(transform=\"pandas\")\n    X_trans_pd = est.fit_transform(X)\n    assert isinstance(X_trans_pd, pd.DataFrame)\n", "def test_wrap_data_with_container_no_index():\n    \"\"\"Check _wrap_data_with_container when original_input is not a DataFrame.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n\n    class Estimator:\n            return np.asarray([f\"X{i}\" for i in range(X.shape[1])], dtype=object)\n\n    estimator = Estimator()\n    output_config = _get_output_config(\"transform\", estimator)\n    output_config[\"dense\"] = \"pandas\"\n\n    wrapped_X = _wrap_data_with_container(\"transform\", X, X, estimator)\n    assert isinstance(wrapped_X, pd.DataFrame)\n    assert_array_equal(wrapped_X.index, range(X.shape[0]))\n", "def test__wrap_in_pandas_container_columns_none():\n    \"\"\"Check _wrap_in_pandas_container when columns is None.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    columns = None\n\n    dense_named = _wrap_in_pandas_container(X, columns=columns)\n    assert isinstance(dense_named, pd.DataFrame)\n    assert_array_equal(dense_named.columns, range(X.shape[1]))\n", "def test__wrap_in_pandas_container_columns_none_with_dataframe():\n    \"\"\"Check _wrap_in_pandas_container with None columns and DataFrame input.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame([[1, 0, 3], [0, 0, 1]], columns=[\"a\", \"b\", \"c\"])\n    wrapped_df = _wrap_in_pandas_container(X_df, columns=None)\n    assert_array_equal(wrapped_df.columns, X_df.columns)\n"], "sample_887": ["def test_calibration_with_dict_fit_params(data):\n    \"\"\"Check that CalibratedClassifierCV accepts dict as fit_params.\"\"\"\n    X, y = data\n\n    class TestClassifier(LogisticRegression):\n            assert \"a\" in fit_params and fit_params[\"a\"] == 1\n            assert \"b\" in fit_params and fit_params[\"b\"] == 2\n            return super().fit(X, y, sample_weight=sample_weight)\n\n    CalibratedClassifierCV(estimator=TestClassifier()).fit(X, y, a=1, b=2)\n", "def test_calibration_display_pos_label_warning(pyplot, iris_data_binary):\n    \"\"\"Check that a warning is raised when `pos_label` is not specified.\"\"\"\n    X, y = iris_data_binary\n\n    lr = LogisticRegression().fit(X, y)\n    with pytest.warns(UserWarning):\n        CalibrationDisplay.from_estimator(lr, X, y)\n", "def test_calibration_display_plot_kwargs(pyplot, iris_data_binary):\n    # Check that extra kwargs are passed to the plot function\n    X, y = iris_data_binary\n\n    lr = LogisticRegression().fit(X, y)\n    viz = CalibrationDisplay.from_estimator(lr, X, y, marker=\"o\", linestyle=\"--\")\n\n    line = viz.ax_.lines[0]\n    assert line.get_marker() == \"o\"\n    assert line.get_linestyle() == \"--\"\n", "def test_calibration_curve_pos_label_multiclass():\n    \"\"\"Check that an error is raised when `y_true` is multiclass and\n    `pos_label` is not specified.\"\"\"\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([0.1, 0.4, 0.8])\n\n    err_msg = (\n        \"Only binary classification is supported. Provided labels {0, 1, 2}.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        calibration_curve(y_true, y_pred)\n", "def test_calibrated_classifier_cv_prefit_estimator():\n    \"\"\"Check that using prefit estimator works as expected.\"\"\"\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    clf = LogisticRegression()\n    clf.fit(X_train, y_train)\n\n    cal_clf = CalibratedClassifierCV(clf, cv=\"prefit\")\n    cal_clf.fit(X_test, y_test)\n\n    assert len(cal_clf.calibrated_classifiers_) == 1\n\n    y_pred_proba = cal_clf.predict_proba(X_test)\n    assert y_pred_proba.shape[0] == X_test.shape[0]\n    assert y_pred_proba.shape[1] == len(np.unique(y))\n"], "sample_888": ["def test_iforest_with_n_features_equals_one():\n    \"\"\"Test whether iforest works when n_features equals one\"\"\"\n    X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n    iforest = IsolationForest()\n    iforest.fit(X)\n\n    assert all(iforest.predict(X) == 1)\n    assert iforest.decision_function(X).shape[0] == X.shape[0]\n    assert iforest.score_samples(X).shape[0] == X.shape[0]\n", "def test_iforest_fit_with_sample_weight():\n    \"\"\"Test Isolation Forest fit with sample weights\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    sample_weight = np.array([0.5, 0.5])\n\n    model = IsolationForest()\n    model.fit(X, y, sample_weight=sample_weight)\n\n    # Check that the estimator was fitted with sample weights\n    assert hasattr(model, \"estimators_\")\n    assert len(model.estimators_) == model.n_estimators\n", "def test_iforest_with_non_finite_features():\n    \"\"\"Test whether iforest handles non-finite features correctly\"\"\"\n\n    # 2-d array with some infinite values\n    X = np.array([[1, 2], [3, np.inf], [4, 5]])\n    iforest = IsolationForest()\n\n    with pytest.raises(ValueError):\n        iforest.fit(X)\n\n    # 2-d array with some NaN values\n    X = np.array([[1, 2], [3, np.nan], [4, 5]])\n    iforest = IsolationForest()\n\n    with pytest.raises(ValueError):\n        iforest.fit(X)\n", "def test_iforest_estimators_features_():\n    # Test that estimators_features_ is correctly set\n    X = np.array([[1, 2], [3, 4]])\n    clf = IsolationForest(max_features=1)\n    clf.fit(X)\n\n    assert len(clf.estimators_features_) == len(clf.estimators_)\n    for features in clf.estimators_features_:\n        assert len(features) == 1\n        assert features[0] < X.shape[1]\n", "def test_iforest_feature_names_out():\n    \"\"\"Check feature names out for IsolationForest.\"\"\"\n    X = np.array([[0, 1], [1, 2]])\n    clf = IsolationForest(n_estimators=10).fit(X)\n    assert clf.feature_names_out_ is None\n\n    X = iris.data\n    feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n    clf = IsolationForest(n_estimators=10).fit(X, feature_names_in=feature_names)\n    assert_array_equal(clf.feature_names_out_, feature_names)\n"], "sample_889": ["def test_calibration_curve_input_validation():\n    # Check calibration_curve raises an error on invalid inputs\n    y_true = np.array([0, 1])\n    y_pred = np.array([0.5, 0.5])\n\n    with pytest.raises(ValueError):\n        calibration_curve(y_true, y_pred, n_bins=0)\n\n    with pytest.raises(ValueError):\n        calibration_curve(y_true, y_pred, strategy=\"invalid\")\n", "def test_calibration_curve_binary():\n    \"\"\"Check calibration_curve function with binary classification\"\"\"\n    y_true = np.array([0, 0, 1, 1])\n    y_pred = np.array([0.1, 0.4, 0.35, 0.8])\n\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n    assert len(prob_true) == len(prob_pred)\n    assert len(prob_true) == 2\n\n    # Check the first bin contains only the first sample\n    assert np.isclose(prob_true[0], 0)\n    assert np.isclose(prob_pred[0], 0.1)\n\n    # Check the second bin contains the remaining samples\n    assert np.isclose(prob_true[1], 2/3)\n    assert np.isclose(prob_pred[1], (0.4 + 0.35 + 0.8) / 3)\n", "def test_calibrated_classifier_cv_supports_sparse_target():\n    \"\"\"Check that `CalibratedClassifierCV` supports sparse target.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = sparse.csr_matrix(np.array([0, 1]))\n\n    calibrated_clf = CalibratedClassifierCV(LogisticRegression())\n    calibrated_clf.fit(X, y)\n", "def test_calibrated_classifier_cv_prefit_estimator():\n    \"\"\"Check that using `cv='prefit'` with an unfitted estimator raises a\n    `NotFittedError`.\"\"\"\n    X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)\n    clf = LinearSVC(C=1)\n    calib_clf = CalibratedClassifierCV(clf, cv=\"prefit\")\n    with pytest.raises(NotFittedError):\n        calib_clf.fit(X, y)\n", "def test_calibration_curve_pos_label_multiclass():\n    \"\"\"Check that calibration_curve raises an error when y_true is multiclass and\n    pos_label is not specified.\"\"\"\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 3, size=10)\n    y_pred = rng.rand(10)\n\n    err_msg = (\n        \"Only binary classification is supported. Provided labels \"\n        \"{0, 1, 2}.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        calibration_curve(y_true, y_pred)\n"], "sample_890": ["def test_get_support_mask_before_fit():\n    # Test that get_support_mask raises an error if called before fit\n\n    sfs = SequentialFeatureSelector(LinearRegression())\n    with pytest.raises(ValueError, match=\"This estimator has not been fitted\"):\n        sfs.get_support_mask()\n", "def test_check_is_fitted(direction):\n    # Make sure that check_is_fitted raises an error if called before fit\n\n    X, y = make_regression(n_features=10)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(), n_features_to_select=\"auto\", direction=direction, cv=2\n    )\n\n    with pytest.raises(NotFittedError):\n        check_is_fitted(sfs)\n\n    sfs.fit(X, y)\n    check_is_fitted(sfs)\n", "def test_check_is_fitted(direction):\n    # Test that check_is_fitted raises an error if fit was not called\n\n    X, y = make_regression(n_features=10)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        direction=direction,\n        cv=2,\n    )\n\n    with pytest.raises(NotFittedError):\n        sfs.get_support()\n\n    with pytest.raises(NotFittedError):\n        sfs.transform(X)\n\n    with pytest.raises(NotFittedError):\n        sfs.get_support(indices=True)\n", "def test_check_cv():\n    # Make sure check_cv is called inside SequentialFeatureSelector\n\n    X, y = make_classification(n_samples=100, n_features=10)\n    groups = np.random.randint(0, 5, size=(X.shape[0],))\n    cv = LeaveOneGroupOut()\n\n    sfs = SequentialFeatureSelector(\n        LinearRegression(), n_features_to_select=\"auto\", cv=cv\n    )\n    sfs.fit(X, y, groups=groups)\n\n    # Check that the number of features selected is correct\n    assert sfs.n_features_to_select_ == X.shape[1] // 2\n\n    # Check that the transform method works correctly\n    X_transformed = sfs.transform(X)\n    assert X_transformed.shape[1] == sfs.n_features_to_select_\n", "def test_groups_cv(cv):\n    # Make sure that SequentialFeatureSelector works with different cv splitters\n\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n    groups = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 3])\n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(),\n        n_features_to_select=3,\n        cv=cv,\n    )\n    if isinstance(cv, LeaveOneGroupOut):\n        with pytest.raises(ValueError, match=\"Cannot use\"):\n            sfs.fit(X, y)\n    else:\n        sfs.fit(X, y)\n        assert sfs.transform(X).shape[1] == 3\n"], "sample_891": ["def test_average_precision_score_at_k():\n    # Test average precision score with k specified\n    y_true = [0, 1, 0, 1, 0]\n    y_score = [0.2, 0.8, 0.4, 0.9, 0.1]\n\n    # Check for different values of k\n    for k in range(1, len(y_true) + 1):\n        ap_at_k = average_precision_score(y_true, y_score, k=k)\n        assert 0 <= ap_at_k <= 1\n", "def test_label_ranking_avg_precision_score_multilabel_with_binary_target():\n    # Test that label_ranking_average_precision_score can handle binary target.\n    # Non-regression test for #14522\n    y_true = np.array([[0, 1], [1, 0]])\n    y_score = np.array([[0.1, 0.9], [0.9, 0.1]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(1)\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function returns an error when the average parameter is not one of the allowed values.\n    rng = check_random_state(404)\n    y_score = rng.rand(20, 3)\n    y_prob = softmax(y_score)\n    y_true = rng.randint(0, 3, size=20)\n\n    with pytest.raises(ValueError, match=\"average must be one of\"):\n        roc_auc_score(y_true, y_prob, average=\"median\")\n", "def test_binary_clf_curve_pos_label():\n    # Check that using string class labels raises an informative\n    # error for any supported string dtype:\n    y_true = np.array([\"a\", \"b\"] * 10)\n    y_score = np.random.rand(20)\n\n    # Should work fine when pos_label is specified\n    roc_curve(y_true, y_score, pos_label=\"a\")\n    precision_recall_curve(y_true, y_score, pos_label=\"a\")\n\n    # Error if no pos_label specified\n    msg = (\n        \"y_true takes value in {'a', 'b'} and pos_label is \"\n        \"not specified: either make y_true take \"\n        \"value in {0, 1} or {-1, 1} or pass pos_label \"\n        \"explicitly.\"\n    )\n    with pytest.raises(ValueError, match=msg):\n        roc_curve(y_true, y_score)\n    with pytest.raises(ValueError, match=msg):\n        precision_recall_curve(y_true, y_score)\n", "def test_roc_auc_score_pos_label_errors():\n    # Raise an error when pos_label is not in binary y_true\n    y_true = np.array([0, 1])\n    y_pred = np.array([0, 1])\n    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\"\n    with pytest.raises(ValueError, match=err_msg):\n        roc_auc_score(y_true, y_pred, pos_label=2)\n    # Raise an error for multilabel-indicator y_true with\n    # pos_label other than 1\n    y_true = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])\n    y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2], [0.2, 0.8]])\n    err_msg = (\n        \"Parameter pos_label is fixed to 1 for multilabel-indicator y_true. \"\n        \"Do not set pos_label or set pos_label to 1.\"\n    )\n    with pytest.raises(ValueError, match=err_msg):\n        roc_auc_score(y_true, y_pred, pos_label=0)\n"], "sample_892": ["def test_adaboost_estimator_with_no_fit_parameter():\n    # check that we raise a ValueError when an estimator does not have\n    # a fit method with sample_weight parameter\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n\n    class DummyEstimator:\n            pass\n\n            return np.zeros(X.shape[0])\n\n    model = AdaBoostClassifier(estimator=DummyEstimator())\n    err_msg = \"AdaBoostClassifier(estimator=DummyEstimator()) doesn't support sample_weight.\"\n    with pytest.raises(ValueError, match=err_msg):\n        model.fit(X, y)\n", "def test_adaboost_regressor_with_constant_target():\n    # Check that AdaBoostRegressor can handle constant target values\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([5, 5, 5])\n\n    model = AdaBoostRegressor()\n    model.fit(X, y)\n\n    assert_array_almost_equal(model.predict(X), np.array([5, 5, 5]))\n", "def test_adaboost_regressor_with_custom_estimator():\n    # Check that AdaBoostRegressor works with a custom estimator.\n    class CustomEstimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n            return self\n\n            return np.zeros(X.shape[0])\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    model = AdaBoostRegressor(estimator=CustomEstimator())\n    model.fit(X, y)\n    assert model.estimator_.fitted_\n    assert_array_almost_equal(model.predict(X), np.zeros(X.shape[0]))\n", "def test_adaboost_regressor_with_constant_target():\n    # Check that AdaBoostRegressor works with constant target.\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 5])\n    model = AdaBoostRegressor()\n    model.fit(X, y)\n    assert_array_almost_equal(model.predict(X), y)\n", "def test_adaboost_classifier_with_estimator_not_supporting_sample_weight():\n    # check that we raise an informative error message when the estimator does\n    # not support sample weights\n\n    class EstimatorNotSupportingSampleWeight:\n            pass\n\n    model = AdaBoostClassifier(estimator=EstimatorNotSupportingSampleWeight())\n\n    err_msg = \"estimator doesn't support sample_weight\"\n    with pytest.raises(ValueError, match=err_msg):\n        model.fit(iris.data, iris.target)\n"], "sample_893": ["def test_plot_tree_max_depth(pyplot):\n    # Test plot_tree with max_depth parameter\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=1)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n    assert nodes[1].get_text() == \"(...)\"\n    assert nodes[2].get_text() == \"(...)\"\n\n    # Test that max_depth=0 raises an error\n    with pytest.raises(ValueError, match=\"max_depth must be at least 1\"):\n        plot_tree(clf, feature_names=feature_names, max_depth=0)\n", "def test_plot_tree_filled(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for filled\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names, filled=True)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n    # Check that the boxes are filled with colors\n    assert nodes[0].get_bbox_patch().get_facecolor() != (1, 1, 1, 1)\n    assert nodes[1].get_bbox_patch().get_facecolor() != (1, 1, 1, 1)\n    assert nodes[2].get_bbox_patch().get_facecolor() != (1, 1, 1, 1)\n", "def test_plot_tree_label(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz for different labels\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test export code with label='root'\n    nodes = plot_tree(clf, feature_names=[\"first feat\", \"sepal_width\"], label='root')\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n    assert nodes[1].get_text() == \"class: -1\"\n    assert nodes[2].get_text() == \"class: 1\"\n\n    # Test export code with label='none'\n    nodes = plot_tree(clf, feature_names=[\"first feat\", \"sepal_width\"], label='none')\n    assert len(nodes) == 3\n    assert nodes[0].get_text() == \"first feat <= 0.0\"\n    assert nodes[1].get_text() == \"\"\n    assert nodes[2].get_text() == \"\"\n\n    # Test export code with label='all' (default)\n    nodes = plot_tree(clf, feature_names=[\"first feat\", \"sepal_width\"])\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n", "def test_plot_tree_max_depth(pyplot):\n    # Check correctness of plot_tree for max_depth parameter\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names, max_depth=0)\n    assert len(nodes) == 1\n    assert nodes[0].get_text() == \"(...)\"\n", "def test_plot_tree_filled(pyplot):\n    # mostly smoke tests\n    # Check correctness of export_graphviz with filled option\n    clf = DecisionTreeClassifier(\n        max_depth=3, min_samples_split=2, criterion=\"gini\", random_state=2\n    )\n    clf.fit(X, y)\n\n    # Test export code\n    feature_names = [\"first feat\", \"sepal_width\"]\n    nodes = plot_tree(clf, feature_names=feature_names, filled=True)\n    assert len(nodes) == 3\n    assert (\n        nodes[0].get_text()\n        == \"first feat <= 0.0\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]\"\n    )\n    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n\n    # check that boxes are filled\n    assert nodes[0].get_bbox_patch().get_facecolor() != (1, 1, 1, 1)\n    assert nodes[1].get_bbox_patch().get_facecolor() != (1, 1, 1, 1)\n    assert nodes[2].get_bbox_patch().get_facecolor() != (1, 1, 1, 1)\n"], "sample_894": ["def test_forest_max_features_mins_samples_split_interactions():\n    # Test the interaction between max_features and min_samples_split.\n    X, y = make_classification(\n        n_samples=100, n_features=10, random_state=np.random.RandomState(0)\n    )\n\n    # When max_features=1 and min_samples_split is greater than 1, there should\n    # be no splits\n    clf = RandomForestClassifier(max_features=1, min_samples_split=10, random_state=0)\n    clf.fit(X, y)\n\n    for est in clf.estimators_:\n        assert est.tree_.node_count == 1\n\n    # When max_features > 1, then min_samples_split should not prevent splitting\n    clf = RandomForestClassifier(max_features=2, min_samples_split=10, random_state=0)\n    clf.fit(X, y)\n\n    for est in clf.estimators_:\n        assert est.tree_.node_count > 1\n", "def test_estimator_param():\n    # Test that `estimator` parameter is correctly set.\n    clf = RandomForestClassifier(estimator=None)\n    assert clf.estimator is None\n\n    est = DecisionTreeClassifier()\n    clf = RandomForestClassifier(estimator=est)\n    assert clf.estimator is est\n", "def test_read_only_buffer_oob_score(monkeypatch):\n    \"\"\"RandomForestClassifier must work on readonly sparse data with oob_score.\n\n    Non-regression test for: https://github.com/scikit-learn/scikit-learn/issues/25333\n    \"\"\"\n    monkeypatch.setattr(\n        sklearn.ensemble._forest,\n        \"Parallel\",\n        partial(Parallel, max_nbytes=100),\n    )\n    rng = np.random.RandomState(seed=0)\n\n    X, y = make_classification(n_samples=100, n_features=200, random_state=rng)\n    X = csr_matrix(X, copy=True)\n\n    clf = RandomForestClassifier(n_jobs=2, oob_score=True, random_state=rng)\n    cross_val_score(clf, X, y, cv=2)\n", "def test_forest_n_features_in_(name):\n    # Test that `n_features_in_` is set correctly.\n    X = iris.data\n    y = iris.target\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False, random_state=1)\n    est.fit(X, y)\n    assert hasattr(est, \"n_features_in_\")\n    assert est.n_features_in_ == X.shape[1]\n", "def test_estimators_samples():\n    # Check that the estimators_samples_ attribute is correctly set\n    X, y = datasets.make_classification(random_state=0)\n    clf = RandomForestClassifier(n_estimators=5, random_state=0)\n    clf.fit(X, y)\n    assert len(clf.estimators_samples_) == 5\n    for samples in clf.estimators_samples_:\n        assert np.isin(samples, np.arange(X.shape[0])).all()\n"], "sample_895": ["def test_column_transformer_feature_names_out_with_function_transformer():\n    \"\"\"Check that ColumnTransformer handles get_feature_names_out with FunctionTransformer.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame({\"feat0\": [1.0, 2.0, 3.0], \"feat1\": [2.0, 3.0, 4.0]})\n    ct = ColumnTransformer(\n        [\n            (\n                \"trans_0\",\n                FunctionTransformer(validate=False, feature_names_out=\"one-to-one\"),\n                [\"feat1\"],\n            ),\n            (\"trans_1\", \"passthrough\", [\"feat0\"]),\n        ]\n    )\n    ct.fit(X_df)\n\n    expected_verbose_names = [\"trans_0__feat1\", \"trans_1__feat0\"]\n    expected_non_verbose_names = [\"feat1\", \"feat0\"]\n\n    assert_array_equal(ct.get_feature_names_out(), expected_verbose_names)\n    ct.set_params(verbose_feature_names_out=False)\n    assert_array_equal(ct.get_feature_names_out(), expected_non_verbose_names)\n", "def test_column_transformer_set_output_numpy():\n    \"\"\"Check column transformer behavior with set_output to numpy.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"], index=[10])\n    ct = ColumnTransformer(\n        [(\"first\", TransWithNames(), [\"a\", \"c\"]), (\"second\", TransWithNames(), [\"d\"])],\n    )\n    ct.set_output(transform=\"numpy\")\n\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, np.ndarray)\n\n    df_test = pd.DataFrame([[1, 2, 3, 4]], columns=df.columns, index=[20])\n    X_trans = ct.transform(df_test)\n    assert isinstance(X_trans, np.ndarray)\n", "def test_column_transformer_feature_names_out_with_set_output():\n    \"\"\"Check feature names out with `set_output`.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        [\n            (\"bycol1\", TransWithNames([\"x\", \"y\"]), [\"b\"]),\n            (\"bycol2\", \"passthrough\", [\"a\"]),\n        ],\n        remainder=\"drop\",\n    )\n    ct.fit(df)\n    ct.set_output(transform=\"pandas\")\n\n    assert_array_equal(\n        ct.get_feature_names_out(), [\"bycol1__x\", \"bycol1__y\", \"bycol2__a\"]\n    )\n", "def test_column_transformer_set_output_mixed_dtype():\n    \"\"\"Check ColumnTransformer outputs mixed dtypes correctly.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"color\": pd.Series([\"green\", \"blue\", \"red\"], dtype=\"object\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n            \"distance\": pd.Series([20, pd.NA, 100], dtype=\"Int32\"),\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=False, dtype=\"int8\"),\n                [\"color\"],\n            ),\n            (\"age\", StandardScaler(), [\"age\"]),\n        ],\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    ).set_output(transform=\"pandas\")\n\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, pd.DataFrame)\n    assert_array_equal(X_trans.columns, ct.get_feature_names_out())\n\n    expected_dtypes = {\n        \"color_blue\": \"int8\",\n        \"color_green\": \"int8\",\n        \"color_red\": \"int8\",\n        \"age\": \"float64\",\n        \"pet\": \"category\",\n        \"height\": \"int64\",\n        \"distance\": \"Int32\",\n    }\n    for col, dtype in X_trans.dtypes.items():\n        assert dtype == expected_dtypes[col]\n", "def test_column_transformer_feature_names_out_dtype():\n    \"\"\"Check that the dtype of feature_names_out is object.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\"a\", \"b\", \"c\"])\n    ct = ColumnTransformer([(\"trans\", TransWithNames(), [\"a\", \"b\"])])\n    ct.fit(X_df)\n    feature_names_out = ct.get_feature_names_out()\n    assert feature_names_out.dtype == object\n"], "sample_896": ["def test_nmf_feature_names_out_with_custom_prefix():\n    \"\"\"Check feature names out for NMF with custom prefix.\"\"\"\n    random_state = np.random.RandomState(0)\n    X = np.abs(random_state.randn(10, 4))\n    nmf = NMF(n_components=3).fit(X)\n\n    names = nmf.get_feature_names_out(prefix=\"custom\")\n    assert_array_equal([f\"custom{i}\" for i in range(3)], names)\n", "def test_nmf_fresh_restarts_max_iter():\n    # Test that the fresh restarts of MiniBatchNMF stop after the specified max number of iterations.\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(6, 5))\n    m = MiniBatchNMF(\n        n_components=3,\n        random_state=0,\n        tol=1e-3,\n        fresh_restarts=True,\n        fresh_restarts_max_iter=5,\n    )\n    m.fit_transform(A)\n\n    assert m.n_iter_ == 5\n", "def test_nmf_partial_fit_weighted_average():\n    # Check that the weighted average of matrices A and B is correctly computed\n    # in partial_fit.\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(100, 5))\n\n    n_components = 5\n    batch_size = 10\n\n    mbnmf = MiniBatchNMF(\n        n_components=n_components,\n        init=\"custom\",\n        random_state=0,\n        batch_size=batch_size,\n    )\n\n    # Force the same init of H (W is recomputed anyway) to be able to compare results.\n    W, H = nmf._initialize_nmf(\n        X, n_components=n_components, init=\"random\", random_state=0\n    )\n\n    A = np.abs(rng.randn(n_components, X.shape[1]))\n    B = np.abs(rng.randn(n_components, X.shape[1]))\n\n    mbnmf.partial_fit(X[:batch_size], W=W[:batch_size], H=A)\n    mbnmf.partial_fit(X[batch_size:], W=W[batch_size:], H=B)\n\n    assert_allclose(mbnmf.components_, 0.5 * (A + B))\n", "def test_minibatch_nmf_partial_fit_fresh_restarts():\n    # Check fit / partial_fit equivalence with fresh restarts.\n    rng = np.random.mtrand.RandomState(42)\n    X = np.abs(rng.randn(100, 5))\n\n    n_components = 5\n    batch_size = 10\n    max_iter = 2\n\n    mbnmf1 = MiniBatchNMF(\n        n_components=n_components,\n        init=\"custom\",\n        random_state=0,\n        max_iter=max_iter,\n        batch_size=batch_size,\n        tol=0,\n        max_no_improvement=None,\n        fresh_restarts=True,\n    )\n    mbnmf2 = MiniBatchNMF(n_components=n_components, init=\"custom\", random_state=0)\n\n    # Force the same init of H (W is recomputed anyway) to be able to compare results.\n    W, H = nmf._initialize_nmf(\n        X, n_components=n_components, init=\"random\", random_state=0\n    )\n\n    mbnmf1.fit(X, W=W, H=H)\n    for i in range(max_iter):\n        for j in range(0, len(X), batch_size):\n            mbnmf2.partial_fit(X[j : j + batch_size], H=H)\n\n    assert mbnmf1.n_steps_ == mbnmf2.n_steps_\n    assert_allclose(mbnmf1.components_, mbnmf2.components_)\n", "def test_nmf_init_custom_without_w():\n    # Check that an error is raised if init='custom' but W is not provided\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    H_init = rng.random_sample((2, 5))\n    with pytest.raises(ValueError, match=\"Custom initialization needs W and H\"):\n        non_negative_factorization(X, H=H_init, init=\"custom\")\n"], "sample_897": ["def test_partial_dependence_display_categorical_features_targeted(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_categorical_with_target_idx(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_categorical_features_targeted(\n    pyplot,\n    clf_diabetes,\n    diabetes,", "def test_partial_dependence_display_sample_weight(pyplot, clf_diabetes, diabetes):\n    \"\"\"Check that we can pass sample weights when computing the partial dependence.\"\"\"\n    sample_weight = np.random.rand(len(diabetes.target))\n\n    disp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, 1],\n        kind=\"average\",\n        sample_weight=sample_weight,\n    )\n\n    assert disp.figure_ is pyplot.gcf()\n    assert disp.axes_.shape == (1, 2)\n    assert all([line is not None for line in disp.lines_.ravel()])\n    assert all([contour is None for contour in disp.contours_.ravel()])\n", "def test_partial_dependence_display_kind_list_lines_shape(\n    pyplot, clf_diabetes, diabetes, features, kind, expected_lines_shape"], "sample_898": ["def test_thresholded_metrics_with_undefined_binary_multiclass():\n    # Test that thresholded metrics raise an error when used with binary or\n    # multiclass data in label indicator format.\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 2, size=(20, ))\n    y_score = random_state.uniform(size=(20, ))\n\n    for name in METRIC_UNDEFINED_BINARY_MULTICLASS:\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            assert_raises(ValueError, metric, y_true, y_score)\n\n    y_true = random_state.randint(0, 4, size=(20, ))\n    y_score = random_state.uniform(size=(20, 4))\n\n    for name in METRIC_UNDEFINED_BINARY_MULTICLASS:\n        metric = ALL_METRICS[name]\n        if name in THRESHOLDED_METRICS:\n            assert_raises(ValueError, metric, y_true, y_score)\n", "def test_label_ranking_average_precision_score_tied_scores():\n    # Test that label_ranking_average_precision_score handles tied scores\n    # correctly.\n\n    # Basic case with two levels of ties.\n    y_true = np.array([[1, 0, 0], [0, 1, 0]])\n    y_score = np.array([[0.75, 0.5, 0.75], [0.75, 0.75, 0.2]])\n\n    # The first sample has two positive labels with the highest score and one\n    # negative label also with the highest score. The second sample has one\n    # positive label with the highest score and another positive label with a\n    # lower score than a negative label. Therefore, the ranking-based average\n    # precision should be (1 + 0.5)/2.\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score),\n                        0.75)\n\n    # Same as above but with another negative label with a lower score.\n    y_true = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])\n    y_score = np.array([[0.75, 0.5, 0.75, 0.25], [0.75, 0.75, 0.2, 0.1]])\n\n    # The introduction of another negative label does not change the average\n    # precision.\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score),\n                        0.75)\n", "def test_roc_auc_score_fpr_tpr():\n    # Test whether roc_auc_score function returns the same value when given\n    # fpr and tpr as the roc_curve function.\n    y_true = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8, 0.9, 0.2, 0.7, 0.6, 0.3, 0.15,\n                        0.5, 0.25, 0.45, 0.85, 0.95])\n\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    auc_value = auc(fpr, tpr)\n\n    assert_almost_equal(roc_auc_score(y_true, y_score), auc_value)\n", "def test_average_precision_score_edge_cases():\n    # Test average precision with edge cases\n    y_true = np.array([0, 1])\n    y_score = np.array([1, 0])\n\n    # Check zero denominator case\n    assert_almost_equal(average_precision_score(y_true, y_score), 0.0)\n\n    y_true = np.array([0, 0, 0])\n    y_score = np.array([1, 2, 3])\n\n    # Check that we can calculate AP when there are no positive samples\n    assert_almost_equal(average_precision_score(y_true, y_score), 0.0)\n", "def test_average_precision_score_edge_cases():\n    # Test average precision with edge cases\n    y_true = np.array([1, 1, 1, 1, 1])\n    y_score = np.array([0.2, 0.3, 0.4, 0.5, 0.6])\n\n    # Check perfect score when all predictions are correct\n    assert_almost_equal(average_precision_score(y_true, y_score), 1.0)\n\n    # Check zero score when all predictions are incorrect\n    y_true = np.array([0, 0, 0, 0, 0])\n    y_score = np.array([0.2, 0.3, 0.4, 0.5, 0.6])\n    assert_almost_equal(average_precision_score(y_true, y_score), 0.0)\n\n    # Check zero score when there are no positive samples\n    y_true = np.array([0, 0, 0, 0, 0])\n    y_score = np.array([0.2, 0.3, 0.4, 0.5, 0.6])\n    assert_almost_equal(average_precision_score(y_true, y_score), 0.0)\n"], "sample_899": ["def test_check_estimator_fit_with_readonly_memmap():\n    # check that fit method does not attempt to modify input data\n    X = np.random.rand(10, 3)\n    y = np.random.randint(0, 2, size=10)\n\n    # create memmap and make it read-only\n    X_mmap = np.memmap(cStringIO.BytesIO(), shape=X.shape, mode='w+')\n    X_mmap[:] = X\n    X_mmap.flush()\n    X_mmap.flags.writeable = False\n\n    y_mmap = np.memmap(cStringIO.BytesIO(), shape=y.shape, mode='w+')\n    y_mmap[:] = y\n    y_mmap.flush()\n    y_mmap.flags.writeable = False\n\n    # try fitting with readonly memmap\n    est = SVC(kernel='linear')\n    check_estimator(est, readonly_memmap=True)\n\n    est.fit(X_mmap, y_mmap)\n\n    assert_array_equal(est.predict(X), est.predict(X_mmap))\n", "def test_check_estimator_ignores_deprecated_methods():\n    # check that check_estimator ignores deprecated methods\n    class TestEstimatorWithDeprecatedMethod(BaseEstimator):\n        @deprecated(\"Deprecated for the purpose of testing \"\n                    \"check_estimator\")\n            return self\n\n            return np.ones(X.shape[0])\n\n    check_estimator(TestEstimatorWithDeprecatedMethod())\n", "def test_check_estimator_sparse_data():\n    # check that check_estimator() works on estimator with sparse data\n\n    # test precomputed kernel\n    est = SVC(kernel='precomputed')\n    X, y = make_blobs(n_samples=50, random_state=1)\n    X_sparse = sp.csr_matrix(X)\n    check_estimators_sparse_data(\"SVC\", est)\n\n    # test sparse matrix\n    est = LinearRegression()\n    X_sparse = sp.csr_matrix(np.random.rand(10, 3))\n    y = np.random.rand(10)\n    est.fit(X_sparse, y)\n    check_estimators_sparse_data(\"LinearRegression\", est)\n", "def test_check_estimator_with_64bit_indices_sparse_matrices():\n    # check that estimators work with 64-bit indices sparse matrices\n    from sklearn.datasets import make_blobs\n    from sklearn.linear_model import LogisticRegression\n\n    X, y = make_blobs(n_samples=100, n_features=10)\n    X_sparse = sp.csr_matrix(X)\n\n    # convert indices to 64-bit\n    X_sparse.indptr = X_sparse.indptr.astype(np.int64)\n    X_sparse.indices = X_sparse.indices.astype(np.int64)\n\n    est = LogisticRegression()\n    est.fit(X_sparse, y)\n    check_estimator(est)\n", "def test_check_estimator_check_dtype_object():\n    # check that estimators raise an informative error message when\n    # receiving an array-like of dtype 'object' and they don't support it.\n\n    class BadEstimator(BaseEstimator):\n            return self\n\n    msg = \"argument must be a string or a number\"\n    assert_raises_regex(TypeError, msg, check_dtype_object,\n                        \"test\", BadEstimator())\n"], "sample_900": ["def test_solver_parameter():\n    # Test that invalid solver parameter raises value error.\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n    clf = MLPClassifier(solver='invalid')\n    assert_raises(ValueError, clf.fit, X, y)\n", "def test_mlp_classifier_output_dtype():\n    # Test that MLPClassifier outputs integer labels.\n    X = X_digits_binary[:50]\n    y = y_digits_binary[:50]\n\n    mlp = MLPClassifier(hidden_layer_sizes=5, random_state=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp.fit(X, y)\n\n    assert np.issubdtype(mlp.predict(X).dtype, np.integer)\n", "def test_mlpclassifier_fit_handle_zeros():\n    # Test that MLPClassifier handles zeros in fit method.\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([0, 1, 1, 0])\n\n    clf = MLPClassifier(hidden_layer_sizes=2)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n\n    assert clf.score(X, y) > 0.9\n", "def test_invalid_learning_rate():\n    # Test that invalid learning rates raise value error.\n    X = [[3, 2], [1, 6]]\n    y = [1, 0]\n    clf = MLPClassifier(learning_rate='invalid')\n\n    assert_raises(ValueError, clf.fit, X, y)\n", "def test_mlp_classifier_early_stopping_stratified():\n    # Test that early stopping samples are stratified for classification\n    X, y = make_classification(n_samples=100, random_state=0)\n    clf = MLPClassifier(max_iter=1, early_stopping=True, validation_fraction=0.2,\n                        solver='adam', random_state=0)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n    assert np.all(np.unique(y) == np.unique(clf.validation_scores_))\n"], "sample_901": ["def test_minibatch_kmeans_warns_max_no_improvement():\n    # check that a warning is raised when max_no_improvement is set\n    X = np.random.rand(100, 5)\n    km = MiniBatchKMeans(n_clusters=5, max_no_improvement=10)\n    assert_warns_message(ConvergenceWarning, \"Converged (lack of improvement \"\n                         \"in inertia) at iteration\", km.fit, X)\n", "def test_minibatch_kmeans_init_size():\n    # Test that init_size parameter is correctly used in MiniBatchKMeans\n    X, _ = make_blobs(n_samples=1000, centers=5, cluster_std=1., random_state=42)\n    mbk = MiniBatchKMeans(n_clusters=5, init_size=200, batch_size=10, n_init=1,\n                          random_state=42)\n    mbk.fit(X)\n    assert hasattr(mbk, 'init_size_')\n    assert mbk.init_size_ == 200\n", "def test_minibatch_kmeans_init_size():\n    # Test that the init_size parameter is used correctly in MiniBatchKMeans\n    mb_kmeans = MiniBatchKMeans(n_clusters=n_clusters, init_size=10,\n                                random_state=42)\n    assert mb_kmeans.init_size == 10\n\n    # Test that an error is raised when init_size is larger than n_samples\n    with pytest.raises(ValueError):\n        MiniBatchKMeans(n_clusters=n_clusters, init_size=n_samples + 1)\n\n    # Test that an error is raised when init_size is smaller than n_clusters\n    with pytest.raises(RuntimeWarning):\n        MiniBatchKMeans(n_clusters=n_clusters, init_size=n_clusters - 1).fit(X)\n", "def test_k_means_sample_weight_zeros():\n    # check that zero sample weights have no effect on KMeans\n    sample_weight = np.ones(n_samples)\n    sample_weight[::2] = 0\n\n    km1 = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n    km2 = KMeans(n_clusters=n_clusters, random_state=42).fit(X, sample_weight=sample_weight)\n\n    assert_array_almost_equal(km1.cluster_centers_, km2.cluster_centers_)\n    assert_array_almost_equal(km1.labels_, km2.labels_)\n    assert_almost_equal(km1.inertia_, km2.inertia_)\n", "def test_minibatch_kmeans_partial_fit_sparse_data():\n    # Test that MiniBatchKMeans.partial_fit works with sparse data\n    X = sp.csr_matrix(np.array([[-1, 0], [1, 0]]))\n    km = MiniBatchKMeans(n_clusters=2)\n    km.partial_fit(X)\n    assert km.cluster_centers_.dtype.kind == \"f\"\n"], "sample_902": ["def test_pipeline_with_none_step_and_memory():\n    X = np.array([[1, 2]])\n    memory = Memory(location='/tmp/mycachedir')\n    pipe = Pipeline([('transf', None), ('clf', Mult())],\n                    memory=memory)\n    pipe.fit(X, y=None)\n    assert_array_equal(pipe.transform(X), X)\n    assert_array_equal(pipe.inverse_transform(X), X)\n", "def test_pipeline_with_none_step_and_memory():\n    X = np.array([[1, 2]])\n    memory = Memory(cachedir=mkdtemp(), verbose=10)\n    try:\n        pipe = Pipeline([('transf', None), ('clf', Mult())], memory=memory)\n        pipe.fit(X, y=None)\n        assert_raises_regex(AttributeError,\n                            \"'NoneType' object has no attribute 'predict'\",\n                            pipe.predict, X)\n    finally:\n        shutil.rmtree(memory.cachedir)\n", "def test_pipeline_inverse_transform():\n    # Test that inverse_transform works on pipelines with a final estimator\n    X = np.array([[1]])\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())])\n    pipe.fit(X, y=None)\n\n    # Check that it doesn't have the method before fitting\n    assert_false(hasattr(pipe, 'inverse_transform'))\n\n    # Check that the output of inverse_transform is correct\n    X_inv = pipe.inverse_transform(pipe.transform(X))\n    assert_array_almost_equal(X_inv, X)\n", "def test_pipeline_get_params_with_memory():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    memory = Memory(cachedir='/tmp', verbose=10)\n    pipe = Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                    memory=memory)\n    params = pipe.get_params(deep=True)\n    assert_equal(params['memory'], memory)\n    assert_equal(params['steps'], pipe.steps)\n", "def test_pipeline_inverse_transform():\n    X = np.array([[1, 2]])\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())])\n    pipe.fit(X, y=None)\n    X_transformed = pipe.transform(X)\n    assert_array_almost_equal(pipe.inverse_transform(X_transformed), X)\n\n    pipe = Pipeline([('transf', NoInvTransf()), ('clf', Mult())])\n    pipe.fit(X, y=None)\n    assert_false(hasattr(pipe, 'inverse_transform'))\n"], "sample_903": ["def test_cosine_metric():\n    # t-SNE should allow 'cosine' metric.\n    random_state = check_random_state(0)\n    X = random_state.randn(5, 2)\n    tsne = TSNE(metric=\"cosine\")\n    tsne.fit_transform(X)\n\n    # Check that the embedding is different when using cosine metric\n    # versus euclidean metric.\n    tsne_euclidean = TSNE(metric=\"euclidean\")\n    assert not np.array_equal(tsne.fit_transform(X), tsne_euclidean.fit_transform(X))\n", "def test_n_components_too_high():\n    # 'n_components' should be less than or equal to 3 when method='barnes_hut'.\n    tsne = TSNE(method='barnes_hut', n_components=4)\n    assert_raises_regexp(ValueError, \"'n_components' should be .*\",\n                         tsne.fit_transform, np.array([[0.0], [1.0]]))\n", "def test_n_components_range():\n    # TSNE should allow n_components to be in range 1-3 for barnes_hut method.\n    random_state = check_random_state(0)\n    X = random_state.randn(50, 2)\n    for n_components in [1, 2, 3]:\n        tsne = TSNE(n_components=n_components, method=\"barnes_hut\")\n        tsne.fit_transform(X)\n\n    for n_components in [4, 5]:\n        tsne = TSNE(n_components=n_components, method=\"barnes_hut\")\n        assert_raises_regexp(ValueError, \"'n_components' should be .*\",\n                             tsne.fit_transform, X)\n", "def test_tsne_embedding_computed():\n    # Ensure TSNE embedding is computed and accessible through the `embedding_` attribute.\n    random_state = check_random_state(0)\n    X = random_state.randn(50, 2)\n    tsne = TSNE(n_components=2, init=\"random\", random_state=0)\n    X_embedded = tsne.fit_transform(X)\n    assert_array_equal(tsne.embedding_, X_embedded)\n", "def test_tsne_embedding_stability():\n    # Ensure that the embedding is stable when random_state is fixed.\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n\n    tsne1 = TSNE(n_components=2, init='random', random_state=0,\n                 perplexity=20, n_iter=100)\n    Y1 = tsne1.fit_transform(X)\n\n    tsne2 = TSNE(n_components=2, init='random', random_state=0,\n                 perplexity=20, n_iter=100)\n    Y2 = tsne2.fit_transform(X)\n\n    assert_array_almost_equal(Y1, Y2)\n"], "sample_904": ["def test_term_reference_with_explicit_title(app):\n    text = (\".. glossary::\\n\"\n            \"\\n\"\n            \"   term1\\n\"\n            \"       description\\n\")\n    restructuredtext.parse(app, text)\n\n    domain = app.env.get_domain(\"std\")\n    refnode = domain.resolve_xref(app.env, 'index', app.builder, 'term', 'term1',\n                                  pending_xref(), nodes.paragraph(text=\"My title\"))\n    assert_node(refnode, nodes.reference, refid=\"term-term1\", astext=\"My title\")\n", "def test_build_reference_node(app):\n    env = mock.Mock(domaindata={})\n    domain = StandardDomain(env)\n    fromdocname = 'fromdoc'\n    builder = mock.Mock(name='builder')\n    docname = 'todoc'\n    labelid = 'labelid'\n    sectname = 'Section Name'\n\n    # Test with default node class (nodes.reference)\n    node = domain.build_reference_node(fromdocname, builder, docname, labelid, sectname, 'ref')\n    assert isinstance(node, nodes.reference)\n    assert node['refuri'] == builder.get_relative_uri.return_value + '#' + labelid\n\n    # Test with custom node class (addnodes.number_reference)\n    node = domain.build_reference_node(fromdocname, builder, docname, labelid, sectname, 'numref',\n                                       nodeclass=addnodes.number_reference)\n    assert isinstance(node, addnodes.number_reference)\n    assert node['refuri'] == builder.get_relative_uri.return_value + '#' + labelid\n", "def test_is_enumerable_node():\n    env = mock.Mock(domaindata={})\n    env.app.registry.enumerable_nodes = {}\n    domain = StandardDomain(env)\n\n    # normal nodes\n    node = nodes.paragraph()\n    assert not domain.is_enumerable_node(node)\n\n    # enumerable nodes\n    node = nodes.figure()\n    assert domain.is_enumerable_node(node)\n\n    node = nodes.table()\n    assert domain.is_enumerable_node(node)\n\n    node = nodes.container()\n    assert domain.is_enumerable_node(node)\n", "def test_get_fignumber_with_no_section_numbers(app):\n    env = mock.Mock()\n    env.toc_secnumbers = {'docname': {}}\n    builder = mock.Mock()\n    domain = StandardDomain(env)\n    with pytest.raises(ValueError):\n        domain.get_fignumber(env, builder, 'section', 'docname', nodes.section())\n", "def test_std_domain_get_full_qualified_name_without_refuri(app):\n    env = mock.Mock(domaindata={})\n    env.app.registry.enumerable_nodes = {}\n    domain = StandardDomain(env)\n\n    # normal references\n    node = nodes.reference()\n    assert domain.get_full_qualified_name(node) is None\n\n    # simple reference to options without refuri\n    node = nodes.reference(reftype='option', reftarget='-l')\n    assert domain.get_full_qualified_name(node) is None\n\n    # options with std:program context\n    kwargs = {'std:program': 'ls'}\n    node = nodes.reference(reftype='option', reftarget='-l', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'ls.-l'\n\n    # options with refuri but no std:program context\n    node = nodes.reference(reftype='option', reftarget='-l', refuri='some-uri')\n    assert domain.get_full_qualified_name(node) is None\n"], "sample_905": ["def test_is_builtin_class_method_builtin():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(str, 'join') is True\n    assert inspect.is_builtin_class_method(list, 'append') is True\n", "def test_is_builtin_class_method_builtin():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(str, 'join') is True\n    assert inspect.is_builtin_class_method(list, 'append') is True\n    assert inspect.is_builtin_class_method(dict, 'keys') is True\n", "def test_is_builtin_class_method_builtin():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(str, 'upper') is True\n    assert inspect.is_builtin_class_method(list, 'append') is True\n\n    class MyList(list):\n            pass\n\n    assert inspect.is_builtin_class_method(MyList, 'append') is True\n    assert inspect.is_builtin_class_method(MyList, 'my_method') is False\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(str, 'join') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n    assert inspect.is_builtin_class_method(list, 'append') is True\n    assert inspect.is_builtin_class_method(dict, 'keys') is True\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert inspect.getmro(C) == (C, B, A, object)\n    assert inspect.getmro(B) == (B, A, object)\n    assert inspect.getmro(A) == (A, object)\n"], "sample_908": ["def test_unparse_arguments():\n    # test unparse arguments with defaults\n    node = ast.parse(\"def foo(a, b=1, c: int = 2, *args, d, e: str = '3', **kwargs): pass\")\n    args = node.body[0].args\n    expected = \"a, b=1, c: int = 2, *args, d, e: str = '3', **kwargs\"\n    assert ast.unparse_arguments(args) == expected\n\n    # test unparse arguments without defaults\n    node = ast.parse(\"def foo(a, b, c: int, *args, d, e: str, **kwargs): pass\")\n    args = node.body[0].args\n    expected = \"a, b, c: int, *args, d, e: str, **kwargs\"\n    assert ast.unparse_arguments(args) == expected\n\n    # test unparse arguments with posonlyargs (Python 3.8+)\n    if sys.version_info > (3, 8):\n        node = ast.parse(\"def foo(a, /, b, c: int = 2, *args, d, e: str = '3', **kwargs): pass\")\n        args = node.body[0].args\n        expected = \"a, /, b, c: int = 2, *args, d, e: str = '3', **kwargs\"\n        assert ast.unparse_arguments(args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    # test unparse arguments with default values\n    node = ast.arguments(\n        args=[ast.arg(arg='x', annotation=None)],\n        vararg=None,\n        kwonlyargs=[ast.arg(arg='y', annotation=None)],\n        kw_defaults=[ast.Num(n=1)],\n        kwarg=None,\n        defaults=[ast.Num(n=2)]\n    )\n    assert ast.unparse_arguments(node) == \"x=2, *, y=1\"\n\n    # test unparse arguments with posonlyargs (Python 3.8+)\n    if sys.version_info > (3, 8):\n        node = ast.arguments(\n            args=[ast.arg(arg='x', annotation=None)],\n            posonlyargs=[ast.arg(arg='z', annotation=None)],\n            vararg=None,\n            kwonlyargs=[ast.arg(arg='y', annotation=None)],\n            kw_defaults=[ast.Num(n=1)],\n            kwarg=None,\n            defaults=[ast.Num(n=2)]\n        )\n        assert ast.unparse_arguments(node) == \"z, /, x=2, *, y=1\"\n", "def test_unparse_arguments():\n    # Create a dummy function node with arguments\n    func_node = ast.parse(\"def foo(a, b=1, *args, c=None, **kwargs): pass\")\n    args_node = func_node.body[0].args\n\n    expected = \"a, b=1, *args, c=None, **kwargs\"\n    assert ast.unparse_arguments(args_node) == expected\n\n    # Test posonlyargs (Python 3.8+)\n    if sys.version_info > (3, 8):\n        func_node = ast.parse(\"def foo(a, /, b=1, *args, c=None, **kwargs): pass\")\n        args_node = func_node.body[0].args\n\n        expected = \"a, /, b=1, *args, c=None, **kwargs\"\n        assert ast.unparse_arguments(args_node) == expected\n", "def test_unparse_arguments():\n    # Create an arguments node with various parameters\n    args = ast.arguments(\n        args=[ast.arg(arg='a', annotation=None), ast.arg(arg='b', annotation=None)],\n        vararg=ast.arg(arg='args', annotation=None),\n        kwonlyargs=[ast.arg(arg='kwonly', annotation=None)],\n        kw_defaults=[ast.Str(s='default')],\n        kwarg=ast.arg(arg='kwargs', annotation=None),\n        defaults=[ast.Str(s='default')]\n    )\n\n    expected = \"a, b, *args, kwonly='default', **kwargs\"\n    assert ast.unparse_arguments(args) == expected\n"], "sample_909": ["def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_parse_usage_section(self):\n    docstring = \"\"\"Usage:\n    Use this function like this.", "def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"", "def test_usage_section(self):\n    docstring = \"\"\""], "sample_910": ["def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        assert 'prefix: WARNING: message1' in warning.getvalue()\n\n    # check if prefix is reset after context\n    logger.warning('message2')\n    assert 'WARNING: message2' in warning.getvalue()\n    assert 'prefix: WARNING: message2' not in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message')\n\n    assert 'prefix: WARNING: message' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        assert 'prefix: WARNING: message1' in warning.getvalue()\n\n    with prefixed_warnings(\"another prefix:\"):\n        logger.warning('message2')\n        assert 'another prefix: WARNING: message2' in warning.getvalue()\n"], "sample_238": ["def test_aggregation_with_expression_wrappers(self):\n    class Double(Func):\n        function = 'POWER'\n        template = \"%(function)s(%(expressions)s, 2)\"\n\n    class Half(Func):\n        function = 'POWER'\n        template = \"%(function)s(%(expressions)s, 0.5)\"\n\n    books = Book.objects.annotate(double_pages=Double('pages'), half_price=Half('price'))\n    b1 = books.get(pk=self.b4.pk)\n    self.assertEqual(b1.double_pages, 122500)\n    self.assertEqual(b1.half_price, Approximate(Decimal(\"14.845\"), places=3))\n\n    double_pages_sum = books.aggregate(Sum('double_pages'))['double_pages__sum']\n    half_price_sum = books.aggregate(Sum('half_price'))['half_price__sum']\n    self.assertEqual(double_pages_sum, 1047207)\n    self.assertEqual(half_price_sum, Approximate(Decimal(\"135.13\"), places=2))\n", "def test_aggregation_subquery_annotation_with_filter(self):\n    long_books_count_qs = Book.objects.filter(\n        publisher=OuterRef('pk'),\n        pages__gt=400,\n    ).values(\n        'publisher'\n    ).annotate(count=Count('pk')).values('count')\n    publishers = Publisher.objects.annotate(\n        long_books_count=Subquery(long_books_count_qs, IntegerField()),\n    ).filter(long_books_count__gt=0).order_by('long_books_count')\n    self.assertEqual(list(publishers), [self.p3, self.p4])\n", "def test_expression_wrapper_annotation(self):\n    wrapper = ExpressionWrapper(F('rating') + F('pages'), output_field=IntegerField())\n    qs = Book.objects.annotate(combined=wrapper).values_list('combined', flat=True)\n    self.assertEqual(list(qs), [451, 531, 304, 354, 1136, 951])\n", "def test_aggregation_subquery_annotation_with_filter(self):\n    published_books = Book.objects.filter(publisher=OuterRef('pk'), rating__gt=4)\n    publisher_qs = Publisher.objects.annotate(\n        has_highly_rated_book=Exists(published_books),\n        count=Count('book'),\n    ).values_list('name', flat=True)\n    self.assertCountEqual(list(publisher_qs), [\n        'Apress',\n        'Morgan Kaufmann',\n        \"Jonno's House of Books\",\n        'Prentice Hall',\n        'Sams',\n    ])\n", "def test_expression_on_aggregation_with_distinct(self):\n    # Create a plain expression\n    class Greatest(Func):\n        function = 'GREATEST'\n\n            return super().as_sql(compiler, connection, function='MAX', **extra_context)\n\n    qs = Publisher.objects.annotate(\n        price_or_median=Greatest(Avg('book__rating', output_field=DecimalField(), distinct=True), Avg('book__price'))\n    ).filter(price_or_median__gte=F('num_awards')).order_by('num_awards')\n    self.assertQuerysetEqual(\n        qs, [1, 3, 7, 9], lambda v: v.num_awards)\n"], "sample_912": ["def test_pytype_from_parent(app):\n    text = (\".. py:class:: Parent\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"\\n\"\n            \".. py:class:: Child(Parent)\\n\")\n    domain = app.env.get_domain('py')\n    restructuredtext.parse(app, text)\n    assert_node(domain.objects['Child'][1],\n                ([desc_signature, ([desc_annotation, \"class \"],\n                                   [desc_name, \"Child\"],\n                                   [desc_addname, \"(Parent)\"])],\n                 desc_content))\n    assert_node(domain.objects['Child.attr'][1],\n                ([desc_signature, ([desc_name, \"attr\"],\n                                   [desc_annotation, \": type\"])],\n                 desc_content))\n", "def test_pydecorator_function_signature(app):\n    text = \".. py:decorator:: deco(param: str) -> str\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"deco\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'deco' in domain.objects\n    assert domain.objects['deco'] == ('index', 'deco', 'function')\n", "def test_py_module_index_ignores(app):\n    text = (\".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.config\\n\"\n            \".. py:module:: sphinx.builders\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    index.ignores = ['sphinx.']\n    assert index.generate() == (\n        [('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )\n", "def test_pydecorator_function(app):\n    text = (\".. py:decorator:: my_decorator\\n\"\n            \"   :module: my_module\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"my_decorator\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'my_module.my_decorator' in domain.objects\n    assert domain.objects['my_module.my_decorator'] == ('index', 'my_module.my_decorator', 'function')\n", "def test_get_full_qualified_name_with_module_and_class_context():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # with py:module and py:class context\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n\n    # with py:module and py:class context, and nested classes\n    kwargs = {'py:module': 'module1', 'py:class': 'Class.NestedClass'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.NestedClass.func'\n"], "sample_914": ["def test_unparse_arguments():\n    # create a function node with various types of arguments\n    func_node = ast.parse(\"def func(a, b: int, c=3, d: str='hello', *e, f: float, g: bool=False, **h): pass\")\n\n    # get the arguments node\n    args_node = func_node.body[0].args\n\n    # unparse the arguments node\n    unparsed_args = ast.unparse_arguments(args_node)\n\n    expected = \"a, b: int, c=3, d: str='hello', *e, f: float, g: bool=False, **h\"\n    assert unparsed_args == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b: int, c=1, d: str='hello', *args, e, f=2, **kwargs): pass\"\n    module = ast.parse(source)\n    expected = \"a, b: int, c=1, d: str='hello', *args, e, f=2, **kwargs\"\n    assert ast.unparse_arguments(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a, b: int, c=1, d: int=2, *e, f: int, g: int=3, **h): pass\"\n    module = ast.parse(source)\n    expected = \"a, b: int, c=1, d: int=2, *e, f: int, g: int=3, **h\"\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    # test unparse arguments with defaults\n    args = ast.arguments(\n        args=[ast.arg(arg='x', annotation=None), ast.arg(arg='y', annotation=None)],\n        vararg=None,\n        kwonlyargs=[],\n        kw_defaults=[],\n        kwarg=None,\n        defaults=[ast.Num(n=1), ast.Num(n=2)]\n    )\n    assert ast.unparse_arguments(args) == \"x=1, y=2\"\n\n    # test unparse arguments with annotations\n    args = ast.arguments(\n        args=[ast.arg(arg='x', annotation=ast.NameConstant(value=int)), ast.arg(arg='y', annotation=ast.NameConstant(value=str))],\n        vararg=None,\n        kwonlyargs=[],\n        kw_defaults=[],\n        kwarg=None,\n        defaults=[None, None]\n    )\n    assert ast.unparse_arguments(args) == \"x: int, y: str\"\n\n    # test unparse arguments with posonlyargs (py38+)\n    if sys.version_info > (3, 8):\n        args = ast.arguments(\n            args=[ast.arg(arg='x', annotation=None), ast.arg(arg='y', annotation=None)],\n            posonlyargs=[ast.arg(arg='a', annotation=None), ast.arg(arg='b', annotation=None)],\n            vararg=None,\n            kwonlyargs=[],\n            kw_defaults=[],\n            kwarg=None,\n            defaults=[ast.Num(n=1), ast.Num(n=2), ast.Num(n=3), ast.Num(n=4)]\n        )\n        assert ast.unparse_arguments(args) == \"a=1, b=2, /, x=3, y=4\"\n", "def test_unparse_arguments():\n    # Simple positional arguments\n    args = ast.arguments(args=[ast.arg(arg='a'), ast.arg(arg='b')], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[])\n    assert ast.unparse_arguments(args) == \"a, b\"\n\n    # Positional arguments with defaults\n    args = ast.arguments(args=[ast.arg(arg='a'), ast.arg(arg='b')], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[ast.Num(n=1)])\n    assert ast.unparse_arguments(args) == \"a, b=1\"\n\n    # Keyword-only arguments\n    args = ast.arguments(args=[], vararg=None, kwonlyargs=[ast.arg(arg='a'), ast.arg(arg='b')], kw_defaults=[None, ast.Num(n=2)], kwarg=None)\n    assert ast.unparse_arguments(args) == \"*a, b=2\"\n\n    # Variable-length positional and keyword arguments\n    args = ast.arguments(args=[ast.arg(arg='a')], vararg=ast.arg(arg='args'), kwonlyargs=[ast.arg(arg='kwonly')], kw_defaults=[None], kwarg=ast.arg(arg='kwargs'))\n    assert ast.unparse_arguments(args) == \"a, *args, kwonly, **kwargs\"\n"], "sample_915": ["def test_getdoc_inherited_decorated_method():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n        @functools.wraps(Foo.meth)\n            pass\n\n    assert inspect.getdoc(Bar().meth) == \"docstring.\"\n", "def test_getdoc_inherited_decorated_method():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n        @functools.wraps(Foo.meth)\n            pass\n\n    doc = inspect.getdoc(Bar().meth, allow_inherited=True)\n    assert doc == \"docstring.\"\n", "def test_getdoc_inherited_decorated_method():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n        @functools.wraps(Foo.meth)\n            pass\n\n    assert inspect.getdoc(Bar().meth) == \"docstring.\"\n", "def test_getdoc_inherited_decorated_method():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n        @functools.wraps(Foo.meth)\n            pass\n\n    doc = inspect.getdoc(Bar().meth, allow_inherited=True)\n    assert doc == \"docstring.\"\n", "def test_getdoc_partial_function():\n        \"\"\"Original docstring.\"\"\"\n        pass\n\n    partial_func = functools.partial(func, 1)\n    assert inspect.getdoc(partial_func) == \"Original docstring.\"\n"], "sample_916": ["def test_lookup_key():\n    rootSymbol = Symbol(None, None, None, None)\n    s1 = rootSymbol.add_name(\"A\")\n    s2 = s1.add_name(\"B\")\n    s3 = s2.add_declaration(parse('function', 'void f()'), \"file\")\n\n    key = s3.get_lookup_key()\n    assert str(key) == '[((A, None), (B, None), (f, c.void-f()))]'\n", "def test_pointer_to_member():\n    check('type', 'int A::*p', {2: 'M1Ai'})\n    check('type', 'int (A::*)p', {2: 'M1Ai'})\n    check('type', 'int A::* const p', {2: 'KM1Ai'})\n    check('type', 'int const A::*p', {2: 'M1AKi'})\n    check('type', 'int A::*volatile p', {2: 'VM1Ai'})\n    check('type', 'int volatile A::*p', {2: 'M1AVi'})\n    check('type', 'int A::*const volatile p', {2: 'VKM1Ai'})\n    check('type', 'int const volatile A::*p', {2: 'M1AVKi'})\n    check('type', 'int (A::*)()p', {2: 'M1AFivE'})\n    check('type', 'int (A::*)() &p', {2: 'M1AFivR_E'})\n    check('type', 'int (A::*)() &&p', {2: 'M1AFivO_E'})\n    check('type', 'int (A::*)() const p', {2: 'M1AKFivE'})\n    check('type', 'int (A::*)() volatile p', {2: 'M1AVFivE'})\n    check('type', 'int (A::*)() const volatile p', {2: 'M1AVKFivE'})\n", "def test_lookup_key():\n    # Create a symbol\n    root = Symbol(None, None, None, None)\n    symbol = root.add_name(ASTNestedName([ASTIdentifier(\"Test\")], rooted=False))\n    # Get the lookup key for the symbol\n    lookup_key = symbol.get_lookup_key()\n    # Check that the lookup key is correct\n    assert str(lookup_key) == '[(Test, None)]'\n", "def test_misuse_of_roles_with_expressions(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"roles-targets-expressions\")\n    # the roles that should be able to generate warnings:\n    allRoles = ['class', 'struct', 'union', 'func', 'member', 'var', 'type', 'concept', 'enum', 'enumerator']\n    ok = [  # targetType, okRoles\n        ('class', ['class', 'struct', 'type']),\n        ('union', ['union', 'type']),\n        ('func', ['func', 'type']),\n        ('member', ['member', 'var']),\n        ('type', ['type']),\n        ('concept', ['concept']),\n        ('enum', ['type', 'enum']),\n        ('enumerator', ['enumerator']),\n        ('tParam', ['class', 'struct', 'union', 'func', 'member', 'var', 'type', 'concept', 'enum', 'enumerator', 'functionParam']),\n        ('functionParam', ['member', 'var']),\n        ('expression', ['expr', 'texpr'])\n    ]\n    warn = []\n    for targetType, roles in ok:\n        txtTargetType = \"function\" if targetType == \"func\" else targetType\n        for r in allRoles:\n            if r not in roles:\n                warn.append(\"WARNING: cpp:{} targets a {} (\".format(r, txtTargetType))\n    warn = list(sorted(warn))\n    for w in ws:\n        assert \"targets a\" in w\n    ws = [w[w.index(\"WARNING:\"):] for w in ws]\n    ws = list(sorted(ws))\n    print(\"Expected warnings:\")\n    for w in warn:\n        print(w)\n    print(\"Actual warnings:\")\n    for w in ws:\n        print(w)\n\n    for i in range(min(len(warn), len(ws))):\n        assert ws[i].startswith(warn[i])\n\n    assert len(ws) == len(warn)\n", "def test_string_literals():\n        ids = 'IE1CIA%s_1aE'\n        idDict = {2: ids % expr, 3: ids % id}\n        check('class', 'template<> C<a[%s]>' % expr, idDict)\n\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n\n        parser = DefinitionParser(expr, location=None,\n                                  config=Config())\n        parser.allowFallbackExpressionParsing = False\n        ast = parser.parse_expression()\n        res = str(ast)\n        if res != expr:\n            print(\"\")\n            print(\"Input:    \", expr)\n            print(\"Result:   \", res)\n            raise DefinitionError(\"\")\n\n    # character-literal\n    exprCheck('\"abc\"', 'LA3_KcE')  # string\n    # TODO: test more string literals\n\n    # from issue #7294\n    exprCheck('L\"abc\"', 'LA3_KwE')\n    exprCheck('u\"abc\"', 'LA3_DnE')\n    exprCheck('U\"abc\"', 'LA3_DNnE')\n\n    # user-defined literals\n    exprCheck('\"abc\"_udl', 'clL_Zli4_udlELA3_KcEE')\n    # TODO: test more user-defined literals\n\n    # raw string literals\n    exprCheck('R\"(abc)\"', 'LA3_KcE')\n    exprCheck('R\"delim(abc)delim\"', 'LA3_KcE')\n    # TODO: test more raw string literals\n"], "sample_918": ["def test_pyexception(app):\n    text = \".. py:exception:: MyException\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"MyException\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n\n    assert 'MyException' in domain.objects\n    assert domain.objects['MyException'] == ('index', 'MyException', 'exception')\n", "def test_pyattribute_signature(app):\n    text = (\".. py:attribute:: attr\\n\"\n            \"   :type: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"attribute\",\n                domain=\"py\", objtype=\"attribute\", noindex=False)\n\n    assert 'attr' in app.env.domains['py'].objects\n    assert app.env.domains['py'].objects['attr'] == ('index', 'attr', 'attribute')\n", "def test_pyclasslike_signature(app):\n    text = \".. py:class:: Class\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"class\",\n                domain=\"py\", objtype=\"class\", noindex=False)\n\n    assert 'Class' in domain.objects\n    assert domain.objects['Class'] == ('index', 'Class', 'class')\n", "def test_pyfunction_with_module_prefix(app):\n    text = (\".. py:function:: example.func\\n\"\n            \"   :module: example\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"example.\"],\n                                                    [desc_name, \"func\"],\n                                                    [desc_parameterlist, ()])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'example.func' in domain.objects\n    assert domain.objects['example.func'] == ('index', 'example.func', 'function')\n", "def test_python_module_index_ignores(app):\n    app.config.modindex_common_prefix = ['sphinx.']\n    text = (\".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.builders\\n\"\n            \".. py:module:: sphinx.builders.html\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('b', [IndexEntry('builders', 1, 'index', 'module-sphinx.builders', '', '', ''),\n                IndexEntry('builders.html', 2, 'index', 'module-sphinx.builders.html', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )\n"], "sample_920": ["def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"\\", "def test_usage_section(self):\n    docstring = \"\"\"", "def test_usage_section(self):\n    docstring = \"\"\"", "def test_usage_section(self):\n    docstring = \"\"\"Usage:\n    The usage text."], "sample_921": ["def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'e'\n    assert argspec.varkw == 'f'\n    assert argspec.defaults == (1, 2)\n\n    # Test with bound method\n    class Foo:\n            pass\n\n    foo = Foo()\n    argspec = inspect.getargspec(foo.meth)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'e'\n    assert argspec.varkw == 'f'\n    assert argspec.defaults == (1, 2)\n", "def test_getdoc_inherited_docstring():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    class Bar(Foo):\n            pass\n\n    assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") is None\n    assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") == \"docstring.\"\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(normal_func) is False\n\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(normal_fun) is False\n\n", "def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(not_singledispatch_fun) is False\n\n"], "sample_922": ["def test_pydecoratorfunction_signature(app):\n    text = \".. py:decorator:: deco\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"deco\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'deco' in domain.objects\n    assert domain.objects['deco'] == ('index', 'deco', 'function')\n", "def test_pyfunction_signature_with_default_value(app):\n    text = \".. py:function:: hello(name: str = 'World') -> str\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"str\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"str\"],\n                                                        [desc_sig_operator, \"=\"],\n                                                        [nodes.inline, \"'World'\"])])])\n", "def test_pydecorator_function_signature(app):\n    text = \".. py:function:: deco\\n   :decorator:\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"deco\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'deco' in domain.objects\n    assert domain.objects['deco'] == ('index', 'deco', 'function')\n", "def test_pydecorator_function(app):\n    text = (\".. py:decorator:: my_decorator\\n\"\n            \"   :module: my_module\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"my_decorator\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert 'my_module.my_decorator' in domain.objects\n    assert domain.objects['my_module.my_decorator'] == ('index', 'my_module.my_decorator', 'function')\n", "def test_pyattribute_with_type_annotation(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: str\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"str\"])])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n"], "sample_925": ["def test_MockObject_repr():\n    mock = _MockObject()\n    mock.__display_name__ = 'MockedObject'\n    assert repr(mock) == 'MockedObject'\n\n    mock_subclass = mock.SomeSubClass()\n    assert repr(mock_subclass) == 'MockedObject.SomeSubClass'\n\n    mock_subclass_method = mock_subclass.some_method()\n    assert repr(mock_subclass_method) == 'MockedObject.SomeSubClass.some_method'\n", "def test_MockObject_repr():\n    mock = _MockObject()\n    mock.__qualname__ = 'SomeName'\n    assert repr(mock) == 'SomeName'\n\n    mock = _MockObject()\n    mock.__display_name__ = 'SomeDisplayName'\n    assert repr(mock) == 'SomeDisplayName'\n", "def test_MockObject_repr_with_subclass():\n    class SubClass(_MockObject):\n        pass\n\n    mock = SubClass()\n    assert repr(mock) == '_MockObject'\n    assert repr(mock.some_attr) == 'SubClass.some_attr'\n    assert repr(mock.some_method) == 'SubClass.some_method'\n", "def test_MockObject_repr():\n    mock = _MockObject()\n    mock.__display_name__ = 'MockedObject'\n    assert repr(mock) == 'MockedObject'\n    assert repr(mock.some_attr) == 'MockedObject.some_attr'\n    assert repr(mock.some_method) == 'MockedObject.some_method'\n\n    # subclassing\n    class SubClass(mock.SomeClass):\n        pass\n\n    obj = SubClass()\n    assert repr(obj) == 'MockedObject.SomeClass'\n", "def test_MockObject_repr():\n    mock = _MockObject()\n    mock.__qualname__ = 'SomeQualName'\n    assert repr(mock) == '_MockObject'\n\n    class SubClass(_MockObject):\n        pass\n\n    sub_mock = SubClass()\n    sub_mock.__qualname__ = 'SomeQualName'\n    assert repr(sub_mock) == '_MockObject'\n"], "sample_928": ["def test_heading():\n    env = Environment()\n    assert heading(env, 'Hello', 1) == 'Hello\\n======='\n    assert heading(env, '\u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a', 2) == '\u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a\\n------------'\n    assert heading(env, '\u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a', 3) == '\u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a\\n^^^^^^^^^^^^'\n", "def test_heading():\n    env = Environment()\n    assert heading(env, 'Hello World', 1) == 'Hello World\\n============='\n    assert heading(env, '\u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a', 2) == '\u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a\\n---------------'\n    assert heading(env, 'Sphinx is a document generator', 3) == 'Sphinx is a document generator\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'\n", "def test_heading():\n    env = Environment()\n    text = 'Heading'\n    level = 1\n    result = heading(env, text, level)\n    assert result == 'Heading\\n=======\\n'\n\n    text = ''\n    result = heading(env, text, level)\n    assert result == '\\n'\n\n    text = 'Long Heading That Should Be Longer Than The Sectioning Char'\n    result = heading(env, text, level)\n    assert len(result.split('\\n')[1]) == len(text)\n\n    level = 2\n    result = heading(env, text, level)\n    assert result.split('\\n')[1][0] == '-'\n\n    level = 3\n    result = heading(env, text, level)\n    assert result.split('\\n')[1][0] == '~'\n", "def test_heading():\n    env = Environment()\n    text = 'Heading Example'\n    level = 1\n\n    result = heading(env, text, level)\n    assert result == 'Heading Example\\n===============\\n'\n\n    level = 2\n    result = heading(env, text, level)\n    assert result == 'Heading Example\\n---------------\\n'\n\n    level = 3\n    result = heading(env, text, level)\n    assert result == 'Heading Example\\n~~~~~~~~~~~~~~~\\n'\n", "def test_heading():\n    env = Environment()\n    assert heading(env, \"Heading\", 1) == \"Heading\\n=========\"\n    assert heading(env, \"Heading\", 2) == \"Heading\\n---------\"\n    assert heading(env, \"Heading\", 3) == \"Heading\\n~~~~~~~~~\"\n\n    # Test with wide characters\n    WIDECHARS[\"test\"] = \"WF\"\n    env.language = \"test\"\n    assert heading(env, \"\", 1) == \"\\n====\"\n    WIDECHARS[\"test\"] = \"WFA\"\n    assert heading(env, \"\", 1) == \"\\n====\"\n"], "sample_929": ["def test_get_index_text_pyfunction(app):\n    domain = app.env.get_domain('py')\n    assert domain.get_index_text('module', ('function', '')) == 'function() (in module module)'\n    assert domain.get_index_text('', ('function', '')) == 'function() (built-in function)'\n", "def test_pytype_output(app):\n    text = (\".. py:function:: hello(name: str) -> str\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][2][1], pending_xref, **{\"py:class\": \"None\"})\n    assert_node(doctree[1][0][3][1], pending_xref, **{\"py:class\": \"None\"})\n\n    text = (\".. py:function:: hello(name: typing.List[str]) -> typing.Tuple[str]\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][2][1][2][1], pending_xref, **{\"py:class\": \"None\"})\n    assert_node(doctree[1][0][3][1][2][1], pending_xref, **{\"py:class\": \"None\"})\n", "def test_pymodule(app):\n    text = (\".. py:module:: example\\n\"\n            \"   :platform: Unix, Windows\\n\"\n            \"   :synopsis: Example module\\n\"\n            \"   :deprecated:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index))\n    assert 'example' in domain.modules\n    assert domain.modules['example'] == ('index', 'module-example',\n                                         'Example module', 'Unix, Windows', True)\n", "def test_pyattribute_with_module(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:attribute:: attr\\n\"\n            \"   :type: Optional[str]\\n\"\n            \"   :value: ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"example.\"],\n                                                    [desc_name, \"attr\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"Optional\"],\n                                                                       [desc_sig_punctuation, \"[\"],\n                                                                       [pending_xref, \"str\"],\n                                                                       [desc_sig_punctuation, \"]\"])],\n                                                    [desc_annotation, \" = ''\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[3][0][2][1], pending_xref, **{\"py:module\": \"example\"})\n    assert_node(doctree[3][0][2][3], pending_xref, **{\"py:module\": \"example\"})\n    assert 'example.attr' in domain.objects\n    assert domain.objects['example.attr'] == ('index', 'example.attr', 'attribute')\n", "def test_pyvariable_signature(app):\n    text = (\".. py:variable:: var\\n\"\n            \"   :type: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"var\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n\n    assert 'var' in app.env.domains['py'].objects\n    assert app.env.domains['py'].objects['var'] == ('index', 'var', 'data')\n"], "sample_930": ["def test_create_index_with_category_key(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :key: category1\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :key: category2\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('category1', [('docutils', [[('', '#index-0')], [], 'category1'])])\n    assert index[1] == ('category2', [('Python', [[('', '#index-1')], [], 'category2'])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Python; documentation tool\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=True)\n\n    # check index is created correctly\n    assert len(index) == 2\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('Python', [[],\n                                     [('documentation tool', [('', '#index-2')]),\n                                      ('interpreter', [('', '#index-1')])],\n                                     None])])\n\n    # check the reference labels are created correctly\n    std = app.env.get_domain('std')\n    assert std.anonlabels['index-0'] == ('index', 'index-0')\n    assert std.anonlabels['index-1'] == ('index', 'index-1')\n    assert std.anonlabels['index-2'] == ('index', 'index-2')\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: docutils\\n\"\n            \".. index:: Python\\n\"\n            \".. index:: pip; install\\n\"\n            \".. index:: pip; upgrade\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')])], None]),\n                              ('pip', [[], [('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n", "def test_create_index_with_non_ascii_name(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :name: r\u00e9f1\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :name: ref2\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('D', [('docutils', [[('', '#r\u00e9f1')], [], None])])\n    assert index[1] == ('P', [('Python', [[('', '#ref2')], [], None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-0')], [], None])])\n\n    # check the reference labels are created correctly\n    std = app.env.get_domain('std')\n    assert std.anonlabels['r\u00e9f1'] == ('index', 'r\u00e9f1')\n    assert std.anonlabels['ref2'] == ('index', 'ref2')\n", "def test_create_index_with_non_ascii_name(app):\n    text = (\".. index:: single: \u0415\u043b\u044c\\n\"\n            \"   :name: \u0435\u043b\u044c-ref\\n\"\n            \".. index:: single: \u0451\u043b\u043a\u0430\\n\"\n            \"   :name: \u0451\u043b\u043a\u0430-ref\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 1\n    assert index[0] == ('\u0415', [('\u0451\u043b\u043a\u0430', [[('', '#\u0451\u043b\u043a\u0430-ref')], [], None]),\n                               ('\u0415\u043b\u044c', [[('', '#\u0435\u043b\u044c-ref')], [], None])])\n\n    # check the reference labels are created correctly\n    std = app.env.get_domain('std')\n    assert std.anonlabels['\u0435\u043b\u044c-ref'] == ('index', '\u0435\u043b\u044c-ref')\n    assert std.anonlabels['\u0451\u043b\u043a\u0430-ref'] == ('index', '\u0451\u043b\u043a\u0430-ref')\n"], "sample_931": ["def test_get_full_qualified_name_for_module():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # module reference\n    node = nodes.reference(reftarget='module1')\n    node['py:module'] = 'module1'\n    assert domain.get_full_qualified_name(node) == 'module1'\n\n    # module reference with class context\n    node = nodes.reference(reftarget='module1')\n    node['py:module'] = 'module1'\n    node['py:class'] = 'Class'\n    assert domain.get_full_qualified_name(node) == 'module1.Class'\n", "def test_pyattribute_annotation_old(app):\n    text = (\".. py:attribute:: attr\\n\"\n            \"   :annotation: = 'value'\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, \" = 'value'\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"attribute\",\n                domain=\"py\", objtype=\"attribute\", noindex=False)\n", "def test_pyfunction_signature_with_typehints(app):\n    text = \".. py:function:: hello(name: str, age: int) -> None\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"str\"])],\n                                      [desc_parameter, ([desc_sig_name, \"age\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"int\"]])])\n", "def test_pyattribute_with_module(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:attribute:: attr\\n\"\n            \"   :type: int\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"example.\"],\n                                                    [desc_name, \"attr\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[3][0][2][1], pending_xref, **{\"py:module\": \"example\"})\n    assert 'example.attr' in domain.objects\n    assert domain.objects['example.attr'] == ('index', 'example.attr', 'attribute')\n", "def test_pyfunction_signature_with_type_annotation(app):\n    text = \".. py:function:: hello(name: str, age: int) -> None\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"str\"])],\n                                      [desc_parameter, ([desc_sig_name, \"age\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"int\"])]),\n                 desc_returns, ([pending_xref, \"None\"],)])\n"], "sample_933": ["def test_gettext_should_write(app, tmp_path):\n    pot_file = tmp_path / 'example.pot'\n    pot_file.write_text('msgid \"\"\\nmsgstr \"\"\\n\"Project-Id-Version: 1.0\\\\n\"\\n')\n    new_content = 'msgid \"\"\\nmsgstr \"\"\\n\"Project-Id-Version: 2.0\\\\n\"\\n'\n\n    assert should_write(str(pot_file), new_content) is True\n\n    new_content = 'msgid \"\"\\nmsgstr \"\"\\n\"Project-Id-Version: 1.0\\\\n\"\\n'\n    assert should_write(str(pot_file), new_content) is False\n\n    pot_file.write_text('Invalid pot file')\n    new_content = 'msgid \"\"\\nmsgstr \"\"\\n\"Project-Id-Version: 1.0\\\\n\"\\n'\n    assert should_write(str(pot_file), new_content) is True\n", "def test_gettext_display_location(app):\n    app.config.gettext_location = False\n    app.builder.build_all()\n    result = (app.outdir / 'index.pot').read_text()\n    assert '#: ' not in result\n\n    app.config.gettext_location = True\n    app.builder.build_all()\n    result = (app.outdir / 'index.pot').read_text()\n    assert '#: ' in result\n\n", "def test_gettext_location_and_uuid(app):\n    app.config.gettext_location = False\n    app.config.gettext_uuid = True\n    app.builder.build_all()\n\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert '#: index_entries.rst:' not in pot\n    assert '# uuid: ' in pot\n\n    app.config.gettext_location = True\n    app.config.gettext_uuid = False\n    app.builder.build_all()\n\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert '#: index_entries.rst:' in pot\n    assert '# uuid: ' not in pot\n", "def test_gettext_location_uuid(app):\n    # Test gettext_location and gettext_uuid config values\n    app.builder.build_all()\n    pot = (app.outdir / 'index_entries.pot').read_text()\n\n    # Check location comments\n    assert '#: ../../source/gettext/index_entries.rst:' in pot\n\n    # Check msgid has UUID\n    _msgid_getter = re.compile(r'msgid \"(.*)\"').search\n    _location_getter = re.compile(r'#: (.*)').search\n    _uuid_getter = re.compile(r'#~ msgid \"(.*)').search\n    for line in pot.splitlines():\n        m = _msgid_getter(line)\n        if m:\n            msgid = m.groups()[0]\n            location_match = _location_getter(line)\n            uuid_match = _uuid_getter(line)\n            if location_match:\n                assert location_match.groups()[0].startswith('../../source/gettext/')\n            if uuid_match:\n                assert len(uuid_match.groups()[0]) == 32\n", "def test_gettext_display_location_and_uuid(app):\n    app.config.gettext_location = False\n    app.config.gettext_uuid = True\n    app.builder.build_all()\n\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert '#: ' not in pot  # location comments should be disabled\n    assert '#, fuzzy' not in pot  # uuids should be enabled\n\n    app.config.gettext_location = True\n    app.config.gettext_uuid = False\n    app.builder.build_all()\n\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert '#: ' in pot  # location comments should be enabled\n    assert '#, fuzzy' in pot  # uuids should be disabled\n"], "sample_936": ["def test_stringify_broken_type():\n    assert stringify(BrokenType) == \"BrokenType\"\n", "def test_stringify_type_hints_broken():\n    assert stringify(BrokenType) == \"BrokenType\"\n    assert stringify(List[BrokenType]) == \"List[BrokenType]\"\n", "def test_stringify_broken_type():\n    assert stringify(BrokenType) == \"<class 'test_util_typing.BrokenType'>\"\n", "def test_stringify_type_hints_broken():\n    assert stringify(BrokenType) == \"<class 'test_util_typing.BrokenType'>\"\n", "def test_stringify_broken_type():\n    assert stringify(BrokenType) == \"BrokenType\"\n"], "sample_937": ["def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a: int, b: str = 'default', *args: list, **kwargs: dict): pass\"\n    module = ast.parse(source)\n    expected = \"a: int, b: str = 'default', *args: list, **kwargs: dict\"\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a: int, b: str = 'default', *args, **kwargs): pass\"\n    module = ast.parse(source)\n    expected = \"a: int, b: str = 'default', *args, **kwargs\"\n    assert ast.unparse(module.body[0].args) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a: int, b: str = 'default', *args, **kwargs): pass\"\n    module = ast.parse(source)\n    expected = \"a: int, b: str = 'default', *args, **kwargs\"\n    assert ast.unparse(module.body[0].args) == expected\n"], "sample_10": ["def test_pprint_include_exclude():\n    t = Table([[1, 2, 3], [4, 5, 6], [7, 8, 9]], names=['a', 'b', 'c'])\n    t.pprint_exclude_names.add('b')\n    assert t.pprint_include_names() is None\n    assert t.pprint_exclude_names() == ('b',)\n\n    t.pprint_include_names.add('a')\n    assert t.pprint_include_names() == ('a',)\n    assert t.pprint_exclude_names() == ('b',)\n\n    t.pprint_exclude_names.remove('b')\n    assert t.pprint_include_names() == ('a',)\n    assert t.pprint_exclude_names() is None\n\n    t.pprint_include_names.remove('a')\n    assert t.pprint_include_names() is None\n    assert t.pprint_exclude_names() is None\n\n    # Check that setting to empty list works\n    t.pprint_exclude_names.set([])\n    assert t.pprint_exclude_names() is None\n\n    # Check that setting to None clears\n    t.pprint_exclude_names.set(['a'])\n    assert t.pprint_exclude_names() == ('a',)\n    t.pprint_exclude_names.set(None)\n    assert t.pprint_exclude_names() is None\n", "    def test_meta(self, table_types):\n        self._setup(table_types)\n        meta = {'a': 1, 'b': 2}\n        t = table_types.Table(self.a, meta=meta)\n        assert t.meta == meta\n", "def test_table_attribute_multiple_tables():\n    # Test that TableAttributes work as expected when multiple tables are created.\n    class MyTable(Table):\n        foo = TableAttribute()\n        bar = TableAttribute(default=[])\n\n    t1 = MyTable()\n    t2 = MyTable()\n    t1.foo = 'foo1'\n    t2.foo = 'foo2'\n    t1.bar.append('bar1')\n    t2.bar.append('bar2')\n\n    assert t1.foo == 'foo1'\n    assert t2.foo == 'foo2'\n    assert t1.bar == ['bar1']\n    assert t2.bar == ['bar2']\n", "def test_remove_row_mixin():\n    \"\"\"Test remove_row() for mixin column\"\"\"\n    sc = SkyCoord([1, 2], [3, 4], unit='deg')\n    t = Table([[2, 1], sc], names=['a', 'sc'])\n    t.remove_row(0)\n    assert np.all(t['a'] == [1])\n    assert np.allclose(t['sc'].ra.to_value('deg'), [2])\n    assert np.allclose(t['sc'].dec.to_value('deg'), [4])\n", "def test_row_index_non_default_key():\n    \"\"\"Test show_in_notebook with row index and non-default key\"\"\"\n    t = Table([[1, 2, 3], [4, 5, 6]], names=('a', 'b'))\n    html = t.show_in_notebook(show_row_index='my_index')\n    assert 'my_index' in html\n"], "sample_19": ["def test_unique_axis_names():\n    \"\"\"\n    Regression test for #12820\n    \"\"\"\n    header = fits.Header()\n    header[\"CTYPE1\"] = \"RA---TAN\"\n    header[\"CTYPE2\"] = \"RA---TAN\"\n    header[\"CRVAL1\"] = 0\n    header[\"CRVAL2\"] = 0\n    header[\"CRPIX1\"] = 1\n    header[\"CRPIX2\"] = 1\n    header[\"CDELT1\"] = 1\n    header[\"CDELT2\"] = 1\n    header[\"CUNIT1\"] = \"deg\"\n    header[\"CUNIT2\"] = \"deg\"\n    header[\"PC1_1\"] = 1\n    header[\"PC1_2\"] = 0\n    header[\"PC2_1\"] = 0\n    header[\"PC2_2\"] = 1\n\n    w = wcs.WCS(header)\n\n    assert w.axis_type_names == [\"RA---TAN\", \"RA---TAN_1\"]\n", "def test_proj_plane_pixel_scales_units():\n    w = wcs.WCS(naxis=2)\n    w.wcs.cunit = [\"deg\", \"arcsec\"]\n    w.wcs.cdelt = [1, 1]\n    assert_quantity_allclose(\n        w.proj_plane_pixel_scales(),\n        [1 * u.deg, (1 * u.arcsec).to(u.deg)],\n        rtol=1e-7,\n    )\n", "def test_sub_spectral():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.set()\n    assert list(w.sub([wcs.WCSSUB_SPECTRAL]).wcs.ctype) == [\"FREQ\"]\n    assert (\n        w.wcs_pix2world([[1, 2, 3]], 0)[0, 2]\n        == w.sub([wcs.WCSSUB_SPECTRAL]).wcs_pix2world([[3]], 0)[0, 0]\n    )\n", "def test_pixel_bounds():\n    w = wcs.WCS(naxis=2)\n    w._pixel_bounds = None\n    assert w.pixel_bounds is None\n    w._pixel_bounds = [(1, 10), (1, 5)]\n    assert w.pixel_bounds == [(1, 10), (1, 5)]\n    w._pixel_bounds = np.array([(1, 10), (1, 5)])\n    assert_array_equal(w.pixel_bounds, np.array([(1, 10), (1, 5)]))\n    w.pixel_bounds = None\n    assert w.pixel_bounds is None\n", "def test_wcs_invalid_units():\n    # Create a WCS object\n    w = wcs.WCS(naxis=2)\n\n    # Set celestial equatorial coordinates\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.crval = [0, 0]\n    w.wcs.crpix = [1, 1]\n    w.wcs.cdelt = [1, 1]\n\n    # Test that invalid units raise an exception\n    with pytest.raises(wcs.wcs.InvalidUnitError):\n        w.wcs.cunit = [\"deg\", \"banana\"]\n"], "sample_55": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.event = Event.objects.create(name='Event 1', start_date='2022-01-01', end_date='2022-01-02')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.joepublicuser = User.objects.create_user(username='joepublic', password='secret')\n        cls.joepublicuser.is_staff = True\n        cls.joepublicuser.save()\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.joepublicuser = User.objects.create_user(username='joepublic', password='secret')\n"], "sample_66": ["    def test_create_from_bytes(self):\n        qd = QueryDict(b'x=1&y=2', mutable=True)\n        self.assertEqual(qd['x'], ['1'])\n        self.assertEqual(qd['y'], ['2'])\n", "    def test_httprequest_encoding(self):\n        request = HttpRequest()\n        self.assertIsNone(request.encoding)\n\n        request.encoding = 'utf-8'\n        self.assertEqual(request.encoding, 'utf-8')\n\n        request.encoding = None\n        self.assertIsNone(request.encoding)\n\n        # Setting encoding should delete the cached POST QueryDict\n        request.POST = QueryDict(mutable=True)\n        request.encoding = 'utf-8'\n        self.assertFalse(hasattr(request, '_post'))\n\n        # Setting encoding should delete the cached GET QueryDict\n        request.GET = QueryDict(mutable=True)\n        request.encoding = 'utf-8'\n        self.assertFalse(hasattr(request, 'GET'))\n", "def test_http_headers_get_all(self):\n    environ = {\n        'CONTENT_TYPE': 'text/html',\n        'CONTENT_LENGTH': '100',\n        'HTTP_HOST': 'example.com',\n        'HTTP_ACCEPT': 'text/plain, text/html',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(headers.get('Host'), 'example.com')\n    self.assertEqual(headers.get('Content-Type'), 'text/html')\n    self.assertEqual(headers.get('Accept'), 'text/plain, text/html')\n    self.assertEqual(headers.get('Non-Existent-Header'), None)\n", "def test_http_headers_repr(self):\n    environ = {\n        'CONTENT_TYPE': 'text/html',\n        'CONTENT_LENGTH': '100',\n        'HTTP_HOST': 'example.com',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(repr(headers), \"HttpHeaders({'Content-Type': 'text/html', 'Content-Length': '100', 'Host': 'example.com'})\")\n", "    def test_create(self):\n        QueryDict('a=1&a=2&b=3')\n        QueryDict({'a': 1})\n        QueryDict([('a', 1), ('b', 2)])\n"], "sample_95": ["    def test_patch_cache_control_decorator(self):\n        @patch_cache_control(private=True, max_age=3600)\n            response = HttpResponse()\n            response['Cache-Control'] = 'must-revalidate'\n            return response\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'must-revalidate', 'max-age=3600', 'private'},\n        )\n", "    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Language'])\n        self.assertEqual(response['Vary'], 'Accept-Language')\n", "    def test_learn_cache_key(self):\n            return HttpResponse('Hello, world!')\n        request = HttpRequest()\n        request.method = 'GET'\n        request.path = '/path/to/page'\n        response = my_view(request)\n        patch_vary_headers(response, ['Accept-Language'])\n        cache_key = learn_cache_key(request, response)\n        self.assertIsNotNone(cache_key)\n", "    def test_add_header(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Language'])\n        self.assertEqual(response['Vary'], 'Accept-Language')\n", "    def test_patch_cache_control_decorator(self):\n        @cache_control(max_age=3600, public=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r['Cache-Control'], 'max-age=3600, public')\n"], "sample_124": ["def test_field_disabled(self):\n    class MyForm(Form):\n        f1 = CharField(max_length=30)\n        f2 = CharField(max_length=30, disabled=True)\n        f3 = CharField(widget=Textarea, disabled=True)\n        f4 = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')], disabled=True)\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\" required></p>'\n        '<p><label for=\"id_f2\">F2:</label> <input disabled id=\"id_f2\" maxlength=\"30\" name=\"f2\" type=\"text\" required></p>'\n        '<p><label for=\"id_f3\">F3:</label> <textarea cols=\"40\" disabled id=\"id_f3\" name=\"f3\" rows=\"10\" required>'\n        '</textarea></p>'\n        '<p><label for=\"id_f4\">F4:</label> <select disabled id=\"id_f4\" name=\"f4\">'\n        '<option value=\"P\">Python</option>'\n        '<option value=\"J\">Java</option>'\n        '</select></p>',\n    )\n    self.assertHTMLEqual(\n        form.as_ul(),\n        '<li><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\" required></li>'\n        '<li><label for=\"id_f2\">F2:</label> <input disabled id=\"id_f2\" maxlength=\"30\" name=\"f2\" type=\"text\" required></li>'\n        '<li><label for=\"id_f3\">F3:</label> <textarea cols=\"40\" disabled id=\"id_f3\" name=\"f3\" rows=\"10\" required>'\n        '</textarea></li>'\n        '<li><label for=\"id_f4\">F4:</label> <select disabled id=\"id_f4\" name=\"f4\">'\n        '<option value=\"P\">Python</option>'\n        '<option value=\"J\">Java</option>'\n        '</select></li>',\n    )\n    self.assertHTMLEqual(\n        form.as_table(),\n        '<tr><th><label for=\"id_f1\">F1", "def test_field_order_with_key_function(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n        field4 = CharField()\n        field5 = CharField()\n\n            super().__init__(*args, **kwargs)\n            self.order_fields(key=lambda field: field.name[::-1])\n\n    p = TestForm()\n    self.assertEqual(list(p.fields), ['field5', 'field4', 'field3', 'field2', 'field1'])\n", "def test_fieldRenderer(self):\n    class CustomRenderer(forms.Renderer):\n            return \"CustomRenderer\"\n\n    class CustomForm(forms.Form):\n        field = forms.CharField()\n\n    form = CustomForm(renderer=CustomRenderer())\n    self.assertEqual(form.as_table(), \"CustomRenderer\")\n", "def test_field_with_choices(self):\n    class MyForm(Form):\n        f1 = CharField(choices=[('P', 'Python'), ('J', 'Java')])\n        f2 = CharField(choices=[('P', 'Python'), ('J', 'Java')], required=False)\n        f3 = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')])\n        f4 = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')], required=False)\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_f1\">F1:</label> '\n        '<select name=\"f1\" id=\"id_f1\" required>'\n        '<option value=\"P\">Python</option><option value=\"J\">Java</option></select></p>'\n        '<p><label for=\"id_f2\">F2:</label> '\n        '<select name=\"f2\" id=\"id_f2\">'\n        '<option value=\"P\">Python</option><option value=\"J\">Java</option></select></p>'\n        '<p><label for=\"id_f3\">F3:</label> '\n        '<select name=\"f3\" id=\"id_f3\" required>'\n        '<option value=\"P\">Python</option><option value=\"J\">Java</option></select></p>'\n        '<p><label for=\"id_f4\">F4:</label> '\n        '<select name=\"f4\" id=\"id_f4\">'\n        '<option value=\"P\">Python</option><option value=\"J\">Java</option></select></p>'\n    )\n", "def test_add_prefix_with_no_prefix(self):\n    class Person(Form):\n        first_name = CharField()\n\n    p = Person()\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n"], "sample_297": ["    def setUpTestData(cls):\n        cls.tag = Tag.objects.create(name='abc')\n        cls.annotation1 = Annotation.objects.create(tag=cls.tag)\n        cls.annotation2 = Annotation.objects.create(tag=cls.tag)\n", "    def test_ticket_24768(self):\n        \"\"\"\n        Test that a Derived QuerySet does not throw an AttributeError.\n        \"\"\"\n        class DerivedQuerySet(models.QuerySet):\n                return self.filter()\n\n        DerivedModel = models.Model\n        DerivedModel.objects = DerivedQuerySet.as_manager()\n        self.assertEqual(list(DerivedModel.objects.all()), [])\n        self.assertEqual(list(DerivedModel.objects.new_method()), [])\n", "    def test_filter_on_filtered_relation(self):\n        staff = Staff.objects.create(name='John')\n        task = Task.objects.create(title='Task', creator=staff)\n        self.assertSequenceEqual(\n            Task.objects.filter(creator__name='John', creator__staff__name='John'),\n            [task]\n        )\n        self.assertSequenceEqual(\n            Task.objects.filter(creator__name='John', creator__staffuser__staff__name='John'),\n            [task]\n        )\n        self.assertSequenceEqual(\n            Task.objects.filter(creator__staff__name='John', creator__staffuser__staff__name='John'),\n            [task]\n        )\n", "    def test_ticket_24912(self):\n        \"\"\"\n        Ensure that a queryset's ordering is preserved when using None as a value\n        in order_by() (#24912).\n        \"\"\"\n        self.n1 = Note.objects.create(note='n1', misc='foo', id=1)\n        self.n2 = Note.objects.create(note='n2', misc='bar', id=2)\n\n        # Test that None in order_by preserves any existing ordering.\n        qs = Note.objects.order_by('note').order_by(None)\n        self.assertEqual(qs.query.order_by, ('note',))\n\n        # Test that None as the only order_by value clears ordering.\n        qs = Note.objects.order_by(None)\n        self.assertEqual(qs.query.order_by, ())\n", "    def test_ticket_24989(self):\n        # A Q() object with an empty __in lookup should be considered empty.\n        qs1 = Tag.objects.filter(Q(name__in=[]))\n        qs2 = Tag.objects.filter(name__in=[])\n        self.assertEqual(str(qs1.query), str(qs2.query))\n"], "sample_136": ["    def test_create_query_dict_from_query_string(self):\n        query_string = \"a=1&b=2&c=3&a=4\"\n        query_dict = QueryDict(query_string)\n        self.assertEqual(query_dict.getlist(\"a\"), [\"1\", \"4\"])\n        self.assertEqual(query_dict.get(\"b\"), \"2\")\n        self.assertEqual(query_dict.get(\"c\"), \"3\")\n", "    def test_getitem(self):\n        q = QueryDict('', mutable=True)\n        q['key'] = 'value'\n        self.assertEqual(q['key'], ['value'])\n", "    def test_create_querydict_from_bytes(self):\n        q = QueryDict(b'key=value')\n        self.assertEqual(q.get('key'), ['value'])\n", "    def test_create_from_bytes(self):\n        q = QueryDict(b'x=1&y=2', mutable=True)\n        self.assertEqual(q['x'], ['1'])\n        self.assertEqual(q['y'], ['2'])\n", "    def test_create(self):\n        q = QueryDict('a=1&a=2&b=3')\n        self.assertEqual(list(q.items()), [('a', '1'), ('a', '2'), ('b', '3')])\n"], "sample_311": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.s1 = Section.objects.create(name='Test section')\n        cls.a1 = Article.objects.create(\n            content='<p>Middle content</p>', date=datetime.datetime(2008, 3, 18, 11, 54, 58), section=cls.s1\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.obj = UnchangeableObject.objects.create()\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.user = User.objects.create_user(username='user', password='secret', email='user@example.com')\n        cls.book = Book.objects.create(name='Test Book')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n"], "sample_319": ["compilation error", "def test_swappable_dependency_added(self):\n    \"\"\"\n    When a swappable dependency is added, the autodetector should create a\n    migration with the correct dependency.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty], [self.author_with_custom_user, self.custom_user]\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\")\n    self.assertMigrationDependencies(\n        changes, \"testapp\", 0, [(\"__setting__\", \"AUTH_USER_MODEL\")]\n    )\n", "def test_swappable_first_dependency(self):\n    \"\"\"\n    The dependency for a swappable model is the first migration of the\n    app where the swappable model is defined.\n    \"\"\"\n    with isolate_lru_cache(apps.get_swappable_settings_name):\n        swappable = ModelState(\n            \"swappable\",\n            \"Swappable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            bases=(AbstractBaseUser,),\n        )\n        other = ModelState(\n            \"other\",\n            \"Other\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"swappable\",\n                    models.ForeignKey(\n                        settings.AUTH_USER_MODEL, models.CASCADE\n                    ),\n                ),\n            ],\n        )\n        changes = self.get_changes([], [swappable, other])\n        self.assertNumberMigrations(changes, \"swappable\", 1)\n        self.assertNumberMigrations(changes, \"other\", 1)\n        self.assertMigrationDependencies(\n            changes,\n            \"other\",\n            0,\n            [(\"swappable\", \"__first__\")],\n        )\n", "def test_add_model_order_with_respect_to_unique_together_index_together(self):\n    changes = self.get_changes(\n        [],\n        [\n            AutodetectorTests.book,\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                ],\n                options={\n                    \"order_with_respect_to\": \"book\",\n                    \"unique_together\": {(\"id\", \"_order\")},\n                    \"index_together\": {(\"name\", \"_order\")},\n                },\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"Author\",\n        options={\n            \"order_with_respect_to\": \"book\",\n            \"unique_together\": {(\"id\", \"_order\")},\n            \"index_together\": {(\"name\", \"_order\")},\n        },\n    )\n", "def test_optimize_migrations(self):\n    \"\"\"\n    MigrationOptimizer should optimize the operations of a migration.\n    \"\"\"\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\n                name=\"Author\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            ),\n            migrations.AddField(\n                model_name=\"author\",\n                name=\"book\",\n                field=models.ForeignKey(\"otherapp.Book\", models.CASCADE),\n            ),\n            migrations.AlterField(\n                model_name=\"author\",\n                name=\"book\",\n                field=models.ForeignKey(\"otherapp.Book\", models.CASCADE),\n            ),\n            migrations.AlterField(\n                model_name=\"author\",\n                name=\"name\",\n                field=models.CharField(max_length=200),\n            ),\n        ]\n\n    optimizer = MigrationOptimizer()\n    optimized_operations = optimizer.optimize(Migration(\"some_migration\", \"test_app\").operations, \"test_app\")\n    self.assertEqual(len(optimized_operations), 2)\n    self.assertEqual(optimized_operations[0].__class__.__name__, \"CreateModel\")\n    self.assertEqual(optimized_operations[1].__class__.__name__, \"AddField\")\n"], "sample_320": ["def test_field_operation_reduce_references_model(self):\n    operation = FieldOperation(\n        \"MoDel\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n    )\n    other_operation = migrations.RenameModel(\"Other\", \"Whatever\")\n    reduced_operation = operation.reduce(other_operation, \"migrations\")\n    self.assertEqual(len(reduced_operation), 1)\n    self.assertEqual(\n        reduced_operation[0].fields[\"field\"].remote_field.model,\n        \"migrations.Whatever\",\n    )\n", "def test_reduce(self):\n    project_state = self.set_up_test_model(\"test_rmfl\")\n    operation = migrations.RemoveField(\"Pony\", \"pink\")\n    # Remove a field that's not used.\n    unused_operation = migrations.RemoveField(\"Pony\", \"weight\")\n    reduced_operation = operation.reduce(unused_operation, \"test_rmfl\")\n    self.assertEqual(reduced_operation, [operation])\n    # Remove a field that's used.\n    used_operation = migrations.RemoveField(\"Pony\", \"pink\")\n    reduced_operation = operation.reduce(used_operation, \"test_rmfl\")\n    self.assertEqual(reduced_operation, [])\n    # Remove an index.\n    index = models.Index(fields=[\"pink\"], name=\"test_rmfl_pony_pink_idx\")\n    add_index_operation = migrations.AddIndex(\"Pony\", index)\n    self.assertTableExists(\"test_rmfl_pony\")\n    with connection.schema_editor() as editor:\n        add_index_operation.database_forwards(\n            \"test_rmfl\", editor, project_state, project_state\n        )\n    self.assertIndexExists(\"test_rmfl_pony\", [\"pink\"])\n    remove_index_operation = migrations.RemoveIndex(\"Pony\", \"test_rmfl_pony_pink_idx\")\n    reduced_operation = operation.reduce(remove_index_operation, \"test_rmfl\")\n    self.assertEqual(reduced_operation, [operation])\n    with connection.schema_editor() as editor:\n        remove_index_operation.database_forwards(\n            \"test_rmfl\", editor, project_state, project_state\n        )\n    self.assertIndexNotExists(\"test_rmfl_pony\", [\"pink\"])\n    # Remove an index, index_together, and unique_together.\n    index_together = models.Index(fields=[\"pink\", \"weight\"], name=\"test_rmfl_pony_pink_weight_idx\")\n    add_index_together_operation = migrations.AddIndex(\"Pony\", index_together)\n    unique_together = models.Index(fields=[\"pink\", \"weight\"], name=\"test_rmfl_pony_pink_93ec1d12_uniq\")\n    add_unique_together_operation = migrations.AddIndex(\"Pony\", unique_together)\n    with connection.schema_editor() as editor:\n        add_index_together_operation.database_forwards(\n            \"test_rmfl\", editor, project_state, project_state\n        )\n        add_unique_together_operation.database_forwards(\n            \"test_rmfl\", editor, project_state, project_state\n        )\n    self.assertIndexExists(\"test_rmfl", "def test_references_field_by_mti_inheritance(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.OneToOneField(\"Other\", models.CASCADE, primary_key=True),\n    )\n    self.assertIs(operation.references_field(\"Model\", \"id\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"id\", \"migrations\"), True)\n", "def test_rename_index_state_forwards_unnamed_index_with_create_model(self):\n    app_label = \"test_rnidsfuiwm\"\n    project_state = self.set_up_test_model(app_label)\n    new_state = project_state.clone()\n\n    # CreateModel.\n    operation = migrations.CreateModel(\n        \"Pony\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"pink\", models.IntegerField()),\n            (\"weight\", models.FloatField()),\n        ],\n        options={\"index_together\": {(\"weight\", \"pink\")}},\n    )\n    operation.state_forwards(app_label, new_state)\n    # RenameIndex.\n    operation = migrations.RenameIndex(\n        \"Pony\", new_name=\"new_pony_test_idx\", old_fields=(\"weight\", \"pink\")\n    )\n    operation.state_forwards(app_label, new_state)\n    self.assertIsNot(old_state, new_state)\n    self.assertEqual(new_state.models[app_label, \"pony\"]._meta.index_together, tuple())\n    self.assertEqual(new_state.models[app_label, \"pony\"]._meta.indexes[0].name, \"new_pony_test_idx\")\n", "def test_alter_field_reloads_state_on_fk_target_type_change(self):\n    app_label = \"test_alflrsfkttc\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"code\", models.CharField(max_length=100, unique=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\n                        \"rider\",\n                        models.ForeignKey(\"%s.Rider\" % app_label, models.CASCADE),\n                    ),\n                ],\n            ),\n        ],\n    )\n    operation = migrations.AlterField(\n        \"Rider\",\n        \"code\",\n        models.IntegerField(unique=True),\n    )\n    self.apply_operations(app_label, project_state, operations=[operation])\n    Pony = project_state.apps.get_model(app_label, \"Pony\")\n    self.assertIsInstance(Pony._meta.get_field(\"rider\").target_field, models.IntegerField)\n"], "sample_387": ["    def test_get_ordering_default(self):\n        class MyModelAdmin(BaseModelAdmin):\n            pass\n\n        ma = MyModelAdmin(None, admin.site)\n        self.assertIsNone(ma.get_ordering(None))\n", "    def setUp(self):\n        super().setUp()\n        Band.objects.create(id=42, name=\"Bogey Blues\")\n        Band.objects.create(id=98, name=\"Green Potatoes\")\n", "    def testInlineAdminFormsetConstructing(self):\n        class MyInline(admin.TabularInline):\n            model = Album\n            formset = forms.BaseInlineFormSet\n\n        class MyAdmin(admin.ModelAdmin):\n            model = Band\n            inlines = [MyInline]\n\n        admin_obj = MyAdmin(Band, admin.site)\n        inline = MyInline(Band, admin.site)\n\n        # Test the inline's template is used for the formset\n        formset_obj = inline.get_formset(request=None, obj=None)\n        self.assertEqual(formset_obj.template, inline.template)\n\n        # Test that we can construct an InlineAdminFormset from a class that\n        # doesn't define an template\n        class MyInline(admin.TabularInline):\n            model = Album\n            formset = forms.BaseInlineFormSet\n            template = None\n\n        inline = MyInline(Band, admin.site)\n\n        # Test the inline's template is used for the formset\n        formset_obj = inline.get_formset(request=None, obj=None)\n        self.assertEqual(formset_obj.template, inline.template)\n\n        # Test that we can construct an InlineAdminFormset from a formset that\n        # does not define a template\n        class MyFormset(forms.models.BaseInlineFormSet):\n            pass\n\n        formset = inline.get_formset(request=None, obj=None)\n        self.assertEqual(formset.template, inline.template)\n", "    def setUp(self):\n        self.user = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=None\n        )\n", "    def test_render(self):\n        widget = widgets.ManyToManyRawIdWidget(\n            Band._meta.get_field(\"members\").remote_field, admin.site\n        )\n        band = Band.objects.create(name=\"Linkin Park\")\n        member1 = Member.objects.create(name=\"Chester\")\n        member2 = Member.objects.create(name=\"Mike\")\n        band.members.add(member1, member2)\n        rendered_widget = widget.render(\"test\", [member1.pk, member2.pk], attrs={})\n        self.assertHTMLEqual(\n            rendered_widget,\n            (\n                '<input type=\"text\" name=\"test\" value=\"%(m1pk)s,%(m2pk)s\" '\n                'class=\"vManyToManyRawIdAdminField\">'\n                '<a href=\"/admin/admin_widgets/member/\" class=\"related-lookup\" '\n                '   id=\"lookup_id_test\" title=\"Lookup\"></a>'\n            )\n            % {\"m1pk\": member1.pk, \"m2pk\": member2.pk},\n        )\n\n        self.assertHTMLEqual(\n            widget.render(\"test\", [member1.pk]),\n            (\n                '<input type=\"text\" name=\"test\" value=\"%(m1pk)s\" '\n                '   class=\"vManyToManyRawIdAdminField\">'\n                '<a href=\"/admin/admin_widgets/member/\" class=\"related-lookup\" '\n                '   id=\"lookup_id_test\" title=\"Lookup\"></a>'\n            )\n            % {\"m1pk\": member1.pk},\n        )\n"], "sample_389": ["    def test_http_request_repr(self):\n        request = HttpRequest()\n        request.method = \"GET\"\n        request.path = \"/path/\"\n        self.assertEqual(repr(request), \"<HttpRequest: GET '/path/'>\")\n", "    def test_create_querydict(self):\n        data = QueryDict(\"a=1&a=2&b=3\")\n        self.assertEqual(data.getlist(\"a\"), [\"1\", \"2\"])\n        self.assertEqual(data.get(\"b\"), \"3\")\n", "    def test_create(self):\n        QueryDict(\"a=1&a=2&b=3\")\n", "    def test_get_host(self):\n        # Test that get_host() returns the correct value with or without the port number\n        request = HttpRequest()\n        request.META = {\n            'HTTP_HOST': 'example.com:8080',\n            'SERVER_NAME': 'example.com',\n            'SERVER_PORT': '8080',\n        }\n        self.assertEqual(request.get_host(), 'example.com:8080')\n\n        request = HttpRequest()\n        request.META = {\n            'HTTP_HOST': 'example.com',\n            'SERVER_NAME': 'example.com',\n            'SERVER_PORT': '80',\n        }\n        self.assertEqual(request.get_host(), 'example.com')\n", "    def test_request_build_absolute_uri_with_path(self):\n        request = HttpRequest()\n        request.path = \"/path/to/resource\"\n        request.path_info = \"/path/to/resource\"\n        request.META[\"SERVER_NAME\"] = \"example.com\"\n        request.META[\"SERVER_PORT\"] = 80\n        self.assertEqual(\n            request.build_absolute_uri(), \"http://example.com/path/to/resource\"\n        )\n"], "sample_396": ["    def test_ticket_12345(self):\n        \"\"\"\n        Test that the 'as' keyword works correctly in `extra()`.\n        \"\"\"\n        # Create a queryset with an extra 'select' parameter\n        qs = Note.objects.extra(select={\"foo\": \"note\"})\n\n        # Check that the 'as' keyword is correctly used\n        with CaptureQueriesContext(connection) as captured_queries:\n            self.assertEqual(qs[0].foo, \"n1\")\n\n        # Check the query\n        self.assertIn(' \"foo\"' in captured_queries[0][\"sql\"], True)\n", "    def test_ticket_25426(self):\n        \"\"\"\n        The subquery table name should be quoted in the returned SQL.\n        \"\"\"\n        with CaptureQueriesContext(connection) as captured_queries:\n            Note.objects.filter(pk__in=Note.objects.values_list('pk', flat=True))\n        sql = captured_queries[0]['sql']\n        self.assertIn(connection.ops.quote_name('subquery'), sql)\n", "    def test_exclude_unique_with_subquery(self):\n        Program.objects.create(identifier_id=1)\n        Program.objects.create(identifier_id=2)\n        Program.objects.create(identifier_id=3)\n        Identifier.objects.create(id=1)\n        Identifier.objects.create(id=2)\n        Identifier.objects.create(id=3)\n        Channel.objects.create(identifier_id=1)\n        Channel.objects.create(identifier_id=2)\n        qs = Identifier.objects.exclude(program__identifier_id__in=[1, 2])\n        self.assertEqual(len(qs), 1)\n        self.assertEqual(qs[0].id, 3)\n        qs = Identifier.objects.exclude(program__identifier_id__in=[1, 2]).distinct()\n        self.assertEqual(len(qs), 1)\n        self.assertEqual(qs[0].id, 3)\n        qs = Identifier.objects.exclude(channel__identifier_id__in=[1, 2]).distinct()\n        self.assertEqual(len(qs), 1)\n        self.assertEqual(qs[0].id, 3)\n", "    def test_ticket_24647(self):\n        \"\"\"\n        QuerySet.values_list() with flat=True should work as expected on an\n        unsliced QuerySet with an annotated values clause.\n        \"\"\"\n        note = Note.objects.create(note=\"n\", misc=\"m\")\n        annotation = Annotation.objects.create(note=note)\n        qs = Annotation.objects.annotate(num_notes=Count(\"notes\")).order_by(\n            \"-num_notes\"\n        )\n        self.assertEqual(\n            list(qs.values_list(\"num_notes\", flat=True)), [1]\n        )\n", "    def test_ticket_24863(self):\n        e1 = ExtraInfo.objects.create(info=\"e1\", note=None, value=None)\n        e2 = ExtraInfo.objects.create(info=\"e2\", note_id=None, value=41)\n        a1 = Author.objects.create(name=\"a1\", num=1001, extra=e1)\n        a2 = Author.objects.create(name=\"a2\", num=2002, extra=e1)\n        a3 = Author.objects.create(name=\"a3\", num=3003, extra=e2)\n        a4 = Author.objects.create(name=\"a4\", num=4004, extra=e2)\n\n        self.assertSequenceEqual(\n            Author.objects.filter(extra__isnull=True),\n            [],\n        )\n        self.assertSequenceEqual(\n            Author.objects.filter(extra__note__isnull=True),\n            [a1, a2, a3, a4],\n        )\n        self.assertSequenceEqual(\n            Author.objects.filter(extra__note_id__isnull=True),\n            [a1, a2, a4],\n        )\n        self.assertSequenceEqual(\n            Author.objects.filter(extra__value__isnull=True),\n            [a1, a2],\n        )\n"], "sample_394": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.b1 = Book.objects.create(name=\"L\u00e6rdommer\")\n        cls.b2 = Book.objects.create(name=\"The Great Gatsby\")\n        cls.p1 = Promo.objects.create(\n            name=\"<Promo for L\u00e6rdommer>\", book=cls.b1\n        )\n        cls.p2 = Promo.objects.create(name=\"Get Gatsby\", book=cls.b2)\n", "    def test_unknown_url_redirects_login_if_not_authenticated(self):\n        unknown_url = \"/test_admin/admin11/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertRedirects(\n            response, \"%s?next=%s\" % (reverse(\"admin11:login\"), unknown_url)\n        )\n", "    def setUpTestData(cls):\n        cls.staff_user = User.objects.create_user(\n            username=\"staff\", password=\"secret\", email=\"staff@example.com\", is_staff=True\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.user1 = User.objects.create_user(\n            username=\"testuser\", password=\"secret\", email=\"testuser@example.com\"\n        )\n        cls.user2 = User.objects.create_user(\n            username=\"testuser2\", password=\"secret\", email=\"testuser2@example.com\"\n        )\n        cls.user3 = User.objects.create_user(\n            username=\"testuser3\", password=\"secret\", email=\"testuser3@example.com\"\n        )\n"], "sample_400": ["def test_rename_field_with_f_expression(self):\n    before = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"bar\", models.IntegerField()),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"baz\",\n                    models.IntegerField(default=models.F(\"bar\")),\n                ),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({\"ask_rename\": True}))\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RenameField\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, old_name=\"bar\", new_name=\"baz\")\n", "def test_alter_field_with_choices(self):\n    before = [\n        ModelState(\n            \"app\",\n            \"Model\",\n            [\n                (\n                    \"field\",\n                    models.CharField(max_length=1, choices=[(\"A\", \"A\"), (\"B\", \"B\")]),\n                ),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Model\",\n            [\n                (\n                    \"field\",\n                    models.CharField(max_length=1, choices=[(\"A\", \"A\"), (\"C\", \"C\")]),\n                ),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(\n        changes,\n        \"app\",\n        0,\n        0,\n        choices=[(\"A\", \"A\"), (\"C\", \"C\")],\n    )\n", "    def test_no_migrations(self):\n        \"\"\"\n        If there are no existing migrations for an app, the autodetector should\n        create a single migration with all operations.\n        \"\"\"\n        changes = MigrationAutodetector(\n            from_state=ProjectState(),\n            to_state=self.make_project_state([self.author_name]),\n        )._detect_changes()\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\")\n", "def test_remove_model_with_field_removed_from_base_model(self):\n    \"\"\"\n    Removing a base field takes place before removing an inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            bases=(\"app.readable\",),\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RemoveField\", \"DeleteModel\"])\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, name=\"title\", model_name=\"readable\"\n    )\n    self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"book\")\n", "def test_mti_inheritance_model_removal_with_intermediate_model(self):\n    Animal = ModelState(\n        \"app\",\n        \"Animal\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    )\n    Mammal = ModelState(\n        \"app\",\n        \"Mammal\",\n        [],\n        bases=(\"app.Animal\",),\n    )\n    Dog = ModelState(\"app\", \"Dog\", [], bases=(\"app.Mammal\",))\n    changes = self.get_changes([Animal, Mammal, Dog], [Animal, Mammal])\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"DeleteModel\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"Dog\")\n"], "sample_405": ["    def test_reduce(self):\n        \"\"\"FieldOperation.reduce removes itself when it's a no-op.\"\"\"\n        operation = FieldOperation(\"Pony\", \"pink\", models.IntegerField(default=1))\n        new_operation = FieldOperation(\"Pony\", \"pink\", models.IntegerField(default=1))\n        self.assertEqual(operation.reduce(new_operation, app_label=None), [])\n        new_operation = FieldOperation(\"Pony\", \"pink\", models.IntegerField(default=2))\n        self.assertEqual(operation.reduce(new_operation, app_label=None), [operation, new_operation])\n", "def test_add_index_state_forwards_with_conditions(self):\n    project_state = self.set_up_test_model(\"test_adinsfwc\")\n    index = models.Index(\n        models.Q(pink__gt=2) | models.Q(weight__lt=3), name=\"test_adinsfwc_pony_pink_idx\"\n    )\n    old_model = project_state.apps.get_model(\"test_adinsfwc\", \"Pony\")\n    new_state = project_state.clone()\n\n    operation = migrations.AddIndex(\"Pony\", index)\n    operation.state_forwards(\"test_adinsfwc\", new_state)\n    new_model = new_state.apps.get_model(\"test_adinsfwc\", \"Pony\")\n    self.assertIsNot(old_model, new_model)\n    self.assertEqual(len(new_state.models[\"test_adinsfwc\", \"pony\"].options[\"indexes\"]), 1)\n", "def test_rename_model_with_self_referential_m2m_through(self):\n    app_label = \"test_rename_model_with_self_referential_m2m_through\"\n\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"ReflexivePony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"ReflexivePonyThrough\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"from_pony\",\n                        models.ForeignKey(\n                            \"test_rename_model_with_self_referential_m2m_through.ReflexivePony\",\n                            models.CASCADE,\n                            related_name=\"from_ponies\",\n                        ),\n                    ),\n                    (\n                        \"to_pony\",\n                        models.ForeignKey(\n                            \"test_rename_model_with_self_referential_m2m_through.ReflexivePony\",\n                            models.CASCADE,\n                            related_name=\"to_ponies\",\n                        ),\n                    ),\n                ],\n            ),\n            migrations.AddField(\n                \"ReflexivePony\",\n                \"related_ponies\",\n                models.ManyToManyField(\n                    \"self\", through=\"test_rename_model_with_self_referential_m2m_through.ReflexivePonyThrough\"\n                ),\n            ),\n        ],\n    )\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameModel(\"ReflexivePony\", \"ReflexivePony2\"),\n            migrations.RenameModel(\"ReflexivePonyThrough\", \"ReflexivePonyThrough2\"),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"ReflexivePony2\")\n    pony = Pony.objects.create()\n    pony.related_ponies.add(pony)\n\n    self.assertEqual(Pony.objects.count(), 1)\n    self.assertEqual(pony.related_ponies.count(), 1)\n    self.assertEqual(\n        Pony._meta.get_field(\"related_ponies\").remote_field.through.objects.count(), 1\n    )\n", "def test_rename_field_m2m(self):\n    project_state = self.set_up_test_model(\n        \"test_rename_field_m2m\", second_model=True, third_model=True\n    )\n\n    project_state = self.apply_operations(\n        \"test_rename_field_m2m\",\n        project_state,\n        operations=[\n            migrations.AddField(\n                \"Pony\", \"places\", models.ManyToManyField(\"Stable\", related_name=\"ponies\")\n            )\n        ],\n    )\n    Pony = project_state.apps.get_model(\"test_rename_field_m2m\", \"Pony\")\n    p = Pony.objects.create(pink=False, weight=4.55)\n    p.places.create()\n\n    project_state = self.apply_operations(\n        \"test_rename_field_m2m\",\n        project_state,\n        operations=[\n            migrations.AlterField(\n                \"Pony\",\n                \"places\",\n                models.ManyToManyField(\n                    to=\"Van\", related_name=\"ponies_renamed_places\"\n                ),\n            )\n        ],\n    )\n    Pony = project_state.apps.get_model(\"test_rename_field_m2m\", \"Pony\")\n    p = Pony.objects.first()\n    p.places.create()\n    self.assertEqual(Pony.objects.count(), 1)\n    self.assertEqual(Pony._meta.get_field(\"places\").remote_field.through.objects.count(), 2)\n", "def test_references_field_by_order_with_respect_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"_order\",\n        models.IntegerField(),\n        model_options={\"order_with_respect_to\": \"other\"},\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"other\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"whatever\", \"migrations\"), False\n    )\n"], "sample_408": ["def test_add_constraints_and_indexes(self):\n    \"\"\"Tests that when a model is created with constraints and indexes, they are added in the right order.\"\"\"\n    author = ModelState(\n        \"otherapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"constraints\": [\n                models.CheckConstraint(\n                    check=models.Q(name__contains=\"Bob\"), name=\"name_contains_bob\"\n                )\n            ],\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n            ],\n        },\n    )\n    changes = self.get_changes([], [author])\n    # Right number of migrations?\n    self.assertEqual(len(changes[\"otherapp\"]), 1)\n    # Right number of operations?\n    self.assertEqual(len(changes[\"otherapp\"][0].operations), 3)\n    # Right order of operations?\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"CreateModel\", \"AddConstraint\", \"AddIndex\"]\n    )\n", "def test_alter_model_table_with_indexes(self):\n    book = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"author\", \"title\"], name=\"book_title_author_idx\")\n            ],\n        },\n    )\n    changes = self.get_changes([book], [book.clone(db_table=\"book_table\")])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\", table=\"book_table\")\n", "def test_add_model_with_field_removed_from_grandparent_model(self):\n    \"\"\"\n    Removing a field from a grandparent model takes place before adding a new \n    inherited model that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [],\n            bases=(\"app.readable\",),\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"readable\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"book\",\n            [],\n            bases=(\"app.readable\",),\n        ),\n        ModelState(\n            \"app\",\n            \"novel\",\n            [\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            bases=(\"app.book\",),\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"RemoveField\", \"CreateModel\"])\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 0, name=\"title\", model_name=\"readable\"\n    )\n    self.assertOperationAttributes(changes, \"app\", 0, 1, name=\"novel\")\n", "def test_add_model_with_suggested_name(self):\n    \"\"\"\n    #24537 - The suggest_name() method should account for the model's\n    db_table and suggest a name without a suffix for the initial\n    migration of an app.\n    \"\"\"\n    class Migration(migrations.Migration):\n        initial = True\n        operations = [\n            migrations.CreateModel(\n                \"Person\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                options={\"db_table\": \"myapp_mymodel\"},\n            )\n        ]\n\n    migration = Migration(\"some_migration\", \"myapp\")\n    self.assertEqual(migration.suggest_name(), \"mymodel\")\n", "def test_generate_altered_db_table_with_non_ascii(self):\n    model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n        {\"db_table\": \"\u00e0uth\u00f8r\"},\n    )\n    changes = self.get_changes(\n        [self.author_empty], [model_state]\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table=\"\u00e0uth\u00f8r\"\n    )\n"], "sample_423": ["def test_auto_now_add_fields_with_default(self):\n    \"\"\"\n    #30162 - Test that adding a model with auto_now_add fields that also have a default value is detected correctly.\n    \"\"\"\n    changes = self.get_changes(\n        [],\n        [\n            ModelState(\n                \"testapp\",\n                \"Model\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"field\", models.DateTimeField(auto_now_add=True, default=timezone.now)),\n                ],\n            )\n        ],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"Model\"\n    )\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, fields=[(\"id\", models.AutoField(primary_key=True)), (\"field\", models.DateTimeField(auto_now_add=True, default=timezone.now))]\n    )\n", "def test_alter_unique_together_with_index(self):\n    author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"name\",)},\n            \"indexes\": [\n                models.Index(fields=[\"age\"]),\n            ],\n        },\n    )\n    author_new_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"age\",)},\n            \"indexes\": [\n                models.Index(fields=[\"name\"]),\n            ],\n        },\n    )\n    changes = self.get_changes([author], [author_new_constraints])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\n            \"AlterUniqueTogether\",\n            \"RemoveIndex\",\n            \"AddIndex\",\n            \"AlterUniqueTogether\",\n        ],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        unique_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        model_name=\"author\",\n        name=\"author_age_idx\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        2,\n        model_name=\"author\",\n        index=models.Index(fields=[\"name\"]),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        3,\n        name=\"author\",\n        unique_together={(\"age\",)},\n    )\n", "def test_create_model_with_unique_constraint(self):\n    \"\"\"Test creation of new model with constraints already defined.\"\"\"\n    author = ModelState(\n        \"otherapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"constraints\": [\n                models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n            ],\n        },\n    )\n    changes = self.get_changes([], [author])\n    added_constraint = models.UniqueConstraint(fields=[\"name\"], name=\"unique_name\")\n    # Right number of migrations?\n    self.assertEqual(len(changes[\"otherapp\"]), 1)\n    # Right number of actions?\n    migration = changes[\"otherapp\"][0]\n    self.assertEqual(len(migration.operations), 2)\n    # Right actions order?\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\", \"AddConstraint\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"Author\")\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, model_name=\"author\", constraint=added_constraint\n    )\n", "def test_alter_unique_together_with_multiple_renames(self):\n    model_state_1 = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field1\", models.IntegerField()),\n            (\"field2\", models.IntegerField()),\n            (\"field3\", models.IntegerField()),\n            (\"field4\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"field1\", \"field2\"), (\"field3\", \"field4\")},\n        },\n    )\n    model_state_2 = ModelState(\n        \"testapp\",\n        \"Model\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field1_renamed\", models.IntegerField()),\n            (\"field2_renamed\", models.IntegerField()),\n            (\"field3_renamed\", models.IntegerField()),\n            (\"field4_renamed\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"field1_renamed\", \"field2_renamed\"), (\"field3_renamed\", \"field4_renamed\")},\n        },\n    )\n\n    changes = self.get_changes([model_state_1], [model_state_2], MigrationQuestioner({\"ask_rename\": True}))\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\", \"RenameField\", \"RenameField\", \"RenameField\", \"AlterUniqueTogether\"])\n", "def test_autodetector_alter_model_table_with_through_models(self):\n    \"\"\"Tests autodetector when altering model table with through models.\"\"\"\n    # Create a model state with a ManyToManyField and its through model\n    model_state = ModelState(\n        \"testapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"authors\",\n                models.ManyToManyField(\n                    \"testapp.Author\", through=\"testapp.BookAuthor\"\n                ),\n            ),\n        ],\n    )\n    through_state = ModelState(\n        \"testapp\",\n        \"BookAuthor\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"book\",\n                models.ForeignKey(\"testapp.Book\", models.CASCADE, related_name=\"+\"),\n            ),\n            (\n                \"author\",\n                models.ForeignKey(\"testapp.Author\", models.CASCADE, related_name=\"+\"),\n            ),\n        ],\n    )\n    author_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n        ],\n    )\n    # Create an altered model state with a different table name\n    new_model_state = ModelState(\n        \"testapp\",\n        \"NewBook\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"authors\",\n                models.ManyToManyField(\n                    \"testapp.Author\", through=\"testapp.NewBookAuthor\"\n                ),\n            ),\n        ],\n        {\"db_table\": \"new_book\"},\n    )\n    new_through_state = ModelState(\n        \"testapp\",\n        \"NewBookAuthor\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"book\",\n                models.ForeignKey(\n                    \"testapp.NewBook\", models.CASCADE, related_name=\"+\"\n                ),\n            ),\n            (\n                \"author\",\n                models.ForeignKey(\"testapp.Author\", models.CASCADE, related_name=\"+\"),\n            ),\n        ],\n    )\n    # Run the autodetector\n    changes = self.get_changes(\n        [model_state, through_state, author_state],\n        [new_model_state, new_through_state, author_state],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number and type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"RenameModel\", \"RenameModel\", \"AlterModelTable\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old"], "sample_424": ["def test_references_field_by_through_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\"Other\", through=\"Through\", through_fields=(\"source\", \"target\")),\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"source\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"target\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_mti_model(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"Other\", models.CASCADE),\n    )\n    # MTI model inherits from models.Model.\n    class MTIModel(models.Model):\n        pass\n\n    self.assertIs(\n        operation.references_field(\"MTIModel\", \"whatever\", \"migrations\"), False\n    )\n    # MTI model inherits from Other.\n    class MTIModel(models.Model, Other):\n        pass\n\n    self.assertIs(operation.references_field(\"MTIModel\", \"whatever\", \"migrations\"), True)\n", "def test_references_field_by_through_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\"Other\", through=\"Through\", through_fields=(\"from\", \"to\")),\n    )\n    self.assertIs(operation.references_field(\"Through\", \"from\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Through\", \"to\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Through\", \"whatever\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Model\", \"from\", \"migrations\"), False)\n    self.assertIs(operation.references_field(\"Other\", \"to\", \"migrations\"), False)\n", "def test_rename_index_with_multiple(self):\n    app_label = \"test_rename_index_with_multiple\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=20)),\n                    (\"age\", models.IntegerField()),\n                ],\n            ),\n            migrations.AddIndex(\n                \"Pony\",\n                models.Index(fields=[\"name\", \"age\"], name=\"pony_name_age_idx\"),\n            ),\n        ],\n    )\n    new_state = project_state.clone()\n    operation = migrations.RenameIndex(\n        \"Pony\", old_name=\"pony_name_age_idx\", new_name=\"pony_name_age_new_idx\"\n    )\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    self.assertIndexNameNotExists(\"test_rename_index_with_multiple_pony\", \"pony_name_age_idx\")\n    self.assertIndexNameExists(\"test_rename_index_with_multiple_pony\", \"pony_name_age_new_idx\")\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    self.assertIndexNameExists(\"test_rename_index_with_multiple_pony\", \"pony_name_age_idx\")\n    self.assertIndexNameNotExists(\"test_rename_index_with_multiple_pony\", \"pony_name_age_new_idx\")\n", "def test_rename_field_with_self_referential_m2m_through(self):\n    app_label = \"test_rename_field_with_self_referential_m2m_through\"\n\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"ReflexivePony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            migrations.CreateModel(\n                \"ReflexivePonyThrough\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"from_pony\",\n                        models.ForeignKey(\n                            \"ReflexivePony\", models.CASCADE, related_name=\"+\"\n                        ),\n                    ),\n                    (\n                        \"to_pony\",\n                        models.ForeignKey(\n                            \"ReflexivePony\", models.CASCADE, related_name=\"+\"\n                        ),\n                    ),\n                ],\n            ),\n            migrations.AddField(\n                \"ReflexivePony\",\n                \"ponies\",\n                models.ManyToManyField(\n                    \"self\",\n                    through=\"test_rename_field_with_self_referential_m2m_through.ReflexivePonyThrough\",\n                ),\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"ReflexivePony\")\n    pony = Pony.objects.create()\n    pony.ponies.add(pony)\n\n    project_state = self.apply_operations(\n        app_label,\n        project_state,\n        operations=[\n            migrations.RenameField(\n                model_name=\"ReflexivePonyThrough\",\n                old_name=\"from_pony\",\n                new_name=\"from_pony_new\",\n            ),\n        ],\n    )\n    Pony = project_state.apps.get_model(app_label, \"ReflexivePony\")\n    pony = Pony.objects.first()\n    pony.ponies.add(pony)\n"], "sample_430": ["def test_alter_unique_together_with_renamed_field(self):\n    author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"birth_date\", models.DateField()),\n        ],\n        {\n            \"unique_together\": {(\"name\", \"birth_date\")},\n        },\n    )\n    author_renamed = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"full_name\", models.CharField(max_length=200)),\n            (\"birth_date\", models.DateField()),\n        ],\n        {\n            \"unique_together\": {(\"full_name\", \"birth_date\")},\n        },\n    )\n    changes = self.get_changes(\n        [author],\n        [author_renamed],\n        MigrationQuestioner({\"ask_rename\": True}),\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"RenameField\", \"AlterUniqueTogether\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        name=\"author\",\n        unique_together={(\"full_name\", \"birth_date\")},\n    )\n", "def test_deep_deconstruct_with(type_):\n    \"\"\"Test deep_deconstruct() with a given type.\"\"\"\n    autodetector = MigrationAutodetector(ProjectState(), ProjectState())\n    value = type_()\n    deconstructed = autodetector.deep_deconstruct(value)\n    reconstructed = autodetector.deep_reconstruct(deconstructed)\n    if isinstance(value, type):\n        # If the original value is a type, compare names as it may have been\n        # reconstructed as a different object.\n        self.assertEqual(reconstructed.__name__, value.__name__)\n    else:\n        self.assertEqual(reconstructed, value)\n", "def test_alter_unique_together_with_index(self):\n    # Create initial model state\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"name\",)},\n            \"indexes\": [\n                models.Index(fields=[\"name\", \"age\"], name=\"author_name_age_idx\")\n            ],\n        },\n    )\n\n    # Alter unique_together and remove existing index\n    author_reversed_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"age\",)},\n            \"indexes\": [\n                models.Index(fields=[\"age\", \"name\"], name=\"author_age_name_idx\")\n            ],\n        },\n    )\n\n    changes = self.get_changes([initial_author], [author_reversed_constraints])\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\n            \"AlterUniqueTogether\",\n            \"RemoveIndex\",\n            \"AddIndex\",\n            \"AlterUniqueTogether\",\n        ],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        unique_together=set(),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        model_name=\"author\",\n        name=\"author_name_age_idx\",\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        2,\n        model_name=\"author\",\n        index=models.Index(fields=[\"age\", \"name\"], name=\"author_age_name_idx\"),\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        3,\n        name=\"author\",\n        unique_together={(\"age\",)},\n    )\n", "def test_alter_unique_together_with_model_change(self):\n    \"\"\"\n    Tests when model and unique_together changes, autodetector must create two\n    operations.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_unique_together],\n        [self.author_renamed_with_unique_together],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"RenameModel\", \"AlterUniqueTogether\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, name=\"newauthor\", unique_together={(\"author\", \"title\")}\n    )\n", "def test_alter_field_with_choices(self):\n    changes = self.get_changes(\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"status\",\n                        models.CharField(max_length=10, choices=[(\"a\", \"A\"), (\"b\", \"B\")]),\n                    ),\n                ],\n            )\n        ],\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\n                        \"status\",\n                        models.CharField(max_length=10, choices=[(\"a\", \"A\"), (\"c\", \"C\")]),\n                    ),\n                ],\n            )\n        ],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"status\")\n"], "sample_439": ["    def test_custom_renderer(self):\n        class CustomForm(Form):\n                super().__init__(*args, **kwargs)\n                self.renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertEqual(form.template_name, CustomRenderer.form_template_name)\n", "def test_form_media_with_none_widget(self):\n    class TestForm(Form):\n        f1 = CharField()\n        f2 = CharField(widget=None)\n\n    form = TestForm()\n    self.assertEqual(form.media, Media())\n", "def test_hidden_initial_gets_id_with_use_required_attribute(self):\n    class MyForm(Form):\n        use_required_attribute = True\n        field1 = CharField(max_length=50, show_hidden_initial=True)\n\n    self.assertHTMLEqual(\n        MyForm().as_table(),\n        '<tr><th><label for=\"id_field1\">Field1:</label></th><td>'\n        '<input id=\"id_field1\" type=\"text\" name=\"field1\" maxlength=\"50\" required>'\n        '<input type=\"hidden\" name=\"initial-field1\" id=\"initial-id_field1\">'\n        \"</td></tr>\",\n    )\n", "def test_media_on_empty_form(self):\n    class TestForm(forms.Form):\n        pass\n\n    form = TestForm()\n    self.assertEqual(form.media, forms.Media())\n", "def test_template_name_div(self):\n    class DivForm(Form):\n        field = CharField()\n\n    self.assertEqual(DivForm().template_name, \"django/forms/div.html\")\n    # Make sure the template_name_div attribute is the property _template_name_div\n    self.assertEqual(DivForm().template_name_div, \"django/forms/div.html\")\n"], "sample_452": ["def test_reduce(self):\n    operation = migrations.RenameModel(\"Model\", \"NewModel\")\n    self.assertEqual(\n        operation.reduce(operation, \"migrations\"),\n        [migrations.DeleteModel(\"Model\"), migrations.CreateModel(\"NewModel\", [])],\n    )\n", "def test_create_model_with_explicitly_named_foreign_key(self):\n    \"\"\"\n    A model with an explicitly named foreign key can be created.\n    \"\"\"\n    project_state = ProjectState()\n    new_state = project_state.clone()\n    operation = migrations.CreateModel(\n        \"Pony\",\n        fields=[\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"friend\",\n                models.ForeignKey(\"OtherPony\", models.CASCADE, db_column=\"other_pony_id\"),\n            ),\n        ],\n    )\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_cmfwenf\", editor, project_state, new_state)\n    self.assertTableExists(\"test_cmfwenf_pony\")\n    self.assertColumnExists(\"test_cmfwenf_pony\", \"other_pony_id\")\n", "def test_rename_model_with_database_table(self):\n    \"\"\"\n    Tests the RenameModel operation on model with db_table.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_rmwdt\", db_table=\"my_custom_table\")\n    # Test the state alteration\n    operation = migrations.RenameModel(\"Pony\", \"Horse\")\n    self.assertEqual(operation.describe(), \"Rename model Pony to Horse\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_rmwdt\", new_state)\n    self.assertNotIn((\"test_rmwdt\", \"pony\"), new_state.models)\n    self.assertIn((\"test_rmwdt\", \"horse\"), new_state.models)\n    # Remember, RenameModel also repoints all incoming FKs and M2Ms\n    self.assertEqual(\n        new_state.models[\"test_rmwdt\", \"rider\"].fields[\"pony\"].remote_field.model,\n        \"test_rmwdt.Horse\",\n    )\n    # Test the database alteration\n    self.assertTableExists(\"my_custom_table\")\n    self.assertTableNotExists(\"test_rmwdt_horse\")\n    if connection.features.supports_foreign_keys:\n        self.assertFKExists(\n            \"test_rmwdt_rider\", [\"pony_id\"], (\"my_custom_table\", \"id\")\n        )\n        self.assertFKNotExists(\n            \"test_rmwdt_rider\", [\"pony_id\"], (\"test_rmwdt_horse\", \"id\")\n        )\n    with connection.schema_editor(\n        atomic=connection.features.supports_atomic_references_rename\n    ) as editor:\n        operation.database_forwards(\"test_rmwdt\", editor, project_state, new_state)\n    self.assertTableNotExists(\"my_custom_table\")\n    self.assertTableExists(\"test_rmwdt_horse\")\n    if connection.features.supports_foreign_keys:\n        self.assertFKNotExists(\n            \"test_rmwdt_rider\", [\"pony_id\"], (\"my_custom_table\", \"id\")\n        )\n        self.assertFKExists(\n            \"test_rmwdt_rider\", [\"pony_id\"], (\"test_rmwdt_horse\", \"id\")\n        )\n", "def test_reduce_remove_add_index(self):\n    operation = migrations.RemoveIndex(\"model\", \"name\")\n    add_operation = migrations.AddIndex(\"model\", models.Index(fields=[\"field\"], name=\"name\"))\n    result = operation.reduce(add_operation, None)\n    self.assertEqual(result, [])\n", "def test_order_with_respect_to_undeclared_references_field(self):\n    state = ProjectState()\n    state.add_model(\n        ModelState(\"migrations\", \"Model\", [(self.gf(\"django.db.models.fields.AutoField\")(primary_key=True), \"id\")])\n    )\n    state.add_model(\n        ModelState(\n            \"migrations\",\n            \"OtherModel\",\n            [\n                (self.gf(\"django.db.models.fields.AutoField\")(primary_key=True), \"id\"),\n                (\n                    self.gf(\"django.db.models.fields.related.ForeignKey\")(\n                        \"migrations.Model\", models.CASCADE\n                    ),\n                    \"model\",\n                ),\n            ],\n            options={\"order_with_respect_to\": \"model\"},\n        )\n    )\n    operation = migrations.AlterOrderWithRespectTo(\"OtherModel\", None)\n    new_state = state.clone()\n    operation.state_forwards(\"migrations\", new_state)\n    self.assertIsNone(\n        new_state.models[\"migrations\", \"othermodel\"].options[\"order_with_respect_to\"]\n    )\n    # And deconstruction\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"AlterOrderWithRespectTo\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(\n        definition[2],\n        {\"name\": \"OtherModel\", \"order_with_respect_to\": None},\n    )\n    # And references model\n    self.assertIs(\n        operation.references_field(\"Model\", \"id\", \"migrations\"), True\n    )\n"], "sample_454": ["    def setUpTestData(cls):\n        cls.p1 = Product.objects.create(price=10, discounted_price=5)\n", "    def setUpTestData(cls):\n        cls.exclusion_constraint_model = models.Model(\n            name=models.CharField(max_length=255),\n            using_tsrange=models.DateTimeField(),\n            range=models.RangeField(models.DateTimeField()),\n            date_range=models.RangeField(models.DateField()),\n        )\n\n        class ExclusionConstraintModel(cls.exclusion_constraint_model):\n            class Meta:\n                constraints = [\n                    models.ExclusionConstraint(\n                        name=\"overlap exclusion\",\n                        expressions=[\n                            (\"using_tsrange\", \"&&\",),\n                            (\"range\", \"&&\",),\n                            (\"date_range\", \"&&\",),\n                        ],\n                    )\n                ]\n\n        cls.exclusion_constraint_model = ExclusionConstraintModel\n        cls.exclusion_constraint_model._meta.apps = models.apps\n        cls.exclusion_constraint_model._meta.app_label = \"model\"\n", "    def test_eq(self):\n        exclusion1 = ExclusionConstraint(\n            name=\"test\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"<>\")]\n        )\n        exclusion2 = ExclusionConstraint(\n            name=\"test\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"<>\")]\n        )\n        self.assertEqual(exclusion1, exclusion2)\n        self.assertEqual(exclusion1, mock.ANY)\n        self.assertNotEqual(exclusion1, UniqueConstraint(fields=[\"field\"], name=\"name\"))\n        self.assertNotEqual(exclusion1, 1)\n", "    def test_eq(self):\n        constraint1 = ExclusionConstraint(\n            name=\"price\",\n            expressions=[(\"price\", \"=\"), (\"discounted_price\", \"<=\")],\n        )\n        constraint2 = ExclusionConstraint(\n            name=\"price\",\n            expressions=[(\"price\", \"=\"), (\"discounted_price\", \"<=\")],\n        )\n        self.assertEqual(constraint1, constraint2)\n        self.assertEqual(constraint1, mock.ANY)\n        constraint3 = ExclusionConstraint(\n            name=\"price\",\n            expressions=[(\"price\", \"=\"), (\"discounted_price\", \">\")],\n        )\n        self.assertNotEqual(constraint1, constraint3)\n        self.assertNotEqual(constraint1, 1)\n", "    def setUpTestData(cls):\n        cls.p1 = Product.objects.create(price=10, discounted_price=5)\n"], "sample_460": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n\n        cls.s1 = Section.objects.create(name=\"Test section\")\n        cls.a1 = Article.objects.create(\n            content=\"<p>Middle content</p>\",\n            date=datetime.datetime(2008, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a2 = Article.objects.create(\n            content=\"<p>Oldest content</p>\",\n            date=datetime.datetime(2000, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n        cls.a3 = Article.objects.create(\n            content=\"<p>Newest content</p>\",\n            date=datetime.datetime(2009, 3, 18, 11, 54, 58),\n            section=cls.s1,\n        )\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n"], "sample_463": ["def test_alter_index_together_non_unique(self):\n    # index_together indexes are non-unique.\n    book_index_together = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"author\", \"title\")},\n        },\n    )\n    book_unique_index_together = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ],\n        {\n            \"unique_together\": {(\"author\", \"title\")},\n        },\n    )\n    changes = self.get_changes(\n        [AutodetectorTests.author_empty, book_index_together],\n        [AutodetectorTests.author_empty, book_unique_index_together],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"AlterIndexTogether\", \"AlterUniqueTogether\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\", index_together=set()\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, name=\"book\", unique_together={(\"author\", \"title\")}\n    )\n", "    def test_alter_index_together_changed_with_unique_together(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\",)},\n                \"unique_together\": {(\"name\",)},\n            },\n        )\n        author_reversed_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"age\",)},\n                \"unique_together\": {(\"age\",)},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_reversed_constraints])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\n                \"AlterIndexTogether\",\n                \"AlterUniqueTogether\",\n            ],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"author\",\n            index_together={(\"age\",)},\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            unique_together={(\"age\",)},\n        )\n", "def test_alter_index_together_with_multiple_indexes(self):\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n            (\"city\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"name\",), (\"age\", \"city\")},\n        },\n    )\n    author_new_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n            (\"city\", models.CharField(max_length=200)),\n        ],\n        {\n            \"index_together\": {(\"name\",), (\"age\",), (\"city\",)},\n        },\n    )\n    changes = self.get_changes([initial_author], [author_new_constraints])\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterIndexTogether\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        index_together={(\"name\",), (\"age\",), (\"city\",)},\n    )\n", "def test_alter_field_with_changed_choices(self):\n    before = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"choices_field\", models.CharField(max_length=1, choices=[(\"a\", \"A\")])),\n            ],\n        )\n    ]\n    after = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"choices_field\",\n                    models.CharField(max_length=1, choices=[(\"a\", \"A\"), (\"b\", \"B\")]),\n                ),\n            ],\n        )\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        choices=[(\"a\", \"A\"), (\"b\", \"B\")],\n    )\n", "def test_alter_custom_primary_key(self):\n    before = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\n                    \"id\",\n                    models.AutoField(\n                        primary_key=True,\n                    ),\n                )\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\n                    \"id\",\n                    models.UUIDField(\n                        primary_key=True,\n                    ),\n                )\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, model_name=\"foo\", name=\"id\")\n    self.assertOperationFieldAttributes(changes, \"app\", 0, 0, primary_key=True)\n"], "sample_491": ["def test_boundfield_as_widget(self):\n    class CustomForm(Form):\n        field1 = CharField()\n        field2 = CharField(disabled=True)\n\n    form = CustomForm(data={\"field1\": \"hello\", \"field2\": \"world\"})\n\n    # Test rendering with the default widget type.\n    widget = form[\"field1\"].as_widget()\n    self.assertIsInstance(widget, Text)\n    self.assertEqual(widget[\"value\"], \"hello\")\n\n    # Test rendering with a custom widget type.\n    widget = form[\"field1\"].as_widget(Textarea)\n    self.assertIsInstance(widget, Textarea)\n    self.assertEqual(widget[\"value\"], \"hello\")\n\n    # Test rendering with a custom widget instance.\n    widget = form[\"field1\"].as_widget(Textarea(attrs={\"class\": \"custom\"}))\n    self.assertIsInstance(widget, Textarea)\n    self.assertEqual(widget[\"value\"], \"hello\")\n    self.assertEqual(widget[\"attrs\"][\"class\"], \"custom\")\n\n    # Test rendering of a disabled field.\n    widget = form[\"field2\"].as_widget()\n    self.assertIsInstance(widget, Text)\n    self.assertEqual(widget[\"value\"], \"world\")\n    self.assertIn(\"disabled\", widget[\"attrs\"])\n\n    # Test passing attrs to as_widget.\n    widget = form[\"field1\"].as_widget(attrs={\"class\": \"custom\"})\n    self.assertIsInstance(widget, Text)\n    self.assertEqual(widget[\"value\"], \"hello\")\n    self.assertEqual(widget[\"attrs\"][\"class\"], \"custom\")\n", "def test_boundfield_build_widget_attrs(self):\n    class CommentForm(Form):\n        name = CharField(max_length=50, required=False)\n        email = EmailField()\n        comment = CharField()\n\n    f = CommentForm(auto_id=False)\n    bf = f[\"email\"]\n    self.assertEqual(\n        bf.build_widget_attrs({\"class\": \"pretty\"}),\n        {\"class\": \"pretty\", \"type\": \"email\", \"name\": \"email\", \"maxlength\": \"320\"},\n    )\n    self.assertEqual(\n        bf.build_widget_attrs({\"class\": \"pretty\"}, widget=f.fields[\"email\"].widget),\n        {\"class\": \"pretty\", \"type\": \"email\", \"name\": \"email\", \"maxlength\": \"320\"},\n    )\n    self.assertEqual(\n        bf.build_widget_attrs({\"class\": \"pretty\"}, widget=f.fields[\"name\"].widget),\n        {\"class\": \"pretty\", \"type\": \"text\", \"name\": \"email\", \"maxlength\": \"50\"},\n    )\n    bf = f[\"name\"]\n    self.assertEqual(\n        bf.build_widget_attrs({\"class\": \"pretty\"}),\n        {\"class\": \"pretty\", \"type\": \"text\", \"name\": \"name\", \"maxlength\": \"50\"},\n    )\n    bf = f[\"comment\"]\n    self.assertEqual(\n        bf.build_widget_attrs({\"class\": \"pretty\"}),\n        {\"class\": \"pretty\", \"type\": \"text\", \"name\": \"comment\"},\n    )\n    f = CommentForm({\"name\": \"John Doe\", \"email\": \"invalid\"}, auto_id=False)\n    bf = f[\"email\"]\n    self.assertEqual(\n        bf.build_widget_attrs({\"class\": \"pretty\"}),\n        {\"class\": \"pretty\", \"type\": \"email\", \"name\": \"email\", \"maxlength\": \"320\"},\n    )\n    bf = f[\"name\"]\n    self.assertEqual(\n        bf.build_widget_attrs({\"class\": \"pretty\"}),\n        {\"class\": \"pretty\", \"type\": \"text\", \"name\": \"name\", \"maxlength\": \"50\"},\n    )\n    bf = f[\"comment\"]\n    self.assertEqual(\n        bf.build_widget_attrs({\"class\": \"pretty\"}),\n        {\"class\": \"pretty\", \"type\": \"text\", \"name\": \"comment\"},\n    )\n", "def test_boundfield_css_classes_with_required_fields(self):\n    class PersonForm(Form):\n        first_name = CharField()\n        last_name = CharField(required=False)\n\n    f = PersonForm()\n    self.assertEqual(f[\"first_name\"].css_classes(), \"required\")\n    self.assertEqual(f[\"last_name\"].css_classes(), \"\")\n", "def test_get_context(self):\n    class FirstNameForm(Form):\n        first_name = CharField()\n\n    f = FirstNameForm()\n    context = f.get_context()\n    self.assertEqual(context, {\"field\": f})\n", "def test_boundfield_render_custom_template(self):\n    class FirstNameForm(Form):\n        first_name = CharField()\n\n    f = FirstNameForm()\n    expected = \"\"\"\n    <div class=\"fieldWrapper\">\n    <label for=\"id_first_name\">First name:</label>\n    <input type=\"text\" name=\"first_name\" required id=\"id_first_name\">\n    </div>\n    \"\"\"\n    self.assertHTMLEqual(f[\"first_name\"].render(template_name=\"forms_tests/custom_field.html\"), expected)\n"], "sample_536": ["def test_ellipse_selector(ax, kwargs):\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    tool = widgets.EllipseSelector(ax, onselect, **kwargs)\n    do_event(tool, 'press', xdata=100, ydata=100, button=1)\n    do_event(tool, 'onmove', xdata=199, ydata=199, button=1)\n\n    # purposely drag outside of axis for release\n    do_event(tool, 'release', xdata=250, ydata=250, button=1)\n\n    if kwargs.get('drawtype', None) not in ['line', 'none']:\n        assert_allclose(tool.geometry,\n                        [[100., 100, 199, 199, 100],\n                         [100, 199, 199, 100, 100]],\n                        err_msg=tool.geometry)\n\n    onselect.assert_called_once()\n    (epress, erelease), kwargs = onselect.call_args\n    assert epress.xdata == 100\n    assert epress.ydata == 100\n    assert erelease.xdata == 199\n    assert erelease.ydata == 199\n    assert kwargs == {}\n", "def test_polygon_selector_create_with_verts(ax):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n    tool = widgets.PolygonSelector(ax, onselect=noop, verts=verts)\n    assert tool.verts == verts\n    for x, y in verts:\n        assert tool._xys[-1] == (x, y)\n", "def test_text_box_empty_validate():\n    ax = get_ax()\n    tool = widgets.TextBox(ax, '', initial='')\n    tool.set_val('example text')\n    tool.set_val('')\n    assert tool.text == ''\n", "def test_polygon_selector_draw_bounding_box(ax):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n    tool = widgets.PolygonSelector(\n        ax, onselect=noop, draw_bounding_box=True)\n    tool.verts = verts\n\n    assert isinstance(tool._box, widgets.RectangleSelector)\n    assert tool._box._selection_completed\n    assert tool._box.extents == (0.1, 0.5, 0.2, 0.9)\n", "def test_cursor(ax):\n    cursor = widgets.Cursor(ax)\n    assert cursor.active is True\n    assert cursor.visible is True\n    assert cursor.useblit is False\n    assert cursor.horizOn is True\n    assert cursor.vertOn is True\n\n    assert cursor.lineh.get_visible() is False\n    assert cursor.linev.get_visible() is False\n\n    event = mock_event(ax, 0.5, 0.5)\n    cursor.onmove(event)\n    assert cursor.lineh.get_visible() is True\n    assert cursor.linev.get_visible() is True\n\n    cursor.set_active(False)\n    assert cursor.active is False\n    cursor.onmove(event)\n    assert cursor.lineh.get_visible() is True\n    assert cursor.linev.get_visible() is True\n\n    cursor.clear(event)\n    cursor.onmove(event)\n    assert cursor.lineh.get_visible() is False\n    assert cursor.linev.get_visible() is False\n"], "sample_539": ["def test_lasso(ax):\n    \"\"\"For lasso, test out the key modifiers\"\"\"\n    tool = widgets.LassoSelector(ax, onselect=noop, useblit=False)\n    tool.extents = (100, 150, 100, 150)\n\n    # drag the rectangle\n    click_and_drag(tool, start=(125, 125), end=(145, 145))\n    assert tool.extents == (120, 170, 120, 170)\n\n    # create from center\n    click_and_drag(tool, start=(100, 100), end=(125, 125), key='control')\n    assert tool.extents == (75, 125, 75, 125)\n\n    # create a square\n    click_and_drag(tool, start=(10, 10), end=(35, 30), key='shift')\n    extents = [int(e) for e in tool.extents]\n    assert extents == [10, 35, 10, 35]\n\n    # create a square from center\n    click_and_drag(tool, start=(100, 100), end=(125, 130), key='ctrl+shift')\n    extents = [int(e) for e in tool.extents]\n    assert extents == [70, 130, 70, 130]\n", "def test_polygon_selector_draw_bounding_box_change(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=False)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Check that no bounding box was drawn.\n    assert tool._box is None\n\n    # Trigger draw_bounding_box to be set to True and the bounding box should\n    # be drawn now.\n    tool.draw_bounding_box = True\n    assert tool._box is not None\n\n    tool.draw_bounding_box = False\n    assert tool._box is None\n", "def test_cursor_visibility(ax):\n    cursor = widgets.Cursor(ax)\n    cursor.set_visible(False)\n    assert not cursor.get_visible()\n    cursor.set_visible(True)\n    assert cursor.get_visible()\n", "def test_LassoSelector(ax):\n    tool = widgets.LassoSelector(ax, onselect=noop)\n    do_event(tool, 'press', xdata=100, ydata=100, button=1)\n    do_event(tool, 'onmove', xdata=125, ydata=125, button=1)\n    do_event(tool, 'release', xdata=150, ydata=150, button=1)\n\n    assert tool.verts == [(100, 100), (125, 125), (150, 150)]\n", "def test_lasso_selector_visible(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot()\n    tool_test = widgets.LassoSelector(ax_test, onselect=noop)\n    tool_test.set_visible(False)\n    tool_test.set_visible(True)\n\n    ax_ref = fig_ref.add_subplot()\n    tool_ref = widgets.LassoSelector(ax_ref, onselect=noop)\n    tool_ref.set_visible(True)\n"], "sample_541": ["def test_lasso_button(ax):\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    tool = widgets.LassoSelector(ax, onselect, button=1)\n    do_event(tool, 'press', xdata=100, ydata=100, button=3)\n    do_event(tool, 'onmove', xdata=125, ydata=125, button=3)\n    do_event(tool, 'release', xdata=150, ydata=150, button=3)\n\n    onselect.assert_not_called()\n", "def test_span_selector_snap_values(ax, snap_values):\n        ax._got_onselect = True\n\n    tool = widgets.SpanSelector(ax, onselect, direction='horizontal',\n                                snap_values=snap_values)\n    tool.extents = (0.2, 0.6)\n    if snap_values is not None:\n        assert tool.extents == (0.3, 0.5)\n    else:\n        assert tool.extents == (0.2, 0.6)\n", "def test_Lasso(ax):\n    \"\"\"For Lasso, test out the basic functionality.\"\"\"\n    tool = widgets.Lasso(ax, [0, 0], callback=noop)\n    tool.verts = [(10, 10), (20, 15), (15, 20)]\n    assert tool.verts == [(10, 10), (20, 15), (15, 20)]\n", "def test_lasso(ax):\n    \"\"\"For lasso, test out the key modifiers\"\"\"\n    tool = widgets.LassoSelector(ax, onselect=noop, button=1)\n\n    do_event(tool, 'press', xdata=100, ydata=100, button=1, key='shift')\n    do_event(tool, 'onmove', xdata=125, ydata=125, button=1)\n\n    do_event(tool, 'release', xdata=150, ydata=150, button=1)\n", "def test_check_buttons_set_and_get(ax):\n    check = widgets.CheckButtons(ax, ('a', 'b', 'c'), (True, False, True))\n    assert check.get_status() == [True, False, True]\n    check.set_active(0)\n    assert check.get_status() == [False, False, True]\n    check.set_active(1)\n    assert check.get_status() == [False, True, True]\n    check.set_active(2)\n    assert check.get_status() == [False, True, False]\n"], "sample_543": ["def test_polygon_selector_ignores_event_outside_polygon(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        ('onmove', dict(xdata=200, ydata=200)),\n        ('press', dict(xdata=200, ydata=200)),\n        ('release', dict(xdata=200, ydata=200)),\n    ]\n    check_polygon_selector(event_sequence, verts, 1, ignore_event_outside=True)\n", "def test_lasso_early_release(ax):\n    tool = widgets.LassoSelector(ax, onselect=noop)\n    do_event(tool, 'press', xdata=50, ydata=50, button=1)\n    do_event(tool, 'onmove', xdata=75, ydata=75, button=1)\n    tool.release(event=mock_event(xdata=100, ydata=100, button=1))\n    assert tool.verts == [(50, 50), (75, 75), (100, 100)]\n", "def test_toolhandles(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot()\n    ax_ref = fig_ref.add_subplot()\n\n    widgets.ToolHandles(ax_test, [0.1, 0.5], [0.2, 0.6],\n                        useblit=False, marker_props={})\n\n    ax_ref.scatter([0.1, 0.5], [0.2, 0.6], marker='o')\n", "def test_polygon_selector_bounding_box(fig_test, fig_ref):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n    ax_test = fig_test.add_subplot()\n\n    tool_test = widgets.PolygonSelector(\n        ax_test, onselect=noop, draw_bounding_box=True)\n\n    ax_ref = fig_ref.add_subplot()\n    tool_ref = widgets.PolygonSelector(\n        ax_ref, onselect=noop, draw_bounding_box=False)\n\n    for ax, tool in [(ax_test, tool_test), (ax_ref, tool_ref)]:\n        event_sequence = [\n            *polygon_place_vertex(*verts[0]),\n            *polygon_place_vertex(*verts[1]),\n            *polygon_place_vertex(*verts[2]),\n            *polygon_place_vertex(*verts[0]),\n        ]\n        for (etype, event_args) in event_sequence:\n            do_event(tool, etype, **event_args)\n", "def test_polygon_selector_rotate(ax):\n    onselect = mock.Mock(spec=noop, return_value=None)\n    tool = widgets.PolygonSelector(ax, onselect, draw_bounding_box=True)\n    event_sequence = [\n        ('press', dict(xdata=50, ydata=50, button=1)),\n        ('onmove', dict(xdata=75, ydata=50, button=1)),\n        ('release', dict(xdata=75, ydata=50, button=1)),\n        ('press', dict(xdata=75, ydata=75, button=1)),\n        ('onmove', dict(xdata=50, ydata=75, button=1)),\n        ('release', dict(xdata=50, ydata=75, button=1)),\n        ('press', dict(xdata=50, ydata=50, button=1)),\n        ('onmove', dict(xdata=50, ydata=50, button=1)),\n        ('release', dict(xdata=50, ydata=50, button=1)),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.extents == (50, 75, 50, 75)\n    initial_center = tool._box.center\n    # Trigger rotation of the selector using the 'r' key\n    do_event(tool, 'on_key_press', key='r')\n    # Move the top-right corner up to rotate the box\n    do_event(tool, 'press', xdata=75, ydata=75, button=1)\n    do_event(tool, 'onmove', xdata=75, ydata=85, button=1)\n    do_event(tool, 'release', xdata=75, ydata=85, button=1)\n    # Release the 'r' key\n    do_event(tool, 'on_key_press', key='r')\n    final_center = tool._box.center\n    assert initial_center == final_center\n    np.testing.assert_allclose(\n        tool._box.extents, (44.64, 80.36, 46.18, 83.82))\n"], "sample_559": ["def test_axes_divider_append_axes_with_sharex():\n    fig, ax = plt.subplots()\n    divider = make_axes_locatable(ax)\n    ax2 = divider.append_axes('top', 1.2, pad=0.1, sharex=ax)\n    ax3 = divider.append_axes('bottom', 1.2, pad=0.1, sharex=ax)\n    ax.plot([1, 2, 3])\n    ax2.plot([1, 2, 3])\n    ax3.plot([1, 2, 3])\n", "def test_inset_locator_multiple_positions():\n    fig, ax = plt.subplots(figsize=[5, 4])\n\n    # prepare the demo image\n    # Z is a 15x15 array\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n    Z2 = np.zeros((150, 150))\n    ny, nx = Z.shape\n    Z2[30:30+ny, 30:30+nx] = Z\n\n    ax.imshow(Z2, extent=extent, interpolation=\"nearest\",\n              origin=\"lower\")\n\n    axins1 = inset_axes(ax, width=1., height=1., bbox_to_anchor=(1, 1),\n                       bbox_transform=ax.transAxes)\n    axins1.imshow(Z2, extent=extent, interpolation=\"nearest\",\n                  origin=\"lower\")\n    axins1.set_xlim(1, 2)\n    axins1.set_ylim(1, 2)\n\n    axins2 = inset_axes(ax, width=0.5, height=0.5, bbox_to_anchor=(0, 0),\n                       bbox_transform=ax.transAxes)\n    axins2.imshow(Z2, extent=extent, interpolation=\"nearest\",\n                  origin=\"lower\")\n    axins2.set_xlim(-1, 0)\n    axins2.set_ylim(-2, -1)\n\n    plt.xticks(visible=False)\n    plt.yticks(visible=False)\n", "def test_anchored_ellipse():\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax.imshow(np.arange(10).reshape((2, 5)))\n    ae = AnchoredEllipse(ax.transData, width=3, height=1, loc='upper right',\n                         pad=0.4, borderpad=0.4, frameon=True)\n    ax.add_artist(ae)\n", "def test_anchored_sizebar():\n    fig, ax = plt.subplots(figsize=(3, 3))\n    axins = zoomed_inset_axes(ax, zoom=2.5, loc='lower right')\n    axins.set_xlim(0, 0.2)\n    axins.set_ylim(0, 0.2)\n\n    asb = AnchoredSizeBar(axins.transData,\n                          0.05,\n                          '5 um',\n                          loc='lower right',\n                          pad=0.1,\n                          borderpad=0.5,\n                          sep=5,\n                          frameon=False)\n    axins.add_artist(asb)\n", "def test_rgb_axes_with_image():\n    fig = plt.figure()\n    ax = RGBAxes(fig, (0.1, 0.1, 0.8, 0.8), pad=0.1)\n    rng = np.random.default_rng(19680801)\n    r = rng.random((5, 5))\n    g = rng.random((5, 5))\n    b = rng.random((5, 5))\n    ax.imshow_rgb(r, g, b, interpolation='none')\n    image = ax.images[0]\n    assert image.get_extents() == (-0.25, 4.75, -0.25, 4.75)\n"], "sample_564": ["def test_3d_quiver_length(fig_test, fig_ref):\n    \"\"\"Test 3D quiver length with different scale values.\"\"\"\n\n    x = np.array([1])\n    y = np.array([2])\n    z = np.array([3])\n    u = np.array([4])\n    v = np.array([5])\n    w = np.array([6])\n\n    fig_test.add_subplot(projection='3d').quiver(x, y, z, u, v, w, length=0.1)\n    fig_ref.add_subplot(projection='3d').quiver(x, y, z, u / 10, v / 10, w / 10)\n", "def test_plot_empty_arrays():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.plot([], [], [], 'o-')\n    ax.plot3D([], [], [], 'o-')\n", "def test_zaxis_set_invert(fig_test, fig_ref):\n    ax1 = fig_test.add_subplot(111, projection='3d')\n    ax2 = fig_ref.add_subplot(111, projection='3d')\n    x = [1, 2, 3]\n    y = [0.0, 0.0, 0.0]\n    z = [0.0, 0.0, 0.0]\n    ax1.plot(x, y, z, 'o-')\n    ax2.plot(x, y, z, 'o-')\n    ax2.set_zlim(1, 0)\n    ax1.zaxis.set_inverted(True)\n", "def test_text3d_position(fig_test, fig_ref):\n    # Test that text3d position is in data coordinates.\n    ax = fig_test.add_subplot(projection=\"3d\")\n    ax.text(1, 1, 1, \"text\")\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 2)\n    ax.set_zlim(0, 2)\n\n    ax = fig_ref.add_subplot(projection=\"3d\")\n    ax.text(1, 1, 1, \"text\")\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 2)\n    ax.set_zlim(0, 2)\n", "def test_mutating_input_arrays_x_and_y(fig_test, fig_ref):\n    \"\"\"\n    Test to see if the `x` and `y` axes do not get mutated\n    after a call to `Axes3D.plot`\n\n    test cases came from GH#8990\n    \"\"\"\n    ax1 = fig_test.add_subplot(111, projection='3d')\n    x = [1, 2, 3]\n    y = [0.0, 0.0, 0.0]\n    z = [0.0, 0.0, 0.0]\n    ax1.plot(x, y, z, 'o-')\n\n    # mutate x and y to get a nontrivial line\n    x[:] = [4, 5, 6]\n    y[:] = [4, 5, 6]\n\n    # draw the same plot without mutating x and y\n    ax2 = fig_ref.add_subplot(111, projection='3d')\n    x = [1, 2, 3]\n    y = [0.0, 0.0, 0.0]\n    z = [0.0, 0.0, 0.0]\n    ax2.plot(x, y, z, 'o-')\n"], "sample_591": ["def test_updateAutoAlign(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, np.nan]), \"b\": (\"x\", [np.nan, 3, 4])}, {\"x\": [0, 1, 2]}\n    )\n    ds1.update(ds2)\n    assert expected.identical(ds1)\n", "    def test_merge_datasets_with_different_types(self):\n        # Create two datasets with different variable types\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2])})\n        ds2 = xr.Dataset({\"b\": (\"x\", [True, False])})\n\n        # Merge the two datasets\n        merged_ds = xr.merge([ds1, ds2])\n\n        # Check that the merged dataset has the correct types\n        assert merged_ds[\"a\"].dtype == np.dtype(\"int64\")\n        assert merged_ds[\"b\"].dtype == np.dtype(\"bool\")\n", "def test_merge_no_conflicts_multi_var(self):\n    data1 = xr.Dataset()\n    data1[\"var1\"] = (\"x\", [1, 2])\n    data1[\"var2\"] = (\"x\", [3, 4])\n    data1.coords[\"x\"] = [0, 1]\n\n    data2 = xr.Dataset()\n    data2[\"var1\"] = (\"x\", [np.nan, 2])\n    data2[\"var3\"] = (\"x\", [5, 6])\n    data2.coords[\"x\"] = [0, 1]\n\n    expected = xr.Dataset()\n    expected[\"var1\"] = (\"x\", [1, 2])\n    expected[\"var2\"] = (\"x\", [3, 4])\n    expected[\"var3\"] = (\"x\", [5, 6])\n    expected.coords[\"x\"] = [0, 1]\n\n    assert expected.identical(data1.merge(data2, compat=\"no_conflicts\"))\n    assert expected.identical(data2.merge(data1, compat=\"no_conflicts\"))\n", "def test_merge_append_dim():\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2, 3])})\n    ds2 = xr.Dataset({\"a\": (\"x\", [4, 5, 6])})\n\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 3, 4, 5, 6])})\n    assert expected.identical(xr.merge([ds1, ds2], dim=\"x\"))\n", "def test_merge_no_conflicts_non_overlapping_variables(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n    ds2 = xr.Dataset({\"b\": (\"y\", [3, 4]), \"y\": [10, 20]})\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2]), \"b\": (\"y\", [3, 4])}, {\"x\": [0, 1], \"y\": [10, 20]}\n    )\n    assert expected.identical(ds1.merge(ds2, compat=\"no_conflicts\"))\n    assert expected.identical(ds2.merge(ds1, compat=\"no_conflicts\"))\n"], "sample_617": ["def test_polyval_degree_dim_not_integer_dtype():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray(\n        [2, 3, 4], dims=\"degree\", coords={\"degree\": [\"zero\", \"one\", \"two\"]}\n    )\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs)\n", "def test_polyval_degree_dim_checks_dataset():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.Dataset(\n        {\"a\": (\"degree\", [2, 3, 4]), \"b\": (\"degree\", [5, 6, 7])},\n        coords={\"degree\": [0, 1, 2]},\n    )\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.drop_vars(\"degree\"))\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.assign_coords(degree=coeffs.degree.astype(float)))\n", "def test_polyval_degree_dim_dtype() -> None:\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.assign_coords(degree=coeffs.degree.astype(float)))\n    xr.polyval(x, coeffs.assign_coords(degree=coeffs.degree.astype(\"int8\")))\n    xr.polyval(x, coeffs.assign_coords(degree=coeffs.degree.astype(\"int16\")))\n    xr.polyval(x, coeffs.assign_coords(degree=coeffs.degree.astype(\"int32\")))\n    xr.polyval(x, coeffs.assign_coords(degree=coeffs.degree.astype(\"int64\")))\n", "def test_polyval_exclude_dims():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    actual = xr.polyval(x, coeffs)\n    assert actual.dims == (\"x\",)\n    assert coeffs.dims == (\"degree\",)\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs, dim=\"x\")\n", "def test_polyval_degree_dim_edge_case() -> None:\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2], dims=\"degree\", coords={\"degree\": [0]})\n    actual = xr.polyval(coord=x, coeffs=coeffs)\n    expected = xr.DataArray([2, 2, 2], dims=\"x\")\n    assert_identical(actual, expected)\n"], "sample_606": ["def test_cross() -> None:\n    # 1D array\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"])\n\n    expected = np.cross(a, b)\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_identical(expected, actual)\n\n    # 2D array with orthogonal dims\n    a = xr.DataArray([[1, 2], [3, 4]], dims=[\"x\", \"y\"])\n    b = xr.DataArray([[5, 6], [7, 8]], dims=[\"y\", \"z\"])\n\n    expected = np.cross(a, b)\n    actual = xr.cross(a, b, dim=\"y\")\n    assert_identical(expected, actual)\n\n    # 2D array with parallel dims\n    a = xr.DataArray([[1, 2], [3, 4]], dims=[\"x\", \"y\"])\n    b = xr.DataArray([[5, 6], [7, 8]], dims=[\"x\", \"y\"])\n\n    expected = np.cross(a, b)\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_identical(expected, actual)\n\n    # 2D array with parallel dims and singleton dimension\n    a = xr.DataArray([[1, 2], [3, 4]], dims=[\"x\", \"y\"])\n    b = xr.DataArray([[[5], [6]], [[7], [8]]], dims=[\"x\", \"y\", \"z\"])\n\n    expected = np.cross(a, b.squeeze())\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_identical(expected, actual)\n\n    # 3D array\n    a = xr.DataArray(\n        [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]],\n        dims=[\"t\", \"x\", \"y\"],\n    )\n    b = xr.DataArray(\n        [[[13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24]]],\n        dims=[\"t\", \"x\", \"y\"],\n    )\n\n    expected = np.cross(a, b)\n    actual = xr.cross(a, b, dim=\"y\")\n    assert_ident", "def test_cross() -> None:\n    a = xr.DataArray(\n        [1, 2, 3],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    b = xr.DataArray(\n        [4, 5, 6],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n\n    expected = xr.DataArray(\n        [-3, 6, -3],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(actual, expected)\n\n    # without coordinate on dimension\n    a = xr.DataArray([1, 2, 3], dims=[\"cartesian\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"])\n\n    expected = xr.DataArray([-3, 6, -3], dims=[\"cartesian\"])\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(actual, expected)\n\n    # 1d vectors\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"])\n\n    expected = xr.DataArray(\n        [12, -6, -3],\n        dims=[\"cartesian\"],\n        coords=dict(cartesian=([\"cartesian\"], [\"x\", \"y\", \"z\"])),\n    )\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(actual, expected)\n\n    # 1d vectors without coordinate on dimension\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"])\n\n    expected = xr.DataArray([12, -6, -3], dims=[\"cartesian\"])\n    actual = xr.cross(a, b, dim=\"cartesian\")\n    assert_identical(actual, expected)\n\n    # broadcasting different length arrays with the same dimension\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"])\n    c = xr.DataArray", "def test_dot_with_nans() -> None:\n    # GH 3694\n    a = np.array([1, 2, np.nan, 4])\n    b = np.array([5, np.nan, 7, 8])\n    da_a = xr.DataArray(a, dims=[\"x\"])\n    da_b = xr.DataArray(b, dims=[\"x\"])\n\n    # join=\"inner\" is the default\n    actual = xr.dot(da_a, da_b)\n    # `dot` sums over the common dimensions of the arguments\n    expected = (da_a * da_b).sum([\"x\"])\n    xr.testing.assert_allclose(expected, actual)\n\n    actual = xr.dot(da_a, da_b, dims=...)\n    expected = (da_a * da_b).sum()\n    xr.testing.assert_allclose(expected, actual)\n", "def test_cross() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"], coords={\"x\": [\"x\", \"y\", \"z\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"], coords={\"x\": [\"x\", \"y\", \"z\"]})\n    expected = xr.DataArray(\n        [-3, 6, -3],\n        dims=[\"x\"],\n        coords={\"x\": [\"x\", \"y\", \"z\"]},\n    )\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_identical(expected, actual)\n\n    # test with different dimension lengths\n    a = xr.DataArray([1, 2], dims=[\"x\"], coords={\"x\": [\"x\", \"y\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"], coords={\"x\": [\"x\", \"y\", \"z\"]})\n    expected = xr.DataArray(\n        [[0, 0, 0], [0, 0, -10], [0, 2, 5]],\n        dims=[\"x\", \"cartesian\"],\n        coords={\"x\": [\"x\", \"y\", \"z\"], \"cartesian\": [\"x\", \"y\", \"z\"]},\n    )\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_identical(expected, actual)\n\n    # test with 2D DataArrays\n    a = xr.DataArray(\n        [[1, 2, 3], [4, 5, 6]],\n        dims=[\"time\", \"x\"],\n        coords={\"time\": [0, 1], \"x\": [\"x\", \"y\", \"z\"]},\n    )\n    b = xr.DataArray(\n        [[4, 5, 6], [1, 2, 3]],\n        dims=[\"time\", \"x\"],\n        coords={\"time\": [0, 1], \"x\": [\"x\", \"y\", \"z\"]},\n    )\n    expected = xr.DataArray(\n        [[-3, 6, -3], [3, -6, 3]],\n        dims=[\"time\", \"x\"],\n        coords={\"time\": [0, 1], \"x\": [\"x\", \"y\", \"z\"]},\n    )\n    actual = xr.cross(a, b, dim=\"x\")\n    assert_identical(expected, actual)\n", "def test_cross() -> None:\n    a = xr.DataArray([1, 2, 3], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    expected = xr.DataArray(\n        [-3, 6, -3],\n        dims=[\"cartesian\"],\n        coords={\"cartesian\": [\"x\", \"y\", \"z\"]},\n    )\n    assert_identical(xr.cross(a, b, dim=\"cartesian\"), expected)\n\n    # cases with size < 3 along dim\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\"]})\n    b = xr.DataArray([4, 5], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\"]})\n    expected = xr.DataArray(-3)\n    assert_identical(xr.cross(a, b, dim=\"cartesian\"), expected)\n\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"z\"]})\n    b = xr.DataArray([4, 5, 6], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    expected = xr.DataArray([12, -6, -3], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"y\", \"z\"]})\n    assert_identical(xr.cross(a, b, dim=\"cartesian\"), expected)\n\n    # case with multiple array size 2\n    a = xr.DataArray([1, 2], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"z\"]})\n    b = xr.DataArray([4, 5], dims=[\"cartesian\"], coords={\"cartesian\": [\"x\", \"z\"]})\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"cartesian\")\n\n    # case with broadcasting\n    a = xr.DataArray(\n        [[1, 2], [3, 4], [5, 6]],\n        dims=[\"time\", \"cartesian\"],\n        coords={\"time\": [0, 1, 2], \"cartesian\": [\"x\", \"y\"]},\n    )\n    b = xr.DataArray"], "sample_720": ["def test_power_transformer_method_warning():\n    pt = PowerTransformer(method='yeo-johnson')\n    X = np.abs(X_2d)\n\n    # A warning should be raised if PowerTransformer.method isn't 'box-cox'\n    warning_message = \"'method' will be set to 'box-cox' in a future version\"\n    assert_warns_message(DeprecationWarning, warning_message,\n                         pt.fit, X)\n", "def test_power_transformer_dtype_exception():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_2d).astype(np.object)\n\n    # An exception should be raised if PowerTransformer is given a non-numeric\n    # array\n    dtype_message = 'X should be a numeric array-like or dataframe'\n    assert_raise_message(TypeError, dtype_message,\n                         pt.fit, X)\n", "def test_power_transformer_method():\n    X = np.abs(X_2d)\n\n    for method in ['yeo-johnson']:\n        assert_raise_message(ValueError,\n                             \"'method' must be one of 'box-cox', \"\n                             \"'yeo-johnson' is not supported\",\n                             PowerTransformer(method=method).fit, X)\n\n    # Test valid method\n    pt = PowerTransformer(method='box-cox')\n    pt.fit(X)\n", "def test_power_transformer_type_exception():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_2d)\n\n    # An exception should be raised if input is not an array-like\n    wrong_type_message = \"Input should be a 2D NumPy array\"\n    assert_raise_message(TypeError, wrong_type_message,\n                         pt.fit, 10)\n    assert_raise_message(TypeError, wrong_type_message,\n                         pt.fit, \"ten\")\n", "def test_power_transformer_inverse_transform_notfitted():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_1col)\n    assert_raises(NotFittedError, pt.inverse_transform, X)\n"], "sample_744": ["def test_power_transformer_inverse():\n    # Test that inverse transformation is inverse of forward transformation\n    X = np.abs(X_2d)\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n", "def test_power_transformer_constant_feature():\n    X = np.array([[1, 0, 3], [1, 0, 4], [1, 0, 5]])\n\n    pt = PowerTransformer(method='box-cox')\n    X_trans = pt.fit_transform(X)\n\n    # Check that the transformation is applied only to non-constant features\n    assert_array_equal(X_trans[:, 0], X[:, 0])\n    assert_array_equal(X_trans[:, 1], X[:, 1])\n\n    # Check that the inverse transformation works correctly\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n", "def test_power_transformer_method_box_cox_lambda_equal_to_zero():\n    # If PowerTransformer(method='box-cox').lambdas_ equals to zero, then\n    # the log function should be applied\n    pt = PowerTransformer(method='box-cox', standardize=False)\n\n    X_fit = np.array([[1., 2.], [3., 4.]])\n    pt.fit(X_fit)\n\n    # Set lambdas_ to zero\n    pt.lambdas_ = np.array([0., 0.])\n\n    X_trans = pt.transform(X_fit)\n    X_inv = pt.inverse_transform(X_trans)\n\n    assert_array_almost_equal(X_inv, X_fit)\n", "def test_power_transformer_function_equivalence():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n        X_trans_class = pt.fit_transform(X)\n        X_trans_func = power_transform(X, standardize=standardize)\n        assert_array_almost_equal(X_trans_class, X_trans_func)\n\n        X_inv_class = pt.inverse_transform(X_trans_class)\n        X_inv_func = power_transform(X_trans_func, standardize=standardize,\n                                     inverse=True)\n        assert_array_almost_equal(X_inv_class, X_inv_func)\n", "def test_power_transformer_sparse_matrix():\n    # PowerTransformer should raise an error for sparse matrices\n    X_sparse = sparse.csr_matrix(X_2d)\n    pt = PowerTransformer(method='box-cox')\n\n    assert_raise_message(ValueError,\n                         \"PowerTransformer does not accept sparse matrices.\",\n                         pt.fit, X_sparse)\n"], "sample_747": ["def test_power_transformer_lambda_equal_0():\n    # Test that PowerTransformer handles zero lambdas correctly\n    X = np.abs(X_2d)\n\n    pt = PowerTransformer(method='box-cox', standardize=True)\n    X_trans = pt.fit_transform(X)\n\n    # Check that lambda value of zero is handled correctly.\n    X_lambda_0 = np.zeros((X.shape[0], 1))\n    X_expected, lambda_expected = stats.boxcox(X_lambda_0.flatten())\n    assert_almost_equal(np.log(X_lambda_0.flatten() + 1), X_expected)\n\n    # Check that X_trans has no NaN values\n    assert_false(np.isnan(X_trans).any())\n", "def test_power_transformer_dtype_exception():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_2d)\n    X_invalid_dtype = np.array(X, dtype='object')\n\n    # An exception should be raised if input data has invalid dtype\n    bad_dtype_message = \"could not convert string to float\"\n    assert_raise_message(ValueError, bad_dtype_message,\n                         pt.fit, X_invalid_dtype)\n", "def test_power_transformer_input_type():\n    pt = PowerTransformer(method='box-cox')\n    X_int = np.array([[1, 2], [3, 4]])\n    X_float = np.array([[1.0, 2.0], [3.0, 4.0]])\n\n    # Check that integer input is cast to float\n    assert_warns_message(DataConversionWarning,\n                         \"Data with input dtype int64 was converted to \"\n                         \"float64 by PowerTransformer.\",\n                         pt.fit, X_int)\n\n    pt.fit(X_float)\n    assert pt.lambdas_.dtype == np.float64\n", "def test_maxabs_scaler_sparse_axis1():\n    # Test MaxAbsScaler with sparse data and axis=1\n    X_sparse = sparse.csr_matrix([[0, 1, 0, 0],\n                                  [1, 0, 0, 0],\n                                  [0, 0, 0, 1]])\n\n    scaler = MaxAbsScaler()\n    X_scaled_sparse = scaler.fit_transform(X_sparse)\n\n    assert_array_almost_equal(X_scaled_sparse.max(axis=0).toarray(),\n                              [[1, 1, 0, 1]])\n\n    X_scaled_sparse = maxabs_scale(X_sparse, axis=1)\n    assert_array_almost_equal(X_scaled_sparse.max(axis=1).toarray(),\n                              [[1], [1], [1]])\n", "def test_power_transformer_invalid_method():\n    X = np.abs(X_2d)\n\n    pt = PowerTransformer(method='invalid')\n    assert_raise_message(ValueError, \"'method' must be one of\",\n                         pt.fit, X)\n\n    pt = PowerTransformer(method=None)\n    assert_raise_message(ValueError, \"'method' must be one of\",\n                         pt.fit, X)\n\n    pt = PowerTransformer(method=1)\n    assert_raise_message(ValueError, \"'method' must be one of\",\n                         pt.fit, X)\n\n    pt = PowerTransformer(method=[1, 2, 3])\n    assert_raise_message(ValueError, \"'method' must be one of\",\n                         pt.fit, X)\n"], "sample_869": ["def test_balanced_accuracy_score_multiclass():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n\n    macro_recall = recall_score(y_true, y_pred, average='macro')\n    with ignore_warnings():\n        balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == pytest.approx(macro_recall)\n    adjusted = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    chance = 1 / len(set(y_true))\n    assert adjusted == pytest.approx((balanced - chance) / (1 - chance))\n", "def test_balanced_accuracy_score_weighted():\n    y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1]\n    y_pred = [0, 0, 0, 1, 1, 1, 0, 0, 0]\n    sample_weight = [1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5]\n    # unweighted\n    recall_unweighted = recall_score(y_true, y_pred, average='macro')\n    # weighted\n    recall_weighted = recall_score(y_true, y_pred, average='macro',\n                                   sample_weight=sample_weight)\n    # balanced accuracy (macro recall)\n    balanced_unweighted = balanced_accuracy_score(y_true, y_pred)\n    balanced_weighted = balanced_accuracy_score(y_true, y_pred,\n                                                sample_weight=sample_weight)\n\n    # check equality to unweighted macro recall\n    assert balanced_unweighted == pytest.approx(recall_unweighted)\n    # check equality to weighted macro recall\n    assert balanced_weighted == pytest.approx(recall_weighted)\n", "def test_brier_score_loss_multiclass():\n    # Check brier_score_loss function for multiclass case\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.3, 0.2, 0.5],\n                       [0.7, 0.2, 0.1], [0.2, 0.6, 0.2], [0.1, 0.2, 0.7]])\n\n    # check error for non-binary y_true\n    err_msg = \"Only binary classification is supported. Labels in y_true: [0 1 2]\"\n    with pytest.raises(ValueError, match=err_msg):\n        brier_score_loss(y_true, y_pred)\n", "def test_jaccard_similarity_score_deprecated():\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[0, 0, 1], [1, 0, 1]])\n\n    with pytest.warns(FutureWarning, match=\"is deprecated since version 0.21\"):\n        jaccard_similarity_score(y_true, y_pred, normalize=True)\n    with pytest.warns(FutureWarning, match=\"is deprecated since version 0.21\"):\n        jaccard_similarity_score(y_true, y_pred, normalize=False)\n", "def test_balanced_accuracy_score_binary():\n    y_true = [0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1]\n    y_pred = [1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n\n    # compute scores with default labels introspection\n    ba = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(ba, 0.5)\n\n    ba = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    assert_almost_equal(ba, 0)\n"], "sample_875": ["def test_brier_score_loss_multioutput():\n    # Check brier_score_loss function for multi-output\n    y_true = np.array([[0, 1, 1], [1, 0, 0]])\n    y_pred = np.array([[0.1, 0.8, 0.9], [0.3, 1.0, 0.95]])\n\n    assert_almost_equal(brier_score_loss(y_true, y_true), 0.0)\n    assert_almost_equal(\n        brier_score_loss(y_true, y_pred),\n        linalg.norm(y_true.ravel() - y_pred.ravel()) ** 2 / (y_true.shape[0] * y_true.shape[1]),\n    )\n\n    y_true = np.array([0, 0, 0])\n    y_pred = np.array([[0.1, 0.8, 0.9]])\n\n    assert_almost_equal(brier_score_loss(y_true, y_true), 0.0)\n    assert_almost_equal(\n        brier_score_loss(y_true, y_pred),\n        linalg.norm(y_true - y_pred[0]) ** 2 / len(y_true),\n    )\n", "def test_multilabel_confusion_matrix_with_y_true_all_zeros():\n    y_true = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    y_pred = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0]])\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(\n        cm,\n        np.array([[[0, 0], [3, 0]], [[0, 0], [2, 1]], [[0, 0], [2, 1]]]),\n    )\n    cm = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    assert_array_equal(\n        cm,\n        np.array([[[0, 1], [0, 0]], [[0, 1], [0, 1]], [[0, 2], [0, 0]]]),\n    )\n", "def test_balanced_accuracy_score_binary_input():\n    # Test that balanced accuracy works for binary input data\n    y_true = [0, 0, 0, 1, 1, 1]\n    y_pred = [0, 0, 1, 1, 1, 0]\n    cm = confusion_matrix(y_true, y_pred)\n    assert (\n        balanced_accuracy_score(y_true, y_pred)\n        == recall_score(y_true, y_pred, average=\"macro\")\n        == (cm[0, 0] / cm[0].sum() + cm[1, 1] / cm[1].sum()) / 2\n    )\n", "def test_balanced_accuracy_score_zero_division():\n    # When a class has no instances, the recall of that class is zero by\n    # definition; it should not be nan, even when zero_division is 'warn'.\n    y_true = [0, 1]\n    y_pred = [1, 1]\n    recall = recall_score(y_true, y_pred, average=None, zero_division='warn')\n    assert np.all(recall == np.array([0., 1.]))\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == recall.mean()\n    # The overall recall is the mean of the class-wise recall.\n", "def test_balanced_accuracy_score_binary():\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([0, 1, 1, 1])\n    cm = confusion_matrix(y_true, y_pred)\n    sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n    specificity = cm[0, 0] / (cm[0, 1] + cm[0, 0])\n    balanced = (sensitivity + specificity) / 2\n    assert balanced == pytest.approx(balanced_accuracy_score(y_true, y_pred))\n"], "sample_906": ["def test_domain_cpp_ast_ptr_to_member():\n    check('type', '{key}int A::*', {2: 'PM1AIiE'}, key='typedef')\n    check('type', '{key}int A::B::*', {2: 'PN1A1B3pMEiE'}, key='typedef')\n    check('type', '{key}int A::*volatile', {2: 'PVPM1AIiE'}, key='typedef')\n    check('type', '{key}int A::*const', {2: 'PKPM1AIiE'}, key='typedef')\n    check('type', '{key}int A::*const volatile', {2: 'PKVPM1AIiE'}, key='typedef')\n    check('type', '{key}int A::*volatile const', {2: 'PKVPM1AIiE'}, key='typedef')\n\n    check('function', 'void f(int A::*)', {2: '1fM1AIiE'})\n    check('function', 'void f(int A::* p)', {2: '1fM1AIiE'})\n    check('function', 'void f(int ::A::* p)', {2: '1fM1AIiE'})\n    check('function', 'void f(int A::*const)', {2: '1fKM1AIiE'})\n    check('function', 'void f(int A::*const&)', {2: '1fRKM1AIiE'})\n    check('function', 'void f(int A::*volatile)', {2: '1fVM1AIiE'})\n    check('function', 'void f(int A::*const volatile)', {2: '1fVKM1AIiE'},\n          output='void f(int A::*volatile const)')\n    check('function', 'void f(int A::*volatile const)', {2: '1fVKM1AIiE'})\n    check('function', 'void f(int A::* p) = 0', {2: '1fM1AIiE'})\n    check('function', 'void f(int A::*const p) = 0', {2: '1fKM1AIiE'})\n    check('function', 'void f(int A::*volatile p) = 0', {2: '1fVM1AIiE'})\n    check('function', 'void f(int A::*const volatile p) = 0", "def test_domain_cpp_ast_array():\n    check('member', '{key}int a[]', {1: 'a__iaA', 2: '1a'})\n    check('member', '{key}int a[2]', {1: 'a__ia2', 2: '1a'})\n    check('member', '{key}int a[2][2]', {1: 'a__ia2A2', 2: '1a'})\n    check('member', '{key}int a[][]', {1: 'a__iaAA', 2: '1a'})\n\n    check('member', '{key}int (*a)[]', {1: 'a__iPA', 2: '1a'})\n    check('member', '{key}int (*a)[2]', {1: 'a__iPA2', 2: '1a'})\n    check('member', '{key}int (*a)[2][2]', {1: 'a__iPA2A2', 2: '1a'})\n    check('member', '{key}int (*a)[][]', {1: 'a__iPAA', 2: '1a'})\n\n    check('member', '{key}int a[][2]', {1: 'a__iaA2', 2: '1a'})\n    check('member', '{key}int (*a)[][2]', {1: 'a__iPAA2', 2: '1a'})\n\n    check('member', '{key}int a[2][]', {1: 'a__ia2A', 2: '1a'})\n    check('member', '{key}int (*a)[2][]', {1: 'a__iPA2A', 2: '1a'})\n\n    check('member', '{key}int a[static 2]', {1: 'a__iaS2', 2: '1a'})\n    check('member', '{key}int a[static const 2]', {1: 'a__iaS2', 2: '1a'})\n    check('member', '{key}int a[*]', {1: 'a__iaLi', 2: '1a'})\n    check('member', '{key}int a[static const restrict volatile 42]', {1: 'a__iaSCRV42', 2: '1a'})\n   ", "def test_domain_cpp_ast_expression_fallback():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(\"a + b\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = True\n    ast = parser.parse_expression()\n    res = str(ast)\n    assert res == \"a + b\"\n    assert isinstance(ast, cppDomain.ASTFallbackExpr)\n    ast.describe_signature(addnodes.desc_signature, 'markType', None, None)\n\n    # if fallback is not allowed, the parser will raise an exception\n    parser = DefinitionParser(\"a + b\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    try:\n        ast = parser.parse_expression()\n        assert False\n    except DefinitionError as ex:\n        assert \"Parse error at 'a'. Expected primary expression.\" in str(ex)\n", "def test_domain_cpp_ast_invalid_base_classes():\n    check('class', '{key}A : B...', {2: '1A'}, key='class')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A : B')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A : private')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A : B,')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A : B, private')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A : B, C private')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A : B private C')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A : B, C,')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A : B, C, private')\n", "def test_domain_cpp_alias_directive():\n    # test the alias directive (this should really be split into multiple tests)\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n    rootSymbol = Symbol(None, None, None, None, None)\n    env = {'c:parent_symbol': rootSymbol,\n           'c:namespace_stack': [rootSymbol]}\n\n    parser = DefinitionParser(\"A::B::C\", location=None,\n                              config=Config())\n    ast = parser.parse_declaration(\"any\", \"any\")\n    parser.assert_end()\n    add_symbols = [\n        # declared symbols, parent, docname, line\n        (\"A\", ast, \"desc1\", 6),\n        (\"B\", ast, \"desc1\", 7),\n        (\"C\", ast, \"desc1\", 8),\n    ]\n    for name, ast, docname, line in add_symbols:\n        symbol = rootSymbol.add_declaration(\n            ast, docname=docname, line=line)\n        ast.symbol = symbol\n    # render the directive\n    node = CAliasObject()\n    node.arguments = [\"A::B::C\"]\n    node.options = {\"maxdepth\": 1}\n    node.run()\n    # check that the rendered symbols are as expected\n    assert node.children[0].astext() == \"namespace A {\\n}\"\n    assert node.children[1].astext() == \"namespace B {\\n}\"\n    assert node.children[2].astext() == \"class C { ... }\\n\"\n"], "sample_907": ["def test_domain_cpp_ast_defaulted_constrained_type():\n    check('type', '{key}T = U', {2: '1T'}, key='using')\n    check('type', '{key}T = typename U::X', {2: '1T'}, key='using')\n    check('type', '{key}T = const U', {2: '1T'}, key='using')\n    check('type', '{key}T = volatile U', {2: '1T'}, key='using')\n    check('type', '{key}T = const volatile U', {2: '1T'}, key='using')\n    check('type', '{key}T = U*', {2: '1T'}, key='using')\n    check('type', '{key}T = U&', {2: '1T'}, key='using')\n    check('type', '{key}T = U&&', {2: '1T'}, key='using')\n    check('type', '{key}T = std::vector<U>', {2: '1T'}, key='using')\n    check('type', '{key}T = std::vector<U*>', {2: '1T'}, key='using')\n    check('type', '{key}T = std::vector<U>&', {2: '1T'}, key='using')\n    check('type', '{key}T = std::vector<U>&&', {2: '1T'}, key='using')\n", "def test_domain_cpp_parse_template_introduction():\n    # check parsing of the template list\n    ast = parse('class', 'abc::ns::foo{{id_0, id_1, id_2}} xyz::bar')\n    assert ast.templatePrefix.templates[0].concept.name == ('abc', 'ns', 'foo')\n    assert len(ast.templatePrefix.templates[0].params) == 3\n\n    ast = parse('class', 'abc::ns::foo{{id_0, id_1, ...id_2}} xyz::bar')\n    assert ast.templatePrefix.templates[0].concept.name == ('abc', 'ns', 'foo')\n    assert len(ast.templatePrefix.templates[0].params) == 3\n\n    # check the correct recursion into template introductions\n    ast = parse('class', 'abc::ns::foo{{id_0, id_1, id_2}} xyz::bar<id_0, id_1, id_2>')\n    assert ast.templatePrefix.templates[0].concept.name == ('abc', 'ns', 'foo')\n    assert len(ast.templatePrefix.templates[0].params) == 3\n    assert len(ast.templatePrefix.templates) == 1\n    assert len(ast.templateParams.params) == 3\n\n    ast = parse('class', 'abc::ns::foo{{id_0, id_1, ...id_2}} xyz::bar<id_0, id_1, id_2...>')\n    assert ast.templatePrefix.templates[0].concept.name == ('abc', 'ns', 'foo')\n    assert len(ast.templatePrefix.templates[0].params) == 3\n    assert len(ast.templatePrefix.templates) == 1\n    assert len(ast.templateParams.params) == 3\n\n    # check error handling\n    with pytest.raises(DefinitionError):\n        parse('class', 'abc::ns::foo id_0, id_1 xyz::bar')\n    with pytest.raises(DefinitionError):\n        parse('class', 'abc::ns::foo{ id_0, id_1} xyz::bar')\n    with pytest.raises(DefinitionError):\n        parse('class', 'abc::ns::foo{id_0, id_1, ... id_2} xyz::bar')\n", "def test_domain_cpp_create_folder(app):\n    app.build(None, [\"create_folderCPP\"])\n    assert (app.outdir / \"create_folderCPP/index.html\").exists()\n", "def test_domain_cpp_lookup_key():\n    rootSymbol = Symbol(None, None, None, None, None, None, None)\n    ast1 = parse('type', 'public bool b')\n    symbol1 = rootSymbol.add_declaration(ast1, docname=\"TestDoc\", line=42)\n    ast2 = parse('type', 'public bool c')\n    symbol2 = rootSymbol.add_declaration(ast2, docname=\"TestDoc\", line=42)\n\n    assert symbol1.get_lookup_key().data == [\n        (ASTNestedNameElement(ASTIdentifier('b'), None), None, '1b')]\n    assert symbol2.get_lookup_key().data == [\n        (ASTNestedNameElement(ASTIdentifier('c'), None), None, '1c')]\n\n    # test the lookup for a symbol which is not found\n    ast3 = parse('type', 'public bool d')\n    symbol3 = rootSymbol.add_declaration(ast3, docname=\"TestDoc\", line=42)\n    result = rootSymbol.direct_lookup(symbol3.get_lookup_key())\n    assert result == symbol3\n", "def test_domain_cpp_print_symbol_lookup_tree():\n    Symbol.debug_show_tree = True\n    root = Symbol(None, None, None, None, None, None, None)\n    s1 = root.add_name(\"A\")\n    s2 = s1.add_name(\"B\")\n    s3 = s2.add_name(\"C\")\n    s3.add_name(\"D\")\n    s1.add_name(\"E\")\n    print(root.dump(0))\n    Symbol.debug_show_tree = False\n"], "sample_911": ["def test_template_introduction():\n    check('class', 'concept C<typename T, typename U> = requires { }', {2: 'I00EC'})\n    check('class', 'concept C<typename T, typename U> = requires (t) { }', {2: 'I0EX1tEEC'})\n    check('class', 'concept C<typename T, typename U> = requires (T t, U u) { }', {2: 'I0EX1T1t1U1uEEC'})\n    check('class', 'concept C<typename T, typename U> = requires (T t, U u, ...) { }', {2: 'I0EX1T1t1U1usp1uEEC'})\n    check('class', 'concept C<typename T, typename U> = requires (T t, U...) { }', {2: 'I0EX1T1tDp1UEEC'})\n    check('class', 'concept C<typename T, typename U> = requires (T t, U u...) { }', {2: 'I0EX1T1t1UDp1uEEC'})\n    check('class', 'concept C<typename T, typename U> = requires (T t...) { }', {2: 'I0EXT1tDpEEEc'})\n    check('class', 'concept C<typename T, typename U> = requires (...) { }', {2: 'I0EEXEEEc'})\n    check('class', 'concept C<typename T, typename U> = requires (T...) { }', {2: 'I0EXTDpEEEc'})\n\n    check('class', 'concept C<typename T, typename U> = requires { sizeof...(2); }', {2: 'I00EC'})\n    check('class', 'concept C<typename T, typename U> = requires (t) { sizeof...(2); }', {2: 'I0EX1tEEC'})\n    check('class', 'concept C<typename T, typename U> = requires (T t, U u) { sizeof...(2); }', {2: 'I0EX1T1t1U1uEEC'})\n    check('class', 'concept C<typename T, typename U> = requires (T t, U u, ...) { sizeof...(2); }', {2: 'I0EX1T1t1U1", "def test_template_parameter_packs():\n    check('function', 'template<typename... Ts> void f(Ts... ts)',\n          {2: 'IDpEv1fDp1Ts'})\n    check('function', 'template<typename... Ts> void f(Ts... ts, int i)',\n          {2: 'IDpEv1fDp1TsLiE'})\n    check('function', 'template<typename... Ts> void f(Ts... ts, int i, char c)',\n          {2: 'IDpEv1fDp1TsLiEc'})\n    check('function', 'template<typename... Ts> void f(int i, Ts... ts)',\n          {2: 'IDpEv1fiDp1Ts'})\n    check('function', 'template<typename... Ts> void f(int i, char c, Ts... ts)',\n          {2: 'IDpEv1fiCcDp1Ts'})\n    check('function', 'template<typename... Ts> void f(Ts... ts, Ts... ts2)',\n          {2: 'IDpEv1fDp1TsDpS1_'})\n\n    check('function', 'template<typename... Ts> void f(Ts... ts, int i, Ts... ts2)',\n          {2: 'IDpEv1fDp1TsLiEDpS1_'})\n", "def test_xref_consistency_xrefs_exist_in_html_output(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?<a .*?>.*?'\n                   r'<code .*?>.*?{xref}</code>.*?</a>').format(role=role, xref=xref)\n        assert re.search(pattern, output), (pattern, test)\n\n    assert_xref('cpp:identifier', 'Sphinx')\n    assert_xref('cpp:class', 'Sphinx')\n    assert_xref('cpp:class', 'Sphinx::version')\n    assert_xref('cpp:var', 'version')\n    assert_xref('cpp:type', 'List')\n    assert_xref('cpp:enum', 'MyEnum')\n    assert_xref('cpp:enumerator', 'MyEnum::A')\n    assert_xref('cpp:any', 'main')\n    assert_xref('cpp:class', 'X')\n    assert_xref('cpp:class', 'X::Y')\n    assert_xref('cpp:class', 'X::Y::Z')\n    assert_xref('cpp:func', 'X::X()')\n    assert_xref('cpp:func', 'X::X(int)')\n    assert_xref('cpp:func', 'X::~X()')\n    assert_xref('cpp:func', 'X::doSomething()')\n    assert_xref('cpp:func', 'X::Y::doSomething()')\n    assert_xref('cpp:func', 'X::Y::Z::doSomething()')\n    assert_xref('cpp:func', 'operator bool()')\n    assert_xref('cpp:func', 'operator bool operator()()')\n    assert_xref('cpp:operator', 'operator bool()')\n    assert_xref('cpp:operator', 'operator bool operator()()')\n    assert_xref('cpp:parent', 'Sphinx::version')\n    assert_xref('cpp:any', 'Sphinx::version')\n    assert_xref('cpp:parent', 'X::X()')\n    assert_xref('cpp:parent', 'X::Y::Y()')\n    assert_xref('cpp:parent', 'X::Y::Z::Z()')\n    assert_xref('cpp:parent', 'X::X(int", "def test_type_using():\n    check('type', 'A = B', {2: '1A'})\n    check('type', 'A = decltype(b)', {2: '1A'})\n    check('type', 'A::B = int', {2: 'N1A1BE'})\n    check('type', 'A::B = decltype(b)', {2: 'N1A1BE'})\n", "def test_template_parameter_qualified_name():\n    # test for template parameter qualified name\n    check('class', 'A<T>::B', {2: \"I0E1AI1TE1B\"}, output='template<> A<T>::B')\n    check('class', 'A<T>::B::B', {2: \"I0E1AI1TE1B1B\"}, output='template<> A<T>::B::B')\n"], "sample_913": ["def test_python_domain_get_objects():\n    domain = PythonDomain(Mock(env=Mock(docnames=[])))\n\n    domain.objects = {\n        \"mod1\": (\"doc1\", \"mod1\", \"module\"),\n        \"mod2.func\": (\"doc2\", \"mod2.func\", \"function\"),\n        \"mod3.Class\": (\"doc3\", \"mod3.Class\", \"class\"),\n    }\n    domain.modules = {\n        \"mod1\": (\"doc1\", \"mod1\", \"\", \"\", False),\n        \"mod2\": (\"doc2\", \"mod2\", \"\", \"\", False),\n        \"mod3\": (\"doc3\", \"mod3\", \"\", \"\", False),\n    }\n\n    objects = list(domain.get_objects())\n    assert sorted(objects) == [\n        (\"mod1\", \"mod1\", \"module\", \"doc1\", \"mod1\", 0),\n        (\"mod2.func\", \"mod2.func\", \"function\", \"doc2\", \"mod2.func\", 1),\n        (\"mod3.Class\", \"mod3.Class\", \"class\", \"doc3\", \"mod3.Class\", 1),\n    ]\n", "def test_get_index_text(app):\n    domain = app.env.get_domain('py')\n\n    # Test for py:function\n    obj = domain.get_index_text('module', ('function', ''))\n    assert obj == 'function() (in module module)'\n\n    # Test for py:data\n    obj = domain.get_index_text('module', ('data', ''))\n    assert obj == 'data (in module module)'\n\n    # Test for py:class\n    obj = domain.get_index_text('module', ('Class', ''))\n    assert obj == 'Class (class in module)'\n\n    # Test for py:exception\n    obj = domain.get_index_text('module', ('Exception', ''))\n    assert obj == 'Exception'\n\n    # Test for py:method\n    obj = domain.get_index_text('module', ('Class.method', ''))\n    assert obj == 'method() (Class method)'\n\n    # Test for py:staticmethod\n    obj = domain.get_index_text('module', ('Class.static_method', ''))\n    assert obj == 'static_method() (Class static method)'\n\n    # Test for py:classmethod\n    obj = domain.get_index_text('module', ('Class.class_method', ''))\n    assert obj == 'class_method() (Class class method)'\n\n    # Test for py:attribute\n    obj = domain.get_index_text('module', ('Class.attribute', ''))\n    assert obj == 'attribute (Class attribute)'\n", "def test_get_index_text():\n    domain = PythonDomain(Mock(env={}))\n    assert domain.get_index_text('module', ('function', None)) == 'function() (in module module)'\n    assert domain.get_index_text('module', ('data', None)) == 'data (in module module)'\n    assert domain.get_index_text('module', ('class', None)) == 'class (class in module)'\n    assert domain.get_index_text('module', ('exception', None)) == 'exception'\n    assert domain.get_index_text('module', ('method', 'class')) == 'method() (module.class method)'\n    assert domain.get_index_text('module', ('staticmethod', 'class')) == 'staticmethod() (module.class static method)'\n    assert domain.get_index_text('module', ('classmethod', 'class')) == 'classmethod() (module.class class method)'\n    assert domain.get_index_text('module', ('attribute', 'class')) == 'attribute (module.class attribute)'\n", "def test_pyattribute_signature_old(app):\n    text = (\".. py:attribute:: attr\\n\"\n            \"   :annotation: = 1\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                    [desc_annotation, \" = 1\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"attribute\",\n                domain=\"py\", objtype=\"attribute\", noindex=False)\n    assert 'attr' in domain.objects\n    assert domain.objects['attr'] == ('index', 'attr', 'attribute')\n", "def test_get_full_qualified_name_with_class_context():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # with py:class context\n    kwargs = {'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'Class.func'\n\n    # with py:module and py:class context\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n"], "sample_917": ["def test_lookup_key():\n    rootSymbol = Symbol(None, None, None, None, None, None)\n\n    # Test that lookup keys for simple and template symbols are created correctly\n    simpleSymbol = rootSymbol.add_name(\"A\")\n    templateSymbol = rootSymbol.add_name(\"B\", templateParams=ASTTemplateParams([ASTTemplateParamType(ASTTemplateKeyParamPackIdDefault(\"class\", None, False, None))]))\n    assert str(simpleSymbol.get_lookup_key()) == \"LookupKey(data=[(ASTNestedNameElement(ASTIdentifier('A'), None), None, None)])\"\n    assert str(templateSymbol.get_lookup_key()) == \"LookupKey(data=[(ASTNestedNameElement(ASTIdentifier('B'), ASTTemplateArgs([ASTTypeBase('class')], False)), ASTTemplateParams([ASTTemplateParamType(ASTTemplateKeyParamPackIdDefault('class', None, False, None))]), None)])\"\n\n    # Test that lookup keys for function and class symbols are created correctly\n    functionSymbol = rootSymbol.add_declaration(ASTDeclaration(\"function\", \"function\", None, None, ASTFunctionDeclarator(\"f\", [], None)), docname=\"TestDoc\")\n    classSymbol = rootSymbol.add_declaration(ASTDeclaration(\"class\", \"class\", None, None, ASTClassDeclarator(\"C\", [], None)), docname=\"TestDoc\")\n    assert str(functionSymbol.get_lookup_key()) == \"LookupKey(data=[(ASTNestedNameElement(ASTIdentifier('f'), None), None, 'function')])\"\n    assert str(classSymbol.get_lookup_key()) == \"LookupKey(data=[(ASTNestedNameElement(ASTIdentifier('C'), None), None, 'class')])\"\n", "def test_xref_label(app, status, warning):\n    test = 'xref_label.html'\n    app.builder.build_all()\n    output = (app.outdir / test).read_text()\n\n    # check if :cpp:any: creates a label for overloaded functions\n    pattern = r'<span class=\"sig-name descname\"><span class=\"descclassname\">(function</span> <span class=\"descname\">foo</span><span class=\"descclassname\">)</span></span>'\n    result = re.search(pattern, output)\n    expect = '''\\", "def test_cross_references():\n    # Create a simple symbol table\n    root = Symbol(None, None, None, None, None, None)\n    symbol1 = root.add_name(\"Test1\", \"1Test1\")\n    symbol2 = root.add_name(\"Test2\", \"1Test2\")\n\n    # Check :any: cross reference\n    node = AliasNode(\"Test1\", root)\n    xref = CPPXRefRole()(None, node, None, node, None)\n    assert xref == (\"Test1\", \"1Test1\")\n\n    # Check :any: cross reference with explicit title\n    node = AliasNode(\"Test1\", root)\n    xref = CPPXRefRole()(None, node, \"Test1Title\", node, None)\n    assert xref == (\"Test1Title\", \"1Test1\")\n\n    # Check specific cross reference\n    node = AliasNode(\"Test2\", root)\n    xref = CPPXRefRole()(None, node, None, node, None)\n    assert xref == (\"Test2\", \"1Test2\")\n\n    # Check specific cross reference with explicit title\n    node = AliasNode(\"Test2\", root)\n    xref = CPPXRefRole()(None, node, \"Test2Title\", node, None)\n    assert xref == (\"Test2Title\", \"1Test2\")\n\n    # Check :any: cross reference with parentheses\n    node = AliasNode(\"Test1()\", root)\n    assert CPPXRefRole()(None, node, None, node, None) == (\"Test1()\", \"1Test1\")\n\n    # Check :any: cross reference with parentheses and explicit title\n    node = AliasNode(\"Test1()\", root)\n    xref = CPPXRefRole()(None, node, \"Test1Title\", node, None)\n    assert xref == (\"Test1Title\", \"1Test1\")\n", "def test_merge_with():\n    root = Symbol(None, None, None, None, None, None)\n    symbol1 = root.add_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"A\"), None)], [False], rooted=False))\n    symbol2 = root.add_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"A\"), None)], [False], rooted=False))\n\n    docnames = [\"doc1\", \"doc2\"]\n    env = docnames\n\n    symbol1.merge_with(symbol2, docnames, env)\n", "def test_lookup_key():\n    # test the LookupKey class\n    key1 = Symbol.LookupKey([(ASTNestedNameElement(ASTIdentifier(\"a\"), None), None, None)])\n    key2 = Symbol.LookupKey([(ASTNestedNameElement(ASTIdentifier(\"a\"), None), None, None)])\n    assert key1 == key2\n    key3 = Symbol.LookupKey([(ASTNestedNameElement(ASTIdentifier(\"b\"), None), None, None)])\n    assert key1 != key3\n    assert hash(key1) == hash(key2)\n    assert hash(key1) != hash(key3)\n"], "sample_923": ["def test_macro_definitions():\n    check('macro', 'MY_MACRO(a, b)', {1: 'MY_MACRO'})\n    check('macro', 'MY_MACRO(int a, double b)', {1: 'MY_MACRO'})\n    check('macro', 'MY_MACRO(...)', {1: 'MY_MACRO'})\n    check('macro', 'MY_MACRO(int a, ...)', {1: 'MY_MACRO'})\n    check('macro', 'MY_MACRO(const char *name, int flags, ...)', {1: 'MY_MACRO'})\n    check('macro', 'MY_MACRO(void (*func)(int), double, ...)', {1: 'MY_MACRO'})\n    check('macro', 'MY_MACRO(void)', {1: 'MY_MACRO'})\n    check('macro', 'MY_MACRO()', {1: 'MY_MACRO'})\n", "def test_for_loop_variable_parsing():\n    check('function', 'void f(T for (int i = 0; i < 10; ++i))',\n          {1: 'f__T', 2: '1f1T'},\n          output='void f(T for(int i = 0; i < 10; ++i))')\n    check('function', 'void f(T for (int i : v))',\n          {1: 'f__T', 2: '1f1T'},\n          output='void f(T for(int i : v))')\n    check('function', 'void f(T for (; ; ))',\n          {1: 'f__T', 2: '1f1T'},\n          output='void f(T for(; ; ))')\n    check('function', 'void f(T for (auto i : v))',\n          {1: 'f__T', 2: '1f1T'},\n          output='void f(T for(auto i : v))')\n", "def test_namespace_definitions():\n    check('namespace', '{key}A', {1: \"A\", 2: \"1A\"})\n    check('namespace', '{key}A::B', {1: \"A::B\", 2: \"N1A1BE\"})\n    check('namespace', '{key}A::B::C', {1: \"A::B::C\", 2: \"N1A1B1CE\"})\n    check('namespace', '{key}::A', {1: \"A\", 2: \"1A\"})\n\n    check('namespace', '{key}A {}', {1: \"A\", 2: \"1A\"})\n    check('namespace', '{key}A::B {}', {1: \"A::B\", 2: \"N1A1BE\"})\n    check('namespace', '{key}A::B::C {}', {1: \"A::B::C\", 2: \"N1A1B1CE\"})\n    check('namespace', '{key}::A {}', {1: \"A\", 2: \"1A\"})\n\n    check('namespace', '{key}inline A {}', {1: \"A\", 2: \"1A\"})\n    check('namespace', '{key}inline A::B {}', {1: \"A::B\", 2: \"N1A1BE\"})\n    check('namespace', '{key}inline A::B::C {}', {1: \"A::B::C\", 2: \"N1A1B1CE\"})\n    check('namespace', '{key}inline ::A {}', {1: \"A\", 2: \"1A\"})\n\n    check('namespace', '{key}A::B::C {}', {1: \"A::B::C\", 2: \"N1A1B1CE\"})\n\n    check('namespace', '{key}A {}', {1: \"A\", 2: \"1A\"})\n    check('namespace', '{key}A::B {}', {1: \"A::B\", 2: \"N1A1BE\"})\n    check('namespace', '{key}A::B::C {}', {1: \"A::B::C\", 2: \"N1A1B1CE\"})\n    check('namespace', '{key}::A {}', {1: \"A\", 2", "def test_c_domain_initialization_without_docname(app, status, warning):\n    app.doctreedir = None\n    app.env.prepare_settings(app.config)\n    domain = app.env.get_domain('c')\n    assert domain.data['root_symbol']\n    assert domain.data['objects']\n", "def test_nested_declarator():\n    check('type', '{key}int (*A::B)::C::* const D::*', {2: 'PMN1A1B1CEKP1D'})\n    check('function', 'void f(int (*A::B)::C::* const D::* p)', {2: '1fPMN1A1B1CEKP1Di'})\n"], "sample_919": ["def test_type_modifiers():\n    # check simple type modifiers\n    check('member', 'const int a', {1: 'a__iC', 2: '1a'})\n    check('member', 'volatile int a', {1: 'a__iV', 2: '1a'})\n    check('member', 'const volatile int a', {1: 'a__iVC', 2: '1a'})\n    check('member', 'volatile const int a', {1: 'a__iVC', 2: '1a'})\n\n    # check pointer to member type modifiers\n    check('function', 'void f(int C::*)', {2: '1fM1Ci'})\n    check('function', 'void f(const int C::*)', {2: '1fKM1Ci'})\n    check('function', 'void f(volatile int C::*)', {2: '1fVM1Ci'})\n    check('function', 'void f(const volatile int C::*)', {2: '1fVKM1Ci'})\n    check('function', 'void f(volatile const int C::*)', {2: '1fVKM1Ci'})\n\n    # check that 'signed' and 'unsigned' are not modifier-like\n    check('type', '{key}signed int B', {1: \"B\", 2: \"1B\"}, key='typedef')\n    check('type', '{key}unsigned int B', {1: \"B\", 2: \"1B\"}, key='typedef')\n", "def test_disabled_fallback_expression_parser():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n\n    class TestParser(DefinitionParser):\n            super().__init__(string, location=None, config=Config())\n            self.allowFallbackExpressionParsing = False\n\n    parser = TestParser('void f(arg_t arg = fn())')\n    with pytest.raises(DefinitionError):\n        parser.parse_declaration(\"function\", \"function\")\n", "def test_expression_parsing_fallback():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser('1 + 2', location=None, config=Config())\n    try:\n        parser._parse_expression_fallback([')'], None)\n    except DefinitionError as e:\n        assert False\n\n    parser = DefinitionParser('A + B', location=None, config=Config())\n    try:\n        parser._parse_expression_fallback([')'], None)\n    except DefinitionError as e:\n        assert False\n\n    parser = DefinitionParser('A + B', location=None, config=Config())\n    parser.allowFallbackExpressionParsing = False\n    with pytest.raises(DefinitionError):\n        parser._parse_expression_fallback([')'], None)\n", "def test_xref_with_multiplicity():\n    check('function', 'void f(std::pair<std::string, long long int> coord)',\n          {1: \"f__std::pair:ss.l:\", 2: \"1fNSt4pairI3std6stringEliEE\"})\n\n    check('function', \"void f(std::vector<std::pair<std::string, long long>> module::blah)\",\n          {1: \"f__std::vector:std::pair:ss.l::C\", 2: \"1fNSt6vectorISt4pairIKNSt6stringElEEE\"})\n", "def test_string_literals():\n    # Test all escaped characters in string literals.\n    check('function', r'void f(const char *s = \"\\a\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\b\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\f\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\n\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\r\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\t\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\v\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\\\\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\'\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\\"\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\?\")', {2: '1fPKc'})\n\n    # Test octal escaped characters.\n    check('function', r'void f(const char *s = \"\\1\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\12\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\123\")', {2: '1fPKc'})\n\n    # Test hexadecimal escaped characters.\n    check('function', r'void f(const char *s = \"\\x1\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\x12\")', {2: '1fPKc'})\n    check('function', r'void f(const char *s = \"\\x123\")', {2: '1fPKc'})\n\n    # Test invalid escaped characters.\n    with pytest.raises(DefinitionError):\n        parse('function"], "sample_924": ["def test_scope_resolution():\n    check('class', '{key}A::B', {1: \"A::B\", 2: \"N1A1BE\"})\n    check('function', 'void A::B::f()', {1: \"A::B::f\", 2: \"N1A1B1fEv\"})\n    check('function', 'void A::B::C::f()', {1: \"A::B::C::f\", 2: \"N1A1B1C1fEv\"})\n    check('member', 'int A::B::C::m', {1: \"A::B::C::m\", 2: \"N1A1B1C1mE\"})\n    check('type', 'typedef A::B::C::T t', {1: \"A::B::C::T\", 2: \"N1A1B1C1TE\"}, key='typedef')\n    check('concept', '{key}A::B::C::Concept', {2: 'N1A1B1C7ConceptE'})\n    check('enum', '{key}A::B::C::E', {2: \"N1A1B1C1E\"})\n    check('enumerator', '{key}A::B::C::E::e', {2: \"N1A1B1C1E1eE\"})\n", "def test_anon_in_concept_definitions():\n    check('concept', '{key}A = requires (int a) { 1 + a == sizeof...(@b) }',\n          {2: 'I0E1A'})\n    check('concept', '{key}A = requires (int a) { 1 + a == sizeof...(@b) };',\n          {2: 'I0E1A'})\n    check('concept', '{key}A = requires (int a) { 1 + a == sizeof...(@b) };',\n          {2: 'I0E1A'})\n", "def test_attribute_parsing():\n        parser = DefinitionParser(attr, location=None, config=None)\n        ast = parser.parse_attribute()\n        parser.assert_end()\n        assert ast.name == name\n        assert ast.args == args\n\n    check('[[nodiscard]]', \"nodiscard\", None)\n    check('[[gnu::unused]]', \"gnu::unused\", None)\n    check('[[using gnu: unused]]', \"gnu::unused\", None)\n    check('[[using gnu : unused]]', \"gnu::unused\", None)\n    check('[[using gnu : unused, xyz]]', \"gnu::unused, xyz\", None)\n    check('[[vendor::attr(1 + 2)]]', \"vendor::attr\", \"1 + 2\")\n    check('[[vendor::attr()]]', \"vendor::attr\", \"\")\n    check('[[vendor::attr]]', \"vendor::attr\", None)\n\n        parser = DefinitionParser(attr, location=None, config=None)\n        with pytest.raises(DefinitionError):\n            parser.parse_attribute()\n\n    check_error(\"[[\")\n    check_error(\"[[ ]]\")\n    check_error(\"[[]\")\n    check_error(\"[[]]\")\n    check_error(\"[[][\")\n", "def test_template_parameter_pack():\n    check('function', 'template<typename... Ts> void f(Ts... args)',\n          {1: None, 2: 'IDpE1fv'})\n    check('function', 'template<typename... Ts> void f(Ts... args, int i)',\n          {1: None, 2: 'IDpE1fv'})\n    check('function', 'template<typename... Ts> void f(int i, Ts... args)',\n          {1: None, 2: 'IDpE1fv'})\n    check('function', 'template<typename... Ts> void f(int i, Ts... args, Ts...)',\n          {1: None, 2: 'IDpE1fv'})\n    check('function', 'template<typename... Ts, typename U> void f(Ts... args, U u)',\n          {1: None, 2: 'IDpE1fv'})\n    check('function', 'template<typename... Ts, typename U, typename... Vs> '\n          'void f(Ts... args, U u, Vs... vs)',\n          {1: None, 2: 'IDpE1fv'})\n", "def test_template_parameter_packs():\n    check('function', 'template<typename... Ts> void f(Ts... ts)',\n          {2: 'IDpE3fJ1TsEEE'})\n    check('function', 'template<typename... Ts> void f(Ts...) = delete',\n          {2: 'IDpE3fDp'})\n    check('function', 'template<typename... Ts> void f(Ts..., int) = delete',\n          {2: 'IDpE3fJ1TsEEEi'})\n    check('function', 'template<typename... Ts> void f(int, Ts...) = delete',\n          {2: 'IDpE3fiDp'})\n    check('function', 'template<typename... Ts> void f(int, Ts..., int) = delete',\n          {2: 'IDpE3fiJ1TsEEEi'})\n\n    check('function', 'template<int... N> void f()',\n          {2: 'I_DpiE1fv'})\n    check('function', 'template<int... N> void f(int, N...) = delete',\n          {2: 'I_DpiE3fiDp'})\n    check('function', 'template<int... N> void f(N..., int) = delete',\n          {2: 'I_DpiE3fJiEEEi'})\n    check('function', 'template<int... N> void f(int, N..., int) = delete',\n          {2: 'I_DpiE3fiJiEEEi'})\n\n    check('function', 'template<typename... Ts> void f(Ts (&... arrays)[5])',\n          {2: 'IDpE3fRA5_J1TsEEE'})\n    check('function', 'template<typename... Ts> void f(Ts (*... arrays)[5])',\n          {2: 'IDpE3fPA5_J1TsEEE'})\n    check('function', 'template<typename... Ts> void f(Ts (C::... arrays)[5])',\n          {2: 'IDpE3fM5_J1TsEEE1C'})\n    check('function', 'template<typename... Ts> void f(Ts (C::*... arrays)[5])',\n          {2: 'IDpE3fM5_J1TsEEE1C'})\n\n    check('function', 'template<typename T, typename U> void f(T, U)',\n          {2: 'I00E3f1"], "sample_927": ["def test_debug_flags(app):\n    app.builder.build_all()\n", "def test_xrefs(api):\n    with pytest.raises(DefinitionError):\n        api.parse_xref(\"void f(std::string)\")\n", "def test_build_domain_cpp_index_common_prefix(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"index\")\n    assert len(ws) == 0\n", "def test_xref_consistency(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_consistency.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_template_parameter_pack_id_v2():\n    check('type', '{key}T = A<1, 2, Ts...>', {2: '1T'}, key='typedef')\n    check('type', '{key}T = A<Ts...>', {2: '1T'}, key='typedef')\n    check('type', '{key}T = A<1, 2, Ts...> const', {2: '1T'}, key='typedef')\n"], "sample_926": ["def test_fallback_expression_parsing():\n    # Explicitly enabled fallback expression parsing\n    check('function', 'void f(A<1 + 2 + 3>)',\n          {2: \"IE1fI1AILi3EE\"}, output='void f(A<1+2+3>)')\n    check('function', 'void f(A<(1 + 2) + 3>)',\n          {2: \"IE1fI1AILi3EE\"}, output='void f(A<(1+2)+3>)')\n    check('function', 'void f(A<(1 + 2) + (3 + 4 + 5)>)',\n          {2: \"IE1fI1AILi21EE\"}, output='void f(A<(1+2)+(3+4+5)>)')\n    check('function', 'void f(A<(1 + 2) * (3 + 4)>)',\n          {2: \"IE1fI1AIli6EE\"}, output='void f(A<(1+2)*(3+4)>)')\n    check('function', 'void f(A<(1 + 2) * (3 + 4), 42>)',\n          {2: \"IE1fI1AIli6EL42EE\"}, output='void f(A<(1+2)*(3+4),42>)')\n", "def test_issue_2079_cpp_template_without_type():\n    check('type', '{key}A = B', {2: '1A'}, key='typedef')\n    check('type', '{key}A = B<>', {2: '1A'}, key='typedef')\n    check('type', '{key}A = B<int>', {2: '1A'}, key='typedef')\n    check('type', '{key}A = B<int, double>', {2: '1A'}, key='typedef')\n\n    check('type', '{key}A = B', {2: '1A'}, key='using')\n    check('type', '{key}A = B<>{}', {2: '1A'}, key='using')\n    check('type', '{key}A = B<int>', {2: '1A'}, key='using')\n    check('type', '{key}A = B<int, double>', {2: '1A'}, key='using')\n\n    check('type', '{key}A = decltype(nullptr_t)', {2: '1A'}, key='typedef')\n    check('type', '{key}A = decltype(nullptr_t)', {2: '1A'}, key='using')\n", "def test_build_domain_cpp_with_invalid_syntax(app, status, warning):\n    app.builder.build_all()\n\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    f = 'invalid_syntax.html'\n    t = (app.outdir / f).read_text()\n    check(r'WARNING: Block quote ends without a blank line', t, f)\n    check(r'WARNING: Explicit markup ends without a blank line', t, f)\n    check(r'WARNING: Inline emphasis start-string without end-string', t, f)\n    check(r'WARNING: Inline strong start-string without end-string', t, f)\n    check(r'WARNING: Inline interpreted text or phrase reference start-string without end-string', t, f)\n    check(r'WARNING: Inline literal start-string without end-string', t, f)\n    check(r'WARNING: Inline substitution_reference start-string without end-string', t, f)\n    check(r'WARNING: Definition list ends without a blank line', t, f)\n    check(r'WARNING: Field list ends without a blank line', t, f)\n    check(r'WARNING: Option list ends without a blank line', t, f)\n", "def test_build_domain_c(app, status, warning):\n    app.builder.build_all()\n    assert_node(app.env.domains['c'].data['root_symbol'].asList(),\n                [desc('CDomainData', children=[\n                    desc('CRootSymbol', children=[\n                        desc('CSymbol', attrs={'ident': 'myFunction'},\n                            children=[\n                                desc('CSymbol', attrs={'ident': 'myFunctionParam'})\n                            ]),\n                        desc('CSymbol', attrs={'ident': 'MyStruct'})\n                    ])\n                ])])\n", "def test_argument_parsing():\n    # paren and no-paren expressions\n    check(\"function\", \"void f((int) a)\", {2: \"1fiE\"})\n    check(\"function\", \"void f()((int) a)\", {2: \"1fvPiE\"})\n    check(\"function\", \"void f((void (*)())(int) a)\", {2: \"1fPFvvEIiE\"})\n\n    # pointer to member (function)\n    check(\"function\", \"void f(int C::* p)\", {2: \"1fM1Ci\"})\n    check(\"function\", \"void f(int C::*const p)\", {2: \"1fKM1Ci\"})\n    check(\"function\", \"void f(int C::*volatile p)\", {2: \"1fVM1Ci\"})\n    check(\"function\", \"void f(int C::*const volatile p)\", {2: \"1fVKM1Ci\"})\n\n    # from issue #3517\n    check(\"function\", \"void f(std::function<void(int, double)>)\", {2: \"1fNSt8functionIFvidEE\"})\n"], "sample_934": ["def test_lookup_key(app, status, warning):\n    app.builder.build_all()\n\n    rootSymbol = app.env.domains['c'].data['root_symbol']\n    symbol = rootSymbol.add_name(ASTNestedName(['TestScope'], rooted=False))\n    lookupKey = symbol.get_lookup_key()\n    assert lookupKey == LookupKey([(ASTIdentifier('TestScope'), None)])\n", "def test_template_introductions():\n    # template introductions are not supported for these\n    for name in ['enum', 'enumerator']:\n        with pytest.raises(DefinitionError):\n            parse(name, '{key}A<int>')\n\n    check('class', 'A<int>', {2: \"I1AIiE1A\"})\n    check('concept', 'C<int>', {2: \"I1CIiE7Concept\"})\n    check('function', 'void f<int>()', {2: \"I1fIiEv\"})\n    check('member', 'int A<int>::a', {2: \"I1AIiE1aE\"})\n    check('type', 'using T = A<int>', {2: \"I1AIiE1T\"}, key='using')\n\n    # check that template introductions are not applied to names that could be mistaken for template names\n    check('function', 'void f<int>()', {2: \"I1fIiEv\"})\n    check('function', 'void operator f<int>()', {2: \"I1flIiEv\"})\n    check('function', 'void operator int f<int>()', {2: \"I1flIiEv\"})\n\n    # check that the c++20 syntax for template template parameters is supported\n    check('class', 'A<Template...>', {2: \"IDpE1A\"})\n    check('class', 'A<Template... Ts>', {2: \"IDpE1A\"})\n    check('class', 'A<Template... Ts>', {2: \"IDpE1A\"})\n    # from #4055\n    check('class', 'A<Template...Ts>', {2: \"IDpE1A\"})  # note the missing space\n    check('class', 'A<Template ... Ts>', {2: \"IDpE1A\"})  # note the extra space\n    check('class', 'A<Template<T...>>', {2: \"II0EDpE1A\"})\n\n    # check that the c++20 syntax for template template parameter packs is supported\n    check('class', 'A<Template... Ts>', {2: \"IDpE1A\"})\n\n    # check that the c++20 syntax for template template parameter argument lists is supported\n    check('class', 'A<Template<T, U>>', {2: \"II0EI1T1UE", "def test_lookup_key(app, status, warning):\n    app.builder.build_all()\n    root = app.env.domains['c'].data['root_symbol']\n    assert root.get_lookup_key() is not None\n\n    # Test with single qualified name\n    symbol = root.add_name(ASTNestedName([ASTIdentifier(\"Test\")], rooted=False))\n    lookup_key = symbol.get_lookup_key()\n    assert len(lookup_key.data) == 1\n\n    # Test with multiple qualified names\n    symbol = root.add_name(ASTNestedName([ASTIdentifier(\"Test\"), ASTIdentifier(\"Nested\")], rooted=False))\n    lookup_key = symbol.get_lookup_key()\n    assert len(lookup_key.data) == 2\n\n    # Test with anonymous identifier\n    symbol = root.add_name(ASTNestedName([ASTIdentifier(\"@anonymous\")], rooted=False))\n    lookup_key = symbol.get_lookup_key()\n    assert len(lookup_key.data) == 1\n\n    # Test with declaration\n    declaration = ASTDeclaration('function', 'function', None, False)\n    symbol = root.add_declaration(declaration, docname=\"test\")\n    lookup_key = symbol.get_lookup_key()\n    assert len(lookup_key.data) == 1\n", "compilation error", "def test_lookup_key():\n    rootSymbol = Symbol(None, None, None, None)\n    s1 = rootSymbol.add_name(ASTNestedName([ASTIdentifier(\"A\")], rooted=False))\n    s2 = s1.add_name(ASTNestedName([ASTIdentifier(\"B\")], rooted=False))\n    s3 = s2.add_name(ASTNestedName([ASTIdentifier(\"C\")], rooted=False))\n    s4 = s3.add_name(ASTNestedName([ASTIdentifier(\"D\")], rooted=False))\n\n    assert str(s1.get_lookup_key()) == \"[((A, None),)]\"\n    assert str(s2.get_lookup_key()) == \"[((A, None),), ((B, None),)]\"\n    assert str(s3.get_lookup_key()) == \"[((A, None),), ((B, None),), ((C, None),)]\"\n    assert str(s4.get_lookup_key()) == \"[((A, None),), ((B, None),), ((C, None),), ((D, None),)]\"\n\n    s5 = s1.add_name(ASTNestedName([ASTIdentifier(\"B\")], rooted=False))\n    d = ASTDeclaration(\"function\", \"function\", ASTFunctionDeclarator(), None)\n    s5.declaration = d\n    assert str(s5.get_lookup_key()) == \"[((A, None),), ((B, 'function'),)]\"\n"], "sample_938": ["def test_custom_man_page(app, status, warning):\n    app.build()\n    assert (app.outdir / 'test_name.2').exists()\n\n    content = (app.outdir / 'test_name.2').read_text()\n    assert '.TH test_name 2' in content\n    assert '.SH NAME\\n\\n\\fBtest_name\\fP \\\\- Test Description\\n' in content\n", "def test_custom_man_pages(app, status, warning):\n    app.build()\n    assert (app.outdir / 'sphinx.2').exists()\n    content = (app.outdir / 'sphinx.2').read_text()\n    assert '.TH SPHINX 2' in content\n", "def test_man_pages_config(app, status, warning):\n    app.build()\n    assert (app.outdir / 'stasi.1').exists()\n\n    content = (app.outdir / 'stasi.1').read_text()\n    assert '.TH STASI 1 \"1.0\"' in content\n    assert '.SH NAME\\nstasi \\- STASI Documentation 1.0' in content\n", "def test_man_pages_config(app, status, warning):\n    app.build()\n    assert (app.outdir / 'test.1').exists()\n\n    content = (app.outdir / 'test.1').read_text()\n    assert r'.TH \"TEST\" \"1\" ' in content\n    assert r'.SH \"NAME\"\\n\\fITest manual page\\fP\\n' in content\n    assert r'.SH \"AUTHOR\"\\n\\fIAuthor1\\fP, \\fIAuthor2\\fP\\n' in content\n", "def test_man_pages_config(app, status, warning):\n    app.build()\n    assert (app.outdir / 'python.1').exists()\n    content = (app.outdir / 'python.1').read_text()\n    assert '.TH python 1' in content\n    assert '.SH NAME' in content\n    assert 'python \\\\- Python Documentation' in content\n"], "sample_932": ["def test_build_domain_cpp_index_common_prefix(app, status, warning):\n    app.builder.build_all()\n    indexfile = app.outdir / 'genindex.html'\n    with indexfile.open('r') as f:\n        indexcontent = f.read()\n    assert 'std::vector' not in indexcontent\n    assert 'vector' in indexcontent\n", "def test_lookup_key(app, status, warning):\n    # Test the lookup key\n    class DomainData:\n        root_symbol = None\n\n    class Env:\n        temp_data = {}\n        ref_context = {}\n        domaindata = {'cpp': DomainData()}\n\n    env = Env()\n    domain = cppDomain.CPPDomain(env)\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    env.domaindata['cpp'].root_symbol = rootSymbol\n    env.temp_data['cpp:parent_symbol'] = rootSymbol\n    env.ref_context['cpp:parent_key'] = rootSymbol.get_lookup_key()\n\n    # Create an empty symbol for testing\n    symbol1 = rootSymbol.add_name(\"Symbol1\", None)\n\n    # Create a second symbol for testing\n    symbol2 = rootSymbol.add_name(\"Symbol2\", None)\n\n    # Test the lookup key\n    lookup_result = rootSymbol._symbol_lookup(\n        symbol2.name, [], lambda x, y, z, w: None, False, None, False, False, False, False)\n\n    assert lookup_result is not None\n    assert lookup_result.symbols is not None\n\n    # Test the duplicate declaration\n    with pytest.raises(_DuplicateSymbolError) as excinfo:\n        rootSymbol.add_declaration(symbol2.declaration, \"TestDoc\")\n    assert excinfo.value.symbol == symbol2\n\n    # Test the lookup key again\n    lookup_result = rootSymbol._symbol_lookup(\n        symbol2.name, [], lambda x, y, z, w: None, False, None, False, False, False, False)\n    assert lookup_result is not None\n    assert lookup_result.symbols is not None\n\n    # Test the merge_with function\n    symbol2.merge_with(symbol1, [\"TestDoc\"], env)\n\n    # Test the lookup key again\n    lookup_result = rootSymbol._symbol_lookup(\n        symbol2.name, [], lambda x, y, z, w: None, False, None, False, False, False, False)\n    assert lookup_result is not None\n    assert lookup_result.symbols is not None\n", "def test_build_domain_cpp_index_common_prefix(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"index-common-prefix\")\n    assert len(ws) == 0\n", "def test_template_with_requires_clause():\n    check('function', 'template<typename T> requires A<T> void f()',\n          {4: 'I0EIQa1ATEv'})\n    check('function', 'template<typename T> requires A<T> && B<T> void f()',\n          {4: 'I0EIQa1ATRKSBTEXv'})\n    check('function', 'template<typename T> requires (A<T> && B<T>) void f()',\n          {4: 'I0EIQaDp1ATRKSBTEXv'})\n    check('function', 'template<typename T> requires (A<T> && ... && B<T>) void f()',\n          {4: 'I0EIQaDpa1ATRKSBTEXv'})\n", "def test_template_introduction():\n    check('class', 'abc::ns::foo{{id_0, id_1, id_2}} {key}xyz::bar',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEN3xyz3barE'})\n    check('class', 'template<abc::ns::foo{{id_0, id_1, id_2}}> {key}xyz::bar<T>',\n          {2: 'I000EXN3abc2ns3fooEI4id_04id_14id_2EEE1TEN3xyz3bar'})\n"], "sample_935": ["def test_add_function_parentheses_for_non_shorthand_function_ref(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n    rolePatterns = [\n        ('', 'Sphinx'),\n        ('', 'Sphinx::version'),\n        ('', 'version'),\n        ('', 'List'),\n        ('', 'MyEnum')\n    ]\n    parenPatterns = [\n        ('ref function without parens ', 'paren_1()'),\n        ('ref function with parens ', 'paren_2()'),\n        ('ref function without parens, explicit title ', 'paren_3_title'),\n        ('ref function with parens, explicit title ', 'paren_4_title'),\n    ]\n\n    f = 'add-function-parentheses.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n    for s in parenPatterns:\n        check(s, t, f)\n    # Test that a function without () does not get parentheses\n    assert 'paren_5()' not in t\n", "def test_resolve_xref(app, status, warning):\n    app.builder.build_all()\n\n        node = app.env.get_domain('cpp').resolve_xref(\n            app.env, 'cpp', app.builder, 'type', name, node_type,\n            contnode=docutils.nodes(emphasis(text=name)))\n        assert_node(contnode, [node_type], refdomain='cpp', reftype='type', reftarget=name, refwarn=False)\n\n    check('UnqualifiedType', addnodes PendingXRef)\n    check('UnqualifiedType<int>', addnodes PendingXRef)\n    check('UnqualifiedTemplate<int>', addnodes PendingXRef)\n    check('QualifiedType', addnodes PendingXRef)\n    check('QualifiedType<int>', addnodes PendingXRef)\n    check('QualifiedTemplate<int>', addnodes PendingXRef)\n    check('std::vector', addnodes PendingXRef)\n    check('std::vector<int>', addnodes PendingXRef)\n    check('std::basic_istream', addnodes PendingXRef)\n    check('std::basic_istream<char>', addnodes PendingXRef)\n    check('NonExistentType', addnodes literal)\n    check('NonExistentType<int>', addnodes literal)\n    check('NonExistentTemplate<int>', addnodes literal)\n    check('NonExistentQualifiedType', addnodes literal)\n    check('NonExistentQualifiedType<int>', addnodes literal)\n    check('NonExistentQualifiedTemplate<int>', addnodes literal)\n    check('std::non_existent_type', addnodes literal)\n    check('std::non_existent_type<int>', addnodes literal)\n    check('std::non_existent_template<int>', addnodes literal)\n", "def test_lookup_key(app, status, warning):\n    app.builder.build_all()\n\n        parser = DefinitionParser(signature, location=None, config=None)\n        try:\n            ast, isShorthand = parser.parse_xref_object()\n            parser.assert_end()\n        except DefinitionError:\n            print(\"parse error in signature: %s\" % signature)\n            assert False\n        return root.direct_lookup(ast.get_lookup_key())\n\n    root = app.env.domains['cpp'].data['root_symbol']\n    assert find_symbol(root, \"A::B::C\") is not None\n\n    nested = find_symbol(root, \"A\")\n    assert nested is not None\n    assert find_symbol(nested, \"B::C\") is not None\n", "def test_build_domain_cpp_id_attributes(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"id-attributes\")\n    assert len(ws) == 0\n", "def test_build_domain_cpp_template_shorthand(app, status, warning):\n    app.builder.build_all()\n\n        text = (app.outdir / file).read_text()\n        if pattern not in text:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n\n    f = 'template-shorthand.html'\n    check(r'\\bXXX\\b', f)\n    check(r'\\bYYY\\b', f)\n    check(r'\\bZZZ\\b', f)\n    check(r'\\bUUU\\b', f)\n    check(r'\\bVVV\\b', f)\n    check(r'\\bWWW\\b', f)\n"], "sample_940": ["def test_is_singledispatch_function():\n    from functools import singledispatch\n\n    @singledispatch\n        return arg\n\n    assert inspect.is_singledispatch_function(fun) is True\n\n        return arg\n\n    assert inspect.is_singledispatch_function(not_singledispatch) is False\n\n", "def test_safemembers():\n    class Foo:\n            self.a = 1\n            self._b = 2\n            self.__c = 3\n\n        @property\n            return 4\n\n        @property\n            return 5\n\n        @property\n            return 6\n\n            pass\n\n    foo = Foo()\n\n    # Test attribute filtering by predicate\n    members = inspect.safe_getmembers(foo, lambda x: not x.startswith(\"_\"))\n    expected_members = [(\"a\", 1), (\"d\", 4), (\"method\", foo.method)]\n    assert sorted(members) == sorted(expected_members)\n\n    # Test attribute filtering by name\n    members = inspect.safe_getmembers(foo, lambda x: x.startswith(\"_\") and not x.startswith(\"__\"))\n    expected_members = [(\"_b\", 2), (\"_e\", 5)]\n    assert sorted(members) == sorted(expected_members)\n\n    # Test attribute filtering with no predicate\n    members = inspect.safe_getmembers(foo, None)\n    assert any(m[0] == \"a\" for m in members)\n    assert any(m[0] == \"d\" for m in members)\n    assert any(m[0] == \"method\" for m in members)\n    assert any(m[0] == \"_b\" for m in members)\n    assert any(m[0] == \"__c\" for m in members)\n    assert any(m[0] == \"__f\" for m in members)\n", "def test_is_builtin_class_method_with_custom_class():\n    class MyClass:\n        pass\n\n    assert is_builtin_class_method(MyClass, '__init__') is False\n    assert is_builtin_class_method(MyClass, '__new__') is False\n", "def test_getfullargspec():\n        pass\n\n    fullargspec = inspect.getargspec(func)\n    assert fullargspec.args == ['a', 'b', 'c', 'd']\n    assert fullargspec.varargs == 'args'\n    assert fullargspec.varkw == 'kwargs'\n    assert fullargspec.defaults == (None,)\n    assert fullargspec.kwonlyargs == []\n    assert fullargspec.kwonlydefaults == {}\n    assert fullargspec.annotations == {}\n", "def test_is_builtin_class_method_with_builtin_method():\n    assert is_builtin_class_method(int, '__init__') is True\n    assert is_builtin_class_method(int, '__new__') is True\n    assert is_builtin_class_method(int, '__add__') is True\n    assert is_builtin_class_method(int, '__abs__') is True\n"], "sample_941": ["def test_stringify_broken_type_hints():\n    assert stringify(BrokenType) == 'tests.test_util_typing.BrokenType'\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_stringify_type_hints_broken_type():\n    class BrokenType:\n        __args__ = int\n\n    assert stringify(BrokenType) == 'tests.test_util_typing.BrokenType'\n", "def test_restify_type_hints_Ellipsis():\n    assert restify(Ellipsis) == '...'\n    assert restify(type(Ellipsis)) == ':class:`ellipsis`'\n", "def test_stringify_type_hints_Ellipsis():\n    assert stringify(Ellipsis) == \"...\"\n"], "sample_939": ["def test_unparse_visit_arg():\n    source = \"def func(arg1: int, arg2: str = 'default') -> None: pass\"\n    module = ast.parse(source)\n    function_def = module.body[0]\n    arg1 = function_def.args.args[0]\n    arg2 = function_def.args.args[1]\n\n    assert ast.unparse(arg1) == \"arg1: int\"\n    assert ast.unparse(arg2) == \"arg2: str = 'default'\"\n", "def test_unparse_complex_cases():\n    # unparse a complex expression\n    source = \"(a + b) * (c - d) / (e ** f)\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == source\n\n    # unparse a function call with arguments\n    source = \"func(a, b, c=True, d=False)\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == source\n\n    # unparse a dictionary with different types of keys and values\n    source = \"{'a': 1, 2: 'b', 3.4: True, None: False}\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == source\n\n    # unparse a set with different types of elements\n    source = \"{1, 'a', 2.3, True, None}\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == source\n", "def test_unparse_visit_arguments():\n    source = \"def func(a, b: int, c=1, d: int=2, *args, f, g: int, h=3, i: int=4, **kwargs): pass\"\n    module = ast.parse(source)\n    expected = \"a, b: int, c=1, d: int=2, *args, f, g: int, h=3, i: int=4, **kwargs\"\n    assert ast.unparse(module.body[0].args, source) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == expected\n", "def test_unparse_arg():\n    source = \"(a: int, b: str, *c, d: float = 1.0, **e)\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == source\n"], "sample_944": ["def test_restify_type_hints_broken_class():\n    class BrokenClass:\n        __module__ = 'builtins'\n\n    assert restify(BrokenClass) == ':class:`BrokenClass`'\n", "def test_get_type_hints():\n        pass\n\n    hints = get_type_hints(func)\n    assert hints == {'a': int, 'b': str, 'return': float}\n\n    class MyClass:\n            pass\n\n    hints = get_type_hints(MyClass.method)\n    assert hints == {'a': int, 'b': str, 'return': float}\n\n    # Test with a broken class\n    class BrokenClass:\n        __annotations__ = 'broken'\n\n    hints = get_type_hints(BrokenClass)\n    assert hints == {}\n\n    # Test with a class that has no __annotations__\n    class NoAnnotationsClass:\n        pass\n\n    hints = get_type_hints(NoAnnotationsClass)\n    assert hints == {}\n", "def test_stringify_type_hints_ForwardRef():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"myint\")) == \"myint\"\n    else:\n        from sphinx.util.typing import ForwardRef\n        assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_restify_broken_class_found():\n    class BrokenClass:\n        __annotations__ = {'foo': 'bar'}\n\n    with pytest.warns(UserWarning):\n        assert restify(BrokenClass) == ':class:`tests.test_util_typing.BrokenClass`'\n", "def test_restify_broken_class():\n    class BrokenClass:\n        __module__ = 'builtins'\n\n    assert restify(BrokenClass) == ':class:`builtins.BrokenClass`'\n"], "sample_943": ["def test_package_file_maxdepth(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'testpkg').makedirs()\n    (outdir / 'testpkg' / '__init__.py').write_text('')\n    (outdir / 'testpkg' / 'example.py').write_text('')\n    (outdir / 'testpkg' / 'subpkg').makedirs()\n    (outdir / 'testpkg' / 'subpkg' / '__init__.py').write_text('')\n    (outdir / 'testpkg' / 'subpkg' / 'subsubpkg').makedirs()\n    (outdir / 'testpkg' / 'subpkg' / 'subsubpkg' / '__init__.py').write_text('')\n    apidoc_main(['--maxdepth', '2', '-o', tempdir, tempdir / 'testpkg'])\n    assert (outdir / 'testpkg.rst').exists()\n    assert (outdir / 'testpkg.example.rst').exists()\n    assert (outdir / 'testpkg.subpkg.rst').exists()\n\n    content = (outdir / 'testpkg.rst').read_text()\n    assert content == (\"testpkg package\\n\"\n                       \"===============\\n\"\n                       \"\\n\"\n                       \"Subpackages\\n\"\n                       \"-----------\\n\"\n                       \"\\n\"\n                       \".. toctree::\\n\"\n                       \"   :maxdepth: 2\\n\"\n                       \"\\n\"\n                       \"   testpkg.subpkg\\n\"\n                       \"\\n\"\n                       \"Submodules\\n\"\n                       \"----------\\n\"\n                       \"\\n\"\n                       \"testpkg.example module\\n\"\n                       \"----------------------\\n\"\n                       \"\\n\"\n                       \".. automodule:: testpkg.example\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n\"\n                       \"\\n\"\n                       \"Module contents\\n\"\n                       \"---------------\\n\"\n                       \"\\n\"\n                       \".. automodule:: testpkg\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n\")\n", "def test_is_excluded(tempdir):\n    excludes = [str(tempdir / 'exclude1'), str(tempdir / 'exclude2' / '**')]\n    assert is_excluded(str(tempdir / 'exclude1'), excludes)\n    assert is_excluded(str(tempdir / 'exclude2' / 'subdir'), excludes)\n    assert not is_excluded(str(tempdir / 'include'), excludes)\n", "def test_module_file_with_header(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'example.py').write_text('')\n    apidoc_main(['--header', 'Example Header', '-o', tempdir, tempdir])\n    assert (outdir / 'example.rst').exists()\n\n    content = (outdir / 'example.rst').read_text()\n    assert \"Example Header\" in content\n", "def test_create_modules_toc_file(tempdir):\n    outdir = path(tempdir)\n    modules = [\"module1\", \"module2\", \"module3\"]\n    opts = namedtuple(\"opts\", \"header maxdepth tocfile suffix dryrun\")(None, 4, \"modules\", \"rst\", False)\n    create_modules_toc_file(modules, opts, user_template_dir=None)\n\n    assert (outdir / 'modules.rst').exists()\n\n    content = (outdir / 'modules.rst').read_text()\n    assert content == (\"Modules\\n\"\n                       \"=======\\n\"\n                       \"\\n\"\n                       \".. toctree::\\n\"\n                       \"   :maxdepth: 4\\n\"\n                       \"\\n\"\n                       \"   module1\\n\"\n                       \"   module2\\n\"\n                       \"   module3\\n\")\n", "def test_is_excluded(tempdir):\n    assert not is_excluded(str(tempdir / 'test.py'), [])\n    assert not is_excluded(str(tempdir / 'test.py'), [str(tempdir / 'other.py')])\n    assert is_excluded(str(tempdir / 'test.py'), [str(tempdir / 'test.py')])\n    assert is_excluded(str(tempdir / 'test.py'), [str(tempdir / '*.py')])\n    assert not is_excluded(str(tempdir / 'test.txt'), [str(tempdir / '*.py')])\n"], "sample_942": ["def test_pairindextypes(app):\n    assert pairindextypes['module'] == _('module')\n    assert pairindextypes['keyword'] == _('keyword')\n    assert pairindextypes['operator'] == _('operator')\n    assert pairindextypes['object'] == _('object')\n    assert pairindextypes['exception'] == _('exception')\n    assert pairindextypes['statement'] == _('statement')\n    assert pairindextypes['builtin'] == _('built-in function')\n", "def test_none_type(app):\n    text = (\".. py:function:: hello() -> None\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                  desc_content)]))\n    assert_node(doctree[1][0][2][0], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n", "def test_get_full_qualified_name_with_module_and_class():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n    node = nodes.reference(reftarget='func', py__module='module1', py__class='Class')\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n", "def test_type_to_xref(app):\n    env = app.env\n    env.ref_context['py:module'] = 'module_a'\n    env.ref_context['py:class'] = 'Class'\n\n    # simple type\n    node = type_to_xref('int', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='int',\n                **{'py:module': 'module_a', 'py:class': 'Class'})\n\n    # dotted type\n    node = type_to_xref('typing.Union', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='class', reftarget='typing.Union',\n                **{'py:module': 'module_a', 'py:class': 'Class'})\n\n    # None type\n    node = type_to_xref('None', env)\n    assert_node(node, pending_xref, refdomain='py', reftype='obj', reftarget='None',\n                **{'py:module': 'module_a', 'py:class': 'Class'})\n", "def test_py_type_constructor_with_tuple(app, status, warning):\n    text = (\".. py:function:: func()\\n\"\n            \"   :return: Tuple[int, str]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_returns, pending_xref, \"Tuple\",\n                 [desc_sig_punctuation, \"[\"],\n                 [pending_xref, \"int\"],\n                 [desc_sig_punctuation, \", \"],\n                 [pending_xref, \"str\"],\n                 [desc_sig_punctuation, \"]\"]])\n"], "sample_945": ["def test_info_field_list_return(app):\n    text = (\".. py:function:: func\\n\"\n            \"\\n\"\n            \"   :return: blah blah\\n\"\n            \"   :rtype: Tuple[str, int]\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, (desc_signature,\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Returns\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :return: + :rtype:\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"Returns\"],\n                 \" -- \",\n                 \"blah blah\",\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"Tuple\"],\n                 [addnodes.literal_emphasis, \"[\"],\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 [addnodes.literal_emphasis, \", \"],\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 [addnodes.literal_emphasis, \"]\"],\n                 \")\"))\n    assert_node(doctree[1][1][0][0][1][0][4], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"Tuple\")\n    assert_node(doctree[1][1][0][0][1][0][6], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n    assert_node(doctree[1][1][0][0][1][0][8], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n", "def test_get_index_text(app):\n    domain = app.env.get_domain('py')\n\n    assert domain.get_index_text('modname', ('funcname', '')) is None\n    assert domain.get_index_text('modname', ('funcname', 'class')) == 'funcname() (modname class method)'\n    assert domain.get_index_text('modname', ('funcname', '')) is None\n    assert domain.get_index_text('modname', ('clasname', 'class')) == 'clasname (class in modname)'\n    assert domain.get_index_text('modname', ('clasname', 'exception')) == 'clasname'\n", "def test_python_role_for_alias_types(app):\n    text = \".. py:function:: hello(name: typing.List[int]) -> typing.Tuple[int, int]\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"name\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [desc_sig_name, pending_xref, \"List\"],\n                                                        [desc_sig_punctuation, \"[\"],\n                                                        [desc_sig_name, pending_xref, \"int\"],\n                                                        [desc_sig_punctuation, \"]\"])])])\n    assert_node(doctree[1][0][2],\n                [desc_returns, ([pending_xref, \"Tuple\"],\n                                [desc_sig_punctuation, \"[\"],\n                                [pending_xref, \"int\"],\n                                [desc_sig_punctuation, \", \"],\n                                [pending_xref, \"int\"],\n                                [desc_sig_punctuation, \"]\"])])\n", "def test_pyattribute_with_union_type_operator(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int | str\")\n    doctree = restructuredtext.parse(app, text)\n    print(doctree)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"int\"],\n                                                                        \" \",\n                                                                        [desc_sig_punctuation, \"|\"],\n                                                                        \" \",\n                                                                        [pending_xref, \"str\"])])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][4], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.attr' in app.env.domains['py'].objects\n    assert app.env.domains['py'].objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n", "def test_info_field_list_raises(app):\n    text = (\".. py:function:: func\\n\"\n            \"\\n\"\n            \"   :raises ValueError: blah blah\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, (desc_signature,\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Raises\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :raises ValueError:\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"ValueError\"],\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[1][1][0][0][1][0][0], pending_xref,\n                refdomain=\"py\", reftype=\"exc\", reftarget=\"ValueError\")\n"], "sample_947": ["def test_build_xref(app, warning):\n    text = \"\"\"", "def test_build_domain_c_type_alias(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"type_alias\")\n    assert len(ws) == 0\n    entries = extract_role_links(app, \"type_alias.html\")\n    assert entries == [\n        ('c.type_alias.MyType', 'MyType', 'MyType'),\n    ]\n", "def test_build_domain_c_alias(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"alias\")\n    assert len(ws) == 0\n    t = (app.outdir / \"alias.html\").read_text()\n    for id_ in ('alias_a', 'alias_b', 'alias_c'):\n        assert 'id=\"c.{}\"'.format(id_) in t\n    entries = extract_role_links(app, \"alias.html\")\n    assert entries == [\n        ('c.alias_a', 'alias_a', 'alias_a'),\n        ('c.alias_b', 'alias_b', 'alias_b'),\n        ('c.alias_c', 'alias_c', 'alias_c'),\n    ]\n", "def test_c_enum_member(app):\n    text = \".. c:enum:: Color\\n\\n\" \\\n           \"   .. c:enumerator:: RED\\n\" \\\n           \"   .. c:enumerator:: GREEN\\n\" \\\n           \"   .. c:enumerator:: BLUE\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"enum\",\n                domain=\"c\", objtype=\"enum\", noindex=False)\n\n    entry = _get_obj(app, 'Color')\n    assert entry == ('index', 'c.Color', 'enum')\n    entry = _get_obj(app, 'RED')\n    assert entry == ('index', 'c.Color.RED', 'enumerator')\n    entry = _get_obj(app, 'GREEN')\n    assert entry == ('index', 'c.Color.GREEN', 'enumerator')\n    entry = _get_obj(app, 'BLUE')\n    assert entry == ('index', 'c.Color.BLUE', 'enumerator')\n", "def test_c_domain_initialization(app):\n    # Test for c domain setup\n    assert 'c' in app.env.domains\n    domain = app.env.domains['c']\n    assert isinstance(domain, CDomain)\n"], "sample_946": ["def test_resolve_xref_for_subclass_by_unqualified_name(app):\n    text = (\".. py:class:: Parent\\n\"\n            \".. py:class:: Child\\n\"\n            \"   :inherits: Parent\\n\"\n            \".. py:method:: Child.method()\\n\"\n            \".. py:method:: Parent.method()\\n\")\n\n    doctree = restructuredtext.parse(app, text)\n\n    refnodes = list(doctree.traverse(pending_xref))\n    assert len(refnodes) == 1\n\n    # get \"Child\" refnode by unqualified name\n    child_refnode = refnodes[0]\n\n    # resolve \"Child\" refnode\n    resolved_child_refnode = app.env.domains['py'].resolve_xref(\n        app.env, \"index\", app.builder, \"class\", \"Child\", child_refnode, child_refnode)\n\n    # check if the resolved refnode points to the \"Child\" class\n    assert resolved_child_refnode.attributes[\"refid\"] == \"index#Child\"\n\n    # check if the \"Child\" class is a subclass of the \"Parent\" class\n    child_class = app.env.domains['py'].objects[\"Child\"]\n    assert child_class[2] == \"class\"\n    assert \"Parent\" in child_class[3]\n", "def test_type_to_xref(app):\n    env = app.env\n    text = \"Union[int, str]\"\n    xref = _parse_annotation(text, env)[0][0]\n    assert_node(xref, pending_xref, **{\"py:module\": None, \"py:class\": None})\n    assert_node(xref[0], pending_xref_condition, condition=\"resolved\")\n    assert_node(xref[0][0], nodes.Text, text=\"Union\")\n    assert_node(xref[1], pending_xref_condition, condition=\"*\")\n    assert_node(xref[1][0], nodes.Text, text=text)\n\n    xref = _parse_annotation(text, env)[0][3]\n    assert_node(xref, pending_xref, **{\"py:module\": None, \"py:class\": None})\n    assert_node(xref[0], pending_xref_condition, condition=\"resolved\")\n    assert_node(xref[0][0], nodes.Text, text=\"str\")\n    assert_node(xref[1], pending_xref_condition, condition=\"*\")\n    assert_node(xref[1][0], nodes.Text, text=\"str\")\n\n    text = \"List\"\n    xref = _parse_annotation(text, env)[0]\n    assert_node(xref, pending_xref, **{\"py:module\": None, \"py:class\": None})\n    assert_node(xref[0], pending_xref_condition, condition=\"resolved\")\n    assert_node(xref[0][0], nodes.Text, text=\"List\")\n    assert_node(xref[1], pending_xref_condition, condition=\"*\")\n    assert_node(xref[1][0], nodes.Text, text=\"List\")\n\n    env.config.python_use_unqualified_type_names = False\n    text = \"Union[int, str]\"\n    xref = _parse_annotation(text, env)[0][0]\n    assert_node(xref, pending_xref, **{\"py:module\": None, \"py:class\": None})\n    assert_node(xref, nodes.Text, text=\"Union\")\n\n    xref = _parse_annotation(text, env)[0][3]\n    assert_node(xref, pending_xref, **{\"py:module\": None, \"py:class\": None})\n    assert_node(xref, nodes.Text, text=\"str\")\n\n    text = \"List\"\n    xref = _parse_annotation(text, env)[0]\n    assert_node(xref, pending_xref, **{\"py:module\": None,", "def test_pyrole_for_non_refdomain_types(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :vartype: Optional[str]\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][1][0][0][1][0][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"Optional\")\n    assert_node(doctree[1][1][0][0][1][0][0][4], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n", "def test_resolve_xref_for_property_with_fallback_to_meth(app, status, warning):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:property:: prop\\n\"\n            \"\\n\"\n            \"Link to :meth:`prop`\")\n    doctree = restructuredtext.parse(app, text)\n    refnodes = list(doctree.traverse(pending_xref))\n    assert len(refnodes) == 1\n    assert_node(refnodes[0], pending_xref, reftype='meth', reftarget='prop',\n                refdomain='py', refexplicit=True)\n    assert_node(refnodes[0][0], desc_sig_name, '')\n", "def test_pyattribute_with_union_type_operator(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int | None\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"int\"],\n                                                                        \" \",\n                                                                        [desc_sig_punctuation, \"|\"],\n                                                                        \" \",\n                                                                        [pending_xref, \"None\"])])],\n                                   [desc_content, ()]))\n    assert_node(doctree[1][1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][1][0][1][3], pending_xref, **{\"py:class\": \"Class\"})\n    assert 'Class.attr' in app.env.domains['py'].objects\n    assert app.env.domains['py'].objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n"], "sample_949": ["def test_invalid_man_page(app, status, warning):\n    app.build()\n    warnings = warning.getvalue().strip()\n    assert \"WARNING: no document found for manpage 'invalid'\" in warnings\n", "def test_write_with_no_man_pages_config(app, status, warning):\n    # Set an empty man_pages config value\n    app.config.man_pages = []\n\n    # Build the manpages\n    app.builder.build_all()\n\n    # Check if a warning was issued\n    warnings = warning.getvalue().strip()\n    assert 'no \"man_pages\" config value found; no manual pages will be written' in warnings\n\n    # Check if no manpages were written\n    assert not (app.outdir / 'sphinxtests.1').exists()\n", "def test_man_pages_config(app, status, warning):\n    # Test man_pages config value is used correctly\n    app.config.man_pages = [('index', 'sphinxtests', 'Sphinx Test 1.0',\n                             ['Sphinx Team'], 1),\n                            ('other', 'otherman', 'Other Man Page 1.0',\n                             ['Other Author'], 5)]\n    app.builder.build_all()\n    assert (app.outdir / 'sphinxtests.1').exists()\n    assert (app.outdir / 'otherman.5').exists()\n", "def test_man_pages_config_empty(app, status, warning):\n    app.config.man_pages = []\n    app.builder.init()\n    assert \"no \\\"man_pages\\\" config value found; no manual pages will be written\" in warning.getvalue()\n", "def test_default_translator_class(app):\n    builder = app.builder\n    assert builder.default_translator_class is ManualPageTranslator\n    assert builder.supported_image_types == []\n"], "sample_950": ["def test_resolve_xref_for_none_type(app):\n    text = \":class:`None`\"\n    doctree = restructuredtext.parse(app, text)\n    refnodes = list(doctree.traverse(pending_xref))\n    assert refnodes[0].get('reftype') == 'obj'\n    assert refnodes[0].get('reftarget') == 'None'\n", "def test_python_domain_resolve_xref(app):\n    domain = PythonDomain(app.env)\n    node = pending_xref()\n    node['reftarget'] = 'test'\n    node['refdomain'] = 'py'\n    node['reftype'] = 'class'\n    ret = domain.resolve_xref(app.env, 'test', app.builder, 'class', 'test', node, None)\n    assert ret is not None\n", "def test_filter_meta_fields(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :meta public:\\n\"\n            \"   :meta private:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    filter_meta_fields(app, 'py', 'class', doctree[3][1][0][1])\n    assert_node(doctree[3][1][0][1], nodes.field_list, fields=0)\n", "def test_pytypedfield_parse_with_colon_in_default(app):\n    text = (\".. py:function:: func\\n\"\n            \"\\n\"\n            \"   :param str arg: = 'http://example.org/arg'\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree,\n                (addnodes.index,\n                 [desc, ([desc_signature, ([desc_name, \"func\"],\n                                           [desc_parameterlist, ()])],\n                         [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0],\n                ([nodes.field_name, \"Parameters\"],\n                 [nodes.field_body, nodes.paragraph]))\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"arg\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 \")\",\n                 \" -- \",\n                 \" \",\n                 \"='http://example.org/arg'\"))\n", "def test_pymethod_with_type_annotation(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth(self, a: int, b: str) -> None\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class method)', 'Class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"meth\"],\n                                                     [desc_parameterlist,\n                                                      [desc_parameter,\n                                                       ([desc_sig_name, \"self\"],\n                                                        [desc_sig_punctuation, \", \"],\n                                                        [desc_sig_name, \"a\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"int\"],\n                                                        [desc_sig_punctuation, \", \"],\n                                                        [desc_sig_name, \"b\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [pending_xref, \"str\"])]],\n                                                     [desc_returns, pending_xref, \"None\"])],\n                                   [desc_content, ()]))\n    assert 'Class.meth' in domain.objects\n    assert domain.objects['Class.meth'] == ('index', 'Class.meth', 'method', False)\n"], "sample_948": ["def test_requires_clause_with_expression():\n    check('function', 'template<typename T> requires (sizeof(T) > 4) void f()',\n          {4: 'I0EIQXst1TE4EEE1fvv'})\n    check('function', 'template<typename T> requires requires sizeof(T) > 4 void f()',\n          {4: 'I0EIQXst1TE4EEE1fvv'})\n    check('function', 'template<typename T> requires (sizeof(T) > 4) && true void f()',\n          {4: 'I0EIQooXst1TE4EEEL1EEE1fvv'})\n    check('function', 'template<typename T> requires requires (sizeof(T) > 4) && true void f()',\n          {4: 'I0EIQooXst1TE4EEEL1EEE1fvv'})\n", "def test_mangle_nested_names():\n    check('function', 'int A::f()', {1: 'A::f', 2: 'N1A1fE'})\n    check('function', 'int A::B::f()', {1: 'A::B::f', 2: 'N1A1B1fE'})\n    check('function', 'int A::B::C::f()', {1: 'A::B::C::f', 2: 'N1A1B1C1fE'})\n    check('function', 'int ::f()', {1: '::f', 2: '1fE'})\n    check('function', 'int ::A::f()', {1: '::A::f', 2: 'N1A1fE'})\n", "def test_alias(app, status, warning):\n    text = (\".. cpp:alias:: std::vector\\n\"\n            \"    A collection of objects of type T that can be easily resized.\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, addnodes.desc)\n    desc = doctree[0]\n    assert desc.classes == ['cpp', 'desc']\n", "def test_template_parameter_pack():\n    check('class', 'template<typename... Ts> {key}A', {2: 'IDpE1A'})\n    check('class', 'template<typename...> {key}A', {2: 'IDpE1A'})\n    check('class', 'template<typename T, typename... Ts> {key}A', {2: 'I0DpE1A'})\n", "def test_template_template_parameter():\n    check('class', '{key}A<int>', {2: 'I0E1AIiE'}, output='template<> {key}A<int>')\n    check('class', '{key}A<short>', {2: 'I0E1AIaE'}, output='template<> {key}A<short>')\n    check('class', '{key}A<int, int>', {2: 'I0E1AIiiEE'}, output='template<> {key}A<int, int>')\n\n    check('class', '{key}A<template<int> typename T>',\n          {2: 'I01EIiEE0E1A'}, output='template<template<int> typename T> {key}A')\n    check('class', '{key}A<template<int> typename T = B>',\n          {2: 'I01EIiEE0E1A'}, output='template<template<int> typename T> {key}A')\n    check('class', '{key}A<template<int> typename T = B<int>>',\n          {2: 'I01EIiEE0E1AIiEEE'}, output='template<template<int> typename T> {key}A')\n\n    check('class', '{key}A<template<int> typename...>',\n          {2: 'I01EIiEEDpE1A'}, output='template<template<int> typename...> {key}A')\n    check('class', '{key}A<template<int> typename... Ts>',\n          {2: 'I01EIiEEDpE1A'}, output='template<template<int> typename... Ts> {key}A')\n\n    check('class', '{key}A<template<int... Is> typename T>',\n          {2: 'I01EIi_DpEE0E1A'}, output='template<template<int... Is> typename T> {key}A')\n    check('class', '{key}A<template<int... Is> typename T = B>',\n          {2: 'I01EIi_DpEE0E1A'}, output='template<template<int... Is> typename T> {key}A')\n    check('class', '{key}A<template<int... Is> typename T = B<int>>',\n          {2: 'I01EIi_DpEE0E1AIiEEE'}, output='template<template<int... Is"], "sample_951": ["def test_getall():\n    class Module:\n        __all__ = ['attr1', 'attr2']\n\n    assert inspect.getall(Module) == ['attr1', 'attr2']\n\n    class Module:\n        pass\n\n    assert inspect.getall(Module) is None\n\n    class Module:\n        __all__ = 'attr'\n\n    assert inspect.getall(Module) == ['attr']\n\n    class Module:\n        __all__ = 1\n\n    with pytest.raises(ValueError):\n        inspect.getall(Module)\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert inspect.getmro(A) == (A, object)\n    assert inspect.getmro(B) == (B, A, object)\n    assert inspect.getmro(C) == (C, B, A, object)\n\n    # Test that getmro works with new-style classes\n    class NewStyleClass(object):\n        pass\n\n    assert inspect.getmro(NewStyleClass) == (NewStyleClass, object)\n\n    # Test that getmro works with classes that have a __bases__ attribute\n    class ClassWithBases(list):\n        pass\n\n    assert inspect.getmro(ClassWithBases) == (ClassWithBases, list, object)\n\n    # Test that getmro works with classes that have a __mro__ attribute\n    class ClassWithMRO(list):\n        __mro__ = (list, object)\n\n    assert inspect.getmro(ClassWithMRO) == (ClassWithMRO, list, object)\n", "def test_is_builtin_class_method_builtin():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(int, '__str__') is True\n    assert inspect.is_builtin_class_method(list, '__len__') is True\n    assert inspect.is_builtin_class_method(str, 'join') is True\n\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is True\n    assert inspect.is_builtin_class_method(MyInt, '__str__') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert inspect.getmro(A) == (A, object)\n    assert inspect.getmro(B) == (B, A, object)\n    assert inspect.getmro(C) == (C, B, A, object)\n"], "sample_952": ["def test_isenumclass():\n    class EnumClass(enum.Enum):\n        A = 1\n        B = 2\n\n    class NonEnumClass:\n        pass\n\n    assert inspect.isenumclass(EnumClass) is True\n    assert inspect.isenumclass(NonEnumClass) is False\n    assert inspect.isenumclass(int) is False\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(int, '__add__') is True\n    assert inspect.is_builtin_class_method(list, '__init__') is True\n    assert inspect.is_builtin_class_method(list, '__append__') is True\n    assert inspect.is_builtin_class_method(str, '__init__') is True\n    assert inspect.is_builtin_class_method(str, '__len__') is True\n\n    class CustomClass:\n            pass\n\n    assert inspect.is_builtin_class_method(CustomClass, '__init__') is False\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B, A):\n        pass\n\n    assert inspect.getmro(A) == (A, object)\n    assert inspect.getmro(B) == (B, A, object)\n    assert inspect.getmro(C) == (C, B, A, object)\n", "def test_getall():\n    class Foo:\n        __all__ = ['a', 'b']\n\n    assert inspect.getall(Foo) == ['a', 'b']\n\n    class Bar:\n        pass\n\n    assert inspect.getall(Bar) is None\n\n    class Baz:\n        __all__ = 1\n\n    with pytest.raises(ValueError):\n        inspect.getall(Baz)\n\n    class Qux:\n        __all__ = [1, 2]\n\n    with pytest.raises(ValueError):\n        inspect.getall(Qux)\n", "def test_getmro():\n    class Parent:\n        pass\n\n    class Child(Parent):\n        pass\n\n    assert inspect.getmro(Parent) == (Parent, object)\n    assert inspect.getmro(Child) == (Child, Parent, object)\n\n    # Test that getmro returns a tuple\n    assert isinstance(inspect.getmro(Parent), tuple)\n    assert isinstance(inspect.getmro(Child), tuple)\n\n    # Test that getmro raises an AttributeError for non-class objects\n    with pytest.raises(AttributeError):\n        inspect.getmro(object())\n"], "sample_953": ["def test_valid_dir(tempdir):\n    d = {\n        'path': tempdir,\n        'sep': True,\n        'dot': '_',\n        'master': 'index',\n        'suffix': '.rst',\n    }\n    assert qs.valid_dir(d)\n\n    d['sep'] = False\n    assert qs.valid_dir(d)\n\n    (tempdir / 'conf.py').write_text('')\n    assert not qs.valid_dir(d)\n\n    (tempdir / 'Makefile').write_text('')\n    assert not qs.valid_dir(d)\n\n    d['sep'] = True\n    (tempdir / 'source' / 'conf.py').write_text('')\n    assert not qs.valid_dir(d)\n\n    (tempdir / '_static').mkdir()\n    assert not qs.valid_dir(d)\n\n    (tempdir / '_templates').mkdir()\n    assert not qs.valid_dir(d)\n\n    (tempdir / 'index.rst').write_text('')\n    assert not qs.valid_dir(d)\n", "def test_quickstart_with_templatedir(tempdir):\n    templatedir = tempdir / 'templatedir'\n    templatedir.mkdir()\n    (templatedir / 'conf.py_t').write_text('print(\"Custom template\")')\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d, templatedir=str(templatedir))\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    assert conffile.read_text().strip() == 'print(\"Custom template\")'\n", "def test_generate_overwrite(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    # Create an existing file\n    existing_file = tempdir / 'index.rst'\n    existing_content = existing_file.read_text()\n    existing_file.write_text('New content')\n\n    # Regenerate\n    qs.generate(d, overwrite=False)\n\n    # Check if file was not overwritten\n    assert existing_file.read_text() == 'New content'\n\n    # Regenerate with overwrite\n    qs.generate(d, overwrite=True)\n\n    # Check if file was overwritten\n    assert existing_file.read_text() == existing_content\n", "def test_valid_dir(tempdir):\n    d = {'path': tempdir}\n    assert qs.valid_dir(d) is True\n\n    d['path'] = tempdir / 'nonexistent'\n    assert qs.valid_dir(d) is True\n\n    (tempdir / 'conf.py').touch()\n    d['path'] = tempdir\n    assert qs.valid_dir(d) is False\n\n    d['sep'] = True\n    assert qs.valid_dir(d) is False\n\n    (tempdir / 'Makefile').touch()\n    assert qs.valid_dir(d) is False\n\n    d['path'] = tempdir / 'source'\n    (tempdir / 'source' / 'conf.py').touch()\n    d['sep'] = False\n    assert qs.valid_dir(d) is False\n", "def test_valid_dir(tempdir):\n    d = {'path': str(tempdir)}\n    assert qs.valid_dir(d) is True\n\n    (tempdir / 'conf.py').touch()\n    assert qs.valid_dir(d) is False\n\n    (tempdir / 'conf.py').unlink()\n    (tempdir / 'Makefile').touch()\n    assert qs.valid_dir(d) is False\n\n    (tempdir / 'Makefile').unlink()\n    d['sep'] = True\n    (tempdir / 'source' / 'conf.py').touch()\n    assert qs.valid_dir(d) is False\n\n    (tempdir / 'source' / 'conf.py').unlink()\n    d['dot'] = '_'\n    d['master'] = 'index'\n    d['suffix'] = '.rst'\n    (tempdir / 'source' / '_static').mkdir()\n    assert qs.valid_dir(d) is False\n"], "sample_954": ["def test_show_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert '<https://www.sphinx-doc.org/> ' in content\n", "def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n\n    # External URL\n    assert ('<https://www.example.com>' in content)\n\n    # Internal URL\n    assert ('<#internal-ref>' in content)\n\n    # manpage URL\n    assert ('<manpage(1)>' in content)\n\n    # mailto URL\n    assert ('<user@example.com>' in content)\n", "def test_man_page_title_with_non_ascii_chars(app, status, warning):\n    app.config.man_pages = [\n        ('index', 'manpage', u'Sphinx \\u2013 manpage', [u'Georg Brandl'], 1)\n    ]\n    app.builder.build_all()\n    content = (app.outdir / 'manpage.1').read_text()\n    assert u'.TH \"SPHINX \\u2013 MANPAGE\" \"1\"' in content\n", "def test_man_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n\n    # check that man_show_urls adds the URL after the link\n    assert r'\\fIhttps://www.sphinx-doc.org/\\fP' in content\n", "def test_manpage_node(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert r'\\fBsphinx-manpage\\fP' in content\n    assert r'\\fBmanpage\\fP' in content\n\n    # test manpage node itself\n    assert '.SH SYNOPSIS' in content\n    assert '.SH DESCRIPTION' in content\n    assert '.SH OPTIONS' in content\n    assert '.SH EXIT STATUS' in content\n    assert '.SH RETURN VALUE' in content\n    assert '.SH ERRORS' in content\n    assert '.SH ENVIRONMENT' in content\n    assert '.SH FILES' in content\n    assert '.SH VERSIONS' in content\n    assert '.SH CONFORMING TO' in content\n    assert '.SH NOTES' in content\n    assert '.SH BUGS' in content\n    assert '.SH EXAMPLE' in content\n    assert '.SH AUTHORS' in content\n    assert '.SH COPYRIGHT' in content\n"], "sample_955": ["def test_unparse_arguments():\n    source = \"def func(a: int, b: str, c=None, d=1, *args, e, f=2, **kwargs): pass\"\n    module = ast.parse(source)\n    expected = \"a: int, b: str, c=None, d=1, *args, e, f=2, **kwargs\"\n    assert ast.unparse(module.body[0].args, source) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == expected\n", "def test_unparse_visit_arg():\n    source = \"def func(arg1: int, arg2: str = 'default', *args, **kwargs): pass\"\n    module = ast.parse(source)\n    func_def = module.body[0]\n    assert ast.unparse(func_def.args) == \"arg1: int, arg2: str = 'default', *args, **kwargs\"\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args, source) == expected\n"], "sample_957": ["def test_stringify_type_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n    assert stringify(ForwardRef(\"tests.test_util_typing.MyClass1\")) == \"tests.test_util_typing.MyClass1\"\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_restify_broken_type_hints_with_args():\n    class BrokenTypeWithArgs:\n        __args__ = (int, str)\n\n    assert restify(BrokenTypeWithArgs) == ':class:`tests.test_util_typing.BrokenTypeWithArgs`'\n", "def test_stringify_type_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_stringify_broken_type_hints():\n    assert stringify(BrokenType) == \"tests.test_util_typing.BrokenType\"\n"], "sample_956": ["def test_inspect_main_url_not_found(capsys, http_server):\n    \"\"\"inspect_main interface, with url argument that returns 404\"\"\"\n    url = http_server.base_url + '/nonexistent'\n    with pytest.raises(Exception):\n        inspect_main([url])\n\n    stdout, stderr = capsys.readouterr()\n    assert stdout == \"\"\n    assert \"404\" in stderr\n", "def test_resolve_reference_detect_inventory(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n        'py3k': ('https://docs.python.org/py3k/', inv_file),\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # check direct reference\n    rn = resolve_reference_detect_inventory(app.env, \n                                            pending_xref('', refdomain='py', reftarget='module1.func', reftype='func'), \n                                            nodes.Text('func'))\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n\n    # check named inventory reference\n    rn = resolve_reference_detect_inventory(app.env, \n                                            pending_xref('', refdomain='py', reftarget='py3k:module1.func', reftype='func'), \n                                            nodes.Text('func'))\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'https://docs.python.org/py3k/sub/foo.html#module1.func'\n\n    # check non-existent reference\n    rn = resolve_reference_detect_inventory(app.env, \n                                            pending_xref('', refdomain='py', reftarget='nonexistent', reftype='func'), \n                                            nodes.Text('nonexistent'))\n    assert rn is None\n\n    # check non-existent named inventory reference\n    rn = resolve_reference_detect_inventory(app.env, \n                                            pending_xref('', refdomain='py', reftarget='py3k:nonexistent', reftype='func'), \n                                            nodes.Text('nonexistent'))\n    assert rn is None\n", "def test_inspect_main_url_error(capsys, http_server):\n    \"\"\"inspect_main interface, with url argument that returns an error\"\"\"\n    http_server.set_response_code(404)\n    http_server.set_response_body(b'Not Found')\n\n    inspect_main([http_server.url])\n\n    stdout, stderr = capsys.readouterr()\n    assert stdout == \"\"\n    assert stderr.startswith('intersphinx inventory ')\n", "def test_inspect_main_invalid_url(capsys, monkeypatch):\n    \"\"\"inspect_main interface, with invalid url argument\"\"\"\n        raise ValueError(\"Invalid URL\")\n\n    monkeypatch.setattr('sphinx.ext.intersphinx.fetch_inventory', mock_fetch_inventory)\n\n    inspect_main([\"https://example.com/nonexistent\"])\n\n    stdout, stderr = capsys.readouterr()\n    assert stdout == \"\"\n    assert stderr.startswith(\"Error: invalid or incompatible inventory:\")\n", "def test_resolve_reference_detect_inventory(tempdir, app, status, warning):\n    \"\"\"Test resolve_reference_detect_inventory() function.\"\"\"\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n        'py3k': ('https://docs.python.org/py3k/', inv_file),\n    })\n\n    load_mappings(app)\n\n    # ordinary direct lookup, use data as is\n    node, contnode = fake_node('py', 'func', 'module1.func', 'foo')\n    res = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert res is not None\n\n    # try splitting the target into 'inv_name:target'\n    node, contnode = fake_node('py', 'func', 'py3k:module1.func', 'foo')\n    res = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert res is not None\n\n    # try with non-existent inventory name\n    node, contnode = fake_node('py', 'func', 'nonexist:module1.func', 'foo')\n    res = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert res is None\n\n    # try with non-existent target\n    node, contnode = fake_node('py', 'func', 'py3k:nonexist', 'foo')\n    res = resolve_reference_detect_inventory(app.env, node, contnode)\n    assert res is None\n"], "sample_958": ["def test_domain_cpp_parse_invalid_pre_v3_type_directive():\n    parser = DefinitionParser(\"pre-v3-type-directive Test\", location=None, config=None)\n    with pytest.raises(DefinitionError):\n        parser.parse_pre_v3_type_definition()\n", "def test_domain_cpp_parse_xref_object():\n        parser = DefinitionParser(string, location=None, config=None)\n        try:\n            name = parser.parse_xref_object()\n            parser.assert_end()\n            return name\n        except DefinitionError as e:\n            logger.warning('Unparseable C cross-reference: %r\\n%s', string, e,\n                           location=None)\n            return None\n\n    assert parse_xref_object('f') is not None\n    assert parse_xref_object('f()') is not None\n    assert parse_xref_object('void f()') is not None\n    assert parse_xref_object('T f()') is not None\n", "def test_domain_cpp_ast_variable_declaration():\n    check('var', 'int a', {1: 'a__i', 2: '1a'})\n    check('var', 'int a = 5', {1: 'a__i', 2: '1a'})\n    check('var', 'const int a = 5', {1: 'a__iC', 2: '1a'})\n    check('var', 'int *a', {1: 'a__iP', 2: '1a'})\n    check('var', 'int *a = &b', {1: 'a__iP', 2: '1a'})\n    check('var', 'int &a = b', {1: 'a__iR', 2: '1a'})\n    check('var', 'int &&a = b', {1: 'a__iO', 2: '1a'})\n    check('var', 'int a[5]', {1: 'a__iA5', 2: '1a'})\n    check('var', 'int a[]', {1: 'a__iA', 2: '1a'})\n    check('var', 'int a[5][5]', {1: 'a__iA5A5', 2: '1a'})\n    check('var', 'int (*a)[5]', {1: 'a__iPA5', 2: '1a'})\n    check('var', 'int (&a)[5]', {1: 'a__iRA5', 2: '1a'})\n    check('var', 'int (&&a)[5]', {1: 'a__iOA5', 2: '1a'})\n", "def test_domain_cpp_parse_type_in_expression():\n    input = '(int) a'\n    parser = DefinitionParser(input, location=None, config=None)\n    parser.allowFallbackExpressionParsing = False\n    ast = parser.parse_expression()\n    res = str(ast)\n    assert res == input\n", "def test_domain_cpp_ast_type_qualifiers():\n    check('type', '{key}int volatile i', {1: \"i__iV\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int const i', {1: \"i__iC\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int const volatile i', {1: \"i__iCV\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int volatile const i', {1: \"i__iVC\", 2: \"1i\"}, key='typedef')\n\n    check('type', '{key}int &i', {1: \"i__iR\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int &&i', {1: \"i__iO\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int const &i', {1: \"i__iCR\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int volatile &i', {1: \"i__iVR\", 2: \"1i\"}, key='typedef')\n\n    check('type', '{key}int const *i', {1: \"i__iPC\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int volatile *i', {1: \"i__iPV\", 2: \"1i\"}, key='typedef')\n\n    check('type', '{key}int restrict i', {1: \"i__i\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int volatile restrict i', {1: \"i__iV\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int const restrict i', {1: \"i__iC\", 2: \"1i\"}, key='typedef')\n    check('type', '{key}int const volatile restrict i', {1: \"i__iCV\", 2: \"1i\"}, key='typedef')\n"], "sample_960": ["def test_pytype_fields(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :ivar int attr: blah blah\\n\"\n            \"   :cvar str class_attr: blah blah\\n\"\n            \"   :vartype attr2: Tuple[str, ...]\\n\"\n            \"   :var attr2: blah blah\\n\")\n    doctree = restructuredtext.parse(app, text)\n    print(doctree)\n\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_addname, \"example.\"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, nodes.field_list, ([nodes.field,\n                                                                     nodes.field,\n                                                                     nodes.field,\n                                                                     nodes.field])])]))\n    assert_node(doctree[3][1][0][0],\n                ([nodes.field_name, \"Variables\"],\n                 [nodes.field_body, nodes.bullet_list, ([nodes.list_item, nodes.paragraph],\n                                                        [nodes.list_item, nodes.paragraph],\n                                                        [nodes.list_item, nodes.paragraph],\n                                                        [nodes.list_item, nodes.paragraph])]))\n\n    # :ivar int attr:\n    assert_node(doctree[3][1][0][0][1][0][0][0],\n                ([addnodes.literal_strong, \"attr\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[3][1][0][0][1][0][0][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n\n    # :cvar str class_attr:\n    assert_node(doctree[3][1][0][0][1][0][1][0],\n                ([addnodes.literal_strong, \"class_attr\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[3][1][0][0][1][0][1][0][2], pending_xref,\n                refdomain=\"py\",", "def test_get_full_qualified_name_for_nested_namespaces(app):\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    node = nodes.reference(reftarget='func')\n    node['py:module'] = 'module_a'\n    node['py:class'] = 'ClassB'\n    assert domain.get_full_qualified_name(node) == 'module_a.ClassB.func'\n\n    node = nodes.reference(reftarget='func')\n    node['py:module'] = 'module_a.module_b'\n    node['py:class'] = 'ClassB.ClassC'\n    assert domain.get_full_qualified_name(node) == 'module_a.module_b.ClassB.ClassC.func'\n", "def test_type_to_xref(app):\n    env = app.env\n    env.ref_context['py:module'] = 'mymodule'\n    env.ref_context['py:class'] = 'MyClass'\n\n    # test type to xref with module and class\n    xref = _parse_annotation(\"MyClass\", env)\n    assert_node(xref, ([pending_xref, \"MyClass\"],))\n    assert_node(xref[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"MyClass\",\n                **{\"py:module\": \"mymodule\", \"py:class\": \"MyClass\"})\n\n    # test type to xref without module and class\n    env.ref_context.pop('py:module', None)\n    env.ref_context.pop('py:class', None)\n    xref = _parse_annotation(\"int\", env)\n    assert_node(xref, ([pending_xref, \"int\"],))\n    assert_node(xref[0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n\n    # test type to xref with None\n    xref = _parse_annotation(\"None\", env)\n    assert_node(xref, ([pending_xref, \"None\"],))\n    assert_node(xref[0], pending_xref, refdomain=\"py\", reftype=\"obj\", reftarget=\"None\")\n\n    # test type to xref with unqualified type name\n    app.config.python_use_unqualified_type_names = True\n    xref = _parse_annotation(\"mymodule.MyClass\", env)\n    assert_node(xref, ([pending_xref_condition, pending_xref_condition],))\n    assert_node(xref[0], pending_xref_condition, condition=\"resolved\")\n    assert_node(xref[1], pending_xref_condition, condition=\"*\")\n    assert_node(xref[1][0], pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"mymodule.MyClass\")\n", "def test_module_fullname(app):\n    domain = PythonDomain(app.env)\n    assert domain.get_full_qualified_name(nodes.reference(reftarget='func')) == 'func'\n    assert domain.get_full_qualified_name(nodes.reference(reftarget='func', py_module='module1')) == 'module1.func'\n    assert domain.get_full_qualified_name(nodes.reference(reftarget='func', py_class='Class')) == 'Class.func'\n    assert domain.get_full_qualified_name(nodes.reference(reftarget='func', py_module='module1', py_class='Class')) == 'module1.Class.func'\n", "def test_module_handling(app):\n    domain = app.env.get_domain('py')\n    # Add a module manually\n    domain.data['modules']['test'] = ('test', 'test', 'A test module', '', False)\n    domain.data['objects']['test'] = ('test', 'test', 'module', False)\n    # Add a submodule manually\n    domain.data['modules']['test.sub'] = ('test.sub', 'test.sub', 'A test submodule', '', False)\n    domain.data['objects']['test.sub'] = ('test.sub', 'test.sub', 'module', False)\n\n    # Clear the domain's module data, simulating a rebuild\n    domain.clear_doc('test')\n    # Check that the module and submodule are gone from the domain's data\n    assert 'test' not in domain.data['modules']\n    assert 'test.sub' not in domain.data['modules']\n    assert 'test' not in domain.data['objects']\n    assert 'test.sub' not in domain.data['objects']\n"], "sample_959": ["def test_domain_cpp_ast_trailing_return():\n    check('function', 'void f() -> int',\n          {1: 'f', 2: '1fi'}, output='void f() -> int')\n    check('function', 'void f() -> std::string',\n          {1: 'f__ss', 2: '1fNSt6stringE'},\n          output='void f() -> std::string')\n    check('function', 'void f() -> int &&',\n          {1: None, 2: '1fOi'}, output='void f() -> int &&')\n    check('function', 'void f() -> int *volatile const &',\n          {1: None, 2: '1fPCVi'}, output='void f() -> int *volatile const &')\n", "def test_domain_cpp_ast_wrong_template_args():\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A<int, string>')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A<template<int> int>')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A<template<int> int, int>')\n    with pytest.raises(DefinitionError):\n        parse('class', '{key}A<int, template<int>>')\n", "def test_domain_cpp_parse_template_param_list(app):\n    # template-parameter-list\n    #   -> template-parameter\n    #    | template-parameter-list , template-parameter\n    # template-parameter\n    #   -> type-parameter\n    #    | parameter-declaration -> type-specifier-seq declarator\n    # type-parameter\n    #   -> class ...[opt] identifier[opt]\n    #    | class identifier[opt] = type-id\n    #    | typename ...[opt] identifier[opt]\n    #    | typename identifier[opt] = type-id\n    #    | template < template-parameter-list > class ...[opt] identifier[opt]\n    #    | template < template-parameter-list > class identifier[opt] = type-id\n    #    | template < template-parameter-list > typename ...[opt] identifier[opt]\n    #    | template < template-parameter-list > typename identifier[opt] = type-id\n    # template template parameters\n\n    text = \"\"\"", "def test_domain_cpp_ast_template_introduction():\n    check('class', 'A::B<>', {2: 'N1A1BIvE'})\n    check('class', 'A::B<int>', {2: 'N1A1BIiEE'})\n    check('class', 'A::B<,>', {2: 'N1A1BIJEEE'})\n    check('class', 'A::B<int,>', {2: 'N1A1BIiJEE'})\n    check('class', 'A::B<, int>', {2: 'N1A1BIJEiEE'})\n    check('class', 'A::B<int, int>', {2: 'N1A1BIiIiEE'})\n    check('class', 'A::B<...>', {2: 'N1A1BIDpE'})\n    check('class', 'A::B<int, ...>', {2: 'N1A1BIiDpE'})\n    check('class', 'A::B<..., int>', {2: 'N1A1BIDpIiEE'})\n    check('class', 'A::B<int, int, ...>', {2: 'N1A1BIiIiDpE'})\n\n    # check that this is allowed for function declarations\n    check('function', 'void f(A::B<> a)', {2: '1fN1A1BIPvE'})\n    check('function', 'void f(A::B<int> a)', {2: '1fN1A1BIiEE'})\n    check('function', 'void f(A::B<,> a)', {2: '1fN1A1BIJEEE'})\n    check('function', 'void f(A::B<int,> a)', {2: '1fN1A1BIiJEE'})\n    check('function', 'void f(A::B<, int> a)', {2: '1fN1A1BIJEiEE'})\n    check('function', 'void f(A::B<int, int> a)', {2: '1fN1A1BIiIiEE'})\n    check('function', 'void f(A::B<...> a)', {2: '1fN1A1BIDpE'})\n    check('function', 'void f(A::B<int, ...> a", "def test_domain_cpp_parse_xref_object():\n    parser = DefinitionParser('f', location=None, config=None)\n\n        ast, shorthand = parser.parse_xref_object()\n        parser.assert_end()\n        assert shorthand == isShorthand\n    parseAndAssert('f', False)\n    parseAndAssert('f()', False)\n    parseAndAssert('void f()', False)\n    parseAndAssert('T f()', False)\n\n    # template declarations\n    parseAndAssert('template<> f()', True)\n    parseAndAssert('template<> void f()', True)\n    parseAndAssert('template<> T f()', True)\n\n    # template introductions\n    parseAndAssert('X{a, b, c} f', True)\n    parseAndAssert('X{a, b, ...c} f', True)\n    parseAndAssert('X{a, b, c} f<g>', True)\n    parseAndAssert('X{a, b, ...c} f<g>', True)\n    parseAndAssert('X{a, b, ...c, d} f<g>', True)\n    parseAndAssert('X{a, b, ...c, d} f<g, h>', True)\n"], "sample_963": ["def test_stringify_broken_type_hints():\n    assert stringify(BrokenType) == 'tests.test_util_typing.BrokenType'\n", "def test_restify_type_hints_Ellipsis():\n    assert restify(Ellipsis) == '...'\n    assert restify(type(Ellipsis)) == ':py:class:`ellipsis`'\n", "def test_stringify_forward_ref():\n    from typing import ForwardRef  # type: ignore\n    my_int = ForwardRef(\"my_int\")\n    assert stringify(my_int) == \"my_int\"\n", "def test_stringify_broken_type_hints():\n    assert stringify(BrokenType) == 'tests.test_util_typing.BrokenType'\n", "def test_stringify_broken_type_hints():\n    assert stringify(BrokenType) == \"tests.test_util_typing.BrokenType\"\n"], "sample_962": ["def test_ismock():\n    assert not ismock(int)\n    assert not ismock(str)\n\n    with mock(['unknown']):\n        import unknown\n        assert ismock(unknown)\n        assert ismock(unknown.secret)\n        assert ismock(unknown.secret.Class)\n", "def test_ismock():\n    with mock(['unknown']):\n        import unknown\n        assert ismock(unknown)\n        assert ismock(unknown.secret)\n        assert ismock(unknown.secret.Class)\n    assert not ismock(int)\n    assert not ismock(str)\n    assert not ismock(MyClass1)\n    assert not ismock(MyClass2)\n\n", "def test_ismock():\n    assert not ismock(int)\n    assert not ismock(str)\n    assert not ismock(None)\n\n    with mock(['unknown']):\n        import unknown\n        assert ismock(unknown)\n        assert ismock(unknown.secret)\n        assert ismock(unknown.secret.Class)\n", "def test_ismock():\n    with mock(['unknown']):\n        import unknown\n        assert ismock(unknown)\n        assert ismock(unknown.secret)\n        assert ismock(unknown.secret.Class)\n\n    assert not ismock(int)\n    assert not ismock(str)\n    assert not ismock(None)\n", "def test_undecorate():\n    mock_obj = _MockObject()\n    mock_obj.__sphinx_decorator_args__ = (\"original\",)\n    assert undecorate(mock_obj) == \"original\"\n\n    class Mock(_MockObject):\n        __sphinx_decorator_args__ = (\"original\",)\n\n    mock_obj = Mock()\n    assert undecorate(mock_obj) == \"original\"\n\n    non_mock_obj = object()\n    assert undecorate(non_mock_obj) == non_mock_obj\n"], "sample_961": ["def test_pyattribute_with_union_type_operator(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int | str\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][1][1][0],\n                ([desc_name, \"attr\"],\n                 [desc_annotation, (\": \",\n                                    [pending_xref, \"int\"],\n                                    \" \",\n                                    [desc_sig_punctuation, \"|\"],\n                                    \" \",\n                                    [pending_xref, \"str\"])]))\n", "def test_info_field_list_returns(app):\n    text = (\".. py:function:: my_func\\n\"\n            \"\\n\"\n            \"   :return: blah blah\\n\"\n            \"   :rtype: str\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"my_func\"],\n                                                    [desc_parameterlist, ()])],\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0],\n                ([nodes.field_name, \"Returns\"],\n                 [nodes.field_body, nodes.paragraph]))\n\n    # :return:\n    assert_node(doctree[1][1][0][0][1][0],\n                (\"blah blah\",))\n\n    # :rtype:\n    assert_node(doctree[1][1][0][0][1][1],\n                ([addnodes.literal_strong, \"Return type\"],\n                 \" -- \",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"]))\n    assert_node(doctree[1][1][0][0][1][1][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\")\n", "def test_get_objects(app):\n    domain = app.env.get_domain('py')\n    objects = list(domain.get_objects())\n    assert len(objects) == 7\n    assert objects[0] == ('Class1', 'Class1', 'class', 'index', 'Class1', 0)\n    assert objects[1] == ('Class2', 'Class2', 'class', 'index', 'Class2', 0)\n    assert objects[2] == ('Class3', 'Class3', 'class', 'index', 'Class3', 0)\n    assert objects[3] == ('Class1.method1', 'Class1.method1', 'method', 'index', 'Class1.method1', 0)\n    assert objects[4] == ('Class1.method2', 'Class1.method2', 'method', 'index', 'Class1.method2', 0)\n    assert objects[5] == ('Class2.method1', 'Class2.method1', 'method', 'index', 'Class2.method1', 0)\n    assert objects[6] == ('Class3.attribute1', 'Class3.attribute1', 'attribute', 'index', 'Class3.attribute1', 0)\n", "def test_python_domain_add_target_and_index(app):\n    text = (\".. py:function:: foo()\\n\"\n            \".. py:class:: Foo\\n\"\n            \"   .. py:method:: bar()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    # Get the first desc node (foo function)\n    desc_node = doctree[1]\n    assert_node(desc_node, addnodes.desc, domain='py', objtype='function')\n    # Get the second desc node (Foo class)\n    desc_node = doctree[3]\n    assert_node(desc_node, addnodes.desc, domain='py', objtype='class')\n    # Get the third desc node (bar method)\n    desc_node = doctree[3][1][1]\n    assert_node(desc_node, addnodes.desc, domain='py', objtype='method')\n    # Check that the targets were added\n    targets = list(doctree.traverse(nodes.target))\n    assert len(targets) == 3\n    for target in targets:\n        assert target.hasattr('refid')\n    # Check that the index entries were added\n    index_entries = list(doctree.traverse(addnodes.index))\n    assert len(index_entries) == 3\n    for entry in index_entries:\n        assert entry.hasattr('entries')\n", "def test_get_full_qualified_name_for_builtins():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # built-in types\n    node = nodes.reference(reftarget='int')\n    assert domain.get_full_qualified_name(node) == 'int'\n\n    node = nodes.reference(reftarget='str')\n    assert domain.get_full_qualified_name(node) == 'str'\n\n    # built-in functions\n    node = nodes.reference(reftarget='len')\n    assert domain.get_full_qualified_name(node) == 'len'\n\n    node = nodes.reference(reftarget='range')\n    assert domain.get_full_qualified_name(node) == 'range'\n"], "sample_965": ["def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(int, '__add__') is True\n    assert inspect.is_builtin_class_method(int, '__repr__') is True\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is True\n    assert inspect.is_builtin_class_method(MyInt, '__add__') is True\n    assert inspect.is_builtin_class_method(MyInt, '__repr__') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(int, '__add__') is True\n    assert inspect.is_builtin_class_method(int, '__str__') is True\n    assert inspect.is_builtin_class_method(str, 'join') is True\n    assert inspect.is_builtin_class_method(str, 'upper') is True\n\n    class MyStr(str):\n            pass\n\n    assert inspect.is_builtin_class_method(MyStr, '__init__') is True\n    assert inspect.is_builtin_class_method(MyStr, '__add__') is True\n    assert inspect.is_builtin_class_method(MyStr, '__str__') is True\n    assert inspect.is_builtin_class_method(MyStr, 'join') is True\n    assert inspect.is_builtin_class_method(MyStr, 'upper') is True\n    assert inspect.is_builtin_class_method(MyStr, 'my_method') is False\n", "def test_getorigbases():\n    class Base:\n        pass\n\n    class Foo(Base):\n        __orig_bases__ = (Base,)\n\n    class Bar(Foo):\n        pass\n\n    assert inspect.getorigbases(Base) is None\n    assert inspect.getorigbases(Foo) == (Base,)\n    assert inspect.getorigbases(Bar) is None\n", "def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(int, '__repr__') is True\n    assert inspect.is_builtin_class_method(int, '__str__') is True\n    assert inspect.is_builtin_class_method(MyInt, '__init__') is True\n    assert inspect.is_builtin_class_method(MyInt, '__repr__') is True\n    assert inspect.is_builtin_class_method(MyInt, '__str__') is True\n    assert inspect.is_builtin_class_method(MyInt, 'my_method') is False\n\n", "def test_is_builtin_class_method_builtin():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n\n"], "sample_966": ["def test_resolve_xref_for_builtins(app):\n    text = (\".. py:module:: example\\n\"\n            \"\\n\"\n            \"   :return: list, tuple, str, float\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"module\", desc_sig_space)],\n                                                    [desc_addname, \"example\"])],\n                                  [desc_content, nodes.paragraph, nodes.Text])]))\n\n    for xref in doctree.traverse(pending_xref):\n        assert xref['refdomain'] == 'py'\n        assert xref['reftype'] == 'class'\n        assert xref['reftarget'] in ['list', 'tuple', 'str', 'float']\n", "def test_get_full_qualified_name_with_module_class_context():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    node = nodes.reference(reftarget='func', py_module='module1', py_class='Class')\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n", "def test_pyfield_parse_type(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :ivar str name: blah blah\\n\"\n            \"   :vartype age: int\\n\"\n            \"   :ivar items: blah blah\\n\"\n            \"   :vartype items: Tuple[str, ...]\\n\"\n            \"   :ivar Dict[str, str] params: blah blah\\n\")\n    doctree = restructuredtext.parse(app, text)\n    print(doctree)\n\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_addname, \"example.\"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[3][1][0][0],\n                ([nodes.field_name, \"Variables\"],\n                 [nodes.field_body, nodes.bullet_list, ([nodes.list_item, nodes.paragraph],\n                                                        [nodes.list_item, nodes.paragraph],\n                                                        [nodes.list_item, nodes.paragraph],\n                                                        [nodes.list_item, nodes.paragraph])]))\n\n    # :ivar str name:\n    assert_node(doctree[3][1][0][0][1][0][0],\n                ([addnodes.literal_strong, \"name\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[3][1][0][0][1][0][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n\n    # :vartype age: + :ivar age:\n    assert_node(doctree[3][1][0][0][1][0][1][0],\n                ([addnodes.literal_strong, \"age\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 \")\",\n                 \" -- \",\n                 \"\"))\n    assert_node(doctree[3][1][0][0][1][0][1][0][2], pending_xref,\n                refdomain=\"py\", reftype", "def test_python_module_index_with_empty_module_synopsis(app):\n    text = (\".. py:module:: docutils\\n\"\n            \"   :synopsis:\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')])],\n        False\n    )\n", "def test_get_full_qualified_name_with_module_and_class():\n    env = Mock(domaindata={})\n    domain = PythonDomain(env)\n\n    # with both py:module and py:class context\n    kwargs = {'py:module': 'module1', 'py:class': 'Class'}\n    node = nodes.reference(reftarget='func', **kwargs)\n    assert domain.get_full_qualified_name(node) == 'module1.Class.func'\n"], "sample_964": ["def test_get_index_text_method_with_module_name(app):\n    domain = app.env.get_domain('py')\n    obj = domain.objects['module_a.submodule.ModTopLevel.mod_child_1']\n    assert domain.get_index_text('module_a.submodule', obj) == \\\n        'mod_child_1() (in module module_a.submodule)'\n", "def test_resolve_xref_for_module_attribute(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:attribute:: example.attr\\n\")\n\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n\n    node = pending_xref(refdomain='py', reftype='attr', reftarget='example.attr')\n    resolved = domain.resolve_xref(app.env, 'index', app.builder, 'attr', 'example.attr', node, None)\n    assert_node(resolved, refuri='index#index')\n\n    node = pending_xref(refdomain='py', reftype='attr', reftarget='example.attr')\n    resolved = domain.resolve_xref(app.env, 'index', app.builder, 'attr', 'example.attr', node, None)\n    assert_node(resolved, refuri='index#index')\n", "def test_pytype_optional_annotation(app):\n    text = (\".. py:class:: Class\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: Optional[int]\")\n    doctree = restructuredtext.parse(app, text)\n\n    # :type: Optional[int]\n    assert_node(doctree[1][1][0][0][1][0][2][0][2],\n                pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"Optional\")\n    assert_node(doctree[1][1][0][0][1][0][2][0][4],\n                pending_xref, refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n", "def test_get_index_text(app):\n    domain = app.env.get_domain('py')\n    assert domain.get_index_text('module', ('Class', 'module')) == 'Class (class in module)'\n    assert domain.get_index_text('module', ('function', 'module')) == 'function() (in module module)'\n    assert domain.get_index_text(None, ('function', None)) == 'function; function()'\n    assert domain.get_index_text(None, ('Class.method', None)) == 'method() (Class method)'\n    assert domain.get_index_text(None, ('Class.property', None)) == 'property (Class property)'\n", "def test_method_without_parentheses(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (nodes.target,\n                          addnodes.index,\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                                    [desc_addname, \"example.\"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[3][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class method)', 'example.Class.meth', '', None)])\n    assert 'example.Class.meth' in domain.objects\n    assert domain.objects['example.Class.meth'] == ('index', 'example.Class.meth', 'method', False)\n"], "sample_967": ["def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://example.com/mathjax.js' in content\n", "def test_install_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://cdn.example.com/mathjax.js' in content\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://example.com/mathjax.js' in content\n    assert MATHJAX_URL not in content\n", "def test_mathjax2_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js' in content)\n    assert ('<script type=\"text/x-mathjax-config\">' not in content)\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'https://example.com/mathjax.js' in content\n"], "sample_970": ["def test_is_builtin_class_method_builtin():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(str, 'join') is True\n    assert inspect.is_builtin_class_method(list, 'append') is True\n", "def test_object_description_with_memory_address():\n    class CustomType:\n            self._value = value\n\n            return \"<CustomType at 0x12345678: %r>\" % self._value\n\n    obj = CustomType(1)\n    description = inspect.object_description(obj)\n    assert description == \"<CustomType: 1>\"\n", "def test_getorigbases():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B, A):\n        pass\n\n    class D(metaclass=types.new_class(\"Meta\", (type,))):\n        pass\n\n    class E:\n        __orig_bases__ = (D,)\n\n    class F(E):\n        pass\n\n    assert inspect.getorigbases(A) is None\n    assert inspect.getorigbases(B) is None\n    assert inspect.getorigbases(C) is None\n    assert inspect.getorigbases(D) is None\n    assert inspect.getorigbases(E) == (D,)\n    assert inspect.getorigbases(F) is None\n\n    with pytest.raises(TypeError):\n        inspect.getorigbases(1)\n\n    with pytest.raises(TypeError):\n        inspect.getorigbases(\"string\")\n\n    with pytest.raises(TypeError):\n        inspect.getorigbases([1, 2, 3])\n", "def test_getorigbases():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert inspect.getorigbases(A) is None\n    assert inspect.getorigbases(B) == (A,)\n    assert inspect.getorigbases(C) == (B,)\n", "def test_safe_getattr_with_infinite_recursion():\n    class Foo:\n        @property\n            return self.bar\n\n    obj = Foo()\n\n    try:\n        inspect.safe_getattr(obj, 'bar')\n    except AttributeError as exc:\n        assert exc.args[0] == 'bar'\n    else:\n        pytest.fail('AttributeError not raised')\n"], "sample_969": ["def test_restify_nested_union():\n    assert restify(Union[Union[int, str], None]) == (\":py:obj:`~typing.Optional`\\\\ \"\n                                                    \"[:py:obj:`~typing.Union`\\\\ \"\n                                                    \"[:py:class:`int`, :py:class:`str`]]\")\n    assert restify(Union[int, Union[str, None]]) == (\":py:obj:`~typing.Optional`\\\\ \"\n                                                    \"[:py:obj:`~typing.Union`\\\\ \"\n                                                    \"[:py:class:`int`, :py:class:`str`]]\")\n    assert restify(Union[Union[int, str], Union[float, None]]) == (\":py:obj:`~typing.Union`\\\\ \"\n                                                                  \"[:py:obj:`~typing.Union`\\\\ \"\n                                                                  \"[:py:class:`int`, :py:class:`str`], \"\n                                                                  \":py:obj:`~typing.Optional`\\\\ \"\n                                                                  \"[:py:class:`float`]]\")\n", "def test_restify_type_hints_ForwardRef():\n    class ForwardRef:\n            self.__forward_arg__ = arg\n\n    forward_ref = ForwardRef(\"str\")\n    assert restify(forward_ref) == \":py:class:`str`\"\n", "def test_restify_type_hints_broken__args__():\n    class BrokenType:\n        __args__ = 'not a list or tuple'\n\n    assert restify(BrokenType) == ':py:class:`tests.test_util_typing.BrokenType`'\n", "def test_restify_forward_ref():\n    class ForwardRefClass:\n        pass\n\n    forward_ref = ForwardRef(\"tests.test_util_typing.ForwardRefClass\")\n    assert restify(forward_ref) == \":py:class:`tests.test_util_typing.ForwardRefClass`\"\n", "def test_restify_type_ForwardRef_with_module():\n    from typing import ForwardRef  # type: ignore\n    class_module = 'module.class'\n    fr = ForwardRef(class_module)\n    assert restify(fr) == \":py:class:`%s`\" % class_module\n"], "sample_971": ["def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"custom prefix:\"):\n        logger.warning('message1')\n\n    assert 'custom prefix: WARNING: message1' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.info('message2')\n\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n    assert 'message2' in status.getvalue()\n    assert 'prefix:' not in status.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings('prefix:'):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'prefix: WARNING: message1\\n' in warning.getvalue()\n    assert 'prefix: WARNING: message2\\n' in warning.getvalue()\n\n    # check nested prefix\n    with logging.prefixed_warnings('prefix1:'):\n        with logging.prefixed_warnings('prefix2:'):\n            logger.warning('message3')\n\n    assert 'prefix2: WARNING: message3\\n' in warning.getvalue()\n\n    # check prefix is reset after context\n    logger.warning('message4')\n    assert 'WARNING: message4\\n' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1')\n    with logging.prefixed_warnings('prefix:'):\n        logger.warning('message2')\n\n    assert 'WARNING: message1' in warning.getvalue()\n    assert 'prefix: WARNING: message2' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    logger.warning('message1')\n    with logging.prefixed_warnings(\"prefix:\"):\n        logger.warning('message2')\n\n    assert 'WARNING: message1' in warning.getvalue()\n    assert 'prefix: WARNING: message2' in warning.getvalue()\n"], "sample_968": ["def test_find_obj_with_module_name_searchmode_1(app):\n    domain = app.env.get_domain('py')\n    obj1 = Mock()\n    obj2 = Mock()\n    obj1.objtype = 'function'\n    obj2.objtype = 'function'\n    domain.objects['my_module.my_function'] = ('mod', 'my_module.my_function', 'function', False)\n    domain.objects['my_function'] = ('mod', 'my_function', 'function', False)\n\n    assert domain.find_obj(app.env, 'my_module', None, 'my_function', None, 1) == \\\n        [('my_module.my_function', ('mod', 'my_module.my_function', 'function', False))]\n", "def test_info_field_list_with_escaped_pipe(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param age: blah blah | even more blah\\n\"\n            \"   :type age: int | str\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree,\n                (addnodes.index,\n                 [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_name, \"Class\"])],\n                         [desc_content, nodes.field_list, nodes.field, (nodes.field_name,\n                                                                        nodes.field_body)])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Parameters\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :param age:\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"age\"],\n                 \" -- blah blah \\| even more blah\"))\n\n    # :type age:\n    assert_node(doctree[1][1][0][0][1][0][1],\n                ([addnodes.literal_strong, \"age\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 [addnodes.literal_emphasis, \" | \"],\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 \")\"))\n    assert_node(doctree[1][1][0][0][1][0][1][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\", **{\"py:class\": \"Class\"})\n    assert_node(doctree[1][1][0][0][1][0][1][4], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\", **{\"py:class\": \"Class\"})\n", "def test_get_index_text(app):\n    domain = app.env.get_domain('py')\n    obj = Mock(docname='index', full_name='my_function', objtype='function')\n    index_text = domain.get_index_text(obj.docname, (obj.full_name, None))\n    assert index_text == 'my_function() (in module index)'\n\n    obj = Mock(docname='index', full_name='MyClass', objtype='class')\n    index_text = domain.get_index_text(obj.docname, (obj.full_name, None))\n    assert index_text == 'MyClass (class in index)'\n\n    obj = Mock(docname='index', full_name='MyException', objtype='exception')\n    index_text = domain.get_index_text(obj.docname, (obj.full_name, None))\n    assert index_text == 'MyException'\n\n    obj = Mock(docname='index', full_name='my_variable', objtype='data')\n    index_text = domain.get_index_text(obj.docname, (obj.full_name, None))\n    assert index_text == 'my_variable (in module index)'\n", "def test_find_obj_with_refspecific(app):\n    domain = app.env.get_domain('py')\n\n    # case 1: non-fuzzy lookup\n    assert domain.find_obj(app.env, None, None, 'io', None, 0) == []\n    assert domain.find_obj(app.env, 'io', None, 'StringIO', None, 0) == []\n\n    # case 2: fuzzy lookup\n    assert domain.find_obj(app.env, None, None, 'io', None, 1) == []\n    assert domain.find_obj(app.env, 'io', None, 'StringIO', None, 1) == []\n\n    # case 3: fuzzy lookup with refspecific\n    assert domain.find_obj(app.env, None, None, 'io.StringIO', None, 1) == []\n    assert domain.find_obj(app.env, None, None, '.StringIO', 'class', 1) == []\n", "def test_python_domain_get_full_qualified_name_for_class(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"   .. py:method:: my_method\")\n    doctree = restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    node = doctree[1][1][0][1][0][0]\n    assert domain.get_full_qualified_name(node) == 'MyClass.my_method'\n"], "sample_972": ["def test_stringify_type_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n    assert stringify(ForwardRef(\"myint\"), \"fully-qualified\") == \"myint\"\n    assert stringify(ForwardRef(\"myint\"), \"smart\") == \"myint\"\n", "def test_get_type_hints():\n        pass\n\n    type_hints = get_type_hints(my_function)\n    assert type_hints == {'a': int, 'b': str, 'return': float}\n\n    class MyClass:\n            pass\n\n    type_hints = get_type_hints(MyClass.my_method)\n    assert type_hints == {'a': int, 'b': str, 'return': float}\n\n    # Test with default values for globalns and localns\n    type_hints = get_type_hints(my_function, {}, {})\n    assert type_hints == {'a': int, 'b': str, 'return': float}\n\n    # Test with None values for globalns and localns\n    type_hints = get_type_hints(my_function, None, None)\n    assert type_hints == {'a': int, 'b': str, 'return': float}\n\n    # Test with ForwardRef\n    from typing import ForwardRef\n    forward_ref = ForwardRef('MyForwardRef')\n    type_hints = get_type_hints(lambda: forward_ref)\n    assert type_hints == {'return': forward_ref}\n\n    # Test with broken class\n    class BrokenClass:\n        __annotations__ = {'a': 'non-existent-type'}\n\n    type_hints = get_type_hints(BrokenClass)\n    assert type_hints == {}\n\n    # Test with KeyError\n    class KeyErrorClass:\n            raise KeyError\n\n    type_hints = get_type_hints(KeyErrorClass)\n    assert type_hints == {}\n", "def test_get_type_hints():\n        pass\n\n    type_hints = get_type_hints(my_function)\n    assert type_hints == {'a': int, 'b': str, 'return': bool}\n\n    class MyClass:\n            pass\n\n    type_hints = get_type_hints(MyClass.my_method)\n    assert type_hints == {'a': int, 'b': str, 'return': bool}\n\n        pass\n\n    type_hints = get_type_hints(my_function_with_typevar)\n    assert type_hints == {'a': T, 'b': T, 'return': T}\n\n        pass\n\n    type_hints = get_type_hints(my_function_with_string_annotation)\n    assert type_hints == {'a': int, 'b': str, 'return': bool}\n", "def test_get_type_hints():\n        return str(my_arg)\n\n    type_hints = get_type_hints(my_function)\n    assert type_hints['my_arg'] == int\n    assert type_hints['return'] == str\n\n        return str(my_arg)\n\n    type_hints = get_type_hints(my_function_without_type_hints)\n    assert type_hints == {}\n\n    class MyClass:\n            return str(my_arg)\n\n    type_hints = get_type_hints(MyClass.my_method)\n    assert type_hints['my_arg'] == int\n    assert type_hints['return'] == str\n", "def test_get_type_hints():\n        pass\n\n    hints = get_type_hints(func)\n    assert hints['x'] == int\n    assert hints['y'] == str\n    assert hints['return'] == type(None)\n\n        pass\n\n    hints = get_type_hints(func_with_defaults)\n    assert hints['x'] == int\n    assert hints['y'] == str\n    assert hints['return'] == type(None)\n\n        # type: (int, str) -> None\n        pass\n\n    hints = get_type_hints(func_with_annotations)\n    assert hints['x'] == int\n    assert hints['y'] == str\n    assert hints['return'] == type(None)\n\n    class MyClass:\n            pass\n\n    hints = get_type_hints(MyClass.__init__)\n    assert hints['x'] == int\n    assert hints['y'] == str\n    assert 'return' not in hints\n"], "sample_974": ["def test_ccode_For():\n    i = symbols('i', integer=True)\n    s = For(i, Range(0, 10), [Assignment(x, x + i)])\n    c = ccode(s)\n    assert c == (\"for (int i=0; i<10; i++){\\n\"\n                 \"   x = i + x;\\n\"\n                 \"}\")\n", "def test_ccode_For():\n    i, m = symbols('i m', integer=True)\n    target = symbols('target')\n    func = symbols('func', cls=Function)\n    for_loop = For(i, Range(m), func(i))\n    expected = (\n        'for (int i=0; i<m; i++){\\n'\n        '   func(i);\\n'\n        '}'\n    )\n    code = ccode(for_loop)\n    assert code == expected\n", "def test_ccode_For():\n    from sympy import For, aug_assign, symbols\n    x, i = symbols('x i')\n    f = For(i, Range(10), [aug_assign(x, '+', i)])\n    assert ccode(f) == (\n        'for (int i=0; i<10; i++){\\n'\n        '   x += i;\\n'\n        '}'\n    )\n", "def test_ccode_For():\n    i = symbols('i', integer=True)\n    s = symbols('s')\n    elements = [x**i, x**(i+1), x**(i+2)]\n    from sympy import For\n    for_loop = For(i, Range(s), Assignment(y, x**i))\n    c = ccode(for_loop)\n    expected = (\n        'for (int i=0; i<s; i++){\\n'\n        '   y = pow(x, i);\\n'\n        '}'\n    )\n    assert c == expected\n", "def test_ccode_For():\n    i = symbols('i', integer=True)\n    f = For(i, Range(0, 10), [x + i])\n    assert ccode(f) == (\n        'for (int i=0; i<10; i++){\\n'\n        '   x + i;\\n'\n        '}')\n"], "sample_973": ["def test_is_builtin_class_method_builtin_types():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(str, '__len__') is True\n    assert inspect.is_builtin_class_method(list, '__getitem__') is True\n    assert inspect.is_builtin_class_method(dict, '__setitem__') is True\n    assert inspect.is_builtin_class_method(set, '__contains__') is True\n    assert inspect.is_builtin_class_method(tuple, '__iter__') is True\n", "def test_object_description_with_memory_address():\n    instance = object()\n    description = inspect.object_description(instance)\n    assert ' at 0x' not in description\n", "def test_is_builtin_class_method_builtin():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(str, '__init__') is True\n    assert inspect.is_builtin_class_method(list, '__init__') is True\n    assert inspect.is_builtin_class_method(dict, '__init__') is True\n    assert inspect.is_builtin_class_method(set, '__init__') is True\n    assert inspect.is_builtin_class_method(tuple, '__init__') is True\n    assert inspect.is_builtin_class_method(type, '__init__') is True\n", "def test_getdoc_inherited_decorated_classmethod():\n    class Foo:\n        @classmethod\n            \"\"\"\n            docstring\n                indented text\n            \"\"\"\n\n    class Bar(Foo):\n        @classmethod\n        @functools.lru_cache()\n            # inherited and decorated classmethod\n            pass\n\n    assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") is None\n    assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") == Foo.meth.__doc__\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n    assert inspect.is_builtin_class_method(int, 'my_method') is False\n    assert inspect.is_builtin_class_method(list, 'append') is True\n    assert inspect.is_builtin_class_method(str, 'upper') is True\n\n"], "sample_975": ["compilation error", "def test_issue_11768():\n    x = Symbol('x')\n    assert nsolve(x**2/(1 - x)/(1 - 2*x)**2 - 100, x, 0.46) == 0.46792545969349058\n", "def test_nsolve_dict():\n    # Test that nsolve accepts and returns a dictionary\n    x = Symbol('x')\n    y = Symbol('y')\n    f1 = 3 * x**2 - 2 * y**2 - 1\n    f2 = x**2 - 2 * x + y**2 + 2 * y - 8\n    f = Matrix((f1, f2)).T\n    x0 = {x: -1, y: 1}\n    result = nsolve(f, (x, y), x0)\n    assert isinstance(result, dict)\n    assert result[x] == Float(nsolve(f1, x, -1).subs(y, nsolve(f2, y, 1)))\n    assert result[y] == Float(nsolve(f2, y, 1).subs(x, nsolve(f1, x, -1)))\n", "def test_nsolve_linear():\n    x, y = symbols('x y')\n    # test both lambdified and non-lambdified forms\n    eq = Eq(2*x - 2, 2)\n    assert nsolve(eq, x, -10) == -9.0\n    assert nsolve(eq.lhs - eq.rhs, x, -10) == -9.0\n    # test for Floats in the equation\n    eq = Eq(2.0*x - 2, 2)\n    assert nsolve(eq, x, -10) == -9.0\n    # test for multiple symbols\n    eq = Eq(2*x - 2, 2*y)\n    assert nsolve(eq, x, -10, dict=True) == {x: -9.0}\n", "def test_unrad():\n    from sympy.solvers.solvers import unrad\n    x, y, z, a, b, c, d = symbols('x y z a b c d')\n\n    # issue 3024\n    assert unrad(sqrt(x) + sqrt(x + 1) + y, x, y) == (\n        y**3 + 2*y**2*x - 3*y**2 - 3*y*x**2 + 12*y*x - 8*y +\n        x**3 - 6*x**2 + 8*x + 8, [])\n\n    # issue 8248\n    assert unrad(sqrt(x) + root(x, 3) - 2, x) == (\n        (x**3 - 12*x**2 + 48*x - 64)*(x**3 + 36*x**2 + 432*x + 1728) - 46656,\n        [_p, _p**6 - x])\n    assert unrad(sqrt(x) + root(x, 3) - 3, x) == (\n        x**6 - 54*x**5 + 1215*x**4 - 14580*x**3 + 109350*x**2 - 437400*x + 531441,\n        [])\n    assert unrad(sqrt(x) + root(x, 3) + y, x, y) == (\n        4*y**9 + 12*y**8*x - 66*y**8 - 264*y**7*x + 264*y**7 + 576*y**6*x**2 -\n        4320*y**6*x + 4104*y**6 + 8640*y**5*x**3 - 92880*y**5*x**2 + 207360*y**5*x -\n        98415*y**5 - 10368*y**4*x**4 + 195264*y**4*x**3 - 870912*y**4*x**2 +\n        1016064*y**4*x + 316536*y**4 - 7680*y**3*x**5 - 90240*y**3*x**4 + 2280960*y**3*x**3 -\n        10321920*y**3*x**2 + 8032128*y**3*x + 2015538*y**3 - 17280*y**2*x**6 -\n        380"], "sample_977": ["def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 10), (y, 1, 10))) == \\\n        \"Hold[Sum[x^2 + y^2, {x, 1, 10}, {y, 1, 10}]]\"\n    assert mcode(Sum(sin(x), (x, 1, oo))) == \"Hold[Sum[Sin[x], {x, 1, Infinity}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 10), (y, 1, 10))) == \\\n        \"Hold[Sum[x^2 + y^2, {x, 1, 10}, {y, 1, 10}]]\"\n    assert mcode(Sum(sin(x), (x, 1, oo))) == \"Hold[Sum[Sin[x], {x, 1, Infinity}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 10), (y, 1, 10))) == \"Hold[Sum[x^2 + y^2, {x, 1, 10}, {y, 1, 10}]]\"\n    assert mcode(Sum(sin(x), (x, 1, oo))) == \"Hold[Sum[Sin[x], {x, 1, Infinity}]]\"\n    assert mcode(Sum(1/x, (x, 1, 10))) == \"Hold[Sum[1/x, {x, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x*y, (x, 1, 10))) == \"Hold[Sum[x*y, {x, 1, 10}]]\"\n    assert mcode(Sum(x*y, (x, 1, 10), (y, 1, 10))) == \"Hold[Sum[x*y, {x, 1, 10}, {y, 1, 10}]]\"\n    assert mcode(Sum(sin(x), (x, 1, 10))) == \"Hold[Sum[Sin[x], {x, 1, 10}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 10), (y, 1, 10))) == \"Hold[Sum[x^2 + y^2, {x, 1, 10}, {y, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 10), (y, 1, 10))) == \\\n        \"Hold[Sum[x^2 + y^2, {x, 1, 10}, {y, 1, 10}]]\"\n"], "sample_976": ["def testDummy_assumptions():\n    d = Dummy('d', real=True, positive=True)\n    assert d.is_real\n    assert d.is_positive\n    d = Dummy('d', real=True, positive=False)\n    assert d.is_real\n    assert not d.is_positive\n", "def test_var():\n    raises(ValueError, lambda: var(''))\n    raises(ValueError, lambda: var(','))\n    raises(ValueError, lambda: var('x,,y,,z'))\n    raises(ValueError, lambda: var(('x', '', 'y', '', 'z')))\n\n    x = var('x')\n    assert x.is_Symbol\n    assert 'x' in globals()\n\n    x, y, z = var('x,y,z')\n    assert x.is_Symbol\n    assert 'x' in globals()\n    assert y.is_Symbol\n    assert 'y' in globals()\n    assert z.is_Symbol\n    assert 'z' in globals()\n\n    x, y, z = var('x y z')\n    assert x.is_Symbol\n    assert 'x' in globals()\n    assert y.is_Symbol\n    assert 'y' in globals()\n    assert z.is_Symbol\n    assert 'z' in globals()\n\n    x, y, z = var('x,y,z', real=True)\n    assert x.is_real and y.is_real and z.is_real\n", "def test_symbols_with_exclude():\n    x, y, z = symbols('x,y,z')\n    a = Wild('a', exclude=[x, y, z])\n    b = Wild('b', exclude=[x, y])\n    assert symbols('a,b', cls=Wild, exclude=[x, y, z]) == (a, b)\n\n    a, b, c = symbols('a,b,c', cls=Wild, exclude=[x, y])\n    assert a.exclude == [x, y]\n    assert b.exclude == [x, y]\n    assert c.exclude == [x, y]\n", "def test_var():\n    raises(ValueError, lambda: var(''))\n    raises(ValueError, lambda: var('1'))\n    raises(ValueError, lambda: var('1 2'))\n    raises(ValueError, lambda: var('1,2'))\n    raises(ValueError, lambda: var('x,1'))\n    raises(ValueError, lambda: var('1,x'))\n    raises(ValueError, lambda: var('x,1,y'))\n    raises(ValueError, lambda: var('x,y,1,z'))\n    raises(ValueError, lambda: var('x1, x1'))\n\n    x = var('x')\n    assert x.is_Symbol\n    assert x.name == 'x'\n\n    x, y = var('x, y')\n    assert x.is_Symbol\n    assert x.name == 'x'\n    assert y.is_Symbol\n    assert y.name == 'y'\n\n    a, b, c = var('a:z:3')\n    assert a.is_Symbol\n    assert a.name == 'a'\n    assert b.is_Symbol\n    assert b.name == 'b'\n    assert c.is_Symbol\n    assert c.name == 'c'\n\n    var('x')\n    assert 'x' in globals()\n    x = Symbol('x')\n    assert globals()['x'] == x\n\n    var('x,y')\n    assert 'x' in globals()\n    assert 'y' in globals()\n    x = Symbol('x')\n    y = Symbol('y')\n    assert globals()['x'] == x\n    assert globals()['y'] == y\n\n    var('x,y', real=True)\n    assert 'x' in globals()\n    assert 'y' in globals()\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert globals()['x'] == x\n    assert globals()['y'] == y\n\n    var('x:z')\n    assert 'x' in globals()\n    assert 'y' in globals()\n    assert 'z' in globals()\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert globals()['x'] == x\n    assert globals()['y'] == y\n    assert globals()['z'] == z\n\n    var('x:z', real=True)\n    assert 'x' in globals()\n    assert 'y' in globals()\n    assert 'z' in globals()\n    x = Symbol('x', real=True)\n    y = Symbol('y", "def test_var():\n    raises(ValueError, lambda: var(''))\n    raises(ValueError, lambda: var(','))\n\n    x, y, z = var('x y z')\n    assert x == Symbol('x')\n    assert y == Symbol('y')\n    assert z == Symbol('z')\n\n    x, y, z = var('x,y,z')\n    assert x == Symbol('x')\n    assert y == Symbol('y')\n    assert z == Symbol('z')\n\n    x, y, z = var('x,y,z', real=True)\n    assert x == Symbol('x', real=True)\n    assert y == Symbol('y', real=True)\n    assert z == Symbol('z', real=True)\n"], "sample_979": ["def test_MatrixExpr_as_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.as_explicit() == ImmutableMatrix([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]]])\n    assert A.T.as_explicit() == ImmutableMatrix([[A[0, 0], A[1, 0]], [A[0, 1], A[1, 1]]])\n    assert (A*A).as_explicit() == ImmutableMatrix([\n        [A[0, 0]*A[0, 0] + A[0, 1]*A[1, 0], A[0, 0]*A[0, 1] + A[0, 1]*A[1, 1]],\n        [A[1, 0]*A[0, 0] + A[1, 1]*A[1, 0], A[1, 0]*A[0, 1] + A[1, 1]*A[1, 1]]])\n", "def test_MatrixExpr_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    A_real, A_imag = A.as_real_imag()\n    assert A_real.shape == A.shape\n    assert A_imag.shape == A.shape\n    assert (A_real + S.ImaginaryUnit*A_imag).simplify() == A\n", "def test_Identity_matrixelement():\n    n = symbols('n')\n    i, j = symbols('i j')\n    I = Identity(n)\n    assert I[i, j] != 0\n    assert I[i, i] == 1\n    assert I[i, j].subs({i: 1, j: 2}) == 0\n    assert I[i, j].subs({i: 1, j: 1}) == 1\n", "def test_MatrixElement_eval_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    A_expr = Matrix([[x, 2], [3*x, 4]])\n    assert A[0, 0].subs(A, A_expr)._eval_derivative(x) == 1\n    assert A[1, 0].subs(A, A_expr)._eval_derivative(x) == 3\n    assert A[0, 1].subs(A, A_expr)._eval_derivative(x) == 0\n    assert A[1, 1].subs(A, A_expr)._eval_derivative(x) == 0\n", "def test_matrixelement_as_real_imag():\n    w_real, w_imag = w[0, 0].as_real_imag()\n    assert w_real == (w[0, 0] + w[0, 0].conjugate()) / 2\n    assert w_imag == (w[0, 0] - w[0, 0].conjugate()) / (2 * S.ImaginaryUnit)\n"], "sample_978": ["def test_repeated_degree_2():\n    d = 2\n    knots = [0, 0, 1, 2, 2, 3, 4, 4]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((1 - 2*x + x**2, Interval(0, 1).contains(x)),\n                                   (0, True))\n    assert splines[1] == Piecewise((x**2/2, Interval(0, 1).contains(x)),\n                                   (Rational(3, 2) - 3*x + x**2/2, Interval(1, 2).contains(x)),\n                                   (0, True))\n    assert splines[2] == Piecewise((Rational(-3, 2) + 3*x - x**2, Interval(1, 2).contains(x)),\n                                   (2 - x, Interval(2, 3).contains(x)),\n                                   (0, True))\n    assert splines[3] == Piecewise((Rational(9, 2) - 6*x + x**2/2, Interval(2, 3).contains(x)),\n                                   (0, True))\n    assert splines[4] == Piecewise((Rational(-11, 2) + 5*x - x**2/2, Interval(2, 3).contains(x)),\n                                   (Rational(8, 2) - 4*x + x**2/2, Interval(3, 4).contains(x)),\n                                   (0, True))\n    assert splines[5] == Piecewise((Rational(8, 2) - 4*x + x**2, Interval(3, 4).contains(x)),\n                                   (0, True))\n", "def test_bspline_basis():\n    d = 0\n    knots = range(5)\n    n = 0\n    spline = bspline_basis(d, knots, n, x)\n    assert spline == Piecewise((1, Interval(0, 1).contains(x)),\n                               (0, True))\n\n    d = 1\n    knots = range(5)\n    n = 0\n    spline = bspline_basis(d, knots, n, x)\n    assert spline == Piecewise((x, Interval(0, 1).contains(x)),\n                               (2 - x, Interval(1, 2).contains(x)),\n                               (0, True))\n\n    d = 1\n    knots = [0, 0, 2, 3, 4]\n    n = 0\n    spline = bspline_basis(d, knots, n, x)\n    assert spline == Piecewise((-x/2 + 1, Interval(0, 2).contains(x)),\n                               (0, True))\n\n    d = 3\n    knots = range(5)\n    n = 0\n    spline = bspline_basis(d, knots, n, x)\n    b0 = Piecewise(\n        (x**3/6, Interval(0, 1).contains(x)),\n        (Rational(2, 3) - 2*x + 2*x**2 - x**3/2, Interval(1, 2).contains(x)),\n        (Rational(-22, 3) + 10*x - 4*x**2 + x**3/2, Interval(2, 3).contains(x)),\n        (Rational(32, 3) - 8*x + 2*x**2 - x**3/6, Interval(3, 4).contains(x)),\n        (0, True)\n    )\n    assert spline == b0\n", "def test_degree_0_with_repeated_knots():\n    d = 0\n    knots = [0, 0, 1, 2, 2, 3, 3, 3]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((1, Interval(0, 1).contains(x)),\n                                   (0, True))\n    assert splines[1] == Piecewise((1, Interval(1, 2).contains(x)),\n                                   (0, True))\n    assert splines[2] == Piecewise((1, Interval(2, 3).contains(x)),\n                                   (0, True))\n", "def test_repeated_degree_2():\n    d = 2\n    knots = [0, 0, 0, 1, 2, 2, 3, 4, 4, 4]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((1 - 2*x + x**2, Interval(0, 1).contains(x)),\n                                   (0, True))\n    assert splines[1] == Piecewise((2*x - x**2, Interval(0, 1).contains(x)),\n                                   (2 - 2*x + x**2/2, Interval(1, 2).contains(x)),\n                                   (0, True))\n    assert splines[2] == Piecewise((x**2/2, Interval(1, 2).contains(x)),\n                                   (3 - 2*x, Interval(2, 3).contains(x)),\n                                   (0, True))\n    assert splines[3] == Piecewise((x**2/2 - x + 1, Interval(2, 3).contains(x)),\n                                   (8 - 4*x + x**2/2, Interval(3, 4).contains(x)),\n                                   (0, True))\n    assert splines[4] == Piecewise((x**2/2 - 2*x + 2, Interval(3, 4).contains(x)),\n                                   (0, True))\n", "def test_repeated_degree_2():\n    d = 2\n    knots = [0, 0, 1, 2, 2, 3, 4, 4]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((1 - 2*x + x**2, Interval(0, 1).contains(x)),\n                   (0, True))\n    b1 = Piecewise((2*x - x**2, Interval(0, 1).contains(x)),\n                   (2 - 4*x + 2*x**2, Interval(1, 2).contains(x)),\n                   (0, True))\n    b2 = Piecewise((-1 + 4*x - 2*x**2, Interval(1, 2).contains(x)),\n                   (0, True))\n    b3 = Piecewise((4 - 4*x + x**2, Interval(2, 3).contains(x)),\n                   (0, True))\n    b4 = Piecewise((-4 + 4*x - x**2, Interval(2, 3).contains(x)),\n                   (8 - 8*x + 2*x**2, Interval(3, 4).contains(x)),\n                   (0, True))\n    b5 = Piecewise((-4 + 8*x - 2*x**2, Interval(3, 4).contains(x)),\n                   (0, True))\n    assert splines[0] == b0\n    assert splines[1] == b1\n    assert splines[2] == b2\n    assert splines[3] == b3\n    assert splines[4] == b4\n    assert splines[5] == b5\n"], "sample_980": ["def test_cycle_structure():\n    p = Permutation(3)\n    assert p.cycle_structure == {1: 4}\n    p = Permutation(0, 6, 3)(1, 2)\n    assert p.cycle_structure == {2: 1, 3: 1, 1: 2}\n    p = Permutation(1, 2, 3)\n    assert p.cycle_structure == {3: 1}\n    p = Permutation(1, 4, 7, 2, 3, 6, 5, 0)\n    assert p.cycle_structure == {8: 1}\n", "def test_is_Empty():\n    assert Permutation([]).is_Empty\n    assert not Permutation([1]).is_Empty\n    assert not Permutation([0, 1, 2]).is_Empty\n    p = Permutation(3)\n    p._array_form = []\n    assert p.is_Empty\n", "def test_from_inversion_vector():\n    p = Permutation([3, 2, 1, 0])\n    v = p.inversion_vector()\n    assert p == Permutation.from_inversion_vector(v)\n    assert p == Permutation.from_inversion_vector(list(v))\n    assert p == Permutation.from_inversion_vector(tuple(v))\n    raises(ValueError, lambda: Permutation.from_inversion_vector([1, 2, 0]))\n", "def test_mul_edge_cases():\n    # Test multiplication with an empty permutation\n    p = Permutation([0, 2, 1])\n    q = Permutation([])\n    assert p * q == p\n    assert q * p == p\n\n    # Test multiplication with a permutation of size 1\n    p = Permutation([0, 2, 1])\n    q = Permutation([0])\n    assert p * q == p\n    assert q * p == p\n\n    # Test multiplication with a permutation that is the identity\n    p = Permutation([0, 2, 1])\n    q = Permutation([0, 1, 2])\n    assert p * q == p\n    assert q * p == p\n\n    # Test multiplication with a permutation that is its own inverse\n    p = Permutation([0, 2, 1])\n    q = Permutation([0, 1, 2])\n    assert p * p == q\n", "def test_mul_inv():\n    p = Permutation([1, 0, 2, 3])\n    q = Permutation([2, 3, 1, 0])\n    assert p.mul_inv(q) == Permutation([2, 0, 3, 1])\n    assert q.mul_inv(p) == Permutation([3, 1, 0, 2])\n"], "sample_981": ["def test_from_inversion_vector():\n    p = Permutation.from_inversion_vector([3, 2, 1, 0, 0])\n    assert p.array_form == [3, 2, 1, 0, 4, 5]\n    raises(ValueError, lambda: Permutation.from_inversion_vector([3, 2, 1]))\n    raises(ValueError, lambda: Permutation.from_inversion_vector([3, 2, 1, 5]))\n    assert Permutation.from_inversion_vector([]) == Permutation(0)\n", "def test_printing_array():\n    Permutation.print_cyclic = False\n    p1 = Permutation([0, 2, 1])\n    assert repr(p1) == 'Permutation([0, 2, 1])'\n    assert str(p1) == '[0, 2, 1]'\n    p2 = Permutation()\n    assert repr(p2) == 'Permutation([])'\n    assert str(p2) == '[]'\n    p3 = Permutation([1, 2, 0, 3])\n    assert repr(p3) == 'Permutation([1, 2, 0, 3])'\n    Permutation.print_cyclic = True\n", "def test_cycle_structure():\n    p = Permutation(0, 1, 2)\n    assert p.cycle_structure == {3: 1}\n    p = Permutation(0, 1)(2, 3)\n    assert p.cycle_structure == {2: 2}\n    p = Permutation(1, 2, 3)(4, 5)\n    assert p.cycle_structure == {3: 1, 2: 1, 1: 2}\n    p = Permutation(1, 2, 3)(4, 5, 6)\n    assert p.cycle_structure == {3: 2}\n    p = Permutation(1, 2, 3)(4, 5)(6)\n    assert p.cycle_structure == {3: 1, 2: 1, 1: 3}\n", "compilation error", "def test_printing_array():\n    Permutation.print_cyclic = False\n    p1 = Permutation([0, 2, 1])\n    assert repr(p1) == 'Permutation([0, 2, 1])'\n    assert str(p1) == '[0, 2, 1]'\n    p2 = Permutation()\n    assert repr(p2) == 'Permutation([])'\n    assert str(p2) == '[]'\n    p3 = Permutation([1, 2, 0, 3])\n    assert repr(p3) == 'Permutation([1, 2, 0, 3])'\n    Permutation.print_cyclic = True\n"], "sample_982": ["def test_primeomega():\n    assert primeomega(2) == 1\n    assert primeomega(2 * 3) == 2\n    assert primeomega(2 * 3 * 5) == 3\n    assert primeomega(3 * 25) == 3\n    assert [primeomega(p) for p in primerange(1, 10)] == [1, 1, 1, 1]\n    assert primeomega(fac(50)) == 148\n    assert primeomega(2 ** 9941 - 1) == 1\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2 ** 31 - 1) == 1\n    assert summation(primeomega(n), (n, 2, 30)) == 67\n", "def test_primeomega():\n    assert primeomega(2) == 1\n    assert primeomega(2 * 3) == 2\n    assert primeomega(2 * 3 * 5) == 3\n    assert primeomega(3 * 25) == 3\n    assert [primeomega(p) for p in primerange(1, 10)] == [1, 1, 1, 1]\n    assert primeomega(fac(50)) == 68\n    assert primeomega(2 ** 9941 - 1) == 1\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2 ** 31 - 1) == 1\n    assert summation(primeomega(n), (n, 2, 30)) == 68\n", "def test_primeomega():\n    assert primeomega(2) == 1\n    assert primeomega(2 * 3) == 2\n    assert primeomega(2 * 3 * 5) == 3\n    assert primeomega(3 * 25) == 3\n    assert [primeomega(p) for p in primerange(1, 10)] == [1, 1, 1, 1]\n    assert primeomega(fac(50)) == 68\n    assert primeomega(2 ** 9941 - 1) == 1\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2 ** 31 - 1) == 1\n    assert summation(primeomega(n), (n, 2, 30)) == 69\n", "def test_primeomega():\n    assert primeomega(2) == 1\n    assert primeomega(2 * 3) == 2\n    assert primeomega(2 * 3 * 5) == 3\n    assert primeomega(3 * 25) == 3\n    assert [primeomega(p) for p in primerange(1, 10)] == [1, 1, 1, 1]\n    assert primeomega(fac(50)) == 68\n    assert primeomega(2 ** 9941 - 1) == 1\n    n = Symbol('n', integer=True)\n    assert primeomega(n)\n    assert primeomega(n).subs(n, 2 ** 31 - 1) == 1\n    assert summation(primeomega(n), (n, 2, 30)) == 77\n", "def test_factorint_with_rational():\n    assert factorint(S(1)/1, visual=True) == 1\n    assert factorint(S(1)/2, visual=True) == 1/(2**1)\n    assert factorint(S(3)/4, visual=True) == 3**1/(2**2)\n    assert factorint(S(-3)/4, visual=True) == -3**1/(2**2)\n    assert factorint(S(1)/1, multiple=True) == []\n    assert factorint(S(1)/2, multiple=True) == [1/2]\n    assert factorint(S(3)/4, multiple=True) == [3, 1/2, 1/2]\n    assert factorint(S(-3)/4, multiple=True) == [-3, 1/2, 1/2]\n\n    assert factorint(S(1)/1) == {}\n    assert factorint(S(1)/2) == {2: -1}\n    assert factorint(S(3)/4) == {2: -2, 3: 1}\n    assert factorint(S(-3)/4) == {-1: 1, 2: -2, 3: 1}\n    assert factorint(S(3)/4, limit=3) == {2: -2, 3: 1}\n    assert factorint(S(18)/24) == {2: -1, 3: 2}\n"], "sample_984": ["def test_MatPow():\n    from sympy import MatrixSymbol\n    assert str(MatrixSymbol(\"X\", 2, 2)**2) == \"X**2\"\n    assert str(MatrixSymbol(\"X\", 2, 2)**-1) == \"X**-1\"\n    assert str(MatrixSymbol(\"X\", 2, 2)**0) == \"X**0\"\n", "def test_Sample():\n    from sympy.stats import Die\n    X = Die('X', 6)\n    assert str(X.pspace.sample) == 'Sample([1, 2, 3, 4, 5, 6])'\n", "def test_trace():\n    A, B = symbols('A B', commutative=False)\n    expr = Tr(A*B)\n    assert sstr(expr) == 'Tr(A*B)'\n", "def test_HadamardProduct():\n    from sympy import MatrixSymbol\n    assert str(MatrixSymbol(\"X\", 2, 2) * MatrixSymbol(\"Y\", 2, 2)) == \"X*Y\"\n    assert str(MatrixSymbol(\"X\", 2, 2) * MatrixSymbol(\"Y\", 2, 3)) == \"X*Y\"\n    assert str(MatrixSymbol(\"X\", 2, 3) * MatrixSymbol(\"Y\", 2, 2)) == \"ShapeError: Matrices X and Y are not compatible for multiplication\"\n", "def test_Transpose():\n    from sympy.matrices.expressions import MatrixSymbol\n    assert str(Transpose(MatrixSymbol(\"X\", 2, 2))) == 'X.T'\n"], "sample_983": ["def test_sparse_matrix_applyfunc():\n    M = SparseMatrix(2, 2, {(0, 0): 1, (1, 1): 2})\n    assert M.applyfunc(lambda x: x**2) == SparseMatrix(2, 2, {(0, 0): 1, (1, 1): 4})\n    assert M.applyfunc(lambda x: 0) == SparseMatrix(2, 2, {})\n    M = SparseMatrix(3, 3, {(0, 0): 1, (1, 1): 2, (2, 2): 3})\n    assert M.applyfunc(lambda x: x + 1) == SparseMatrix(3, 3, {(0, 0): 2, (1, 1): 3, (2, 2): 4})\n", "def test_is_symmetric():\n    M = SparseMatrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    assert M.is_symmetric()\n    M = SparseMatrix([[1, 1, 0], [1, 1, 0], [0, 0, 1]])\n    assert M.is_symmetric()\n    M = SparseMatrix([[1, 2, 0], [3, 1, 0], [0, 0, 1]])\n    assert not M.is_symmetric()\n", "def test_is_symmetric():\n    x = Symbol('x')\n    assert SparseMatrix(1, 1, [1]).is_symmetric()\n    assert SparseMatrix(2, 2, [1, 0, 0, 1]).is_symmetric()\n    assert not SparseMatrix(2, 2, [1, 1, 0, 1]).is_symmetric()\n    assert SparseMatrix(3, 3, [1, x, x, x, 2, x, x, x, 3]).is_symmetric()\n    assert not SparseMatrix(3, 3, [1, x, x, x, 3, x, x, x, 3]).is_symmetric()\n", "def test_sparse_matrix_get_item():\n    A = SparseMatrix(((1, 2), (3, 4)))\n    assert A[0, 0] == 1\n    assert A[0, 1] == 2\n    assert A[1, 0] == 3\n    assert A[1, 1] == 4\n    assert A[0] == 1\n    assert A[1] == 2\n    assert A[2] == 3\n    assert A[3] == 4\n    raises(IndexError, lambda: A[4])\n    raises(IndexError, lambda: A[4, 4])\n    raises(TypeError, lambda: A[\"a\", \"b\"])\n    assert A[:, 0] == SparseMatrix([1, 3])\n    assert A[:, 1] == SparseMatrix([2, 4])\n    assert A[0, :] == SparseMatrix([[1, 2]])\n    assert A[1, :] == SparseMatrix([[3, 4]])\n    assert A[:, :] == A\n    raises(TypeError, lambda: A[:, \"a\"])\n    raises(TypeError, lambda: A[\"a\", :])\n", "def test_liupc():\n    S = SparseMatrix([\n        [1, 0, 3, 2],\n        [0, 0, 1, 0],\n        [4, 0, 0, 5],\n        [0, 6, 7, 0]])\n    R, parent = S.liupc()\n    assert R == [[0], [], [0], [1, 2]]\n    assert parent == [4, 3, 4, 4]\n    assert S.row_structure_symbolic_cholesky() == [[0], [], [0], [1, 2]]\n    assert S.nnz() == 7\n    assert S.RL == [(0, 0, 1), (0, 2, 3), (0, 3, 2), (1, 2, 1), (2, 0, 4),\n        (2, 3, 5), (3, 1, 6), (3, 2, 7)]\n    assert S.CL == [(0, 0, 1), (2, 0, 4), (3, 1, 6), (0, 2, 3), (1, 2, 1), (3, 2, 7), (0, 3, 2), (2, 3, 5)]\n"], "sample_985": ["def test_as_expr_set():\n    # Issue #22273: using a set in as_expr\n    from sympy import cos, sin\n    a = {cos(x), sin(x)}\n    b = {sin(x), cos(x)}\n    assert a.as_expr() == b.as_expr()\n", "def test_real_root():\n    from sympy import real_root, symbols, I\n    x, y, z = symbols('x y z', real=True)\n    assert real_root(-8, 3) == -2\n    assert real_root(-8, 3, 2) == -2*(-1)**(2/3)\n    assert real_root(-4, 2) == -2\n    assert real_root(-4, 2, 1) == 2*I\n    assert real_root(-27, 3) == -3\n    assert real_root(-32, 5) == -2\n    assert real_root(-32, 5, 2) == -2*(-1)**(2/5)\n", "def test_real_root():\n    from sympy import real_root, Rational\n\n    assert real_root(-8, 3) == -2\n    assert real_root(-2, 3) == real_root(-2, 3)\n    assert real_root(-Rational(27, 8), 3) == -Rational(3, 2)\n\n    from sympy.abc import x\n    assert real_root(x, 3).subs(x, -8) == -2\n    assert real_root(x**3, 3).subs(x, -2) == -2\n", "def test_doitEvalf():\n    x = symbols('x')\n    f1 = sin(x) + cos(x)\n    assert f1.doit() == f1\n    assert f1.doit(evalf=True) == f1\n    assert f1.doit(deep=False) == f1\n    assert f1.doit(deep=False, evalf=True) == f1\n    assert f1.doit(num_terms=2) == f1\n    assert f1.evalf() == f1\n    assert f1.evalf(deep=False) == f1\n    assert f1.evalf(n=2) == f1\n    assert f1.evalf(2) == f1\n", "def test_rewrite_Heaviside():\n    x, y, z = symbols('x y z')\n    assert Max(x, y).rewrite(Heaviside) == x*Heaviside(x - y) + y*Heaviside(-(x - y))\n    assert Min(x, y).rewrite(Heaviside) == x*Heaviside(-(x - y)) + y*Heaviside(x - y)\n    assert Max(x, y, z).rewrite(Heaviside) == x*Heaviside(x - y)*Heaviside(x - z) + \\\n        y*Heaviside(y - x)*Heaviside(y - z) + z*Heaviside(z - x)*Heaviside(z - y)\n    assert Min(x, y, z).rewrite(Heaviside) == x*Heaviside(-(x - y))*Heaviside(-(x - z)) + \\\n        y*Heaviside(-(y - x))*Heaviside(-(y - z)) + z*Heaviside(-(z - x))*Heaviside(-(z - y))\n"], "sample_986": ["def test_evalf_maxprec():\n    raises(ValueError, lambda: pi.evalf(-1))\n    raises(ValueError, lambda: pi.evalf(0))\n    raises(ValueError, lambda: pi.evalf(1000001))\n    assert pi.evalf(1000000) == pi.evalf()\n", "def test_issue_10555():\n    from sympy import integrate, exp\n    assert integrate(exp(-x**2), (x, -oo, oo)).evalf() == 1.7724538509055159\n    assert integrate(exp(-x**2), (x, -oo, oo)).evalf(50) == 1.772453850905515881919427556567825376987\n", "def test_evalf_equality():\n    from sympy import Eq, Symbol\n    x = Symbol('x')\n    assert Eq(x, 1.000000000000001).evalf() == False\n    assert Eq(x, 1.000000000000001).evalf(chop=True) == Eq(x, 1)\n", "def test_evalf_substitution_with_nested_symbols():\n    x = Symbol('x')\n    y = Symbol('y')\n    expr = x + y\n    assert expr.evalf(subs={x: 1, y: 2}) == 3\n    assert expr.evalf(subs={x: 1, y: x + 1}) == 3\n    assert expr.evalf(subs={x: 1, y: x + y}) == 3  # this should raise an error?\n    raises(ValueError, lambda: expr.evalf(subs={x: 1, y: x + y}))\n", "def test_evalf_atan():\n    assert NS(atan(1), 15) == '0.785398163397448'\n    assert NS(atan(1e10), 15) == '1.57079632679490'\n    assert NS(atan(-1e10), 15) == '-1.57079632679490'\n    assert NS(atan(1e-10), 15) == '1.00000000000000e-10'\n    assert NS(atan(-1e-10), 15) == '-1.00000000000000e-10'\n    assert NS(atan(0), 15) == '0'\n    assert NS(atan(I), 15) == '0.881378051021399*I'\n    assert NS(atan(-I), 15) == '-0.881378051021399*I'\n    assert NS(atan(1 + 2*I), 15) == '1.31126264447598 + 0.881378051021399*I'\n    assert NS(atan(1 - 2*I), 15) == '-1.31126264447598 - 0.881378051021399*I'\n"], "sample_987": ["def test_evalf_hypsum():\n    from sympy.functions.special.hyper import hyper\n    from mpmath import mpf, almosteq\n    for a, b, c, z, eps in [\n        ([1, 2], [3], 4, mpf(0.5), 1e-50),\n        ([1, 2], [3, 4], 5, mpf(0.5), 1e-50),\n        ([1, 2, 3], [4, 5, 6], 7, mpf(0.5), 1e-50),\n        ([1, 2], [3], 4, mpf(1.5), 1e-50),\n        ([1, 2], [3, 4], 5, mpf(1.5), 1e-50),\n        ([1, 2, 3], [4, 5, 6], 7, mpf(1.5), 1e-50),\n    ]:\n        h = hyper(a, b, z)\n        assert almosteq(h, hyper(a, b, c, z, eps=eps), abs_eps=eps)\n", "def test_evalf_bernoulli():\n    assert bernoulli(1).evalf() == -S.Half\n    assert bernoulli(2).evalf() == Rational(1, 6)\n    assert bernoulli(3).evalf() == 0\n    assert bernoulli(4).evalf() == Rational(-1, 30)\n    assert bernoulli(5).evalf() == 0\n    assert bernoulli(10).evalf() == Rational(5, 66)\n    assert bernoulli(12).evalf() == Rational(-691, 2730)\n    assert bernoulli(0, 1).evalf() == 1\n    assert bernoulli(1, 1).evalf() == -S.Half + 1\n    assert bernoulli(2, 1).evalf() == Rational(1, 6) + 1\n    assert bernoulli(3, 1).evalf() == 1\n    assert bernoulli(4, 1).evalf() == Rational(-1, 30) + 1\n    assert bernoulli(5, 1).evalf() == 1\n    assert bernoulli(10, 1).evalf() == Rational(5, 66) + 1\n    assert bernoulli(12, 1).evalf() == Rational(-691, 2730) + 1\n", "def test_evalf_NaN():\n    assert (0/0).evalf() is S.NaN\n    assert (S.Infinity/S.Infinity).evalf() is S.NaN\n    assert (S.NegativeInfinity/S.NegativeInfinity).evalf() is S.NaN\n    assert (S.NegativeInfinity/S.Infinity).evalf() is S.NaN\n    assert (S.Infinity/S.NegativeInfinity).evalf() is S.NaN\n    assert (S.Infinity + S.NegativeInfinity).evalf() is S.NaN\n    assert (S.NegativeInfinity + S.Infinity).evalf() is S.NaN\n", "def test_evalf_piecewise():\n    f = Piecewise((1, x > 0), (0, True))\n    assert f.evalf(subs={x: 1}) == 1\n    assert f.evalf(subs={x: -1}) == 0\n    f = Piecewise((x, x > 0), (-x, True))\n    assert f.evalf(subs={x: 1}) == 1\n    assert f.evalf(subs={x: -1}) == 1\n", "def test_evalf_hypsum():\n    from sympy.functions.special.hyper import hyper\n    assert NS(hyper((1, 2), (3,), 0.5), 5) == '1.7184'\n    assert NS(hypsum(1/factorial(n), n, 0), 5) == '2.7183'\n"], "sample_988": ["def test_issue_19663():\n    assert Eq(2, zoo) == S.false\n    assert Eq(zoo, 2) == S.false\n    assert Eq(zoo, zoo) == S.true\n    assert Ne(2, zoo) == S.true\n    assert Ne(zoo, 2) == S.true\n    assert Ne(zoo, zoo) == S.false\n", "def test_integer_nthroot():\n    assert integer_nthroot(10**26, 26) == (10, True)\n    assert integer_nthroot(10**28, 26) == (None, False)\n    assert integer_nthroot(2**10, 10) == (2, True)\n    assert integer_nthroot(2**12, 10) == (None, False)\n    assert integer_nthroot(2**15, 3) == (None, False)\n    assert integer_nthroot(2**15, 5) == (2**3, True)\n", "def test_issue_13851():\n    x = symbols('x')\n    f = Function('f')\n    assert Eq(f(2*x), f(x + x)) is S.true\n    assert Eq(f(2*x), f(2*x)) is S.true\n", "def test_igcdex():\n    assert igcdex(2, 3) == (-1, 1, 1)\n    assert igcdex(10, 12) == (-1, 1, 2)\n    assert igcdex(100, 2004) == (-20, 1, 4)\n    assert igcdex(0, 0) == (0, 1, 0)\n", "def test_issue_15318():\n    from sympy.solvers.inequalities import solve_univariate_inequality\n    x = Symbol('x', real=True)\n    assert solve_univariate_inequality(x**2 >= 0, x) == Interval(-oo, oo)\n    assert solve_univariate_inequality(x**2 > 0, x) == Union(Interval(-oo, 0, True, True), Interval(0, oo, True, True))\n    assert solve_univariate_inequality(x**2 <= 0, x) == FiniteSet(0)\n    assert solve_univariate_inequality(x**2 < 0, x) == EmptySet\n"], "sample_989": ["def test_mod_inverse_iterated():\n    p = prev = 2\n    for i in range(2, 1000):\n        p = nextprime(p)\n        n = mod_inverse(i, p)\n        assert (i * n) % p == 1\n        assert n < p\n        assert n > 0\n        assert n != prev\n        prev = n\n", "def test_issue_20271():\n    # Test for fix of issue 20271\n    from sympy import limit, sin, cos\n    x = Symbol('x')\n    assert limit(1 / x, x, S.Infinity) == 0\n    assert limit(sin(x) / x, x, S.Infinity) == 0\n    assert limit(cos(x) / x, x, S.Infinity) == 0\n    assert limit(1 / (x**2), x, S.Infinity) == 0\n    assert limit(sin(x) / (x**2), x, S.Infinity) == 0\n    assert limit(cos(x) / (x**2), x, S.Infinity) == 0\n", "def test_Float_issue_13470():\n    import pickle\n    f = Float((1, 5, 0))\n    assert f == -5\n    assert f._mpf_ == (1, 5, 0, 3)\n    f2 = pickle.loads(pickle.dumps(f))\n    assert f2._mpf_ == (1, 5, 0, 3)\n", "def test_mod_inverse_no_division_property():\n    n = 11\n    a = 3\n    inverse = mod_inverse(a, n)\n    assert pow(a * inverse, 1, n) == 1\n", "def test_mpf_norm_nonzero_mantissa():\n    # Test mpf_norm with a non-zero mantissa\n    from mpmath.libmp.libmpf import _normalize\n    import mpmath.libmp as mlib\n    rnd = mlib.round_nearest\n    mpf = (1, long(1), -123, -1, 53, rnd)  \n    assert _normalize(mpf, 53) != (0, long(0), 0, 0)\n    mpf = (1, long(1), -456, -2, 53, rnd)  \n    assert _normalize(mpf, 53) != (0, long(0), 0, 0)\n    mpf = (1, long(1), -789, -3, 53, rnd)  \n    assert _normalize(mpf, 53) != (0, long(0), 0, 0)\n"], "sample_990": ["def test_hyp_as_real_imag():\n    x, y = symbols('x,y', real=True)\n    for func in (sinh, cosh, tanh, coth, sech, csch):\n        for arg in (1, x, y):\n            expr = func(arg)\n            assert expr.as_real_imag() == (expr, 0)\n            assert expr.as_real_imag(deep=False) == (expr, 0)\n", "def test_issue_21258():\n    x = Symbol('x')\n    assert asinh(x).as_leading_term(x) == x\n    assert acosh(x).as_leading_term(x) == I*pi/2\n    assert atanh(x).as_leading_term(x) == x\n    assert acoth(x).as_leading_term(x) == I*pi/2\n    assert asech(x).as_leading_term(x) == log(2/x)\n    assert acsch(x).as_leading_term(x) == 1/x\n", "def test_is_even_odd():\n    x = Symbol('x')\n    assert sinh(x).is_even == False\n    assert sinh(x).is_odd == True\n\n    assert cosh(x).is_even == True\n    assert cosh(x).is_odd == False\n\n    assert tanh(x).is_even == False\n    assert tanh(x).is_odd == True\n\n    assert coth(x).is_even == False\n    assert coth(x).is_odd == True\n\n    assert sech(x).is_even == True\n    assert sech(x).is_odd == False\n\n    assert csch(x).is_even == False\n    assert csch(x).is_odd == True\n\n    assert asinh(x).is_even == False\n    assert asinh(x).is_odd == True\n\n    assert acosh(x).is_even == False\n    assert acosh(x).is_odd == False\n\n    assert atanh(x).is_even == False\n    assert atanh(x).is_odd == True\n\n    assert acoth(x).is_even == False\n    assert acoth(x).is_odd == True\n\n    assert asech(x).is_even == False\n    assert asech(x).is_odd == False\n\n    assert acsch(x).is_even == False\n    assert acsch(x).is_odd == True\n", "def test_csch_as_real_imag():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert csch(x + I*y).as_real_imag() == \\\n        (sinh(x)*cosh(x)/(sinh(x)**2 + cos(y)**2), -sin(y)*cos(y)/(sinh(x)**2 + cos(y)**2))\n", "def test_real_imag():\n    x, y = symbols('x,y', real=True)\n    for func in (sinh, cosh, tanh, coth, csch, sech):\n        z = func(x + I*y)\n        assert z.as_real_imag() == (func(x)*cos(y), func(x)*sin(y))\n        assert z.as_real_imag(deep=False) == (func(x)*cos(y), func(x)*sin(y))\n"], "sample_992": ["def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(CustomPrintedObject()) == 'mpmath'\n    cpo = CustomPrintedObject()\n    assert p.doprint(cpo) == 'mpmath'\n    expr = x**y\n    assert p.doprint(expr) == 'x**y'\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert p.module_imports == {'mpmath': {'acos', 'pi'}}\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    expr = acos(x)\n    assert 'numpy' not in p.module_imports\n    assert 'mpmath' in p.doprint(expr)\n    cpo = CustomPrintedObject()\n    assert 'mpmath' in p.doprint(cpo)\n", "def test_pycode_custom_printed_object():\n    obj = CustomPrintedObject()\n    assert NumPyPrinter().doprint(obj) == 'numpy'\n    assert MpmathPrinter().doprint(obj) == 'mpmath'\n    assert PythonCodePrinter().doprint(obj) != 'numpy'  # Shouldn't resolve to numpy\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    assert p.doprint(x**y) == 'x**y'\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert p.doprint(acos(x)) == 'mpmath.acos(x)'\n    assert 'mpmath' in p.module_imports\n    assert p.doprint(Assignment(x, 2)) == 'x = 2'\n    assert p.doprint(Assignment(x, CustomPrintedObject())) == 'x = mpmath'\n", "def test_MpmathPrinter():\n    p = MpmathPrinter()\n    assert p.doprint(pi) == 'mpmath.pi'\n    assert p.module_imports == {'mpmath': {'pi'}}\n    assert p.doprint(acos(x)) == 'mpmath.acos(x)'\n    assert 'acos' in p.module_imports['mpmath']\n    expr = CustomPrintedObject()\n    assert p.doprint(expr) == 'mpmath'\n"], "sample_991": ["def test_product_with_zero():\n    assert product(0, (n, 1, 5)) == 0\n    assert product(n, (n, 0, 5)) == 0\n    assert product(n**2, (n, -3, 3)) == 0\n", "def test_issue_is_convergent():\n    n = Symbol('n', integer=True)\n    p = Product(2**(-n), (n, 1, oo))\n    assert p.is_convergent() is S.false\n\n    p = Product(2**(1/n), (n, 1, oo))\n    assert p.is_convergent() is S.false\n\n    p = Product(1/n**(S(1)/2), (n, 1, oo))\n    assert p.is_convergent() is S.false\n\n    p = Product(1/n**(S(2)), (n, 1, oo))\n    assert p.is_convergent() is S.false\n\n    p = Product(cos(pi/n), (n, 1, oo))\n    assert p.is_convergent() is S.true\n\n    p = Product(cos(1/n), (n, 1, oo))\n    assert p.is_convergent() is S.false\n", "def test_product_with_non_integer_step():\n    # issue 21346\n    n = Symbol('n')\n    k = Symbol('k')\n    p = Product(n, (k, 0, n, S(1)/2))\n    assert p.doit() == Product(n, (k, 0, n, S(1)/2))\n", "def test_issue_change_index_and_reverse_order():\n    x, y, n = symbols('x y n', integer=True)\n\n    assert Product(x, (x, 1, 5)).change_index(x, -x).reverse_order(0) == \\\n           Product(1/(-x), (x, -5, -1))\n    assert Product(x*y, (x, 1, 5), (y, 1, 3)).change_index(x, -x).reverse_order(0, 1) == \\\n           Product((-x)*y, (x, -5, -1), (y, 4, 0))\n    assert Product(x, (x, 1, n)).change_index(x, -x).reverse_order(0) == \\\n           Product(1/(-x), (x, -n, -1))\n    assert Product(x*y, (x, 1, n), (y, 1, 3)).change_index(x, -x).reverse_order(0, 1) == \\\n           Product((-x)*y, (x, -n, -1), (y, 4, 0))\n    assert Product(x*y, (x, 1, 5), (y, 1, n)).change_index(x, -x).change_index(y, -y).reverse_order(0, 1) == \\\n           Product((-x)*(-y), (x, -5, -1), (y, -n, -1))\n", "def test_Product_is_zero():\n    assert Product(0, (n, 1, 5)).is_zero\n    assert not Product(1, (n, 1, 5)).is_zero\n    assert not Product(n, (n, 1, 5)).is_zero\n    assert not Product(1/n, (n, 1, 5)).is_zero\n"], "sample_993": ["def test_FreeGroupElm_cyclic_subword():\n    w = x**5*y*x**2*y**-4*x\n    assert w.cyclic_subword(2, 6) == x**3*y\n    assert w.cyclic_subword(7, 15) == y*x**2*y**-4*x**2*y\n    assert w.cyclic_subword(10, 10) == F.identity\n\n", "def test_FreeGroupElm_power_of():\n    assert (x*y).power_of(x*y)\n    assert (x*y)**2 == x*y*x*y\n    assert not (x*y).power_of(x*z)\n    assert (x**-3*y*x**3).power_of(x**-3*y*x**3)\n    assert not (x*y).power_of(x)\n    assert (x**2).power_of(x)\n    assert (x**-3).power_of(x**-1)\n    assert (x**-3).power_of(x)\n", "def test_FreeGroupElm_cyclic_reduction():\n    w1 = x**2*y**3*x**-1\n    w2 = x**-3*y**-1*x**5\n    assert w1.cyclic_reduction() == x*y**3\n    assert w2.cyclic_reduction() == y**-1*x**2\n    assert w1.identity_cyclic_reduction() == x*y**3\n    assert w2.identity_cyclic_reduction() == x**2*y**-1\n    w3 = x**2*y**-1*x**-1\n    assert w3.cyclic_reduction(removed=True) == (y**-1, x**2)\n    assert w3.identity_cyclic_reduction() == x*y**-1\n", "def test_FreeGroupElm_eliminate_words():\n    w = x**5*y*x**2*y**-4*x\n    assert w.eliminate_words([x, y]) == F.identity\n    w3 = x**2*y**3*x**-1*y\n    assert w3.eliminate_words({x: x**2, y: y**-1}) == x**4*y**-3*x**-2*y**-1\n    assert w3.eliminate_words({x: y, y: x}) == y**2*x**3*y**-1*x\n    assert w3.eliminate_words([x*y]) == y**-2*x*y**3\n", "def test_FreeGroupElm_subword():\n    w = x**5*y*x**2*y**-4*x\n    assert w.subword(2, 6) == x**3*y\n    assert w.subword(3, 3) == F.identity\n    assert w.subword(1, 1) == F.identity\n\n    raises(ValueError, lambda: w.subword(-1, 3))\n    raises(ValueError, lambda: w.subword(1, 15))\n    raises(ValueError, lambda: w.subword(3, 1))\n"], "sample_994": ["def test_issue_21166():\n    assert sympify(123456789012345) == Integer(123456789012345)\n    assert Integer(12345678901234567890)._to_mpmath_integer() == 12345678901234567890\n", "def test_mod_inverse_symbolic():\n    x = symbols('x')\n    n = symbols('n', positive=True, integer=True)\n    assert mod_inverse(3, n) == mod_inverse(3, n)\n    assert mod_inverse(x, n) == mod_inverse(x, n)\n    assert mod_inverse(x, 3) == mod_inverse(x, 3)\n    assert mod_inverse(2, n) == mod_inverse(2, n)\n    assert mod_inverse(x**2 + 1, n) == mod_inverse(x**2 + 1, n)\n", "def test_issue_13669():\n    assert S(123456789).factors() == {3: 2, 3607: 1, 3803: 1}\n", "def test_mod_inverse_issue():\n    x = 3\n    m = 11\n    assert mod_inverse(x, m) * x % m == 1\n    x = 3\n    m = 15\n    assert mod_inverse(-x, m) * x % m == -1 % m\n", "def test_mod_inverse_with_Rational():\n    assert mod_inverse(Rational(3, 5), 11) == 4\n    assert mod_inverse(Rational(-3, 5), 11) == -4\n    raises(ValueError, lambda: mod_inverse(Rational(1, 2), 4))\n    raises(ValueError, lambda: mod_inverse(Rational(3, 4), 5))\n"], "sample_995": ["def test_huge_Float_precision():\n    # Make sure Floats can handle huge precisions\n    huge = Float('1.' + '9'*1000, '')\n    assert huge._prec == 1000\n    assert len(huge._mpf_[1]) == 1000\n    assert huge == Float('1.' + '9'*1000, 1000)\n    assert huge != Float('1.' + '8'*1000, 1000)\n", "def test_AlgebraicNumber_evaluate():\n    from sympy import symbols, sqrt, Eq\n\n    # explicit root expression\n    x = symbols('x')\n    a = AlgebraicNumber(sqrt(2) + 3, [1, 0, -6, 1])\n    assert a.evalf() == a._expr.evalf()\n\n    # implicit root expression\n    a = AlgebraicNumber(Rational(1, 2) + sqrt(2)/2, [1, -1, 1])\n    assert a.evalf() == a._expr.evalf()\n\n    # not a root expression, but a simple expression\n    a = AlgebraicNumber(sqrt(2) + 1)\n    assert a.evalf() == a._expr.evalf()\n\n    # not a root expression, but an equation\n    a = AlgebraicNumber(Eq(x**2, 4), [x])\n    assert a.evalf() == a._expr.args[0].evalf()\n", "compilation error", "def test_issue_12345_rational_0_denominator():\n    raises(ZeroDivisionError, lambda: Rational(1, 0))\n    raises(ZeroDivisionError, lambda: Rational(S(1), 0))\n    raises(ZeroDivisionError, lambda: Rational(1, S(0)))\n    raises(ZeroDivisionError, lambda: Rational(S(1), S(0)))\n", "def test_issue_13813():\n    assert Rational(\"123 456/789\") == Rational(98018, 789)\n    assert Rational(\"22/7\") == Rational(22, 7)\n    assert Rational(\"-11/13\") == Rational(-11, 13)\n    assert Rational(\"2 + 3/4\") == Rational(11, 4)\n    assert Rational(\"-2 + 3/4\") == Rational(-5, 4)\n    assert Rational(\"2 + -3/4\") == Rational(5, 4)\n    assert Rational(\"-2 + -3/4\") == Rational(-11, 4)\n    assert Rational(\"2-3/4\") == Rational(5, 4)\n    assert Rational(\"-2-3/4\") == Rational(-11, 4)\n    assert Rational(\"2 - 3/4\") == Rational(5, 4)\n    assert Rational(\"-2 - 3/4\") == Rational(-11, 4)\n    assert Rational(\"2 - -3/4\") == Rational(11, 4)\n    assert Rational(\"-2 - -3/4\") == Rational(-5, 4)\n    assert Rational(\"3 / 4\") == Rational(3, 4)\n    assert Rational(\"3/ -4\") == Rational(-3, 4)\n    assert Rational(\"3 /-4\") == Rational(-3, 4)\n    assert Rational(\" -3 / 4\") == Rational(-3, 4)\n    assert Rational(\" -3/4\") == Rational(-3, 4)\n    assert Rational(\"-3 /4\") == Rational(-3, 4)\n    assert Rational(\"2.\") == 2\n    assert Rational(\"2\") == 2\n    assert Rational(\".5\") == Rational(1, 2)\n    assert Rational(\"0.5\") == Rational(1, 2)\n    assert Rational(\"1.5\") == Rational(3, 2)\n    assert Rational(\"1 1/2\") == Rational(3, 2)\n    assert Rational(\"1+1/2\") == Rational(3, 2)\n"], "sample_996": ["def test_product_with_zero_term():\n    n = Symbol('n', integer=True)\n    assert product(0, (n, 1, 10)) == 0\n    assert Product(0, (n, 1, 10)).doit() == 0\n    assert product(n*(n-5), (n, 1, 6)) == 0\n    assert Product(n*(n-5), (n, 1, 6)).doit() == 0\n", "def test_issue_18473():\n    m = Symbol('m')\n    p = Product(m, (m, 3, 1)).doit()\n    assert p == 1/(Product(m, (m, 1, 3)).doit())\n", "def test_product_doit_issues():\n    from sympy.abc import i\n    # issue 20295\n    assert Product(1/(1+1/i), (i, 1, 3)).doit() == 6/7\n    # issue 20296\n    assert Product(2*i, (i, 1, 3)).doit() == 24\n", "def test_issue_product_function_change():\n    n = Symbol('n')\n    k = Symbol('k')\n    expr1 = Product(n + 1 / 2**k, (k, 0, n-1))\n    expr2 = Product(n + 1 / 2**k, (n, 0, k-1))\n    assert expr1.function == n + 1 / 2**k\n    assert expr2.function == n + 1 / 2**k\n    assert expr1.limits == ((k, 0, n-1),)\n    assert expr2.limits == ((n, 0, k-1),)\n    assert expr1.doit().subs(n, 2) == S(15)/2\n    assert expr2.doit().subs(k, 2) == S(5)/2\n", "def test_issue_doit_finite_product():\n    n = Symbol('n')\n    k = Symbol('k')\n    p = Product(2*k, (k, 1, n)).doit()\n    assert p == 2**n * factorial(n)\n"], "sample_997": ["def test_issue_20765():\n    transformations = standard_transformations + (implicit_application,)\n    x = Symbol('x')\n    assert parse_expr(\"sin x**2\", transformations=transformations) == sin(x**2)\n    assert parse_expr(\"sin(x**2)\", transformations=transformations) == sin(x**2)\n    assert parse_expr(\"sin x**2 + cos x**2\", transformations=transformations) == sin(x**2) + cos(x**2)\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    f = Function('f')\n    inputs = {\n        'f := x -> x**2': Eq(f, lambda x: x**2),\n        'f = x -> x**2': Eq(f, lambda x: x**2)\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    sin = Function('sin')\n    tan = Function('tan')\n    assert parse_expr(\"10sin**2 x**2 + 3xyz + tan theta\",\n        transformations=transformations) == 10*sin(x**2)**2 + 3*x*y*z + tan(Symbol('theta'))\n    assert parse_expr(\"2sin x + 3cos y + exp z\", \n        transformations=transformations) == 2*sin(x) + 3*Function('cos')(y) + exp(z)\n    assert parse_expr(\"10sin**2 x**2 + 3xyz + tan theta\", \n        transformations=transformations) == 10*sin(x**2)**2 + 3*x*y*z + tan(Symbol('theta'))\n    assert parse_expr(\"(x + y)sin x\", transformations=transformations) == (x + y)*sin(x)\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + \\\n                        (implicit_multiplication_application, )\n\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    sin = Function('sin')\n    cos = Function('cos')\n    tan = Function('tan')\n\n    assert parse_expr(\"10sin**2 x**2 + 3xyz + tan theta\",\n        transformations=transformations) == 10*sin(x**2)**2 + 3*x*y*z + tan(Symbol('theta'))\n    assert parse_expr(\"xyz\", transformations=transformations) == x*y*z\n    assert parse_expr(\"2xy\", transformations=transformations) == 2*x*y\n    assert parse_expr(\"3sin x cos x\", transformations=transformations) == 3*sin(x)*cos(x)\n    assert parse_expr(\"3sin x y cos x\", transformations=transformations) == 3*sin(x)*y*cos(x)\n", "def test_implicit_application():\n    transformations = standard_transformations + \\\n                        (implicit_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"cot x + csc x\", transformations=transformations) == cot(x) + csc(x)\n"], "sample_998": ["def test_issue_14340():\n    assert latex(exp_polar(I*pi)) == r\"e^{i \\pi}\"\n", "def test_issue_13965():\n    assert latex(S.Reals - FiniteSet(x)) == r\"\\mathbb{R} \\setminus \\left\\{x\\right\\}\"\n    assert latex(S.Reals - Interval(0, 1)) == r\"\\mathbb{R} \\setminus \\left[0, 1\\right]\"\n", "def test_issue_10246():\n    assert latex(Symbol('alpha_1')) == r'\\alpha_{1}'\n    assert latex(Symbol('beta_12')) == r'\\beta_{12}'\n    assert latex(Symbol('Dij_0')) == r'D_{ij 0}'\n    assert latex(Symbol('Znu_0')) == r'Z_{\\nu 0}'\n    assert latex(Symbol('Z_0')) == r'Z_{0}'\n    assert latex(Symbol('D_0')) == r'D_{0}'\n    assert latex(Symbol('D_0_nu')) == r'D_{0 \\nu}'\n    assert latex(Symbol('D_0__nu')) == r'D_{0}^{\\nu}'\n    assert latex(Symbol('C_D_0')) == r'C_{D 0}'\n    assert latex(Symbol('C__D_0')) == r'C^{D 0}'\n    assert latex(Symbol('C__D__0')) == r'C^{D}_{0}'\n    assert latex(Symbol('D__0')) == r'D^{0}'\n    assert latex(Symbol('D__0_nu')) == r'D^{0}_{\\nu}'\n    assert latex(Symbol('D__0__nu')) == r'D^{0 \\nu}'\n    assert latex(Symbol('D__0__nu__k')) == r'D^{0 \\nu}_{k}'\n", "def test_issue_19392():\n    assert latex(S.Mod) == r'\\mathop{\\%\\!}'\n    assert latex(Mod(x, y)) == r'x\\bmod{y}'\n", "def test_latex_conjugate_with_symbol_names():\n    x = Symbol('x')\n    symbol_names = {x: \"x_i\"}\n    assert latex(conjugate(x), symbol_names=symbol_names) == r\"\\overline{x_i}\"\n    assert latex(conjugate(2*x), symbol_names=symbol_names) == r\"2 \\overline{x_i}\"\n    assert latex(conjugate(x**2), symbol_names=symbol_names) == r\"\\overline{x_i^{2}}\"\n"], "sample_999": ["def test_issue_14311():\n    from sympy.tensor.array import MutableSparseNDimArray\n    from sympy.tensor.array.expressions.from_indexed import _parse_indexed_expression\n    from sympy.tensor.array.expressions.array_expressions import ZeroMatrix\n    A = IndexedBase('A', shape=(3, 3))\n    i, j = symbols('i j', integer=True)\n    indexed_expr = A[i, j]\n    array_expr = _parse_indexed_expression(indexed_expr)\n    assert latex(array_expr) == r\"\\left[\\begin{matrix}A_{0, 0} & A_{0, 1} & A_{0, 2}\\\\A_{1, 0} & A_{1, 1} & A_{1, 2}\\\\A_{2, 0} & A_{2, 1} & A_{2, 2}\\end{matrix}\\right]\"\n    assert latex(ZeroMatrix((3, 3))) == r\"\\left[\\begin{matrix}0 & 0 & 0\\\\0 & 0 & 0\\\\0 & 0 & 0\\end{matrix}\\right]\"\n    # Test with a different indexing starting point:\n    array_expr_custom_start = MutableSparseNDimArray(shape=(3, 3))\n    array_expr_custom_start[(1, 1)] = 1\n    assert latex(array_expr_custom_start) == r\"\\left[\\begin{matrix}0 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 0\\end{matrix}\\right]\"\n", "def test_issue_14725():\n    e = -2*x\n    assert latex(e)  == r'- 2 x'\n", "def test_latex_transpose():\n    from sympy.matrices import MatrixSymbol\n    assert latex(Transpose(x)) == r\"x^T\"\n    assert latex(Transpose(MatrixSymbol('M', 3, 3))) == r\"M^T\"\n    assert latex(Transpose(MatrixSymbol('M', 3, 3))**2) == r\"\\left(M^T\\right)^{2}\"\n", "def test_issue_16560():\n    from sympy import Symbol, Function, Eq, latex\n\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z', zero=True)\n\n    f = Function('f')\n\n    eq = Eq(x + y, z)\n    assert latex(f(eq)) == r'f{\\left (x + y = 0 \\right )}'\n", "def test_latex_Set():\n    assert latex(Set()) == r\"\\emptyset\"\n    assert latex(Set(x)) == r\"\\left\\{x\\right\\}\"\n    assert latex(Set(x, y)) == r\"\\left\\{x, y\\right\\}\"\n    assert latex(Set(x, y, z)) == r\"\\left\\{x, y, z\\right\\}\"\n"], "sample_1000": ["def test_octave_LambertW():\n    x = symbols('x')\n    assert mcode(LambertW(x, -1)) == 'lambertw(-1, x)'\n    assert mcode(LambertW(x, 0)) == 'lambertw(x)'\n    assert mcode(LambertW(x, 1)) == 'lambertw(1, x)'\n    assert mcode(LambertW(x, 2)) == 'lambertw(2, x)'\n", "def test_user_functions():\n    x = symbols('x')\n    f = Function('f')\n    g = Function('g')\n    user_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    A = MatrixSymbol('A', 1, 3)\n    assert mcode(f(x) + g(x) + g(A), user_functions=user_functions) == \\\n        \"existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn(A)\"\n", "def test_octave_matrix_slice():\n    A = Matrix([[1, 2, 3], [4, 5, 6]])\n    assert mcode(A[:, 0]) == \"[1; 4]\"\n    assert mcode(A[:, 1:]) == \"[2 3; 5 6]\"\n    assert mcode(A[0, :]) == \"[1 2 3]\"\n    assert mcode(A[1:, :]) == \"[4 5 6]\"\n    assert mcode(A[0:2, 0:2]) == \"[1 2; 4 5]\"\n    assert mcode(A[0:2, 1:3]) == \"[2 3; 5 6]\"\n", "def test_octave_user_functions():\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n    g = Function('g')\n\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n\n    expr1 = f(x) + g(x)\n    assert octave_code(expr1, user_functions=custom_functions) == \\\n        \"existing_octave_fcn(x) + my_fcn(x)\"\n\n    expr2 = f(x) + g(Matrix([[x, y], [y, x]]))\n    assert octave_code(expr2, user_functions=custom_functions) == \\\n        \"existing_octave_fcn(x) + my_mat_fcn([x y; y x])\"\n", "def test_tensor_indexed():\n    from sympy.tensor import IndexedBase, Idx\n    A = IndexedBase('A')\n    i = Idx('i', 3)\n    j = Idx('j', 4)\n    assert mcode(A[i, j]) == 'A(sub2ind(size(A), i, j))'\n"], "sample_1003": ["def test_Method_postprocess():\n    opt = {'method': 'something'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'something'}\n", "def test_Method_postprocess():\n    opt = {'method': 'something'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'something'}\n", "def test_Method_postprocess():\n    opt = {'method': 'something'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'something'}\n", "def test_Method_postprocess():\n    opt = {'method': 'method_name'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'method_name'}\n", "def test_Method_postprocess():\n    opt = {'method': 'something'}\n    Method.postprocess(opt)\n\n    assert opt == {'method': 'something'}\n"], "sample_1001": ["def test_latex_FreeModule():\n    from sympy.polys.domains import QQ\n    R = QQ.old_poly_ring(x, y)\n    F = R.free_module(2)\n    assert latex(F) == '{\\mathbb{Q}\\left[x, y\\right]}^{2}'\n    assert latex(F.submodule([x, y], [1, x**2])) == \\\n        r\"\\left< {\\left[ {x},{y} \\right]},{\\left[ {1},{x^{2}} \\right]} \\right>\"\n\n", "def test_issue_15517():\n    a = MatrixSymbol('a', 3, 3)\n    b = MatrixSymbol('b', 3, 3)\n    B = BlockMatrix([[a, b]])\n    assert latex(B) == r'\\begin{bmatrix}a & b\\\\ \\end{bmatrix}'\n", "def test_latex_RandomDomain():\n    from sympy.stats import Normal, Die, Exponential, pspace, where\n    X = Normal('x1', 0, 1)\n    assert latex(where(X > 0)) == r\"Domain: 0 < x_{1} \\wedge x_{1} < \\infty\"\n\n    D = Die('d1', 6)\n    assert latex(where(D > 4)) == r\"Domain: d_{1} = 5 \\vee d_{1} = 6\"\n\n    A = Exponential('a', 1)\n    B = Exponential('b', 1)\n    assert latex(\n        pspace(Tuple(A, B)).domain) == \\\n        r\"Domain: 0 \\leq a \\wedge 0 \\leq b \\wedge a < \\infty \\wedge b < \\infty\"\n", "def test_issue_15880():\n    from sympy.tensor.array import Array\n    from sympy import Matrix\n    M = Matrix([[x, 0], [0, x]])\n    assert latex(Array(M)) == r\"\\left[\\begin{matrix}x & 0\\\\0 & x\\end{matrix}\\right]\"\n    assert latex(Array(M), mat_str='bmatrix') == r\"\\begin{bmatrix}x & 0\\\\0 & x\\end{bmatrix}\"\n", "def test_lamda():\n    assert latex(Symbol('lamda')) == r\"\\lambda\"\n    assert latex(Symbol('Lamda')) == r\"\\Lambda\"\n    assert latex(Symbol('lambda_1')) == r\"\\lambda_{1}\"\n    assert latex(Symbol('Lamda_1')) == r\"\\Lambda_{1}\"\n    assert latex(Symbol('lambda1')) == r\"\\lambda1\"\n    assert latex(Symbol('Lamda1')) == r\"\\Lambda1\"\n    assert latex(Symbol('lambda_1_2')) == r\"\\lambda_{1 2}\"\n    assert latex(Symbol('Lamda_1_2')) == r\"\\Lambda_{1 2}\"\n"], "sample_1002": ["def test_Float_mpf_norm():\n    from mpmath.libmp import from_int, from_rational\n    mpf = from_int(2, 10, -1)\n    assert mpf_norm(mpf, 10) == from_int(2, 10, 2)\n\n    mpf = from_rational(2, 3, 10, -1)\n    assert mpf_norm(mpf, 10) == from_rational(2, 3, 10, 2)\n", "def test_issue_20051():\n    from sympy import simplify, Symbol\n    x = Symbol('x')\n    assert simplify(x - x - 1) == -1\n    assert simplify(x - x + 1) == 1\n    assert simplify(x - x + 0) == 0\n    assert simplify(x - x - oo) == -oo\n    assert simplify(x - x + oo) == oo\n    assert simplify(x - x - pi) == -pi\n    assert simplify(x - x + pi) == pi\n", "def test_integer_log():\n    raises(ValueError, lambda: integer_log(2, 1))\n    raises(ValueError, lambda: integer_log(0, 2))\n    raises(ValueError, lambda: integer_log(1.1, 2))\n    raises(ValueError, lambda: integer_log(1, 2.2))\n\n    assert integer_log(1, 2) == (0, True)\n    assert integer_log(1, 3) == (0, True)\n    assert integer_log(2, 3) == (0, False)\n    assert integer_log(3, 3) == (1, True)\n    assert integer_log(3*2, 3) == (1, False)\n    assert integer_log(3**2, 3) == (2, True)\n    assert integer_log(3*4, 3) == (2, False)\n    assert integer_log(3**3, 3) == (3, True)\n    assert integer_log(27, 5) == (2, False)\n    assert integer_log(2, 3) == (0, False)\n    assert integer_log(-4, -2) == (2, False)\n    assert integer_log(27, -3) == (3, False)\n    assert integer_log(-49, 7) == (0, False)\n    assert integer_log(-49, -7) == (2, False)\n\n    p = 3327481194821503\n    b = 175230445194412\n    assert integer_log(p * b, b) == (1, False)\n    assert integer_log(p * b**20, b) == (20, False)\n    assert integer_log(p * b**20 + p, b) == (20, False)\n    assert integer_log(p * b**20 - p, b) == (20, False)\n    assert integer_log(p * b**20 + p + b, b) == (20, False)\n    assert integer_log(p * b**20 - p - b, b) == (20, False)\n", "def testFloat\u0436ownload_rational_approx():\n    assert Float(1.3)._as_mpf_val(50) == Rational(13, 10)._as_mpf_val(50)\n    assert Float(1.3, 2)._as_mpf_val(2) == Rational(13, 10)._as_mpf_val(2)\n    assert Float(.3**99, 10)._as_mpf_val(10) == Rational(3, 10)**99._as_mpf_val(10)\n", "def test_issue_10840():\n    assert comp(sqrt(2), sqrt(2).n(3)) is False\n    assert comp(sqrt(2), sqrt(2).n(4)) is True\n    assert comp(sqrt(2), sqrt(2).n(100)) is True\n    assert comp(sqrt(2), sqrt(2).n(3), '') is True\n    assert comp(sqrt(2), sqrt(2).n(4), '') is True\n    assert comp(sqrt(2), sqrt(2).n(100), '') is True\n"], "sample_1004": ["def test_contains():\n    C = ConditionSet\n    I = S.Integers\n    c = C(x, x < 1, I)\n    assert c.contains(0) == And(0 < 1, Contains(0, I))\n    assert c.contains(-1) == And(-1 < 1, Contains(-1, I))\n    assert c.contains(y) == And(y < 1, Contains(y, I))\n", "def test_condition_set_creation():\n    C = ConditionSet\n    I = S.Integers\n\n    # Test with an empty condition\n    assert C(x, S.true, I) == I\n\n    # Test with a condition that is always false\n    assert C(x, S.false, I) == S.EmptySet\n\n    # Test with an empty base set\n    assert C(x, x < 1, S.EmptySet) == S.EmptySet\n\n    # Test with a base set that contains the dummy symbol\n    assert C(x, x < 1, {x, y}) == C(x, x < 1, {y})\n\n    # Test with a base set that is a ConditionSet\n    assert C(x, x < 1, C(x, x < 2, I)) == C(x, (x < 1) & (x < 2), I)\n\n    # Test with a condition that does not contain the dummy symbol\n    raises(ValueError, lambda: C(x, y < 1, I))\n", "def test_contains():\n    c = ConditionSet(x, x < 1, S.Integers)\n    assert c.contains(0) == (0 < 1) & Contains(0, S.Integers)\n    assert c.contains(L) == (L < 1) & Contains(L, S.Integers)\n    assert c.contains(5) == (5 < 1) & Contains(5, S.Integers)\n    assert c.contains(x) == (x < 1) & Contains(x, S.Integers)\n", "def test_CondSet_contains():\n    sin_sols_principal = ConditionSet(x, Eq(sin(x), 0),\n                                      Interval(0, 2*pi, False, True))\n    assert sin_sols_principal.contains(pi) == And(Eq(sin(pi), 0), pi < 2*pi)\n    assert sin_sols_principal.contains(3*pi) == And(Eq(sin(3*pi), 0), 3*pi < 2*pi)\n    assert sin_sols_principal.contains(5) == And(Eq(sin(5), 0), 5 < 2*pi)\n\n    condition_set = ConditionSet(x, x**2 > 4, S.Reals)\n    assert condition_set.contains(5) == And(5**2 > 4, 5 < oo)\n    assert condition_set.contains(1) == And(1**2 > 4, 1 < oo)\n    assert condition_set.contains(0) == And(0**2 > 4, 0 < oo)\n", "def test_conditionset_lambda_dummy():\n    # Make sure we handle the case where the dummy symbol is a lambda\n    C = ConditionSet\n    I = S.Integers\n    c = C(x, x < 1, I)\n    assert c == C(Dummy('lambda'), x < 1, I).subs(Dummy('lambda'), x)\n\n    # Make sure the condition is properly updated when replacing the dummy\n    assert C(x, x < 1, I).subs(x, y) == C(y, y < 1, I)\n    assert C(Dummy('lambda'), x < 1, I).subs(Dummy('lambda'), y) == C(y, x < 1, I)\n    assert C(Dummy('lambda'), Dummy('lambda') < 1, I).subs(Dummy('lambda'), y) == C(y, y < 1, I)\n"], "sample_1005": ["def test_latex_printing_of_Dkim_with_superscript():\n    # Test for printing of Dkim with superscript\n    i, k, m = symbols('i k m')\n    z = Symbol('z', complex=True)\n    assert latex(DiracDelta(m, k)**i) == r'\\left(\\delta^{\\left( k \\right)}\\left( m \\right)\\right)^{i}'\n", "def test_latex_TensorProduct_printing_of_MatMul():\n    from sympy.tensor.functions import TensorProduct\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    C = MatrixSymbol(\"C\", 3, 3)\n    assert latex(TensorProduct(A, B*C)) == r\"A \\otimes B C\"\n", "def test_latex_Quaternion():\n    from sympy import Quaternion\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == r'1 + 2 i + 3 j + 4 k'\n    assert latex(q**2) == r'\\left(1 + 2 i + 3 j + 4 k\\right)^{2}'\n", "def test_issue_15788():\n    from sympy.tensor.array import ImmutableDenseNDimArray\n\n    # Test case for issue #15788\n    M = ImmutableDenseNDimArray([1, 2, 3])\n    assert latex(M) == r\"\\left[\\begin{matrix}1 & 2 & 3\\end{matrix}\\right]\"\n", "def test_issue_14547():\n    assert latex(asin(x).diff(x, 3)) == r\"\\frac{3 \\left(2 x^{2} + 1\\right) \\sqrt{- x^{2} + 1}}{\\left(x^{2} - 1\\right)^{\\frac{5}{2}}}\"\n    assert latex(acot(x).diff(x, 3)) == r\"\\frac{6 x \\left(x^{2} + 1\\right) - 2 x^{3}}{\\left(x^{2} + 1\\right)^{4}}\"\n    assert latex(acot(x).diff(x, 2)) == r\"- \\frac{x^{2} + 1 - 2 x^{2}}{\\left(x^{2} + 1\\right)^{3}}\"\n    assert latex(atan(x).diff(x, 2)) == r\"- \\frac{2 x}{\\left(x^{2} + 1\\right)^{2}}\"\n    assert latex(acoth(x).diff(x, 2)) == r\"- \\frac{1}{\\left(x^{2} - 1\\right)^{2}}\"\n    assert latex(acot(x).diff(x)) == r\"- \\frac{1}{x^{2} + 1}\"\n    assert latex(acot(x).diff(x, 3)) == r\"2 \\left(- \\frac{1}{x^{2} + 1}\\right)^{3} - \\frac{6 x^{2}}{\\left(x^{2} + 1\\right)^{3}}\"\n    assert latex(x**3 * sin(x).diff(x, 2)) == r\"- x^{3} \\sin{\\left (x \\right )}\"\n"], "sample_1006": ["def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert subfactorial(-1) == S.Zero\n    assert subfactorial(0) == S.One\n    assert subfactorial(1) == S.Zero\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is None\n    assert subfactorial(5).is_integer is True\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_nonnegative is None\n    assert subfactorial(5).is_nonnegative is True\n\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(k).is_odd is None\n    assert subfactorial(4).is_odd is True\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(3).is_even is True\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(-1) == zoo\n    assert subfactorial(-2) == zoo\n    assert subfactorial(0) == 1\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(5).is_integer is True\n    assert subfactorial(-5).is_integer is None\n    assert subfactorial(5).is_even is True\n    assert subfactorial(4).is_odd is True\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(5).is_positive is True\n    assert subfactorial(n).is_real is None\n    assert subfactorial(5).is_real is True\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    nf = Symbol('nf', integer=True, nonnegative=False)\n\n    assert subfactorial(-2) == zoo\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(nf).func == subfactorial\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is None\n    assert subfactorial(nf).is_integer is None\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_nonnegative is None\n    assert subfactorial(nf).is_nonnegative is None\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    nz = Symbol('nz', integer=True, nonzero=True)\n    z = Symbol('z', integer=True, zero=True)\n    k = Symbol('k', integer=True)\n    kp = Symbol('kp', integer=True, positive=True)\n    kn = Symbol('kn', integer=True, negative=True)\n    u = Symbol('u', negative=True)\n    p = Symbol('p', positive=True)\n    t = Symbol('t', nonnegative=True)\n    a = Symbol('a', integer=True, nonnegative=True)\n    b = Symbol('b', integer=True, nonnegative=True)\n    x = Symbol('x')\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(kp).is_integer is True\n    assert subfactorial(nz).is_integer is True\n    assert subfactorial(z).is_integer is True\n\n    assert subfactorial(kp).is_positive is True\n    assert subfactorial(kp).is_nonnegative is True\n    assert subfactorial(nz).is_nonnegative is None\n    assert subfactorial(z).is_nonnegative is True\n\n    assert subfactorial(n).is_real is None\n    assert subfactorial(kp).is_real is True\n    assert subfactorial(nz).is_real is True\n    assert subfactorial(z).is_real is True\n\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(kp).is_odd is None\n    assert subfactorial(nz).is_odd is None\n    assert subfactorial(z).is_odd is False\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(kp).is_even is None\n    assert subfactorial(nz).is_even is None\n    assert subfactorial(z).is_even is True\n\n    assert subfactorial(kp).rewrite(uppergamma) == uppergamma(kp + 1, -1)/S.Exp1\n    assert subfactorial(nz).rewrite(uppergamma) == uppergamma(nz + 1, -1)/S.Exp1\n    assert subfactorial(z).rewrite(uppergamma) == uppergamma(1", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    kn = Symbol('kn', integer=True, negative=True)\n    u = Symbol('u', negative=True)\n    p = Symbol('p', positive=True)\n    z = Symbol('z', zero=True)\n    nt = Symbol('nt', integer=False)\n    kt = Symbol('kt', integer=False)\n\n    assert subfactorial(-1) == 0\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(7) == 1854\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n    assert subfactorial(kn).is_integer is None\n    assert subfactorial(u).is_integer is None\n    assert subfactorial(p).is_integer is None\n    assert subfactorial(z).is_integer is True\n    assert subfactorial(nt).is_integer is None\n    assert subfactorial(kt).is_integer is None\n\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(k).is_positive is True\n    assert subfactorial(kn).is_positive is None\n    assert subfactorial(u).is_positive is None\n    assert subfactorial(p).is_positive is None\n    assert subfactorial(z).is_positive is True\n    assert subfactorial(nt).is_positive is None\n    assert subfactorial(kt).is_positive is None\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_nonnegative is True\n    assert subfactorial(kn).is_nonnegative is None\n    assert subfactorial(u).is_nonnegative is None\n    assert subfactorial(p).is_nonnegative is None\n    assert subfactorial(z).is_nonnegative is True\n    assert subfactorial(nt).is_nonnegative is None\n    assert subfactorial(kt).is_nonnegative is None\n\n    assert subfactorial"], "sample_1007": ["def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    nn = Symbol('nn', integer=True, nonnegative=False)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n    assert subfactorial(nn).is_integer is None\n\n    assert subfactorial(k).is_nonnegative is True\n    assert subfactorial(nn).is_nonnegative is None\n\n    assert subfactorial(k).is_even is None\n    assert subfactorial(k).is_odd is None\n    assert subfactorial(2*k).is_even is True\n    assert subfactorial(2*k).is_odd is False\n    assert subfactorial(2*k + 1).is_even is False\n    assert subfactorial(2*k + 1).is_odd is True\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n", "def test_subfactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert subfactorial(-2) == S.NaN\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(2*n).func == subfactorial\n\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer\n\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(k).is_positive is None\n\n    assert subfactorial(x).is_real is None\n    assert subfactorial(n).is_real is None\n    assert subfactorial(k).is_real is True\n\n    assert subfactorial(n).is_composite is None\n    assert subfactorial(k).is_composite is None\n\n    assert subfactorial(oo) == oo\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1) / S.Exp1\n", "def test_subfactorial():\n    from sympy.functions.combinatorial.factorials import subfactorial\n    from sympy import uppergamma, S, I\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n    assert subfactorial(S(3)/2) == uppergamma(S(5)/2, -1)/S.Exp1\n    assert subfactorial(I) == uppergamma(1 + I, -1)/S.Exp1\n\n    n = S(3)/2\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n\n    n = S(5)\n    assert subfactorial(n).is_nonnegative is True\n    assert subfactorial(n).is_integer is True\n    assert subfactorial(n).is_odd is True\n\n    n = S(3)/2\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(n).is_integer is False\n    assert subfactorial(n).is_odd is None\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n\n    assert subfactorial(-1) == S.NaN\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(k + 1).is_even is True\n\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(k).is_odd is None\n    assert subfactorial(k + 2).is_odd is True\n\n    assert subfactorial(n).is_positive is None\n    assert subfactorial(k).is_positive is True\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_nonnegative is True\n\n    assert subfactorial(k).rewrite(uppergamma) == uppergamma(k + 1, -1)/S.Exp1\n", "def test_subfactorial():\n    from sympy import S, I\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    u = Symbol('u', integer=False)\n    p = Symbol('p', integer=True, positive=True)\n    np = Symbol('np', integer=True, nonpositive=True)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n    assert subfactorial(u).is_integer is None\n\n    assert subfactorial(p).is_even is True\n    assert subfactorial(p).is_odd is False\n    assert subfactorial(np).is_even is None\n    assert subfactorial(np).is_odd is None\n\n    assert subfactorial(p).is_positive is True\n    assert subfactorial(np).is_positive is None\n\n    assert subfactorial(n).is_real is None\n    assert subfactorial(k).is_real is True\n    assert subfactorial(u).is_real is None\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(k).is_nonnegative is True\n    assert subfactorial(u).is_nonnegative is None\n\n    assert subfactorial(p).is_zero is False\n    assert subfactorial(np).is_zero is None\n\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n    assert subfactorial(k).rewrite(uppergamma) == uppergamma(k + 1, -1)/S.Exp1\n\n    raises(ValueError, lambda: subfactorial(I))\n    raises(ValueError, lambda: subfactorial(n + I))\n    raises(ValueError, lambda: subfactorial(n - I))\n"], "sample_1008": ["def test_coordinate_sym_properties():\n    A = ReferenceFrame('A')\n    q1, q2, q3 = symbols('q1 q2 q3')\n    B = A.orientnew('B', 'Axis', [q1, A.z])\n    assert isinstance(B[0], CoordinateSym) and B[0].frame == B\n    assert isinstance(B[1], CoordinateSym) and B[1].frame == B\n    assert isinstance(B[2], CoordinateSym) and B[2].frame == B\n    assert B[0].index == 0 and B[1].index == 1 and B[2].index == 2\n    assert B[0] == CoordinateSym('B_x', B, 0)\n    assert B[1] == CoordinateSym('B_y', B, 1)\n    assert B[2] == CoordinateSym('B_z', B, 2)\n    assert hash(B[0]) == hash(CoordinateSym('B_x', B, 0))\n    assert hash(B[1]) == hash(CoordinateSym('B_y', B, 1))\n    assert hash(B[2]) == hash(CoordinateSym('B_z', B, 2))\n    assert B[0] != B[1] and B[0] != B[2]\n    assert B[1] != B[2]\n", "def test_orient():\n    N = ReferenceFrame('N')\n    q1 = dynamicsymbols('q1')\n    A = ReferenceFrame('A')\n    A.orient(N, 'Axis', [q1, N.z])\n    assert A._dcm_dict[N] == Matrix([\n        [cos(q1), -sin(q1), 0],\n        [sin(q1), cos(q1), 0],\n        [0, 0, 1]\n    ])\n", "def test_coordinate_sym_properties():\n    A = ReferenceFrame('A')\n    x, y, z = A[0], A[1], A[2]\n    assert x.frame == A\n    assert y.frame == A\n    assert z.frame == A\n    assert x == x\n    assert x != y\n    assert hash(x) != hash(y)\n    assert A.x == A.x\n    assert A.x != A.y\n", "def test_coordinate_sym_eq():\n    A = ReferenceFrame('A')\n    assert A[0] == CoordinateSym('Ax', A, 0)\n    assert A[1] == CoordinateSym('Ay', A, 1)\n    assert A[2] == CoordinateSym('Az', A, 2)\n    assert A[0] != CoordinateSym('Ax', A, 1)\n    assert A[0] != CoordinateSym('Bx', A, 0)\n    B = ReferenceFrame('B')\n    assert A[0] != CoordinateSym('Ax', B, 0)\n", "def test_coordinate_sym_constructor():\n    A = ReferenceFrame('A')\n    a_x = CoordinateSym('a_x', A, 0)\n    a_y = CoordinateSym('a_y', A, 1)\n    a_z = CoordinateSym('a_z', A, 2)\n    \n    assert a_x.frame == A\n    assert a_y.frame == A\n    assert a_z.frame == A\n    \n    assert a_x == A[0]\n    assert a_y == A[1]\n    assert a_z == A[2]\n    \n    assert a_x != A[1]\n    assert a_x != A[2]\n    assert a_y != A[0]\n    assert a_y != A[2]\n    assert a_z != A[0]\n    assert a_z != A[1]\n    \n    assert hash(a_x) == hash(A[0])\n    assert hash(a_y) == hash(A[1])\n    assert hash(a_z) == hash(A[2])\n\n    try:\n        CoordinateSym('a_x', A, 3)\n        assert False\n    except ValueError:\n        assert True\n"], "sample_1009": ["def test_Vector_magnitude():\n    q1, q2, q3 = symbols('q1 q2 q3')\n    N = ReferenceFrame('N')\n    v1 = q1 * N.x + q2 * N.y + q3 * N.z\n    v2 = v1.normalize()\n    assert v1.magnitude() == sqrt(q1**2 + q2**2 + q3**2)\n    assert v2.magnitude() == 1\n", "def test_Vector_magnitude_normalize():\n    N = ReferenceFrame('N')\n    v1 = 3*N.x + 4*N.y\n    assert v1.magnitude() == 5\n    assert v1.normalize() == (3/5)*N.x + (4/5)*N.y\n    assert v1.normalize().magnitude() == 1\n\n    v2 = N.x + N.y + N.z\n    assert v2.magnitude() == sqrt(3)\n    assert v2.normalize() == (1/sqrt(3))*N.x + (1/sqrt(3))*N.y + (1/sqrt(3))*N.z\n    assert v2.normalize().magnitude() == 1\n", "def test_vector_magnitude_normalize():\n    N = ReferenceFrame('N')\n    a, b, c = symbols('a b c')\n    v = a * N.x + b * N.y + c * N.z\n    assert v.magnitude() == sqrt(a**2 + b**2 + c**2)\n    assert v.normalize() == (a / sqrt(a**2 + b**2 + c**2)) * N.x + (b / sqrt(a**2 + b**2 + c**2)) * N.y + (c / sqrt(a**2 + b**2 + c**2)) * N.z\n", "def test_vector_normalize_magnitude():\n    N = ReferenceFrame('N')\n    u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n    v = u1 * N.x + u2 * N.y + u3 * N.z\n\n    assert v.magnitude() == (u1**2 + u2**2 + u3**2)**0.5\n    assert v.normalize() == v / v.magnitude()\n    assert v.normalize().magnitude() == 1\n", "def test_Vector_magnitude():\n    N = ReferenceFrame('N')\n    u1, u2, u3 = dynamicsymbols('u1 u2 u3')\n    v = u1 * N.x + u2 * N.y + u3 * N.z\n    assert v.magnitude() == sympy.sqrt(u1**2 + u2**2 + u3**2)\n    v_magnitude = v.magnitude()\n    assert v_magnitude.diff(u1) == u1 / sympy.sqrt(u1**2 + u2**2 + u3**2)\n    assert v_magnitude.diff(u2) == u2 / sympy.sqrt(u1**2 + u2**2 + u3**2)\n    assert v_magnitude.diff(u3) == u3 / sympy.sqrt(u1**2 + u2**2 + u3**2)\n"], "sample_1011": ["def test_user_functions():\n    # single user function\n    f = Function('f')\n    custom_functions = {f: \"existing_octave_fcn\"}\n    assert mcode(f(x), user_functions=custom_functions) == \"existing_octave_fcn(x)\"\n\n    # multiple user functions\n    g = Function('g')\n    custom_functions = {f: \"existing_octave_fcn\",\n                        g: \"another_existing_octave_fcn\"}\n    assert mcode(f(x) + g(x), user_functions=custom_functions) == \\\n        \"existing_octave_fcn(x) + another_existing_octave_fcn(x)\"\n\n    # function with multiple rules\n    custom_functions = {g: [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n                            (lambda x: not x.is_Matrix, \"my_fcn\")]}\n    mat = Matrix([[1, x]])\n    assert mcode(g(x) + g(mat), user_functions=custom_functions) == \\\n        \"my_fcn(x) + my_mat_fcn([1 x])\"\n", "def test_Indexed_printing():\n    # test cases for indexed printing\n    A = MatrixSymbol(\"A\", 3, 3)\n    i = Symbol('i', integer=True)\n    j = Symbol('j', integer=True)\n\n    assert mcode(A[i, j]) == \"A(i + 1, j + 1)\"\n    assert mcode(3 * A[i, j]) == \"3*A(i + 1, j + 1)\"\n    assert mcode(A[i, 0]) == \"A(i + 1, 1)\"\n", "def test_Indexed_printing():\n    # test cases for issue #11821\n    A = IndexedBase(\"A\", shape=(3, 3))\n    i = Symbol('i', integer=True)\n    j = Symbol('j', integer=True)\n\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(3 * A[i, j]) == \"3*A(i, j)\"\n\n    B = IndexedBase(\"B\", shape=(3, 3))\n    C = IndexedBase(\"C\", shape=(3, 3))\n\n    F = C[i, j].subs(C, A - B)\n    assert mcode(F) == \"(-B(i, j) + A(i, j))\"\n", "def test_Indexed_printing():\n    # test cases for Indexed printing\n    A = symbols('A', cls=IndexedBase)\n    i, j = symbols('i j', integer=True)\n\n    assert mcode(A[i]) == \"A(i)\"\n    assert mcode(A[i + 1]) == \"A(i + 1)\"\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(A[i, j] + 1) == \"A(i, j) + 1\"\n    assert mcode(A[i, j] + A[i, j]) == \"2*A(i, j)\"\n    assert mcode(A[i, j] * A[i, j]) == \"A(i, j).^2\"\n", "def test_octave_user_functions():\n    f = Function('f')\n    g = Function('g')\n    h = Function('h')\n    x = symbols('x')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")],\n        \"h\": \"my_h_fcn\"\n    }\n    assert mcode(f(x), user_functions=custom_functions) == \"existing_octave_fcn(x)\"\n    assert mcode(g(x), user_functions=custom_functions) == \"my_fcn(x)\"\n    assert mcode(g(Matrix([[x]])), user_functions=custom_functions) == \"my_mat_fcn([x])\"\n    assert mcode(h(x), user_functions=custom_functions) == \"my_h_fcn(x)\"\n"], "sample_1012": ["def test_pycode_PythonCodePrinterSETTING_fully_qualified_modules():\n    prntr = PythonCodePrinter({'fully_qualified_modules': False})\n    assert prntr.doprint(pi) == 'pi'\n    assert prntr.module_imports == {'math': {'pi'}}\n    assert prntr.doprint(acos(x)) == 'acos(x)'\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n", "def test_printing_infinities():\n    p = PythonCodePrinter()\n    assert p.doprint(oo) == 'float(\\'inf\\')'\n    assert p.doprint(-oo) == 'float(\\'-inf\\')'\n    assert p.doprint(zoo) == 'float(\\'nan\\')'\n", "def test_infinity():\n    p = PythonCodePrinter()\n    assert p.doprint(oo) == 'float(\\'inf\\')'\n    assert p.doprint(-oo) == 'float(\\'-inf\\')'\n    assert p.doprint(zoo) == 'float(\\'nan\\')'\n\n    p = MpmathPrinter()\n    assert p.doprint(oo) == 'mpmath.inf'\n    assert p.doprint(-oo) == '-mpmath.inf'\n    assert p.doprint(zoo) == 'mpmath.nan'\n\n    p = NumPyPrinter()\n    assert p.doprint(oo) == 'numpy.inf'\n    assert p.doprint(-oo) == '-numpy.inf'\n    assert p.doprint(zoo) == 'numpy.nan'\n\n    p = SciPyPrinter()\n    assert p.doprint(oo) == 'numpy.inf'\n    assert p.doprint(-oo) == '-numpy.inf'\n    assert p.doprint(zoo) == 'numpy.nan'\n", "def test_pycode_Piecewise_fallback():\n    p = PythonCodePrinter()\n    expr = Piecewise((1, Eq(x, 0)), (2, x>6), evaluate=False)\n    assert p.doprint(expr) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    p = NumPyPrinter()\n    assert p.doprint(expr) == 'numpy.select([(x == 0), (x > 6)], [1, 2], default=numpy.nan)'\n    p = MpmathPrinter()\n    assert p.doprint(expr) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    p = SciPyPrinter()\n    assert p.doprint(expr) == 'numpy.select([(x == 0), (x > 6)], [1, 2], default=numpy.nan)'\n", "def test_PythonCodePrinter_printing_of_infinity_and_nan():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n    assert prntr.doprint(zoo) == \"float('nan')\"\n"], "sample_1010": ["def test_latex_PolyElement_with_Add():\n    Ruv, u,v = ring(\"u,v\", ZZ)\n    Rxyz, x,y,z = ring(\"x,y,z\", Ruv)\n\n    poly = PolyElement((u**2 + 3*u*v + 1)*x**2*y + u + 1, Rxyz)\n    assert latex(poly) == r\"\\left({u}^{2} + 3 u v + 1\\right) {x}^{2} y + u + 1\"\n\n    poly = PolyElement((u**2 + 3*u*v + 1)*x**2*y + (u + 1)*x, Rxyz)\n    assert latex(poly) == r\"\\left({u}^{2} + 3 u v + 1\\right) {x}^{2} y + \\left(u + 1\\right) x\"\n", "def test_KroneckerProduct_printing():\n    from sympy.tensor.functions import KroneckerProduct\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert latex(KroneckerProduct(A, B)) == r\"A \\otimes B\"\n", "def test_issue_16124():\n    from sympy.printing.latex import LatexPrinter\n    lp = LatexPrinter()\n    assert lp._print_Mul(-2, x + 1, order='old') == r'-2 \\left(x + 1\\right)'\n    assert lp._print_Mul(2, x + 1, order='old') == r'2 \\left(x + 1\\right)'\n    assert lp._print_Mul(S.One/2, x + 1, order='old') == r'\\frac{x + 1}{2}'\n    assert lp._print_Mul(y, x + 1, order='old') == r'y \\left(x + 1\\right)'\n    assert lp._print_Mul(-y, x + 1, order='old') == r'- y \\left(x + 1\\right)'\n", "def test_latex_FreeModule():\n    from sympy.polys.domains import QQ\n    R = QQ.old_poly_ring(x)\n    F = R.free_module(2)\n    assert latex(F) == r'{\\mathbb{Q}\\left[x\\right]}^{2}'\n", "def test_latex_symbol_names():\n    x, y = symbols('x y')\n    symbol_names = {'x': r'\\mathbf{x}', 'y': r'\\mathbf{y}'}\n    assert latex(x + y, symbol_names=symbol_names) == r'\\mathbf{x} + \\mathbf{y}'\n    assert latex(x**2, symbol_names=symbol_names) == r'\\mathbf{x}^{2}'\n    assert latex(x + y, symbol_names={x: r'\\mathbf{x}'}) == r'\\mathbf{x} + y'\n"], "sample_1013": ["def test_lambdify_unsupported_modules():\n    raises(TypeError, lambda: lambdify(x, x + 1, modules=\"unsupported_module\"))\n", "def test_lambdify_tensorflow_indices():\n    # Test for issue 14655\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n    A = IndexedBase('A')\n    i, j = symbols('i j')\n    f = lambdify(A, Sum(A[i, j], (i, 0, 1), (j, 0, 1)), 'tensorflow')\n    b = tensorflow.constant([[1, 2], [3, 4]])\n    fcall = f(b)\n    s = tensorflow.Session()\n    assert s.run(fcall) == 10\n", "def test_lambdastr_nested_arrays():\n    f = lambdastr((x, [y, z]), x*y + z)\n    assert f == 'lambda x,_1: (lambda x,y,z: (x*y + z))(x,_1[0],_1[1])'\n\n    f = lambdastr(([x, [y, z]], w), x*y*w + z)\n    assert f == 'lambda _0,w: (lambda x,y,z,w: (x*y*w + z))(_0[0],_0[1][0],_0[1][1],w)'\n", "def test_lambdify_callable():\n    # Test for issue 16073\n    f = lambdify(x, Lambda(x, x**2))\n    assert f(2) == 4\n", "def test_lambdastr():\n    assert lambdastr(x, x**2) == 'lambda x: (x**2)'\n    assert lambdastr((x, y, z), [z, y, x]) == 'lambda x,y,z: ([z, y, x])'\n    assert lambdastr((x, (y, z)), x + y) == 'lambda _0,_1: (lambda x,y,z: (x + y))(_0,_1[0],_1[1])'\n"], "sample_1015": ["def test_ccode_While():\n    assert ccode(While(x < y, [x + y])) == 'while (x < y) {\\n   x + y;\\n}'\n", "def test_ccode_While():\n    assert ccode(While(x < 2, [aug_assign(x, '+', 1)])) == (\n        'while (x < 2) {\\n'\n        '   x += 1;\\n'\n        '}'\n    )\n", "def test_ccode_While():\n    i = symbols('i', integer=True)\n    y = symbols('y')\n    body = aug_assign(y, '+', i)\n    whl = While(i < 10, body)\n    assert ccode(whl) == 'while (i < 10) {\\n   y += i;\\n}'\n", "def test_ccode_While():\n    assert ccode(While(x < y, [aug_assign(x, '+', 1)])) == (\n        \"while (x < y) {\\n\"\n        \"   x += 1;\\n\"\n        \"}\"\n    )\n", "def test_ccode_While():\n    assert ccode(While(x < 1, [aug_assign(y, '*', x)])) == (\n        \"while (x < 1) {\\n\"\n        \"   y *= x;\\n\"\n        \"}\"\n    )\n"], "sample_1014": ["def test_free_symbols():\n    from sympy.abc import x, y, z\n    md = ImmutableDenseNDimArray([[x, y], [x*z, x*y*z]])\n    assert md.free_symbols == {x, y, z}\n\n    sd = ImmutableSparseNDimArray(md)\n    assert sd.free_symbols == {x, y, z}\n\n    mdn = ImmutableDenseNDimArray([1, 2, 3])\n    assert mdn.free_symbols == set()\n\n    sdn = ImmutableSparseNDimArray([1, 2, 3])\n    assert sdn.free_symbols == set()\n", "def test_free_symbols():\n    from sympy.abc import x, y, z\n    md = ImmutableDenseNDimArray([[x, y], [x*z, x*y*z]])\n    assert md.free_symbols == {x, y, z}\n\n    sd = ImmutableSparseNDimArray(md)\n    assert sd.free_symbols == {x, y, z}\n\n    mdn = md.applyfunc(lambda x: x*3)\n    assert mdn.free_symbols == {x, y, z}\n\n    sdn = sd.applyfunc(lambda x: x/2)\n    assert sdn.free_symbols == {x, y, z}\n", "def test_mutable_array_properties():\n    mutable_array = MutableDenseNDimArray([1, 2, 3])\n    assert mutable_array.free_symbols == set()\n\n    mutable_array = MutableDenseNDimArray([1, Symbol('x'), 3])\n    assert mutable_array.free_symbols == {Symbol('x')}\n\n    from sympy.abc import x, y\n    mutable_array = MutableDenseNDimArray([[x, y], [x*y, x+y]])\n    assert mutable_array.free_symbols == {x, y}\n", "def test_free_symbols():\n    from sympy.abc import x, y, z\n    md = ImmutableDenseNDimArray([[x, y], [x*z, x*y*z]])\n    assert md.free_symbols == {x, y, z}\n\n    ms = ImmutableSparseNDimArray(md)\n    assert ms.free_symbols == {x, y, z}\n\n    md2 = ImmutableDenseNDimArray([1, 2, 3])\n    assert md2.free_symbols == set()\n\n    ms2 = ImmutableSparseNDimArray([1, 2, 3])\n    assert ms2.free_symbols == set()\n", "def test_free_symbols():\n    x, y = symbols(\"x y\")\n    M = ImmutableDenseNDimArray([[x, y], [x*y, x+y]])\n    assert M.free_symbols == {x, y}\n    Ms = ImmutableSparseNDimArray([[x, y], [x*y, x+y]])\n    assert Ms.free_symbols == {x, y}\n    Mo = ImmutableDenseNDimArray([x, y, x*y])\n    assert Mo.free_symbols == {x, y}\n    Mos = ImmutableSparseNDimArray([x, y, x*y])\n    assert Mos.free_symbols == {x, y}\n"], "sample_1017": ["def test_as_set_interval():\n    assert (x >= -2).as_set() == Interval(-2, oo)\n    assert (x <= 2).as_set() == Interval(-oo, 2)\n    assert (x > -2).as_set() == Interval(-2, oo, left_open=True)\n    assert (x < 2).as_set() == Interval(-oo, 2, right_open=True)\n", "def test_issue_20935():\n    from sympy.abc import x\n    raises(TypeError, lambda: x+x+x < 1)\n    raises(TypeError, lambda: Eq(x+x+x, 1))\n    raises(TypeError, lambda: Not(x+x+x))\n    raises(TypeError, lambda: And(x+x+x, True))\n", "def test_as_set_real():\n    assert (sin(x) > 0).as_set() == Interval(0, pi) + Interval(2*pi, 3*pi)\n    assert (sin(x) < 0).as_set() == Interval(-pi, 0) + Interval(pi, 2*pi)\n", "def test_as_set_real():\n    # Constants\n    assert S.true.as_set() == Interval(-oo, oo)\n    assert S.false.as_set() == EmptySet()\n\n    # Relational\n    assert (x < 1).as_set() == Interval(-oo, 1, True, True)\n    assert (x > 1).as_set() == Interval(1, oo, True, True)\n    assert (x <= 1).as_set() == Interval(-oo, 1)\n    assert (x >= 1).as_set() == Interval(1, oo)\n\n    # And\n    assert (x < 1 & x > 0).as_set() == Interval(0, 1, True, True)\n    assert (x < 1 & x > 1).as_set() == EmptySet()\n\n    # Or\n    assert (x < 1 | x > 0).as_set() == Interval(-oo, oo)\n    assert (x < 0 | x > 1).as_set() == Union(Interval(-oo, 0, True, True), Interval(1, oo, True, True))\n", "def test_as_set_fallback():\n    from sympy import Interval, S\n    x = symbols('x', real=True)\n    assert Piecewise((x >= 1, x <= 2)).as_set() == Interval(1, 2)\n    assert Piecewise((x < 1, x > 2), (True, True)).as_set() == S.UniversalSet\n    assert Piecewise((x < 1, x > 2), (False, False)).as_set() == S.EmptySet\n"], "sample_1016": ["def test_IndexedBase_printing():\n    # test cases for issue #11821\n    A = IndexedBase(\"A\")\n    B = IndexedBase(\"B\")\n\n    assert mcode(A[0]) == \"A(1)\"\n    assert mcode(B[1, 2, 3]) == \"B(2, 3, 4)\"\n    assert mcode(3 * A[0]) == \"3*A(1)\"\n    assert mcode(A[0] + B[1, 2, 3]) == \"A(1) + B(2, 3, 4)\"\n    assert mcode(A[0] * B[1, 2, 3]) == \"A(1).*B(2, 3, 4)\"\n", "def test_octave_full_prec():\n    assert mcode(pi, full_prec=True) == 'pi'\n    assert mcode(pi, full_prec='auto') == 'pi'\n    assert mcode(pi, full_prec=False) == \"3.141592653589793\"\n    assert mcode(Rational(1, 2), full_prec=True) == '1/2'\n    assert mcode(Rational(1, 2), full_prec='auto') == '1/2'\n    assert mcode(Rational(1, 2), full_prec=False) == '0.5'\n", "def test_printing_ofIndexed():\n    # Test cases for printing of Indexed\n    A = IndexedBase('A')\n    B = IndexedBase('B')\n    i = Symbol('i', integer=True)\n    j = Symbol('j', integer=True)\n    m = Symbol('m', integer=True)\n    assert mcode(A[i]) == \"A(i)\"\n    assert mcode(A[i + 1]) == \"A(i + 1)\"\n    assert mcode(A[i, j]) == \"A(i, j)\"\n    assert mcode(A[i, j, m]) == \"A(i, j, m)\"\n    assert mcode(A[i]*A[j]) == \"A(i).*A(j)\"\n    assert mcode(A[i] + A[j]) == \"A(i) + A(j)\"\n    assert mcode(A[i] - A[j]) == \"A(i) - A(j)\"\n    assert mcode(A[i] + A[j]*B[i]) == \"A(i) + A(j).*B(i)\"\n    assert mcode(A[i] - A[j]*B[i]) == \"A(i) - A(j).*B(i)\"\n    assert mcode(A[i]/A[j]) == \"A(i)./A(j)\"\n", "def test_Infinity():\n    assert mcode(oo) == \"inf\"\n    assert mcode(-oo) == \"-inf\"\n    assert mcode(float(\"inf\")) == \"inf\"\n    assert mcode(float(\"-inf\")) == \"-inf\"\n", "def test_Indexed_printing():\n    # test cases for issue #11821\n    A = IndexedBase(\"A\", shape=(3, 3))\n    i = Symbol('i', integer=True)\n    j = Symbol('j', integer=True)\n\n    assert mcode(A[i, j]) == \"A(%s, %s)\" % (mcode(i + 1), mcode(j + 1))\n    assert mcode(3 * A[i, j]) == \"3*A(%s, %s)\" % (mcode(i + 1), mcode(j + 1))\n"], "sample_1018": ["def test_fcode_While():\n    x, y = symbols('x y')\n\n    w = While(x < y, [Assignment(x, x + 1)])\n    sol = fcode(w)\n    assert sol == (\"      do while (x < y)\\n\"\n                   \"         x = x + 1\\n\"\n                   \"      end do\")\n", "def test_fcode_While():\n    x = symbols('x')\n    expr = While(x > 0, [Assignment(x, x - 1)])\n    assert fcode(expr) == (\"      do while (x > 0)\\n\"\n                           \"         x = x - 1\\n\"\n                           \"      end do\")\n", "def test_fcode_While():\n    x = symbols('x')\n    expr = While(x < 10, [Assignment(x, x + 1)])\n    result = fcode(expr)\n    expected = (\n        \"      do while (x < 10)\\n\"\n        \"         x = x + 1\\n\"\n        \"      end do\"\n    )\n    assert result == expected\n", "def test_fcode_Stream():\n    printer = FCodePrinter({'standard': 2003})\n    assert printer._print_Stream('stdout') == 'input_unit'\n    assert printer._print_Stream('stderr') == 'error_unit'\n    assert 'iso_c_binding' in printer.module_uses\n    assert 'stdint=>input_unit' in printer.module_uses['iso_c_binding']\n    assert 'stdint=>error_unit' in printer.module_uses['iso_c_binding']\n", "def test_fcode_While():\n    x, y = symbols('x,y')\n    f = While(x < y, [Assignment(x, x + 1)])\n    sol = fcode(f)\n    assert sol == (\"      do while (x < y)\\n\"\n                   \"         x = x + 1\\n\"\n                   \"      end do\")\n"], "sample_1020": ["def test_Sum():\n    assert mcode(Sum(x, (x, 1, 10))) == \"Hold[Sum[x, {x, 1, 10}]]\"\n    assert mcode(Sum(x*y, (x, 1, 5), (y, 1, 10))) == \"Hold[Sum[x*y, {x, 1, 5}, {y, 1, 10}]]\"\n    assert mcode(Sum(sin(x), (x, 0, oo))) == \"Hold[Sum[Sin[x], {x, 0, Infinity}]]\"\n    assert mcode(Sum(x**2, (x, 1, -5))) == \"Hold[Sum[x^2, {x, 1, -5}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 10), (y, 1, 10))) == \\\n        \"Hold[Sum[x^2 + y^2, {x, 1, 10}, {y, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 0, 10))) == \"Hold[Sum[x^2, {x, 0, 10}]]\"\n    assert mcode(Sum(x*y, (x, 1, 10), (y, 1, 10))) == \"Hold[Sum[x*y, {x, 1, 10}, {y, 1, 10}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 0, 10), (y, 0, 10))) == \"Hold[Sum[x^2 + y^2, {x, 0, 10}, {y, 0, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x*y, (x, 1, 5), (y, 1, 3))) == \"Hold[Sum[x*y, {x, 1, 5}, {y, 1, 3}]]\"\n    assert mcode(Sum(sin(x), (x, 0, pi))) == \"Hold[Sum[Sin[x], {x, 0, Pi}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 10), (y, 1, 10))) == \"Hold[Sum[x^2 + y^2, {x, 1, 10}, {y, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x*y, (x, 1, 10), (y, 1, 10))) == \"Hold[Sum[x*y, {x, 1, 10}, {y, 1, 10}]]\"\n"], "sample_1021": ["def test_quaternion_operations_with_complex_numbers():\n    q = Quaternion(1, 2, 3, 4)\n    c = 2 + 3j\n\n    assert q + c == Quaternion(3, 5, 3, 4)\n    assert c + q == Quaternion(3, 5, 3, 4)\n    assert q * c == Quaternion(-5, 11, 38, -5)\n    assert c * q == Quaternion(-5, 11, 38, -5)\n\n    q2 = Quaternion(1 + 2j, 3 + 4j, 5 + 6j, 7 + 8j, real_field=False)\n    assert q2 + c == Quaternion(3 + 5j, 3 + 4j, 5 + 6j, 7 + 8j)\n    assert c + q2 == Quaternion(3 + 5j, 3 + 4j, 5 + 6j, 7 + 8j)\n    assert q2 * c == Quaternion((2 + 3j)*(1 + 2j), (2 + 3j)*(3 + 4j), (2 + 3j)*(5 + 6j), (2 + 3j)*(7 + 8j))\n    assert c * q2 == Quaternion((2 + 3j)*(1 + 2j), (2 + 3j)*(3 + 4j), (2 + 3j)*(5 + 6j), (2 + 3j)*(7 + 8j))\n", "def test_quaternion_mul_and_div():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 * q2 == Quaternion(-60, 12, 30, 24)\n    assert q2 * q1 == Quaternion(-60, 36, 14, 52)\n\n    q1_norm = q1.norm()\n    q2_norm = q2.norm()\n    assert q1 / q1_norm == q1.normalize()\n    assert q2 / q2_norm == q2.normalize()\n\n    q3 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field = False)\n    q4 = Quaternion(1, 4, 7, 8)\n\n    assert q3 * q4 == Quaternion((3 + 4*I)*(-60) - (2 + 5*I)*(-6) - 0*(-26) - (7 + 8*I)*28,\n                                 (3 + 4*I)*(-6) + (2 + 5*I)*(-60) + 0*(-52) + (7 + 8*I)*(-12),\n                                 (3 + 4*I)*(-26) + (2 + 5*I)*52 + 0*(-60) + (7 + 8*I)*6,\n                                 (3 + 4*I)*28 + (2 + 5*I)*12 + 0*(-6) + (7 + 8*I)*(-60))\n", "def test_quaternion_from_axis_angle():\n    axis = (1, 0, 0)\n    angle = pi/2\n    q = Quaternion.from_axis_angle(axis, angle)\n    assert q == Quaternion(cos(angle/2), axis[0]*sin(angle/2), axis[1]*sin(angle/2), axis[2]*sin(angle/2))\n\n    axis = (0, 1, 0)\n    angle = pi/4\n    q = Quaternion.from_axis_angle(axis, angle)\n    assert q == Quaternion(cos(angle/2), axis[0]*sin(angle/2), axis[1]*sin(angle/2), axis[2]*sin(angle/2))\n\n    axis = (0, 0, 1)\n    angle = pi\n    q = Quaternion.from_axis_angle(axis, angle)\n    assert q == Quaternion(cos(angle/2), axis[0]*sin(angle/2), axis[1]*sin(angle/2), axis[2]*sin(angle/2))\n", "def test_quaternion_real_field():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(1, 2, 3, 4, real_field=False)\n    q3 = Quaternion(1 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n\n    assert q1.real_field\n    assert not q2.real_field\n    assert not q3.real_field\n\n    assert q1.add(2 + 3*I) == Quaternion(3, 5, 3, 4)\n    assert q2.add(2 + 3*I) == Quaternion(3 + 3*I, 2, 3, 4)\n    assert q3.add(2 + 3*I) == Quaternion((3 + 7*I), (2 + 5*I), 0, (7 + 8*I))\n\n    assert q1.mul(2 + 3*I) == Quaternion(-10, 11, 38, -5)\n    assert q2.mul(2 + 3*I) == Quaternion(2 + 3*I, 2*(2 + 3*I), 3*(2 + 3*I), 4*(2 + 3*I))\n    assert q3.mul(2 + 3*I) == Quaternion((2 + 3*I)*(1 + 4*I), (2 + 3*I)*(2 + 5*I), 0, (2 + 3*I)*(7 + 8*I))\n", "def test_quaternion_non_numeric_input():\n    x = symbols('x')\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(x, x**2, x**3, x**4)\n\n    assert q1 + q2 == Quaternion(1 + x, 2 + x**2, 3 + x**3, 4 + x**4)\n    assert q1 * q2 == Quaternion(-30*x**2 - 12*x**3 - 12*x**4 + x, \n                                 12*x + 10*x**2 + 12*x**3 - 8*x**4, \n                                 12*x + 6*x**2 + 20*x**3 + 6*x**4, \n                                 12*x - 8*x**2 - 6*x**3 + 22*x**4)\n    assert q2.pow(2) == Quaternion(-x**8 - x**6 - 30*x**4 + x**2, \n                                   2*x**5 + 4*x**7, \n                                   4*x**3 + 6*x**6, \n                                   4*x**4 + 2*x**5 + 2*x**7)\n    assert q2.exp() == Quaternion(E*cos(sqrt(x**8 + 4*x**6 + 4*x**4)), \n                                  x**2*sqrt(x**8 + 4*x**6 + 4*x**4)*E*sin(sqrt(x**8 + 4*x**6 + 4*x**4))/(x**8 + 4*x**6 + 4*x**4), \n                                  x**3*sqrt(x**8 + 4*x**6 + 4*x**4)*E*sin(sqrt(x**8 + 4*x**6 + 4*x**4))/(x**8 + 4*x**6 + 4*x**4), \n                                  x**4*sqrt(x**8 + 4*x**6 + 4*x**4)*E*sin(sqrt(x**8 + 4*x**6 + 4*x**4))/(x**8 + 4*x**6 + 4*x**4))\n    assert q2.pow_cos_sin(2) == Quaternion(x**2*(x**8 + 4*x**6 + 4*x**4)*cos(2*acos(x/sqrt(x**8 +"], "sample_1022": ["def test_split_symbols_custom():\n    transformations = standard_transformations + (split_symbols_custom(lambda x: x != 'xyz'),)\n    cases = {\n        'xyz': 'xyz',\n        'xy': 'x*y',\n        'x y z': 'x*y*z',\n    }\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) ==\n               parse_expr(expected))\n\n    transformations = standard_transformations + (split_symbols_custom(lambda x: x.startswith('split')),)\n    cases = {\n        'splitme': 'split*me',\n        'dontsplitme': 'dontsplitme',\n    }\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) ==\n               parse_expr(expected))\n", "def test_custom_symbol_splitting():\n    transformations = standard_transformations + (\n        split_symbols_custom(lambda x: x not in ('unsplittable', 'names')),\n        implicit_multiplication,\n    )\n    local_dict = { 'e': sympy.E }\n    cases = {\n        'xe': 'E*x',\n        'Iy': 'I*y',\n        'ee': 'E*E',\n        'unsplittablex': 'unsplittable*x',\n        'namesy': 'names*y',\n    }\n    for case, expected in cases.items():\n        assert(parse_expr(case, local_dict=local_dict,\n                          transformations=transformations) ==\n               parse_expr(expected))\n\n        return False\n    transformations = standard_transformations + (\n        split_symbols_custom(never_split),\n        implicit_multiplication,\n    )\n    assert(parse_expr('xe', local_dict=local_dict,\n                      transformations=transformations) == sympy.Symbol('xe'))\n", "def test_split_symbols_custom():\n    transformations = standard_transformations + (\n        split_symbols_custom(lambda x: False),\n        implicit_multiplication,\n    )\n    cases = {\n        'unsplittable': 'unsplittable',\n        'names': 'names',\n        'splitme': 's*p*l*i*t*m*e',\n    }\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) ==\n               parse_expr(expected))\n", "def test_split_symbols_custom():\n    transformations = standard_transformations + (split_symbols_custom(lambda s: s != 'unsplittable'),)\n    transformations2 = transformations + (implicit_multiplication,)\n    cases = {\n        'unsplittable': 'unsplittable',\n        'xunsplittable': 'x*unsplittable',\n        'unsplittablex': 'unsplittable*x',\n        'unsplittablexunsplittable': 'unsplittable*x*unsplittable',\n    }\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations2) ==\n               parse_expr(expected))\n\n    # Make sure it still splits other names\n    assert(parse_expr('abcdef', transformations=transformations2) ==\n           parse_expr('a*b*c*d*e*f'))\n", "def test_split_symbols_custom():\n    transformations = standard_transformations + (convert_xor,)\n    transformations2 = transformations + (split_symbols_custom(can_split),\n                                          implicit_multiplication)\n\n    cases = {\n        'unsplittable': 'unsplittable',\n        'namesx': 'names*x',\n        'xyz': 'x*y*z',\n    }\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations2)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n"], "sample_1019": ["def test_monotonic_sign():\n    from sympy import Dummy, S\n    nn = Dummy(integer=True, nonnegative=True)\n    p = Dummy(integer=True, positive=True)\n    p2 = Dummy(integer=True, positive=True)\n    assert _monotonic_sign(nn + 1) == 1\n    assert _monotonic_sign(p - 1).is_Symbol\n    assert _monotonic_sign(nn*p + 1) == 1\n    assert _monotonic_sign(p2*p + 1) == 2\n    assert _monotonic_sign(nn - 1) is None\n", "def test_monotonic_sign():\n    assert _monotonic_sign(x**2 + 1) == 1\n    assert _monotonic_sign(-x**2 - 1) == -1\n    assert _monotonic_sign(x**3) is None\n    assert _monotonic_sign(2*x + 3) is None\n    assert _monotonic_sign(x**2 + 2*x + 1) == 1\n    assert _monotonic_sign(-x**2 - 2*x - 1) == -1\n    from sympy import symbols\n    p = symbols('p', positive=True)\n    n = symbols('n', negative=True)\n    np = symbols('np', nonpositive=True)\n    nn = symbols('nn', nonnegative=True)\n    assert _monotonic_sign(p) == 1\n    assert _monotonic_sign(n) == -1\n    assert _monotonic_sign(np) == -1\n    assert _monotonic_sign(nn) == 1\n    assert _monotonic_sign(p**2) == 1\n    assert _monotonic_sign(p**3) == 1\n    assert _monotonic_sign(p**4) == 1\n    assert _monotonic_sign(n**2) == 1\n    assert _monotonic_sign(n**3) == -1\n    assert _monotonic_sign(n**4) == 1\n", "def test_factor_terms_Dummy():\n    expr = x - 2\n    assert factor_terms(expr) == expr\n    d = Dummy()\n    expr = x - 2*d\n    assert factor_terms(expr) == -2*d + x\n    expr = x - 2*d**2\n    assert factor_terms(expr) == -2*d**2 + x\n    expr = x - 2*d**2 - 4*d - 3\n    assert factor_terms(expr) == -(2*d + 3)*(d + 1) + x\n", "def test_monotonic_sign():\n    assert _monotonic_sign(x) is None\n    assert _monotonic_sign(x**2) == S.One\n    assert _monotonic_sign(-x**2) == S.NegativeOne\n    assert _monotonic_sign(x**(2*y + 3)) is None\n    assert _monotonic_sign(sqrt(x)) == S.One\n    assert _monotonic_sign(-sqrt(x)) == S.NegativeOne\n    assert _monotonic_sign(x**(S(3)/2)) == S.One\n    assert _monotonic_sign(x**(S(5)/2)) == S.One\n    assert _monotonic_sign(x**(S(3)/2)*y**(S(5)/2)) is None\n    assert _monotonic_sign(x**(S(3)/2)*y**(S(5)/2),) is None\n    assert _monotonic_sign(-x**(S(3)/2)) == S.NegativeOne\n    assert _monotonic_sign(-x**(S(5)/2)) == S.NegativeOne\n    assert _monotonic_sign(x**(S(1)/3)) == S.One\n    assert _monotonic_sign(-x**(S(1)/3)) == Dummy('neg', negative=True)\n    assert _monotonic_sign(x**(S(2)/3)) == S.One\n    assert _monotonic_sign(x**(S(4)/3)) == S.One\n    assert _monotonic_sign(-x**(S(2)/3)) == S.NegativeOne\n    assert _monotonic_sign(-x**(S(4)/3)) == S.NegativeOne\n    assert _monotonic_sign(x**(S(2)/3)*y**(S(4)/3)) is None\n    assert _monotonic_sign(x**(S(2)/3)*y**(S(4)/3),) is None\n    assert _monotonic_sign(x**(S(1)/3)*y**(S(2)/3)) is None\n    assert _monotonic_sign(x**(S(2)/3)*y**(S(1)/3)) is None\n    assert _monotonic_sign(x**(S(1)/3)*y**(S(4)/3)) is None\n    assert _monotonic_sign(x**(S(4)/3)*y**(", "def test_monotonic_sign():\n    assert _monotonic_sign(x) is None\n    assert _monotonic_sign(1) == 1\n    assert _monotonic_sign(-1) == -1\n    assert _monotonic_sign(x**2) == 1\n    assert _monotonic_sign(x**3) is None\n    assert _monotonic_sign(-x**3) is None\n    assert _monotonic_sign(x**4) == 1\n    assert _monotonic_sign(-x**4) == -1\n    assert _monotonic_sign(2*x + 3) is None\n    assert _monotonic_sign((x + 1)**2) == 1\n    assert _monotonic_sign(-(x + 1)**2) == -1\n    assert _monotonic_sign((x - 1)**2) == 1\n    assert _monotonic_sign(-(x - 1)**2) == -1\n    assert _monotonic_sign(1 - x**2) == Dummy('npos', nonpositive=True)\n    assert _monotonic_sign(x**2 - 1) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(-x**2 - 1) == Dummy('neg', negative=True)\n    assert _monotonic_sign(x**2 + 1) == Dummy('pos', positive=True)\n"], "sample_1023": ["def test_sieve_attributes():\n    s = Sieve()\n    assert s._n > 0\n    assert len(s._list) > 0\n    assert len(s._tlist) > 0\n    assert len(s._mlist) > 0\n    assert s._list == s.primerange(2, s._list[-1] + 1)\n    assert s._tlist[2] == 1\n    assert s._mlist[2] == -1\n", "def test_sieve_slice():\n    s = Sieve()\n    assert s[1:10] == [2, 3, 5, 7, 11, 13, 17, 19, 23]\n    assert s[5:10] == [11, 13, 17, 19, 23]\n    assert s[5:5] == []\n    assert s[:5] == [2, 3, 5, 7, 11]\n    assert s[5:] == [11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n    assert s[:] == s._list\n", "def test_sieve_attributes():\n    s = Sieve()\n    assert hasattr(s, '_n')\n    assert hasattr(s, '_list')\n    assert hasattr(s, '_tlist')\n    assert hasattr(s, '_mlist')\n    assert len(s._list) == len(s._tlist) == len(s._mlist) == s._n\n", "def test_sieve_extend_to_no():\n    sieve._reset()\n    sieve.extend_to_no(10)\n    assert len(sieve._list) >= 10\n    sieve.extend_to_no(50)\n    assert len(sieve._list) >= 50\n    assert sieve._list[9] == 23\n    assert sieve._list[49] == 229\n", "def test_sieve_reset():\n    sieve._reset(prime=True)\n    assert len(sieve._list) == 6\n    sieve.extend_to_no(100)\n    assert len(sieve._list) > 100\n    sieve._reset(prime=True)\n    assert len(sieve._list) == 6\n    sieve._reset(totient=True)\n    assert len(sieve._tlist) == 6\n    sieve.totientrange(1000, 1005)\n    assert len(sieve._tlist) > 1000\n    sieve._reset(totient=True)\n    assert len(sieve._tlist) == 6\n    sieve._reset(mobius=True)\n    assert len(sieve._mlist) == 6\n    sieve.mobiusrange(1000, 1005)\n    assert len(sieve._mlist) > 1000\n    sieve._reset(mobius=True)\n    assert len(sieve._mlist) == 6\n    sieve._reset(prime=True, totient=True, mobius=True)\n    assert len(sieve._list) == 6\n    assert len(sieve._tlist) == 6\n    assert len(sieve._mlist) == 6\n"], "sample_1025": ["def test_print_not():\n    p = PythonCodePrinter()\n    expr = ~x\n    assert p.doprint(expr) == 'not x'\n", "def test_PythonCodePrinter_Not():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.doprint(~x) == 'not x'\n    assert prntr.doprint(~(x & y)) == 'not (x and y)'\n    assert prntr.doprint(~(x | y)) == 'not (x or y)'\n", "def test_pycode_infinity():\n    assert pycode(oo) == 'float(\\'inf\\')'\n    assert pycode(-oo) == 'float(\\'-inf\\')'\n    assert pycode(zoo) == 'float(\\'nan\\')'\n", "def test_printing_non_finite_numbers():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(oo) == 'float(\\'inf\\')'\n    assert prntr.doprint(-oo) == 'float(\\'-inf\\')'\n    assert prntr.doprint(zoo) == 'float(\\'nan\\')'\n", "def test_PythonCodePrinter_infinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n    assert prntr.doprint(zoo) == \"float('nan')\"\n"], "sample_1024": ["def test_mpf_log():\n    from mpmath.libmp.libmpf import mpf_log\n    assert mpf_log((1, long(16), 1, 2)) == 4\n    assert mpf_log((0, long(1), 1, 1)) == -1\n", "def test_as_coeff_Mul():\n    x = Symbol('x')\n    assert S(2).as_coeff_Mul() == (2, S(1))\n    assert S(2).as_coeff_Mul(x) == (2, S(1))\n    assert pi.as_coeff_Mul() == (1, pi)\n    assert pi.as_coeff_Mul(x) == (1, pi)\n    assert (2*pi).as_coeff_Mul() == (2, pi)\n    assert (2*pi).as_coeff_Mul(x) == (2*pi, S(1))\n", "def test_Float_comparison():\n    assert Float(1.1) > 1\n    assert Float(1.1) >= 1\n    assert Float(1.1) < 2\n    assert Float(1.1) <= 2\n    assert Float(1.1) == Float(1.1)\n    assert Float(1.1) != Float(1.2)\n    assert Float(1.1) > Float(1.0)\n    assert Float(1.1) >= Float(1.0)\n    assert Float(1.1) < Float(2.0)\n    assert Float(1.1) <= Float(2.0)\n", "def test_Rational_precision():\n    # Make sure Rational inputs for keyword args work\n    assert Float(Rational(1, 1), dps=Integer(15))._prec == 53\n    assert Float(Rational(1, 1), precision=Integer(15))._prec == 15\n    assert type(Float(Rational(1, 1), precision=Integer(15))._prec) == int\n    assert sympify(srepr(Float(Rational(1, 1), precision=15))) == Float(Rational(1, 1), precision=15)\n", "def test_comp_nonstandard_inputs():\n    raises(ValueError, lambda: comp(\"a\", 1, \"\"))\n    assert comp(1, \"a\", \"\") is False\n    raises(ValueError, lambda: comp(1, \"a\", \"b\"))\n    raises(ValueError, lambda: comp(1, 1, \"b\"))\n"], "sample_1026": ["def test_lambdify_lambda():\n    f = lambdify(x, Lambda(x, x**2))\n    assert f(2) == 4\n", "def test_lambdify_ITE_multiple_conditions():\n    # Test ITE with multiple conditions\n    expr = ITE((x > 0, y), (x < 0, z), (True, x))\n    f = lambdify((x, y, z), expr)\n    assert f(1, 2, 3) == 2\n    assert f(-1, 2, 3) == 3\n    assert f(0, 2, 3) == 0\n", "compilation error", "def test_lambdastr():\n    # Test if lambdastr returns a string that can be evaluated to a lambda function\n    f = lambdastr(x, x**2)\n    assert f == 'lambda x: (x**2)'\n    assert eval(f)(2) == 4\n", "def test_lambdify_lambda():\n    f = lambdify(x, Lambda(x, x**2))\n    assert f(2) == 4\n"], "sample_1029": ["def test_MonogenicFiniteExtension():\n    A = FiniteExtension(Poly(x**2 + 1, x))\n    assert srepr(A) == \"FiniteExtension(Poly(x**2 + 1, x, domain='ZZ'))\"\n", "def test_MonogenicFiniteExtension():\n    A = FiniteExtension(Poly(x**2 + 1, x))\n    assert srepr(A) == \"FiniteExtension(Poly(x**2 + 1, x, domain='ZZ'))\"\n    assert eval(srepr(A)) == A\n", "def test_MonogenicFiniteExtension():\n    assert srepr(FiniteExtension(Poly(x**2 + 1, x))) == \\\n        \"FiniteExtension(Poly(x**2 + 1, x, domain='ZZ'))\"\n    assert eval(srepr(FiniteExtension(Poly(x**2 + 1, x))), ENV) == FiniteExtension(Poly(x**2 + 1, x))\n", "def test_MonogenicFiniteExtension():\n    A = FiniteExtension(Poly(x**2 + 1, x))\n    assert srepr(A) == \"FiniteExtension(Poly(x**2 + 1, x, domain='ZZ'))\"\n    assert eval(srepr(A), ENV) == A\n", "def test_Predicate():\n    from sympy import Predicate\n    sT(Predicate(\"P\"), \"Predicate('P')\")\n"], "sample_1028": ["def test_Mod_is_odd_even():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True, positive=True)\n    z = Symbol('z', integer=True, negative=True)\n    assert (x % 2).is_even is True\n    assert (x % 2).is_odd is None\n    assert Mod(x + 1, 2).is_even is None\n    assert Mod(x + 1, 2).is_odd is True\n    assert Mod(x + y, y).is_even is None\n    assert Mod(x + y, y).is_odd is None\n    assert Mod(x + z, z).is_even is None\n    assert Mod(x + z, z).is_odd is None\n", "def test_Mod_is_finite():\n    assert Mod(x, y).is_finite is None\n    assert Mod(x, 1).is_finite is True\n    assert Mod(x, 2).is_finite is True\n    assert Mod(x, oo).is_finite is None\n    assert Mod(x, -oo).is_finite is None\n    assert Mod(oo, x).is_finite is False\n    assert Mod(-oo, x).is_finite is False\n    assert Mod(oo, oo).is_finite is None\n    assert Mod(-oo, -oo).is_finite is None\n", "def test_Mod_is_integer():\n    x = Symbol('x', integer=True, positive=True)\n    y = Symbol('y', integer=True, positive=True)\n    assert Mod(x, y).is_integer\n    assert Mod(x, 3).is_integer\n    assert Mod(3, x).is_integer\n    assert Mod(x, y).is_integer is True\n    assert Mod(x, 3).is_integer is True\n    assert Mod(3, x).is_integer is True\n    assert Mod(0.5, x).is_integer is False\n    assert Mod(x, 0.5).is_integer is None\n", "def test_Mod_with_integer_powers():\n    assert Mod(x**3, 2**3) == Mod(x**3, 8)\n    assert Mod(x**3, 2**-3) == Mod(x**3, Rational(1, 8))\n    assert Mod(x**-3, 2**3) == Mod(x**-3, 8)\n    assert Mod(x**-3, 2**-3) == Mod(x**-3, Rational(1, 8))\n", "def test_Mod_is_odd():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', odd=True)\n    m = Symbol('m', even=True)\n\n    assert Mod(n, 2).is_odd is None\n    assert Mod(k, 2).is_odd is True\n    assert Mod(m, 2).is_odd is False\n"], "sample_1027": ["def test_issue_13285():\n    f = Poly(x**3 + x + 1, x)\n    assert f.subs({x: 1}) == f.as_expr().subs({x: 1})\n", "def test_poly_lift():\n    f = Poly(x + 1, x, domain=QQ)\n    assert f.lift() == Poly(x + 1, x, domain=ZZ)\n\n    f = Poly(x + 1, x, domain=RR)\n    assert f.lift() == Poly(x + 1, x, domain=QQ)\n\n    f = Poly(x + 1, x, domain=ZZ)\n    raises(CoercionFailed, lambda: f.lift())\n", "def test_Poly_eq_method():\n    assert Poly(x, x, modulus=3).eq(Poly(x, x, modulus=3)) is True\n    assert Poly(x, x, modulus=3).eq(Poly(x, x, modulus=5)) is False\n", "def test_Poly_eq_ne_with_improved_coverage():\n    # Improve coverage of Poly.__eq__ and Poly.__ne__\n    assert Poly(x + 1, x) != Poly(x + 1, x, domain='QQ')\n    assert Poly(x + 1, x, modulus=2) != Poly(x + 1, x, modulus=3)\n    assert Poly(x + 1, x, modulus=2) == Poly(x + 1, x, modulus=2)\n\n    assert Poly(x + 1, x) != x\n    assert Poly(x + 1, x) != 2*x + 1\n\n    assert Poly(x + 1, x) != Poly(x + 1, y)\n    assert Poly(x + 1, x) != Poly(x + 1, x, y)\n", "def test_monic_with_generators():\n    f = 2*x**2 + x + 1\n\n    assert Poly(f, x).as_expr() == f\n    assert Poly(f, x).as_expr(x) == f.subs(x, x)\n    assert Poly(f, x, y).as_expr(x) == f.subs(x, x).subs(y, y)\n    assert Poly(f, x, y).as_expr(y) == f.subs(x, x).subs(y, y)\n\n    assert Poly(f, x).as_expr({x: y}) == f.subs(x, y)\n    assert Poly(f, x).as_expr({x: y}, x) == f.subs(x, y).subs(x, x)\n    assert Poly(f, x, y).as_expr({x: y}, x) == f.subs(x, y).subs(x, x).subs(y, y)\n"], "sample_1031": ["def test_get_unit():\n    ms = UnitSystem((m, s), (c,))\n    Js = Quantity(\"Js\")\n    Js.set_dimension(action)\n    Js.set_scale_factor(1)\n\n    raises(KeyError, lambda: ms.get_unit(Js.dimension))\n\n    mks = ms.extend((kg,), (Js,))\n    assert mks.get_unit(Js.dimension) == Js\n", "def test_get_unit():\n    ms = UnitSystem((m, s), (c,))\n    assert ms.get_unit(length) == m\n    assert ms.get_unit(velocity) == c\n    raises(KeyError, lambda: ms.get_unit(mass))\n", "def test_get_quantity_from_name():\n    ms = UnitSystem((m, s), (c,))\n    assert ms.get_quantity_from_name(\"meter\") == m\n    assert ms.get_quantity_from_name(\"second\") == s\n    assert ms.get_quantity_from_name(\"speed_of_light\") == c\n    raises(KeyError, lambda: ms.get_quantity_from_name(\"nonexistent_unit\"))\n", "def test_get_units_dim():\n    # Create a UnitSystem with base units of length, mass, and time\n    ms = UnitSystem((m, kg, s), (c,))\n\n    # Check that get_units_dim returns the correct units for each dimension\n    assert ms.get_units_dim(length) == m\n    assert ms.get_units_dim(mass) == kg\n    assert ms.get_units_dim(time) == s\n    assert ms.get_units_dim(velocity) == c\n\n    # Check that get_units_dim raises a ValueError for an unknown dimension\n    raises(ValueError, lambda: ms.get_units_dim(action))\n", "def test_get_quantity_from_unit_system():\n    ms = UnitSystem((m, s), (c,))\n    Js = Quantity(\"Js\")\n    Js.set_dimension(action)\n    Js.set_scale_factor(1)\n    mks = ms.extend((kg,), (Js,))\n\n    assert mks.get_quantity(\"meter\") == m\n    assert mks.get_quantity(\"second\") == s\n    assert mks.get_quantity(\"c\") == c\n    assert mks.get_quantity(\"Js\") == Js\n\n    raises(KeyError, lambda: mks.get_quantity(\"ampere\"))\n"], "sample_1030": ["def test_closest_points():\n    raises(ValueError, lambda: closest_points(Point(0, 0)))\n    raises(ValueError, lambda: closest_points(Point(0, 0), Point(0, 0)))\n    points = [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5)]\n    assert closest_points(*points) == {(Point2D(0, 0), Point2D(1, 1))}\n    points = [(0, 0), (1, 1), (3, 3), (4, 4), (5, 5)]\n    assert closest_points(*points) == {(Point2D(0, 0), Point2D(1, 1)), (Point2D(3, 3), Point2D(4, 4))}\n", "def test_closest_points():\n    raises(ValueError, lambda: closest_points(Point(0, 0)))\n    points = [(1, -1), (1, -2), (3, -1), (-5, -2), (15, -4)]\n    assert len(closest_points(*points)) == 1\n    triangle_points = [(0, 0), (3, 0), (3, 4)]\n    assert len(closest_points(*triangle_points)) == 1\n    segment_points = [(0, 0), (2, 0), (1, 0)]\n    assert len(closest_points(*segment_points)) == 1\n    same_x_points = [(0, 0), (0, 2), (0, 1)]\n    assert len(closest_points(*same_x_points)) == 1\n", "def test_closest_points():\n    points = [(1, 1), (1, 2), (3, 1), (-5, 2), (15, 4)]\n    assert closest_points(*points) == {(Point2D(1, 1), Point2D(1, 2))}\n    triangle = Polygon((0, 0), (3, 0), (3, 4))\n    assert closest_points(*triangle.vertices) == {(Point2D(0, 0), Point2D(3, 0))}\n    raises(ValueError, lambda: closest_points((1, 1)))\n    raises(ValueError, lambda: closest_points((1, 1), (1, 1)))\n", "def test_closest_points():\n    p1, p2, p3, p4 = Point(0, 0), Point(1, 0), Point(0, 1), Point(2, 2)\n    assert closest_points(p1, p2) == {(p1, p2)}\n    assert closest_points(p1, p2, p3) == {(p1, p2), (p1, p3)}\n    assert closest_points(p1, p2, p3, p4) == {(p1, p2), (p1, p3)}\n    assert closest_points(p1, p1) == set()\n    raises(ValueError, lambda: closest_points(p1))\n", "def test_closest_points():\n    points = [(0, 0), (1, 1), (3, 3), (1, 3)]\n    assert closest_points(*points) == {(Point2D(0, 0), Point2D(1, 1))}\n    points = [(0, 0), (1, 0), (1, 1), (3, 3), (1, 3)]\n    assert closest_points(*points) == {(Point2D(0, 0), Point2D(1, 0)), (Point2D(1, 0), Point2D(1, 1))}\n    raises(ValueError, lambda: closest_points(Point(0, 0)))\n"], "sample_1032": ["def test_Multiple_MinMax_with_common_element():\n    from sympy.abc import a, b, c, d\n    assert Min(Max(a, b), Max(a, c)) == Max(a, Min(b, c))\n    assert Min(Max(a, b), Max(b, c)) == Max(b, Min(a, c))\n    assert Min(Max(a, b), Max(c, a)) == Max(a, Min(b, c))\n    assert Min(Max(a, b), Max(c, b)) == Max(b, Min(a, c))\n    assert Min(Max(a, b), Max(c, d)) == Min(Max(a, b), Max(c, d))\n\n    assert Max(Min(a, b), Min(a, c)) == Min(a, Max(b, c))\n    assert Max(Min(a, b), Min(b, c)) == Min(b, Max(a, c))\n    assert Max(Min(a, b), Min(c, a)) == Min(a, Max(b, c))\n    assert Max(Min(a, b), Min(c, b)) == Min(b, Max(a, c))\n    assert Max(Min(a, b), Min(c, d)) == Max(Min(a, b), Min(c, d))\n", "def test_incompatible_function_arguments():\n    from sympy.abc import x, y, z\n    from sympy.functions.elementary.trigonometric import sin, cos\n    from sympy.functions.elementary.exponential import exp\n    from sympy.functions.special.error_functions import erf\n    from sympy.functions.special.delta_functions import DiracDelta\n\n    incompatible_functions = [sin, cos, exp, erf, DiracDelta]\n\n    for func in incompatible_functions:\n        assert Min(x, func(x)).func == Min\n        assert Min(x, func(x)).args == (x, func(x))\n        assert Max(x, func(x)).func == Max\n        assert Max(x, func(x)).args == (x, func(x))\n\n    # Mixing incompatible functions in Min/Max\n    for func1, func2 in it.product(incompatible_functions, repeat=2):\n        if func1 != func2:\n            assert Min(x, func1(x), func2(y)).func == Min\n            assert Min(x, func1(x), func2(y)).args == (x, func1(x), func2(y))\n            assert Max(x, func1(x), func2(y)).func == Max\n            assert Max(x, func1(x), func2(y)).args == (x, func1(x), func2(y))\n\n    # Using more than one variable with incompatible functions in Min/Max\n    assert Min(x, sin(x), y, cos(y), z, exp(z)).func == Min\n    assert Min(x, sin(x), y, cos(y), z, exp(z)).args == (x, sin(x), y, cos(y), z, exp(z))\n    assert Max(x, sin(x), y, cos(y), z, exp(z)).func == Max\n    assert Max(x, sin(x), y, cos(y), z, exp(z)).args == (x, sin(x), y, cos(y), z, exp(z))\n", "def test_sqrt():\n    from sympy.abc import x\n    assert sqrt(-1) == I\n    assert sqrt(4) == 2\n    assert sqrt(0) == 0\n    assert sqrt(x**2) == Abs(x)\n    assert sqrt((2*x)**2) == 2*Abs(x)\n    assert sqrt(x**4) == x**2\n    assert sqrt(2*x**3) == sqrt(2)*x*Abs(x)\n    assert sqrt(x**2*y**4) == x*y**2*Abs(y)\n    assert sqrt(36*x**4) == 6*x**2\n    assert sqrt(0, evaluate=False) == sqrt(0)\n    assert sqrt(x**2, evaluate=False) == sqrt(x**2)\n", "def test_rewrite_as_Abs_max():\n    from sympy.functions.elementary.complexes import Abs\n    from sympy.abc import x, y, z\n    e = Max(x, y, z)\n    a = e.rewrite(Abs)\n    assert not a.has(Min, Max)\n    assert a == (x + y + z + Abs(x - y) + Abs(x - z) + Abs(y - z))/3 + Abs((x + y + z + Abs(x - y) + Abs(x - z) + Abs(y - z))/3 - x) + Abs((x + y + z + Abs(x - y) + Abs(x - z) + Abs(y - z))/3 - y) + Abs((x + y + z + Abs(x - y) + Abs(x - z) + Abs(y - z))/3 - z)\n\n    e = Max(x, y)\n    a = e.rewrite(Abs)\n    assert not a.has(Min, Max)\n    assert a == (x + y + Abs(x - y))/2 + Abs((x + y + Abs(x - y))/2 - x) + Abs((x + y + Abs(x - y))/2 - y)\n", "def test_issue_18923():\n    from sympy import Symbol, Min, Max, Heaviside\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    # Test Max and Min with derivatives\n    assert Max(x, y).diff(x) == Heaviside(x - y)\n    assert Min(x, y).diff(x) == Heaviside(-x + y)\n    assert Max(x, y, z).diff(x) == Heaviside(x - y)*Heaviside(x - z)\n    assert Min(x, y, z).diff(x) == Heaviside(-x + y)*Heaviside(-x + z)\n"], "sample_1033": ["def test_Mul_hermitian():\n    a, b = symbols('a b', hermitian=True)\n    c, d = symbols('c d')\n\n    assert (a*b).is_hermitian\n    assert (a*b).is_antihermitian is False\n\n    assert (a*c).is_hermitian is None\n    assert (a*c).is_antihermitian is None\n\n    assert (c*d).is_hermitian is None\n    assert (c*d).is_antihermitian is None\n\n    assert (I*a*b).is_hermitian is False\n    assert (I*a*b).is_antihermitian\n", "def test_Add_as_numer_denom():\n    a, b = symbols('a b')\n    assert (a + b).as_numer_denom() == (a + b, 1)\n    assert (1 + 2).as_numer_denom() == (3, 1)\n    assert (1 + a).as_numer_denom() == (a + 1, 1)\n", "def test_issue_15873():\n    # Test that Add handles complex zero properly\n    e = -2*I + (1 + I)**2\n    assert e.is_zero is None\n    assert e.as_real_imag()[0].is_zero\n    assert e.as_real_imag()[1].is_zero\n    assert (e + 1).as_real_imag()[0].is_zero is False\n    assert (e + 1).as_real_imag()[1].is_zero\n    assert (e - 1).as_real_imag()[0].is_zero is False\n    assert (e - 1).as_real_imag()[1].is_zero\n", "def test_issue_18478():\n    a, b = symbols('a b', real=True)\n    c, d = symbols('c d', real=True, positive=True)\n\n    assert (a + b).as_real_imag() == (a + b, 0)\n    assert (a + c).as_real_imag() == (a + c, 0)\n    assert (c + d).as_real_imag() == (c + d, 0)\n\n    assert (a + I*b).as_real_imag() == (a, b)\n    assert (a + I*c).as_real_imag() == (a, c)\n    assert (c + I*d).as_real_imag() == (c, d)\n", "def test_issue_16339():\n    assert Add(*[1]*1001).is_even is None  # len(args) > 1000\n    assert Add(*[1]*1001).subs({1: 2}).is_even is None  # len(args) > 1000\n    assert Add(*[1]*999).is_even is None  # len(args) > 100\n    assert Add(*[1]*999).subs({1: 2}).is_even is None  # len(args) > 100\n    assert Add(*[1]*100).is_even is None  \n    assert Add(*[1]*100).subs({1: 2}).is_even is None  \n    assert Add(*[1]*99).is_even is True  \n    assert Add(*[1]*99).subs({1: 2}).is_even is True  \n"], "sample_1034": ["def test_apply_grover():\n    numqubits = 2\n    v = lambda qubits: qubits == IntQubit(2, nqubits=numqubits)\n    expected = IntQubit(2, nqubits=numqubits)\n    assert qapply(apply_grover(v, numqubits)) == expected\n\n    numqubits = 3\n    v = lambda qubits: qubits == IntQubit(3, nqubits=numqubits)\n    expected = IntQubit(3, nqubits=numqubits)\n    assert qapply(apply_grover(v, numqubits)) == expected\n", "def test_apply_grover():\n    nqubits = 2\n    oracle = lambda qubits: qubits == IntQubit(2, nqubits=nqubits)\n    expected = IntQubit(2, nqubits=nqubits)\n    assert qapply(apply_grover(oracle, nqubits)) == expected\n", "def test_apply_grover():\n    nqubits = 2\n    oracle = lambda qubits: qubits == IntQubit(2, nqubits=nqubits)\n    result = apply_grover(oracle, nqubits)\n    expected = IntQubit(2, nqubits=nqubits)\n    assert qapply(result) == expected\n", "def test_apply_grover():\n    numqubits = 2\n    f = lambda qubits: qubits == IntQubit(2, nqubits=numqubits)\n    expected = IntQubit(2, nqubits=numqubits)\n    assert qapply(apply_grover(f, numqubits)) == expected\n\n    numqubits = 3\n    f = lambda qubits: qubits == IntQubit(5, nqubits=numqubits)\n    result = apply_grover(f, numqubits)\n    # check if IntQubit(5) has the highest probability\n    max_prob = 0\n    for i in range(2**numqubits):\n        prob = (result.coeff(IntQubit(i, numqubits=numqubits)))**2\n        if prob > max_prob:\n            max_prob = prob\n            max_state = i\n    assert max_state == 5\n", "def test_apply_grover():\n    nqubits = 2\n    expected = IntQubit(2, nqubits=nqubits)\n    v = lambda qubits: qubits == expected\n    assert qapply(apply_grover(v, nqubits)) == expected\n\n    nqubits = 3\n    expected = IntQubit(5, nqubits=nqubits)\n    v = lambda qubits: qubits == expected\n    assert qapply(apply_grover(v, nqubits)) == expected\n\n    nqubits = 4\n    expected = IntQubit(2, nqubits=nqubits)\n    v = lambda qubits: qubits == expected\n    assert qapply(apply_grover(v, nqubits)) == expected\n"], "sample_1035": ["def test_measure_all():\n    q = IntQubit(0)/sqrt(2) + IntQubit(1)/sqrt(2)\n    result = q.measure_all()\n    assert len(result) == 2\n    assert result[0][0] == IntQubit(0)\n    assert result[1][0] == IntQubit(1)\n    assert result[0][1] == 1/2\n    assert result[1][1] == 1/2\n", "def test_grover():\n    nqubits = 2\n    basis_states = superposition_basis(nqubits)\n    v = OracleGate(nqubits, return_one_on_one)\n    result = apply_grover(basis_states, v)\n    assert qapply(result) == IntQubit(1, nqubits=nqubits)\n", "def test_grover():\n    nqubits = 2\n    basis_states = superposition_basis(nqubits)\n    v = OracleGate(nqubits, return_one_on_one)\n    result = apply_grover(basis_states, v)\n    assert qapply(result) == IntQubit(1, nqubits=nqubits)\n", "def test_measure_all():\n    nqubits = 2\n    basis_states = superposition_basis(nqubits)\n    results = measure_all(basis_states, normalize=True)\n    expected_results = [(IntQubit(0, nqubits=nqubits), 1/4),\n                        (IntQubit(1, nqubits=nqubits), 1/4),\n                        (IntQubit(2, nqubits=nqubits), 1/4),\n                        (IntQubit(3, nqubits=nqubits), 1/4)]\n    assert results == expected_results\n", "def test_measure_all():\n    # Test with a single qubit\n    q = IntQubit(0)\n    result = measure_all(q)\n    assert len(result) == 1\n    assert result[0][0] == q\n    assert result[0][1] == 1\n\n    # Test with a superposition\n    q = (IntQubit(0) + IntQubit(1))/sqrt(2)\n    result = measure_all(q)\n    assert len(result) == 2\n    assert result[0][0] == IntQubit(0)\n    assert result[0][1] == 1/2\n    assert result[1][0] == IntQubit(1)\n    assert result[1][1] == 1/2\n\n    # Test with a larger qubit\n    q = IntQubit(2, nqubits=2)\n    result = measure_all(q)\n    assert len(result) == 1\n    assert result[0][0] == q\n    assert result[0][1] == 1\n"], "sample_1036": ["def test_matmul_noncommutative():\n    from sympy import Symbol\n    a, b = Symbol('a', commutative=False), Symbol('b', commutative=False)\n    assert Mul(a, b).doit() == Mul(a, b)\n    assert Mul(a, b).args_cnc() == [[], [a, b]]\n    assert Mul(a, 2, b).args_cnc() == [[2], [a, b]]\n    assert Mul(2, a, b).args_cnc() == [[2], [a, b]]\n    assert Mul(n, a, 2, b).args_cnc() == [[2*n], [a, b]]\n", "def test_mul_with_symbolic_power():\n    a = Symbol('a', positive=True)\n    assert Mul(a, a**x).simplify() == a**(x + 1)\n", "def test_mul_as_coeff_Mul():\n    assert Mul(2, A).as_coeff_Mul()[0] == 2\n    assert Mul(2, A).as_coeff_Mul()[1] == A\n    assert Mul(2, 3, A).as_coeff_Mul()[0] == 6\n    assert Mul(2, 3, A).as_coeff_Mul()[1] == A\n", "def test_matmul_args_cnc_empty():\n    assert MatMul().args_cnc() == [[], []]\n    assert MatMul(n).args_cnc() == [[n], []]\n    assert MatMul(A).args_cnc() == [[], [A]]\n", "def test_Mul_with_coeff():\n    from sympy import Symbol, Mul\n    n = Symbol('n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    assert Mul(2, A).doit() == 2 * A\n    assert Mul(n, A).doit() == n * A\n"], "sample_1037": ["def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert MatMul(A, B, C).doit() == MatMul(A, MatMul(B, C)).doit()\n    assert MatMul(A, B, C).doit() == MatMul(MatMul(A, B), C).doit()\n    assert MatMul(A, Identity(2), B).doit() == MatMul(A, B).doit()\n    assert MatMul(A, ZeroMatrix(2, 2), B).doit() == ZeroMatrix(2, 2)\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n\n    assert MatMul(A, B, C).doit() == MatMul(A, B, C)\n    assert MatMul(A, B, C).doit(deep=True) == MatMul(A, B, C)\n\n    X = Matrix([[1, 2], [3, 4]])\n    Y = Matrix([[5, 6], [7, 8]])\n\n    assert MatMul(X, A).doit() == MatMul(X, A)\n    assert MatMul(X, A).doit(deep=True) == ImmutableMatrix([[A[0, 0] + 2*A[1, 0], A[0, 1] + 2*A[1, 1]], [3*A[0, 0] + 4*A[1, 0], 3*A[0, 1] + 4*A[1, 1]]])\n\n    assert MatMul(X, Y, A).doit() == MatMul(X, Y, A)\n    assert MatMul(X, Y, A).doit(deep=True) == ImmutableMatrix([[19*A[0, 0] + 22*A[1, 0], 19*A[0, 1] + 22*A[1, 1]], [43*A[0, 0] + 50*A[1, 0], 43*A[0, 1] + 50*A[1, 1]]])\n", "def test_MatMul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert (A*B*C).doit() == MatMul(A, B, C).doit()\n    assert (A*B*C).doit() == A.doit()*B.doit()*C.doit()\n    assert (A*B*C).doit(deep=False) == MatMul(A, B, C)\n    assert MatMul(A, B, C).doit() == MatMul(A, B, C).doit(deep=False)\n    assert MatMul(A, B, C).doit() == MatMul(A, B, C)\n", "def test_MatMul_entry():\n    i, j = symbols('i j')\n    M = MatMul(A, B)\n    assert M[i, j] == Sum(A[i, k]*B[k, j], (k, 0, n-1))\n    assert M[2, 3] == Sum(A[2, k]*B[k, 3], (k, 0, n-1))\n    assert M[i, j].subs(B, MatrixSymbol('C', m, l)) == Sum(A[i, k]*MatrixSymbol('C', m, l)[k, j], (k, 0, n-1))\n", "def test_MatMul_MatAdd_doit():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n    D = MatrixSymbol(\"D\", 2, 2)\n\n    M = Matrix([[1, 2], [3, 4]])\n    N = Matrix([[5, 6], [7, 8]])\n\n    assert (A * (B + C)).doit() == A * B + A * C\n    assert ((B + C) * A).doit() == B * A + C * A\n\n    assert (A * (B + C + D)).doit() == A * B + A * C + A * D\n    assert ((B + C + D) * A).doit() == B * A + C * A + D * A\n\n    assert (A * (M + N)).doit() == A * M + A * N\n    assert ((M + N) * A).doit() == M * A + N * A\n"], "sample_1038": ["def test_MatAdd_doit():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n    assert MatAdd(A, B, C).doit() == A + B + C\n    assert MatAdd(A, B, -B, C).doit() == A + C\n    assert MatAdd(A, B, -A, C).doit() == B + C\n    assert MatAdd(A, B, -B, -A).doit() == ZeroMatrix(2, 2)\n", "def test_MatrixElement_with_matrix_derivative():\n    A = MatrixSymbol(\"A\", 2, 2)\n    i, j = symbols(\"i, j\")\n    Aij = A[i, j]\n    assert Aij.diff(A[0, 0]) == KroneckerDelta(i, 0)*KroneckerDelta(j, 0)\n    assert Aij.diff(A[0, 1]) == KroneckerDelta(i, 0)*KroneckerDelta(j, 1)\n    assert Aij.diff(A[1, 0]) == KroneckerDelta(i, 1)*KroneckerDelta(j, 0)\n    assert Aij.diff(A[1, 1]) == KroneckerDelta(i, 1)*KroneckerDelta(j, 1)\n", "def test_equals():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert A.equals(B) == Eq(A, B)\n    assert (2*A).equals(2*B) == Eq(A, B)\n    assert (2*A).equals(B) == Eq(2*A, B)\n\n    M = Matrix([[1, 2], [3, 4]])\n    assert M.equals(M) == True\n    assert M.equals(2*M) == False\n    assert M.equals(Matrix([[1, 2], [3, 5]])) == False\n", "def test_as_real_imag():\n    A = MatrixSymbol(\"A\", 2, 2)\n    A_as_real_imag = A.as_real_imag()\n    assert len(A_as_real_imag) == 2\n    assert all(isinstance(i, MatrixExpr) for i in A_as_real_imag)\n", "def test_as_real_imag():\n    M = MatrixSymbol('M', 2, 2)\n    M_ri = M.as_real_imag()\n    assert len(M_ri) == 2\n    assert all(isinstance(i, MatrixExpr) for i in M_ri)\n"], "sample_1039": ["def test_print_random_symbol():\n    A = RandomSymbol('A')\n    assert mpp.doprint(A) == '<mi>A</mi>'\n    assert mp.doprint(A) == '<ci>A</ci>'\n", "def test_print_random_symbol():\n    R = RandomSymbol('R')\n    assert mpp.doprint(R) == '<mi>R</mi>'\n    assert mp.doprint(R) == '<ci>R</ci>'\n", "def test_print_random_symbol():\n    A = RandomSymbol('A')\n    assert mpp.doprint(A) == '<mi>A</mi>'\n    assert mp.doprint(A) == '<ci>A</ci>'\n", "def test_print_random_symbol():\n    R = RandomSymbol('R')\n    assert mpp.doprint(R) == '<mi>R</mi>'\n    assert mp.doprint(R) == '<ci>R</ci>'\n", "def test_print_random_symbol():\n    A = RandomSymbol('A')\n    assert mpp.doprint(A) == '<mi>A</mi>'\n    assert mp.doprint(A) == '<ci>A</ci>'\n"], "sample_1040": ["def test_print_random_symbol():\n    A = RandomSymbol('A')\n    assert mpp.doprint(A) == '<mi>A</mi>'\n    assert mp.doprint(A) == '<ci>A</ci>'\n", "def test_print_random_symbol():\n    r = RandomSymbol('R')\n    assert mpp.doprint(r) == '<mi>R</mi>'\n    assert mp.doprint(r) == '<ci>R</ci>'\n", "def test_print_random_symbol():\n    A = RandomSymbol('A')\n    assert mpp.doprint(A) == '<mi>A</mi>'\n    assert mp.doprint(A) == '<ci>A</ci>'\n", "def test_print_random_symbol():\n    R = RandomSymbol('R')\n    assert mpp.doprint(R) == '<mi>R</mi>'\n    assert mp.doprint(R) == '<ci>R</ci>'\n", "def test_print_random_symbol():\n    A = RandomSymbol('A')\n    assert mpp.doprint(A) == '<mi>A</mi>'\n    assert mp.doprint(A) == '<ci>A</ci>'\n"], "sample_1041": ["def test_applyfunc():\n    A = MatrixSymbol(\"A\", 2, 2)\n    func = lambda x: x**2\n    assert A.applyfunc(func) == Matrix([\n        [func(A[0, 0]), func(A[0, 1])],\n        [func(A[1, 0]), func(A[1, 1])]])\n", "def test_GenericIdentity():\n    i = GenericIdentity()\n    A = MatrixSymbol(\"A\", n, n)\n\n    assert i == i\n    assert i != A\n    assert A != i\n\n    assert i.is_Identity\n\n    raises(TypeError, lambda: i.shape)\n    raises(TypeError, lambda: i.rows)\n    raises(TypeError, lambda: i.cols)\n\n    assert MatMul() == i\n    assert MatMul(i, A) == A\n    # Make sure it is hashable\n    hash(i)\n", "def test_generic_identity_matrix():\n    i = GenericIdentity()\n    A = MatrixSymbol(\"A\", n, n)\n\n    assert i == i\n    assert i != A\n    assert A != i\n\n    assert i.is_Identity\n\n    raises(TypeError, lambda: i.shape)\n    raises(TypeError, lambda: i.rows)\n    raises(TypeError, lambda: i.cols)\n\n    assert MatMul() == i\n    assert MatMul(i, A) == A\n    # Make sure it is hashable\n    hash(i)\n", "def test_generic_identity_matrix():\n    i = GenericIdentity()\n    A = MatrixSymbol(\"A\", n, n)\n\n    assert i == i\n    assert i != A\n    assert A != i\n\n    assert i.is_Identity\n\n    raises(TypeError, lambda: i.shape)\n    raises(TypeError, lambda: i.rows)\n    raises(TypeError, lambda: i.cols)\n\n    assert MatMul() == i\n    assert MatMul(i, A) == MatMul(A)\n    # Make sure it is hashable\n    hash(i)\n", "def test_GenericIdentity():\n    I = GenericIdentity()\n    A = MatrixSymbol(\"A\", n, n)\n\n    assert I == I\n    assert I != A\n    assert A != I\n\n    assert I.is_Identity\n\n    raises(TypeError, lambda: I.shape)\n    raises(TypeError, lambda: I.rows)\n    raises(TypeError, lambda: I.cols)\n\n    assert MatMul() == I\n    assert MatMul(I, A) == MatMul(A)\n"], "sample_1042": ["def test_IndexedBase_shape_with_integer():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', shape=(3, 4))\n    assert a.shape == Tuple(3, 4)\n    assert Indexed(a, i, j).ranges == [Tuple(0, 2), Tuple(0, 3)]\n    assert Indexed(a, i, j).shape == Tuple(3, 4)\n    raises(IndexException, lambda: a[i])\n    raises(IndexException, lambda: a[i, i, j])\n    F = IndexedBase(\"F\", shape=5)\n    assert F.shape == Tuple(5)\n    assert F[i].subs(i, j) == F[j]\n    raises(IndexException, lambda: F[i, j])\n", "def test_IndexedBase_eq():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', shape=(2, 3))\n    b = IndexedBase('b', shape=(2, 3))\n    assert a == b.subs(b, a)\n    assert a[i, j] == b[i, j].subs(b, a)\n    assert a != b\n    assert a[i, j] != b[i, j]\n", "def test_Indexed_free_symbols():\n    i, j = symbols('i j', integer=True)\n    a = symbols('a')\n    A = IndexedBase(a)\n    assert A[i, j].free_symbols == {A, a, i, j}\n    assert A[i, j].expr_free_symbols == {A}\n", "def test_IndexedBase_strides():\n    A = IndexedBase('A', strides=(1, 2), shape=(10, 10))\n    i, j = symbols('i j', integer=True)\n    assert A[i, j].base.strides == (1, 2)\n    assert A.base.strides == (1, 2)\n\n    B = IndexedBase('B', shape=(10, 10))\n    assert B[i, j].base.strides is None\n    assert B.base.strides is None\n\n    C = IndexedBase('C', strides='C', shape=(10, 10))\n    assert C[i, j].base.strides == 'C'\n    assert C.base.strides == 'C'\n\n    D = IndexedBase('D', strides='F', shape=(10, 10))\n    assert D[i, j].base.strides == 'F'\n    assert D.base.strides == 'F'\n", "def test_IndexedBase_shape_with_one_dimension():\n    i = symbols('i', integer=True)\n    a = IndexedBase('a', shape=(5,))\n    assert a.shape == Tuple(5,)\n    assert a[i].shape == Tuple(5,)\n"], "sample_1043": ["def test_user_functions():\n    assert mcode(f(x, y, z), user_functions={'f': 'MyF'}) == \"MyF[x, y, z]\"\n    assert mcode(sin(x) ** cos(x), user_functions={'sin': 'MySin', 'cos': 'MyCos'}) == \"MySin[x]^MyCos[x]\"\n", "def test_comment():\n    assert mcode(x, postprocess=None) == \"x\"\n    assert mcode(x, postprocess=lambda x: \"(* {} *)\".format(x)) == \"(* x *)\"\n", "def test_user_functions():\n    user_functions = {'f': 'MyF', 'g': 'MyG'}\n    assert mcode(f(x, y, z), user_functions=user_functions) == \"MyF[x, y, z]\"\n    assert mcode(sin(x) ** cos(x), user_functions=user_functions) == \"Sin[x]^Cos[x]\"\n    user_functions = {'sin': 'MySin'}\n    assert mcode(sin(x) ** cos(x), user_functions=user_functions) == \"MySin[x]^Cos[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == 'MySin[x]'\n    assert mcode(cos(x), user_functions={'sin': 'MySin'}) == 'Cos[x]'\n    assert mcode(sin(x), user_functions={'sin': lambda *x: True, 'MySin': 'MySin'}) == 'sin[x]'\n    assert mcode(sin(x), user_functions={'sin': [(lambda x: True, 'MySin')]}) == 'MySin[x]'\n    assert mcode(cos(x), user_functions={'sin': [(lambda x: True, 'MySin')]}) == 'Cos[x]'\n", "def test_comment():\n    assert mcode._get_comment(\"This is a comment\") == \"(* This is a comment *)\"\n"], "sample_1044": ["def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True)\n    assert (x**y).is_integer\n    assert (x**2.5).is_integer is None\n    assert (2.5**y).is_integer is None\n    z = Symbol('z')\n    assert (x**z).is_integer is None\n    assert (z**y).is_integer is None\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(16, 4) == (2, True)\n    assert integer_nthroot(81, 4) == (3, True)\n    assert integer_nthroot(80, 4) == (3, False)\n    assert integer_nthroot(1, 10) == (1, True)\n    assert integer_nthroot(-1, 10) == False # This case was changed to fix the logic\n", "def test_Pow_is_nonzero():\n    z = Symbol('z')\n    k = Symbol('k', integer=True, nonzero=False)\n    l = Symbol('l', integer=True, nonzero=True)\n    assert z**2 != 0\n    assert (z**2).is_nonzero is None\n    assert (z**k).is_nonzero is None\n    assert (z**l).is_nonzero is True\n    assert (S.Zero**l).is_nonzero is False\n", "def test_Pow_is_finite():\n    x = Symbol('x', finite=True)\n    y = Symbol('y', finite=True)\n    assert (x**y).is_finite is None\n    assert (x**2).is_finite is True\n    assert (2**y).is_finite is True\n    assert (S.Pi**y).is_finite is True\n    assert (S.Pi**x).is_finite is True\n    assert (2**S.Pi).is_finite is True\n    assert (S.Pi**2).is_finite is True\n    assert (S.Pi**(S.Pi + S.Pi)).is_finite is True\n    assert (S.Pi**(S.Pi - S.Pi)).is_finite is True\n    assert (S.Pi**(S.Pi * S.Pi)).is_finite is True\n    assert (S.Pi**(S.Pi / S.Pi)).is_finite is True\n    assert (S.Infinity**2).is_finite is False\n    assert (S.NegativeInfinity**3).is_finite is False\n    assert (S.Infinity**S.Infinity).is_finite is False\n    assert (S.NegativeInfinity**S.Infinity).is_finite is False\n", "def test_Pow_is_finite():\n    x = Symbol('x', finite=True)\n    y = Symbol('y', finite=True)\n    assert (x**y).is_finite\n    x = Symbol('x', finite=False)\n    assert (x**y).is_finite is None\n    x = Symbol('x', finite=True)\n    y = Symbol('y', finite=False)\n    assert (x**y).is_finite is None\n    x = Symbol('x', finite=False)\n    assert (x**y).is_finite is None\n    assert (S.Infinity**2).is_finite is False\n    assert ((S.Infinity + 1)**2).is_finite is False\n    assert (S.NegativeInfinity**3).is_finite is False\n    assert ((S.NegativeInfinity - 1)**3).is_finite is False\n    assert (S.Infinity**0).is_finite\n    assert (S.NegativeInfinity**0).is_finite\n    assert (S.Infinity**-1).is_finite\n    assert (S.NegativeInfinity**-1).is_finite\n    assert (S.Infinity**S.NegativeInfinity).is_finite is None\n    assert (S.Infinity**S.Infinity).is_finite is False\n    assert (S.NegativeInfinity**S.NegativeInfinity).is_finite is None\n    assert (S.NegativeInfinity**S.Infinity).is_finite is False\n"], "sample_1045": ["def test_issue_13154():\n    assert 1/(1/S.Infinity) == S.Infinity\n    assert 1/(1/S.NegativeInfinity) == S.NegativeInfinity\n", "def test_NegativeOne_power():\n    assert((S.NegativeOne)**12 == S.One)\n    assert((S.NegativeOne)**S.NaN == S.NaN)\n    assert((S.NegativeOne)**11 == S.NegativeOne)\n    assert((S.NegativeOne)**10 == S.One)\n", "def test_Float_floor_ceiling():\n    a = Float(4.6)\n\n    assert(a.floor() == 4)\n    assert(a.ceiling() == 5)\n", "def test_mod_inverse_large_input():\n    assert mod_inverse(1234567890123456789, 9876543210987654321) == 1357028424279861914\n    assert mod_inverse(9876543210987654321, 1234567890123456789) == 664646359394449145\n", "def test_issue_20745():\n    assert mpf_norm((0, long(0), -123, -1), 53) == (0, long(0), 0, 0)\n    assert mpf_norm((0, long(0), -456, -2), 53) == (0, long(0), 0, 0)\n    assert mpf_norm((1, long(0), -789, -3), 53) == (0, long(0), 0, 0)\n"], "sample_1047": ["def test_issue_noninteger_nonzero():\n    x = Symbol('x', noninteger=True)\n    assert x.is_nonzero is None\n    x = Symbol('x', noninteger=True, nonzero=True)\n    assert x.is_nonzero is True\n", "def test_issue_14542():\n    x = Symbol('x', real=True)\n    assert (I*x + 1).is_imaginary is None\n    assert (I*x + 1).is_real is None\n    assert (x + I).is_imaginary is None\n    assert (x + I).is_real is None\n", "def test_issue_11056():\n    i = Symbol('i', integer=True)\n    c = Symbol('c', complex=True)\n    assert (c**i).is_complex\n    assert (c**(2*i)).is_complex\n    assert (c**(i/2)).is_complex\n    assert (c**(i*S.Pi)).is_complex\n", "def test_issue_commutative():\n    x = Symbol('x', commutative=False)\n    assert x.is_real is False\n    assert x.is_complex is False\n    assert x.is_nonzero is None\n    y = Symbol('y')\n    assert (x*y).is_commutative is False\n    assert (y*x).is_commutative is False\n    assert (x + y).is_commutative is None\n    assert (y + x).is_commutative is None\n", "def test_issue_10423():\n    n = Symbol('n', negative=True)\n    assert (n**4).is_positive\n    assert (n**3).is_negative\n    assert ((-2)**n).is_real is None\n"], "sample_1046": ["def test_tensor_element():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A = tensorhead(\"A\", [L]*2, [[1]*2])\n    t = A(i, j)\n    te = TensorElement(t, {i: 1})\n    assert te.get_free_indices() == [j]\n    assert te.get_indices() == [j]\n\n    te = TensorElement(t, {j: 1})\n    assert te.get_free_indices() == [i]\n    assert te.get_indices() == [i]\n\n    te = TensorElement(t, {i: 1, j: 1})\n    assert te.get_free_indices() == []\n    assert te.get_indices() == []\n", "def test_tensor_element():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A = tensorhead(\"A\", [L, L], [[1], [1]])\n\n    te = TensorElement(A(i, j), {i: 1})\n    assert te.get_free_indices() == [j]\n    assert te.get_indices() == [j]\n    assert te.free == [(j, 0)]\n    assert te.dum == []\n    assert te.expr == A(i, j)\n    assert te.index_map == {i: 1}\n\n    te = TensorElement(A(i, j), {i: 1, j: 2})\n    assert te.get_free_indices() == []\n    assert te.get_indices() == []\n    assert te.free == []\n    assert te.dum == []\n    assert te.expr == A(i, j)\n    assert te.index_map == {i: 1, j: 2}\n", "def test_tensor_replacement_with_Indexed():\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    i0 = tensor_indices(\"i0\", L)\n    A, B, C, D = tensorhead(\"A B C D\", [L], [[1]])\n    H = tensorhead(\"H\", [L, L], [[1], [1]])\n    K = tensorhead(\"K\", [L, L, L, L], [[1], [1], [1], [1]])\n\n    expr = A(i)*B(j)\n    repl = {A(i): Indexed('a', i), B(j): Indexed('b', j)}\n    assert expr.replace_with_arrays(repl, [i, j]) == Indexed('a', i)*Indexed('b', j)\n\n    expr = H(i, j)*A(i)*B(j)\n    repl = {A(i): Indexed('a', i), B(j): Indexed('b', j), H(i, j): Indexed('h', i, j)}\n    assert expr.replace_with_arrays(repl, [i, j]) == Indexed('h', i, j)*Indexed('a', i)*Indexed('b', j)\n\n    expr = Sum(A(i)*B(i), (i, 1, 3))\n    repl = {A(i): Indexed('a', i), B(j): Indexed('b', j)}\n    assert expr.replace_with_arrays(repl, []) == Sum(Indexed('a', i)*Indexed('b', i), (i, 1, 3))\n", "def test_replace_with_arrays_metric():\n    L = TensorIndexType(\"L\", dim=2)\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A, B, C, D = tensorhead(\"A B C D\", [L], [[1]])\n    H = tensorhead(\"H\", [L, L], [[1], [1]])\n\n    repl = {A(i): [1, 2], L: [[1, 0], [0, -1]]}\n    expr = A(i) * A(-i)\n    assert expr.replace_with_arrays(repl, []) == -3\n\n    repl = {A(i): [1, 2], L: [[1, 0], [0, -1]]}\n    expr = A(-i) * A(i)\n    assert expr.replace_with_arrays(repl, []) == -3\n", "def test_tensor_element():\n    Lorentz = TensorIndexType(\"L\", dim=4)\n    i, j, k, l = tensor_indices(\"i j k l\", Lorentz)\n    x, y, z, t = symbols(\"x y z t\")\n    A = tensorhead(\"A\", [Lorentz]*2, [[1]*2])\n    A.data = [[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]\n    te = TensorElement(A(i, j), {i: 1})\n    assert te.free == [(j, 1)]\n    assert te.free_args == [j]\n    assert te.dum == []\n    assert te.get_indices() == [j]\n    assert te.get_free_indices() == [j]\n    assert te.data == [1, 2, 3, 4]\n    te = TensorElement(A(i, j), {i: 1, j: 0})\n    assert te.free == []\n    assert te.free_args == []\n    assert te.dum == []\n    assert te.get_indices() == []\n    assert te.get_free_indices() == []\n    assert te.data == 1\n"], "sample_1050": ["def test_PythonCodePrinter_piecewise_without_condition():\n    prntr = PythonCodePrinter()\n    piecewise = Piecewise((1, Eq(x, 0)), (2, True))\n    assert prntr.doprint(piecewise) == '((1) if (x == 0) else (2))'\n", "def test_python_code_printer_tensor():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct, CodegenArrayContraction, CodegenArrayDiagonal\n    from sympy.codegen.array_utils import CodegenArrayPermuteDims, CodegenArrayElementwiseAdd\n\n    t1 = CodegenArrayTensorProduct(x, y, z)\n    t2 = CodegenArrayContraction(x, (0, 1))\n    t3 = CodegenArrayDiagonal(x, (0, 2))\n    t4 = CodegenArrayPermuteDims(x, (1, 0, 2))\n    t5 = CodegenArrayElementwiseAdd(x, y)\n    \n    p = PythonCodePrinter()\n    \n    assert p.doprint(t1) == \"x*y*z\"\n    assert p.doprint(t2) == \"np.einsum(x, (0, 1))\"  #does not work yet, todo\n    assert p.doprint(t3) == \"np.diagonal(x, 0, axis1=0, axis2=2)\"\n    assert p.doprint(t4) == \"np.transpose(x, (1, 0, 2))\"\n    assert p.doprint(t5) == \"np.add.reduce((x, y))\"\n", "def test_PythonCodePrinter_tensor():\n    prntr = PythonCodePrinter()\n    from sympy.tensor import IndexedBase\n    A = IndexedBase(\"A\")\n    p = IndexedBase(\"p\")\n    assert prntr.doprint(A[1, 2, 3]) == \"A[1, 2, 3]\"\n    assert prntr.doprint(p[1, 2]) == \"p[1, 2]\"\n    from sympy.tensor.array import NDimArray\n    arr = NDimArray([1, 2, 3], shape=(1, 3))\n    assert prntr.doprint(arr) == \"[[1, 2, 3]]\"\n    arr = NDimArray([1, 2, 3, 4], shape=(2, 2))\n    assert prntr.doprint(arr) == \"[[1, 2], [3, 4]]\"\n", "def test_PythonCodePrinter_Infinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n    assert prntr.doprint(zoo) == \"float('nan')\"\n    assert prntr.doprint(Rational(1, 0)) == \"float('inf')\"\n    assert prntr.doprint(-Rational(1, 0)) == \"float('-inf')\"\n    assert prntr.doprint(ComplexInfinity) == \"float('nan')\"\n", "def test_PythonCodePrinter_printing_of_infinities_and_non_finite_numbers():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n    assert prntr.doprint(zoo) == \"float('nan')\"\n"], "sample_1048": ["def test_parabola_intersection():\n    p1 = Point(0,0)\n    l1 = Line(Point(1, -2), Point(-1,-2))\n    parabola1 = Parabola(p1, l1)\n\n    # Intersection with Ellipse\n    ellipse1 = Ellipse(Point(0, 0), 2, 5)\n    assert parabola1.intersection(ellipse1) == [Point2D(-2, 0), Point2D(2, 0)]\n\n    # Intersection with Line\n    line1 = Line(Point(-7, 3), Point(12, 3))\n    assert parabola1.intersection(line1) == [Point2D(-4, 3), Point2D(4, 3)]\n\n    # Intersection with Segment\n    segment1 = Segment2D((-12, -65), (14, -68))\n    assert parabola1.intersection(segment1) == []\n\n    # Intersection with Ray\n    ray1 = Ray2D((0, 0), (1, 1))\n    assert len(parabola1.intersection(ray1)) == 1\n\n    # Intersection with Parabola\n    parabola2 = Parabola(Point(0, 0), Line(Point(0, 1), Point(1, 1)))\n    assert len(parabola1.intersection(parabola2)) == 1\n\n    # Intersection with Point on Parabola\n    point1 = Point(1, -1)\n    assert parabola1.intersection(point1) == [point1]\n\n    # Intersection with Point not on Parabola\n    point2 = Point(1, 1)\n    assert parabola1.intersection(point2) == []\n\n    # Intersection with 3D Line\n    line3d = Line(Point3D(1, 1, 1), Point3D(2, 2, 2))\n    raises(TypeError, lambda: parabola1.intersection(line3d))\n", "def test_parabola_intersection():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    p3 = Point(-3, 7)\n    p4 = Point(6, 6)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p1, d2)\n    pa3 = Parabola(p2, d2)\n    e1 = Ellipse(Point(0, 0), 2, 5)\n    l1 = Line(Point(-7, 3), Point(12, 3))\n    s1 = Segment((-12, -65), (14, -68))\n    r1 = Ray((1, 3), (7, 3))\n\n    # Intersection\n    assert len(pa1.intersection(e1)) == 2\n    assert len(pa1.intersection(l1)) == 2\n    assert len(pa1.intersection(s1)) == 0\n    assert len(pa1.intersection(r1)) == 2\n    assert len(pa1.intersection(pa2)) == 2\n    assert pa1.intersection(p4) == []\n    assert pa1.intersection(p1) == [p1]\n    assert len(pa1.intersection(pa3)) == 1\n", "def test_parabola_intersection():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p2, d2)\n    ell = Ellipse(p1, 2, 5)\n    l1 = Line(Point(-7, 3), Point(12, 3))\n    seg1 = Segment2D((-12, -65), (14, -68))\n\n    assert len(pa1.intersection(ell)) == 2\n    assert pa1.intersection(ell) == [Point2D(-2, 0), Point2D(2, 0)]\n    assert len(pa1.intersection(l1)) == 2\n    assert pa1.intersection(l1) == [Point2D(-4, 3), Point2D(4, 3)]\n    assert len(pa1.intersection(seg1)) == 0\n    assert pa1.intersection(seg1) == []\n    assert len(pa1.intersection(pa2)) == 0  # no intersection points\n    assert pa1.intersection(pa1) == [pa1]  # a parabola intersects itself\n", "def test_parabola_intersection():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    p3 = Point(0, 4)\n    p4 = Point(6, 0)\n    p5 = Point(1, 1)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    d3 = Line(Point(4, 0), slope=oo)\n    d4 = Line(Point(7, 6), slope=0)\n    d5 = Line(Point(0, 5), slope=0)\n    d6 = Line(Point(3, 8), slope=oo)\n\n    pa1 = Parabola(None, d2)\n    pa2 = Parabola(directrix=d1)\n    pa3 = Parabola(p1, d1)\n    pa4 = Parabola(p2, d2)\n    pa5 = Parabola(p2, d4)\n    pa6 = Parabola(p3, d2)\n    pa7 = Parabola(p2, d1)\n    pa8 = Parabola(p4, d1)\n    pa9 = Parabola(p4, d3)\n    e1 = Ellipse(p1, 5, 3)\n    e2 = Ellipse(p5, 3, 1)\n    l1 = Line(Point(-7, 3), Point(12, 3))\n    s1 = Segment2D(Point(-12, -65), Point(14, -68))\n    r1 = Ray2D(Point(2, 0), Point(2, -1))\n\n    assert pa6.intersection(e1) == [Point2D(-2, 0), Point2D(2, 0)]\n    assert pa6.intersection(e2) == []\n    assert pa6.intersection(pa6) == [pa6]\n    assert pa6.intersection(pa3) == []\n    assert pa3.intersection(l1) == [Point2D(-4, 3), Point2D(4, 3)]\n    assert pa3.intersection(s1) == []\n    assert pa3.intersection(r1) == []\n    assert pa3.intersection(d5) == [Point2D(-2, 5), Point2D(2, 5)]\n   ", "def test_parabola_intersection():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    p3 = Point(4, 2)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n\n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p2, d2)\n    e1 = Ellipse(p1, 2, 5)\n    e2 = Ellipse(p2, 2, 5)\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    l2 = Line(Point(-7, 3), Point(12, 3))\n    s1 = Segment2D((-12, -65), (14, -68))\n    s2 = Segment2D((-1, 2), (1, 2))\n    r1 = Ray2D((-1, 2), (1, 2))\n    r2 = Ray2D((1, 2), (-1, 2))\n\n    # Intersection\n    assert pa1.intersection(e1) == [Point2D(-2, 0), Point2D(2, 0)]\n    assert pa2.intersection(e2) == []\n    assert pa1.intersection(l1) == []\n    assert pa1.intersection(l2) == [Point2D(-4, 3), Point2D(4, 3)]\n    assert pa1.intersection(s1) == []\n    assert pa1.intersection(s2) == [Point2D(0, 2)]\n    assert pa1.intersection(r1) == [Point2D(0, 2)]\n    assert pa1.intersection(r2) == [Point2D(0, 2)]\n    assert pa1.intersection(pa2) == []\n    assert pa1.intersection(p1) == [p1]\n    assert pa1.intersection(p3) == []\n    assert pa1.intersection(pa1) == [pa1]\n    raises(TypeError, lambda: pa1.intersection(Line3D(p1, p2)))\n    raises(TypeError, lambda: pa1.intersection(Point3D(0, 0, 0)))\n    raises(TypeError, lambda: pa1.intersection(1))\n"], "sample_1049": ["def test_parameter_value():\n    from sympy.geometry import Plane, Point3D\n    from sympy.abc import t, u, v\n\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    p3 = Point3D(7, 8, 9)\n    pl = Plane(p1, p2, p3)\n\n    # check single parameter\n    pt = pl.arbitrary_point(t).subs(t, pi/4)\n    assert pl.parameter_value(pt, t) == {t: pi/4}\n\n    # check 2 parameters\n    pt = pl.arbitrary_point(u, v).subs({u: 1, v: 2})\n    assert pl.parameter_value(pt, u, v) == {u: 1, v: 2}\n\n    # check default behavior for not on plane\n    pt = Point3D(0, 0, 0)\n    raises(ValueError, lambda: pl.parameter_value(pt, t))\n\n    # check default behavior for not on plane\n    raises(ValueError, lambda: pl.parameter_value(pt, u, v))\n\n    # check parameter_value when pt is on plane but not on circle\n    from sympy.geometry import Circle\n    c = Circle((0, 0, 0), 5, (0, 0, 1))\n    p = Plane(c.center, normal_vector=c.normal_vector)\n    pt = c.arbitrary_point(t).subs(t, pi/4)\n    off_circle = c.center + 2*(pt - c.center)\n    assert p.parameter_value(off_circle, t) == {t: pi/4}\n", "def test_parameter_value():\n    t, u, v = symbols('t u v', real=True)\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n\n    pt_on_circle = p.arbitrary_point(t).subs(t, pi/4)\n    assert pt_on_circle.distance(p.p1) == 1\n    assert p.parameter_value(pt_on_circle, t) == {t: pi/4}\n\n    off_circle = p.p1 + (pt_on_circle - p.p1)*2\n    assert off_circle.distance(p.p1) == 2\n    assert p.parameter_value(off_circle, t) == {t: pi/4}\n\n    pt = p.arbitrary_point(u, v)\n    params = p.parameter_value(off_circle, u, v)\n    assert pt.subs(params) == off_circle\n", "def test_parameter_value():\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    t, u, v = symbols('t u v')\n    assert p.parameter_value(p.arbitrary_point(t).subs(t, pi/4), t) == {t: pi/4}\n    assert p.parameter_value(p.arbitrary_point(u, v).subs({u: 1, v: 2}), u, v) == {u: 1, v: 2}\n    projection = Point3D(5/3, 5/3, 2/3)\n    assert p.parameter_value(projection, t) == {t: pi/4}\n    assert p.parameter_value(projection, u, v) == {u: sqrt(10)/10, v: sqrt(10)/30}\n", "def test_parameter_value():\n    from sympy import Plane, Point, pi, symbols\n    from sympy.abc import t, u, v\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n\n    on_circle = p.arbitrary_point(t).subs(t, pi/4)\n    assert on_circle.distance(p.p1) == 1\n    assert p.parameter_value(on_circle, t) == {t: pi/4}\n\n    off_circle = p.p1 + (on_circle - p.p1)*2\n    assert off_circle.distance(p.p1) == 2\n    assert p.parameter_value(off_circle, t) == {t: pi/4}\n\n    pt = p.arbitrary_point(u, v)\n    sol = p.parameter_value(on_circle, u, v)\n    assert pt.subs(sol) == on_circle\n\n    sol = p.parameter_value(off_circle, u, v)\n    assert pt.subs(sol) == off_circle\n\n    # test with unnamed symbols\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    t, u, v = symbols('t u v')\n    on_circle = p.arbitrary_point(t).subs(t, pi/4)\n    assert on_circle.distance(p.p1) == 1\n    assert p.parameter_value(on_circle, t) == {t: pi/4}\n", "def test_plane_parameter_value():\n    from sympy import Plane, Point, pi, sin, cos\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    on_circle = p.arbitrary_point('t').subs('t', pi/4)\n    assert p.parameter_value(on_circle, 't') == {'t': pi/4}\n    off_circle = p.p1 + (on_circle - p.p1)*2\n    assert p.parameter_value(off_circle, 't') == {'t': pi/4}\n    u, v = p.parameter_value(on_circle, 'u', 'v')\n    assert u == sqrt(10)/10 and v == sqrt(10)/30\n    u, v = p.parameter_value(off_circle, 'u', 'v')\n    assert u == sqrt(10)/5 and v == sqrt(10)/15\n"], "sample_1051": ["def test_dotprint_kwargs():\n    text = dotprint(x+2, rankdir='LR', ordering='in')\n    assert \"rankdir\" in text\n    assert \"LR\" in text\n    assert \"ordering\" in text\n    assert \"in\" in text\n", "def test_dotprint_kwargs():\n    text = dotprint(x+2, rankdir='LR')\n    assert 'rankdir=\"LR\"' in text\n    text = dotprint(x+2, ordering='in')\n    assert 'ordering=\"in\"' in text\n", "def test_dotprint_styles():\n    styles = [(Basic, {'color': 'red', 'shape': 'box'}),\n              (Expr,  {'color': 'green'})]\n    text = dotprint(x+2, styles=styles)\n    assert '\"color\"=\"green\"' in text\n    assert '\"shape\"=\"box\"' not in text\n    assert '\"color\"=\"red\"' not in text\n    assert '\"shape\"=\"ellipse\"' in text\n", "def test_dotprint_custom_style():\n    styles = [(Symbol, {'color': 'red', 'shape': 'box'}),\n              (Integer, {'color': 'green', 'shape': 'circle'})]\n    text = dotprint(x+2, styles=styles)\n    assert '\"color\"=\"red\"' in text\n    assert '\"shape\"=\"box\"' in text\n    assert '\"color\"=\"green\"' in text\n    assert '\"shape\"=\"circle\"' in text\n", "def test_dotprint_styles():\n    styles = [(Symbol, {'color': 'red', 'shape': 'box'}),\n              (Expr,  {'color': 'green'})]\n    text = dotprint(x+2, styles=styles)\n    assert '\"color\"=\"green\"' in text\n    assert '\"color\"=\"red\"' in text\n    assert '\"shape\"=\"box\"' in text\n"], "sample_1052": ["def test_julia_codegen():\n    x, y, z = symbols('x,y,z')\n    expr1 = (x + y)*z\n    expr2 = (x - y)*z\n    result = codegen((\"test1\", expr1), (\"test2\", expr2), \"julia\", \"file\", header=False, empty=False)\n    expected = [\n        (\"file.jl\",\n        \"function test1(x, y, z)\\n\"\n        \"    return z*(x + y)\\n\"\n        \"end\\n\"\n        \"function test2(x, y, z)\\n\"\n        \"    return z*(x - y)\\n\"\n        \"end\\n\"\n        )\n    ]\n    assert result == expected\n", "def test_julia_codegen():\n    from sympy.printing.julia import JuliaCodePrinter\n\n    x, y = symbols('x y')\n    expr = x + y\n    routine = make_routine(\"test\", expr)\n    code_gen = JuliaCodeGen(printer=JuliaCodePrinter())\n    source = get_string(code_gen.dump_jl, [routine])\n    expected = (\n        'function test(x, y)\\n'\n        '    return x + y\\n'\n        'end\\n'\n    )\n    assert source == expected\n", "def test_codegen_withprinter():\n    from sympy.printing.ccode import C99CodePrinter\n    class CustomPrinter(C99CodePrinter):\n            return \"fastpow({}, {})\".format(self._print(expr.base),\n                                            self._print(expr.exp))\n    x, y = symbols('x y')\n    expr = x**y\n    result = codegen((\"test\", expr), \"C99\", \"test\", header=False, empty=False,\n                     printer=CustomPrinter())\n    source = result[0][1]\n    expected = (\n        '#include \"test.h\"\\n'\n        '#include <math.h>\\n'\n        'double test(double x, double y) {\\n'\n        '   double test_result;\\n'\n        '   test_result = fastpow(x, y);\\n'\n        '   return test_result;\\n'\n        '}\\n'\n    )\n    assert source == expected\n", "def test_julia_codegen():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y)*z\n    routine = make_routine(\"test\", expr)\n    code_gen = JuliaCodeGen()\n    source = get_string(code_gen.dump_jl, [routine])\n    expected = (\n        'function test(x, y, z)\\n'\n        '   test_result = z*(x + y)\\n'\n        '   return test_result\\n'\n        'end\\n'\n    )\n    assert source == expected\n", "def test_c_with_custom_printer():\n    from sympy.printing.ccode import C99CodePrinter\n    class CustomPrinter(C99CodePrinter):\n            return \"fastpow({}, {})\".format(self._print(expr.base),\n                                            self._print(expr.exp))\n    x, y = symbols('x y')\n    expr = x**y\n    gen = C99CodeGen(printer=CustomPrinter())\n    result = codegen(('expr', expr), header=False, empty=False, code_gen=gen)\n    source = result[0][1]\n    expected = (\n        '#include \"expr.h\"\\n'\n        '#include <math.h>\\n'\n        'double expr(double x, double y) {\\n'\n        '   double expr_result;\\n'\n        '   expr_result = fastpow(x, y);\\n'\n        '   return expr_result;\\n'\n        '}\\n'\n    )\n    assert source == expected\n"], "sample_1054": ["def test_issue_12977():\n    interval1 = Interval(0, 1)\n    interval2 = Interval(2, 3)\n    cr = ComplexRegion(interval1 * interval2)\n\n    assert 1 + 2*I in cr\n    assert 0 + 2*I in cr\n    assert 0 + 3*I in cr\n    assert 1 + 3*I in cr\n    assert -1 + 2*I not in cr\n    assert 0 + 1*I not in cr\n    assert 1 + 4*I not in cr\n    assert 0 + 4*I not in cr\n", "def test_ComplexRegion_product():\n    # Rectangular form\n    c1 = ComplexRegion(Interval(1, 2)*Interval(1, 2))\n    c2 = ComplexRegion(Interval(2, 3)*Interval(2, 3))\n    assert c1 * c2 == ComplexRegion(Interval(1, 2)*Interval(1, 2)*Interval(2, 3)*Interval(2, 3))\n\n    # Polar form\n    c3 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=True)\n    c4 = ComplexRegion(Interval(1, 2)*Interval(0, pi), polar=True)\n    assert c3 * c4 == ComplexRegion(Interval(0, 1)*Interval(0, pi)*Interval(1, 2)*Interval(0, pi), polar=True)\n\n    # Mixed form\n    c5 = ComplexRegion(Interval(1, 2)*Interval(1, 2))\n    c6 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=True)\n    assert c5 * c6 == ComplexRegion(Interval(1, 2)*Interval(1, 2)*Interval(0, 1)*Interval(0, pi), polar=True)\n", "def test_issue_11897():\n    c = ComplexRegion(Interval(1, 2)*Interval(1, 2))\n    assert c.a_interval == Interval(1, 2)\n    assert c.b_interval == Interval(1, 2)\n    assert c.psets == (Interval(1, 2) * Interval(1, 2),)\n    assert c.polar == False\n    assert c.variables == (x, y)\n    assert c.expr == x + I*y\n\n    c = ComplexRegion(Interval(1, 2)*Interval(1, 2), polar=True)\n    assert c.a_interval == Interval(1, 2)\n    assert c.b_interval == Interval(1, 2)\n    assert c.psets == (Interval(1, 2) * Interval(1, 2),)\n    assert c.polar == True\n    assert c.variables == (r, theta)\n    assert c.expr == r*(cos(theta) + I*sin(theta))\n", "def test_issue_13526():\n    from sympy import Interval, pi\n    from sympy.geometry import Point\n    interval = Interval(0, 2*pi)\n    complex_region = ComplexRegion(Interval(1, 2)*interval, polar=True)\n    assert Interval(-2, 2).intersect(complex_region) == Interval(-2, 2)\n    assert complex_region.intersect(Interval(0, 1)) == Interval(1, 2)\n    assert Interval(0, 2).intersect(complex_region) == Interval(1, 2)\n    assert complex_region.intersect(Interval(-1, 0)) == S.EmptySet\n    assert complex_region.intersect(FiniteSet(1 + 1j)) == FiniteSet(1 + 1j)\n    assert complex_region.intersect(FiniteSet(1, 2)) == FiniteSet(1, 2)\n    assert complex_region.intersect(FiniteSet(1, 2, 3)) == FiniteSet(1, 2)\n    assert complex_region.intersect(FiniteSet(1, 2, 1 + 1j)) == FiniteSet(1, 2, 1 + 1j)\n    assert complex_region.intersect(FiniteSet(Point(1, 0), Point(2, 0), Point(3, 0))) == FiniteSet(1, 2)\n", "def test_issue_12342():\n    a = Interval(0, 1)\n    b = Interval(2, 3)\n    cr = ComplexRegion(a*b)\n\n    # test complex conjugate\n    assert (1 + 2*I).conjugate() in cr\n    assert (1 + 4*I).conjugate() not in cr\n\n    # test complex number with zero imaginary part\n    assert 1 + 0*I in cr\n    assert 2 + 0*I not in cr\n"], "sample_1053": ["def test_issue_14243():\n    # Ensure Float precision is preserved when creating a new Float from\n    # a string or when unpickling a Float\n    from pickle import dumps, loads\n    \n    # From string\n    assert Float('0.2', 3)._prec == 3\n    assert Float(Float('0.2', 3))._prec == 3\n    \n    # From pickle\n    assert loads(dumps(Float('0.2', 3)))._prec == 3\n", "def test_AlgebraicNumber():\n    from sympy import AlgebraicNumber, sqrt, extension, QQ\n\n    # create instance from expression\n    AlgebraicNumber(sqrt(2))\n\n    # minimal polynomial\n    assert AlgebraicNumber(sqrt(2)).minpoly == x**2 - 2\n\n    # algebraic number is zero if and only if its norm is zero\n    assert AlgebraicNumber(sqrt(2)).is_zero is False\n    assert AlgebraicNumber(sqrt(2)).norm == 2\n    assert AlgebraicNumber(sqrt(2)).is_real is True\n    assert AlgebraicNumber(1 + sqrt(2)).is_real is True\n\n    AlgebraicNumber(sqrt(2)).coeffs()\n    AlgebraicNumber(sqrt(2)).native_coeffs()\n\n    AlgebraicNumber(sqrt(2)).to_algebraic_integer()\n\n    AlgebraicNumber(sqrt(2))._eval_evalf()\n\n    AlgebraicNumber(sqrt(2)).as_expr()\n\n    AlgebraicNumber(sqrt(2)).as_expr(x)\n\n    # add two algebraic numbers\n    AlgebraicNumber(sqrt(2)) + AlgebraicNumber(sqrt(3))\n", "def test_Rational_as_numer_denom():\n    p, q = Rational(3, 4).as_numer_denom()\n    assert p == 3\n    assert q == 4\n", "def test_mod_inverse_with_rational_modulus():\n    assert mod_inverse(3, S(5)/2) == S(3)/2\n    assert mod_inverse(5, S(3)/2) == S(3)/5\n    assert mod_inverse(123, S(456)/789) == S(123)/456\n    assert mod_inverse(S(3)/2, S(5)/3) == S(3)/5\n    raises(ValueError, lambda: mod_inverse(S(2)/7, S(5)/2))\n", "def test_NegativeOne_power():\n    assert((S.NegativeOne)**11 == S.NegativeOne)\n    assert((S.NegativeOne)**12 == S.One)\n"], "sample_1055": ["def test_decipher_affine():\n    assert decipher_affine(\"BCD\", (1, 1)) == \"ABC\"\n    assert decipher_affine(\"QTW\", (3, 16)) == \"ABC\"\n    assert decipher_affine(\"214\", (-1, 1), symbols=\"1234\") == \"123\"\n", "def test_elgamal_key_length():\n    dk = elgamal_private_key(digit=100)\n    ek = elgamal_public_key(dk)\n    assert len(ek[0].bit_length()) > 100\n", "def test_bifid_encipher_decipher():\n    for key in ['key', 'letmein', 'a']:\n        for s in ['attackatdawn', 'abcdef', 'a']:\n            enciphered = encipher_bifid(s, key)\n            deciphered = decipher_bifid(enciphered, key)\n            assert s == deciphered\n", "def test_lfsr_sequence_large_values():\n    F = FF(7)\n    key = [F(1), F(2), F(3), F(4), F(5)]\n    fill = [F(3), F(4), F(5), F(6), F(1)]\n    n = 20\n    sequence = lfsr_sequence(key, fill, n)\n    assert len(sequence) == n\n    for elem in sequence:\n        assert elem in F\n", "def test_decipher_shift():\n    assert decipher_shift(\"ZAB\", -1) == \"ABC\"\n    assert decipher_shift(\"ABC\", 1) == \"ZAB\"\n    assert decipher_shift(\"ABC\", 0) == \"ABC\"\n"], "sample_1057": ["def test_render_as_module_imports():\n    # Ensure render_as_module generates the correct imports\n    content = Print(\"Hello, World!\")\n    module_str = render_as_module(content, standard='python3')\n    assert \"import sympy\" in module_str\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules': True})\n    printer.doprint(content)\n    fully_qualified_module_str = render_as_module(content, standard='python3')\n    assert \"from sympy import\" not in fully_qualified_module_str\n", "def test_render_as_module_with_fully_qualified_modules():\n    content = Print(\"Hello, World!\")\n    result = render_as_module(content, standard='python3')\n    assert 'import' in result\n    printer = PythonCodePrinter({'standard': 'python3', 'fully_qualified_modules': True})\n    printer.doprint(content)\n    module_imports_str = '\\n'.join('import %s' % k for k in printer.module_imports)\n    assert module_imports_str in result\n", "def test_render_as_module_with_fully_qualified_modules():\n    printer = Print(1)\n    result = render_as_module(printer, standard='python3')\n    assert 'import builtins' in result\n    assert 'builtins.print(1)' in result\n", "def test_render_as_module_with_fully_qualified_modules():\n    content = Print('Hello, World!')\n    result = render_as_module(content, standard='python3')\n    assert 'import builtins' in result\n    assert 'builtins.print(' in result\n", "def test_render_as_module_imports():\n    content = Print('hello')\n    module = render_as_module(content)\n    assert 'import sympy' in module\n    assert 'print(\"hello\")' in module\n\n    module = render_as_module(content, standard='python3')\n    assert 'import sympy' in module\n    assert 'print(\"hello\")' in module\n"], "sample_1056": ["def test_numexpr_functions():\n    prntr = NumExprPrinter()\n    assert prntr._print_sin(x) == 'sin(x)'\n    assert prntr._print_cos(x) == 'cos(x)'\n    assert prntr._print_tan(x) == 'tan(x)'\n    assert prntr._print_asin(x) == 'arcsin(x)'\n    assert prntr._print_acos(x) == 'arccos(x)'\n    assert prntr._print_atan(x) == 'arctan(x)'\n    assert prntr._print_atan2(x, y) == 'arctan2(x, y)'\n    assert prntr._print_exp(x) == 'exp(x)'\n    assert prntr._print_log(x) == 'log(x)'\n    assert prntr._print_sqrt(x) == 'sqrt(x)'\n    assert prntr._print_Abs(x) == 'abs(x)'\n    assert prntr._print_conjugate(x) == 'conj(x)'\n    assert prntr._print_re(x) == 'real(x)'\n    assert prntr._print_im(x) == 'imag(x)'\n    assert prntr._print_where(x, y, z) == 'where(x, y, z)'\n    assert prntr._print_complex(x, y) == 'complex(x, y)'\n", "def test_ITE():\n    # Test if-then-else\n    ite = (x < 1).ite(2, 3)\n    l = lambdarepr(ite)\n    assert l == \"((2) if (x < 1) else (3))\"\n    eval(\"lambda x: \" + l)\n", "def test_numexpr_printer():\n    printer = NumExprPrinter()\n    assert printer._print_ImaginaryUnit(1) == '1j'\n    assert printer._print_Function(sin(x)) == 'sin(x)'\n    assert printer.doprint(x + y) == \"evaluate('x + y', truediv=True)\"\n\n    # Test blacklisted functions\n    raises(TypeError, lambda: printer._print_Matrix(Matrix([[x, y]])))\n    raises(TypeError, lambda: printer._print_list([x, y]))\n    raises(TypeError, lambda: printer._print_dict({x: y}))\n", "def test_ITE():\n    # Test if-then-else (ITE) expressions.\n    ite_expr = (x if x > 0 else -x)\n    assert lambdarepr(ite_expr) == \"((x) if (x > 0) else (-x))\"\n    ite_expr = (x*y if x > 0 else -x*y)\n    assert lambdarepr(ite_expr) == \"((x*y) if (x > 0) else (-x*y))\"\n", "def test_numexpr_functions():\n    x = symbols(\"x\")\n    printer = NumExprPrinter()\n    \n    # Test some numexpr functions\n    assert printer.doprint(sin(x)) == \"evaluate('sin(x)', truediv=True)\"\n    assert printer.doprint(sqrt(x)) == \"evaluate('sqrt(x)', truediv=True)\"\n    assert printer.doprint(x**2) == \"evaluate('(x**2)', truediv=True)\"\n    assert printer.doprint(abs(x)) == \"evaluate('abs(x)', truediv=True)\"\n    assert printer.doprint(x + 1) == \"evaluate('(x + 1)', truediv=True)\"\n    \n    # Test unsupported function\n    class UnsupportedFunction(Expr):\n        pass\n    \n    raises(TypeError, lambda: printer.doprint(UnsupportedFunction()))\n"], "sample_1058": ["def test_print_Assignment_with_Piecewise():\n    p = PythonCodePrinter()\n    expr = Piecewise((1, Eq(x, 0)), (2, x>6))\n    assmnt = Assignment(x, expr)\n    assert p.doprint(assmnt) == \"x = ((1) if (x == 0) else (2) if (x > 6) else None)\"\n", "def test_issue_20246():\n    prntr = PythonCodePrinter()\n    piecewise = Piecewise((1, Eq(x, 0)), (2, x>6), evaluate=False)\n    assert prntr._print(piecewise) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n    assert prntr.doprint(piecewise) == '((1) if (x == 0) else (2) if (x > 6) else None)'\n", "compilation error", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**-1) == 'sympy.Pow(A, -1)'\n    assert p.doprint(A**5) == 'sympy.Pow(A, 5)'\n    assert p.doprint(A.inv()) == 'sympy.inv(A)'\n    assert p.doprint(A.det()) == 'sympy.det(A)'\n    assert p.doprint(A.transpose()) == 'sympy.transpose(A)'\n", "def test_print_Piecewise_withoutconds():\n    prntr = PythonCodePrinter()\n    assert prntr._print_Piecewise(Piecewise((1, Eq(x, 0)), (2, True))) == '((1) if (x == 0) else 2)'\n    assert prntr._print_Piecewise(Piecewise((1, Eq(x, 0)), (2, True), evaluate=False)) == '((1) if (x == 0) else 2)'\n"], "sample_1060": ["def test_PythonCodePrinter_Relational():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(Eq(x, y)) == '(x == y)'\n    assert prntr.doprint(Le(x, y)) == '(x <= y)'\n    assert prntr.doprint(Gt(x, y)) == '(x > y)'\n    assert prntr.doprint(x < y) == '(x < y)'\n    assert prntr.doprint(x >= y) == '(x >= y)'\n    assert prntr.doprint(x != y) == '(x != y)'\n", "def test_printing_infinities():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n    assert prntr.doprint(zoo) == \"float('nan')\"\n", "def test_pycode_infinity():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(oo) == \"float('inf')\"\n    assert prntr.doprint(-oo) == \"float('-inf')\"\n    assert prntr.doprint(zoo) == \"float('nan')\"\n    assert prntr.doprint(-zoo) == \"float('nan')\"\n", "def test_PythonCodePrinter_log():\n    p = PythonCodePrinter()\n    assert p.doprint(x**2.5) == 'x**2.5'\n    assert p.doprint(2*x + 3*y - 4*z) == '2*x + 3*y - 4*z'\n    assert p.doprint(sign(log(x))) == '(0.0 if math.log(x) == 0 else math.copysign(1, math.log(x)))'\n", "def test_PythonCodePrinter_print_seq():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint((1, 2, 3)) == '(1, 2, 3)'\n    assert prntr.doprint([1, 2, 3]) == '(1, 2, 3)'\n    assert prntr.doprint((1, (2, 3))) == '(1, (2, 3))'\n"], "sample_1062": ["def test_as_f_sign_1():\n    assert as_f_sign_1(x + 1) == (1, x, 1)\n    assert as_f_sign_1(x - 1) == (1, x, -1)\n    assert as_f_sign_1(-x + 1) == (-1, x, -1)\n    assert as_f_sign_1(-x - 1) == (-1, x, 1)\n    assert as_f_sign_1(2*x + 2) == (2, x, 1)\n    assert as_f_sign_1(x + x) is None\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n", "def test_as_f_sign_1():\n    assert as_f_sign_1(cos(x) - 1) == (1, cos(x), -1)\n    assert as_f_sign_1(cos(x) + 1) == (1, cos(x), 1)\n    assert as_f_sign_1(-cos(x) + 1) == (-1, cos(x), -1)\n    assert as_f_sign_1(-cos(x) - 1) == (-1, cos(x), 1)\n    assert as_f_sign_1(2*cos(x) + 2) == (2, cos(x), 1)\n", "def test_TR2i_with_powers():\n    assert TR2i(1/sin(x)**2) == 1 + cot(x)**2\n    assert TR2i(1/cos(x)**2) == 1 + tan(x)**2\n    assert TR2i(sin(x)**2/cos(x)**2) == tan(x)**2\n    assert TR2i(cos(x)**2/sin(x)**2) == cot(x)**2\n    assert TR2i(cos(x)/sin(x)**2) == cot(x)/sin(x)\n    assert TR2i(sin(x)/cos(x)**2) == tan(x)/cos(x)\n", "def test_TR22():\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 + tan(x)**4) == (sec(x)**2 - 1)**2 + 1\n    assert TR22(1 + cot(x)**4) == (csc(x)**2 - 1)**2 + 1\n"], "sample_1061": ["def test_Pow_mod():\n    x = Symbol('x')\n    y = Symbol('y')\n    n = Symbol('n', integer=True)\n\n    assert (x**2).mod(y) == (x.mod(y))**2\n    assert (x**2).mod(y**2) == (x.mod(y**2))**2\n    assert (x**3).mod(y) == ((x.mod(y))**2*x).mod(y)\n\n    t = symbols('t', real=False)\n    assert (2*t)**3 % 7 == 64*t**3 % 7\n\n    assert ((2**n)**3).mod(7) == (2**3)**n % 7\n\n    z = Symbol('z', integer=True, positive=True)\n    assert (2**z).mod(7) == Pow(2, z, evaluate=False) % 7\n\n    # issue 10738\n    assert (S(2)**(n - 1)).mod(7) == (2**(n-1)) % 7\n", "def test_Rational_rounding():\n    # Check rounding for Rationals\n    assert Rational(11, 10).round() == 1\n    assert Rational(9, 10).round() == 1\n    assert Rational(1, 10).round() == 0\n    assert Rational(-11, 10).round() == -1\n    assert Rational(-9, 10).round() == -1\n    assert Rational(-1, 10).round() == 0\n    # Check rounding for Rationals with a specific number of decimal places\n    assert Rational(11, 10).round(1) == Rational(11, 10)\n    assert Rational(9, 10).round(1) == Rational(9, 10)\n    assert Rational(1, 10).round(1) == Rational(1, 10)\n    assert Rational(-11, 10).round(1) == Rational(-11, 10)\n    assert Rational(-9, 10).round(1) == Rational(-9, 10)\n    assert Rational(-1, 10).round(1) == Rational(-1, 10)\n    # Check rounding when decimal place is larger than number of decimal places in Rational\n    assert Rational(1, 10).round(3) == Rational(1, 10)\n    assert Rational(-1, 10).round(3) == Rational(-1, 10)\n", "def test_Pow_simple_evaluate():\n    assert Pow(0, 0, evaluate=True) is S.One\n    assert Pow(1, 1, evaluate=True) is S.One\n    assert Pow(0, 1, evaluate=True) is S.Zero\n    assert Pow(1, 2, evaluate=True) is S.One\n    assert Pow(1, 0, evaluate=True) is S.One\n    assert Pow(2, 1, evaluate=True) is S(2)\n    assert Pow(0, 3.5, evaluate=True) is S.Zero\n    assert Pow(0, -3, evaluate=True) is S.ComplexInfinity\n    assert Pow(0, -4.5, evaluate=True) is S.ComplexInfinity\n    assert Pow(2, 3, evaluate=True) == 8\n    assert Pow(S.Half, -2, evaluate=True) == 4\n    assert Pow(S.Half, 2, evaluate=True) == S.Quarter\n    assert Pow(S(2), S(3), evaluate=True) == 8\n    assert Pow(2, -3, evaluate=True) == S.Rational(1, 8)\n    assert Pow(S(2), -3, evaluate=True) == S.Rational(1, 8)\n", "def test_Pow_is_real():\n    assert Pow(2, 3, evaluate=False).is_real\n    assert Pow(2, 3.5, evaluate=False).is_real\n    assert Pow(2, -3, evaluate=False).is_real\n    assert Pow(2, -3.5, evaluate=False).is_real\n    assert not Pow(-2, 3.5, evaluate=False).is_real\n    assert not Pow(-2, -3.5, evaluate=False).is_real\n    assert Pow(S.Half, S.Half).is_real\n    assert not Pow(S.Half, S.Half + I).is_real\n    assert Pow(-1, I).is_real\n    assert Pow(I, 2).is_real\n    assert Pow(I, 4).is_real\n    assert not Pow(I, 1).is_real\n    assert not Pow(I, 3).is_real\n", "compilation error"], "sample_1059": ["def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    m = Symbol(\"m\", negative=True)\n    alpha = Symbol(\"alpha\")\n\n    # Associated Laguerre polynomials:\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(1, alpha, x) == alpha + 1 - x\n    assert assoc_laguerre(2, alpha, x) == (alpha + 2)*(alpha + 1)/2 - (alpha + 2)*x + x**2/2\n    assert assoc_laguerre(3, alpha, x) == (alpha + 3)*(alpha + 2)*(alpha + 1)/6 - (alpha + 3)*(alpha + 2)*x/2 + (alpha + 3)*x**2/2 - x**3/6\n\n    X = assoc_laguerre(n, alpha, x)\n    assert isinstance(X, assoc_laguerre)\n\n    assert assoc_laguerre(n, alpha, 0) == binomial(n + alpha, alpha)\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(n, alpha, oo) == oo\n    assert assoc_laguerre(n, alpha, -oo) == oo\n\n    assert conjugate(assoc_laguerre(n, alpha, x)) == assoc_laguerre(n, conjugate(alpha), conjugate(x))\n\n    _k = Dummy('k')\n    assert assoc_laguerre(n, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        gamma(alpha + n + 1)*Sum((-1)**_k*x**_k*RisingFactorial(-n, _k)/\n        (factorial(_k)*gamma(_k + alpha + 1)), (_k, 0, n))/factorial(n))\n    assert assoc_laguerre(m, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        gamma(alpha + m + 1)*Sum((-1)**_k*x**_k*RisingFactorial(-m, _k)/\n        (factorial(_k)*gamma(_k + alpha + 1)), (_k, 0, m))/factorial(m))\n\n   ", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    m = Symbol(\"m\", negative=True)\n    alpha = Symbol(\"alpha\")\n\n    # Associated Laguerre polynomials:\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(1, alpha, x) == alpha - x + 1\n    assert assoc_laguerre(2, alpha, x) == (alpha**2/2 + 3*alpha/2 + x**2/2 +\n                                           x*(-alpha - 2) + 1)\n\n    X = assoc_laguerre(n, alpha, x)\n    assert isinstance(X, assoc_laguerre)\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, alpha, 0) == binomial(alpha + n, alpha)\n    assert assoc_laguerre(m, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        exp(x)*gamma(alpha + m + 1)/factorial(m)*Sum((-x)**_k *\n            RisingFactorial(m + 1, _k)/(factorial(_k)*gamma(_k + alpha + 1)),\n            (_k, 0, -m - 1)))\n\n    assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - 1, alpha + 1, x)\n    _k = Dummy('k')\n    assert diff(assoc_laguerre(n, alpha, x), alpha).dummy_eq(Sum(assoc_laguerre(_k, alpha, x) /\n        (-alpha + n), (_k, 0, n - 1)))\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, alpha, x).fdiff(1))\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, alpha, x).fdiff(4))\n", "def test_jacobi_normalized():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi_normalized(n, a, b, x) == \\\n           (jacobi(n, a, b, x)/sqrt(2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)\n                                    /((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1))))\n\n    assert jacobi_normalized(0, a, b, x) == 1\n    assert jacobi_normalized(1, a, b, x) == (a/2 - b/2 + x*(a/2 + b/2 + 1)) * \\\n        sqrt((a + b + 1)/(2**(a + b + 1)*gamma(a + 2)*gamma(b + 2)/(factorial(1)*gamma(a + b + 2))))\n\n    raises(ValueError, lambda: jacobi_normalized(-2.1, a, b, x))\n    raises(ValueError, lambda: jacobi_normalized(Dummy(positive=True, integer=True), 1, 2, oo))\n", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    alpha = Symbol(\"alpha\")\n    m = Symbol(\"m\", negative=True)\n\n    # Associated Laguerre polynomials:\n    assert assoc_laguerre(0, alpha, x) == 1\n    assert assoc_laguerre(1, alpha, x) == alpha + 1 - x\n    assert assoc_laguerre(2, alpha, x) == (alpha + 1)*(alpha + 2)/2 - x*(2*alpha + 3) + x**2\n    assert assoc_laguerre(3, alpha, x) == 1/6*(alpha + 1)*(alpha + 2)*(alpha + 3) - \\\n        1/2*x*(alpha + 1)*(2*alpha + 3) + x**2*(alpha + 2) - x**3\n\n    X = assoc_laguerre(n, alpha, x)\n    assert isinstance(X, assoc_laguerre)\n\n    assert assoc_laguerre(n, alpha, 0) == binomial(n + alpha, alpha)\n    assert assoc_laguerre(n, alpha, oo) == (-1)**n*oo\n    assert assoc_laguerre(n, alpha, -oo) == oo\n\n    assert conjugate(assoc_laguerre(n, alpha, x)) == assoc_laguerre(n, conjugate(alpha), conjugate(x))\n\n    _k = Dummy('k')\n\n    assert assoc_laguerre(n, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        gamma(n + alpha + 1)/factorial(n)*Sum((-1)**_k*x**_k*RisingFactorial(-n, _k)/\n            (gamma(_k + alpha + 1)*factorial(_k)), (_k, 0, n)))\n\n    assert assoc_laguerre(m, alpha, x).rewrite(\"polynomial\").dummy_eq(\n        x**-m*exp(x)*gamma(alpha + 1)/gamma(m + alpha + 1)*Sum(x**_k*RisingFactorial(\n            m + alpha + 1, _k)/gamma(_k + alpha + 1), (_k, 0, -m)))\n\n    assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - ", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    m = Symbol(\"m\", positive=True)\n    a = Symbol(\"a\")\n\n    # Generalized Laguerre polynomials:\n    assert assoc_laguerre(0, a, x) == 1\n    assert assoc_laguerre(1, a, x) == a + 1 - x\n    assert assoc_laguerre(2, a, x) == (a + 2)*(a + 1)/2 - x*(a + 2) + x**2/2\n    assert assoc_laguerre(3, a, x) == (-x**3/6 + x**2*(a + 3)/2 -\n        x*(a + 2)*(a + 3)/2 + (a + 1)*(a + 2)*(a + 3)/6)\n\n    X = assoc_laguerre(n, a, x)\n    assert isinstance(X, assoc_laguerre)\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, a, 0) == binomial(a + n, n)\n\n    assert assoc_laguerre(m, a, oo) == oo\n    assert assoc_laguerre(m, a, -oo) == oo * (-1)**m\n\n    assert conjugate(assoc_laguerre(n, a, x)) == assoc_laguerre(n, conjugate(a), conjugate(x))\n\n    _k = Dummy('k')\n    assert assoc_laguerre(n, a, x).rewrite(\"polynomial\").dummy_eq(gamma(a + n +\n        1)/factorial(n)*Sum((-1)**_k*x**_k*RisingFactorial(-n, _k)/(factorial(\n        _k)*gamma(_k + a + 1)), (_k, 0, n)))\n\n    assert diff(assoc_laguerre(n, a, x), x) == -assoc_laguerre(n - 1, a + 1, x)\n    assert diff(assoc_laguerre(n, a, x), a) == Sum(assoc_laguerre(\n        _k, a, x)/(-a + n), (_k, 0, n - 1))\n\n    raises(ArgumentIndexError,"], "sample_1063": ["def test_lambdastr_nested_args():\n    # Test that lambdastr handles nested args correctly\n    x, y, z = symbols('x y z')\n    f = lambdastr((x, (y, z)), x + y)\n    assert f.startswith('lambda _0,_1: (lambda x,y,z:')\n", "def test_lambdify_lambda():\n    f1 = lambdify(x, lambda y: y**2)\n    assert f1(2) == 4\n\n    f2 = lambdify(x, Lambda(x, x**2))\n    assert f2(2) == 4\n", "def test_lambdify_with_function():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n        return 2*x\n\n    f = lambdify(x, my_func(x), 'numpy')\n    assert f(5) == 10\n    assert numpy.all(f(numpy.array([1, 2, 3])) == numpy.array([2, 4, 6]))\n", "def test_lambdify_lambda():\n    f = lambdify(x, Lambda(x, x**2))\n    assert f(2) == 4\n", "def test_lambdastr_Derivative_arg_issue_16468():\n    f = Function('f')(x)\n    fx = f.diff()\n    assert lambdastr((f, fx), f/fx, dummify=False) == 'lambda f, fx: (f/fx)'\n    assert lambdastr((f, fx), f/fx) == 'lambda _0, _1: (_0/_1)'\n    assert lambdastr((fx, f), f/fx) == 'lambda _0, _1: (_1/_0)'\n    lambdastr_fx = lambdastr(fx, 1 + fx)\n    assert lambdastr_fx == 'lambda fx: (fx + 1)'\n    assert eval(lambdastr_fx)(41) == 42\n"], "sample_1064": ["def test_tensorflow_max_min():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Max(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.maximum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.random())\n\n    expr = Min(x, y)\n    assert tensorflow_code(expr) == \"tensorflow.math.minimum(x, y)\"\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.random())\n", "def test_derivative():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Derivative(x**2, x)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.gradients(x**2, x)[0]\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = Derivative(x**3, x, 2)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.gradients(tensorflow.gradients(x**3, x)[0], x)[0]\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n\n    expr = Derivative(x**2*y, x)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.gradients(x**2*y, x)[0]\"\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.random())\n\n    expr = Derivative(x**2*y, y)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.gradients(x**2*y, y)[0]\"\n    _compare_tensorflow_scalar((x, y), expr, rng=lambda: random.random())\n", "def testcodegen_derivative():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    x = Symbol('x')\n    y = Symbol('y')\n    M = MatrixSymbol(\"M\", 2, 2)\n\n    # Simple derivative of a Symbol\n    expr = Derivative(x, x)\n    assert tensorflow_code(expr) == 'tensorflow.gradients(x, x)[0]'\n    f = lambdify((x,), expr, 'tensorflow')\n    y = f(1.0)\n    assert y == 1.0\n\n    # Simple derivative of a Matrix\n    expr = Derivative(M, x)\n    assert tensorflow_code(expr) == 'tensorflow.gradients(M, x)[0]'\n    f = lambdify((M, x), expr, 'tensorflow')\n    ma = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n    y = f(ma, 1.0)\n    assert (y == [[0, 0], [0, 0]]).all()\n\n    # Nested derivatives\n    expr = Derivative(Derivative(x, x), x)\n    assert tensorflow_code(expr) == 'tensorflow.gradients(tensorflow.gradients(x, x)[0], x)[0]'\n    f = lambdify((x,), expr, 'tensorflow')\n    y = f(1.0)\n    assert y == 0.0\n\n    # Mixed derivatives\n    expr = Derivative(Derivative(x*y, x), y)\n    assert tensorflow_code(expr) == 'tensorflow.gradients(tensorflow.gradients(x*y, x)[0], y)[0]'\n    f = lambdify((x, y), expr, 'tensorflow')\n    y_val = f(1.0, 1.0)\n    assert y_val == 1.0\n", "def test_tensorflow_derivative():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    x, y = symbols('x y')\n    f = Function('f')\n    expr = Derivative(f(x, y), x)\n    assert tensorflow_code(expr) == \\\n        'tensorflow.gradients(f(x, y), x)[0]'\n\n    expr = Derivative(f(x, y), x, 2)\n    assert tensorflow_code(expr) == \\\n        'tensorflow.gradients(tensorflow.gradients(f(x, y), x)[0], x)[0]'\n\n    expr = Derivative(f(x, y), x, y)\n    assert tensorflow_code(expr) == \\\n        'tensorflow.gradients(tensorflow.gradients(f(x, y), x)[0], y)[0]'\n\n    # compare with actual tensorflow computation\n    graph = tf.Graph()\n    with graph.as_default():\n        session = tf.compat.v1.Session(graph=graph)\n\n        x_val = 1.0\n        y_val = 2.0\n        x_tf = tf.Variable(x_val)\n        y_tf = tf.Variable(y_val)\n\n        f_tf = x_tf**2 * y_tf\n        dx_tf = tf.gradients(f_tf, x_tf)[0]\n        dy_tf = tf.gradients(f_tf, y_tf)[0]\n        dxy_tf = tf.gradients(dx_tf, y_tf)[0]\n\n        dx_val = session.run(dx_tf)\n        dy_val = session.run(dy_tf)\n        dxy_val = session.run(dxy_tf)\n\n        assert abs(dx_val - 4.0) < 1e-6\n        assert abs(dy_val - 1.0) < 1e-6\n        assert abs(dxy_val - 2.0) < 1e-6\n", "def test_tensorflow_derivative():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Derivative(sin(x), x)\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.gradients(tensorflow.math.sin(x), x)[0]\"\n    _compare_tensorflow_scalar((x,), expr, rng=lambda: random.random())\n"], "sample_1065": ["def test_subfactorial():\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(7) == 1854\n    assert subfactorial(8) == 14833\n    assert subfactorial(9) == 133496\n    assert subfactorial(10) == 1334961\n    assert subfactorial(-1) == S.NaN\n    assert subfactorial(-5) == S.NaN\n    assert subfactorial(S.Half) == S.NaN\n    assert subfactorial(I) == S.NaN\n    assert subfactorial(oo) == oo\n    assert subfactorial(nan) == nan\n\n    x = symbols('x')\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(5*x).is_integer is None\n    assert subfactorial(x).is_nonnegative is None\n    assert subfactorial(5*x).is_nonnegative is None\n    assert subfactorial(x).is_even is None\n    assert subfactorial(x).is_odd is None\n\n    n = symbols('n', integer=True)\n    assert subfactorial(n).is_integer\n    assert subfactorial(n).is_nonnegative\n    assert subfactorial(n).is_even is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(2*n).is_even\n\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1)/S.Exp1\n", "def test_subfactorial():\n    from sympy import I, uppergamma, exp\n    n = Symbol('n')\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/exp(1)\n    assert subfactorial(n).rewrite(exp) == subfactorial(n)\n\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n\n    assert isinstance(subfactorial(I), subfactorial)\n    assert isinstance(subfactorial(n), subfactorial)\n\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(5).is_integer is True\n    assert subfactorial(6).is_integer is True\n\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(5).is_nonnegative is True\n    assert subfactorial(6).is_nonnegative is True\n\n    assert subfactorial(n).is_even is None\n    assert subfactorial(5).is_even is False\n    assert subfactorial(6).is_even is True\n\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(5).is_odd is True\n    assert subfactorial(6).is_odd is False\n", "def test_subfactorial():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    u = Symbol('u', negative=True)\n    v = Symbol('v', nonnegative=True)\n    p = Symbol('p', positive=True)\n    z = Symbol('z', zero=True)\n    nt = Symbol('nt', integer=False)\n    kt = Symbol('kt', integer=False)\n\n    assert subfactorial(nan) == nan\n    assert subfactorial(oo) == oo\n\n    assert subfactorial(-1) == 0\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(7) == 1854\n    assert subfactorial(8) == 14833\n    assert subfactorial(9) == 133496\n    assert subfactorial(10) == 1334961\n    assert subfactorial(k + 1) == k*subfactorial(k) + subfactorial(k - 1)\n    assert subfactorial(n).func == subfactorial\n\n    assert subfactorial(x).is_integer is None\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(k).is_integer is True\n    assert subfactorial(nt).is_integer is None\n    assert subfactorial(u).is_integer is None\n    assert subfactorial(p).is_integer is None\n\n    assert subfactorial(x).is_even is None\n    assert subfactorial(n).is_even is None\n    assert subfactorial(k).is_even is None\n    assert subfactorial(nt).is_even is None\n    assert subfactorial(u).is_even is None\n    assert subfactorial(p).is_even is None\n    assert subfactorial(0).is_even is False\n    assert subfactorial(1).is_even is True\n    assert subfactorial(2).is_even is False\n    assert subfactorial(3).is_even is True\n\n    assert subfactor", "def test_subfactorial():\n    from sympy import subfactorial\n    n = Symbol('n', integer=True)\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(-1) == S.NaN\n    assert subfactorial(-4) == S.NaN\n    assert subfactorial(0) == 1\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(4).is_integer is True\n    assert subfactorial(-3).is_integer is False\n    assert subfactorial(n).is_even is None\n    assert subfactorial(4).is_even is False\n    assert subfactorial(3).is_even is False\n    assert subfactorial(6).is_even is False\n    assert subfactorial(0).is_even is False\n    assert subfactorial(1).is_even is True\n    assert subfactorial(2).is_even is False\n    assert subfactorial(5).is_even is False\n    assert subfactorial(-3).is_even is False\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(4).is_odd is True\n    assert subfactorial(3).is_odd is True\n    assert subfactorial(6).is_odd is True\n    assert subfactorial(0).is_odd is True\n    assert subfactorial(1).is_odd is False\n    assert subfactorial(2).is_odd is True\n    assert subfactorial(5).is_odd is True\n    assert subfactorial(-3).is_odd is False\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(4).is_nonnegative is True\n    assert subfactorial(3).is_nonnegative is True\n    assert subfactorial(6).is_nonnegative is True\n    assert subfactorial(0).is_nonnegative is True\n    assert subfactorial(1).is_non", "def test_subfactorial():\n    from sympy import subfactorial, factorial, uppergamma, E, pi, I\n    n = Symbol('n', integer=True)\n\n    assert subfactorial(-4) == zoo\n    assert subfactorial(-1) == 0\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(6) == 265\n    assert subfactorial(10) == 1334961\n\n    assert subfactorial(n).func == subfactorial\n    assert subfactorial(n).is_integer is None\n    assert subfactorial(n).is_nonnegative is None\n    assert subfactorial(n).is_odd is None\n    assert subfactorial(n).is_even is None\n\n    assert subfactorial(4).is_integer\n    assert subfactorial(4).is_nonnegative\n    assert subfactorial(5).is_odd\n\n    assert subfactorial(4).is_positive\n    assert subfactorial(-1).is_nonpositive\n    assert subfactorial(-4).is_nonpositive is None\n\n    assert subfactorial(4).is_real\n    assert subfactorial(-4).is_real\n\n    assert subfactorial(n).rewrite(factorial) == subfactorial(n)\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/E\n    assert subfactorial(1).rewrite(uppergamma) == uppergamma(2, -1)/E\n    assert subfactorial(0).rewrite(uppergamma) == uppergamma(1, -1)/E\n\n    assert subfactorial(n).evalf() == subfactorial(n)\n    assert subfactorial(3).evalf() == 2\n    assert subfactorial(n).evalf(6) == subfactorial(n)\n    assert subfactorial(3).evalf(6) == 2\n\n    assert subfactorial(pi) == subfactorial(pi)\n    assert subfactorial(I) == subfactorial(I)\n\n    assert subfactorial(S(-5)/2) == uppergamma(S(-3)/2, -1)/E\n    assert"], "sample_1067": ["def test_issue_11447():\n    x = Symbol('x')\n    w = Wild('w')\n    assert (x - 2).match(2 - w) is None\n", "def test_issue_3539_non_commutative():\n    x = Symbol('x', commutative=False)\n    a = Wild('a')\n    assert (x - 2).match(a - x) is None\n    assert (6/x).match(a*x) is None\n    assert (6/x**2).match(a/x) == {a: 6/x}\n", "def test_custom_match_methods():\n    class CustomMatcher:\n            return expr == 1\n\n    x = Symbol('x')\n    p = Wild('p')\n    assert x.match(p) == {p: x}\n    assert x.match(CustomMatcher()) is None\n    assert 1.match(CustomMatcher()) == {}\n", "def test_issue_5308():\n    x = Symbol('x')\n    p = Wild('p')\n    q = Wild('q')\n    assert ((2*x)**(2*x)).match(p**q) == {p: 2*x, q: 2*x}\n    assert ((2*x)**(2*x)).match((2*x)**p) == {p: 2*x}\n    assert ((2*x)**(2*x)).match(p**(2*x)) == {p: 2*x}\n", "def test_match_with_noncommutative_wildcard():\n    A, B, C = symbols('A B C', commutative=False)\n    w = Wild('w', commutative=False)\n    x = Symbol('x')\n    p = Wild('p')\n\n    assert (A*B).match(w) == {w: A*B}\n    assert (x*A).match(w) == {w: x*A}\n    assert (A*x).match(w) == {w: A*x}\n\n    assert (A*B).match(p*w) == {w: B, p: A}\n    assert (A*B).match(w*p) == {w: A, p: B}\n    assert (x*A).match(p*w) == {w: A, p: x}\n    assert (x*A).match(w*p) == {w: x, p: A}\n    assert (A*x).match(p*w) == {w: x, p: A}\n    assert (A*x).match(w*p) == {w: A, p: x}\n"], "sample_1066": ["def test_mathml_MatMul():\n    from sympy import MatMul\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 4)\n    C = MatrixSymbol('C', 4, 5)\n    expr = MatMul(A, B, C).doit()\n    assert mathml(expr, printer='presentation') == \\\n        '<mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi>'\\\n        '<mo>&InvisibleTimes;</mo><mi>C</mi></mrow>'\n    assert mathml(-MatMul(A, B, C).doit(), printer='presentation') == \\\n        '<mrow><mo>-</mo><mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi>'\\\n        '<mo>&InvisibleTimes;</mo><mi>C</mi></mrow></mrow>'\n    assert mathml(MatMul(-A, B, -C).doit(), printer='presentation') == \\\n        '<mrow><mo>-</mo><mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi>'\\\n        '<mo>&InvisibleTimes;</mo><mi>C</mi></mrow></mrow>'\n    assert mathml(MatMul(-A, -B, C).doit(), printer='presentation') == \\\n        '<mrow><mo>-</mo><mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi>'\\\n        '<mo>&InvisibleTimes;</mo><mi>C</mi></mrow></mrow>'\n    assert mathml(MatMul(A, -B, -C).doit(), printer='presentation') == \\\n        '<mrow><mi>A</mi><mo>&InvisibleTimes;</mo><mi>B</mi>'\\\n        '<mo>&InvisibleTimes;</mo><mi>C</mi></mrow>'\n", "def test_print_hbar():\n    assert mpp.doprint(hbar) == '<mi>&#x210F;</mi>'\n    assert mp.doprint(hbar) == '<hbar/>'\n", "def test_print_hbar():\n    assert mpp.doprint(hbar) == '<mi>&#x210F;</mi>'\n", "def test_mathml_Inverse():\n    from sympy import MatrixSymbol\n    X = MatrixSymbol('X', 2, 2)\n    assert mathml(Inverse(X), printer='presentation') == \\\n        '<msup><mi>X</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup>'\n    assert mathml(Inverse(X + X), printer='presentation') == \\\n        '<msup><mfenced><mrow><mi>X</mi><mo>+</mo><mi>X</mi></mrow></mfenced><mrow><mo>-</mo><mn>1</mn></mrow></msup>'\n    assert mathml(Inverse(X).T, printer='presentation') == \\\n        '<msup><msup><mi>X</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>T</mo></msup>'\n    assert mathml(Inverse(X.T), printer='presentation') == \\\n        '<msup><msup><mi>X</mi><mo>T</mo></msup><mrow><mo>-</mo><mn>1</mn></mrow></msup>'\n", "def test_print_inverse():\n    from sympy.matrices import MatrixSymbol, Inverse\n    X = MatrixSymbol('X', 2, 2)\n    assert mathml(Inverse(X), printer='presentation') == \\\n        '<msup><mi>X</mi><mn>-1</mn></msup>'\n    assert mathml(Inverse(X) + X, printer='presentation') == \\\n        '<mrow><msup><mi>X</mi><mn>-1</mn></msup><mo>+</mo><mi>X</mi></mrow>'\n    assert mathml(Inverse(X)*X, printer='presentation') == \\\n        '<mrow><msup><mi>X</mi><mn>-1</mn></msup><mo>&InvisibleTimes;</mo><mi>X</mi></mrow>'\n"], "sample_1068": ["def test_user_functions():\n    x = symbols('x')\n    f = implemented_function('f', Lambda(x, 2*x))\n    assert octave_code(f(x)) == \"f(x)\"\n    g = implemented_function('g', Lambda(x, 2*x))\n    user_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    assert octave_code(f(x), user_functions=user_functions) == \"existing_octave_fcn(x)\"\n    assert octave_code(g(x), user_functions=user_functions) == \"my_fcn(x)\"\n    assert octave_code(g(Matrix([[x]])), user_functions=user_functions) == \"my_mat_fcn([x])\"\n", "def test_octave_inline_substitution():\n    # test inline substitution for symbols\n    assert mcode(pi*x - pi, inline=True) == \"pi*(x - 1)\"\n    assert mcode(pi*x - pi, inline=False) == \"pi*x - pi\"\n    assert mcode(x**2 - x, inline=True) == \"x.*(x - 1)\"\n    assert mcode(x**2 - x, inline=False) == \"x.^2 - x\"\n    # test inline substitution for expressions with multiple terms\n    assert mcode(pi*x - pi*y, inline=True) == \"pi*(x - y)\"\n    assert mcode(pi*x - pi*y, inline=False) == \"pi*x - pi*y\"\n    # test inline substitution for expressions with multiple factors\n    assert mcode(pi*x*y - pi*x, inline=True) == \"pi*x*(y - 1)\"\n    assert mcode(pi*x*y - pi*x, inline=False) == \"pi*x.*y - pi*x\"\n", "def test_Pow_infinite():\n    assert mcode(S.Half**oo) == \"0\"\n    assert mcode(S.Half**(-oo)) == \"inf\"\n    assert mcode(2**oo) == \"inf\"\n    assert mcode(2**(-oo)) == \"0\"\n    assert mcode(S.Zero**oo) == \"0\"\n    assert mcode(S.Zero**(-oo)) == \"inf\"\n    assert mcode(S.One**oo) == \"1\"\n    assert mcode(S.One**(-oo)) == \"1\"\n    assert mcode(S.Infinity**oo) == \"inf\"\n    assert mcode(S.Infinity**(-oo)) == \"0\"\n    assert mcode(S.NegativeInfinity**oo) == \"0\"\n    assert mcode(S.NegativeInfinity**(-oo)) == \"inf\"\n    assert mcode((-2)**oo) == \"% Not supported in Octave:\\n% Pow\\n(-2)**oo\"\n    assert mcode((-S.Half)**(-oo)) == \"% Not supported in Octave:\\n% Pow\\n(-1/2)**-oo\"\n", "def test_octave_user_functions():\n    f = Function('f')\n    g = implemented_function('g', Lambda(x, 2*x))\n    assert mcode(f(x)) == '% Not supported in Octave:\\n% f\\nf(x)'\n    assert mcode(g(x)) == '% Not supported in Octave:\\n% g\\ng(x)'\n    user_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    assert mcode(f(x), user_functions=user_functions) == 'existing_octave_fcn(x)'\n    assert mcode(g(x), user_functions=user_functions) == 'my_fcn(x)'\n    assert mcode(g(Matrix([[1, 2], [3, 4]])), user_functions=user_functions) == \\\n        'my_mat_fcn([1 2; 3 4])'\n", "def test_user_functions():\n    f = Function('f')\n    g = Function('g')\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n\n    x = symbols('x')\n    A = Matrix([[1, x]])\n    assert octave_code(f(x), user_functions=custom_functions) == \"existing_octave_fcn(x)\"\n    assert octave_code(g(x), user_functions=custom_functions) == \"my_fcn(x)\"\n    assert octave_code(g(A), user_functions=custom_functions) == \"my_mat_fcn([1 x])\"\n    assert octave_code(f(A), user_functions=custom_functions) == \"existing_octave_fcn([1 x])\"\n"], "sample_1070": ["def test_issue_18164():\n    assert exp(x + y, evaluate=False).is_Mul is False\n    assert exp(x + y).is_Mul\n    assert exp(x + y, evaluate=False).is_Add is False\n    assert exp(x + y).is_Add is False\n", "def test_log_nonpositive():\n    x = Symbol('x', nonpositive=True)\n    assert log(x).is_extended_real is False\n", "def test_refine_log():\n    x = symbols('x', real=True)\n    assert refine(log(x), Q.is_positive(x)) == log(x)\n    assert refine(log(x), Q.is_negative(x)) == log(-x) + I*pi\n    assert refine(log(x), Q.is_zero(x)) == -oo\n\n    x = symbols('x')\n    assert refine(log(x), Q.is_positive(x)) == log(x)\n    assert refine(log(x), Q.is_negative(x)) == log(-x) + I*pi\n    assert refine(log(x), Q.is_zero(x)) == -oo\n", "def test_log_nonpositive():\n    x = Symbol('x', nonpositive=True)\n    assert log(x).is_extended_real is False\n    y = Symbol('y', nonnegative=True)\n    assert log(y).is_extended_real is None\n    z = Symbol('z', positive=True)\n    assert log(z).is_extended_real is True\n\n    assert log(-x).is_extended_real is None\n    assert log(-y).is_extended_real is None\n    assert log(-z).is_extended_real is False\n", "def test_log_powers():\n    x = Symbol('x', real=True)\n    assert log(x**0) == 0\n    assert log(x**1) == log(x)\n    assert log(x**2) == 2*log(x)\n    assert log(x**-1) == -log(x)\n    assert log(x**-2) == -2*log(x)\n    assert log((x**-2)**3) == -6*log(x)\n\n    y = Symbol('y', positive=True)\n    assert log(y**0) == 0\n    assert log(y**1) == log(y)\n    assert log(y**2) == 2*log(y)\n    assert log(y**-1) == -log(y)\n    assert log(y**-2) == -2*log(y)\n    assert log((y**-2)**3) == -6*log(y)\n"], "sample_1069": ["def test_glsl_code():\n    x, y = symbols('x,y')\n    assert glsl_code(x*y) == \"x*y\"\n    assert glsl_code(sin(x)) == \"sin(x)\"\n    assert glsl_code(log(x)) == \"log(x)\"\n    assert glsl_code(exp(x)) == \"exp(x)\"\n    assert glsl_code(x**3.0) == \"pow(x, 3.0)\"\n    assert glsl_code(sqrt(x)) == \"sqrt(x)\"\n    assert glsl_code(1/sqrt(x)) == \"1.0/sqrt(x)\"\n    assert glsl_code(exp(-x)) == \"exp(-x)\"\n    assert glsl_code(2**x) == \"pow(2, x)\"\n    assert glsl_code(x**y) == \"pow(x, y)\"\n    assert glsl_code(x**(y**z)) == \"pow(x, pow(y, z))\"\n    assert glsl_code((x+y)**2) == \"pow((x + y), 2)\"\n", "def test_glsl_code_pow():\n    assert glsl_code(x**3) == \"pow(x, 3.0)\"\n    assert glsl_code(x**(y**3)) == \"pow(x, pow(y, 3.0))\"\n    assert glsl_code(x**Rational(2, 3)) == 'pow(x, 2.0/3.0)'\n    g = implemented_function('g', Lambda(x, 2*x))\n    assert glsl_code(1/(g(x)*3.5)**(x - y**x)/(x**2 + y)) == \\\n        \"1.0/pow(3.5*2*x, -x + pow(y, x))/(x*x + y)\"\n", "def test_glsl_code():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    v = MatrixSymbol('v', 3, 1)\n    h = MatrixSymbol('h', 1, 3)\n    C = HadamardProduct(A, B)\n    assert glsl_code(C) == \"A*B\"\n    assert glsl_code(C*v) == \"(A*B)*v\"\n    assert glsl_code(h*C*v) == \"h*(A*B)*v\"\n    assert glsl_code(C*A) == \"(A*B)*A\"\n    assert glsl_code(C*x*y) == \"(x*y)*(A*B)\"\n    mat = Matrix([\n        [ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8],\n        [ 9, 10, 11],\n        [12, 13, 14]])\n    assert glsl_code(mat, mat_nested = True) == (\n        \"float[5][3](\\n\"\n        \"   float[]( 0,  1,  2),\\n\"\n        \"   float[]( 3,  4,  5),\\n\"\n        \"   float[]( 6,  7,  8),\\n\"\n        \"   float[]( 9, 10, 11),\\n\"\n        \"   float[](12, 13, 14)\\n\"\n        \")\"\n    )\n    assert glsl_code(mat, mat_nested = False) == (\n        \"float[15](\\n\"\n        \"   0,  1,  2,\\n\"\n        \"   3,  4,  5,\\n\"\n        \"   6,  7,  8,\\n\"\n        \"   9, 10, 11,\\n\"\n        \"  12, 13, 14\\n\"\n        \") /* a 5x3 matrix */\"\n    )\n    mat = Matrix([[1, sin(2/x), 3*pi/x/5]])\n    assert glsl_code(mat) == \"[1 sin(2.0/x) 3.0*pi/(5.0*x)]\"\n    mat = Matrix([[sin(x/2)]])\n    assert glsl_code(mat", "def test_glsl_code():\n    assert glsl_code(x**2) == 'pow(x, 2.0)'\n    assert glsl_code(x**2, assign_to='a') == 'float a = pow(x, 2.0);'\n    assert glsl_code(S(2)) == '2.0'\n    assert glsl_code(pi) == '3.141592653589793'\n    assert glsl_code(pi**3, assign_to='r') == 'float r = pow(3.141592653589793, 3.0);'\n    assert glsl_code(sin(x), assign_to='y') == 'float y = sin(x);'\n    assert glsl_code(cos(x**2), assign_to='y') == 'float y = cos(pow(x, 2.0));'\n", "def test_glsl_codegamma():\n    n = Symbol('n')\n    assert glsl_code(gamma(x)) == 'gamma(x)'\n    assert glsl_code(uppergamma(n, x)) == 'uppergamma(n, x)'\n    assert glsl_code(lowergamma(n, x)) == 'lowergamma(n, x)'\n    assert glsl_code(polygamma(x, y)) == 'polygamma(x, y)'\n"], "sample_1071": ["def test_check_dimensions():\n    from sympy.physics.units import meter, second, kilogram\n    from sympy import sin\n\n    assert check_dimensions(meter + meter) == 2*meter\n    assert check_dimensions(meter + second) == meter + second\n    raises(ValueError, lambda: check_dimensions(meter + 1))\n    raises(ValueError, lambda: check_dimensions(1 + meter))\n    raises(ValueError, lambda: check_dimensions(meter + kilogram))\n    raises(ValueError, lambda: check_dimensions(sin(meter/meter)*meter + 1))\n", "def test_check_dimensions():\n    from sympy.physics.units import meter, second, newton, kilogram\n    from sympy.physics.units.util import check_dimensions\n\n    assert check_dimensions(meter + meter) == 2*meter\n    assert check_dimensions(meter - meter) == 0\n    assert check_dimensions(2*meter) == 2*meter\n    assert check_dimensions(meter/second + meter/second) == 2*meter/second\n    assert check_dimensions(newton + kilogram*meter/second**2) == 2*kilogram*meter/second**2\n\n    raises(ValueError, lambda: check_dimensions(meter + 1))\n    raises(ValueError, lambda: check_dimensions(1 + meter))\n    raises(ValueError, lambda: check_dimensions(2 + meter/second))\n    raises(ValueError, lambda: check_dimensions(meter/second + 2))\n", "def test_check_dimensions():\n    from sympy.physics.units import meter, second\n\n    assert check_dimensions(meter) == meter\n    assert check_dimensions(second) == second\n    assert check_dimensions(meter/second) == meter/second\n    assert check_dimensions(meter + second) == meter + second\n    assert check_dimensions(meter + meter) == 2*meter\n\n    raises(ValueError, lambda: check_dimensions(meter + 1))\n    raises(ValueError, lambda: check_dimensions(1 + meter))\n    raises(ValueError, lambda: check_dimensions(second + 1))\n    raises(ValueError, lambda: check_dimensions(1 + second))\n    raises(ValueError, lambda: check_dimensions(meter/second + 1))\n    raises(ValueError, lambda: check_dimensions(1 + meter/second))\n", "def test_check_dimensions():\n    assert check_dimensions(meter + meter) == 2*meter\n    assert check_dimensions(meter - meter) == 0\n    assert check_dimensions(2*meter + 3*meter) == 5*meter\n    assert check_dimensions(meter + second) == meter + second\n    raises(ValueError, lambda: check_dimensions(1 + meter))\n    raises(ValueError, lambda: check_dimensions(meter + 1))\n    raises(ValueError, lambda: check_dimensions(2 + 3*meter))\n    raises(ValueError, lambda: check_dimensions(2*meter + 3))\n    assert check_dimensions(meter*(1 + 1)) == 2*meter\n    assert check_dimensions((1 + 1)*meter) == 2*meter\n", "def test_check_dimensions():\n    # Check that adding unitless values to dimensional quantities raises an error\n    raises(ValueError, lambda: check_dimensions(3 + meter))\n    raises(ValueError, lambda: check_dimensions(meter + 3))\n    raises(ValueError, lambda: check_dimensions(3 + meter + second))\n    raises(ValueError, lambda: check_dimensions(meter + second + 3))\n\n    # Check that dimensional quantities with different dimensions raise an error\n    raises(ValueError, lambda: check_dimensions(meter + second))\n    raises(ValueError, lambda: check_dimensions(meter + kilogram))\n    raises(ValueError, lambda: check_dimensions(meter + kelvin))\n\n    # Check that adding dimensional quantities with the same dimension does not raise an error\n    assert check_dimensions(meter + kilometer) == meter + kilometer\n    assert check_dimensions(kilogram + gram) == kilogram + gram\n\n    # Check that multiplying dimensional quantities does not raise an error\n    assert check_dimensions(meter * second) == meter * second\n    assert check_dimensions(kilogram * meter / second**2) == kilogram * meter / second**2\n"], "sample_1073": ["def test_is_sqrt():\n    from sympy.simplify.sqrtdenest import is_sqrt\n    assert is_sqrt(sqrt(3)) is True\n    assert is_sqrt(sqrt(4)) is True\n    assert is_sqrt(2) is False\n    assert is_sqrt(root(2, 3)) is False\n    assert is_sqrt(root(2, 4)) is True\n", "def test_sqrt_match():\n    z = sqrt(2) + sqrt(sqrt(3))\n    a, b, r = _sqrt_match(z)\n    assert a == sqrt(2)\n    assert b == 1\n    assert r == sqrt(3)\n    z = sqrt(2) + sqrt(sqrt(3)) + sqrt(sqrt(5))\n    a, b, r = _sqrt_match(z)\n    assert a == sqrt(2)\n    assert b == 1\n    assert r == sqrt(3)\n    z = sqrt(2) + sqrt(sqrt(3)*sqrt(5))\n    a, b, r = _sqrt_match(z)\n    assert a == sqrt(2)\n    assert b == sqrt(5)\n    assert r == sqrt(3)\n", "def test_sqrt_match():\n    z = 1 + sqrt(2) + sqrt(2)*sqrt(3) + 2*sqrt(1+sqrt(5))\n    assert _sqrt_match(z) == [1 + sqrt(2) + sqrt(6), 2, 1 + sqrt(5)]\n\n    z = sqrt(2)*sqrt(3)\n    assert _sqrt_match(z) == []\n\n    z = sqrt(2) + 2\n    assert _sqrt_match(z) == [2, 1, 2]\n\n    z = sqrt(1 + sqrt(2))\n    assert _sqrt_match(z) == []\n", "def test_sqrt_match():\n    assert _sqrt_match(1 + r2) == [1, 1, 2]\n    assert _sqrt_match(1 + r2 + r3) == [1 + r2, 1, 3]\n    assert _sqrt_match(1 + r2 + r3 + r5) == [1 + r2, 1, 15 + 6*r10]\n    assert _sqrt_match(1 + r2 + r3 + r5 + r7) == [1 + r2 + r3, 1, 105 + 46*r10 + 15*r14]\n    assert _sqrt_match(1 + r2 + r3 + r5 + r7 + r10 + r11 + r13 + r14 + r15) == [\n        1 + r2 + r3 + r5 + r7 + r11, 1, 255 + 128*r10 + 70*r14 + 30*r15 + 30*r23 + 21*r26]\n", "def test_is_algebraic():\n    assert is_algebraic(sqrt(2)*(3/(sqrt(7) + sqrt(5)*sqrt(2)))) is True\n    assert is_algebraic(sqrt(2)*(3/(sqrt(7) + sqrt(5)*cos(2)))) is False\n    assert is_algebraic(sqrt(2)*sqrt(3)*sqrt(7)) is True\n    assert is_algebraic(sqrt(2)*sqrt(3)*cos(7)) is False\n    assert is_algebraic(sqrt(2)*sqrt(3)*sqrt(7)*sqrt(11)) is True\n    assert is_algebraic(sqrt(2)*sqrt(3)*sqrt(7)*cos(11)) is False\n"], "sample_1072": ["def test_frac_complex():\n    assert frac(1 + I) == frac(1) + I*frac(1)\n    assert frac(1 + 2*I) == frac(1) + I*frac(2)\n    assert frac(I) == I\n    assert frac(-I) == -I\n    assert frac(0.5 + I) == 0.5 + I\n    assert frac(-0.5 + I) == 0.5 + I\n\n    x = Symbol('x', real=True)\n    assert frac(x + I) == frac(x) + I\n    assert frac(x - I) == frac(x) - I\n\n    y = Symbol('y', real=True)\n    assert frac(x + y*I) == frac(x) + I*frac(y)\n    assert frac(x - y*I) == frac(x) - I*frac(y)\n\n    assert frac(1 + pi*I) == frac(1) + I*frac(pi)\n    assert frac(1 - pi*I) == frac(1) - I*frac(pi)\n\n    assert frac(-1 + I) == frac(-1) + I*frac(1)\n    assert frac(-1 - I) == frac(-1) - I*frac(1)\n", "def test_frac_rewrite():\n    x = Symbol('x')\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n    assert frac(x).rewrite(floor).subs(x, -pi) == frac(-pi)\n    assert frac(x).rewrite(floor).subs(x, E) == frac(E)\n    assert frac(x).rewrite(ceiling).subs(x, -pi) == frac(-pi)\n    assert frac(x).rewrite(ceiling).subs(x, E) == frac(E)\n\n    y = Symbol('y', real=True)\n    assert frac(y).rewrite(floor) == y - floor(y)\n    assert frac(y).rewrite(ceiling) == y + ceiling(-y)\n    assert frac(y).rewrite(floor).subs(y, pi) == frac(pi)\n    assert frac(y).rewrite(floor).subs(y, -E) == frac(-E)\n    assert frac(y).rewrite(ceiling).subs(y, pi) == frac(pi)\n    assert frac(y).rewrite(ceiling).subs(y, -E) == frac(-E)\n\n    assert Eq(frac(x), x - floor(x))\n    assert Eq(frac(x), x + ceiling(-x))\n    assert Eq(frac(y), y - floor(y))\n    assert Eq(frac(y), y + ceiling(-y))\n", "def test_frac_nonreal():\n    p_i = Symbol('p_i', integer=True, positive=True)\n    n_i = Symbol('p_i', integer=True, negative=True)\n    np_i = Symbol('np_i', integer=True, nonpositive=True)\n    nn_i = Symbol('nn_i', integer=True, nonnegative=True)\n    p_r = Symbol('p_r', real=True, positive=True)\n    n_r = Symbol('n_r', real=True, negative=True)\n    np_r = Symbol('np_r', real=True, nonpositive=True)\n    nn_r = Symbol('nn_r', real=True, nonnegative=True)\n\n    i = Symbol('i', imaginary=True)\n    ni = Symbol('ni', imaginary=True, negative=True)\n    pi = Symbol('pi', imaginary=True, positive=True)\n    npi = Symbol('npi', imaginary=True, nonpositive=True)\n    ppi = Symbol('ppi', imaginary=True, nonnegative=True)\n\n    # Nonreal frac argument, integer rhs\n    assert frac(i) <= p_i\n    assert not frac(i) <= n_i\n    assert (frac(i) <= np_i).has(Le)\n    assert (frac(i) <= nn_i).has(Le)\n    assert frac(i) < p_i\n    assert not frac(i) < n_i\n    assert not frac(i) < np_i\n    assert (frac(i) < nn_i).has(Lt)\n    assert not frac(i) >= p_i\n    assert frac(i) >= n_i\n    assert frac(i) >= np_i\n    assert (frac(i) >= nn_i).has(Ge)\n    assert not frac(i) > p_i\n    assert frac(i) > n_i\n    assert (frac(i) > np_i).has(Gt)\n    assert (frac(i) > nn_i).has(Gt)\n\n    assert not Eq(frac(i), p_i)\n    assert not Eq(frac(i), n_i)\n    assert Eq(frac(i), np_i).has(Eq)\n    assert Eq(frac(i), nn_i).has(Eq)\n\n    assert Ne(frac(i), p_i)\n    assert Ne(frac(i), n_i)\n    assert Ne(frac(i), np_i).has(Ne)\n    assert Ne(frac(i), nn_i).has(Ne)\n\n    # Nonreal frac argument, real rhs\n    assert (frac(i) <= p_r).has(Le)\n", "def test_frac_periodicity():\n    x = Symbol('x')\n    assert frac(x) == frac(x + 1)\n    assert frac(x) == frac(x + 2)\n    assert frac(x) == frac(x - 1)\n    assert frac(x) == frac(x - 2)\n\n    assert frac(x + 1) == frac(x)\n    assert frac(x + 2) == frac(x)\n    assert frac(x - 1) == frac(x)\n    assert frac(x - 2) == frac(x)\n\n    assert frac(x + S(3)/2) == frac(x + S(1)/2)\n    assert frac(x - S(3)/2) == frac(x - S(1)/2)\n\n    assert frac(x + 1.5) == frac(x + 0.5)\n    assert frac(x - 1.5) == frac(x - 0.5)\n", "def test_RoundFunction_eval():\n    # test for RoundFunction._eval_number\n    assert floor(2.5).evalf() == 2\n    assert ceiling(2.5).evalf() == 3\n\n    # test for RoundFunction._eval_nseries\n    x = Symbol('x')\n    assert floor(x + 2).nseries(x, 0, 3) == 2\n    assert ceiling(x + 2).nseries(x, 0, 3) == 2\n\n    # test for RoundFunction._eval_is_finite\n    assert floor(2.5).is_finite\n    assert ceiling(2.5).is_finite\n\n    # test for RoundFunction._eval_is_real\n    assert floor(2.5).is_real\n    assert ceiling(2.5).is_real\n\n    # test for RoundFunction._eval_is_integer\n    assert floor(2.5).is_integer\n    assert ceiling(2.5).is_integer\n"], "sample_1075": ["def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert expand_func(beta(x, y)) == gamma(x)*gamma(y) / gamma(x + y)\n", "def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert expand_func(beta(x, y)) == gamma(x)*gamma(y) / gamma(x + y)\n", "def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert beta(x, y)._eval_expand_func() == gamma(x)*gamma(y) / gamma(x + y)\n", "def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert beta(x, y)._eval_expand_func() == gamma(x)*gamma(y) / gamma(x + y)\n", "def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert beta(x, y)._eval_expand_func() == gamma(x)*gamma(y) / gamma(x + y)\n"], "sample_1074": ["def test_strong_presentation():\n    G = SymmetricGroup(3)\n    H = AlternatingGroup(3)\n    iso = is_isomorphic(G, H)\n    assert iso is False\n    G = PermutationGroup(Permutation(0,1,2), Permutation(0,1))\n    H = PermutationGroup(Permutation(0,1,2))\n    iso = is_isomorphic(G, H)\n    assert iso is False\n    assert G.strong_presentation().order() == G.order()\n    assert H.strong_presentation().order() == H.order()\n", "def test_is_abelian():\n    a = Permutation([1, 2, 0])\n    b = Permutation([1, 0, 2])\n    G = PermutationGroup([a, b])\n    assert G.is_abelian is False\n    G = PermutationGroup([a])\n    assert G.is_abelian is True\n", "def test_is_cyclic_method():\n    # the is_cyclic method should return True for all groups\n    # of prime order\n    for i in (2, 3, 5, 7, 11):\n        C = CyclicGroup(i)\n        assert C.is_cyclic\n    # the is_cyclic method should return True for\n    # all abelian groups with a cyclic presentation\n    for i in (1, 2, 3):\n        Ab = AbelianGroup(i, i+2)\n        assert Ab.is_cyclic\n    # the is_cyclic method should return False for\n    # the alternating group of degree 4\n    A = AlternatingGroup(4)\n    assert A.is_cyclic is False\n    # the is_cyclic method should return False for\n    # the symmetric group of degree 4\n    S = SymmetricGroup(4)\n    assert S.is_cyclic is False\n", "def test_polycyclic_group():\n    G = PermutationGroup(Permutation(0, 1, 2))\n    P = G.polycyclic_group()\n    assert P.order() == G.order()\n", "def test_isomorphism():\n    G1 = SymmetricGroup(4)\n    G2 = PermutationGroup(Permutation(0,1,2,3), Permutation(0,2), Permutation(0,3))\n    G3 = PermutationGroup(Permutation(0,1,3,2), Permutation(0,2), Permutation(0,3))\n    assert is_isomorphic(G1, G2)\n    assert is_isomorphic(G1, G3)\n    assert not is_isomorphic(G1, AlternatingGroup(4))\n    G4 = PermutationGroup(Permutation(0,3,2,1), Permutation(0,3,1,2))\n    G5 = PermutationGroup(Permutation(0,3,2,1), Permutation(0,1,2,3))\n    assert is_isomorphic(G4, G5)\n"], "sample_1076": ["def test_Printer_print_Relational():\n    p = PythonCodePrinter()\n\n    assert p._print_Relational(Eq(x, y)) == '(x == y)'\n    assert p._print_Relational(Le(x, y)) == '(x <= y)'\n    assert p._print_Relational(Gt(x, y)) == '(x > y)'\n\n    n = NumPyPrinter()\n\n    assert n._print_Relational(Eq(x, y)) == 'numpy.equal(x, y)'\n    assert n._print_Relational(Le(x, y)) == 'numpy.less_equal(x, y)'\n    assert n._print_Relational(Gt(x, y)) == 'numpy.greater(x, y)'\n", "def test_PythonCodePrinter_print_While():\n    prntr = PythonCodePrinter()\n    i = symbols('i')\n    whl = While(i < 5, [AugAssign(i, '+', 1)])\n    assert prntr.doprint(whl) == 'while i < 5:\\n    i += 1'\n", "def test_issue_piecewise_with_multiple_conditions():\n    from sympy import And, Or, Not, Piecewise\n    expr = Piecewise((1, Eq(x, 0) & Eq(y, 0)), (2, x > 6))\n    assert PythonCodePrinter().doprint(expr) == '((1) if (x == 0 and y == 0) else (2) if (x > 6) else None)'\n    assert NumPyPrinter().doprint(expr) == 'numpy.select([(x == 0) & (y == 0), (x > 6)], [1, 2], default=numpy.nan)'\n    assert MpmathPrinter().doprint(expr) == '((1) if (x == 0 and y == 0) else (2) if (x > 6) else None)'\n    assert SciPyPrinter().doprint(expr) == 'numpy.select([(x == 0) & (y == 0), (x > 6)], [1, 2], default=numpy.nan)'\n", "def test_issue_20523():\n    from sympy import exp_polar\n    prntr = NumPyPrinter()\n\n    assert prntr.doprint(exp_polar(x)) == 'numpy.exp(1j*x)'\n", "def test_printing_of_sinc():\n    from sympy import sinc\n\n    expr = sinc(x)\n    expected_result = \"numpy.sinc(x / math.pi)\"\n\n    prntr = NumPyPrinter({'standard' : 'python3'})\n    assert prntr.doprint(expr) == expected_result\n\n    prntr = PythonCodePrinter({'standard' : 'python3'})\n    assert prntr.doprint(expr) == \"  # Not supported in Python:\\n  # sinc\\nsinc(x)\"\n"], "sample_1077": ["def test_issue_16947():\n    assert S.Rationals.as_relational(x) == Eq(x, x)\n    assert S.Naturals.as_relational(x) == And(Eq(floor(x), x), x >= S.One, x < oo)\n    assert S.Integers.as_relational(x) == And(Eq(floor(x), x), -oo < x, x < oo)\n", "def test_issue_20680():\n    assert ImageSet(Lambda(x, 1/x), S.Integers).is_iterable is False\n    assert ImageSet(Lambda(x, x), S.Integers).is_iterable is True\n    assert ImageSet(Lambda(x, x), S.Reals).is_iterable is False\n", "def test_normalize_theta_set_numerical():\n    assert normalize_theta_set(Interval(0, pi)) == Interval(0, pi)\n    assert normalize_theta_set(Interval(0, 2*pi)) == Interval.Ropen(0, 2*pi)\n    assert normalize_theta_set(Interval(0, 3*pi)) == Interval.Ropen(0, 2*pi)\n    assert normalize_theta_set(Interval(0, 4*pi)) == Interval.Ropen(0, 2*pi)\n    assert normalize_theta_set(Interval(-pi, pi)) == Interval.Ropen(0, 2*pi)\n    assert normalize_theta_set(Interval(-2*pi, 2*pi)) == Interval.Ropen(0, 2*pi)\n    assert normalize_theta_set(Interval(-3*pi, 3*pi)) == Interval.Ropen(0, 2*pi)\n    assert normalize_theta_set(Interval(-4*pi, 4*pi)) == Interval.Ropen(0, 2*pi)\n", "def test_ComplexRegion_is_subset():\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    upper_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    assert upper_half_unit_disk.is_subset(unit_disk) == True\n    assert unit_disk.is_subset(upper_half_unit_disk) == False\n    assert unit_disk.is_subset(S.Complexes) == True\n    assert S.Complexes.is_subset(unit_disk) == False\n", "def test_issue_19699():\n    assert imageset(Lambda(x, x/2), S.Integers) == S.Rationals\n    assert imageset(Lambda(x, x/3), S.Integers) == S.Rationals\n    assert imageset(Lambda(x, x/6), S.Integers) == S.Rationals\n    assert imageset(Lambda(x, x/2), S.Naturals) != S.Rationals\n    assert imageset(Lambda(x, x/2), S.Naturals0) != S.Rationals\n"], "sample_1078": ["def test_Indexed_range_indexing():\n    n, m = symbols('n m', integer=True)\n    i, j = symbols('i j', cls=Idx)\n    A = IndexedBase('A', shape=(n, m))\n    assert A[i, j].ranges == [(0, n - 1), (0, m - 1)]\n    assert A[1, j].ranges == [None, (0, m - 1)]\n    assert A[i, 1].ranges == [(0, n - 1), None]\n    assert A[1, 1].ranges == [None, None]\n", "def test_IndexedBase_with_Dummy():\n    A = IndexedBase(\"A\")\n    i = Dummy(integer=True)\n    j = symbols(\"j\", integer=True)\n    assert A[i].is_Indexed\n    assert A[i].base == A\n    assert A[i].indices == (i,)\n    assert A[i, j].is_Indexed\n    assert A[i, j].base == A\n    assert A[i, j].indices == (i, j)\n", "def test_IndexedBase_constructor():\n    a, b = symbols('a b')\n    assert IndexedBase(a).label == a\n    assert IndexedBase(a, shape=(3, 4)).label == a\n    assert IndexedBase(a).shape is None\n    assert IndexedBase(a, shape=(3, 4)).shape == Tuple(3, 4)\n    assert IndexedBase(a, strides='C').strides == 'C'\n    assert IndexedBase(a, strides='F').strides == 'F'\n    assert IndexedBase(a, offset=5).offset == 5\n    raises(TypeError, lambda: IndexedBase(a, shape=5))\n    raises(TypeError, lambda: IndexedBase(a, strides='D'))\n    raises(TypeError, lambda: IndexedBase(a, offset='5'))\n", "def test_IndexedBase_free_symbols():\n    i, j = symbols('i j', integer=True)\n    a = symbols('a')\n    A1 = IndexedBase(a, shape=(i, j))\n    A2 = IndexedBase(a)\n    assert A1.free_symbols == {a, i, j}\n    assert A2.free_symbols == {a}\n    assert A1[i, j].free_symbols == {a, i, j}\n    assert A2[i, j].free_symbols == {a, i, j}\n", "def test_IndexedBase_free_symbols():\n    A = IndexedBase(\"A\", shape=(Symbol('m'), Symbol('n')))\n    i, j = symbols('i j', integer=True)\n    assert A.free_symbols == set()\n    assert A[i, j].free_symbols == {i, j, A.label, Symbol('m'), Symbol('n')}\n    assert A.free_symbols == set()\n    assert A[i, j].expr_free_symbols == set()\n"], "sample_1079": ["def test_orthogonal_direction():\n    a = Point(1, 2, 3)\n    b = a.orthogonal_direction\n    assert b.dot(a) == 0\n    assert not b.is_zero\n    a = Point(1, 0)\n    b = a.orthogonal_direction\n    assert b.dot(a) == 0\n    assert not b.is_zero\n    a = Point(0, 1)\n    b = a.orthogonal_direction\n    assert b.dot(a) == 0\n    assert not b.is_zero\n", "def test_bounds():\n    p2d = Point2D(1, 2)\n    assert p2d.bounds == (1, 2, 1, 2)\n", "def test_orthogonal_direction():\n    p1 = Point(1, 1)\n    p2 = Point(1, 1, 1)\n    assert p1.orthogonal_direction == Point(-1, 1)\n    assert p2.orthogonal_direction == Point(-1, 1, 0)\n    p3 = Point(0, 1, 1)\n    assert p3.orthogonal_direction == Point(1, 0, 0)\n    p4 = Point(1, 0, 1)\n    assert p4.orthogonal_direction == Point(0, 1, 0)\n    p5 = Point(0, 0, 1)\n    assert p5.orthogonal_direction == Point(1, 0, 0)\n", "def test_orthogonal_direction():\n    p1 = Point2D(1, 2)\n    p2 = Point3D(1, 2, 3)\n    assert p1.orthogonal_direction == Point2D(-2, 1)\n    assert p2.orthogonal_direction == Point3D(-2, 1, 0)\n    assert p1.orthogonal_direction.is_zero == False\n    assert p2.orthogonal_direction.is_zero == False\n    assert Point2D(0, 1).orthogonal_direction == Point2D(1, 0)\n    assert Point3D(0, 0, 1).orthogonal_direction == Point3D(1, 0, 0)\n    assert Point3D(0, 1, 0).orthogonal_direction == Point3D(0, 0, 1)\n", "def test_orthogonal_direction():\n    p1 = Point(1, 2)\n    p2 = Point(3, 4)\n    p3 = Point(3, 4, 5)\n    p4 = Point(0, 0)\n    p5 = Point(0, 0, 0)\n\n    assert p1.orthogonal_direction == Point(-2, 1)\n    assert p2.orthogonal_direction == Point(-2, 1)\n    assert p3.orthogonal_direction == Point(-4, 3, 0)\n    assert p4.orthogonal_direction == Point(1, 0)\n    assert p5.orthogonal_direction == Point(1, 0, 0)\n"], "sample_1080": ["def test_refine_Pow_simplify():\n    assert refine((-1)**(x + 2)) == (-1)**(x)\n    assert refine((-1)**(x + 4)) == (-1)**(x)\n    assert refine((-1)**(x + 1)) == (-1)**(x + 1)\n    assert refine((-1)**(x + 3)) == (-1)**(x + 1)\n", "def test_refine_Pow_with_non_numeric_base():\n    from sympy.abc import x, y\n    from sympy import Q, Symbol\n    x = Symbol('x', real=True)\n    assert refine((-x)**y, Q.even(y)) == x**y\n    assert refine((-x)**y, Q.odd(y)) == -x**y\n", "def test_refine_hierarchical():\n    x = Symbol('x', real=True)\n    expr = sqrt(x**2)\n    assert refine(expr, Q.real(x)) == Abs(x)\n    assert refine(expr, Q.positive(x)) == x\n    assert refine(expr, Q.negative(x)) == -x\n\n    expr = Abs(x)\n    assert refine(expr, Q.real(x)) == expr\n    assert refine(expr, Q.positive(x)) == x\n    assert refine(expr, Q.negative(x)) == -x\n\n    expr = sqrt(x**2) + Abs(x)\n    assert refine(expr, Q.real(x)) == Abs(x) + Abs(x)\n    assert refine(expr, Q.positive(x)) == 2*x\n    assert refine(expr, Q.negative(x)) == -2*x\n", "def test_refine_Pow():\n    assert refine((-1)**x, Q.even(x)) == 1\n    assert refine((-1)**x, Q.odd(x)) == -1\n    assert refine((-2)**x, Q.even(x)) == 2**x\n    assert refine((-1)**(x + y), Q.even(x)) == (-1)**y\n    assert refine((-1)**(x + y + z), Q.odd(x) & Q.odd(z)) == (-1)**y\n    assert refine((-1)**(x + y + 2), Q.odd(x)) == (-1)**(y + 1)\n    assert refine((-1)**(x + 3), True) == (-1)**(x + 1)\n    assert refine((-1)**((-1)**x/2 - S.Half), Q.integer(x)) == (-1)**x\n    assert refine((-1)**((-1)**x/2 + S.Half), Q.integer(x)) == (-1)**(x + 1)\n    assert refine((-1)**((-1)**x/2 + 5*S.Half), Q.integer(x)) == (-1)**(x + 1)\n    assert refine((-1)**((-1)**x/2 - 7*S.Half), Q.integer(x)) == (-1)**(x + 1)\n    assert refine((-1)**((-1)**x/2 - 9*S.Half), Q.integer(x)) == (-1)**x\n    assert refine((-2)**((-1)**x/2 - S.Half), Q.integer(x)) != (-2)**x\n    assert refine((2*I)**((-1)**x/2 - S.Half), Q.integer(x)) != (2*I)**x\n    assert refine((-2*I)**((-1)**x/2 - S.Half), Q.integer(x)) != (-2*I)**x\n", "compilation error"], "sample_1081": ["def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(220, 220) is False\n    assert is_amicable(284, 1184) is False\n    assert is_amicable(1, 1) is False\n    assert is_amicable(220, 1184) is False\n    assert is_amicable(284, 284) is False\n    assert is_amicable(1184, 1184) is False\n", "def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(1, 2) is False\n    assert is_amicable(10, 15) is False\n", "def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(1, 1) is False\n    assert is_amicable(10, 15) is False\n", "def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(10, 20) is False\n", "def test_is_amicable():\n    assert is_amicable(220, 284) is True\n    assert is_amicable(1184, 1210) is True\n    assert is_amicable(2620, 2924) is True\n    assert is_amicable(5020, 5564) is True\n    assert is_amicable(6232, 6368) is True\n    assert is_amicable(10744, 10856) is True\n    assert is_amicable(12285, 14595) is True\n    assert is_amicable(17296, 18416) is True\n    assert is_amicable(63020, 76084) is True\n    assert is_amicable(66928, 66992) is True\n"], "sample_1082": ["def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y))/(1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x)/(1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == \\\n        (3*tanh(x) - tanh(x)**3)/(1 - 3*tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y))/(1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x)/(1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == \\\n        (3*tanh(x) - tanh(x)**3) / (1 - 3*tanh(x)**2)\n", "def test_is_real():\n    x = Symbol('x', real=True)\n    assert sinh(x).is_real is True\n    assert cosh(x).is_real is True\n    assert tanh(x).is_real is True\n    assert coth(x).is_real is True\n    assert sech(x).is_real is True\n    assert csch(x).is_real is True\n    \n    x = Symbol('x', real=False)\n    assert sinh(x).is_real is None\n    assert cosh(x).is_real is None\n    assert tanh(x).is_real is None\n    assert coth(x).is_real is None\n    assert sech(x).is_real is None\n    assert csch(x).is_real is None\n\n    x = Symbol('x')\n    assert sinh(x).is_real is None\n    assert cosh(x).is_real is None\n    assert tanh(x).is_real is None\n    assert coth(x).is_real is None\n    assert sech(x).is_real is None\n    assert csch(x).is_real is None\n", "def test_issue_7568():\n    x = Symbol('x')\n    assert sech(x).rewrite(cosh) == 1/cosh(x)\n    assert csch(x).rewrite(sinh) == 1/sinh(x)\n    assert coth(x).rewrite(tanh) == 1/tanh(x)\n    assert asech(x).rewrite(acosh) == acosh(1/x)\n    assert acsch(x).rewrite(asinh) == asinh(1/x)\n    assert acoth(x).rewrite(atanh) == atanh(1/x)\n", "def test_real_imag():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert sinh(x + I*y).as_real_imag() == (sinh(x)*cos(y), cosh(x)*sin(y))\n    assert cosh(x + I*y).as_real_imag() == (cosh(x)*cos(y), sinh(x)*sin(y))\n    assert tanh(x + I*y).as_real_imag() == (sinh(2*x)/(2*cosh(x)**2 - 1 + cos(2*y)),\n                                            sin(2*y)/(2*cosh(x)**2 - 1 + cos(2*y)))\n    assert coth(x + I*y).as_real_imag() == (sinh(2*x)/(2*sinh(x)**2 + 1 - cos(2*y)),\n                                            -sin(2*y)/(2*sinh(x)**2 + 1 - cos(2*y)))\n    assert sech(x + I*y).as_real_imag() == (cosh(x)*cos(y)/(cosh(x)**2 + sin(y)**2),\n                                            -sinh(x)*sin(y)/(cosh(x)**2 + sin(y)**2))\n    assert csch(x + I*y).as_real_imag() == (cosh(x)*sin(y)/(cos(y)**2 + sinh(x)**2),\n                                            sinh(x)*cos(y)/(cos(y)**2 + sinh(x)**2))\n"], "sample_1084": ["def test_issue_17355b():\n    S53 = ImageSet(Lambda(n, 5*n + 3), S.Integers)\n    assert S53.intersect(ImageSet(Lambda(n, 5*n + 3), S.Integers)) == S53\n", "def test_issue_18220():\n    from sympy import Symbol\n    x = Symbol('x', real=True)\n    assert Interval(0, 1).as_relational(x) == (x >= 0) & (x <= 1)\n    x = Symbol('x', integer=True)\n    assert Interval(0, 1).as_relational(x) == Eq(x, 0) | Eq(x, 1)\n", "def test_issue_17363():\n    f = ImageSet(Lambda(x, x + sin(x)), S.Integers)\n    g = ImageSet(Lambda(x, x + cos(x)), S.Integers)\n    assert f.intersect(g) != S.EmptySet\n", "def test_issue_12342():\n    c1 = ComplexRegion(Interval(1, 2)*Interval(2, 3))\n    c2 = ComplexRegion(Interval(1, 5)*Interval(1, 3), polar=True)\n\n    assert c1.intersect(c2) == Intersection(c1, c2, evaluate=False)\n    assert c2.intersect(c1) == Intersection(c2, c1, evaluate=False)\n", "def test_issue_19252():\n    from sympy import root\n    assert imageset(Lambda(x, 3*root(x, 3)), S.Integers\n        ) == ImageSet(Lambda(x, 3*x), S.Integers)\n"], "sample_1083": ["def test_issue_21134():\n    z = Symbol('z')\n    assert sinh(z).as_real_imag()[0].is_real\n    assert sinh(z).as_real_imag()[1].is_real\n    assert cosh(z).as_real_imag()[0].is_real\n    assert cosh(z).as_real_imag()[1].is_real\n    assert tanh(z).as_real_imag()[0].is_real\n    assert tanh(z).as_real_imag()[1].is_real\n    assert coth(z).as_real_imag()[0].is_real\n    assert coth(z).as_real_imag()[1].is_real\n    assert sech(z).as_real_imag()[0].is_real\n    assert sech(z).as_real_imag()[1].is_real\n    assert csch(z).as_real_imag()[0].is_real\n    assert csch(z).as_real_imag()[1].is_real\n", "def test_hyperbolic_rewrites():\n    x = Symbol('x')\n    assert sinh(x).rewrite(csch) == 1/csch(x)\n    assert sinh(x).rewrite(sech) == tanh(x)/sech(x)\n    assert cosh(x).rewrite(csch) == coth(x)/csch(x)\n    assert cosh(x).rewrite(sech) == 1/sech(x)\n    assert tanh(x).rewrite(csch) == 1/(csch(x)*coth(x))\n    assert tanh(x).rewrite(sech) == I*sech(x)/sech(x + I*pi/2)\n    assert coth(x).rewrite(csch) == csch(x)/sinh(x)\n    assert coth(x).rewrite(sech) == coth(x)*cosh(x)\n    assert csch(x).rewrite(sech) == sech(I*pi/2 - x)/I\n    assert sech(x).rewrite(csch) == I*csch(I*pi/2 - x)\n", "def test_finite():\n    x, y = symbols('x, y')\n    assert sinh(x).is_finite is None\n    assert cosh(x).is_finite is None\n    assert tanh(x).is_finite is None\n    assert coth(x).is_finite is None\n    assert sinh(y).is_finite is None\n    assert cosh(y).is_finite is None\n    assert tanh(y).is_finite is None\n    assert coth(y).is_finite is None\n    assert sinh(1).is_finite is True\n    assert cosh(1).is_finite is True\n    assert tanh(1).is_finite is True\n    assert coth(1).is_finite is True\n    assert sinh(x).subs(x, 1).is_finite is True\n    assert cosh(x).subs(x, 1).is_finite is True\n    assert tanh(x).subs(x, 1).is_finite is True\n    assert coth(x).subs(x, 1).is_finite is True\n    assert sinh(x).subs(x, oo).is_finite is False\n    assert cosh(x).subs(x, oo).is_finite is False\n    assert tanh(x).subs(x, oo).is_finite is True\n    assert coth(x).subs(x, oo).is_finite is True\n    assert asinh(x).is_finite is None\n    assert acosh(x).is_finite is None\n    assert atanh(x).is_finite is None\n    assert acoth(x).is_finite is None\n    assert asinh(y).is_finite is None\n    assert acosh(y).is_finite is None\n    assert atanh(y).is_finite is None\n    assert acoth(y).is_finite is None\n    assert asinh(1).is_finite is True\n    assert acosh(2).is_finite is True\n    assert atanh(1).is_finite is False\n    assert acoth(1).is_finite is False\n    assert asinh(x).subs(x, 1).is_finite is True\n    assert acosh(x).subs(x, 2).is_finite is True\n    assert atanh(x).subs(x, 1).is_finite is False\n    assert acoth", "def test_issue_21966():\n    from sympy import cosh, asinh, sqrt, I, simplify\n    x = Symbol('x', real=True)\n    z = Symbol('z')\n    assert simplify(cosh(asinh(x))) == sqrt(x**2 + 1)\n    assert simplify(cosh(asinh(sqrt(z**2 + 1)))) == sqrt(z**2 + 2)\n    assert simplify(cosh(asinh(I*x))) == sqrt(-x**2 + 1)\n", "def test_tanh_real_assumptions():\n    x = Symbol('x', real=True)\n    assert tanh(x).is_real is True\n    assert tanh(x).is_imaginary is False\n    assert tanh(x).is_complex is True\n"], "sample_1086": ["def test_DMP_printing():\n    from sympy.polys.domains import ZZ\n    from sympy.polys import Poly\n\n    R, x, y = Poly_ring(\"x,y\", ZZ)\n\n    # testing DMP printing with rep, dom and ring\n    f = x + y**2\n\n    assert str(f) == \"Poly(x + y**2, x, y, domain='ZZ')\"\n    assert repr(f) == \"Poly(x + y**2, x, y, domain='ZZ')\"\n", "def test_issue_14321():\n    assert sstr(log(x, 2)) == 'log(x, 2)'\n", "def test_AlgebraicNumber():\n    from sympy import AlgebraicNumber\n    a = AlgebraicNumber(sqrt(2) + sqrt(3), alias='a')\n    assert str(a) == 'a'\n    assert str(AlgebraicNumber(sqrt(2) + sqrt(3))) == 'sqrt(2) + sqrt(3)'\n", "def test_AlgebraicNumber():\n    from sympy import AlgebraicNumber\n    an = AlgebraicNumber(sqrt(2) + sqrt(3), [sqrt(2), sqrt(3)])\n    assert str(an) == 'sqrt(2) + sqrt(3)'\n    # test alias printing\n    an = AlgebraicNumber(sqrt(2) + sqrt(3), [sqrt(2), sqrt(3)], alias='x')\n    assert str(an) == 'sqrt(2) + sqrt(3)'\n", "def test_predicates():\n    assert str(Q.is_true) == \"Q.is_true\"\n    assert str(Q.is_false) == \"Q.is_false\"\n    assert str(Q.is_real(x)) == \"Q.is_real(x)\"\n    assert str(Q.is_positive(x)) == \"Q.is_positive(x)\"\n    assert str(Q.is_integer(x)) == \"Q.is_integer(x)\"\n"], "sample_1085": ["compilation error", "def test_Float_hash():\n    x = Float(1.1, 10)\n    y = Float(1.1, 10)\n    assert hash(x) == hash(y)\n    assert x == y\n", "def test_issue_17615():\n    assert unchanged(Integer, 1, evaluate=False) == Integer(1)\n    assert unchanged(Integer, -1, evaluate=False) == Integer(-1)\n    assert unchanged(Float, 1.1, evaluate=False) == Float(1.1)\n    assert unchanged(Float, '-1.1', evaluate=False) == Float(-1.1)\n    assert unchanged(Rational, 1, 2, evaluate=False) == Rational(1, 2)\n    assert unchanged(Rational, -1, 2, evaluate=False) == Rational(-1, 2)\n", "def test_mpf_normlize():\n    assert mpf_norm((0, 0, 0, 0), 53) == fzero\n    assert mpf_norm((0, 0, 0, 53), 53) == (0, 0, 0, 53)\n    assert Float._new((0, 0, 0, 0), 53) == S.Zero\n    assert Float._new((0, 0, 0, 53), 53) == S.NaN\n", "def test_mpf_norm_rounding():\n    # Test mpf_norm with different rounding modes\n    from mpmath.libmp import round_nearest, round_up, round_down, round_floor, round_ceiling\n\n    mpf = (0, 12345678901234567890, -50, 53)\n    assert mpf_norm(mpf, 53, round_nearest) == mpf\n    assert mpf_norm(mpf, 52, round_nearest) == (0, 12345678901234567900, -50, 52)\n    assert mpf_norm(mpf, 53, round_up) == (0, 12345678901234567900, -50, 53)\n    assert mpf_norm(mpf, 53, round_down) == mpf\n    assert mpf_norm(mpf, 53, round_floor) == (0, 12345678901234567900, -50, 53)\n    assert mpf_norm(mpf, 53, round_ceiling) == mpf\n"], "sample_1087": ["def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n"], "sample_1088": ["def test_viete():\n    r1, r2, r3 = symbols('r1,r2,r3')\n    assert viete(x**2 + a*x + b, [r1, r2], x) == [(r1 + r2, -a), (r1*r2, b)]\n    assert viete(x**3 + a*x**2 + b*x + c, [r1, r2, r3], x) == [\n        (r1 + r2 + r3, -a), (r1*r2 + r1*r3 + r2*r3, b), (r1*r2*r3, -c)]\n    raises(ValueError, lambda: viete(1, x))\n    raises(ValueError, lambda: viete(x**2 + 1, [r1, r2, r3], x))\n    raises(MultivariatePolynomialError, lambda: viete(x**2 + y**2, x))\n", "def test_viete():\n    # Test that Vieta's formulas work for polynomials of different degrees\n    assert viete(x**2 + 2*x + 1) == [(x, -2), (1, 1)]\n    assert viete(x**3 + 3*x**2 + 2*x + 1) == [(x + y, -3), (x*y, 2), (1, 1)]\n    assert viete(x**4 + 4*x**3 + 6*x**2 + 4*x + 1) == [(x + y + z, -4), (x*y + x*z + y*z, 6), (x*y*z, -4), (1, 1)]\n\n    # Test that Vieta's formulas work for polynomials with different variables\n    assert viete(x**2 + y*x + 1, x) == [(x, -y), (1, 1)]\n    assert viete(y**3 + x*y**2 + 2*y + 1, y) == [(y, -x), (1, 2), (1, 1)]\n\n    # Test that Vieta's formulas work for polynomials with different roots\n    assert viete((x - 1)*(x + 1), roots=[1, -1]) == [(1 - 1, -(-1 - 1)), (1*(-1), 1)]\n    assert viete((x - 2)*(x + 3), roots=[2, -3]) == [(2 - 3, -(-3 - 2)), (2*(-3), 1)]\n\n    # Test that Viete's formulas raise errors for invalid input\n    raises(ValueError, lambda: viete(x**2 + 2*x + 1, roots=[1]))\n    raises(MultivariatePolynomialError, lambda: viete(x**2 + y**2 + 2*x + 1))\n", "def test_viete():\n    assert viete(x**2 + a*x + b, [r1, r2], x) == [(r1 + r2, -a), (r1*r2, b)]\n    assert viete(x**3 + a*x**2 + b*x + c, [r1, r2, r3], x) == [\n        (r1 + r2 + r3, -a),\n        (r1*r2 + r1*r3 + r2*r3, b),\n        (r1*r2*r3, -c),\n    ]\n\n    raises(MultivariatePolynomialError, lambda: viete(x*y + 1, [r1, r2]))\n    raises(ValueError, lambda: viete(1, [r1, r2]))\n    raises(ValueError, lambda: viete(x + 1, [r1, r2, r3]))\n", "def test_viete():\n    assert viete(x**2 + a*x + b, [1, 2], x) == [(1 + 2, -a), (1*2, b)]\n    assert viete(x**3 + a*x**2 + b*x + c, [1, 2, 3], x) == \\\n        [(1 + 2 + 3, -a), (1*2 + 1*3 + 2*3, b), (1*2*3, -c)]\n\n    raises(ValueError, lambda: viete(0, [], x))\n    raises(ValueError, lambda: viete(x, [1, 2], x))\n\n    raises(ComputationFailed, lambda: viete(x + y, [1, 2], x))\n    raises(MultivariatePolynomialError, lambda: viete(x + y, [1], x))\n", "def test_viete():\n    x, a, b, c, r1, r2 = symbols('x,a:c,r1:3')\n\n    assert viete(a*x**2 + b*x + c, [r1, r2], x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n    assert viete(x**2 + 6*x + 9, [r1, r2], x) == [(r1 + r2, -6), (r1*r2, 9)]\n    assert viete(x**2 + 2*x + 1, [r1, r2], x) == [(r1 + r2, -2), (r1*r2, 1)]\n\n    raises(ValueError, lambda: viete(x**2 + 2*x + 1, [r1, r2, r1], x))\n    raises(ValueError, lambda: viete(x**2 + 2*x + 1, [r1], x))\n    raises(MultivariatePolynomialError, lambda: viete(x**2 + 2*y*x + 1, [r1, r2], x))\n    raises(MultivariatePolynomialError, lambda: viete(x**2 + 2*y*x + 1, [r1, r2], y))\n"], "sample_1089": ["def test_mask_nc():\n    x, y = symbols('x,y', commutative=False)\n    A, B = symbols('A,B', commutative=False)\n\n    eq, rep, syms = _mask_nc(x*y - y*x, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): x*y - y*x} and not syms\n\n    eq, rep, syms = _mask_nc(x*y + y*x, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): x*y + y*x} and not syms\n\n    eq, rep, syms = _mask_nc(x*y - y*x + 1, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): x*y - y*x} and not syms\n\n    eq, rep, syms = _mask_nc(x*y + y*x + 1, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): x*y + y*x} and not syms\n\n    eq, rep, syms = _mask_nc(x*y - y*x + x - y, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): x*y - y*x} and syms == [x, y]\n\n    eq, rep, syms = _mask_nc(x*y + y*x + x - y, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): x*y + y*x} and syms == [x, y]\n\n    eq, rep, syms = _mask_nc(A*x*y*B - B*y*x*A, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): A*x*y*B - B*y*x*A} and not syms\n\n    eq, rep, syms = _mask_nc(A*x*y*B + B*y*x*A, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): A*x*y*B + B*y*x*A} and not syms\n\n    eq, rep, syms = _mask_nc(A*x*y*B - B*y*x*A + x - y, 'nc')\n    assert rep == {Symbol('nc0', commutative=False): A*x*y*B - B*y*x*A} and syms == [x, y, Symbol('nc0', comm", "def test_mask_nc():\n    from sympy.physics.secondquant import Commutator, NO, F, Fd\n    from sympy import symbols, Mul\n\n    A, B, C = symbols('A,B,C', commutative=False)\n    # one nc-symbol\n    expr, rep, nc_symbols = _mask_nc(A**2 - x**2)\n    assert expr == expr.simplify()\n    assert expr == expr.expand()\n\n    assert expr == _mask_nc(expr)[0]\n    assert rep == _mask_nc(expr)[1]\n\n    assert expr == _mask_nc(expr, 'd')[0]\n    assert rep == _mask_nc(expr, 'd')[1]\n\n    # multiple nc-symbols\n    expr, rep, nc_symbols = _mask_nc(A**2 - B**2)\n    assert nc_symbols == [A, B]\n    assert rep == {}\n    assert expr == A**2 - B**2\n\n    # nc-object with nc-symbols but no others outside of it\n    expr, rep, nc_symbols = _mask_nc(1 + x*Commutator(A, B))\n    assert nc_symbols == []\n    assert rep == {_mask_nc(expr)[0].args[1]: Commutator(A, B)}\n    assert expr == 1 + x*_mask_nc(expr)[0].args[1]\n\n    # multiple nc-objects and nc-symbols\n    expr, rep, nc_symbols = _mask_nc(A*Commutator(A, B) + B*Commutator(A, C))\n    assert sorted(nc_symbols, key=lambda x: x.name) == sorted([_mask_nc(expr)[0].args[0].args[1],\n                                                              _mask_nc(expr)[0].args[1].args[0]], key=lambda x: x.name)\n    assert rep == {_mask_nc(expr)[0].args[0].args[1]: Commutator(A, B),\n                  _mask_nc(expr)[0].args[1].args[0]: Commutator(A, C)}\n\n    assert sorted(expr.args, key=lambda x: str(x)) == sorted((A*_mask_nc(expr)[0].args[0].args[1],\n                                                             B*_mask_nc(expr)[0].args[1].args[0]), key=lambda x: str(x))\n\n    # multiple nc-objects\n    expr, rep, nc_symbols = _mask_nc(x*Commutator(A, B) + x*", "def test_mask_nc():\n    from sympy.physics.secondquant import Commutator, NO, F, Fd\n    from sympy.abc import x, y\n    A, B, C = symbols('A,B,C', commutative=False)\n\n    # One nc-symbol:\n    e, r, s = _mask_nc(A**2 - x**2)\n    assert e == _mask_nc(A**2 - x**2, 'd')[0]\n    assert e == _d0**2 - x**2\n    assert r == {_d0: A}\n    assert s == []\n\n    # Multiple nc-symbols:\n    e, r, s = _mask_nc(A**2 - B**2)\n    assert e == A**2 - B**2\n    assert r == {}\n    assert s == [A, B]\n\n    # An nc-object with nc-symbols but no others outside of it:\n    e, r, s = _mask_nc(1 + x*Commutator(A, B))\n    assert e == 1 + x*_d0\n    assert r == {_d0: Commutator(A, B)}\n    assert s == []\n\n    # Multiple nc-objects:\n    eq = x*Commutator(A, B) + x*Commutator(A, C)*Commutator(A, B)\n    e, r, s = _mask_nc(eq)\n    assert e == x*_d0 + x*_d1*_d0\n    assert r == {_d0: Commutator(A, B), _d1: Commutator(A, C)}\n    assert s == [_d0, _d1]\n\n    # Multiple nc-objects and nc-symbols:\n    eq = A*Commutator(A, B) + B*Commutator(A, C)\n    e, r, s = _mask_nc(eq)\n    assert e == A*_d0 + B*_d1\n    assert r == {_d0: Commutator(A, B), _d1: Commutator(A, C)}\n    assert s == [_d0, _d1, A, B]\n\n    # An nc-object with no nc-symbols:\n    class Foo(Basic): is_commutative = False\n    e, r, s = _mask_nc(x + Foo(), 'd')\n    assert e == x + _d0\n    assert r == {_d0", "def test_mask_nc():\n    a, b = symbols('a b', commutative=False)\n    x, y = symbols('x y', commutative=True)\n    eq = a*b + b*a\n    masked, d, nc = _mask_nc(eq)\n    assert nc == []\n    assert d == {}\n    assert masked == eq\n\n    eq = a*b - b*a\n    masked, d, nc = _mask_nc(eq)\n    assert nc == []\n    assert d == {}\n    assert masked == eq\n\n    eq = a + x*y\n    masked, d, nc = _mask_nc(eq)\n    assert nc == []\n    assert d == {}\n    assert masked == eq\n\n    eq = x*y + y*x\n    masked, d, nc = _mask_nc(eq)\n    assert nc == []\n    assert d == {}\n    assert masked == 2*x*y\n\n    eq = sin(a) + sin(b)\n    masked, d, nc = _mask_nc(eq)\n    assert nc == []\n    assert d == {Dummy(commutative=False): sin(a), Dummy(commutative=False): sin(b)}\n    assert masked == Dummy(commutative=False) + Dummy(commutative=False)\n\n    eq = x + a\n    masked, d, nc = _mask_nc(eq)\n    assert nc == []\n    assert d == {Dummy(commutative=False): a}\n    assert masked == x + Dummy(commutative=False)\n", "def test_Factors_special_handling():\n    # check special handling of -1 and I in Factors\n    f = Factors(-2*x*y)\n    assert f.factors == {S.NegativeOne: 1, S(2): 1, x: 1, y: 1}\n    f = Factors(2*I*x*y)\n    assert f.factors == {I: 1, S(2): 1, x: 1, y: 1}\n    f = Factors(-I*x*y)\n    assert f.factors == {S.NegativeOne: 1, I: 1, x: 1, y: 1}\n    f = Factors(-2*I*x*y)\n    assert f.factors == {S.NegativeOne: 1, I: 1, S(2): 1, x: 1, y: 1}\n    f = Factors(-(2*I)*x*y)\n    assert f.factors == {S.NegativeOne: 1, I: 1, S(2): 1, x: 1, y: 1}\n"], "sample_1091": ["def test_relational_as_set_with_symbols():\n    A = Eq(x + y, 1)\n    B = Eq(2*x + 2*y, 2)\n    assert A.as_set() == B.as_set()\n\n    A = Eq(x + y, 1)\n    B = Eq(x + y, 2)\n    assert A.as_set() != B.as_set()\n\n    A = Eq(x, 1)\n    assert A.as_set() == FiniteSet(1)\n\n    A = Eq(x, y)\n    assert A.as_set() == ConditionSet(x, Eq(x, y), Reals)\n\n    A = Ne(x, y)\n    assert A.as_set() == ConditionSet(x, Ne(x, y), Reals)\n", "def test_issue_16587():\n    with warns_deprecated_sympy():\n        assert Eq(x) == Eq(x, 0)\n        assert Eq(x).args == (x, 0)\n        assert Eq(x).lhs == x\n        assert Eq(x).rhs == 0\n", "def test_reversed_reversedsign_reversed_reversedsign():\n    for f in (Eq, Ne, Ge, Gt, Le, Lt):\n        assert f(x, y).reversed.reversedsign.reversed.reversedsign == f(x, y)\n\n    for f in (Eq, Ne, Ge, Gt, Le, Lt):\n        assert f(-x, y).reversed.reversedsign.reversed.reversedsign == f(-x, y)\n\n    for f in (Eq, Ne, Ge, Gt, Le, Lt):\n        assert f(x, -y).reversed.reversedsign.reversed.reversedsign == f(x, -y)\n\n    for f in (Eq, Ne, Ge, Gt, Le, Lt):\n        assert f(-x, -y).reversed.reversedsign.reversed.reversedsign == f(-x, -y)\n", "def test_issue_16587():\n    with warns_deprecated_sympy():\n        assert Eq(x) == Eq(x, 0)\n", "def test_issue_19762():\n    n = symbols('n')\n    assert (n > 0).simplify() == (n > 0)\n"], "sample_1090": ["def test_mul():\n    with evaluate(False):\n        p = oo * oo\n        assert isinstance(p, Mul) and p.args == (oo, oo)\n        p = 5 * oo\n        assert isinstance(p, Mul) and p.args == (5, oo)\n        p = oo * 5\n        assert isinstance(p, Mul) and p.args == (5, oo)\n        p = -oo * 5\n        assert isinstance(p, Mul) and p.args == (-5, oo)\n        p = -5 * oo\n        assert isinstance(p, Mul) and p.args == (-5, oo)\n\n    with evaluate(False):\n        assert S.One * 1 == Mul(1, 1)\n        assert 1 * S.One == Mul(1, 1)\n\n        assert S(4) * 3 == Mul(4, 3)\n        assert 3 * S(4) == Mul(4, 3)\n\n        assert S(2) * 4 == Mul(2, 4)\n        assert 4 * S(2) == Mul(2, 4)\n\n        assert S(6) / 3 * 3 == Mul(2, 3)\n        assert S.One / 3 * 6 * 3 == Mul(2, 3)\n\n        assert 9 ** S(2) * 2 == Mul(81, 2)\n        assert S(2) ** 9 * 2 == Mul(512, 2)\n\n        assert S(2) / 2 * 2 == Mul(1, 2)\n        assert S.One / 2 * 2 * 2 == Mul(1, 2)\n\n        assert S(2) / 3 * 3 + 1 == Add(2, 1)\n        assert 1 + S(2) / 3 * 3 == Add(1, 2)\n\n        assert S(4) / 7 * 7 - 3 == Add(4, -3)\n        assert -3 + S(4) / 7 * 7 == Add(-3, 4)\n\n        assert S(2) / 4 * 4 * 4 == Mul(2, 4)\n        assert 4 * (S(2) / 4) * 4 == Mul(2, 4)\n\n        assert S(6) / 3 * 3 == Mul(", "def test_Pow_evaluate():\n    with evaluate(False):\n        p = oo ** 2\n        assert isinstance(p, Pow) and p.args == (oo, 2)\n        p = 2 ** oo\n        assert isinstance(p, Pow) and p.args == (2, oo)\n        p = oo ** -2\n        assert isinstance(p, Pow) and p.args == (oo, -2)\n        p = 2 ** -oo\n        assert isinstance(p, Pow) and p.args == (2, -oo)\n        p = oo ** 0\n        assert isinstance(p, Pow) and p.args == (oo, 0)\n        p = 0 ** oo\n        assert isinstance(p, Pow) and p.args == (0, oo)\n        p = 0 ** -oo\n        assert isinstance(p, Pow) and p.args == (0, -oo)\n        p = -oo ** 2\n        assert isinstance(p, Pow) and p.args == (-oo, 2)\n        p = 2 ** -oo\n        assert isinstance(p, Pow) and p.args == (2, -oo)\n        p = (-2) ** oo\n        assert isinstance(p, Pow) and p.args == (-2, oo)\n        p = (-2) ** -oo\n        assert isinstance(p, Pow) and p.args == (-2, -oo)\n        p = 2 ** nan\n        assert isinstance(p, Pow) and p.args == (2, nan)\n        p = nan ** 2\n        assert isinstance(p, Pow) and p.args == (nan, 2)\n", "def test_complex():\n    with evaluate(False):\n        p = oo * S.ImaginaryUnit\n        assert isinstance(p, Mul) and p.args == (oo, S.ImaginaryUnit)\n        p = S.ImaginaryUnit * oo\n        assert isinstance(p, Mul) and p.args == (oo, S.ImaginaryUnit)\n        p = -oo * S.ImaginaryUnit\n        assert isinstance(p, Mul) and p.args == (-oo, S.ImaginaryUnit)\n        p = S.ImaginaryUnit * -oo\n        assert isinstance(p, Mul) and p.args == (-oo, S.ImaginaryUnit)\n        p = nan + 5\n        assert isinstance(p, Add) and p.args == (nan, 5)\n        p = nan * 5\n        assert isinstance(p, Mul) and p.args == (nan, 5)\n        p = nan - 5\n        assert isinstance(p, Add) and p.args == (nan, -5)\n        p = nan / 5\n        assert isinstance(p, Mul) and p.args == (nan, S.One / 5)\n        p = nan ** 5\n        assert isinstance(p, Pow) and p.args == (nan, 5)\n", "def test_evaluate_is_Not_Global():\n    # Check evaluate(False) does not affect the global evaluate setting\n    with evaluate(False):\n        assert (x + x).args == (x, x)\n    assert (x + x).args == (2*x,)\n", "def test_div():\n    with evaluate(False):\n        p = oo / oo\n        assert isinstance(p, Mul) and p.args == (oo, 1/oo)\n        p = -oo / oo\n        assert isinstance(p, Mul) and p.args == (-oo, 1/oo)\n        p = oo / -oo\n        assert isinstance(p, Mul) and p.args == (oo, 1/-oo)\n        p = oo / 5\n        assert isinstance(p, Mul) and p.args == (oo, 1/5)\n        p = 5 / oo\n        assert isinstance(p, Mul) and p.args == (5, 1/oo)\n        p = -5 / oo\n        assert isinstance(p, Mul) and p.args == (-5, 1/oo)\n        p = nan / 5\n        assert isinstance(p, Mul) and p.args == (nan, 1/5)\n        p = 5 / nan\n        assert isinstance(p, Mul) and p.args == (5, 1/nan)\n\n    with evaluate(True):\n        p = oo / oo\n        assert p is nan\n        p = -oo / oo\n        assert p is nan\n        p = oo / -oo\n        assert p is nan\n        p = oo / 5\n        assert p is oo\n        p = 5 / oo\n        assert p is S.Zero\n        p = -5 / oo\n        assert p is S.Zero\n        p = nan / 5\n        assert p is nan\n        p = 5 / nan\n        assert p is nan\n"], "sample_1092": ["def test_cse_main_postprocessing():\n    x, y = symbols('x y')\n    eq = (x + 1 + exp((x + 1)/(y + 1)) + cos(y + 1))\n    res = cse(eq, postprocess=cse_main.cse_separate)\n    assert len(res[0]) == 3\n    assert res[0][0][0].func == symbols('x0').func\n    assert res[0][0][1] == y + 1\n    assert res[0][1][0].func == symbols('x').func\n    assert res[0][1][1] == x + 1\n    assert res[0][2][0].func == symbols('x1').func\n    assert res[0][2][1] == x + 1\n    assert res[1][0] == x1 + exp(x1/x0) + cos(x0)\n", "def test_issue_reps_toposort():\n    x, y, z, w = symbols('x y z w')\n    reps = [(x, y + z), (y, w), (z, w), (w, 2)]\n    reps = cse_main.reps_toposort(reps)\n    assert reps == [(w, 2), (z, w), (y, w), (x, y + z)]\n", "def test_issue_19847():\n    a = Symbol('a')\n    b = Symbol('b')\n    c = Symbol('c')\n    d = Symbol('d')\n\n    A = Matrix([[a, b], [c, d]])\n    B = Matrix([[a, b], [c, d]])\n\n    expr = A * B\n    cse_expr = cse(expr)\n\n    assert cse_expr[0] == [(x0, Matrix([[a, b], [c, d]]))]\n    assert cse_expr[1][0] == x0**2\n", "def test_issue_18340():\n    from sympy import Eq, Function\n    f = Function('f')\n    eqs = [Eq(f(x, y), 2*x + 3*y), Eq(f(x, y), x + 2*y)]\n    cse_eqs = cse(eqs, postprocess=cse_main.cse_separate)\n    assert cse_eqs == ([], eqs)\n", "def test_issue_20061():\n    from sympy.simplify.cse_main import cse_separate\n    m, n, a, b = symbols('m n a b')\n    M = MatrixSymbol('M', m, n)\n    N = MatrixSymbol('N', n, m)\n    expr = [a * M * N, b * M * N]\n    subst, reduced = cse(expr)\n    assert subst == [(x0, M*N)]\n    assert reduced == [a*x0, b*x0]\n    subst, reduced = cse(expr, postprocess=cse_separate)\n    assert subst == [(x0, M*N), (x1, a*x0), (x2, b*x0)]\n    assert reduced == [x1, x2]\n"], "sample_1093": ["def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    smat = SparseMatrix(2, 5, {(0, 1): 3})\n    assert p.doprint(smat) == 'sympy.Matrix([[0, 3, 0, 0, 0], [0, 0, 0, 0, 0]])'\n    assert 'sympy' in p.module_imports\n\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.Exp1) == 'sympy.E'\n", "def test_issue_Symbol_in_reserved_words_list():\n    import warnings\n\n    p = IndexedBase('if')\n\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', UserWarning)\n        assert pycode(p[0]) == 'if_[0]'\n", "def test_SymPyPrinter():\n    from sympy.functions import (acos, acosh, asin, asinh, atan, atanh, cos,\n        cosh, erf, erfc, exp, exp_polar, factorial, gamma, log, loggamma,\n        sin, sinh, sqrt, tan, tanh)\n    p = SymPyPrinter()\n\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(x**0.5) == 'sympy.sqrt(x)'\n    assert p.doprint(x**-0.5) == '1/sympy.sqrt(x)'\n    assert p.doprint(x**0.2) == 'x**(1/5)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n    assert p.doprint(acosh(x)) == 'sympy.acosh(x)'\n    assert p.doprint(asin(x)) == 'sympy.asin(x)'\n    assert p.doprint(asinh(x)) == 'sympy.asinh(x)'\n    assert p.doprint(atan(x)) == 'sympy.atan(x)'\n    assert p.doprint(atanh(x)) == 'sympy.atanh(x)'\n    assert p.doprint(cos(x)) == 'sympy.cos(x)'\n    assert p.doprint(cosh(x)) == 'sympy.cosh(x)'\n    assert p.doprint(erf(x)) == 'sympy.erf(x)'\n    assert p.doprint(erfc(x)) == 'sympy.erfc(x)'\n    assert p.doprint(exp(x)) == 'sympy.exp(x)'\n    assert p.doprint(factorial(x)) == 'sympy.factorial(x)'\n    assert p.doprint(gamma(x)) == 'sympy.gamma(x)'\n    assert p.doprint(log(x)) == 'sympy.log(x)'\n    assert p.doprint(loggamma(x)) == 'sympy.loggamma(x)'\n    assert p.doprint(sin(x)) == 'sympy.sin(x)'\n    assert p.doprint(sinh(x)) == 'sympy.sinh(x)'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(tan(x)) == 'sympy.tan(x)'\n    assert p.d", "def test_issue_SymPyPrinter_print():\n    from sympy import Symbol, Function\n\n    x = Symbol('x')\n\n    class f(Function):\n        pass\n\n    expr = f(x)\n\n    p = SymPyPrinter()\n    assert p.doprint(expr) == 'sympy.Function('f')(x)'\n", "def test_SymPyPrinter():\n    prntr = SymPyPrinter()\n\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.doprint(Mod(x, 2)) == 'sympy.Mod(x, 2)'\n    assert prntr.doprint(And(x, y)) == 'sympy.And(x, y)'\n    assert prntr.doprint(Or(x, y)) == 'sympy.Or(x, y)'\n\n    assert prntr.doprint(pi) == 'sympy.pi'\n    assert prntr.doprint(x**Rational(1, 2)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n\n    assert prntr.doprint(acos(x)) == 'sympy.acos(x)'\n    assert prntr.doprint(Assignment(x, 2)) == 'x = 2'\n    assert prntr.doprint(Piecewise((1, Eq(x, 0)),\n                        (2, x>6))) == 'sympy.Piecewise((1, sympy.Eq(x, 0)), (2, x > 6))'\n    assert prntr.doprint(sign(x)) == 'sympy.sign(x)'\n"], "sample_1094": ["def test_replace():\n    x, y, z = symbols('x y z')\n    f = x + y\n    assert f.replace(x, z) == z + y\n    assert f.replace(x + y, z) == z\n    assert f.replace(x, z, map=True) == (z + y, {x: z})\n    assert f.replace(x + y, z, map=True) == (z, {x + y: z})\n", "def test_replace():\n    x, y, z = symbols('x y z')\n    f = x + y + z\n    assert f.replace(x, y) == y + y + z\n    assert f.replace((x, y), (y, x)) == y + x + z\n    assert f.replace((x, y), (y, z)) == y + z + z\n    assert f.replace(lambda x: x.is_Symbol, lambda x: x**2) == x**2 + y**2 + z**2\n    assert f.replace({x: y, y: x}, simultaneous=True) == y + x + z\n    assert f.replace({x: y, y: x}, simultaneous=False) == x + x + z\n    assert f.replace({x: y, y: x, z: y}, simultaneous=True) == y + x + y\n    assert f.replace({x: y, y: x, z: y}, simultaneous=False) == x + x + y\n", "def test_compare_pretty():\n    x, y, z = symbols('x y z')\n    assert Basic._compare_pretty(1, x) == -1\n    assert Basic._compare_pretty(x, 1) == 1\n    assert Basic._compare_pretty(x, x*y) == -1\n    assert Basic._compare_pretty(x*y, x) == 1\n    assert Basic._compare_pretty(x, x) == 0\n", "def test_compare_pretty():\n    from sympy import sqrt\n    x = Symbol('x')\n    assert Basic._compare_pretty(sqrt(x), sqrt(2)) == -1\n    assert Basic._compare_pretty(sqrt(x), sqrt(x + 1)) == -1\n    assert Basic._compare_pretty(sqrt(x + 1), sqrt(x)) == 1\n    assert Basic._compare_pretty(sqrt(x), x) == -1\n    assert Basic._compare_pretty(x, sqrt(x)) == 1\n", "def test_class_key():\n    assert Basic.class_key() == (5, 0, 'Basic')\n    assert Atom.class_key() == (2, 0, 'Atom')\n"], "sample_1095": ["def test_AppliedPermutation():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    ap = AppliedPermutation(p, x)\n\n    assert ap.subs(x, 0) == 1\n    assert ap.subs(x, 1) == 2\n    assert ap.subs(x, 2) == 0\n\n    assert ap.doit() == ap\n    assert apdummy = Symbol('_apdummy')\n\n    assert ap.subs(x, apdummy).subs(apdummy, 0) == 1\n    assert ap.subs(x, apdummy).subs(apdummy, 1) == 2\n    assert ap.subs(x, apdummy).subs(apdummy, 2) == 0\n\n    assert str(ap) == 'AppliedPermutation((0 1 2), x)'\n    assert repr(ap) == 'AppliedPermutation((0 1 2), x)'\n", "def test_applied_permutation():\n    x = Symbol('x', integer=True)\n    p = Permutation(0, 1, 2)\n    ap = AppliedPermutation(p, x)\n    assert repr(ap) == 'AppliedPermutation((0 1 2), x)'\n    assert str(ap) == 'AppliedPermutation((0 1 2), x)'\n    assert ap.permutation == p\n    assert ap.expr == x\n    assert ap.subs(x, 0) == 1\n    assert ap.subs(x, 1) == 2\n    assert ap.subs(x, 2) == 0\n    assert ap.subs(x, 3) == 3\n", "def test_permutation_apply_edge_cases():\n    p = Permutation(0, 1, 2)\n    raises(TypeError, lambda: p.apply(-1))\n    raises(TypeError, lambda: p.apply(3))\n    raises(NotImplementedError, lambda: p.apply(Integer(1)/2))\n    raises(NotImplementedError, lambda: p.apply(S.Infinity))\n    raises(NotImplementedError, lambda: p.apply(S.NegativeInfinity))\n", "def test_AppliedPermutation():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    ap = AppliedPermutation(p, x)\n\n    assert ap.subs(x, 0) == 1\n    assert ap.subs(x, 1) == 2\n    assert ap.subs(x, 2) == 0\n    assert ap.subs(x, 3) == AppliedPermutation(p, 3)\n\n    raises(NotImplementedError, lambda: AppliedPermutation(p, 3.5))\n    raises(NotImplementedError, lambda: AppliedPermutation(p, -1))\n", "def test_AppliedPermutation():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    ap = AppliedPermutation(p, x)\n    assert ap.subs(x, 0) == 1\n    assert ap.subs(x, 1) == 2\n    assert ap.subs(x, 2) == 0\n    assert ap.subs(x, 3) == 3\n    assert ap.subs(x, x) == ap\n"], "sample_1096": ["def test_Indexed_free_symbols():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, j)\n    assert A.free_symbols == {A, A.base.label, i, j}\n    assert A.expr_free_symbols == {A}\n    assert A.base.free_symbols == {A.base.label}\n", "def test_IndexedBase_derivative():\n    i, j = symbols('i j', integer=True)\n    x, y = symbols('x y')\n    A = IndexedBase('A')\n    assert A[i, j].diff(x) == 0\n    assert A[i, j].diff(y) == 0\n    assert A[i, j].diff(A[i, j]) == 1\n    assert A[i, j].diff(A[i, i]) == KroneckerDelta(i, j)\n    assert A[i, j].diff(A[j, i]) == KroneckerDelta(i, j)\n    assert A[i, j].diff(A[i, k]) == KroneckerDelta(j, k)\n    assert A[i, j].diff(A[k, j]) == KroneckerDelta(i, k)\n    assert A[i, j].diff(A[k, l]) == KroneckerDelta(i, k)*KroneckerDelta(j, l)\n", "def test_IndexedBase_str():\n    A = IndexedBase(\"A\")\n    assert str(A) == \"A\"\n    A = IndexedBase(\"A\", shape=(1, 2, 3))\n    assert str(A) == \"A\"\n    i, j, k = symbols(\"i j k\", integer=True)\n    assert str(A[i, j, k]) == \"A[i, j, k]\"\n    assert str(A[1, 2, 3]) == \"A[1, 2, 3]\"\n", "def test_IndexedBase_is_commutative():\n    A = IndexedBase('A', shape=(1, 2))\n    B = IndexedBase('B', shape=(2, 1))\n    assert A.is_commutative\n    assert B.is_commutative\n    assert (A[0, 0] * B[0, 0]).is_commutative\n", "def test_IndexedBase_with_Dummy():\n    i = Dummy('i', integer=True)\n    A = IndexedBase('A')\n    assert A[i].base == A\n    assert A[i].indices == (i,)\n"], "sample_1097": ["def test_deblock_non_blockmatrix():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    X = BlockMatrix([[A, B], [C, D]])\n    assert deblock(A) == A\n    assert deblock(X) == X\n", "def test_BlockMatrixEquals():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, k)\n    D = MatrixSymbol('D', m, k)\n    X = BlockMatrix([[A, C]])\n    Y = BlockMatrix([[B, D]])\n\n    assert X.equals(Y) == False\n    assert X.equals(BlockMatrix([[A, C]]))\n    assert X.equals(BlockMatrix([[B, C]])) == False\n    assert X.equals(BlockMatrix([[A, D]])) == False\n", "def test_blockcut_real_matrix():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (2, 2))\n    assert B.blocks[0, 1].as_real_imag()[0] == Matrix([[2, 3]])\n", "def test_blockcut_bounds():\n    A = MatrixSymbol('A', 4, 4)\n    B = blockcut(A, (1, 1, 2), (2, 2))\n    assert B.blocks[0, 0].shape == (1, 2)\n    assert B.blocks[0, 1].shape == (1, 2)\n    assert B.blocks[1, 0].shape == (1, 2)\n    assert B.blocks[1, 1].shape == (1, 2)\n    assert B.blocks[2, 0].shape == (2, 2)\n    assert B.blocks[2, 1].shape == (2, 2)\n", "def test_deblock_2x2():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n\n    B = BlockMatrix([[A, B], [C, D]])\n\n    BB = BlockMatrix([[B]])\n\n    assert deblock(BB).equals(B)\n"], "sample_1098": ["def test_meijerg_unpolarify():\n    from sympy import exp_polar\n    a = exp_polar(2*pi*I)*x\n    b = x\n    assert meijerg([], [], [0], [], a).argument == b\n    assert meijerg([], [], [-2, 0], [], a).argument == a\n    assert meijerg([], [], [0, -2], [], a).argument == a\n    assert meijerg([], [], [], [0], a).argument == a\n    assert meijerg([], [], [], [-2, 0], a).argument == a\n    assert meijerg([], [], [], [0, -2], a).argument == a\n    assert meijerg([], [], [0, -2, 0], [], a).argument == a\n    assert meijerg([], [], [], [0, -2, 0], a).argument == a\n\n    assert meijerg([], [1, 1], [0], [], a).argument == b\n    assert meijerg([], [1, 1], [], [0], a).argument == b\n    assert meijerg([1, 1], [1, 1], [0, -2, 0], [], a).argument == a\n    assert meijerg([1, 1], [1, 1], [], [0, -2, 0], a).argument == a\n", "def test_hyper_nseries():\n    z = symbols('z')\n    assert hyper((1, 2), (1,), z).nseries(z, n=10) == \\\n           1 + z + z**2 + z**3 + z**4 + z**5 + z**6 + z**7 + z**8 + z**9 + O(z**10)\n    assert hyper((1, 2), (1, 3), z).nseries(z, n=10) == \\\n           1 + z + z**2/2 + z**3/6 + z**4/24 + z**5/120 + z**6/720 + z**7/5040 + z**8/40320 + z**9/362880 + O(z**10)\n", "def test_hyper_eval():\n    from sympy import hyperexpand\n    from sympy.abc import a, b, c\n    a1, b1, c1 = randcplx(), randcplx(), randcplx() + 5\n    assert hyperexpand(hyper([a, b], [c], 1)) == \\\n        gamma(c)*gamma(-a - b + c)/(gamma(-a + c)*gamma(-b + c))\n    assert abs(hyperexpand(hyper([a1, b1], [c1], 1)).n()\n               - hyper([a1, b1], [c1], 1).n()) < 1e-10\n\n    assert hyperexpand(hyper([], [], z)) == exp(z)\n    assert hyperexpand(hyper([1, 2, 3], [], z)) == hyper([1, 2, 3], [], z)\n    assert hyperexpand(meijerg([[1, 1], []], [[1], [0]], z)) == log(z + 1)\n    assert hyperexpand(meijerg([[1, 1], []], [[], []], z)) == \\\n        meijerg([[1, 1], []], [[], []], z)\n", "def test_meijerg_unpolarify():\n    from sympy import exp_polar\n    a = exp_polar(2*pi*I)*x\n    b = x\n    assert meijerg([0], [], [], [], a).argument == b\n    assert meijerg([], [0], [], [], a).argument == b\n    assert meijerg([], [], [0], [], a).argument == b\n    assert meijerg([], [], [], [0], a).argument == b\n    assert meijerg([], [1], [], [], a).argument == a\n    assert meijerg([], [], [1], [], a).argument == a\n    assert meijerg([], [], [], [1], a).argument == a\n", "def test_hyper_eval():\n    from sympy import hyperexpand\n    from sympy.abc import a, b, c\n    assert hyper((a, b), (c,), 1).eval() == gamma(c)*gamma(-a - b + c)/(gamma(-a + c)*gamma(-b + c))\n    assert hyperexpand(hyper([1, 1], [2], z)) == -log(1 - z)\n    assert hyper((1,), (Rational(4, 3), Rational(5, 3)), 1).eval() == 9*gamma(Rational(4, 3))/5\n"], "sample_1099": ["def test_eval_partial_derivative_expr2():\n\n    tau, alpha = symbols(\"tau alpha\")\n\n    # this is only some special expression\n    # tested: vector derivative\n    # tested: scalar derivative\n    # tested: tensor derivative\n    base_expr2 = H(i, j)*H(-i, -j) + A(i)*A(-i)*A(j)*A(-j) + tau**alpha*A(j)*A(-j)\n\n    tensor_derivative = PartialDerivative(base_expr2, H(k, m))._perform_derivative()\n    vector_derivative = PartialDerivative(base_expr2, A(k))._perform_derivative()\n    scalar_derivative = PartialDerivative(base_expr2, tau)._perform_derivative()\n\n    assert (tensor_derivative - (L.delta(i, -k)*L.delta(j, -m)*H(-i, -j) +\n        L.delta(-i, k)*L.delta(-j, m)*H(i, j))).expand() == 0\n\n    assert (vector_derivative - (tau**alpha*A(-k)*A(j)*A(-j) +\n        A(-k)*A(i)*A(-i)*A(j)*A(-j) +\n        2*A(i)*A(-i)*L.delta(j, -k)*A(-j) +\n        2*H(i, j)*L.metric(-i, -L_0)*L.delta(L_0, -k)*A(-j) +\n        2*H(i, j)*A(i)*L.metric(-j, -L_0)*L.delta(L_0, -k))).expand() == 0\n\n    assert scalar_derivative - alpha*1/tau*tau**alpha*A(j)*A(-j) == 0\n", "def test_doit_partial_derivative_sum_rule():\n    tau = symbols(\"tau\")\n\n    # check sum rule for D(tensor, symbol)\n    expr1aa = PartialDerivative(A(i), tau)\n\n    assert expr1aa.doit() == PartialDerivative(A(i), tau)\n\n    expr1ab = PartialDerivative(A(i) + B(i), tau)\n\n    assert (expr1ab.doit() ==\n            PartialDerivative(A(i), tau) +\n            PartialDerivative(B(i), tau))\n\n    expr1ac = PartialDerivative(A(i) + B(i) + C(i), tau)\n\n    assert (expr1ac.doit() ==\n            PartialDerivative(A(i), tau) +\n            PartialDerivative(B(i), tau) +\n            PartialDerivative(C(i), tau))\n\n    # check sum rule for D(tensor, D(j))\n    expr1ba = PartialDerivative(A(i), D(j))\n\n    assert expr1ba.doit() ==\\\n        PartialDerivative(A(i), D(j))\n    expr1bb = PartialDerivative(A(i) + B(i), D(j))\n\n    assert (expr1bb.doit() ==\n            PartialDerivative(A(i), D(j)) +\n            PartialDerivative(B(i), D(j)))\n\n    expr1bc = PartialDerivative(A(i) + B(i) + C(i), D(j))\n    assert expr1bc.doit() ==\\\n        PartialDerivative(A(i), D(j))\\\n        + PartialDerivative(B(i), D(j))\\\n        + PartialDerivative(C(i), D(j))\n\n    # check sum rule for D(tensor, H(j, k))\n    expr1ca = PartialDerivative(A(i), H(j, k))\n    assert expr1ca.doit() ==\\\n        PartialDerivative(A(i), H(j, k))\n    expr1cb = PartialDerivative(A(i) + B(i), H(j, k))\n    assert (expr1cb.doit() ==\n            PartialDerivative(A(i), H(j, k))\n            + PartialDerivative(B(i), H(j, k)))\n    expr1cc = PartialDerivative(A(i) + B(i) + C(i), H(j, k))\n    assert (expr1cc.doit() ==\n            PartialDerivative(A(i), H(j, k))\n            + PartialDerivative(B(i), H(j, k))\n            + PartialDerivative(C(i), H(j, k)))\n\n    # check sum rule for", "def test_eval_partial_derivative_expr2():\n\n    tau, alpha = symbols(\"tau alpha\")\n\n    # this is only some special expression\n    # tested: vector derivative\n    # tested: scalar derivative\n    # tested: tensor derivative\n    base_expr2 = A(i)*H(-i, j) + A(i)*A(-i)*A(j) + tau**alpha*A(j) + H(j, k)*A(-k)\n\n    tensor_derivative = PartialDerivative(base_expr2, H(k, m))._perform_derivative()\n    vector_derivative = PartialDerivative(base_expr2, A(k))._perform_derivative()\n    scalar_derivative = PartialDerivative(base_expr2, tau)._perform_derivative()\n\n    assert (tensor_derivative - (A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*L.delta(j, -m) + \n        L.delta(j, -k)*L.delta(L_0, -m)*A(-L_0))) == 0\n\n    assert (vector_derivative - (tau**alpha*L.delta(j, -k) +\n        L.delta(L_0, -k)*A(-L_0)*A(j) +\n        A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*A(j) +\n        A(L_0)*A(-L_0)*L.delta(j, -k) +\n        L.delta(L_0, -k)*H(-L_0, j) + \n        H(j, L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k))).expand() == 0\n\n    assert scalar_derivative - alpha*1/tau*tau**alpha*A(j) == 0\n", "def test_eval_partial_derivative_expr2():\n    tau, alpha = symbols(\"tau alpha\")\n\n    # this is only some special expression\n    # tested: vector derivative\n    # tested: scalar derivative\n    # tested: tensor derivative\n    base_expr2 = H(i, j)*H(-i, -j) + A(i)*A(-i)*A(j)*A(-j) + tau**alpha*A(j)*A(-j)\n\n    tensor_derivative = PartialDerivative(base_expr2, H(k, m))._perform_derivative()\n    vector_derivative = PartialDerivative(base_expr2, A(k))._perform_derivative()\n    scalar_derivative = PartialDerivative(base_expr2, tau)._perform_derivative()\n\n    assert (tensor_derivative - (L.delta(L_0, -k)*H(-L_0, -m) + L.delta(L_0, -m)*H(-L_0, -k))) == 0\n\n    assert (vector_derivative - (tau**alpha*L.delta(j, -k)*A(-j) +\n        tau**alpha*L.delta(-j, -k)*A(j) +\n        L.delta(L_0, -k)*A(-L_0)*A(j)*A(-j) +\n        A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*A(j)*A(-j) +\n        A(L_0)*A(-L_0)*L.delta(j, -k)*A(-j) +\n        A(L_0)*A(-L_0)*A(j)*L.delta(-j, -k))).expand() == 0\n\n    assert scalar_derivative - alpha*1/tau*tau**alpha*A(j)*A(-j) == 0\n", "def test_eval_partial_derivative_expr2():\n    tau, alpha, beta, gamma = symbols(\"tau alpha beta gamma\")\n    \n    base_expr2 = tau**alpha*A(i)*H(-i, j) + beta*A(i)*A(-i)*A(j) + gamma*tau**alpha*A(j)\n    \n    tensor_derivative = PartialDerivative(base_expr2, H(k, m))._perform_derivative()\n    vector_derivative = PartialDerivative(base_expr2, A(k))._perform_derivative()\n    scalar_derivative = PartialDerivative(base_expr2, tau)._perform_derivative()\n\n    assert (tensor_derivative - tau**alpha*A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*L.delta(j, -m)) == 0\n\n    assert (vector_derivative - (tau**alpha*L.delta(j, -k) + \n        L.delta(L_0, -k)*beta*A(-L_0)*A(j) +\n        tau**alpha*A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -k)*A(j) +\n        beta*A(L_0)*A(-L_0)*L.delta(j, -k) +\n        tau**alpha*L.delta(L_0, -k)*H(-L_0, j) +\n        gamma*tau**alpha*L.delta(j, -k))).expand() == 0\n\n    assert (vector_derivative.contract_metric(L.metric).contract_delta(L.delta) -\n        (tau**alpha*L.delta(j, -k) + beta*A(L_0)*A(-L_0)*L.delta(j, -k) + tau**alpha*H(-k, j) + \n         2*beta*A(j)*A(-k) + gamma*tau**alpha*L.delta(j, -k))).expand() == 0\n\n    assert scalar_derivative - alpha*1/tau*tau**alpha*A(i)*H(-i, j) - alpha*gamma*1/tau*tau**alpha*A(j) == 0\n"], "sample_1100": ["def test_issue_18671():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    expr = x**(2*n)\n    assert expr.is_always_positive\n    assert expr.is_positive is None\n    expr = x**(2*n + 1)\n    assert expr.is_always_positive is None\n    assert expr.is_positive is None\n", "def test_Pow_is_algebraic():\n    from sympy import cos, pi, sqrt\n    x = Symbol('x')\n    assert (2**x).is_algebraic is None\n    assert (2**Rational(1, 3)).is_algebraic is True\n    assert (2**cos(pi/3)).is_algebraic is None\n    assert (2**sqrt(2)).is_algebraic is False\n    assert (2**I).is_algebraic is False\n", "def test_Pow_finite_recursion():\n    # test case for infinite recursion in Pow._eval_is_finite\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    assert (x**n).is_finite\n", "def test_issue_17411():\n    assert Pow(-1, S.Half, evaluate=False).is_real is None\n    assert Pow(-1, S.Half).is_real is False\n", "def test_Mul_is_polynomial():\n    x, y, z = symbols('x y z')\n    assert (x*y).is_polynomial(x) is True\n    assert (x*y).is_polynomial(y) is True\n    assert (x*y).is_polynomial(z) is True\n    assert (x/y).is_polynomial(x) is True\n    assert (x/y).is_polynomial(y) is False\n    assert (x/y).is_polynomial(z) is True\n    assert (x**y).is_polynomial(x) is False\n    assert (x**y).is_polynomial(y) is False\n    assert (x**y).is_polynomial(z) is False\n    assert (x**3).is_polynomial(x) is True\n    assert (x**3).is_polynomial(y) is True\n    assert (x**3).is_polynomial(z) is True\n"], "sample_1101": ["def test_schur_number():\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(0.5))\n    raises(ValueError, lambda: SchurNumber(S.Infinity - 1))\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert isinstance(SchurNumber(5), SchurNumber)\n    assert SchurNumber(6).lower_bound() == 364\n    assert SchurNumber(S.Infinity) == S.Infinity\n", "def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n\n    assert SchurNumber(S.Infinity) == S.Infinity\n    assert SchurNumber(0) == 0\n\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(S.Half))\n\n    x = symbols(\"x\")\n    raises(ValueError, lambda: SchurNumber(x).lower_bound())\n    assert SchurNumber(5).lower_bound() == 121\n", "def test_SchurNumber():\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5).lower_bound() == (3**5 - 1)/2\n    assert SchurNumber(S.Infinity) == S.Infinity\n", "def test_SchurNumber():\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5).lower_bound() == (3**5 - 1)/2\n    x = symbols(\"x\")\n    raises(ValueError, lambda: SchurNumber(x))\n    assert SchurNumber(S.Infinity) is S.Infinity\n", "def test_SchurNumber():\n    raises(ValueError, lambda: SchurNumber(S.Infinity - 1))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5).lower_bound() == 121\n    assert SchurNumber(S.Infinity) == S.Infinity\n    x = symbols(\"x\")\n    raises(ValueError, lambda: SchurNumber(x))\n"], "sample_1104": ["def test_MatPow_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert str(A**(-1)) == \"A**(-1)\"\n    assert str(A**3) == \"A**3\"\n    assert str(A**(3/2)) == \"A**(3/2)\"\n    assert str(A**(-3/2)) == \"A**(-3/2)\"\n    assert str(A*B**(-1)) == \"A*B**(-1)\"\n", "def test_Dimension():\n    from sympy import Dimension\n    assert str(Dimension('length')) == 'length'\n    assert str(Dimension('time')) == 'time'\n", "def test_issue_15201():\n    from sympy.combinatorics import Permutation\n    Permutation.print_cyclic = True\n    p = Permutation([0, 2, 3, 1])\n    assert sstr(p) == \"(1 3 2)\"\n    Permutation.print_cyclic = False\n    assert sstr(p) == \"Permutation([0, 2, 3, 1])\"\n    Permutation.print_cyclic = True\n    assert sstr(p) == \"(1 3 2)\"\n", "def test_DeferredVector():\n    from sympy.vector import CoordSys3D\n    C = CoordSys3D('C')\n    assert str(C.x) == \"C.x\"\n    assert str(C.y) == \"C.y\"\n    assert str(C.z) == \"C.z\"\n", "def test_DMP_printing():\n    from sympy.polys.domains import ZZ\n    from sympy.polys.domains import QQ\n    R, x = ZZ['x'].obj\n    F, x = QQ['x'].obj\n    f = x**3 - 6*x**2 + 11*x - 6\n    assert str(R.convert(f)) == \"Poly(x**3 - 6*x**2 + 11*x - 6, x, domain='ZZ')\"\n    assert str(F.convert(f)) == \"Poly(x**3 - 6*x**2 + 11*x - 6, x, domain='QQ')\"\n\n    g = x**3 - x**2 + 11*x - 6\n    assert str(R.convert(g)) == \"Poly(x**3 - x**2 + 11*x - 6, x, domain='ZZ')\"\n    assert str(F.convert(g)) == \"Poly(x**3 - x**2 + 11*x - 6, x, domain='QQ')\"\n\n    R, x, y = ZZ['x, y'].obj\n    F, x, y = QQ['x, y'].obj\n    f = x**3 - 6*x**2*y + 11*x*y**2 - 6*y**3\n    assert str(R.convert(f)) == \"Poly(x**3 - 6*x**2*y + 11*x*y**2 - 6*y**3, x, y, domain='ZZ')\"\n    assert str(F.convert(f)) == \"Poly(x**3 - 6*x**2*y + 11*x*y**2 - 6*y**3, x, y, domain='QQ')\"\n"], "sample_1103": ["def test_as_real_imag():\n    # issue 15286\n    assert (I**-1).as_real_imag() == (0, -1)\n    assert ((-I)**-1).as_real_imag() == (0, 1)\n    # issue 15397\n    assert (I**4.5).as_real_imag() == (0, -1)\n    assert ((-I)**4.5).as_real_imag() == (0, -1)\n", "def test_issue_16448():\n    assert (-2)**Rational(3, 2) == -2*sqrt(2)\n    assert (-2)**Rational(5, 2) == -4*2*sqrt(2)\n", "def test_Pow_is_algebraic():\n    x = Symbol('x')\n    assert (x**2).is_algebraic is None\n    assert (2**x).is_algebraic is None\n\n    k = Symbol('k', integer=True)\n    assert (2**k).is_algebraic is True\n\n    n = Symbol('n', integer=True, negative=True)\n    assert (2**n).is_algebraic is True\n\n    m = Symbol('m', integer=True, nonnegative=True)\n    assert (2**m).is_algebraic is True\n\n    p = Symbol('p', integer=True, positive=True)\n    assert (2**p).is_algebraic is True\n\n    r = Symbol('r', rational=True)\n    assert (2**r).is_algebraic is True\n\n    assert (2**I).is_algebraic is False\n    assert (2**(I*pi)).is_algebraic is True\n    assert (2**(I*pi/2)).is_algebraic is True\n    assert (2**(3*I*pi/2)).is_algebraic is True\n    assert (2**(2*I*pi)).is_algebraic is True\n", "def test_Pow_is_complex():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (x**I).is_complex\n    assert (y**x).is_complex is None\n    assert (2**x).is_complex is None\n    assert (2**y).is_complex is True\n", "def test_Mul_hermitian_antihermitian():\n    a = Symbol('a', hermitian=True, zero=False)\n    b = Symbol('b', hermitian=True)\n    c = Symbol('c', hermitian=False)\n    d = Symbol('d', antihermitian=True)\n    e1 = Mul(a, b, c, evaluate=False)\n    e2 = Mul(b, a, c, evaluate=False)\n    e3 = Mul(a, b, c, d, evaluate=False)\n    e4 = Mul(b, a, c, d, evaluate=False)\n    e5 = Mul(a, c, evaluate=False)\n    e6 = Mul(a, c, d, evaluate=False)\n    assert e1.is_hermitian is None\n    assert e2.is_hermitian is None\n    assert e1.is_antihermitian is None\n    assert e2.is_antihermitian is None\n    assert e3.is_antihermitian is None\n    assert e4.is_antihermitian is None\n    assert e5.is_antihermitian is None\n    assert e6.is_antihermitian is None\n"], "sample_1105": ["def test_refine_withIdentity():\n    assert refine(C*C.T*Identity(n)*D, Q.orthogonal(C)).doit() == D\n    assert refine(C*C.T*Identity(n)*Identity(n)*D, Q.orthogonal(C)).doit() == D\n", "compilation error", "def test_as_coeff_matrices():\n    assert MatMul(2, A, 3, B).as_coeff_matrices() == (6, [A, B])\n    assert MatMul(A, B).as_coeff_matrices() == (1, [A, B])\n    assert MatMul(2, A).as_coeff_matrices() == (2, [A])\n    assert MatMul(2).as_coeff_matrices() == (2, [])\n", "def test_entry():\n    assert (A*B).as_explicit()[0,0] == (A.as_explicit() @ B.as_explicit())[0,0]\n    assert MatMul(2, A, B).as_explicit()[0,0] == 2 * (A.as_explicit() @ B.as_explicit())[0,0]\n    assert MatMul(2, A, B, C).as_explicit()[0,0] == 2 * (A.as_explicit() @ B.as_explicit() @ C.as_explicit())[0,0]\n", "def test_MatMul_evaluate_False():\n    # Test that evaluate=False works correctly\n    assert MatMul(A, Identity(m), B, evaluate=False).args == (A, Identity(m), B)\n    assert MatMul(A, ZeroMatrix(m, k), evaluate=False).args == (A, ZeroMatrix(m, k))\n    assert MatMul(A, 2, B, evaluate=False).args == (A, 2, B)\n"], "sample_1102": ["def test_issue_18365():\n    x = Symbol('x')\n    P = Poly(x**2 + x + 1, x)\n    assert P.coeff_monomial(x**2 + x) == 0\n", "def test_issue_18423():\n    p = Poly(2*x**2 - 3*x - 2, x)\n    assert p.real_roots() == [-0.5, 2]\n    assert p.real_roots(multiple=False) == {(-0.5, 1), (2, 1)}\n    assert p.real_roots(multiple=True) == [-0.5, 2]\n", "def test_issue_18257():\n    assert factor(2*x**2 + 3*x + 1, modulus=7) == (x + 1)*(2*x + 1)\n    assert factor(3*x**2 + 2*x + 1, modulus=7) == (x + 5)*(3*x + 4)\n", "def test_Poly_floordiv():\n    p = Poly(x**2 + 1)\n    q = Poly(2*x + 1)\n    assert p.floordiv(q) == Poly(x/2 + 1)\n    assert Poly(2*x + 1, x, domain='ZZ').floordiv(Poly(x + 1, x, domain='ZZ')) == Poly(2, x, domain='ZZ')\n", "def test_issue_18087():\n    p = Poly(x**2 + 1, x)\n    assert div(p, p, order='lex') == (1, 0)\n    assert div(p, p, order='grevlex') == (1, 0)\n"], "sample_1106": ["def test_matadd_evaluate():\n    A_val = Matrix([[1, 2], [3, 4]])\n    B_val = Matrix([[5, 6], [7, 8]])\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatAdd(A_val, B_val, evaluate=True) == A_val + B_val\n    assert MatAdd(A, B, evaluate=False) == A + B\n    assert MatAdd(A, B, evaluate=True) == MatAdd(A, B)\n", "def test_matadd_doit():\n    assert MatAdd(C, 2, D).args == (C, 2, D)\n    assert MatAdd(C, 2, D).doit().args == (C, 2, D)\n    assert MatAdd(C, ZeroMatrix(n, n)).doit() == C\n    assert MatAdd(C, C, D).doit() == MatAdd(2*C, D)\n", "def test_MatAdd_doit():\n    assert MatAdd(A, ZeroMatrix(n, m), evaluate=True) == A\n    assert MatAdd(A, B, evaluate=True) == A + B\n    assert MatAdd(A, -A, evaluate=True) == ZeroMatrix(n, m)\n    assert MatAdd(A, -A, evaluate=False).doit() == ZeroMatrix(n, m)\n", "def test_matadd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatAdd(A, B).doit() == A + B\n    assert MatAdd(A, 2*B).doit() == A + 2*B\n    assert MatAdd(2*A, B).doit() == 2*A + B\n    assert MatAdd(A, ZeroMatrix(2, 2)).doit() == A\n    assert MatAdd(ZeroMatrix(2, 2), A).doit() == A\n", "def test_matadd_doit():\n    A_expr = Matrix([[1, 2], [3, 4]])\n    B_expr = Matrix([[5, 6], [7, 8]])\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatAdd(A, B, evaluate=False).doit() == A + B\n    assert MatAdd(A_expr, B_expr, evaluate=False).doit() == A_expr + B_expr\n    assert MatAdd(A_expr, B, evaluate=False).doit() == A_expr + B\n"], "sample_1108": ["def test_roundrobin():\n    from itertools import islice\n    r = roundrobin('ABC', 'D', 'EF')\n    assert ''.join(islice(r, 10)) == 'ADEBCFADBE'\n", "def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert list(gen) == ['A', 'D', 'E', 'B', 'F', 'C']\n    gen = roundrobin([1, 2], [3, 4, 5])\n    assert list(gen) == [1, 3, 2, 4, 5]\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin('ABC', 'D', 'EF', 'G')) == [\n        'A', 'D', 'E', 'G', 'B', 'F', 'C']\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin('ABC', [], 'EF')) == [\n        'A', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([], 'D', 'EF')) == ['D', 'E', 'F']\n    assert list(roundrobin([], '', 'EF')) == ['E', 'F']\n    assert list(roundrobin([], '', '')) == []\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n"], "sample_1107": ["def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert next(gen) == 'A'\n    assert next(gen) == 'D'\n    assert next(gen) == 'E'\n    assert next(gen) == 'B'\n    assert next(gen) == 'F'\n    assert next(gen) == 'C'\n    raises(StopIteration, lambda: next(gen))\n", "def test_multiset():\n    assert multiset('ab') == {'a': 1, 'b': 1}\n    assert multiset('aab') == {'a': 2, 'b': 1}\n    assert multiset([1, 2, 1]) == {1: 2, 2: 1}\n    assert multiset((1, 2, 1)) == {1: 2, 2: 1}\n    assert multiset({1, 2, 1}) == {1: 1, 2: 1}\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin('ABC', 'D', 'EF', 'G')) == [\n        'A', 'D', 'E', 'G', 'B', 'F', 'C']\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 0, 0))) == [\n        (1, 0, 0), (-1, 0, 0)]\n    assert list(permute_signs([1, 0, -1, 1])) == [\n        [1, 0, -1, 1], [1, 0, 1, 1], [1, 0, -1, -1], [1, 0, 1, -1],\n        [-1, 0, -1, 1], [-1, 0, 1, 1], [-1, 0, -1, -1], [-1, 0, 1, -1]]\n    assert list(permute_signs([])) == [()]\n", "def test_roundrobin():\n    gen = roundrobin('ABC', [1, 2], (3, 4))\n    assert next(gen) == 'A'\n    assert next(gen) == 1\n    assert next(gen) == 3\n    assert next(gen) == 'B'\n    assert next(gen) == 2\n    assert next(gen) == 4\n    raises(StopIteration, lambda: next(gen))\n\n    gen = roundrobin([1, 2], (3, 4, 5), 'ABC')\n    assert next(gen) == 1\n    assert next(gen) == 3\n    assert next(gen) == 'A'\n    assert next(gen) == 2\n    assert next(gen) == 4\n    assert next(gen) == 'B'\n    assert next(gen) == 5\n    assert next(gen) == 'C'\n    raises(StopIteration, lambda: next(gen))\n"], "sample_1109": ["def test_frac_is_integer():\n    assert frac(4).is_integer\n    assert frac(3.5).is_integer is None\n    assert frac(-3).is_integer\n", "def test_frac_is_extended_real():\n    r = Symbol('r', real=True)\n    assert frac(r).is_extended_real\n    i = Symbol('i', imaginary=True)\n    assert not frac(i).is_extended_real\n\n", "def test_frac_with_symbols():\n    x, y = symbols('x, y', real=True)\n    z = Symbol('z')\n    assert frac(x + y) == frac(x + y)\n    assert frac(x + 2) == frac(x)\n    assert frac(x + y + z) == frac(x + y + z)\n    assert frac(2*x) == frac(2*x)\n    assert frac(x/2) == frac(x/2)\n    assert frac(x + pi) == frac(x + pi)\n    assert frac(x + E) == frac(x + E)\n    assert frac(x + I*y) == I*frac(y) + frac(x)\n", "def test_frac_negative():\n    assert frac(-pi) == 1 - pi\n    assert frac(-E) == 1 - E\n    assert frac(-3.7) == 0.3\n    assert frac(-2) == 0\n\n    x = Symbol('x')\n    assert frac(-x) == -frac(x) + 1\n    assert frac(-x).rewrite(floor) == -x - floor(-x)\n    assert frac(-x).rewrite(ceiling) == 1 - x - ceiling(-x)\n\n    r = Symbol('r', real=True)\n    assert frac(-r) == 1 - (-r - floor(-r))\n\n    i = Symbol('i', imaginary=True)\n    assert frac(-i) == I*frac(-i/I)\n", "def test_frac_with_negative_values():\n    assert frac(-1) == 0\n    assert frac(-Rational(1, 2)) == Rational(1, 2)\n    assert frac(-pi) == 1 - pi\n    assert frac(-E) == 1 - E\n    assert frac(-I) == -I\n    assert frac(-I/2) == -I/2\n\n    x = Symbol('x', real=True)\n    assert frac(-x) == -frac(x)\n\n    n = Symbol('n', integer=True, negative=True)\n    assert frac(n) == 0\n    assert frac(n/2) == frac(n/2)\n\n    r = Symbol('r', real=True, negative=True)\n    assert frac(r).is_real\n    assert frac(r).is_finite\n    assert frac(r).is_nonnegative\n    assert frac(r).is_nonpositive is None\n    assert frac(r) >= 0\n    assert frac(r) < 1\n    assert (frac(r) > 0).has(Gt)\n    assert (frac(r) <= 0).has(Le)\n    assert (frac(r) >= 1).has(Ge)\n    assert frac(r) < oo\n    assert frac(r) > -oo\n"], "sample_1110": ["def test_issue_18706():\n    from sympy import BlockMatrix\n    M = BlockMatrix([[x, y], [MatrixSymbol('z', 1, 2), x]])\n    assert NumPyPrinter().doprint(M) == 'numpy.block([[x, y], [z, x]])'\n    assert SciPyPrinter().doprint(M) == 'numpy.block([[x, y], [z, x]])'\n    assert NumPyPrinter().doprint(M) == 'numpy.block([[x, y], [z, x]])'\n    assert MpmathPrinter().doprint(M) == '[[x, y], [z, x]]'\n", "def test_SymPyPrinter():\n    from sympy import Function\n    p = SymPyPrinter()\n    f = Function('f')\n    assert p.doprint(f(x, y)) == 'sympy.Function('f')(x, y)'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert p.doprint(S.Catalan) == 'sympy.Catalan'\n    assert p.doprint(S.NaN) == 'sympy.nan'\n    assert p.doprint(S.Infinity) == 'sympy.oo'\n    assert p.doprint(S.NegativeInfinity) == '-sympy.oo'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == \"sympy.Matrix(A).inv()\"\n    assert p.doprint(A**5) == \"sympy.Matrix(A)**5\"\n    assert p.doprint(Identity(3)) == \"sympy.eye(3)\"\n", "def test_issue_21948():\n    from sympy.codegen.ast import Elementwise\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n\n    prntr = NumPyPrinter()\n    pw = A**B\n    assert prntr.doprint(pw) == 'numpy.power(A, B)'\n\n    matmul = Elementwise(A, B, lambda i, j: A[i, j] * B[i, j])\n    assert prntr.doprint(matmul) == 'numpy.multiply(A, B)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert p.doprint(S.NaN) == 'sympy.nan'\n    assert p.doprint(S.Infinity) == 'sympy.oo'\n    assert p.doprint(S.NegativeInfinity) == '-sympy.oo'\n"], "sample_1111": ["def test_constant():\n    x = Symbol('x')\n    lines = [\n        '      2 |-------------------------------------------------------',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '      0 |-------------------------------------------------------',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(2, -1, 1))\n\n    lines = [\n        '      0 |-------------------------------------------------------',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '     -2 |-------------------------------------------------------',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(-2, -1, 1))\n", "def test_linspace_edge_cases():\n    assert linspace(0, 10, 1) == [0]\n    assert linspace(0, 10, 2) == [0, 10]\n    assert linspace(0, 0, 5) == [0, 0, 0, 0, 0]\n    assert linspace(10, 0, 5) == [10, 7.5, 5, 2.5, 0]\n    assert linspace(0, 10, 0) == []\n", "def test_textplot_str_expression_with_constant_value():\n    x = Symbol('x')\n    lines = [\n        '      2 |-------------------------------------------------------',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '      1 |-------------------------------------------------------',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '        |                                                     ',\n        '      0 |-------------------------------------------------------',\n        '         0                          1                          2'\n    ]\n    assert lines == list(textplot_str(1, 0, 2, H=21))\n", "def test_is_valid():\n    assert not textplot_str.is_valid(None)\n    assert not textplot_str.is_valid(float('inf'))\n    assert not textplot_str.is_valid(float('nan'))\n    assert not textplot_str.is_valid(complex(1, 2))\n    assert textplot_str.is_valid(1.0)\n    assert textplot_str.is_valid(0.0)\n    assert textplot_str.is_valid(-1.0)\n", "def test_invalid_input():\n    x = Symbol('x')\n    lines = list(textplot_str(x, None, 1))\n    assert len(lines) == 1\n    assert lines[0] == '         -1                         0                          1'\n\n    lines = list(textplot_str(x, 0, None))\n    assert len(lines) == 1\n    assert lines[0] == '         0                          0.5                        1'\n\n    lines = list(textplot_str(x, float('inf'), 1))\n    assert len(lines) == 1\n    assert lines[0] == '         -1                         0                          1'\n\n    lines = list(textplot_str(x, 0, float('inf')))\n    assert len(lines) == 1\n    assert lines[0] == '         0                          0.5                        1'\n\n    lines = list(textplot_str(x, float('nan'), 1))\n    assert len(lines) == 1\n    assert lines[0] == '         -1                         0                          1'\n\n    lines = list(textplot_str(x, 0, float('nan')))\n    assert len(lines) == 1\n    assert lines[0] == '         0                          0.5                        1'\n"], "sample_1112": ["def test_is_palindromic():\n    assert all(is_palindromic(i) for i in (-11, 1, 22, 121))\n    assert not is_palindromic(10)\n    assert not is_palindromic(123)\n    assert is_palindromic(0o121, 8)\n    assert is_palindromic(0x1331, 16)\n    assert is_palindromic(12321, 10)\n    assert is_palindromic(-12321, 10)\n", "def test_is_palindromic():\n    assert all(is_palindromic(i) for i in (-11, 1, 22, 121))\n    assert not is_palindromic(88, 8)\n    assert not is_palindromic(0o121)\n    assert is_palindromic(121, 8) and is_palindromic(121)\n    assert is_palindromic(12321)\n    assert not is_palindromic(123456)\n    assert is_palindromic(-12321)\n    raises(ValueError, lambda: is_palindromic(123, 1))\n", "def test_is_palindromic():\n    assert is_palindromic(121) is True\n    assert is_palindromic(-121) is True\n    assert is_palindromic(12) is False\n    assert is_palindromic(0o121, 8) is True\n    assert is_palindromic(0o122, 8) is False\n    assert is_palindromic(0x12321, 16) is True\n    assert is_palindromic(0x123456, 16) is False\n    raises(ValueError, lambda: is_palindromic(123, 1))\n", "def test_is_palindromic():\n    assert is_palindromic(121, 10) is True\n    assert is_palindromic(123, 10) is False\n    assert is_palindromic(0o121, 8) is True\n    assert is_palindromic(-121, 10) is True\n    assert is_palindromic(0x121, 16) is True\n    assert is_palindromic(1234, 10) is False\n    assert is_palindromic(1221, 10) is True\n", "def test_is_palindromic():\n    assert all([is_palindromic(i) for i in (-11, 1, 22, 121)])\n    assert not is_palindromic(88, 8)\n    assert is_palindromic(121, 8)\n    assert is_palindromic(0o171)\n    assert not is_palindromic(0o121)\n    assert not is_palindromic(123456)\n    raises(ValueError, lambda: is_palindromic(2, 1))\n    raises(ValueError, lambda: is_palindromic(2, 0))\n    raises(ValueError, lambda: is_palindromic(2, -1))\n"], "sample_1113": ["def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (1, 3))\n    assert isinstance(B, BlockMatrix)\n    assert B.blocks[0, 1] == ImmutableMatrix(1, 3, [1, 2, 3])\n    assert B.blocks[1, 0] == ImmutableMatrix(3, 1, [4, 8, 12])\n    assert B.blocks[1, 1] == ImmutableMatrix(3, 3, [5, 6, 7, 9, 10, 11, 13, 14, 15])\n", "def test_block_diagonal_matrix():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    C = BlockDiagMatrix(A, B)\n\n    assert C.blocks == ImmutableMatrix([[A, ZeroMatrix(n, m)], [ZeroMatrix(m, n), B]])\n    assert C.blockshape == (2, 2)\n    assert C.shape == (n + m, n + m)\n\n    assert C._eval_inverse() == BlockDiagMatrix(A.inverse(), B.inverse())\n    assert C._eval_transpose() == BlockDiagMatrix(A.transpose(), B.transpose())\n\n    assert C.is_structurally_symmetric\n    assert not C.is_Identity\n\n    D = BlockDiagMatrix(Identity(n), Identity(m))\n    assert D.is_Identity\n", "def test_block_matrix_trace():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    BM = BlockMatrix([[A, ZeroMatrix(2, 3)], [ZeroMatrix(3, 2), B]])\n    assert BM.trace() == A.trace() + B.trace()\n", "def test_block_inverse():\n    A11 = MatrixSymbol('A11', n, n)\n    A12 = MatrixSymbol('A12', n, m)\n    A21 = MatrixSymbol('A21', m, n)\n    A22 = MatrixSymbol('A22', m, m)\n    A = BlockMatrix([[A11, A12], [A21, A22]])\n\n    Ainv = A.inverse()\n    assert Ainv.shape == (n + m, n + m)\n\n    # Check that A * Ainv == I\n    I = Identity(n + m)\n    assert (A * Ainv).equals(I)\n    assert (Ainv * A).equals(I)\n", "def test_blockinverse_1x1():\n    A = MatrixSymbol('A', 2, 2)\n    B = BlockMatrix([[A]])\n    assert blockinverse_1x1(Inverse(B)) == BlockMatrix([[Inverse(A)]])\n"], "sample_1114": ["def test_Range_reversed():\n    r = Range(1, 10, 2)\n    assert list(reversed(r)) == list(range(9, 0, -2))\n    assert list(reversed(Range(1, 10, 1))) == list(range(9, 0, -1))\n    assert list(reversed(Range(1, 10))) == list(range(9, 0, -1))\n    raises(ValueError, lambda: reversed(Range(1, oo)))\n    raises(ValueError, lambda: reversed(Range(-oo, 10)))\n    raises(ValueError, lambda: reversed(Range(-oo, oo)))\n", "def test_issue_20390():\n    assert Range(1, 10).as_relational(x) == (x >= 1) & (x <= 9) & Eq(x, floor(x))\n    assert Range(1, 10, 2).as_relational(x) == (x >= 1) & (x <= 9) & Eq(x % 2, 1) & Eq(x, floor(x))\n    assert Range(-5, 5).as_relational(x) == (x >= -5) & (x <= 4) & Eq(x, floor(x))\n    assert Range(-5, 5, 2).as_relational(x) == (x >= -5) & (x <= 3) & Eq(x % 2, 1) & Eq(x, floor(x))\n", "def test_issue_20495():\n    assert Range(1, oo, 2).intersect(Range(3, oo, 3)) == Range(9, oo, 6)\n    assert Range(1, oo, 2).intersect(Range(2, oo, 3)) == Range(10, oo, 6)\n    assert Range(3, oo, 3).intersect(Range(2, oo, 2)) == Range(10, oo, 6)\n", "def test_issue_21064():\n    assert imageset(x, x**2, S.Naturals0) == Range(0, oo)\n    assert imageset(x, x**3, S.Integers) == S.Integers\n    assert imageset(x, x**4, S.Integers) == Range(0, oo)\n    assert imageset(x, x**4, S.Naturals0) == Range(0, oo)\n", "def test_ComplexRegion_is_subset():\n    from sympy import Interval, ComplexRegion, Union, S\n\n    # Rectangular form\n    c1 = ComplexRegion(Interval(1, 3)*Interval(2, 4))\n    c2 = ComplexRegion(Interval(1, 4)*Interval(1, 5))\n    c3 = ComplexRegion(Interval(1, 3)*Interval(2, 4), polar=True)\n    c4 = ComplexRegion(Interval(1, 4)*Interval(1, 5), polar=True)\n\n    assert c1.is_subset(c2) == True\n    assert c2.is_subset(c1) == False\n    assert c3.is_subset(c4) == False\n    assert c4.is_subset(c3) == False\n\n    # Union of ComplexRegion\n    c5 = ComplexRegion(Interval(1, 3)*Interval(2, 4))\n    c6 = ComplexRegion(Interval(4, 6)*Interval(2, 4))\n    c7 = ComplexRegion(Union(Interval(1, 3)*Interval(2, 4), Interval(4, 6)*Interval(2, 4)))\n\n    assert c5.is_subset(c7) == True\n    assert c7.is_subset(c5) == False\n    assert c6.is_subset(c7) == True\n    assert c7.is_subset(c6) == False\n\n    # is_subset of Complexes\n    c8 = ComplexRegion(S.Reals*S.Reals)\n\n    assert c1.is_subset(c8) == True\n    assert c8.is_subset(c1) == False\n    assert c8.is_subset(S.Complexes) == True\n"], "sample_1116": ["def test_refine_Inverse():\n    assert refine(C.I, Q.unitary(C)) == C.conjugate()\n    raises(ValueError, lambda: refine(C.I, Q.singular(C)))\n    assert refine(C.I, Q.singular(E)) == C.I\n    assert refine(C.I, Q.orthogonal(E)) == C.I\n", "def test_inverse_derivative():\n    from sympy import symbols, MatrixSymbol, diff\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    A_diff = diff(A, x)\n    assert (A.I)._eval_derivative_matrix_lines(x)[0].first_pointer == -A_diff * A.I.T\n    assert (A.I)._eval_derivative_matrix_lines(x)[0].second_pointer == A.I\n", "def test_inverse_assumptions():\n    raises(ValueError, lambda: refine(C.I, Q.singular(C)))\n    assert refine(C.I, Q.unitary(C)) == C.conjugate()\n    assert refine(C.I) == C.I\n", "def test_derivative_inverse():\n    from sympy import symbols\n    x = symbols('x')\n    assert Inverse(C).diff(x) == -C.I*C.diff(x)*C.I\n", "def test_derivative_matrix_lines():\n    from sympy import symbols\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    A_diff = MatrixSymbol('dA', 2, 2)\n    assert (Inverse(A))._eval_derivative_matrix_lines(x) == A._eval_derivative_matrix_lines(x)\n"], "sample_1115": ["def test_tensor_element():\n    L = TensorIndexType(\"L\")\n    i0, i1 = tensor_indices('i0:2', L)\n    A = TensorHead('A', [L, L])\n    te = TensorElement(A(i0, i1), {i0: 2})\n    assert te.get_free_indices() == [i1]\n    assert te.data == None\n", "def test_replace_with_arrays_add():\n    L = TensorIndexType(\"L\", dim=4)\n    A = TensorHead(\"A\", [L]*2)\n    i, j, k, l = symbols(\"i j k l\")\n\n    expr = A(i, j) + A(j, i)\n    repl = {A(i, j): [[1, 2], [3, 4]]}\n\n    assert expr.replace_with_arrays(repl, [i, j]) == Array([[2, 5], [5, 8]])\n    assert expr.replace_with_arrays(repl, [j, i]) == Array([[2, 5], [5, 8]])\n", "def test_tensor_partial_derivative():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    L_0 = TensorIndex(\"L_0\", L)\n\n    A, B, C, D = tensor_heads(\"A B C D\", [L])\n    H = TensorHead(\"H\", [L, L])\n    K = TensorHead(\"K\", [L]*4)\n\n    x, y = symbols(\"x y\")\n\n    expr = H(i, j)\n    assert expr.diff(x) == 0\n\n    expr = x*A(i)\n    assert expr.diff(x) == A(i)\n\n    expr = x*y*A(i)\n    assert expr.diff(x) == y*A(i)\n\n    expr = A(i)*A(j)\n    assert expr.diff(x) == 0\n\n    expr = x*A(i)*A(j)\n    assert expr.diff(x) == A(i)*A(j)\n\n    expr = x*y*A(i)*A(j)\n    assert expr.diff(x) == y*A(i)*A(j)\n\n    expr = H(i, j)\n    assert expr._eval_partial_derivative(A(i)) == 0\n\n    expr = A(i)*A(-i)\n    assert expr._eval_partial_derivative(A(j)) == 2*A(j)\n\n    expr = x*A(i)*A(-i)\n    assert expr._eval_partial_derivative(A(j)) == 2*x*A(j)\n\n    expr = x*y*A(i)*A(-i)\n    assert expr._eval_partial_derivative(A(j)) == 2*x*y*A(j)\n\n    expr = A(i)*A(-i)\n    assert expr._eval_partial_derivative(B(j)) == 0\n\n    expr = A(i)*A(-i)\n    result = expr._eval_partial_derivative(H(k, -k))\n    assert result == 0\n\n    expr = A(i)*A(-i)*A(j)*A(-j)\n    result = expr._eval_partial_derivative(H(k, -k))\n    assert result == 0\n\n    expr = H(i, j)\n    result = expr._eval_partial_derivative(H(i, j))\n    assert result == 0\n\n    expr = H(i, j)\n    result = expr._eval_partial_derivative(H(j, i))\n    assert result == 0\n\n    expr = A(i)*H(i, j)\n    result = expr._eval_partial_derivative(A(i))\n    assert result == H(L_0, j", "def test_tensor_element():\n    L = TensorIndexType(\"L\", dim=4)\n    A = TensorHead(\"A\", [L]*2)\n    i, j = tensor_indices(\"i j\", L)\n    x, y = symbols(\"x y\")\n\n    assert A(x, y).get_free_indices() == [x, y]\n    assert A(x, y).get_indices() == [x, y]\n\n    assert A(x, -x).get_free_indices() == []\n    assert A(x, -x).get_indices() == [L_0, -L_0]\n\n    assert (A(x, y) + A(y, x)).get_free_indices() == [x, y]\n    assert (A(x, y) + A(y, x)).get_indices() == [x, y]\n", "def test_tensor_element():\n    Lorentz = TensorIndexType(\"Lorentz\", dummy_name=\"L\")\n    a, b, c, d = tensor_indices('a,b,c,d', Lorentz)\n    g = Lorentz.metric\n    p, q = tensor_heads('p q', [Lorentz])\n\n    te = TensorElement(p(a), {a: 1})\n    assert te.free_indices == [a]\n    assert te.free == [(a, 0)]\n    assert te.dum == []\n    assert te.coeff == 1\n    assert te.nocoeff == te\n\n    te = TensorElement(p(a), {a: -2})\n    assert te.free_indices == []\n    assert te.free == []\n    assert te.dum == []\n    assert te.coeff == 1\n    assert te.nocoeff == te\n\n    te = TensorElement(p(a)*p(b), {a: 1})\n    assert te.free_indices == [b]\n    assert te.free == [(b, 1)]\n    assert te.dum == []\n    assert te.coeff == 1\n    assert te.nocoeff == te\n\n    te = TensorElement(p(a)*p(b), {a: -2})\n    assert te.free_indices == [b]\n    assert te.free == [(b, 1)]\n    assert te.dum == []\n    assert te.coeff == 1\n    assert te.nocoeff == te\n"], "sample_1117": ["def test_real_elements_on_matrix_slices():\n    X = MatrixSymbol('X', 4, 4)\n    B = MatrixSlice(X, (1, 3), (1, 3))\n    C = MatrixSlice(X, (0, 3), (1, 3))\n    assert ask(Q.real_elements(B), Q.real_elements(X))\n    assert ask(Q.real_elements(C), Q.real_elements(X))\n", "def test_real_elements_factorization():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.real_elements(X.LUdecomposition()), Q.real_elements(X))\n    assert ask(Q.real_elements(X.cholesky()), Q.real_elements(X) & Q.positive_definite(X))\n    assert ask(Q.real_elements(X.QRdecomposition()), Q.real_elements(X))\n", "def test_integer_elements_on_matrix_multiplication():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    alpha = Symbol('alpha', integer=True)\n    assert ask(Q.integer_elements(X*Y), Q.integer_elements(X) & Q.integer_elements(Y))\n    assert ask(Q.integer_elements(alpha*X), Q.integer_elements(X))\n    assert not ask(Q.integer_elements(alpha*X), Q.real_elements(X))\n    assert ask(Q.integer_elements(X**2), Q.integer_elements(X))\n    assert ask(Q.integer_elements(X + Identity(4)), Q.integer_elements(X))\n    assert ask(Q.integer_elements(X + ZeroMatrix(4, 4)), Q.integer_elements(X))\n    assert ask(Q.integer_elements(X + OneMatrix(4, 4))) is None\n", "def test_real_element_assumptions_with_transpose():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.real_elements(X), Q.real_elements(X.T))\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n    assert ask(Q.integer_elements(X), Q.integer_elements(X.T))\n    assert ask(Q.complex_elements(X.T), Q.complex_elements(X))\n    assert ask(Q.complex_elements(X), Q.complex_elements(X.T))\n", "def test_invertible_elements():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.invertible_elements(X), Q.invertible_elements(X))\n    assert ask(Q.invertible_elements(X**2), Q.invertible_elements(X))\n    assert ask(Q.invertible_elements(X.T), Q.invertible_elements(X))\n    assert ask(Q.invertible_elements(X.I), Q.invertible_elements(X) & Q.invertible(X))\n    alpha = Symbol('alpha')\n    assert ask(Q.invertible_elements(alpha*X), Q.invertible_elements(X) & Q.invertible(alpha))\n    assert ask(Q.invertible_elements(Identity(3)))\n    assert not ask(Q.invertible_elements(ZeroMatrix(3, 3)))\n    assert ask(Q.invertible_elements(OneMatrix(1, 1)))\n    assert not ask(Q.invertible_elements(OneMatrix(3, 3)))\n    from sympy.matrices.expressions.fourier import DFT\n    assert ask(Q.invertible_elements(DFT(3)))\n"], "sample_1118": ["def test_matpow():\n    assert MatPow(C, 2).doit() == C*C\n    assert MatPow(C, -1).doit() == C.I\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, S.One).doit() == C\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n    raises(TypeError, lambda: MatPow(n, 2))\n    assert MatPow(C, -2).doit() == MatPow(C.I, 2).doit()\n    assert MatPow(C, 2).T == MatPow(C.T, 2)\n", "def test_matpow():\n    assert MatPow(C, 2).args == (C, 2)\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(C, -2).doit() == Inverse(C)*Inverse(C)\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    assert MatPow(Identity(n), 2).doit() == Identity(n)\n    assert MatPow(Identity(n), -2).doit() == Identity(n)\n    assert MatPow(C, 2).T == MatPow(C.T, 2)\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    raises(TypeError, lambda: MatPow(1, 2))\n    raises(TypeError, lambda: MatPow(C, 'a'))\n", "def test_matpow():\n    assert MatPow(C, 2).args == (C, 2)\n    assert MatPow(C, 2).shape == C.shape\n    assert MatPow(C, 0) == Identity(C.rows)\n    assert MatPow(C, 1) == C\n    assert MatPow(Identity(n), 3) == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 3) == ZeroMatrix(n, n)\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n    raises(TypeError, lambda: MatPow(n, 2))\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(MatPow(C, 2), 3).doit() == C**6\n    assert MatPow(C, -1).T == MatPow(C.T, -1)\n", "def test_matpow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, -2).shape == (n, n)\n    assert MatPow(C, 0).shape == (n, n)\n    assert MatPow(C, 0).doit() == Identity(n)\n\n    assert isinstance(MatPow(C, S(3)/2), MatPow)\n\n    assert MatPow(C, 2).doit() == C*C\n    assert MatPow(C, -2).doit() == C.I*C.I\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 3).doit() == ZeroMatrix(n, n)\n\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n\n    # regression test\n    assert MatPow(C, 0).doit(deep=False) == Identity(n)\n\n    # test entry\n    assert MatPow(C, 2)._entry(0, 0) == (C*C)._entry(0, 0)\n\n    # test doit with MatMul\n    assert MatPow(MatPow(C, 2), 3).doit() == MatPow(C, 6).doit()\n", "def test_matpow():\n    assert MatPow(C, 2).args == (C, 2)\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 3).doit() == ZeroMatrix(n, n)\n\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2)._entry(1, 1))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -2).doit())\n    raises(TypeError, lambda: MatPow(n, 2))\n"], "sample_1119": ["def test_inv():\n    assert C.inv() == C.I\n    assert (C.T).inv() == (C.inv()).T\n    assert (C.I).inv() == C\n    assert (3 * Identity(n)).inv() == Identity(n) / 3\n    assert Identity(n).inv() == Identity(n)\n    raises(NonSquareMatrixError, lambda: A.inv())\n    raises(NonSquareMatrixError, lambda: (A * B).inv())\n    raises(NonInvertibleMatrixError, lambda: ZeroMatrix(n, n).inv())\n    raises(NonSquareMatrixError, lambda: OneMatrix(n, m).inv())\n    raises(NonInvertibleMatrixError, lambda: OneMatrix(2, 2).inv())\n", "def test_matpow():\n    assert MatPow(C, 0) == Identity(C.rows)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, -2) == Inverse(C)*Inverse(C)\n    assert MatPow(C, 2) == C*C\n\n    assert MatPow(Identity(n), 0) == Identity(n)\n    assert MatPow(Identity(n), 1) == Identity(n)\n    assert MatPow(Identity(n), 2) == Identity(n)\n    assert MatPow(Identity(n), -1) == Identity(n)\n    assert MatPow(Identity(n), -2) == Identity(n)\n\n    assert MatPow(ZeroMatrix(n, n), 0) == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 1) == ZeroMatrix(n, n)\n    assert MatPow(ZeroMatrix(n, n), 2) == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -2))\n\n    assert MatPow(OneMatrix(n, n), 0) == Identity(n)\n    assert MatPow(OneMatrix(n, n), 1) == OneMatrix(n, n)\n    assert MatPow(OneMatrix(n, n), 2) == OneMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(OneMatrix(n, n), -1))\n    raises(NonInvertibleMatrixError, lambda: MatPow(OneMatrix(n, n), -2))\n", "def test_inverse_properties():\n    # Properties of the inverse\n    assert (C.I).I == C\n    assert (C*D).I == D.I*C.I\n    assert (C.T).I == (C.I).T\n    assert (3*C).I == C.I / 3\n    assert (C**-1).I == C\n    assert (C**2).I == (C.I)**2\n", "def test_pow():\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, -1) == Inverse(C)\n    assert MatPow(C, -2) == Inverse(C)*Inverse(C)\n    assert MatPow(C, 2).doit() == C*C\n    assert MatPow(C, 3).doit() == C*C*C\n    assert MatPow(C, -3).doit() == Inverse(C)*Inverse(C)*Inverse(C)\n\n    assert MatPow(eye(3), 0) == Identity(3)\n    assert MatPow(eye(3), 1) == eye(3)\n    assert MatPow(eye(3), -1) == eye(3)\n\n    assert MatPow(ZeroMatrix(2, 2), 0) == Identity(2)\n    assert MatPow(ZeroMatrix(2, 2), 1) == ZeroMatrix(2, 2)\n    assert isinstance(MatPow(ZeroMatrix(2, 2), -1), Inverse)\n", "def test_Inverse_doit():\n    assert Inverse(Identity(n)).doit() == Identity(n)\n    assert Inverse(2*Identity(n)).doit() == Identity(n)/2\n    assert Inverse(MatPow(C, -1)).doit() == C\n    assert Inverse(MatPow(C, 0)).doit() == Identity(n)\n    assert Inverse(MatPow(C, 1)).doit() == C.I\n    assert Inverse(MatPow(C, 2)).doit() == C.I*C.I\n    assert Inverse(MatPow(C, -2)).doit() == C*C\n"], "sample_1120": ["def test_transpose_matrix_element():\n    A = MatrixSymbol('A', 2, 2)\n    assert transpose(A[0, 1]) == A[1, 0]\n", "def test_matrix_symbol_inverse():\n    A = MatrixSymbol('A', n, n)\n    assert A.I.shape == A.shape\n    assert A.I.I == A\n    assert A.I*A == A*A.I == Identity(n)\n    assert (A.I).I == A\n    assert (2*A).I == (1/2)*A.I\n", "def test_generic_identity_doit():\n    I = GenericIdentity()\n    assert I.doit() == I\n", "def test_matrix_applyfunc():\n    from sympy import sin\n    A = MatrixSymbol('A', 2, 2)\n    assert A.applyfunc(sin) == Matrix([\n        [sin(A[0, 0]), sin(A[0, 1])],\n        [sin(A[1, 0]), sin(A[1, 1])]])\n", "def test_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = A + I * B\n    real, imag = C.as_real_imag()\n    assert real == A\n    assert imag == B\n"], "sample_1121": ["def test_Mul_conjugate():\n    a, b = symbols('a b', real=True)\n    z, w = symbols('z w')\n    assert (a*b).conjugate() == a*b\n    assert (a*z).conjugate() == a*z.conjugate()\n    assert (a*w*z).conjugate() == a*w.conjugate()*z.conjugate()\n    assert (a*b*z).conjugate() == a*b*z.conjugate()\n    assert (z*w).conjugate() == z.conjugate()*w.conjugate()\n", "compilation error", "def test_issue_8088():\n    # Test that atan2(0, x) and atan2(0, -x) are correctly\n    # distinguished.\n    a = atan2(0, x)\n    b = atan2(0, -x)\n    assert a.simplify() == 0\n    assert b.simplify() == pi\n    assert a != b\n", "def test_Mul_flatten_hyper_re():\n    from sympy.simplify.hyperexpand import hyperexpand\n    from sympy.functions.special.hyper import hyper\n    from sympy.abc import x\n    # issue 9668\n    assert hyperexpand(hyper((-1, 1), (2,), x)*x*(2 - x)) == \\\n        hyperexpand(hyper((-1, 1), (3,), x)*x*(3 - x))\n", "def test_Mul_flatten_args():\n    a, b = symbols('a b', commutative=False)\n    assert Mul(a, b, a, evaluate=False).args == (a, b, a)\n    assert Mul(a, b, a, evaluate=False).as_ordered_factors() == [a, b, a]\n    assert Mul(a, b, a).args == (a**2, b)\n    assert Mul(a, b, a).as_ordered_factors() == [a**2, b]\n"], "sample_1122": ["def test_polar_lift():\n    from sympy import exp_polar, pi, I, polar_lift, sin\n    x = Symbol('x')\n\n    assert polar_lift(1) == 1\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(2 + 3*I) == 2 + 3*I\n    assert polar_lift((2 + 3*I)*x) == polar_lift(2 + 3*I)*polar_lift(x)\n\n    assert polar_lift(0) == 0\n    assert polar_lift(exp_polar(I*pi)) == exp_polar(I*pi)\n    assert polar_lift(sin(x)) == sin(x)\n    assert polar_lift(polar_lift(x)) == polar_lift(x)\n", "def test_issue_20597():\n    from sympy import symbols, MatrixSymbol\n    x, y = symbols('x y', real=True)\n    A = MatrixSymbol('A', 2, 2)\n    z = x + I*y\n    assert re(A*z).simplify() == A*x\n    assert im(A*z).simplify() == A*y\n", "def test_conjugate_unique_handling():\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n\n    assert conjugate(exp(I*x)) == exp(-I*x)\n    assert conjugate(exp_polar(I*x)) == exp_polar(-I*x)\n    assert conjugate(exp_polar(I*p)) == exp_polar(-I*p)\n", "def test_polar_lift():\n    from sympy import polar_lift, exp_polar, pi, I\n    x = Symbol('x')\n    assert polar_lift(1) == exp_polar(0)\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(exp_polar(I*pi/2)) == exp_polar(I*pi/2)\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(2*x) == 2*polar_lift(x)\n    assert polar_lift(x + 1) == polar_lift(x + 1)\n", "def test_polar_lift():\n    from sympy import polar_lift, exp_polar, I, pi\n\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == exp_polar(0)\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n\n    x = Symbol('x', real=True)\n    assert polar_lift(x) == x*exp_polar(0)\n    assert polar_lift(-x) == x*exp_polar(I*pi)\n\n    x = Symbol('x', positive=True)\n    assert polar_lift(x) == x\n\n    x = Symbol('x', negative=True)\n    assert polar_lift(x) == x*exp_polar(I*pi)\n\n    x = Symbol('x')\n    assert polar_lift(x).conjugate() == polar_lift(x.conjugate())\n    assert polar_lift(x).as_real_imag() == (Abs(x)*cos(arg(x)), Abs(x)*sin(arg(x)))\n"], "sample_1123": ["def test_conditionset_eval_subs():\n    s = Symbol('s', positive=True)\n    n = Symbol('n', negative=True)\n    z = Symbol('z', zero=True)\n    cs1 = ConditionSet(x, x > 0, Interval(-1, 1))\n    cs2 = ConditionSet(x, x < 0, Interval(-1, 1))\n    cs3 = ConditionSet(x, x > 0, Interval(0, 1))\n    cs4 = ConditionSet(x, x < 0, Interval(-1, 0))\n    assert cs1.subs(x, s) == Interval(0, 1)\n    assert cs1.subs(x, n) == S.EmptySet\n    assert cs1.subs(x, z) == S.EmptySet\n    assert cs2.subs(x, s) == S.EmptySet\n    assert cs2.subs(x, n) == Interval(-1, 0)\n    assert cs2.subs(x, z) == S.EmptySet\n    assert cs3.subs(x, s) == Interval(0, 1)\n    assert cs3.subs(x, n) == S.EmptySet\n    assert cs3.subs(x, z) == S.EmptySet\n    assert cs4.subs(x, s) == S.EmptySet\n    assert cs4.subs(x, n) == Interval(-1, 0)\n    assert cs4.subs(x, z) == S.EmptySet\n", "def test_CondSet_with_lambda():\n    C = ConditionSet\n    I = S.Integers\n    c = C(x, x < 1, I)\n    assert c.sym == x\n    assert c.condition == x < 1\n    assert c.base_set == I\n\n    c = C(Lambda(x, x**2), Lambda(x, x**2) < 1, I)\n    assert c.sym == Lambda(x, x**2)\n    assert c.condition == Lambda(x, x**2) < 1\n    assert c.base_set == I\n", "def test_ConditionSet_with_lambda():\n    I = S.Integers\n    C = ConditionSet\n    assert C(Lambda(x, x**2), Lambda(x, x**2) > 4, I) == C(x, x**2 > 4, I)\n    raises(TypeError, lambda: C(Lambda(x, x**2), Lambda(x, x) > 4, I))\n    raises(TypeError, lambda: C(x, Lambda(x, x) > 4, 1))\n", "def test_CondSet_eq():\n    C = ConditionSet\n    I = S.Integers\n    assert C(x, x < 1, I) == C(x, x < 1, I)\n    assert C(x, x < 1, I) != C(x, x < 2, I)\n    assert C(x, x < 1, I) != C(x, x < 1, S.Reals)\n    assert C(x, x < 1, I) != C(y, y < 1, I)\n    assert C(x, x < 1, I) != EmptySet\n    assert C(x, x < 1, I) != Interval(0, 1)\n    assert C(x, x < 1, I) != FiniteSet(1, 2)\n", "def test_conditionset_as_relational():\n    assert ConditionSet(x, x < 1, Interval(0, 1)\n        ).as_relational(0.5) == (x < 1).subs(x, 0.5)\n    assert ConditionSet(x, x < 1, Interval(0, 1)\n        ).as_relational(2) is S.false\n    assert ConditionSet(x, y < 1, Interval(0, 1)\n        ).as_relational(0.5) == (y < 1) & (0 <= 0.5) & (0.5 <= 1)\n    assert ConditionSet(x, x < 1, FiniteSet(1, 2, 3)\n        ).as_relational(1) is S.false\n"], "sample_1124": ["def test_FracElement_sub():\n    F, x,y,z = field(\"x,y,z\", ZZ)\n    f = (x**2 + 3*y)/z\n\n    assert f - f == 0\n    assert f.sub(x, x) == f\n    assert f.sub(y, 0) == x**2/z\n    assert f.sub(z, z) == f\n    assert f.sub(x, 1) == (3*y + 1)/z\n    assert f.sub(y, 1) == (x**2 + 3)/z\n    assert f.sub(z, 1) == x**2 + 3*y\n    assert f.sub(x, x**2) == (x**4 + 3*y)/z\n    assert f.sub(y, y**2) == (x**2 + 3*y**2)/z\n    assert f.sub(z, z**2) == (x**2 + 3*y)/z**2\n", "def test_FracElement_set_field():\n    F, x, y = field(\"x,y\", QQ)\n    G, u, v = field(\"u,v\", QQ)\n\n    f = (x**2 + 3*y)/y\n    g = f.set_field(G)\n\n    assert g == (u**2 + 3*v)/v\n    assert g.field == G\n    assert g.parent() == G.to_domain()\n", "def test_FracElement_set_field():\n    F, x, y = field(\"x,y\", ZZ)\n    G, a, b = field(\"a,b\", ZZ)\n    f = (3*x**2*y - x*y**2)/(7*y**3 + 1)\n    g = f.set_field(G)\n\n    assert g.field == G\n    assert dict(g.numer) == {(2, 1): 3, (1, 2): -1}\n    assert dict(g.denom) == {(0, 3): 7, (0, 0): 1}\n", "def test_FracElement_set_field():\n    F, x, y = field(\"x,y\", ZZ)\n    f = (3*x**2*y - x*y)/(7*y**3 + 1)\n\n    G, X, Y = field(\"x,y\", QQ)\n    g = f.set_field(G)\n    assert g == (3*X**2*Y - X*Y)/(7*Y**3 + 1)\n    assert g.domain == G.domain\n", "def test_FracElement_set_field():\n    F, x,y = field(\"x,y\", ZZ)\n    new_F = FracField(\"x,y\", QQ, lex)\n    f = (7*x - 9)/y\n\n    assert f.set_field(new_F).field == new_F\n    assert f.field != f.set_field(new_F).field\n    assert f == f.set_field(new_F)\n"], "sample_1125": ["def test_hermitian_operator():\n    H = Operator('H', is_hermitian=True)\n    assert Dagger(H) == H\n    assert H.inv() == H**(-1)\n", "def test_differential_operator():\n    from sympy import Function, Symbol, Derivative\n    from sympy.physics.quantum.operator import DifferentialOperator\n    from sympy.physics.quantum.state import Wavefunction\n\n    x = Symbol('x')\n    f = Function('f')\n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n    w = Wavefunction(x**2, x)\n    assert d.variables == (x,)\n    assert d.function == f(x)\n    assert d.expr == Derivative(f(x), x)\n    assert d.free_symbols == {x}\n    assert d._apply_operator_Wavefunction(w) == Wavefunction(2*x, x)\n", "def test_hermitian_operator():\n    H = Operator('H', is_hermitian=True)\n    assert Dagger(H) == H\n    assert H.inv() == H\n    assert H.inv().is_hermitian is True\n", "def test_unitary_operator():\n    U = Operator('U', is_unitary=True)\n    assert Dagger(U) == U.inv()\n    assert U*Dagger(U) == IdentityOperator()\n    assert Dagger(U)*U == IdentityOperator()\n", "def test_hermitian_operator():\n    H = Operator('H', is_hermitian=True)\n    assert Dagger(H) == H\n    assert Dagger(H)*H == H**2\n    assert H*Dagger(H) == H**2\n\n    U = Operator('U', is_unitary=True)\n    assert Dagger(U)*U == IdentityOperator()\n    assert U*Dagger(U) == IdentityOperator()\n"], "sample_1126": ["def test_dagger_add():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A + Dagger(B)) == Dagger(A) + Dagger(Dagger(B))\n    assert Dagger(Dagger(A) + Dagger(B)) == A + B\n", "def test_power():\n    A = Operator('A')\n    assert Dagger(A**3) == Dagger(A)**3\n    assert Dagger(A**0) == Dagger(A)**0\n    assert Dagger(A**-2) == Dagger(A)**-2\n", "def test_dagger_power():\n    O = Operator('O')\n    assert Dagger(O**2) == Dagger(O)**2\n    assert Dagger(O**3) == Dagger(O)**3\n    assert Dagger(O**-2) == Dagger(O)**-2\n    assert Dagger(O**-3) == Dagger(O)**-3\n", "def test_dagger_add():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A + Dagger(B)) == Dagger(A) + B\n    assert Dagger(Dagger(A) + B) == A + Dagger(B)\n    assert Dagger(A + IdentityOperator()) == Dagger(A) + IdentityOperator()\n", "def test_dagger_add():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A + Dagger(B)) == Dagger(A) + Dagger(Dagger(B))\n    assert Dagger(Dagger(A) + Dagger(B)) == Dagger(Dagger(A)) + Dagger(Dagger(B))\n"], "sample_1128": ["def test_auto_point_acc_multiple_point_path():\n    t = dynamicsymbols._t\n    q1, q2 = dynamicsymbols('q1 q2')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, q1 * B.x)\n    P.set_acc(B, q2 * B.y)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * B.z)\n    P3 = Point('P3')\n    P3.set_pos(P2, 10 * q1 * B.y)\n    assert P3.acc(B) == 10 * (q1.diff(t, t) * B.y + 2 * q1.diff(t) * q2.diff(t) * B.x) + \\\n                          (q1.diff(t, t)) * B.z + q2 * B.y\n", "def test_set_acc():\n    N = ReferenceFrame('N')\n    P = Point('P')\n    a = 10 * N.x\n    P.set_acc(N, a)\n    assert P.acc(N) == a\n    a2 = 20 * N.y\n    P.set_acc(N, a2)\n    assert P.acc(N) == a2\n", "def test_auto_point_vel_with_intermediate_point_vel_defined_in_different_frame():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, u1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    O = Point('O')\n    O.set_vel(N, u2 * N.y)\n    P1.set_pos(O, q1 * N.x)\n    P1.set_vel(N, u2 * N.z)\n    raises(ValueError, lambda: P.vel(N)) # Velocity of P cannot be determined because vel of P1 is defined\n    # in different frame\n    assert P1.vel(N) == u2 * N.z\n", "def test_point_vel_with_intermediate_frame():\n    t = dynamicsymbols._t\n    q1, q2, u1 = dynamicsymbols('q1 q2 u1')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    B.set_ang_vel(N, u1 * B.z)\n    P = Point('P')\n    P.set_vel(B, q1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P1.set_vel(N, q1 * N.z + q2 * N.y)\n    assert P1.vel(N) == q1 * N.z + q2 * N.y\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * B.z)\n    assert P2.vel(N) == (q1 * (B.z + N.z) + q2 * N.y + \n                         q2.diff(t) * (B.y ^ (u1 * B.z))) \n", "def test_auto_point_vel_loop_in_tree():\n    t = dynamicsymbols._t\n    q1, q2 = dynamicsymbols('q1 q2')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, q1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P.set_pos(P1, q1 * B.z)\n    raises(ValueError, lambda : P1.vel(B)) # O.vel(N) is not defined, loop in tree\n"], "sample_1127": ["def test_symmetric_permutation_group():\n    G = SymmetricPermutationGroup(5)\n    assert len(G.orbit(0)) == 5\n    assert Permutation(0, 1) in G\n    assert Permutation(0, 4) in G\n    assert G.order() == 120\n    assert G.degree == 5\n    assert Permutation(0, 1, 2, 3, 4) in G\n    assert Permutation(4, 3, 2, 1, 0) in G\n    assert G.identity == Permutation(4)\n    assert G.is_transitive()\n", "def test_symmetric_permutation_group():\n    G = SymmetricPermutationGroup(3)\n    assert G.degree == 3\n    assert G.order() == 6\n    assert G.identity == Permutation(2)\n    assert Permutation(0, 1) in G\n    assert Permutation(0, 1, 2) in G\n    assert Permutation(1, 2) in G\n", "def test_symmetric_permutation_group():\n    G = SymmetricPermutationGroup(5)\n    assert G.degree == 5\n    assert G.order() == 120\n    assert G.identity == Permutation(4)\n", "def test_is_alternating():\n    a = Permutation(0, 1, 2)\n    b = Permutation(0, 1, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 2, 1)\n    b = Permutation(1, 2, size=3)\n    assert PermutationGroup(a, b).is_alternating == True\n\n    a = Permutation(0, 1, 2, 3)\n    b = Permutation(0, 3)(1, 2)\n    assert PermutationGroup(a, b).is_alternating == False\n", "def test_transitivity_degree():\n    # transitivity degree of the trivial group is 1\n    triv = PermutationGroup([Permutation([0])])\n    assert triv.transitivity_degree == 1\n    # transitivity degree of the symmetric group S_n is n\n    for i in range(2, 10):\n        S = SymmetricGroup(i)\n        assert S.transitivity_degree == i\n    # transitivity degree of the alternating group A_n is n-1 for n > 2\n    for i in range(3, 10):\n        A = AlternatingGroup(i)\n        assert A.transitivity_degree == i-1\n"], "sample_1129": ["def test_issue_SymPyPrinter():\n    prntr = SymPyPrinter()\n\n    assert prntr.doprint(S.Pi) == 'sympy.pi'\n    assert prntr.doprint(S.Exp1) == 'sympy.E'\n    assert prntr.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert prntr.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert prntr.doprint(S.Catalan) == 'sympy.Catalan'\n    assert prntr.doprint(S.Infinity) == 'sympy.oo'\n    assert prntr.doprint(S.NegativeInfinity) == '-sympy.oo'\n    assert prntr.doprint(S.NaN) == 'sympy.nan'\n", "def test_Log1p():\n    from sympy import log\n\n    expr1 = log(1 + x)\n    expr2 = log1p(x)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(1 + x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log(1 + x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log(1 + x)'\n    assert prntr.doprint(expr2) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log(mpmath.mpf(1) + x)'\n    assert prntr.doprint(expr2) == 'mpmath.log1p(x)'\n", "compilation error", "def test_sinc():\n    from sympy import sinc\n\n    expr = sinc(x)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.sinc(x/math.pi)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.sinc(x/math.pi)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # sinc\\nsinc(x)'\n", "def test_SymPyPrinter():\n    from sympy import sin, cos\n\n    p = SymPyPrinter()\n\n    assert p.doprint(x**y) == 'x**y'\n    assert p.doprint(Mod(x, 2)) == 'sympy.Mod(x, 2)'\n    assert p.doprint(And(x, y)) == 'sympy.And(x, y)'\n    assert p.doprint(Or(x, y)) == 'sympy.Or(x, y)'\n    assert p.doprint(pi) == 'sympy.pi'\n    assert p.doprint(x**Rational(1, 2)) == 'sympy.sqrt(x)'\n    assert p.doprint(expm1(x)) == 'sympy.expm1(x)'\n    assert p.doprint(log1p(x)) == 'sympy.log1p(x)'\n    assert p.doprint(cosm1(x)) == 'sympy.cosm1(x)'\n    assert p.doprint(sin(x)) == 'sympy.sin(x)'\n    assert p.doprint(cos(x)) == 'sympy.cos(x)'\n"], "sample_1130": ["def test_point_locatenew():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = P1.locatenew('P2', 10 * N.x)\n    assert P2.pos_from(P1) == 10 * N.x\n    assert P1.pos_from(P2) == -10 * N.x\n", "def test_auto_velcandidate_multiple_vels_warning_arises():\n    q, u = dynamicsymbols('q u')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    Q = Point('Q')\n    O.set_vel(N, u * N.x)\n    P.set_vel(N, u *N.y)\n    Q.set_vel(N, u * N.z)\n    P1 = Point('P1')\n    P1.set_pos(O, q * N.z)\n    P1.set_pos(P, q * N.y)\n    P1.set_pos(Q, q * N.x)\n    with warnings.catch_warnings(): \n        warnings.simplefilter(\"error\")\n        raises(UserWarning ,lambda: P1.vel(N))\n", "def test_auto_vel_multiple_candidate_neighbors_warning_msg():\n    P = Point('P')\n    P1 = Point('P1')\n    P2 = Point('P2')\n    P3 = Point('P3')\n    N = ReferenceFrame('N')\n    P.set_vel(N, N.x)\n    P1.set_pos(P, N.x)\n    P2.set_pos(P, N.y)\n    P3.set_pos(P1, N.z)\n    P3.set_pos(P2, N.z)\n    with warnings.catch_warnings(record = True) as w: #Two paths from P to P3, thus a warning is raised\n        warnings.simplefilter(\"always\")\n        P3.vel(N)\n        assert issubclass(w[-1].category, UserWarning)\n        assert 'Velocity automatically calculated based on point P1 but it is also possible from points(s):[P2].' in str(w[-1].message)\n", "def test_auto_vel_point_has_multiple_vels_but_in_different_frames():\n    q, u = dynamicsymbols('q u')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    O.set_vel(N, u * N.x)\n    O.set_vel(B, u * B.x)\n    P.set_pos(O, q * N.x)\n    raises(ValueError, lambda : P.vel(B)) # O's pos from P not defined in frame B\n", "def test_locatenew_invalid_name():\n    N = ReferenceFrame('N')\n    P = Point('P')\n    raises(TypeError, lambda: P.locatenew(1, N.x))\n    raises(TypeError, lambda: P.locatenew(None, N.x))\n    raises(TypeError, lambda: P.locatenew(N, N.x))\n"], "sample_1131": ["def test_lambertw():\n    from sympy import LambertW\n\n    expr = LambertW(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.lambertw(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # LambertW\\nLambertW(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # LambertW\\nLambertW(x)'\n", "def test_lambertw():\n    from sympy import LambertW\n    expr = LambertW(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.lambertw(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # LambertW\\nLambertW(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # LambertW\\nLambertW(x)'\n", "def test_lambertw():\n    from sympy import lambertw\n\n    expr = lambertw(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.lambertw(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # lambertw\\nlambertw(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # lambertw\\nlambertw(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.lambertw(x)'\n", "def test_fresnelcos():\n    from sympy import fresnelc\n\n    expr1 = fresnelc(x)\n    expr2 = fresnelc(x, 2)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.fresnel(x)[1]'\n    assert prntr.doprint(expr2) == 'scipy.special.fresnel(x)[1]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with NumPy:\\n  # fresnelc\\nfresnelc(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python with NumPy:\\n  # fresnelc\\nfresnelc(x, 2)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python:\\n  # fresnelc\\nfresnelc(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python:\\n  # fresnelc\\nfresnelc(x, 2)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.fresnelc(x)'\n    assert prntr.doprint(expr2) == 'mpmath.fresnelc(x, 2)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    expr = acos(x)\n    assert 'sympy' not in p.module_imports\n    assert p.doprint(expr) == 'sympy.acos(x)'\n    assert 'sympy' in p.module_imports\n    smat = SparseMatrix(2, 5, {(0, 1): 3})\n    assert p.doprint(smat) == 'sympy.Matrix([[0, 3, 0, 0, 0], [0, 0, 0, 0, 0]])'\n    assert 'sympy' in p.module_imports\n\n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.Exp1) == 'sympy.E'\n\n    expr = expm1(x)\n    assert p.doprint(expr) == 'sympy.exp(x) - 1'\n    expr = log1p(x)\n    assert p.doprint(expr) == 'sympy.log(x + 1)'\n    expr = loggamma(x)\n    assert p.doprint(expr) == 'sympy.log(sympy.gamma(x))'\n    expr = cosm1(x)\n    assert p.doprint(expr) == 'sympy.cos(x) - 1'\n"], "sample_1132": ["def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([0, 1], [2, 3], [4, 5, 6])) == [\n        0, 2, 4, 1, 3, 5, 6]\n", "def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert list(gen) == ['A', 'D', 'E', 'B', 'F', 'C']\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2, 3], [4, 5], [6, 7, 8, 9])) == [1, 4, 6, 2, 5, 7, 3, 8, 9]\n", "def test_roundrobin():\n    from itertools import islice\n    items = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    expected_result = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    result = list(roundrobin(islice(items, 5), islice(items, 5)))\n    assert result == expected_result\n", "def test_roundrobin():\n    from itertools import cycle\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert ''.join(islice(gen, 3)) == 'ADE'\n    assert ''.join(islice(gen, 3)) == 'BDF'\n    assert ''.join(islice(gen, 3)) == 'CEF'\n    assert ''.join(islice(cycle(gen), 6)) == 'ADEBDF'\n\n    assert ''.join(roundrobin('ABC', 'D', 'EF')) == 'ADEBDFCEF'\n\n    assert ''.join(roundrobin([0, 1], [2, 3])) == '0231'\n    assert ''.join(roundrobin([0, 1], [2, 3], [4])) == '02413'\n"], "sample_1133": ["def test_refraction_angle_TIR():\n    m1 = Medium('m1', n=1.33)\n    m2 = Medium('m2', n=1)\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    angle_of_incidence = critical_angle(m1, m2)\n    raises(ValueError, lambda: refraction_angle(angle_of_incidence + 0.1, m1, m2))\n    assert refraction_angle(angle_of_incidence - 0.1, m1, m2) > 0\n    raises(ValueError, lambda: refraction_angle(r1, m1, m2, plane=P))\n    raises(ValueError, lambda: refraction_angle(1, m1, m2))\n    raises(ValueError, lambda: refraction_angle(-1, m1, m2))\n    raises(ValueError, lambda: refraction_angle(1.5, m1, m2))\n    raises(ValueError, lambda: refraction_angle(0.5, m2, m1, plane=P))\n", "def test_refraction_angle_normal_ray_is_not_concurrent_with_incident_ray():\n    n1, n2 = symbols('n1, n2')\n    m1 = Medium('m1')\n    m2 = Medium('m2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    normal_ray = Ray3D(Point3D(1, 1, 1), Point3D(0, 0, 1))\n    raises(ValueError, lambda: refraction_angle(r1, m1, m2, normal=normal_ray))\n", "def test_refractive_index_of_medium():\n    n1 = Medium('m1', n=1.5)\n    n2 = 2.0\n    assert refractive_index_of_medium(n1) == 1.5\n    assert refractive_index_of_medium(n2) == 2.0\n    raises(TypeError, lambda: refractive_index_of_medium(None))\n", "def test_refraction_angle_angle_of_incidence():\n    assert ae(refraction_angle(0.1, 1, 1.33), 0.075, 3)\n    assert ae(refraction_angle(0.5, 1, 1.33), 0.35, 2)\n    assert ae(refraction_angle(0.8, 1, 1.33), 0.55, 2)\n    raises(ValueError, lambda: refraction_angle(1.57, 1, 1.33))\n    raises(ValueError, lambda: refraction_angle(-0.1, 1, 1.33))\n", "def test_refractive_index_of_medium():\n    n1 = 1.33\n    m1 = Medium('m1', n=n1)\n    assert refractive_index_of_medium(m1) == n1\n    assert refractive_index_of_medium(n1) == n1\n    n2 = symbols('n2')\n    assert refractive_index_of_medium(n2) == n2\n"], "sample_1135": ["def test_Mul_is_algebraic_expr():\n    x = Symbol('x', algebraic=True)\n    y = Symbol('y')\n    assert (2*x).is_algebraic_expr\n    assert (2*y).is_algebraic_expr is None\n    assert (x*y).is_algebraic_expr is None\n    assert (x**2).is_algebraic_expr\n    assert (y**2).is_algebraic_expr is None\n    assert (1/x).is_algebraic_expr\n    assert (1/y).is_algebraic_expr is None\n    assert (1/(x*y)).is_algebraic_expr is None\n", "def test_Mul_hermitian():\n    a = Symbol('a', hermitian=True)\n    b = Symbol('b', antihermitian=True)\n    c = Symbol('c', real=True)\n    d = Symbol('d', imaginary=True)\n\n    assert (a*c).is_hermitian\n    assert (a*d).is_antihermitian\n    assert (b*c).is_antihermitian\n    assert (b*d).is_hermitian\n\n    assert (a*I).is_antihermitian\n    assert (a*(-I)).is_antihermitian\n    assert (b*I).is_hermitian\n    assert (b*(-I)).is_hermitian\n\n    assert (c*I).is_imaginary\n    assert (c*(-I)).is_imaginary\n    assert (d*I).is_real\n    assert (d*(-I)).is_real\n\n    assert ((a + b)*I).is_hermitian is None\n    assert ((a - b)*I).is_hermitian is None\n    assert ((a + b)*(-I)).is_hermitian is None\n    assert ((a - b)*(-I)).is_hermitian is None\n", "def test__keep_coeff():\n    a, b, c, d = symbols('a b c d')\n    e = (3 + sqrt(2))**(2*a + 3*b - 4*c - d)\n    assert _keep_coeff(e.coeff, e._args, sign=True).is_Mul\n", "def test_issue_20314():\n    a, b, c, d = symbols('a b c d')\n    expr = a*b*c*d\n    result = expr.as_coeff_Mul()\n    assert result[0] == 1 and result[1] == a*b*c*d\n    result = expr.as_coeff_Mul(rational=True)\n    assert result[0] == 1 and result[1] == a*b*c*d\n", "def test_Mul_hermitian():\n    a, b, c = symbols('a b c')\n    assert (a*b).is_hermitian is None\n    assert (a*b).is_antihermitian is None\n    assert (a*(b + b.conjugate())).is_hermitian is None\n    assert (a*(b - b.conjugate())).is_antihermitian is None\n\n    # See issue #7689\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n\n    assert (x * (y - I * z)).is_hermitian is None\n    assert (x * (y + I * z)).is_hermitian is None\n    assert (x * y * z).is_hermitian is True\n    assert (x * y * (z + I)).is_antihermitian is False\n    assert (x * y * (z - I)).is_antihermitian is False\n"], "sample_1134": ["def test_issue_18788():\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices, TensorHead\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    i0 = tensor_indices(\"i_0\", L)\n    A = TensorHead(\"A\", [L])\n    expr = PartialDerivative(A(i), A(i))\n    assert latex(expr) == r\"\\frac{\\partial}{\\partial {A{}^{L_{0}}}}{A{}^{L_{0}}}\"\n", "def test_latex_EmptySet():\n    assert latex(S.EmptySet) == r\"\\emptyset\"\n", "def test_emptyPrinter():\n    from sympy.printing.latex import LatexPrinter\n    printer = LatexPrinter()\n    assert printer.emptyPrinter(\"foo\") == r\"\\mathtt{\\text{<foo>}}\"\n", "def test_latex_mixed_numbers():\n    from sympy import Rational\n    assert latex(Rational(3, 4)) == r'\\frac{3}{4}'\n    assert latex(Rational(5, 4)) == r'\\frac{5}{4}'\n    assert latex(Rational(3, 4), fold_short_frac=True) == '3 / 4'\n    assert latex(Rational(5, 4), fold_short_frac=True) == '5 / 4'\n", "def test_tr():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    from sympy import trace\n    assert latex(2*trace(A)) == r'2 \\operatorname{tr}\\left(A\\right)'\n    assert latex(trace(2*A)) == r'\\operatorname{tr}\\left(2 A\\right)'\n    assert latex(trace(A**2)) == r'\\operatorname{tr}\\left(A^{2}\\right)'\n"], "sample_1136": ["def test_issue_19589():\n    f = x**2 + x*y + y**2 + x*z + y*z + z**2\n    g = x*y - x*z - y*z\n    F = Poly(f, x, y, z)\n    G = Poly(g, x, y, z)\n\n    assert F % G == Poly(x**2 + x*y + x*z + y**2 + y*z + z**2, x, y, z)\n", "def test_issue_19795():\n    p = Poly(x**2 + x + 1, x, modulus=3)\n    assert p.as_expr() == x**2 + x + 1\n", "def test_issue_20691():\n    f = Poly(x**3 - sqrt(2)*x**2 + x + sqrt(2), x, domain='EX')\n    g = Poly(x**2 + 1, x, domain='EX')\n    res = div(f, g, order='lex')\n    assert res[0] == Poly(x - sqrt(2), x, domain='EX')\n    assert res[1] == Poly(-x + 2*sqrt(2), x, domain='EX')\n    assert f == res[0]*g + res[1]\n", "def test_Poly_from_poly_unify():\n    f = Poly(x**2 + 1, x, domain='ZZ')\n    g = Poly(x**2 + 2*x + 1, x, domain='QQ')\n\n    F, G = f.unify(g)\n    assert Poly(f, g).rep == G.rep\n\n", "def test_issue_19535():\n    assert Poly(sqrt(2) + 2, x).coeff_monomial(1) == sqrt(2) + 2\n"], "sample_1137": ["def test_quantity_simplify():\n    assert quantity_simplify(kilo*foot*inch) == 250*foot**2/3\n    assert quantity_simplify(foot - 6*inch) == foot/2\n    assert quantity_simplify(foot - 2*foot) == -foot\n    assert quantity_simplify(foot + 2*foot) == 3*foot\n    assert quantity_simplify(2*foot - foot) == foot\n", "def test_check_dimensions():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    assert check_dimensions(u + v) == u + v\n    assert check_dimensions(u - v) == u - v\n    raises(ValueError, lambda: check_dimensions(u + w))\n    raises(ValueError, lambda: check_dimensions(u - w))\n    raises(ValueError, lambda: check_dimensions(u + 1))\n    raises(ValueError, lambda: check_dimensions(u - 1))\n    raises(ValueError, lambda: check_dimensions(1 - exp(u / w)))\n    assert check_dimensions(u * w) == u * w\n    assert check_dimensions(u / w) == u / w\n", "def test_quantity_simplify():\n    from sympy.physics.units import foot, inch\n    from sympy.physics.units.prefixes import kilo\n    from sympy.physics.units.util import quantity_simplify\n\n    assert quantity_simplify(kilo*foot*inch) == 250*foot**2/3\n    assert quantity_simplify(foot - 6*inch) == foot/2\n", "def test_check_dimensions():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    assert check_dimensions(u + v) == u + v\n    assert check_dimensions(u - v) == u - v\n    assert check_dimensions(u * v) == u * v\n    assert check_dimensions(u / v) == u / v\n\n    raises(ValueError, lambda: check_dimensions(u + w))\n    raises(ValueError, lambda: check_dimensions(u - w))\n    raises(ValueError, lambda: check_dimensions(u + 1))\n    raises(ValueError, lambda: check_dimensions(u - 1))\n    raises(ValueError, lambda: check_dimensions(1 - exp(u / w)))\n", "def test_check_dimensions():\n    q = Quantity(\"q1\")\n    q.set_global_relative_scale_factor(S(5000), meter)\n\n    assert check_dimensions(q + 1) == q + 1\n    raises(ValueError, lambda: check_dimensions(q + second))\n    raises(ValueError, lambda: check_dimensions(q - second))\n    raises(ValueError, lambda: check_dimensions(1 - exp(q / second)))\n"], "sample_1138": ["def test_as_f_sign_1():\n    assert as_f_sign_1(x + 1) == (1, x, 1)\n    assert as_f_sign_1(x - 1) == (1, x, -1)\n    assert as_f_sign_1(-x + 1) == (-1, x, -1)\n    assert as_f_sign_1(-x - 1) == (-1, x, 1)\n    assert as_f_sign_1(2*x + 2) == (2, x, 1)\n    assert as_f_sign_1(1) == None\n    assert as_f_sign_1(x) == None\n    assert as_f_sign_1(x + x) == None\n", "def test_TR22():\n    assert TR22(sin(x)**2/cos(x)**2) == sec(x)**2 - 2\n    assert TR22(cos(x)**2/sin(x)**2) == csc(x)**2 - 2\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22((sin(x)**2/cos(x)**2)**2) == (sec(x)**2 - 1)**2\n    assert TR22((cos(x)**2/sin(x)**2)**2) == (csc(x)**2 - 1)**2\n", "def test_as_f_sign_1():\n    assert as_f_sign_1(x + 1) == (1, x, 1)\n    assert as_f_sign_1(x - 1) == (1, x, -1)\n    assert as_f_sign_1(-x + 1) == (-1, x, -1)\n    assert as_f_sign_1(-x - 1) == (-1, x, 1)\n    assert as_f_sign_1(2*x + 2) == (2, x, 1)\n    assert as_f_sign_1(2*x - 2) == (2, x, -1)\n    assert as_f_sign_1(0) is None\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n", "def test_TR22():\n    assert TR22(tan(x)**2) == sec(x)**2 - 1\n    assert TR22(cot(x)**2) == csc(x)**2 - 1\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(tan(x)**4) == (sec(x)**2 - 1)**2\n    assert TR22(cot(x)**4) == (csc(x)**2 - 1)**2\n"], "sample_1139": ["def test_ComplexRegion_polar_to_rectangular():\n    from sympy import Interval, pi, sin, cos, I\n\n    r = symbols('r', real=True)\n    theta = symbols('theta', real=True)\n\n    # Simple conversion\n    rectangular_region = ComplexRegion(Interval(0, 1)*Interval(0, pi/2), polar=True)\n    assert rectangular_region.polar == True\n    assert rectangular_region._psets == (ProductSet(Interval(0, 1), Interval(0, pi/2)),)\n\n    # More complex conversion\n    region = Interval(1, 2)*Interval(pi/4, pi/2)\n    polar_region = ComplexRegion(region, polar=True)\n    x_expr = r*cos(theta)\n    y_expr = r*sin(theta)\n    new_x_range = Interval(x_expr.subs({r: 1, theta: pi/4}), x_expr.subs({r: 2, theta: pi/4}))\n    new_y_range = Interval(y_expr.subs({r: 1, theta: pi/4}), y_expr.subs({r: 2, theta: pi/2}))\n    assert polar_region.polar == True\n    assert polar_region._psets == (ProductSet(new_x_range, new_y_range),)\n\n    # Test real set input\n    real_set = Interval(1, 2)\n    complex_region = ComplexRegion.from_real(real_set)\n    assert complex_region == CartesianComplexRegion(real_set * FiniteSet(0))\n", "def test_Range_slice_iter():\n    r = Range(10)\n    assert list(r[:5]) == [0, 1, 2, 3, 4]\n    assert list(r[5:]) == [5, 6, 7, 8, 9]\n    assert list(r[::-1]) == [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n    assert list(r[1:8:2]) == [1, 3, 5, 7]\n    assert list(r[8:1:-2]) == [8, 6, 4, 2]\n", "def test_imageset_reversed():\n    from sympy.abc import n\n    assert imageset(Lambda(n, -n), S.Integers).reversed == imageset(Lambda(n, n), S.Integers)\n    assert imageset(Lambda(n, n + 1), S.Integers).reversed == imageset(Lambda(n, -n - 1), S.Integers)\n    assert imageset(Lambda(n, -n), S.Naturals).reversed == imageset(Lambda(n, n), S.Naturals0)\n    assert imageset(Lambda(n, n), S.Naturals).reversed == imageset(Lambda(n, -n), S.Naturals0)\n", "def test_image_measure():\n    from sympy.abc import n\n    f1 = ImageSet(Lambda(n, n*pi), S.Integers)\n    assert f1.measure == oo\n    f2 = ImageSet(Lambda(n, 2*n), Interval(0, pi))\n    assert f2.measure == 2*pi\n    f3 = ImageSet(Lambda(n, 2*n*pi + pi/2), S.Integers)\n    assert f3.measure == oo\n    f4 = ImageSet(Lambda(n, n*I*pi), S.Integers)\n    assert f4.measure == oo\n    f5 = ImageSet(Lambda(n, 2*I*n*pi + pi/2), S.Integers)\n    assert f5.measure == oo\n    f6 = ImageSet(Lambda(n, log(n)), S.Integers)\n    assert f6.measure == oo\n    f7 = ImageSet(Lambda(n, n**2), S.Integers)\n    assert f7.measure == oo\n    f8 = ImageSet(Lambda(n, Abs(n)), S.Integers)\n    assert f8.measure == oo\n    f9 = ImageSet(Lambda(n, exp(n)), S.Naturals0)\n    assert f9.measure == oo\n", "def test_issue_18513():\n    assert Range(1, 7, 2).contains(x) == Contains(x, Range(1, 7, 2), evaluate=False)\n    assert Range(1, 7, 2).contains(10) == False\n    assert Range(1, 7, 2).contains(3) == True\n    assert Range(1, 7, 2).contains(x + 1) == Contains(x + 1, Range(1, 7, 2), evaluate=False)\n"], "sample_1141": ["def test_issue_22754():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = (A*B)*(B*A)\n    expanded_expr = expand(expr)\n    expected_expr = A*B*A*B\n    assert expanded_expr == expected_expr\n", "def test_expr_as_coefficients_dict():\n    A = MatrixSymbol('A', 2, 2)\n    expr = 3*A + 2*A\n    d = expr.as_coefficients_dict()\n    assert d[A] == 5\n", "def test_expr_builders():\n    from sympy.core.expr import ExprBuilder, AtomicExpr\n\n    a, b = symbols('a b')\n\n    # Ensure AtomicExpr can't have any args\n    raises(ValueError, lambda: AtomicExpr(a))\n\n    # Ensure ExprBuilder can't be instantiated with an invalid op\n    raises(TypeError, lambda: ExprBuilder(1, args=[a, b]))\n\n    # Ensure ExprBuilder recreate an expression after modification\n    eb = ExprBuilder(Add, args=[a, b])\n    eb.append_argument(c)\n    assert eb.build() == a + b + c\n\n    # Ensure ExprBuilder recreate an expression after modification\n    eb = ExprBuilder(Mul, args=[a, b])\n    eb.args[0] = c\n    assert eb.build() == c * b\n\n    # Ensure ExprBuilder recreate an expression after modification with validation\n    eb = ExprBuilder(Add, args=[a, b], validator=lambda *args: None)\n    eb.append_argument(c, check=False)\n    assert eb.build(check=False) == a + b + c\n    raises(TypeError, lambda: eb.build(check=True))\n\n    # Ensure ExprBuilder search for an element\n    eb = ExprBuilder(Mul, args=[a, b])\n    assert eb.search_element(c) is None\n    assert eb.search_element(a) == (0,)\n\n    eb = ExprBuilder(Add, args=[a, b, Mul(c, d)])\n    assert eb.search_element(d) == (2, 1)\n", "def test_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.as_real_imag() == (re(A), im(A))\n    assert A.as_real_imag() == (A.as_real_imag()[0], A.as_real_imag()[1])\n", "def test_Atom_is_commutative():\n    assert S.Half.is_commutative\n    assert pi.is_commutative\n    assert S.Pi.is_commutative\n    assert (1/S(2)).is_commutative\n"], "sample_1140": ["def test_pretty_cot():\n    expr = cot(x)\n    ascii_str = \\", "def test_pretty_Subs():\n    x, y, z = symbols('x y z')\n    expr = Subs(f(x), x, ph**2)\n    ascii_str = \\", "def test_hadron_tensor():\n    from sympy.tensor.tensor import TensorHead\n    L = TensorIndexType(\"L\")\n    j, k, l, m = tensor_indices(\"j k l m\", L)\n\n    i0, i1, i2, i3 = tensor_indices(\"i0:4\", L)\n    A = TensorHead(\"A\", [L])\n    B = TensorHead(\"B\", [L])\n    C = TensorHead(\"C\", [L])\n    E = TensorHead(\"E\", [L])\n    F = TensorHead(\"F\", [L])\n    Akm = A(k, m)\n    Ekm = E(k, m)\n    Fik = F(i0, k)\n    Bjm = B(-j, -m)\n    Cjl = C(-j, -l)\n    assert pretty(Akm) == \"A_{km}\"\n    assert pretty(Ekm) == \"E_{km}\"\n    assert pretty(Fik) == \"F^{ik}\"\n    assert pretty(Cjl) == \"C^{jl}\"\n    assert pretty(Bjm) == \"B^{jm}\"\n", "def test_pretty_complex_conjugate():\n    expr = x - conjugate(x)\n    ucode_str = \\", "def test_pretty_MatMul():\n    a, b, c, d = symbols('a b c d')\n\n    # test matrix-like arguments\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    expr = X*Y\n    ascii_str = \\"], "sample_1142": ["def test_MatrixExpr_from_index_summation():\n    from sympy import Sum, symbols, Dummy\n    A = MatrixSymbol(\"A\", n, m)\n    B = MatrixSymbol(\"B\", m, n)\n    i, j, k = symbols(\"i j k\")\n    expr = Sum(A[i, j]*B[j, k], (j, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n\n    expr = Sum(A[j, i]*B[j, k], (j, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A.T*B\n\n    expr = Sum(A[i, i], (i, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A.trace()\n\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, m-1), (k, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B.T*A.T\n", "def test_matrix_derivative():\n    from sympy.tensor.array.array_derivatives import ArrayDerivative\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    X = MatrixSymbol('X', n, n)\n\n    raises(NotImplementedError, lambda: _matrix_derivative(A, B))\n    raises(NotImplementedError, lambda: _matrix_derivative(A, B.T))\n\n    assert _matrix_derivative(A, X) == ZeroMatrix(n, n)\n    assert _matrix_derivative(B.T*X*B, X) == B.T*B\n    assert _matrix_derivative(B.T*X*B, B) == Matrix([\n        [B[0, 0]*X[0, 0] + B[1, 0]*X[1, 0], B[0, 0]*X[0, 1] + B[1, 0]*X[1, 1]],\n        [B[0, 1]*X[0, 0] + B[1, 1]*X[1, 0], B[0, 1]*X[0, 1] + B[1, 1]*X[1, 1]],\n    ])\n\n    assert _matrix_derivative(A*X*A.T, X) == A.T*A\n    assert _matrix_derivative(X**2, X) == Matrix([[2*X[0, 0], 2*X[0, 1]], [2*X[1, 0], 2*X[1, 1]]])\n\n    # Higher dimensional arrays:\n    from sympy.tensor.array.array_derivatives import ArrayDerivative\n    from ...tensor.array.expressions.array_expressions import ArrayTensorProduct\n    from ...tensor.array.expressions.array_expressions import ArrayContraction\n    from ...tensor.array.expressions.conv_array_to_matrix import convert_array_to_matrix\n    from sympy.core.expr import ExprBuilder\n\n    assert isinstance(convert_array_to_matrix(_matrix_derivative(A*X*B, B)), MatrixExpr)\n    assert _matrix_derivative(A*X*B, B).rank() == 2\n    assert ArrayDerivative(A*X*B, B).rank() == 3\n\n    assert isinstance(convert_array_to_matrix(_matrix_derivative(A*X*B, B)), MatrixExpr)\n    assert _matrix_derivative(A*X*B, B).rank() == 2\n    assert ArrayDerivative(A*X*B, B).rank() == 3\n\n   ", "def test_matrix_derivative():\n    from sympy import sin, cos, exp\n    A = MatrixSymbol('A', n, n)\n    x = symbols('x')\n\n    # Check scalar derivative\n    f = x**2\n    assert _matrix_derivative(f, x) == 2*x\n\n    # Check derivative with respect to matrix element\n    f = A[0, 1]**2\n    assert _matrix_derivative(f, A[0, 1]) == 2*A[0, 1]\n\n    # Check matrix derivative\n    f = A**2\n    assert _matrix_derivative(f, A) == 2*A\n\n    # Check matrix derivative with function\n    f = sin(A)\n    assert _matrix_derivative(f, A) == cos(A)\n", "def test_MatrixExpr_from_index_summation():\n    from sympy.abc import i, j, k\n    from sympy import Sum\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    C = MatrixSymbol(\"C\", 3, 3)\n\n    expr1 = Sum(A[i, j]*B[j, k], (j, 0, 2))\n    assert MatrixExpr.from_index_summation(expr1) == A*B\n\n    expr2 = Sum(A[j, i]*B[j, k], (j, 0, 2))\n    assert MatrixExpr.from_index_summation(expr2) == A.T*B\n\n    expr3 = Sum(A[i, i], (i, 0, 2))\n    assert MatrixExpr.from_index_summation(expr3) == A.trace()\n\n    expr4 = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, 2), (k, 0, 2))\n    assert MatrixExpr.from_index_summation(expr4) == A*B.T*A.T\n", "def test_from_index_summation():\n    from sympy import Sum, symbols\n    from sympy.abc import i, j, k, l, m, n\n\n    A = MatrixSymbol(\"A\", n, m)\n    B = MatrixSymbol(\"B\", m, l)\n    C = MatrixSymbol(\"C\", n, l)\n    D = MatrixSymbol(\"D\", l, n)\n\n    expr = Sum(A[i, j]*B[j, k], (j, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B\n\n    expr = Sum(A[j, i]*B[j, k], (j, 0, m-1))\n    assert MatrixExpr.from_index_summation(expr) == A.T*B\n\n    expr = Sum(A[i, j]*B[j, k]*C[i, k], (j, 0, m-1), (k, 0, l-1), (i, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B*C.T\n\n    expr = Sum(A[i, j]*B[j, k]*D[k, i], (j, 0, m-1), (k, 0, l-1), (i, 0, n-1))\n    assert MatrixExpr.from_index_summation(expr) == A*B*D\n"], "sample_1143": ["def test_Rational_round():\n    assert Rational(10, 7).round() == 1\n    assert Rational(16, 7).round() == 2\n    assert Rational(15, 7).round() == 2\n    assert Rational(21, 7).round() == 3\n", "def test_issue_14779():\n    # test the _as_mpf_val method for odd and even integers\n    assert Integer(10)._as_mpf_val(50) == mpmath.mp.make_mpf((0, 1310720, -19, 50))\n    assert Integer(-10)._as_mpf_val(50) == mpmath.mp.make_mpf((1, 1310720, -19, 50))\n    assert Integer(11)._as_mpf_val(50) == mpmath.mp.make_mpf((0, 1376256, -19, 50))\n    assert Integer(-11)._as_mpf_val(50) == mpmath.mp.make_mpf((1, 1376256, -19, 50))\n", "def test_Float_hash():\n    f1 = Float(1.23)\n    f2 = Float(1.23)\n    assert hash(f1) == hash(f2)\n    f3 = Float(1.24)\n    assert hash(f1) != hash(f3)\n", "def test_mpf_to_Rational():\n    # Test conversion of mpf to Rational.\n    assert Rational(mpf('0.1')) == Rational(1, 10)\n    assert Rational(mpf('0.5')) == Rational(1, 2)\n    assert Rational(mpf('3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679')) == Rational(245850922/78256779)\n    # Test large mpf (issue #21555)\n    assert Rational(mpf('1.' + '0'*100 + '1') * 10**100).p == 10**100 + 1\n", "def test_issue_2885():\n    assert comp(1.0, 1) is True\n    assert comp(1, 1.0) is True\n    assert comp(1, 1) is True\n    assert comp(1.0, 1.0) is True\n"], "sample_1145": ["def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[0, 0], Q.symmetric(X)) == X[0, 0]\n    assert refine(X[0, 1], Q.antisymmetric(X)) != X[0, 1]\n", "def test_refine_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n", "def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 1], Q.symmetric(X)) == X[1, 1]\n    assert refine(X[0, 1] + X[1, 0], Q.symmetric(X)) == 2*X[0, 1]\n", "def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 1], Q.symmetric(X)) == X[1, 1]\n    assert refine(X[0, 0], Q.symmetric(X)) == X[0, 0]\n    Y = MatrixSymbol('Y', 2, 2)\n    assert refine(Y[0, 1], Q.symmetric(Y)) == Y[0, 1]\n    assert refine(Y[1, 0], Q.symmetric(Y)) == Y[0, 1]\n", "def test_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[0, 0], Q.symmetric(X)) == X[0, 0]\n    assert refine(X[1, 1], Q.symmetric(X)) == X[1, 1]\n    assert refine(X[2, 2], Q.symmetric(X)) == X[2, 2]\n"], "sample_1144": ["def test_requires_partial_derivative():\n    x, y, z, t, nu = symbols('x y z t nu')\n    n = symbols('n', integer=True)\n\n    f = x * y\n    assert requires_partial(Derivative(f, x, y)) is True\n    assert requires_partial(Derivative(f, y, x)) is True\n    assert requires_partial(Derivative(f, x, x)) is False\n    assert requires_partial(Derivative(f, y, y)) is False\n\n    f = x ** n\n    assert requires_partial(Derivative(f, x, x)) is False\n    assert requires_partial(Derivative(f, x, n)) is False\n\n    f = symbols('f', cls=Function)\n    assert requires_partial(Derivative(f(x, y), x, y)) is True\n    assert requires_partial(Derivative(f(x, y), y, x)) is True\n    assert requires_partial(Derivative(f(x, y), x, x)) is False\n    assert requires_partial(Derivative(f(x, y), y, y)) is False\n", "def test_requires_partial_edge_cases():\n    x, y = symbols('x y')\n\n    # test requires_partial with a non-Derivative object\n    assert requires_partial(x) is False\n\n    # test requires_partial with a Derivative object that has no free symbols\n    f = Derivative(5, x)\n    assert requires_partial(f) is False\n\n    # test requires_partial with a Derivative object that has no variables\n    f = Derivative(x, evaluate=False)\n    assert requires_partial(f) is False\n\n    # test requires_partial with a Derivative object that has a single variable\n    f = Derivative(x, x)\n    assert requires_partial(f) is False\n\n    # test requires_partial with a Derivative object that has multiple variables\n    f = Derivative(x*y, x, y)\n    assert requires_partial(f) is True\n", "def test_requires_partial_free_symbols():\n    x, y, z = symbols('x y z')\n    f = x * y\n    assert requires_partial(Derivative(f, x)) is True\n    assert requires_partial(Derivative(f, y)) is True\n    assert requires_partial(Derivative(f, z)) is True\n    f = x ** 2\n    assert requires_partial(Derivative(f, x)) is False\n    assert requires_partial(Derivative(f, y)) is False\n    assert requires_partial(Derivative(f, z)) is False\n", "def test_requires_partial_free_symbols():\n    x, y, z = symbols('x y z')\n    f = x * y\n    assert requires_partial(Derivative(f, x, y)) is True\n\n    g = x ** 2\n    assert requires_partial(Derivative(g, x, x)) is False\n\n    h = x * y * z\n    assert requires_partial(Derivative(h, x)) is True\n    assert requires_partial(Derivative(h, y)) is True\n    assert requires_partial(Derivative(h, z)) is True\n", "def test_split_super_sub_edge_cases():\n    assert split_super_sub(\"a^\") == (\"a\", [\"\",], [])\n    assert split_super_sub(\"a__\") == (\"a\", [\"\",], [])\n    assert split_super_sub(\"_a\") == (\"\", [], [\"a\"])\n    assert split_super_sub(\"^a\") == (\"\", [\"a\"], [])\n    assert split_super_sub(\"__a\") == (\"\", [\"a\"], [])\n    assert split_super_sub(\"a_\") == (\"a\", [], [\"\"])\n    assert split_super_sub(\"\") == (\"\", [], [])\n    assert split_super_sub(\"a\") == (\"a\", [], [])\n    assert split_super_sub(\"a1\") == (\"a\", [], [\"1\"])\n"], "sample_1148": ["def test_applyfunc():\n    A = MatrixSymbol('A', 2, 2)\n    f = Function('f')\n    assert A.applyfunc(f) == Matrix([\n        [f(A[0, 0]), f(A[0, 1])],\n        [f(A[1, 0]), f(A[1, 1])]])\n    B = Matrix([[1, 2], [3, 4]])\n    assert B.applyfunc(f) == Matrix([\n        [f(1), f(2)],\n        [f(3), f(4)]])\n    assert A.applyfunc(f).applyfunc(sin) == Matrix([\n        [sin(f(A[0, 0])), sin(f(A[0, 1]))],\n        [sin(f(A[1, 0])), sin(f(A[1, 1]))]])\n", "def test_as_coeff_Mul():\n    assert A.as_coeff_Mul()[0] == 1\n    assert A.as_coeff_Mul()[1] == A\n    assert (2*A).as_coeff_Mul()[0] == 2\n    assert (2*A).as_coeff_Mul()[1] == A\n", "def test_matrix_derivative():\n    from sympy.tensor.array.array_derivatives import ArrayDerivative\n    from sympy import sin, cos\n    x = Symbol('x')\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    X = MatrixSymbol('X', 3, 3)\n\n    expr1 = sin(x)*A\n    expr2 = cos(x)*B\n    expr = expr1*expr2\n    res = _matrix_derivative(expr, X)\n    assert res == ZeroMatrix(3, 3)\n\n    expr = sin(x)*X\n    res = _matrix_derivative(expr, X)\n    assert res == sin(x)*Identity(3)\n", "def test_MatrixElement_applyfunc():\n    A = MatrixSymbol('A', 2, 2)\n    X = Matrix([[1, 2], [3, 4]])\n    assert A[0, 0].applyfunc(sin).subs(A, X) == sin(1)\n", "def test_matrix_derivative():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n    f = x**2\n    assert MatrixExpr._matrix_derivative(f, x) == 2*x\n    assert MatrixExpr._matrix_derivative(A, A[0, 0]) == ZeroMatrix(2, 2)\n    raises(NotImplementedError, lambda: MatrixExpr._matrix_derivative(A, A))\n"], "sample_1149": ["def test_singleton_access():\n\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n\n    # Test accessing singleton instance through S\n    assert S.MySingleton is MySingleton()\n\n    # Test KeyError if attribute is not installed\n    try:\n        S.NonExistent\n        assert False, \"Expected AttributeError\"\n    except AttributeError:\n        pass\n\n    # Test calling S as a function\n    assert S(1) is Rational(1, 1)\n", "def test_Singleton_registry():\n    class MySingleton1(Basic, metaclass=Singleton):\n        pass\n\n    class MySingleton2(Basic, metaclass=Singleton):\n        pass\n\n    assert S.MySingleton1 is MySingleton1()\n    assert S.MySingleton2 is MySingleton2()\n    assert S.MySingleton1 is not S.MySingleton2\n    del S.MySingleton1\n    del S.MySingleton2\n\n    # Ensure that S can still access them even after they've been deleted\n    assert S.MySingleton1 is MySingleton1()\n    assert S.MySingleton2 is MySingleton2()\n", "def test_singleton_repr_and_access():\n    assert repr(S) == \"S\"\n    assert S(1) == 1\n    assert S(1) is S.One\n    assert S(1)/2 == Rational(1, 2)\n    assert S(1)/2 is not S.Half\n    assert S(1)/2 == S.Half\n\n    # Check for error when attribute does not exist\n    try:\n        S.NonExistent\n        assert False\n    except AttributeError:\n        assert True\n", "def test_Singleton_repr_and_access():\n\n    assert repr(S) == \"S\"\n    assert S(1) == 1\n    assert S(1) is S.One\n    assert S('1/2') == Rational(1, 2)\n    assert S('1/2') is not S.One\n\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert MySingleton() is S.MySingleton\n    assert MySingleton() is S('MySingleton')\n", "def test_singleton_with_args():\n    class MySingleton(Basic, metaclass=Singleton):\n            self.value = value\n\n    MySingleton('test')  # force instantiation\n    assert MySingleton() is MySingleton('other')\n    assert S.MySingleton is MySingleton()\n    assert S.MySingleton.value == 'test'\n"], "sample_1147": ["def test_latex_Quaternion_inverse():\n    q = Quaternion(x, y, z, t)\n    assert latex(q**-1) == r\"\\frac{1}{x^{2} + y^{2} + z^{2} + t^{2}}\\left(t - x i - y j - z k\\right)\"\n", "def test_latex_decimal_separator_with_full_prec():\n    x, y, z, t = symbols('x y z t')\n    k, m, n = symbols('k m n', integer=True)\n    f, g, h = symbols('f g h', cls=Function)\n\n    # comma decimal_separator\n    assert(latex([1, 2.3, 4.5], decimal_separator='comma', full_prec=True) == r'\\left[ 1; \\  2{,}3000000000000000444089209850062616169452667236328; \\  4{,}4999999999999991118215802998747676610946651567947\\right]')\n    assert(latex(FiniteSet(1, 2.3, 4.5), decimal_separator='comma', full_prec=True) == r'\\left\\{1; 2{,}3000000000000000444089209850062616169452667236328; 4{,}4999999999999991118215802998747676610946651567947\\right\\}')\n    assert(latex((1, 2.3, 4.6), decimal_separator='comma', full_prec=True) == r'\\left( 1; \\  2{,}3000000000000000444089209850062616169452667236328; \\  4{,}5999999999999991559779423717576196232651554251285\\right)')\n    assert(latex((1,), decimal_separator='comma', full_prec=True) == r'\\left( 1;\\right)')\n\n    # period decimal_separator\n    assert(latex([1, 2.3, 4.5], decimal_separator='period', full_prec=True) == r'\\left[ 1, \\  2.3000000000000000444089209850062616169452667236328, \\  4.4999999999999991118215802998747676610946651567947\\right]')\n    assert(latex(FiniteSet(1, 2.3, 4.5), decimal_separator='period', full_prec=True) == r'\\left\\{1, 2.3000000000000000444089209850062616169452667236328, 4.499999999999999111821580299874", "def test_latex_issue_21218():\n    from sympy import Symbol, MatrixSymbol\n    from sympy.printing.latex import LatexPrinter\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    x = Symbol(\"x\")\n\n    assert LatexPrinter().doprint(A.diff(x)) == r\"\\frac{\\partial}{\\partial x} \\mathbf{A}\"\n", "def test_transpose():\n    A = MatrixSymbol('A', 3, 3)\n    assert latex(A.T) == r\"A^{T}\"\n", "def test_ConditionSet():\n    from sympy.solvers.inequalities import solve_univariate_inequality\n    from sympy.solvers.solvers import solve\n    from sympy.logic.boolalg import And, Or\n    x, y = symbols('x y')\n    conditionset1 = ConditionSet(x**2, (x >= 0) & (x < 5))\n    conditionset2 = ConditionSet(x**2, (x >= 1) & (x < 5))\n    conditionset3 = ConditionSet(1, x > 0)\n    conditionset4 = ConditionSet(x, x < 0)\n    conditionset5 = ConditionSet(x*y, x > 0)\n    conditionset6 = ConditionSet(x*y, x < 0)\n    conditionset7 = ConditionSet(x*y, Or(x > 0, x < 0))\n    assert latex(conditionset1) == r\"\\left\\{x^{2}\\; \\middle|\\; x \\geq 0 \\wedge x < 5 \\right\\}\"\n    assert latex(conditionset2) == r\"\\left\\{x^{2}\\; \\middle|\\; x \\geq 1 \\wedge x < 5 \\right\\}\"\n    assert latex(conditionset3) == r\"\\left\\{1\\; \\middle|\\; x > 0 \\right\\}\"\n    assert latex(conditionset4) == r\"\\left\\{x\\; \\middle|\\; x < 0 \\right\\}\"\n    assert latex(conditionset5) == r\"\\left\\{x y\\; \\middle|\\; x > 0 \\right\\}\"\n    assert latex(conditionset6) == r\"\\left\\{x y\\; \\middle|\\; x < 0 \\right\\}\"\n    assert latex(conditionset7) == r\"\\left\\{x y\\; \\middle|\\; x > 0 \\vee x < 0 \\right\\}\"\n"], "sample_1146": ["def test_latex_NDimArray_other_formats():\n    x, y, z, w = symbols(\"x y z w\")\n\n    for ArrayType in (ImmutableDenseNDimArray, ImmutableSparseNDimArray,\n                      MutableDenseNDimArray, MutableSparseNDimArray):\n        M = ArrayType([[1 / x, y], [z, w]])\n        M1 = ArrayType([1 / x, y, z])\n\n        M2 = tensorproduct(M1, M)\n        M3 = tensorproduct(M, M)\n\n        assert latex(M, mat_delim='(') == \\\n            r'\\left(\\left(\\begin{matrix}\\frac{1}{x} & y\\\\z & w\\end{matrix}\\right)\\right)'\n        assert latex(M, mat_delim=None) == \\\n            r'\\begin{matrix}\\frac{1}{x} & y\\\\z & w\\end{matrix}'\n        assert latex(M, mat_delim=None, mat_str='bmatrix') == \\\n            r'\\begin{bmatrix}\\frac{1}{x} & y\\\\z & w\\end{bmatrix}'\n\n        assert latex(M3, mat_delim=None, mat_str='array') == \\\n            r\"\"\"\\begin{array}{cccc}\n            \\frac{1}{x^{2}} & \\frac{y}{x} & z & \\frac{w z}{x} \\\\\n            \\frac{y}{x} & y^{2} & w & y w \\\\\n            \\frac{z}{x} & y z & \\frac{z^{2}}{x} & \\frac{w z^{2}}{x} \\\\\n            \\frac{w}{x} & y w & \\frac{w z}{x} & \\frac{w^{2}}{x}\n            \\end{array}\"\"\"\n", "def test_decimal_separator_long_frac_ratio():\n    assert latex(1/(x + y)/2, long_frac_ratio=0, decimal_separator='comma') == \\\n        r'\\frac{1}{2} \\left(x + y\\right)^{-1}'\n    assert latex(1/(x + y)/2, long_frac_ratio=2, decimal_separator='period') == \\\n        r'\\frac{1}{2 \\left(x + y\\right)}'\n", "def test_latex_applied_symbols():\n    from sympy.printing.latex import latex, translate, greek_letters_set, tex_greek_dictionary\n    from sympy import Symbol\n    assert latex(Symbol(\"a'\")) == r\"{a'}\"\n    assert latex(Symbol('a\"')) == r'{a\"}'\n    assert latex(Symbol('k^2')) == r'{k^{2}}'\n    assert latex(Symbol('k_2')) == r'{k_{2}}'\n    assert latex(Symbol('k_2^3')) == r'{{k_{2}}^{3}}'\n    assert latex(Symbol('BetaHatDotOverBar')) == r'\\dot{\\hat{\\bar{\\Beta}}}'\n    assert latex(Symbol('betaHatDotOverBar')) == r'\\dot{\\hat{\\bar{\\beta}}}'\n    assert latex(Symbol('ZBarDot')) == r'{\\overline{Z}}{\\dot{}}'\n    assert latex(Symbol('ZDotBar')) == r'{\\dot{\\overline{Z}}}'\n    assert latex(Symbol('ZHatBarDot')) == r'{\\hat{\\overline{Z}}}{\\dot{}}'\n    assert latex(Symbol('ZDotHatBar')) == r'{\\hat{\\overline{{\\dot{Z}}}}}'\n    assert latex(Symbol('ZBarDotHat')) == r'{\\hat{{\\overline{Z}}{\\dot{}}}}'\n    assert latex(Symbol('ZDotBarHat')) == r'{\\hat{{\\dot{\\overline{Z}}}}}'\n    assert latex(Symbol('k_2Dot')) == r'{\\dot{k}_{2}}'\n    assert latex(Symbol('k_2DotHat')) == r'{\\hat{\\dot{k}}_{2}}'\n    assert latex(Symbol('kprimePrime')) == r\"{k''}\"\n    assert latex(Symbol('k_2primePrimeHat')) == r\"{\\hat{k}_{2}''}\"\n    assert latex(Symbol('xBar')) == r'{\\overline{x}}'\n    assert latex(Symbol('xBreve')) == r'{\\breve{x}}'\n    assert latex(Symbol('xCheck')) == r'{\\check{x}}'\n    assert latex(Symbol('xHatDot')) == r'{\\hat{\\dot{x}}}'\n    assert latex(Symbol('xBreveDot')) == r'{\\breve{\\dot{x}}}'\n    assert latex(Symbol('xCheckDot')) == r'{\\check{\\dot{x}}", "def test_transfer_function():\n    from sympy.physics.control.lti import TransferFunction\n    s = symbols('s')\n    G = TransferFunction(s**2 + 3*s + 2, s + 1, s)\n    assert latex(G) == r\"\\frac{s^2 + 3 s + 2}{s + 1}\"\n", "def test_multiline_latex_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    expected = r'\\begin{align*}f = & \\begin{cases} x & \\text{for}\\: x < 1 \\\\x^{2} & \\text{otherwise} \\end{cases}\\end{align*}'\n    assert multiline_latex('f', expr, 1) == expected\n"], "sample_1150": ["def test_issue_20441():\n    from sympy import Rational\n    assert ImageSet(Lambda(x, Rational(1, x)), S.Naturals).intersect(S.Rationals) == ImageSet(Lambda(x, Rational(1, x)), S.Naturals)\n", "def test_issue_20349():\n    assert ImageSet(Lambda(x, x**2), S.Rationals).is_subset(S.Reals)\n    assert ImageSet(Lambda(x, x**2), S.Rationals).is_subset(S.Rationals) is None\n", "def test_issue_20218():\n    assert ImageSet(Lambda(x, x/2), S.Integers).intersect(\n        Interval.open(0, pi)).dummy_eq(\n        ImageSet(Lambda(x, pi*x/4), Range(1, 4)))\n", "def test_Range_diophantine():\n    assert Range(3, 7, 2).as_relational(x) == Eq(x, 3) | Eq(x, 5)\n    assert Range(0, 10, 3).as_relational(x) == Eq(x, 0) | Eq(x, 3) | Eq(x, 6) | Eq(x, 9)\n    assert Range(-oo, 5, 2).as_relational(x) == And(x < 5, Eq(Mod(x, 2), 0))\n    assert Range(-3, oo, 2).as_relational(x) == And(x >= -3, Eq(Mod(x, 2), 0))\n    assert Range(3, -3, -2).as_relational(x) == Or(And(x <= -3, Eq(Mod(x, -2), 0)), And(x >= 3, Eq(Mod(x, -2), 0)))\n", "def test_image_is_iterable():\n    assert ImageSet(Lambda(x, 1), S.Integers).is_iterable\n    assert ImageSet(Lambda(x, x), S.Integers).is_iterable\n    assert ImageSet(Lambda(x, x**2), S.Integers).is_iterable\n    assert not ImageSet(Lambda(x, x/2 + Rational(1, 3)), S.Integers).is_iterable\n"], "sample_1152": ["def test_powsimp_function_as_coefficient():\n    # Test that a function as a coefficient doesn't interfere with powsimp\n    f = Function('f')\n    b = Symbol('b')\n    assert powsimp(f(2**b) * 2**b) == f(2**b) * 2**b\n", "def test_powsimp_with_negative_rational_powers():\n    x, y = symbols('x y', positive=True)\n    assert powsimp(x**(-Rational(1, 2))*y**(-Rational(1, 3))) == y**(-Rational(1, 3))/x**(Rational(1, 2))\n    assert powsimp((x**(-Rational(1, 2)))**2) == x**(-1)\n    assert powsimp((x**(-Rational(1, 3)))**3) == x**(-1)\n    assert powsimp((x**(-Rational(2, 3)))**Rational(3, 2)) == x**(-1)\n", "def test_powsimp_other_noncommutative():\n    x, y, z = symbols('x,y,z')\n    A, B, C = symbols('A B C', commutative=False)\n    n = symbols('n', integer=True)\n\n    assert powsimp(A**n*A**n, combine='all') == A**(2*n)\n    assert powsimp(A**n*A**n, combine='base') == A**n*A**n\n    assert powsimp(A**n*A**n, combine='exp') == A**(2*n)\n\n    assert powsimp(A**x*A**y, combine='all') == A**(x + y)\n    assert powsimp(A**x*A**y, combine='base') == A**x*A**y\n    assert powsimp(A**x*A**y, combine='exp') == A**(x + y)\n\n    assert powsimp((A**x)**(y*z)) == (A**x)**(y*z)\n    assert powsimp((A**x)**(y*z), combine='all') == A**(x*y*z)\n    assert powsimp((A**x)**(y*z), combine='base') == (A**x)**(y*z)\n    assert powsimp((A**x)**(y*z), combine='exp') == A**(x*y*z)\n\n    assert powsimp(A**x*B**x, combine='all') == A**x*B**x\n    assert powsimp(A**x*B**x, combine='base') == A**x*B**x\n    assert powsimp(A**x*B**x, combine='exp') == A**x*B**x\n\n    assert powsimp(B**x*A**x, combine='all') == B**x*A**x\n    assert powsimp(B**x*A**x, combine='base') == B**x*A**x\n    assert powsimp(B**x*A**x, combine='exp') == B**x*A**x\n", "def test_powsimp_with_fractions():\n    x, y = symbols('x y')\n    assert powsimp((x**2)**Rational(1, 2)) == x\n    assert powsimp((x**2)**Rational(1, 3)) == x**(2/3)\n    assert powsimp((x**Rational(1, 2))**2) == x\n    assert powsimp((x**Rational(1, 3))**3) == x\n    assert powsimp((x**Rational(2, 3))**3) == x**2\n    assert powsimp((x**Rational(2, 3))**Rational(3, 2)) == x\n    assert powsimp(x**Rational(1, 2)*x**Rational(1, 3)) == x**(5/6)\n", "def test_powsimp_issue():\n    from sympy import Rational\n    x = symbols('x')\n    a = symbols('a')\n    assert powsimp(x**(2*a/3)*x**(4*a/3)) == x**(2*a)\n    assert powsimp(x**(2*a)*x**(4*a)) == x**(6*a)\n    assert powsimp(x**(Rational(1, 2)) * x**(Rational(1, 3))) == x**(Rational(5, 6))\n"], "sample_1151": ["def test_Mul_Mul_evaluate():\n    # issue 20872\n    y = Symbol('y')\n    z = Symbol('z')\n\n    e = Mul(1, Mul(y, z, evaluate=False), 2, evaluate=False)\n    assert e.doit(evaluate=False).func == Mul\n    assert e.doit(evaluate=False).args == (2*y*z,)\n\n    e = Mul(1, Mul(y, z, evaluate=False), 2)\n    assert e.func == Mul\n    assert e.args == (2*y*z,)\n\n    e = Mul(1, Mul(y, z, evaluate=False), 2, evaluate=False)\n    assert e.doit().func == Mul\n    assert e.doit().args == (2*y*z,)\n", "def test_Mul_hermitian_base_hermitian_exp():\n    x = Symbol('x', hermitian=True)\n    assert (2**x).is_hermitian is None\n    assert (I**x).is_hermitian is None\n    assert (I**x).is_antihermitian is None\n    assert ((-I)**x).is_hermitian is None\n    assert ((-I)**x).is_antihermitian is None\n    assert ((-2)**x).is_hermitian is None\n    assert ((-2)**x).is_antihermitian is None\n    assert (2**x).subs({x: 2}).is_hermitian is True\n    assert (I**x).subs({x: 2}).is_hermitian is False\n    assert (I**x).subs({x: 2}).is_antihermitian is True\n    assert ((-I)**x).subs({x: 2}).is_hermitian is False\n    assert ((-I)**x).subs({x: 2}).is_antihermitian is True\n    assert ((-2)**x).subs({x: 2}).is_hermitian is True\n    assert ((-2)**x).subs({x: 2}).is_antihermitian is False\n", "def test_rmul():\n    class TS:\n            self.x = x\n\n            return self.x * other\n\n    assert 2 * TS(3) == 6\n", "def test_issue_20181():\n    assert Mod(Mod(x, 3) + x, 3) == Mod(2*x, 3)\n    assert Mod(Mod(x, 3) - x, 3) == Mod(-2*x, 3)\n", "def test_Mod_rewrite_as_floor():\n    from sympy.functions.elementary.integers import floor\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Mod(x, y).rewrite(floor) == x - y*floor(x/y)\n"], "sample_1153": ["def test_transpose_function():\n    from sympy import transpose, Function\n    f = Function('f')\n    x = Symbol('x')\n    assert transpose(f(x)) == f(x)\n    assert transpose(f(x, x)) == f(x, x)\n    f = Function('f', nofargs=(1, 2))\n    assert transpose(f(x, x)) == f(x, x)\n    assert transpose(f(x)) == f(x)\n", "def test_polar_lift():\n    x = Symbol('x')\n    assert polar_lift(0) == 0\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(1 + 1*I) == exp_polar(I*atan2(1, 1))*sqrt(2)\n    assert polar_lift(3 + 4*I) == exp_polar(I*atan2(4, 3))*5\n    assert polar_lift(-3 - 4*I) == exp_polar(I*atan2(-4, -3)+I*pi)*5\n    assert polar_lift(-5) == 5*exp_polar(I*pi)\n", "def test_transpose_conjugate():\n    from sympy import transpose, conjugate\n    from sympy.matrices import MatrixSymbol\n\n    A = MatrixSymbol('A', 2, 2)\n    assert transpose(conjugate(A)) == conjugate(transpose(A))\n    assert conjugate(transpose(A)) == transpose(conjugate(A))\n", "def test_conjugate_issue_19594():\n    x = symbols('x', real=True)\n    y = symbols('y', real=True)\n    assert conjugate(sqrt(x - y*I)) == sqrt(x + y*I)\n", "def test_polar_lift():\n    from sympy import polar_lift, principal_branch, exp_polar\n    x = Symbol('x')\n    assert polar_lift(x).is_polar is True\n    assert polar_lift(x).as_real_imag()[0] == re(x)\n\n    assert polar_lift(2 + I) == 2*exp_polar(I*atan2(1, 2))\n    assert polar_lift(1 + I) == sqrt(2)*exp_polar(I*pi/4)\n\n    assert polar_lift(exp_polar(7 - 3*I)) == exp_polar(7 - 3*I)\n\n    assert polar_lift(2*x).as_real_imag()[0] == 2*re(x)\n    assert polar_lift(2*x).as_real_imag()[1] == 2*im(x)\n\n    assert polar_lift(1/x) == 1/polar_lift(x)\n\n    assert re(polar_lift(x)).is_real is True\n    assert im(polar_lift(x)).is_real is True\n\n    assert polar_lift(x).conjugate() == polar_lift(x.conjugate())\n    assert polar_lift(x).diff(x) == Derivative(polar_lift(x), x)\n\n    assert abs(polar_lift(x)) == abs(x)\n    assert polar_lift(x).__abs__() == abs(x)\n\n    p = Symbol('p', positive=True)\n    assert polar_lift(p) == p\n\n    assert unpolarify(polar_lift(x)) == x\n    assert unpolarify(polar_lift(2 + I)) == 2 + I\n\n    assert polar_lift(x).is_finite is None\n    assert polar_lift(1).is_finite is True\n    assert polar_lift(1 + I).is_finite is True\n"], "sample_1154": ["def test__linsolve_non_square():\n    # More equations than unknowns\n    eqs = [x + y - 1, x - y - 2, 2*x - 3]\n    assert _linsolve(eqs, [x, y]) is None\n\n    # Fewer equations than unknowns\n    eqs = [x + y - 1]\n    sol = {x: 1 - y, y: y}\n    assert _linsolve(eqs, [x, y]) == sol\n\n    # Fewer equations than unknowns with no solution\n    eqs = [x + y + z, x + y + z - 1]\n    assert _linsolve(eqs, [x, y, z]) is None\n", "def test__linsolve_underdetermined():\n    # Test underdetermined systems\n    assert _linsolve([x + y], [x, y]) == {x: -y, y: y}\n    assert _linsolve([x + y, x + z], [x, y, z]) == {x: z - y, y: y, z: z}\n    assert _linsolve([x + y, y + z], [x, y, z]) == {x: -z, y: -z, z: z}\n", "def test__linsolve_underdetermined():\n    assert _linsolve([x + y], [x, y]) == {x: -y, y: y}\n    assert _linsolve([x + y, x + z], [x, y, z]) == {x: -y - z, y: y, z: z}\n    assert _linsolve([x + y, y + z], [x, y, z]) == {x: -y, y: -z, z: z}\n    assert _linsolve([x + y + z], [x, y, z]) == {x: -y - z, y: y, z: z}\n", "def test__linsolve_underdetermined():\n    # Test the case where the system is underdetermined\n    assert _linsolve([x + y - 1], [x, y]) == {x: 1 - y, y: y}\n    assert _linsolve([x + y, x + z], [x, y, z]) == {x: -y, y: y, z: y}\n    assert _linsolve([x + y + z - 1], [x, y, z]) == {x: 1 - y - z, y: y, z: z}\n", "def test__linsolve_underdetermined():\n    # Test that underdetermined systems are handled correctly\n    assert _linsolve([x + y - 1], [x, y]) == {x: 1 - y, y: y}\n    assert _linsolve([x + y], [x, y]) == {x: -y, y: y}\n    assert _linsolve([x + y + z - 1], [x, y, z]) == {x: 1 - y - z, y: y, z: z}\n"], "sample_1155": ["def test_extension_option_with_algebraic_and_float():\n    result = construct_domain([sqrt(2), 3.14], extension=True)\n    assert isinstance(result[0], EX)\n    assert result[1] == [EX(sqrt(2)), EX(3.14)]\n", "def test_algebraic_extension():\n    alg = QQ.algebraic_field(GoldenRatio)\n    assert construct_domain([GoldenRatio, 1/GoldenRatio, 1], extension=True) == \\\n        (alg, [alg.convert(GoldenRatio), alg.convert(1/GoldenRatio), alg.convert(1)])\n\n    alg = QQ.algebraic_field(Catalan)\n    assert construct_domain([Catalan, 1/Catalan, 1], extension=True) == \\\n        (alg, [alg.convert(Catalan), alg.convert(1/Catalan), alg.convert(1)])\n\n    alg = QQ.algebraic_field(E)\n    assert construct_domain([E, 1/E, 1], extension=True) == \\\n        (alg, [alg.convert(E), alg.convert(1/E), alg.convert(1)])\n", "def test_extension_option():\n    assert construct_domain([GoldenRatio], extension=True) != (EX, [EX(GoldenRatio)])\n    assert construct_domain([Catalan], extension=True) != (EX, [EX(Catalan)])\n    assert construct_domain([E], extension=True) != (EX, [EX(E)])\n    assert construct_domain([I], extension=True) == (ZZ_I, [ZZ_I(0, 1)])\n    assert construct_domain([I, sqrt(2)], extension=True) != (EX, [EX(I), EX(sqrt(2))])\n", "def test_extension_option():\n    # Test that the extension option works as expected\n    assert construct_domain([E, sqrt(2)], extension=True) == (EX, [EX(E), EX(sqrt(2))])\n    assert construct_domain([E, sqrt(2)], extension=None) == (EX, [EX(E), EX(sqrt(2))])\n    assert construct_domain([GoldenRatio, sqrt(2)], extension=True) == (EX, [EX(GoldenRatio), EX(sqrt(2))])\n    assert construct_domain([GoldenRatio, sqrt(2)], extension=None) == (EX, [EX(GoldenRatio), EX(sqrt(2))])\n    assert construct_domain([Catalan, sqrt(2)], extension=True) == (EX, [EX(Catalan), EX(sqrt(2))])\n    assert construct_domain([Catalan, sqrt(2)], extension=None) == (EX, [EX(Catalan), EX(sqrt(2))])\n", "def test_extension_with_irrational_numbers():\n    assert construct_domain([E, pi, Catalan], extension=True) == (EX, [EX(E), EX(pi), EX(Catalan)])\n    assert construct_domain([GoldenRatio, I], extension=True) == (EX, [EX(GoldenRatio), EX(I)])\n    assert construct_domain([sqrt(2)*I, sqrt(3)], extension=True) == (EX, [EX(sqrt(2)*I), EX(sqrt(3))])\n"], "sample_1157": ["def test_implicit_multiplication_application():\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert parse_expr(\"10sin**2 x**2 + 3xyz + tan theta\",\n        transformations=transformations) == 3*x*y*z + 10*sin(x**2)**2 + tan(Symbol('theta'))\n    assert parse_expr(\"abc\", transformations=transformations) == Symbol('a')*Symbol('b')*Symbol('c')\n    assert parse_expr(\"3.0a\", transformations=transformations) == Float(3)*Symbol('a')\n    assert parse_expr(\"1.0e+100x\", transformations=transformations) == Float(\"1.e+100\")*Symbol('x')\n    assert parse_expr(\"2.5e-5a\", transformations=transformations) == Float(\"2.5e-5\")*Symbol('a')\n    assert parse_expr(\"3 10**2 x\", transformations=transformations) == 3*(10**2)*x\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    f = Function('f')\n    assert parse_expr('lambda x: x**2', transformations=transformations) == Lambda(x, x**2)\n    assert parse_expr('lambda x,y: x+y', transformations=transformations) == Lambda((x, y), x + y)\n    assert parse_expr('lambda x: f(x)', transformations=transformations) == Lambda(x, f(x))\n", "def test_implicit_application():\n    transformations = standard_transformations + \\\n                      (implicit_application,)\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n\n    assert parse_expr(\"f x\", transformations=transformations) == f(x)\n    assert parse_expr(\"f x y\", transformations=transformations) == f(x, y)\n    assert parse_expr(\"f x, y\", transformations=transformations) == f(x, y)\n    assert parse_expr(\"f(x) y\", transformations=transformations) == f(x)*y\n", "def test_auto_symbol():\n    transformations = standard_transformations + (auto_symbol,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"x\", transformations=transformations) == x\n    assert parse_expr(\"y\", transformations=transformations) == y\n    assert parse_expr(\"xyz\", transformations=transformations) == Symbol('xyz')\n    assert parse_expr(\"xy\", transformations=transformations) == Symbol('xy')\n    assert parse_expr(\"x y\", transformations=transformations) == x*y\n    assert parse_expr(\"x y z\", transformations=transformations) == x*y*Symbol('z')\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr('lambda x: x+1', transformations=transformations) == Lambda(x, x+1)\n    assert parse_expr('lambda x, y: x+y', transformations=transformations) == Lambda((x, y), x+y)\n"], "sample_1156": ["def test_tanh_real():\n    x = Symbol('x', real=True)\n    assert tanh(x).is_real is True\n    assert tanh(x).is_finite is True\n    assert tanh(0).is_real is True\n", "def test_hyperbolic_functions_with_intervals():\n    from sympy import Interval, pi, I\n    from sympy.sets import imageset\n    from sympy.solvers.inequalities import solve_univariate_inequality\n    from sympy.abc import x\n\n    x_interval = Interval(-2*pi, 2*pi)\n    solutions = solve_univariate_inequality(sinh(x) > 0, x, relational=False)\n    assert imageset(sinh, x_interval) == Interval(-sinh(2*pi), sinh(2*pi))\n\n    solutions = solve_univariate_inequality(cosh(x) > 0, x, relational=False)\n    assert imageset(cosh, x_interval) == Interval(1, cosh(2*pi))\n\n    solutions = solve_univariate_inequality(tanh(x) > 0, x, relational=False)\n    assert imageset(tanh, x_interval) == Interval(-1, 1)\n\n    solutions = solve_univariate_inequality(coth(x) > 0, x, relational=False)\n    assert imageset(coth, x_interval) == Interval(-oo, -1) | Interval(1, oo)\n\n    solutions = solve_univariate_inequality(sech(x) > 0, x, relational=False)\n    assert imageset(sech, x_interval) == Interval(sech(2*pi), 1)\n\n    solutions = solve_univariate_inequality(csch(x) > 0, x, relational=False)\n    assert imageset(csch, x_interval) == Interval(csch(2*pi), oo) | Interval(-oo, -csch(2*pi))\n\n    assert imageset(acosh, Interval(1, 2)) == Interval(0, acosh(2))\n    assert imageset(acosh, Interval(-oo, 1)) == S.EmptySet\n\n    assert imageset(asech, Interval(0, 1)) == Interval(acosh(oo), acosh(1))\n    assert imageset(asech, Interval(1, 2)) == Interval(acosh(1), acosh(S.Half))\n    assert imageset(asech, Interval(-2, 0)) == Interval(acosh(-S.Half), acosh(oo))\n    assert imageset(asech, Interval(-oo, 0)) == Interval(acosh(0), acosh(oo))\n\n    assert imageset(asinh, Interval(-oo, -1", "def test_real_assumptions_sinh():\n    z = Symbol('z', real=True)\n    assert sinh(z).is_real is True\n    assert sinh(I*z).is_real is False\n    assert sinh(z + I).is_real is False\n    assert sinh(2*z + 3*I).is_real is False\n", "def test_is_real():\n    x, y = symbols('x,y')\n    z = Symbol('z', real=False)\n\n    assert sinh(x).is_real is True\n    assert sinh(z).is_real is None\n    assert cosh(x).is_real is True\n    assert cosh(z).is_real is None\n    assert tanh(x).is_real is True\n    assert tanh(z).is_real is None\n    assert coth(x).is_real is True\n    assert coth(z).is_real is None\n    assert sech(x).is_real is True\n    assert sech(z).is_real is None\n    assert csch(x).is_real is True\n    assert csch(z).is_real is None\n", "def test_hyperbolic_is_finite():\n    x = Symbol('x')\n    for func in [sinh, cosh, tanh, sech, csch, coth]:\n        assert func(x).is_finite is None\n        assert func(1).is_finite is True\n        assert func(I).is_finite is True\n        assert func(oo).is_finite is False\n    for func in [asinh, acosh, atanh, acoth]:\n        assert func(x).is_finite is None\n        assert func(1).is_finite is True\n        assert func(I).is_finite is True\n        assert func(oo).is_finite is False\n    for func in [asech, acsch]:\n        assert func(x).is_finite is None\n        assert func(1).is_finite is True\n        assert func(I).is_finite is True\n        assert func(oo).is_finite is True\n"], "sample_1158": ["def test_issue_18565():\n    # tests that sympify on an OrderedDict doesn't convert\n    # the OrderedDict to a dict\n    from collections import OrderedDict\n    d = OrderedDict([(1, x), (2, y)])\n    assert type(sympify(d)) == OrderedDict\n", "def test_issue_18485():\n    # A custom converter should take precedence over built-in converters\n    class CustomFloat(float):\n        pass\n\n    converter[CustomFloat] = lambda x: x**2\n\n    assert sympify(CustomFloat(2.0)) == 4.0\n\n    del converter[CustomFloat]\n", "def test_issue_14706_strict():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    assert sympify(numpy.int(0), strict=True) == Integer(0)\n    assert sympify(numpy.float(0), strict=True) == Float(0)\n    assert sympify(numpy.complex(0), strict=True) == Integer(0)\n    assert sympify(numpy.float64(0), strict=True) == Float(0)\n    assert sympify(numpy.complex64(0), strict=True) == Float(0)\n    assert sympify(numpy.int8(0), strict=True) == Integer(0)\n    assert sympify(numpy.int16(0), strict=True) == Integer(0)\n    assert sympify(numpy.int32(0), strict=True) == Integer(0)\n    assert sympify(numpy.int64(0), strict=True) == Integer(0)\n    assert sympify(numpy.float32(0), strict=True) == Float(0)\n\n    raises(SympifyError, lambda: sympify(numpy.array([1]), strict=True))\n    raises(SympifyError, lambda: sympify(numpy.array([1.0]), strict=True))\n    raises(SympifyError, lambda: sympify(numpy.array([1.0+1j]), strict=True))\n", "def test_issue_20244():\n    # make sure that in sympify, convert_xor=False will not affect exponentiation\n    assert sympify('x**2', convert_xor=False) == x**2\n    assert sympify('x**2', convert_xor=True) == x**2\n    # make sure that in sympify, convert_xor=True will convert ^ to **\n    assert sympify('x^2', convert_xor=True) == x**2\n    # make sure that in sympify, convert_xor=False will not convert ^ to **\n    assert sympify('x^2', convert_xor=False) == Xor(x, 2)\n", "def test_issue_20124():\n    class MyFunction:\n        pass\n\n    my_function = MyFunction()\n    assert sympify(my_function) is my_function\n    raises(SympifyError, lambda: _sympify(my_function))\n"], "sample_1161": ["def test_issue_22573():\n    a = Symbol('a')\n    b = Symbol('b')\n    assert sstr(a/b) == 'a/b'\n    assert sstr(1/(a/b)) == 'b/a'\n    assert sstr(a/(b**-1)) == 'a*b'\n    assert sstr(a/(1/b)) == 'a*b'\n    assert sstr(1/(1/(1/a))) == 'a'\n", "def test_Array_Element():\n    from sympy.tensor.array.expressions.array_expressions import ArraySymbol, ArrayElement, ZeroArray\n    A = ArraySymbol(\"A\", (2, 3))\n    assert str(ArrayElement(A, (1, 2))) == 'A[1, 2]'\n    assert str(ArrayElement(A, (x, y))) == 'A[x, y]'\n    assert str(ArrayElement(A, (x + 1, y))) == 'A[x + 1, y]'\n    assert str(ZeroArray((3, 3), (x, y))) == '0'\n", "def test_Dimension():\n    assert str(Dimension(\"length\")) == \"length\"\n", "def test_Dimension():\n    assert str(Dimension(\"length\")) == \"length\"\n    assert str(Dimension(\"time\")) == \"time\"\n    assert str(Dimension(\"mass\")) == \"mass\"\n    assert str(Dimension(\"current\")) == \"current\"\n    assert str(Dimension(\"temperature\")) == \"temperature\"\n    assert str(Dimension(\"amount_of_substance\")) == \"amount_of_substance\"\n    assert str(Dimension(\"luminous_intensity\")) == \"luminous_intensity\"\n", "def test_AlgebraicNumber():\n    from sympy import AlgebraicNumber\n    a = AlgebraicNumber(2, [1, 2, 3])\n    assert sstr(a) == \"RootOf(_x**2 + 2*_x + 3, 2)\"\n    assert sstr(a, sympy_integers=True) == \"RootOf(_x**2 + 2*_x + 3, 2)\"\n"], "sample_1159": ["def test_check_assumptions_against():\n    x = Symbol('x')\n    assert check_assumptions(1, against=x) is True\n    assert check_assumptions(-1, against=x) is True\n    assert check_assumptions(x, against=x) is True\n    assert check_assumptions(2*x + 1, against=x) is True\n    assert check_assumptions(2*x - 1, against=x) is True\n    assert check_assumptions(x - 1, against=x) is True\n", "def test_check_assumptions_against():\n    x = Symbol('x', positive=True)\n    y = Symbol('y', positive=True)\n    assert check_assumptions(x, against=y) is True\n    assert check_assumptions(x, against=-y) is False\n    assert check_assumptions(-x, against=y) is False\n    i = Symbol('i', integer=True)\n    assert check_assumptions(i, against=x) is None\n    assert check_assumptions(x, against=i) is None\n    assert check_assumptions(i, against=i) is True\n    assert check_assumptions(i, against=-i) is None\n", "def test_extended_real():\n    x = Symbol('x', extended_real=True)\n    y = Symbol('y', real=True)\n    assert x.is_real is None\n    assert x.is_extended_real is True\n    assert y.is_extended_real is True\n    assert (x + y).is_extended_real is True\n", "def test_common_assumptions():\n    x = Symbol('x', real=True)\n    y = Symbol('y', integer=True)\n    z = Symbol('z', real=False)\n\n    assert common_assumptions([x, y, z]) == {\n        'commutative': True,\n        'complex': True,\n        'composite': False,\n        'even': False,\n        'extended_negative': False,\n        'extended_nonnegative': False,\n        'extended_nonpositive': False,\n        'extended_nonzero': False,\n        'extended_positive': False,\n        'extended_real': False,\n        'finite': True,\n        'imaginary': False,\n        'infinite': False,\n        'integer': False,\n        'irrational': False,\n        'negative': False,\n        'noninteger': False,\n        'nonnegative': False,\n        'nonpositive': False,\n        'nonzero': False,\n        'odd': False,\n        'positive': False,\n        'prime': False,\n        'rational': False,\n        'real': False,\n        'transcendental': False,\n        'zero': False\n    }\n\n    assert common_assumptions([x, y], ['integer', 'real']) == {\n        'integer': False,\n        'real': True\n    }\n\n    assert common_assumptions([x, y, z], ['integer', 'real']) == {\n        'integer': False,\n        'real': False\n    }\n", "def test_check_assumptions_complex():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert check_assumptions(x + y*I, real=False) is True\n    assert check_assumptions(x + y*I, real=True) is False\n    assert check_assumptions(x + y*I, complex=True) is True\n    assert check_assumptions(x + y*I, imaginary=None) is None\n    assert check_assumptions(x, imaginary=False) is True\n"], "sample_1160": ["def test_issue_18999():\n    n = Symbol('n', integer=True)\n    assert Range(n, 5).is_finite_set is True\n    assert Range(n, -5).is_finite_set is True\n    assert Range(n, n+5).is_finite_set is True\n    assert Range(n, n-5).is_finite_set is True\n    assert Range(n, n+5*n).is_finite_set is False\n    assert Range(n, n-5*n).is_finite_set is True\n", "def test_issue_19495():\n    assert imageset(x, x**2, Interval(1, 2)) == Interval(1, 4)\n    assert imageset(x, x**2, Interval(-1, 2)) == Interval(0, 4)\n    assert imageset(x, x**2, Interval(-2, 0)) == Interval(0, 4)\n    assert imageset(x, x**2, Interval(-2, -1)) == Interval(1, 4)\n", "def test_issue_18999():\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True)\n    assert Range(n, m).is_finite_set == (n >= m)\n    assert Range(n, m, 2).is_finite_set == (n >= m)\n    assert Range(n, m, -1).is_finite_set == (n <= m)\n    assert Range(n, m, -2).is_finite_set == (n <= m)\n    assert Range(n, n + 1).is_finite_set == True\n    assert Range(n, n - 1, -1).is_finite_set == True\n    assert Range(n, n).is_finite_set == True\n    assert Range(n, n, 2).is_finite_set == True\n    assert Range(n, n + 10).is_finite_set == True\n    assert Range(n, n - 10, -1).is_finite_set == True\n    assert Range(n, m).size == Max(m - n, 0)\n    assert Range(n, m, 2).size == Max((m - n + 1) // 2, 0)\n    assert Range(n, m, -1).size == Max(n - m, 0)\n    assert Range(n, m, -2).size == Max((n - m + 1) // 2, 0)\n    assert Range(n, n + 1).size == 1\n    assert Range(n, n - 1, -1).size == 1\n    assert Range(n, n).size == 0\n    assert Range(n, n, 2).size == 0\n    assert Range(n, n + 10).size == 10\n    assert Range(n, n - 10, -1).size == 10\n", "def test_issue_18999():\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True)\n    assert Range(n, n + 49).is_finite_set is True\n    assert Range(n, 0).is_finite_set is True\n    assert Range(-3, n + 7).is_finite_set is True\n    assert Range(n, m).is_finite_set is not True  # can't determine without knowing n, m values\n    assert Range(n + m, m - n).is_finite_set is not True  # can't determine without knowing n, m values\n    assert Range(n, n + m + n).is_finite_set is not True  # can't determine without knowing m value\n", "def test_issue_19143():\n    asserts = [\n        # Test Case 1: No overlap\n        (Interval(0, 1), Interval(2, 3), S.EmptySet),\n        # Test Case 2: Partial overlap\n        (Interval(0, 2), Interval(1, 3), Interval(1, 2)),\n        # Test Case 3: Full overlap\n        (Interval(0, 4), Interval(1, 3), Interval(1, 3)),\n        # Test Case 4: One interval contains the other\n        (Interval(0, 3), Interval(1, 3), Interval(1, 3)),\n        # Test Case 5: Infinte intervals\n        (Interval(-oo, 0), Interval(-oo, 1), Interval(-oo, 0)),\n        # Test Case 6: One interval is finite, one is infinite\n        (Interval(0, oo), Interval(1, 2), Interval(1, 2)),\n        # Test Case 7: Negative infinite intervals\n        (Interval(-oo, -2), Interval(-oo, -1), Interval(-oo, -2)),\n        # Test Case 8: Positive infinite intervals\n        (Interval(2, oo), Interval(1, oo), Interval(2, oo)),\n    ]\n\n    for interval1, interval2, expected_result in asserts:\n        assert interval1.intersect(interval2) == expected_result\n"], "sample_1162": ["def test_MatMul_kind():\n    A = MatrixSymbol('A', 2,2)\n    B = MatrixSymbol('B', 2,2)\n    assert MatMul(A, B).kind is MatrixKind(NumberKind)\n", "def test_MatMul_kind():\n    A = MatrixSymbol('A', 2, 2)\n    assert MatMul(A, A).kind is MatrixKind(NumberKind)\n    assert MatMul(A, comm_x).kind is MatrixKind(NumberKind)\n    assert MatMul(noncomm_x, A).kind is MatrixKind(UndefinedKind)\n", "def test_MatMul_kind():\n    A = MatrixSymbol('A', 2,2)\n    assert MatMul(A, A).kind is MatrixKind(NumberKind)\n    assert MatMul(A, comm_x).kind is MatrixKind(NumberKind)\n    assert MatMul(A, noncomm_x).kind is UndefinedKind\n", "def test_MatMul_kind():\n    A = MatrixSymbol('A', 2,2)\n    assert MatMul(A, A).kind is MatrixKind(NumberKind)\n    B = MatrixSymbol('B', 2,2, commutative=False)\n    assert MatMul(B, B).kind is MatrixKind(NumberKind)\n", "def test_MatMul_kind():\n    A = MatrixSymbol('A', 2,2)\n    B = MatrixSymbol('B', 2,2)\n    assert MatMul(A, B).kind is MatrixKind(NumberKind)\n    assert MatMul(A, 2).kind is MatrixKind(NumberKind)\n    assert MatMul(2, A).kind is MatrixKind(NumberKind)\n"], "sample_1163": ["def test_polar_lift():\n    from sympy import (polar_lift, I, exp_polar,\n                       pi, arg, im, sqrt, oo)\n    z = Symbol('z')\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == 1\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(2 + 3*I) == 2*sqrt(13)/13 + 3*sqrt(13)*I/13\n    assert polar_lift(im(z)) == im(polar_lift(z))\n    assert polar_lift(oo) == oo\n    assert polar_lift(-oo) == oo\n    assert polar_lift(oo*I) == oo\n    assert polar_lift(-oo*I) == oo\n    assert polar_lift(-oo - 3) == oo\n    assert polar_lift(oo - 3) == oo\n", "def test_polar_lift():\n    x = Symbol('x')\n    assert polar_lift(1) == exp_polar(0)\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(x) == x*exp_polar(0)\n    assert polar_lift(-x) == x*exp_polar(I*pi)\n    assert polar_lift(2 + 3*I) == polar_lift(2 + 3*I)\n    assert polar_lift(0) == 0\n    assert polar_lift(oo) == oo\n    assert polar_lift(-oo) == -oo\n", "def test_transpose_conjugate_adjoint():\n    from sympy import transpose, conjugate, adjoint\n    from sympy.abc import x\n    from sympy.matrices import MatrixSymbol\n\n    M = MatrixSymbol('M', 2, 2)\n    assert transpose(M).conjugate() == M.adjoint()\n    assert transpose(M).adjoint() == M.conjugate()\n    assert conjugate(M).transpose() == M.adjoint()\n    assert conjugate(M).adjoint() == M.transpose()\n    assert adjoint(M).conjugate() == M.transpose()\n    assert adjoint(M).transpose() == M.conjugate()\n\n    f = x**2\n    assert transpose(f).conjugate() == f.adjoint()\n    assert transpose(f).adjoint() == f.conjugate()\n    assert conjugate(f).transpose() == f.adjoint()\n    assert conjugate(f).adjoint() == f.transpose()\n    assert adjoint(f).conjugate() == f.transpose()\n    assert adjoint(f).transpose() == f.conjugate()\n", "def test_periodic_argument_derivative():\n    from sympy import periodic_argument, sin, cos, I, pi\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    p = Symbol('p', positive=True)\n\n    assert periodic_argument(x, p).diff(x) == I*x.diff(x)/x\n    assert periodic_argument(exp(x), p).diff(x) == I*exp(x)/exp(x)\n    assert periodic_argument(sin(x), p).diff(x) == I*cos(x)/sin(x)\n    assert periodic_argument(cos(x), p).diff(x) == -I*sin(x)/cos(x)\n    assert periodic_argument(x*y, p).diff(x) == I*y/x\n    assert periodic_argument(x*y, p).diff(y) == I*x/y\n", "def test_polar_lift():\n    from sympy import polar_lift, exp_polar, I, pi\n    z = Symbol('z')\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == 1\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(exp_polar(I*pi)) == exp_polar(I*pi)\n    assert polar_lift(z) == polar_lift(z)\n    assert polar_lift(z).subs(z, 1) == 1\n    assert polar_lift(z).subs(z, -1) == exp_polar(I*pi)\n    assert polar_lift(z).subs(z, I) == exp_polar(I*pi/2)\n    assert polar_lift(z).subs(z, -I) == exp_polar(-I*pi/2)\n"], "sample_1165": ["def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(0, 0, 0, 0)\n\n    assert q1 / q2 == q1 * q2.inverse()\n    raises(ValueError, lambda: q1 / q3)\n\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    assert 2 / q1 == 2 * q1.inverse()\n\n    q4 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    assert q4 / (2 + 3*I) == Quaternion((3 + 4*I)/(2 + 3*I), (2 + 5*I)/(2 + 3*I), 0, (7 + 8*I)/(2 + 3*I))\n", "def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(2, 3, 4, 5)\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    assert 2 / q1 == 2 * q1.inverse()\n    raises(TypeError, lambda: q1 / (2 + 3*I))\n    raises(TypeError, lambda: (2 + 3*I) / q1)\n", "def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(1, 1, 1, 1)\n\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    assert q1 / q3 == Quaternion(1, 1, 1, 1)\n    assert (q1 / q2).norm() == 1\n    assert q1 / (1 + 1j) == Quaternion(1/(1 + 1j), 2/(1 + 1j), 3/(1 + 1j), 4/(1 + 1j))\n    assert (1 + 1j) / q1 == Quaternion(1/30 - 1j/30, 1/15 + 1j/15, 1/10 + 1j/10, 2/15 + 2j/15)\n", "def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(0, 0, 0, 0)\n\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    raises(ZeroDivisionError, lambda: q1 / 0)\n    raises(ValueError, lambda: q1 / q3)\n", "def test_quaternion_division():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 / q2 == q1 * q2.inverse()\n    assert q1 / 2 == Quaternion(1/2, 1, 3/2, 2)\n    assert 2 / q1 == 2 * q1.inverse()\n    q0 = Quaternion(0, 0, 0, 0)\n    raises(ValueError, lambda: 2 / q0)\n"], "sample_1164": ["def test_cg_simp():\n    a = CG(1, 1, 0, 0, 1, 1)\n    b = CG(1, 0, 0, 0, 1, 0)\n    c = CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(a+b+c) == 3\n    assert cg_simp(a**2) == (a**2).doit()\n    assert cg_simp(b**2) == (b**2).doit()\n    assert cg_simp(c**2) == (c**2).doit()\n    assert cg_simp(a*b) == (a*b).doit()\n    assert cg_simp(b*c) == (b*c).doit()\n    assert cg_simp(c*a) == (c*a).doit()\n    assert cg_simp(a*b*c) == (a*b*c).doit()\n", "def test_cg_simp():\n    j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3')\n    expr1 = CG(j1, m1, j2, m2, j3, m3) + CG(j1, m1, j2, m2, j3, m3)\n    assert cg_simp(expr1) == 2*CG(j1, m1, j2, m2, j3, m3)\n\n    expr2 = CG(1, 1, 1, -1, 1, 0) + CG(1, -1, 1, 1, 1, 0)\n    assert cg_simp(expr2) == sqrt(2)\n\n    expr3 = CG(j1, m1, j2, m2, j3, m3) * CG(j1, m1, j2, m2, j3, m3)\n    assert cg_simp(expr3) == CG(j1, m1, j2, m2, j3, m3)**2\n", "def test_cg_simp():\n    a = CG(1, 1, 0, 0, 1, 1)\n    b = CG(1, 0, 0, 0, 1, 0)\n    c = CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(a + b + c) == 3\n", "def test_cg_simp():\n    j, m, jp, mp = symbols('j m jp mp')\n    assert cg_simp(CG(j, m, jp, mp, j + jp, m + mp)**2) == 1\n    assert cg_simp(CG(j, m, jp, mp, j + jp, m + mp)*CG(j, m, jp, mp, j + jp - 1, m + mp)) == 0\n    assert cg_simp(CG(1, 1, 1, 1, 2, 2)) == 1\n    assert cg_simp(CG(1, 1, 1, -1, 2, 0)) == sqrt(2)/2\n    assert cg_simp(CG(1, 1, 1, 0, 2, 1)) == sqrt(2)/2\n", "def test_cg_simp():\n    # Test for numerical alpha,beta\n    a = CG(1, 1, 0, 0, 1, 1)\n    b = CG(1, 0, 0, 0, 1, 0)\n    c = CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(a + b + c) == 3\n    # Test for numerical alpha,alphap,beta,betap\n    a = CG(1, 1, 1, 1, 2, 2)\n    b = CG(1, 1, 1, -1, 2, 0)\n    c = CG(1, -1, 1, 1, 2, 0)\n    d = CG(1, -1, 1, -1, 2, -2)\n    assert cg_simp(a*b + c*d) == 0\n    # Test for symbolic alpha,alphap,beta,betap\n    a = CG(1, 1, 1, 1, 2, 2)\n    b = CG(1, 1, 1, -1, 2, 0)\n    c = CG(1, -1, 1, 1, 2, 0)\n    d = CG(1, -1, 1, -1, 2, -2)\n    alpha, alphap, beta, betap = symbols('alpha alphap beta betap')\n    a = a.subs({1: alpha, -1: alphap})\n    b = b.subs({1: alpha, -1: alphap})\n    c = c.subs({1: beta, -1: betap})\n    d = d.subs({1: beta, -1: betap})\n    assert cg_simp(a*b + c*d) == 0\n"], "sample_1166": ["def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (5, 6, 2)) == (-2, -2, -1)\n", "def test_term_div():\n    x, y, z = symbols('x y z')\n\n    assert term_div(((x, 2), 2), ((x, 1), 1), ZZ) == ((x, 1), 2)\n    assert term_div(((x, 2), 2), ((x, 3), 1), ZZ) is None\n\n    raises(ExactQuotientFailed, lambda: term_div(((x, 2), 2), ((x, 3), 1), QQ))\n    raises(ExactQuotientFailed, lambda: term_div(((x, 2), 2), ((y, 3), 1), QQ))\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (5, 2, 0)) == (2 - 2, 2, 1)\n", "def test_monomial_as_expr():\n    x, y, z = symbols('x y z')\n    monom = Monomial((2, 3, 4), (x, y, z))\n    assert monom.as_expr() == x**2*y**3*z**4\n    assert monom.as_expr(x, y, z) == x**2*y**3*z**4\n    raises(ValueError, lambda: monom.as_expr(a, b, c))\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 5), (1, 2, 3)) == (2, 2, 2)\n    assert monomial_ldiv((1, 2, 3), (4, 5, 6)) == (-3, -3, -3)\n"], "sample_1167": ["def test_Quaternion_conjugate():\n    q = Quaternion(x, y, z, t)\n    assert latex(q.conjugate()) == r\"x - y i - z j - t k\"\n    q = Quaternion(x, y, z, x*t)\n    assert latex(q.conjugate()) == r\"x - y i - z j - t x k\"\n    q = Quaternion(x, y, z, x + t)\n    assert latex(q.conjugate()) == r\"x - y i - z j - \\left(t + x\\right) k\"\n", "def test_parenthesize_super():\n    from sympy.printing.latex import LatexPrinter\n    from sympy import Symbol, Pow\n    lp = LatexPrinter()\n    x = Symbol(\"x\")\n    assert lp.parenthesize_super(lp._print(x)) == r\"{{x}}\"\n    assert lp.parenthesize_super(lp._print(x**2)) == r\"{{x^{2}}}\"\n    assert lp.parenthesize_super(lp._print(Pow(x**2, 2))) == r\"{{x^{2}}}^{2}\"\n    assert lp.parenthesize_super(lp._print(Pow(x**2, -2))) == r\"{{x^{2}}}^{-2}\"\n    assert lp.parenthesize_super(lp._print(Pow(x**(-2), 2))) == r\"{{x^{-2}}}^{2}\"\n    assert lp.parenthesize_super(lp._print(Pow(x**(-2), -2))) == r\"{{x^{-2}}}^{-2}\"\n", "def test_latex_subs_with_piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    subs_expr = expr.subs(x, y)\n    assert latex(subs_expr) == r\"\\begin{cases} y & \\text{for}\\: y < 1 \\\\y^{2} & \\text{otherwise} \\end{cases}\"\n", "def test_PartialDerivative():\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices, TensorHead\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A = TensorHead(\"A\", [L])\n    pd = PartialDerivative(A(i), A(j))\n    assert latex(pd) == r\"\\frac{\\partial}{\\partial {A{}^{j}}}{A{}^{i}}\"\n", "def test_label_eqn():\n    # issue 21445\n    from sympy.parsing.sympy_parser import parse_expr\n    from sympy.printing import latex\n    expr = parse_expr('Eq(2*x+1,10)')\n    assert latex(expr, mode='equation') == r'\\begin{equation}2 x + 1 = 10\\end{equation}'\n    assert latex(expr, mode='equation*') == r'\\begin{equation*}2 x + 1 = 10\\end{equation*}'\n"], "sample_1168": ["def test_roundrobin():\n    import itertools\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n    assert ''.join(roundrobin('ABC', 'D', 'EF')) == 'ADEBFC'\n    assert list(roundrobin('ABC', 'D', 'EF', 'G')) == [\n        'A', 'D', 'E', 'G', 'B', 'F', 'C']\n    assert list(roundrobin('ABCD', 'EFGH', 'IJKL')) == [\n        'A', 'E', 'I', 'B', 'F', 'J', 'C', 'G', 'K', 'D', 'H', 'L']\n    assert list(roundrobin('ABCD', 'EFGH', 'IJKL', 'MNOP')) == [\n        'A', 'E', 'I', 'M', 'B', 'F', 'J', 'N', 'C', 'G', 'K', 'O', 'D', 'H', 'L', 'P']\n    assert list(roundrobin(itertools.count(), 'ah', itertools.count(2))) == [\n        0, 'a', 2, 1, 'h', 3]\n", "def test_roundrobin():\n    gen1 = (i for i in range(3))\n    gen2 = (i for i in range(3, 5))\n    gen3 = (i for i in range(5, 10))\n    assert list(roundrobin(gen1, gen2, gen3)) == [0, 3, 5, 1, 4, 6, 2, 7, 8, 9]\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E', 'B', 'F', 'C']\n", "def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert ''.join(gen) == 'ADEBFC'\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', [1, 2], (3, 4))) == [\n        'A', 1, 3, 'B', 2, 4]\n"], "sample_1169": ["def test_wicks_theorem_fermi():\n    p, q, r, s = symbols('p,q,r,s', above_fermi=True)\n\n    str = F(p)*Fd(q)\n    assert wicks(str) == NO(F(p)*Fd(q)) + KroneckerDelta(p, q)\n    str = Fd(p)*F(q)\n    assert wicks(str) == NO(Fd(p)*F(q))\n\n    str = F(p)*Fd(q)*F(r)*Fd(s)\n    nstr = wicks(str)\n    fasit = NO(\n        KroneckerDelta(p, q)*KroneckerDelta(r, s)\n        + KroneckerDelta(p, q)*AnnihilateFermion(r)*CreateFermion(s)\n        + KroneckerDelta(r, s)*AnnihilateFermion(p)*CreateFermion(q)\n        - KroneckerDelta(p, s)*AnnihilateFermion(r)*CreateFermion(q)\n        - AnnihilateFermion(p)*AnnihilateFermion(r)*CreateFermion(q)*CreateFermion(s))\n    assert nstr == fasit\n", "def test_substitute_dummies_exception():\n    i, j = symbols('i j', below_fermi=True, cls=Dummy)\n    raises(SubstitutionOfAmbigousOperatorFailed, lambda: substitute_dummies(Fd(i)*Fd(j)*Fd(i) + 2))\n", "def test_dummy_order_well_defined_AT():\n    aa, bb = symbols('a b', above_fermi=True)\n    i, j, k, l, m = symbols('i j k l m', below_fermi=True, cls=Dummy)\n    a, b, c, d, e = symbols('a b c d e', above_fermi=True, cls=Dummy)\n    p, q = symbols('p q', cls=Dummy)\n    p1, p2, p3, p4 = symbols('p1 p2 p3 p4', above_fermi=True, cls=Dummy)\n    p5, p6, p7, p8 = symbols('p5 p6 p7 p8', above_fermi=True, cls=Dummy)\n    h1, h2, h3, h4 = symbols('h1 h2 h3 h4', below_fermi=True, cls=Dummy)\n    h5, h6, h7, h8 = symbols('h5 h6 h7 h8', below_fermi=True, cls=Dummy)\n\n    A = Function('A')\n    B = Function('B')\n\n    from sympy.utilities.iterables import variations\n\n    # A*A*A*A*B  --  ordering of p5 and p4 is used to figure out the rest\n    template = atv(p1, p2, p4, p1)*atv(p2, p3, p3, p5)*atv(p5, p4)\n    permutator = variations([a, b, c, d, e], 5)\n    base = template.subs(zip([p1, p2, p3, p4, p5], next(permutator)))\n    for permut in permutator:\n        subslist = zip([p1, p2, p3, p4, p5], permut)\n        expr = template.subs(subslist)\n        assert substitute_dummies(expr) == substitute_dummies(base)\n", "def test_invalid_use_of_BosonState():\n    raises(ValueError, lambda: BosonState([2, 2], 1))\n    raises(ValueError, lambda: BosonState((2, 2), 1))\n    raises(ValueError, lambda: BosonState((2, 2), fermi_level=1))\n", "def test_issue_contract_fermions_with_non_fermion_operators():\n    a = Symbol('a')\n    p, q, r, s = symbols('p,q,r,s', cls=Dummy)\n    assert contraction(F(p), a*F(q)) == 0\n    assert contraction(F(p), a)*F(q) == 0\n    assert contraction(Fd(p), a*Fd(q)) == 0\n"], "sample_1170": ["def test_DMP_str():\n    from sympy.polys.domains import ZZ\n    f = ZZ[x, y].from_expr(x**2*y + x)\n    assert str(f) == 'x**2*y + x'\n    assert sstr(f) == 'x**2*y + x'\n", "def test_ElementwiseApplyFunction_printing():\n    X = MatrixSymbol('X', 2, 2)\n    expr = ElementwiseApplyFunction(sin, X)\n    assert str(expr) == \"sin.(X)\"\n", "def test_ElementwiseApplyFunc():\n    n = Symbol('n', integer=True)\n    X = MatrixSymbol('X', n, n)\n    expr = ElementwiseApplyFunc(sin, X)\n    assert str(expr) == \"sin.(X)\"\n", "def test_issue_2860():\n    assert str(Pow(S(2), -1.0, evaluate=False)) == '2**(-1.0)'\n    assert str(Pow(S(2), 1.0, evaluate=False)) == '2**1.0'\n", "def test_ArraySymbol():\n    assert str(ArraySymbol(\"A\", (2,3))) == \"A\"\n    assert sstr(ArraySymbol(\"A\", (2,3))) == \"A\"\n    assert sstrrepr(ArraySymbol(\"A\", (2,3))) == \"ArraySymbol('A', (2, 3))\"\n    assert str(ArraySymbol(\"A\", (2,3))[1,2]) == \"A[1, 2]\"\n    assert sstr(ArraySymbol(\"A\", (2,3))[1,2]) == \"A[1, 2]\"\n    assert sstrrepr(ArraySymbol(\"A\", (2,3))[1,2]) == \"ArrayElement(A, (1, 2))\"\n"], "sample_1171": ["def test_issue_18345():\n    from sympy.solvers.solveset import solveset\n    eq = 2*x**3 + 3*x**2 + x + 1\n    s = solveset(eq, x, domain=S.Rationals)\n    assert s.is_empty\n", "def test_issue_19691():\n    i = Symbol('i', integer=True)\n    assert Range(i, i + 10).intersect(Range(5, 15)) == Intersection(Range(i, i + 10), Range(5, 15), evaluate=False)\n    assert Range(i, i + 10, 2).intersect(Range(5, 15)) == Intersection(Range(i, i + 10, 2), Range(5, 15), evaluate=False)\n    assert Range(i, 15, 2).intersect(Range(5, 15)) == Intersection(Range(i, 15, 2), Range(5, 15), evaluate=False)\n    assert Range(5, i + 10).intersect(Range(5, 15)) == Intersection(Range(5, i + 10), Range(5, 15), evaluate=False)\n    assert Range(5, i + 10, 2).intersect(Range(5, 15)) == Intersection(Range(5, i + 10, 2), Range(5, 15), evaluate=False)\n    assert Range(5, 15, 2).intersect(Range(5, i + 10)) == Intersection(Range(5, 15, 2), Range(5, i + 10), evaluate=False)\n", "def test_issue_18446():\n    c1 = ComplexRegion(Interval(0, 2)*Interval(0, 1), polar=True)\n    c2 = ComplexRegion(Interval(1, 3)*Interval(0, 1), polar=True)\n    assert Intersection(c1, c2) == ComplexRegion(Interval(1, 2)*Interval(0, 1), polar=True)\n", "def test_issue_19501():\n    from sympy.abc import n\n    from sympy import ceiling\n\n    assert Range(5, 10).size == 5\n    assert Range(10, 5, -1).size == 5\n    assert Range(10, 15, 2).size == 3\n    assert Range(1, 10, 3).size == 3\n    assert Range(oo, -oo, -3).size is S.Infinity\n    assert Range(ceiling(2.9), 10, 3).size == 3\n\n    assert Range(n, n + 1).size == 1\n    assert Range(n, n + 2).size == 2\n    assert Range(n, n + 3).size == 3\n    assert Range(n, n + 1, 2).size == 1\n    assert Range(n, n + 2, 2).size == 1\n    assert Range(n, n + 3, 2).size == 2\n    assert Range(n, n + 1, 3).size == 1\n    assert Range(n, n + 2, 3).size == 1\n    assert Range(n, n + 3, 3).size == 1\n    assert Range(n, n + 4, 3).size == 2\n", "def test_issue_18413():\n    r1 = Range(2, oo)\n    r2 = Range(-oo, 2)\n    r3 = Range(0, 4, 2)\n\n    assert r1._contains(0) == False\n    assert r2._contains(2) == False\n    assert r3._contains(3) == False\n    assert r3._contains(1) == False\n\n    assert r1._contains(2) == True\n    assert r2._contains(1) == True\n    assert r3._contains(0) == True\n    assert r3._contains(2) == True\n"], "sample_1173": ["def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation, )\n    x = Symbol('x')\n    inputs = {\n        'lambda x: x**2': Lambda(x, x**2),\n        'lambda x,y: x + y': Lambda((x, Symbol('y')), x + Symbol('y')),\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, transformations=transformations) == result\n", "def test_parse_expr_with_lambda():\n    x = Symbol('x')\n    f = Function('f')\n    lambda_func = Function('Lambda')\n    assert parse_expr('lambda x: x*2', transformations=(lambda_notation,)) == lambda_func(x, x*2)\n    assert parse_expr('lambda x, y: x*y', transformations=(lambda_notation,)) == lambda_func((x, Symbol('y')), x*Symbol('y'))\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    assert parse_expr('lambda x: x**2', transformations=transformations) == (x**2)\n    assert parse_expr('Lambda(x, x**2)') == (x**2)\n", "def test_lambda_notation():\n    transformations = standard_transformations + (lambda_notation,)\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"lambda: 1\", transformations=transformations) == Lambda(None, 1)\n    assert parse_expr(\"lambda x: x\", transformations=transformations) == Lambda(x, x)\n    assert parse_expr(\"lambda x, y: x + y\", transformations=transformations) == Lambda((x, y), x + y)\n", "def test_parse_expr_evaluate_false_with_exponent_and_parentheses():\n    transformations = standard_transformations\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('(2**x)**y', transformations=transformations, evaluate=False) == (2**x)**y\n    assert parse_expr('2**(x+y)', transformations=transformations, evaluate=False) == 2**(x+y)\n    assert parse_expr('(2**x)**(y+1)', transformations=transformations, evaluate=False) == (2**x)**(y+1)\n    assert parse_expr('2**(x*(y+1))', transformations=transformations, evaluate=False) == 2**(x*(y+1))\n"], "sample_1172": ["def test_solve_generic():\n    x0, y0, x1, y1, r = symbols('x0 y0 x1 y1 r')\n\n    f_1 = (x - 1)**2 + (y - 1)**2 - r**2\n    f_2 = (x - 2)**2 + (y - 2)**2 - r**2\n    s = sqrt(2*r**2 - 1)\n    a = (3 - s)/2\n    b = (3 + s)/2\n    assert solve_generic([f_1, f_2], Options((x, y), {'domain': 'ZZ'})) == [(a, b), (b, a)]\n\n    f_1 = (x - 1)**2 + (y - 2)**2 - r**2\n    f_2 = (x - 1)**2 + (y - 1)**2 - r**2\n\n    assert solve_generic([f_1, f_2], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(1 - sqrt((2*r - 1)*(2*r + 1))/2, Rational(3, 2)),\n         (1 + sqrt((2*r - 1)*(2*r + 1))/2, Rational(3, 2))]\n\n    query = lambda expr: expr.is_Pow and expr.exp is S.Half\n\n    f_1 = (x - 1 )**2 + (y - 2)**2 - r**2\n    f_2 = (x - x1)**2 + (y - 1)**2 - r**2\n\n    result = solve_generic([f_1, f_2], Options((x, y), {'domain': 'ZZ'}))\n\n    assert len(result) == 2 and all(len(r) == 2 for r in result)\n    assert all(r.count(query) == 1 for r in flatten(result))\n\n    f_1 = (x - x0)**2 + (y - y0)**2 - r**2\n    f_2 = (x - x1)**2 + (y - y1)**2 - r**2\n\n    result = solve_generic([f_1, f_2], Options((x, y), {'domain': 'ZZ'}))\n\n    assert len(result) == 2 and all(len(r) ==", "def test_solve_generic():\n    assert solve_generic([Poly(x - 1)], Options((x,), {'domain': 'ZZ'})) == [(1,)]\n\n    assert solve_generic([Poly(y - x, y, x), Poly(y - x - 1, y, x)],\n                         Options((y, x), {'domain': 'ZZ'})) is None\n\n    assert solve_generic([Poly(y - x**2, y, x), Poly(y + x**2, y, x)],\n                         Options((y, x), {'domain': 'ZZ'})) == [(0, 0)]\n\n    assert solve_generic([Poly(2*x - 3, x), Poly(y*Rational(3, 2) - 2*x, y, x),\n                          Poly(z - 5*y, z, y, x)],\n                         Options((x, y, z), {'domain': 'QQ'})) == \\\n        [(Rational(3, 2), Integer(2), Integer(10))]\n\n    assert solve_generic([Poly(x*y - 2*y, x, y), Poly(2*y**2 - x**2, x, y)],\n                         Options((x, y), {'domain': 'ZZ'})) == \\\n        [(0, 0), (2, -sqrt(2)), (2, sqrt(2))]\n", "def test_solve_generic():\n    x, y, z = symbols('x y z')\n\n    f_1 = x**2 + y + z - 1\n    f_2 = x + y**2 + z - 1\n    f_3 = x + y + z**2 - 1\n\n    a, b = sqrt(2) - 1, -sqrt(2) - 1\n\n    assert solve_generic([Poly(f_1, x, y, z), Poly(f_2, x, y, z), Poly(f_3, x, y, z)], Options((x, y, z), {'domain': 'ZZ'})) == \\\n        [(0, 0, 1), (0, 1, 0), (1, 0, 0)]\n\n    f_1 = x**2 + y - 1\n    f_2 = x + y**2 - 1\n\n    assert solve_generic([Poly(f_1, x, y), Poly(f_2, x, y)], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(0, -1), (0, 1)]\n\n    f_1 = x*y - 2*y\n    f_2 = 2*y**2 - x**2\n\n    assert solve_generic([Poly(f_1, x, y), Poly(f_2, x, y)], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(0, 0), (2, -sqrt(2)), (2, sqrt(2))]\n", "def test_solve_generic():\n    x0, y0, x1, y1, r = symbols('x0 y0 x1 y1 r')\n\n    f_1 = (x - 1)**2 + (y - 1)**2 - r**2\n    f_2 = (x - 2)**2 + (y - 2)**2 - r**2\n    s = sqrt(2*r**2 - 1)\n    a = (3 - s)/2\n    b = (3 + s)/2\n    assert solve_generic([Poly(f_1, x, y), Poly(f_2, x, y)], Options((x, y), {'domain': 'ZZ'})) == [(a, b), (b, a)]\n\n    f_1 = (x - 1)**2 + (y - 2)**2 - r**2\n    f_2 = (x - 1)**2 + (y - 1)**2 - r**2\n\n    assert solve_generic([Poly(f_1, x, y), Poly(f_2, x, y)], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(1 - sqrt((2*r - 1)*(2*r + 1))/2, Rational(3, 2)),\n         (1 + sqrt((2*r - 1)*(2*r + 1))/2, Rational(3, 2))]\n\n    f_1 = x**2 + y\n    f_2 = x + y**4\n    assert solve_generic([Poly(f_1, x, y), Poly(f_2, x, y)], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(0, 0), (-1/4, -1/256)]\n", "def test_solve_generic():\n    f_1 = x**2 + y - 2\n    f_2 = x - y + 1\n\n    assert solve_generic([Poly(f_1, x, y), Poly(f_2, x, y)], Options((x, y), {'domain': 'ZZ'})) == [(-1, 1)]\n\n    f_1 = x**2 + y - 3\n    f_2 = x - y + 4\n\n    assert solve_generic([Poly(f_1, x, y), Poly(f_2, x, y)], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(1/2 - sqrt(21)/2, 7/2 - sqrt(21)/2), (1/2 + sqrt(21)/2, 7/2 + sqrt(21)/2)]\n\n    f_1 = x**2 + y\n    f_2 = x + y**4\n\n    assert solve_generic([Poly(f_1, x, y), Poly(f_2, x, y)], Options((x, y), {'domain': 'ZZ'})) == \\\n        [(0, 0), (1/4, -1/16**(1/3)), (1/4, (1/16)**(1/3)*(1/2 - sqrt(3)*I/2)), \n         (1/4, (1/16)**(1/3)*(1/2 + sqrt(3)*I/2))]\n"], "sample_1174": ["def test_issue_20445():\n    from sympy import symbols, Eq, solve, Function\n    x, y = symbols('x y', real=True)\n    f = Function('f')\n    # In the following case, as_real_imag is called internally\n    # from solve and used to return an error.\n    # The error was fixed but a test case was needed.\n    # Here it is:\n    assert solve(Eq(f(x + I*y), 0), x) == []\n", "def test_polar_lift_periodic_argument_principal_branch_coverage():\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n    n = Symbol('n', negative=True)\n    assert polar_lift(x).is_polar is True\n    assert polar_lift(p).is_polar is True\n    assert polar_lift(n).is_polar is True\n    assert periodic_argument(x, p).is_real is True\n    assert periodic_argument(p, p).is_real is True\n    assert periodic_argument(n, p).is_real is True\n    assert principal_branch(x, p).is_polar is True\n    assert principal_branch(p, p).is_polar is True\n    assert principal_branch(n, p).is_polar is True\n    assert polar_lift(x).as_base_exp().is_polar is True\n    assert periodic_argument(x, p).as_base_exp().is_polar is None\n    assert principal_branch(x, p).as_base_exp().is_polar is True\n    assert polar_lift(x).as_numer_denom().is_polar is True\n    assert periodic_argument(x, p).as_numer_denom().is_polar is None\n    assert principal_branch(x, p).as_numer_denom().is_polar is True\n", "def test_principal_branch_coverage():\n    from sympy import principal_branch, exp_polar, pi, I\n    x = Symbol('x')\n    assert principal_branch(1, pi).args[0] == exp_polar(0)\n    assert principal_branch(exp_polar(2*pi*I), 2*pi).args[0] == 1\n    assert principal_branch(x, oo).args[0] == x\n    assert principal_branch(exp_polar(I*pi/2), pi).args[0] == exp_polar(I*pi/2)\n    assert principal_branch(exp_polar(3*I*pi/2), pi).args[0] == exp_polar(-I*pi/2)\n", "def test_issue_21476():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    u = Symbol('u', nonnegative=True)\n\n    assert Abs(x + I).as_real_imag() == (sqrt(x**2 + 1), 0)\n    assert Abs(x + I*y).as_real_imag() == (sqrt(x**2 + y**2), 0)\n    assert Abs(u + I).as_real_imag() == (sqrt(u**2 + 1), 0)\n", "def test_periodic_argument_edge_cases():\n    from sympy import periodic_argument, pi, I, exp_polar, oo\n    assert periodic_argument(exp_polar(2*pi*I), 2*pi) == 0\n    assert periodic_argument(exp_polar(-4*pi*I), 2*pi) == 0\n    assert periodic_argument(exp_polar(I*pi/2), 4*pi) == pi/2\n    assert periodic_argument(exp_polar(-I*pi/2), 4*pi) == -pi/2\n    assert periodic_argument(1, 2*pi) == 0\n    assert periodic_argument(-1, 2*pi) == pi\n    assert periodic_argument(1, oo) == 0\n    assert periodic_argument(-1, oo) == pi\n"], "sample_1175": ["def test_pretty_RandomSymbol():\n    from sympy.stats import RandomSymbol\n    assert pretty(RandomSymbol('x')) == 'x'\n    assert pretty(RandomSymbol('x')(symbol='y')) == 'y'\n", "def test_pretty_NDimArray():\n    from sympy.tensor.array import ImmutableDenseNDimArray, ImmutableSparseNDimArray, MutableDenseNDimArray, MutableSparseNDimArray\n\n    M = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    assert pretty(M) == \"[[1, 2], [3, 4]]\"\n    assert upretty(M) == \"[[1, 2], [3, 4]]\"\n\n    M = ImmutableSparseNDimArray([[1, 2], [3, 4]])\n    assert pretty(M) == \"[[1, 2], [3, 4]]\"\n    assert upretty(M) == \"[[1, 2], [3, 4]]\"\n\n    M = MutableDenseNDimArray([[1, 2], [3, 4]])\n    assert pretty(M) == \"[[1, 2], [3, 4]]\"\n    assert upretty(M) == \"[[1, 2], [3, 4]]\"\n\n    M = MutableSparseNDimArray([[1, 2], [3, 4]])\n    assert pretty(M) == \"[[1, 2], [3, 4]]\"\n    assert upretty(M) == \"[[1, 2], [3, 4]]\"\n\n    M = ImmutableDenseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    assert pretty(M) == \"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\"\n    assert upretty(M) == \"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\"\n\n    M = ImmutableSparseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    assert pretty(M) == \"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\"\n    assert upretty(M) == \"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\"\n\n    M = MutableDenseNDimArray([[[1, 2], [3, 4", "def test_pretty_unordered():\n    expr = Mul(2, 3, x, evaluate=False)\n    assert pretty(expr, order='none') == '2*3*x'\n    assert upretty(expr, order='none') == '2\u22c53\u22c5x'\n\n    expr = Mul(2, x, 3, evaluate=False)\n    assert pretty(expr, order='none') == '2*x*3'\n    assert upretty(expr, order='none') == '2\u22c5x\u22c53'\n\n    expr = Add(2, 3, x, evaluate=False)\n    assert pretty(expr, order='none') == '2 + 3 + x'\n    assert upretty(expr, order='none') == '2 + 3 + x'\n\n    expr = Add(2, x, 3, evaluate=False)\n    assert pretty(expr, order='none') == '2 + x + 3'\n    assert upretty(expr, order='none') == '2 + x + 3'\n", "def test_pretty_BooleanFunction():\n    # Test that the printer dispatcher correctly handles functions\n    class MyFunction(BoolFunction):\n        pass\n    x = MyFunction(x)\n    assert pretty(x) == str(x) == \"MyFunction(x)\"\n    assert upretty(x) == \"MyFunction(x)\"\n", "def test_issue_18643():\n    from sympy.physics.quantum.constants import hbar\n    assert pretty(hbar) == 'hbar'\n    assert upretty(hbar) == '\u210f'\n"], "sample_1176": ["def test_Rational_ceiling_floor():\n    a = Rational(4, 3)\n\n    assert a.floor() == 1\n    assert a.ceiling() == 2\n", "def test_issue_20886():\n    x = Symbol('x')\n    assert str(sqrt(x**2 - 2*x + 1)) == 'sqrt((x - 1)**2)'\n    assert str(sqrt(x**2 + 2*x + 1)) == 'sqrt((x + 1)**2)'\n    assert str(sqrt(x**2 + 2*x + 2)) == 'sqrt(x**2 + 2*x + 2)'\n    assert str(sqrt(-x**2 - 2*x - 2)) == 'sqrt(-(x + 1)**2 - 1)'\n", "def test_float_rounding():\n    assert Float('1.123456789012345', '15')._mpf_ == mpf('1.123456789012345', 53)\n    assert Float('1.123456789012345', '16')._mpf_ == mpf('1.123456789012345', 53)\n    assert Float('1.123456789012345', '17')._mpf_ == mpf('1.123456789012345', 53)\n    assert Float('1.123456789012345', '18')._mpf_ == mpf('1.123456789012345', 64)\n    assert Float('1.123456789012345', '19')._mpf_ == mpf('1.123456789012345', 64)\n    assert Float('1.123456789012345', '20')._mpf_ == mpf('1.123456789012345', 64)\n", "def test_mpf_tuple_input():\n    mpf1 = (0, int(123), -45, 53)\n    mpf2 = (0, int(123), 0, 53)\n    assert Float(mpf1)._mpf_ == mpf1\n    assert Float(mpf2)._mpf_ == mpf2\n    assert Float(mpf1) + Float(mpf2) == Float((0, int(123), -45, 53)) + Float((0, int(123), 0, 53))\n", "def test_issue_22085():\n    assert (oo + I) * oo is oo\n    assert (oo + I) * -oo is -oo\n    assert (oo + I) * zoo is zoo\n    assert (-oo + I) * oo is -oo\n    assert (-oo + I) * -oo is oo\n    assert (-oo + I) * zoo is zoo\n    assert (zoo + I) * oo is zoo\n    assert (zoo + I) * -oo is zoo\n    assert (zoo + I) * zoo is zoo\n"], "sample_1177": ["def test_issue_22567():\n    from sympy.functions.elementary.complexes import arg\n    x = Symbol('x')\n    assert arg(x).is_finite is True\n    assert arg(x).is_real is True\n", "def test_issue_22513():\n    x = Symbol('x', real=True)\n    assert sign(sign(x)*I) == I\n    assert sign(sign(x)*-I) == -I\n    assert sign(sign(x)*x) == sign(x)\n", "def test_polar_lift():\n    from sympy.functions.elementary.complexes import polar_lift\n    x, y = symbols('x y')\n    a, b = symbols('a b', real=True)\n\n    assert polar_lift(nan) is nan\n\n    assert polar_lift(oo) is oo\n    assert polar_lift(-oo) is oo\n\n    assert polar_lift(0) == 0\n\n    assert polar_lift(1) == 1\n    assert polar_lift(-1) == exp_polar(I*pi)\n\n    assert polar_lift(E) == E\n    assert polar_lift(-E) == E*exp_polar(I*pi)\n\n    assert polar_lift(x).as_real_imag() == (re(x)*cos(arg(x)), re(x)*sin(arg(x)))\n    assert polar_lift(x).args == (x,)\n    assert polar_lift(x*I).as_real_imag() == (im(x), re(x))\n    assert polar_lift(r*I).as_real_imag() == (0, r)\n    assert polar_lift(r).as_real_imag() == (r, 0)\n    assert polar_lift(i*I).as_real_imag() == (-im(i), re(i))\n    assert polar_lift(i).as_real_imag() == (re(i), im(i))\n\n    assert polar_lift(x + y).args == (x + y,)\n    assert polar_lift(x + r).args == (x + r,)\n    assert polar_lift(x + r*I).args == (x + r*I,)\n    assert polar_lift(x + y*I).args == (x + y*I,)\n\n    assert polar_lift(log(2*I)).args == (log(2*I),)\n    assert polar_lift((2 + I)**2).args == ((2 + I)**2,)\n\n    assert polar_lift(conjugate(x)).args == (conjugate(x),)\n    assert conjugate(polar_lift(x)).args == (conjugate(polar_lift(x)),)\n\n    assert polar_lift(x).diff(x) == polar_lift(1)\n    assert polar_lift(x).diff(y) == 0\n\n    assert polar_lift(sqrt(a + b*I)).args == (sqrt(a + b*I),)\n    assert polar_lift(a * (2 + b*I)).args == (a * (2 + b*I),)\n\n    assert polar_lift((1 + sqrt(a + b*I))/2).args == ((1 + sqrt(a + b*I))/2,)\n\n    assert polar_lift", "def test_issue_9263():\n    # Ensure that re and im can handle powers of numbers with a rational\n    # exponent.\n    a, b = symbols('a b', real=True)\n    assert re((1 + I)**Rational(1, 2)) == sqrt(2)/2\n    assert im((1 + I)**Rational(1, 2)) == sqrt(2)/2\n    assert re((a + I*b)**Rational(1, 2)) == \\\n        (a**2 + b**2)**Rational(1, 4)*cos(atan2(b, a)/2)\n    assert im((a + I*b)**Rational(1, 2)) == \\\n        (a**2 + b**2)**Rational(1, 4)*sin(atan2(b, a)/2)\n    assert re((a + I*b)**Rational(1, 3)) == \\\n        (a**2 + b**2)**Rational(1, 6)*cos(atan2(b, a)/3)\n    assert im((a + I*b)**Rational(1, 3)) == \\\n        (a**2 + b**2)**Rational(1, 6)*sin(atan2(b, a)/3)\n    assert re((-2)**Rational(1, 4)) == 2**Rational(1, 4)*cos(pi*Rational(5, 4))\n    assert im((-2)**Rational(1, 4)) == 2**Rational(1, 4)*sin(pi*Rational(5, 4))\n", "def test_polar_lift():\n    from sympy.functions.elementary.complexes import polar_lift\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == exp_polar(0)\n    assert polar_lift(-1) == exp_polar(I*pi)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(y) == y*exp_polar(0)\n    assert polar_lift(1 + I) == polar_lift(1 + I)\n    assert polar_lift((1 + I)**2) == polar_lift(2*I)\n    assert polar_lift((1 + I)**4) == exp_polar(I*2*pi)\n\n    assert polar_lift(1).as_real_imag() == (1, 0)\n    assert polar_lift(I).as_real_imag() == (0, 1)\n    assert polar_lift(1 + I).as_real_imag() == (1, 1)\n\n    assert conjugate(polar_lift(x)) == polar_lift(conjugate(x))\n    assert polar_lift(conjugate(x)) == conjugate(polar_lift(x))\n\n    assert polar_lift(x).diff(x) == Derivative(polar_lift(x), x)\n    assert polar_lift(y).diff(y) == exp_polar(0)\n\n    assert re(polar_lift(x)) == re(polar_lift(x))\n    assert im(polar_lift(x)) == im(polar_lift(x))\n    assert re(polar_lift(y)) == y\n    assert im(polar_lift(y)) == 0\n"], "sample_1178": ["def test_Variable__as_Declaration():\n    vx = Variable(x, type=real)\n    decl = Declaration(vx)\n    assert decl.variable == vx\n    vxy = Variable(x, type=real, value=y)\n    assert vxy.as_Declaration() == Declaration(vxy)\n    assert vx.as_Declaration(value=y) == Declaration(vxy)\n    assert vx.as_Declaration(type=integer) == Declaration(Variable(x, type=integer))\n    assert vx.as_Declaration(attrs={value_const}) == Declaration(Variable(x, type=real, attrs={value_const}))\n", "def test_Element():\n    elem = Element('x', (1, 2, 3))\n    assert elem.symbol.name == 'x'\n    assert elem.indices == (1, 2, 3)\n    assert elem.strides == None\n    assert elem.offset == None\n    assert elem.kwargs() == {}\n    assert elem == Element('x', '123', strides=None, offset=None)\n    assert elem != Element('x', '123', strides=(4, 5, 6))\n    assert elem != Element('x', '123', offset=5)\n    elem2 = Element('x', (1, 2, 3), strides='lmn', offset='o')\n    assert elem2.strides == ('l', 'm', 'n')\n    assert elem2.offset.name == 'o'\n    assert elem2 != elem\n    assert elem2.kwargs() == {}\n    assert elem2.func(*elem2.args) == elem2\n", "def test_Element():\n    elem = Element('x', 'ijk')\n    assert elem.symbol.name == 'x'\n    assert elem.indices == (Symbol('i'), Symbol('j'), Symbol('k'))\n    assert elem.strides == none\n    assert elem.offset == none\n    assert elem == Element('x', 'ijk')\n\n    elem2 = Element('x', 'ijk', strides='lmn', offset='o')\n    assert elem2.strides == Tuple(Symbol('l'), Symbol('m'), Symbol('n'))\n    assert elem2.offset == Symbol('o')\n    assert elem2.func(*elem2.args) == elem2\n\n    assert elem != elem2\n", "def test_Element():\n    elem = Element(x, (i, j))\n    assert str(elem.symbol) == 'x'\n    assert elem.indices == Tuple(i, j)\n    assert elem.strides == None\n    assert elem.offset == None\n    assert elem == Element(x, (i, j))\n    assert elem != Element(y, (i, j))\n    assert elem != Element(x, (j, i))\n    assert elem.func(*elem.args) == elem\n    elem2 = Element(x, (i, j), strides=(2, 3))\n    assert elem != elem2\n    assert elem2.strides == Tuple(2, 3)\n    elem3 = Element(x, (i, j), strides=(2, 3), offset=4)\n    assert elem3.offset == 4\n    assert elem3 != elem2\n    assert Element('A', 'i').indices == Tuple(symbols('i'))\n    assert Element('B', symbols('i')).indices == Tuple(symbols('i'))\n    raises(TypeError, lambda: Element('A', (1, 2, 'c')))\n", "def test_Stream():\n    st = Stream('stderr')\n    assert st.name == String('stderr')\n    assert st == Stream('stderr')\n    assert st != Stream('stdout')\n    assert st.func(*st.args) == st\n"], "sample_1179": ["def test_ArraySymbol_printing():\n    A = ArraySymbol(\"A\", shape=(3, 3))\n    B = ArraySymbol(\"B\", shape=(3, 3))\n\n    assert str(A - A*B - B) == \"A - A*B - B\"\n    assert str(A*B - (A+B)) == \"-A + A*B - B\"\n", "def test\u8ba2\u5355():\n    # Issue 21862\n    assert str(S.One) == \"1\"\n    assert sstr(S.One, sympy_integers=True) == \"S(1)\"\n", "def test_ElementwiseApplyFunction():\n    from sympy.tensor.array import Array, tensorarray\n    from sympy.tensor.array.expressions.array_expressions import ArrayElementwiseApplyFunc\n    tensor = tensorarray([1, 2, 3])\n    exp = sin(tensor)\n\n    assert str(tensor) == \"Array([1, 2, 3])\"\n    assert str(exp) == \"sin(Array([1, 2, 3]))\"\n    assert str(tensor.applyfunc(sin)) == \"sin(Array([1, 2, 3]))\"\n    assert str(tensor.applyfunc(lambda x: 1/x)) == \"Lambda(x, 1/x).(Array([1, 2, 3]))\"\n    assert str(tensor.applyfunc(lambda x, y: x + y)) == \"Lambda((x, y), x + y).(Array([1, 2, 3]))\"\n", "def test_DMP_printing():\n    F, a = ZZ['a'].objgen()\n    R, x = ZZ['a']['x'].objgen()\n    f = x**3 + 2*x**2 + 3*x + 4\n    assert str(R.convert(f)) == \"Poly(x**3 + 2*x**2 + 3*x + 4, x, domain='ZZ[a]')\"\n\n    F, a = QQ['a'].objgen()\n    R, x = QQ['a']['x'].objgen()\n    f = x**3 + 2*x**2 + 3*x + 4\n    assert str(R.convert(f)) == \"Poly(x**3 + 2*x**2 + 3*x + 4, x, domain='QQ[a]')\"\n\n    F, a = ZZ['a'].objgen()\n    R, x = ZZ['a']['x'].objgen()\n    f = x**3 + 2*x**2 + 3*x + 4\n    g = 2*x**3 - 3*x**2 + 4*x - 5\n    assert str(R.convert(f) + R.convert(g)) == \"Poly(3*x**3 - x**2 + 7*x - 1, x, domain='ZZ[a]')\"\n\n    F, a = QQ['a'].objgen()\n    R, x = QQ['a']['x'].objgen()\n    f = x**3 + 2*x**2 + 3*x + 4\n    g = 2*x**3 - 3*x**2 + 4*x - 5\n    assert str(R.convert(f) + R.convert(g)) == \"Poly(3*x**3 - x**2 + 7*x - 1, x, domain='QQ[a]')\"\n", "def test_ElementwiseApplyFunction():\n    A = MatrixSymbol(\"A\", 2, 2)\n    expr = ElementwiseApplyFunction(sin, A)\n    assert str(expr) == \"sin.(A)\"\n    expr = ElementwiseApplyFunction(lambda x: x**2, A)\n    assert str(expr) == \"<lambda>.(A)\"\n"], "sample_1181": ["def test_scipy_print_methods():\n    prntr = SciPyPrinter()\n    assert hasattr(prntr, '_print_erf')\n    assert hasattr(prntr, '_print_factorial')\n    assert hasattr(prntr, '_print_beta')\n    assert hasattr(prntr, '_print_airyai')\n    assert hasattr(prntr, '_print_airyaiprime')\n    assert hasattr(prntr, '_print_airybi')\n    assert hasattr(prntr, '_print_airybiprime')\n    assert hasattr(prntr, '_print_Integral')\n", "def test_scipy_special_printer():\n    if not import_module('scipy'):\n        skip(\"SciPy not installed\")\n\n    printer = SciPyPrinter()\n\n    # Test printer for special functions\n    assert printer.doprint(sqrt(x)) == 'scipy.special.sqrt(x)'\n    assert printer.doprint(log1p(x)) == 'scipy.special.log1p(x)'\n    assert printer.doprint(expm1(x)) == 'scipy.special.expm1(x)'\n\n    # Test printer for constants\n    assert printer.doprint(sqrt(2)) == 'scipy.constants.sqrt(2)'\n    assert printer.doprint(sqrt(3)) == 'scipy.constants.sqrt(3)'\n", "def test_numpy_print_methods_scipy():\n    prntr = SciPyPrinter()\n    assert hasattr(prntr, '_print_assoc_legendre')\n    assert hasattr(prntr, '_print_lowergamma')\n    assert hasattr(prntr, '_print_uppergamma')\n    assert hasattr(prntr, '_print_betainc')\n    assert hasattr(prntr, '_print_betainc_regularized')\n    assert hasattr(prntr, '_print_fresnels')\n    assert hasattr(prntr, '_print_fresnelc')\n    assert hasattr(prntr, '_print_airyai')\n    assert hasattr(prntr, '_print_airyaiprime')\n    assert hasattr(prntr, '_print_airybi')\n    assert hasattr(prntr, '_print_airybiprime')\n", "def test_scipy_print_methods():\n    prntr = SciPyPrinter()\n    assert hasattr(prntr, '_print_erf')\n    assert hasattr(prntr, '_print_factorial')\n    assert hasattr(prntr, '_print_gamma')\n    assert hasattr(prntr, '_print_loggamma')\n", "def test_scipy_printer_on_matrix_inverse():\n    if not import_module('scipy'):\n        skip(\"SciPy not installed\")\n\n    M = MatrixSymbol(\"M\", 3, 3)\n    assert SciPyPrinter().doprint(M.inv()) == 'scipy.linalg.inv(M)'\n"], "sample_1180": ["def test_affine_rank():\n    assert Point.affine_rank(Point(1, 1), Point(2, 2)) == 1\n    assert Point.affine_rank(Point(1, 1), Point(1, 2)) == 1\n    assert Point.affine_rank(Point(1, 1), Point(2, 3)) == 2\n    assert Point.affine_rank(Point(1, 1, 1), Point(2, 2, 2), Point(3, 3, 3)) == 1\n    assert Point.affine_rank(Point(1, 1, 1), Point(1, 1, 2), Point(1, 2, 1)) == 2\n    assert Point.affine_rank(Point(1, 1, 1), Point(1, 1, 2), Point(1, 2, 1), Point(2, 1, 1)) == 3\n    assert Point.affine_rank() == -1\n", "def test_affine_rank():\n    assert Point.affine_rank() == -1\n    assert Point.affine_rank(Point(0, 0)) == 0\n    assert Point.affine_rank(Point(0, 0), Point(1, 1)) == 1\n    assert Point.affine_rank(Point(0, 0), Point(1, 0), Point(0, 1)) == 2\n    assert Point.affine_rank(Point(0, 0, 0), Point(1, 0, 0), Point(0, 1, 0)) == 2\n    assert Point.affine_rank(Point(0, 0, 0), Point(1, 0, 0), Point(0, 1, 0), Point(0, 0, 1)) == 3\n", "def test_ambient_dimension():\n    assert Point(1, 2).ambient_dimension == 2\n    assert Point(1, 2, 3).ambient_dimension == 3\n    assert Point(1, 2, 3, 4).ambient_dimension == 4\n    assert Point2D(1, 2).ambient_dimension == 2\n    assert Point3D(1, 2, 3).ambient_dimension == 3\n", "def test_project():\n    a = Point(1, 1)\n    b = Point(2, 2)\n    assert a.project(b) == Point(1, 1)\n    assert a.project(b - a) == a\n    assert a.project(a - b) == a\n    assert a.project(b*2) == Point(2, 2)\n    c = Point(3, 3, 3)\n    raises(ValueError, lambda: a.project(c))\n    raises(ValueError, lambda: a.project(Point(0, 0)))\n", "def test_direction_cosine():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(2, 3, 5)\n    direction_cosine = p1.direction_cosine(p2)\n    assert len(direction_cosine) == 3\n    assert all(isinstance(i, Basic) for i in direction_cosine)\n\n    # Test with integer coordinates\n    p1 = Point3D(1, 0, 0)\n    p2 = Point3D(2, 0, 0)\n    direction_cosine = p1.direction_cosine(p2)\n    assert direction_cosine == [1, 0, 0]\n\n    # Test with zero vector\n    p1 = Point3D(1, 1, 1)\n    p2 = Point3D(1, 1, 1)\n    raises(ValueError, lambda: p1.direction_cosine(p2))\n"], "sample_1182": ["def test_expm1_log1p_cosm1():\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expm1(x)) == 'scipy.special.expm1(x)'\n    assert prntr.doprint(log1p(x)) == 'numpy.log1p(x)'\n    assert prntr.doprint(cosm1(x)) == 'scipy.special.cosm1(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expm1(x)) == 'numpy.expm1(x)'\n    assert prntr.doprint(log1p(x)) == 'numpy.log1p(x)'\n    assert prntr.doprint(cosm1(x)) == 'numpy.cos(x) - 1'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expm1(x)) == 'math.expm1(x)'\n    assert prntr.doprint(log1p(x)) == 'math.log1p(x)'\n    assert prntr.doprint(cosm1(x)) == 'math.cos(x) - 1'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expm1(x)) == 'mpmath.expm1(x)'\n    assert prntr.doprint(log1p(x)) == 'mpmath.log1p(x)'\n    assert prntr.doprint(cosm1(x)) == 'mpmath.cos(x) - 1'\n", "def test_issue_16535_16536_cosm1():\n    prntr = SciPyPrinter()\n    assert prntr.doprint(cosm1(x)) == 'scipy.special.cosm1(x)'\n\n    prntr = NumPyPrinter()\n    assert \"Not supported\" in prntr.doprint(cosm1(x))\n\n    prntr = PythonCodePrinter()\n    assert \"Not supported\" in prntr.doprint(cosm1(x))\n\n    prntr = MpmathPrinter()\n    assert \"Not supported\" in prntr.doprint(cosm1(x))\n", "def test_issue_log1p():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(log1p(x)) == \"math.log(x + 1)\"\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(log1p(x)) == \"(mpmath.log(x + 1))\"\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(log1p(x)) == \"scipy.special.log1p(x)\"\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(log1p(x)) == \"numpy.log1p(x)\"\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(log1p(x)) == \"sympy.functions.elementary.exponential.log(x + 1)\"\n", "def test_SymPyPrinter_printcode():\n    from sympy import atan2, gamma\n    from sympy.abc import a, b\n\n    prntr = SymPyPrinter()\n\n    assert prntr.doprint(x**y) == 'x**y'\n    assert prntr.doprint(gamma(x)) == 'sympy.gamma(x)'\n    assert prntr.doprint(expm1(x)) == 'sympy.exp(x) - 1'\n    assert prntr.doprint(log1p(x)) == 'sympy.log(x + 1)'\n    assert prntr.doprint(cosm1(x)) == 'sympy.cos(x) - 1'\n\n    assert prntr.doprint(sign(x)) == 'sympy.sign(x)'\n    assert prntr.doprint(Rational(1, 2)) == 'sympy.Rational(1, 2)'\n    assert prntr.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert prntr.doprint(atan2(y, x)) == 'sympy.atan2(y, x)'\n    assert prntr.doprint(uppergamma(a, x)) == 'sympy.uppergamma(a, x)'\n    assert prntr.doprint(lowergamma(a, x)) == 'sympy.lowergamma(a, x)'\n    assert prntr.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert prntr.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert prntr.doprint(S.Catalan) == 'sympy.Catalan'\n    assert prntr.doprint(S.TribonacciConstant) == 'sympy.TribonacciConstant'\n    assert prntr.doprint(S.ModularFormsG2) == 'sympy.ModularFormsG2'\n    assert prntr.doprint(S.NaN) == 'sympy.NaN'\n    assert prntr.doprint(S.Infinity) == 'sympy.oo'\n    assert prntr.doprint(S.NegativeInfinity) == '-sympy.oo'\n\n    # Matrix\n    assert prntr.doprint(Identity(3)) == 'sympy.eye(3)'\n    assert prntr.doprint(MatrixSymbol(\"A\",", "def test_print_Pow():\n    prntr = PythonCodePrinter()\n    assert prntr._print_Pow(x**y) == \"x**y\"\n\n    prntr = MpmathPrinter()\n    assert prntr._print_Pow(x**y) == \"x**y\"\n\n    prntr = NumPyPrinter()\n    assert prntr._print_Pow(x**y) == \"x**y\"\n\n    prntr = SciPyPrinter()\n    assert prntr._print_Pow(x**y) == \"x**y\"\n\n    prntr = SymPyPrinter()\n    assert prntr._print_Pow(x**y) == \"x**y\"\n"], "sample_1184": ["def test_FreeSpace():\n    d = symbols('d')\n    fs = FreeSpace(d)\n    assert streq(fs, Matrix([[1, d], [0, 1]]))\n    assert streq(fs*A, Matrix([[a11 + a21*d, a12 + a22*d], [a21, a22]]))\n", "def test_GeometricRay():\n    d, h, angle = symbols('d h angle')\n    ray = GeometricRay(h, angle)\n    fs = FreeSpace(d)\n    assert (fs * ray).height == h + angle*d\n    assert (fs * ray).angle == angle\n", "def test_gaussian_conj():\n    s, z_r, f = symbols('s z_r f')\n    s_out, z_r_out, m = gaussian_conj(s, z_r, f)\n    assert streq(s_out, 1 / (-1/(s + z_r**2/(-f + s)) + 1/f))\n    assert streq(z_r_out, z_r/(1 - s**2/f**2 + z_r**2/f**2))\n    assert streq(m, 1/sqrt(1 - s**2/f**2 + z_r**2/f**2))\n", "def test_BeamParameter():\n    p = BeamParameter(530e-9, 1, w=1e-3)\n    assert N(p.q) == N(1 + 1.88679245283019*I*pi)\n    assert N(p.w_0) == N(1e-3)\n    assert N(p.z_r) == N(5.92753330865999)\n", "def test_BeamParameter():\n    wavelen, z, z_r, w, n = symbols('wavelen z z_r w n')\n    p = BeamParameter(wavelen, z, z_r=z_r, n=n)\n    assert p.wavelen == wavelen\n    assert p.z == z\n    assert p.z_r == z_r\n    assert p.n == n\n    assert p.q == z + I*z_r\n    assert p.w_0 == sqrt(z_r/pi/n*wavelen)\n    assert p.w == p.w_0*sqrt(1 + (z/z_r)**2)\n    assert p.divergence == wavelen/pi/p.w_0\n    assert p.gouy == atan2(z, z_r)\n    assert p.waist_approximation_limit == 2*wavelen/pi\n"], "sample_1185": ["def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([Abs(x), x**2 + 3*x - 4, cos(x)], x) == Abs(cos(x)**2 + 3*cos(x) - 4)\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([Max(2*x, x**2), x**2 + 3*x], x) == Max(2*(x**2 + 3*x), (x**2 + 3*x)**2)\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([x**2 + 2*x + 1, x**2], x) == x**4 + 2*x**2 + 1\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n    assert compogen([Max(sqrt(x), x**2), 2*x + 3], x) == Max(sqrt(2*x + 3), (2*x + 3)**2)\n    assert compogen([x**2 + x + 1], x) == x**2 + x + 1\n"], "sample_1183": ["def test_FracField_to_domain():\n    F = ZZ.frac_field(x)\n    assert F.to_domain() == F\n    assert F.to_domain().to_domain() == F\n    assert F.to_domain().to_field() == F\n    assert F.to_domain().to_field().to_domain() == F\n\n    F = QQ.frac_field(x)\n    assert F.to_domain() == F\n    assert F.to_domain().to_domain() == F\n    assert F.to_domain().to_field() == F\n    assert F.to_domain().to_field().to_domain() == F\n\n    F = ZZ.frac_field(x, y)\n    assert F.to_domain() == F\n    assert F.to_domain().to_domain() == F\n    assert F.to_domain().to_field() == F\n    assert F.to_domain().to_field().to_domain() == F\n\n    F = QQ.frac_field(x, y)\n    assert F.to_domain() == F\n    assert F.to_domain().to_domain() == F\n    assert F.to_domain().to_field() == F\n    assert F.to_domain().to_field().to_domain() == F\n", "def test_FracField_to_domain():\n    F, x, y = field(\"x, y\", ZZ)\n\n    assert F.to_domain().domain == F.domain\n    assert F.to_domain().symbols == F.symbols\n    assert F.to_domain().order == F.order\n", "def test_FracField_from_domain():\n    K = ZZ.frac_field(x, y)\n    assert K.from_domain(2, ZZ) == K(2)\n    assert K.from_domain(2, QQ) == K(2)\n    assert K.from_domain(x, ZZ[x]) == K(x)\n    assert K.from_domain(x, QQ[x]) == K(x)\n    assert K.from_domain(x, ZZ[x, y]) == K(x)\n    assert K.from_domain(x, QQ[x, y]) == K(x)\n    assert K.from_domain(1/x, ZZ.frac_field(x)) == K(1/x)\n    assert K.from_domain(1/x, QQ.frac_field(x)) == K(1/x)\n    assert K.from_domain(1/x, ZZ.frac_field(x, y)) == K(1/x)\n    assert K.from_domain(1/x, QQ.frac_field(x, y)) == K(1/x)\n", "def test_FracField_to_domain():\n    K, x, y = field(\"x,y\", ZZ)\n    assert K.to_domain() == K\n    assert K.get_domain() == K\n    assert K.get_ring() == K.to_ring()\n    assert K.get_field() == K\n    assert K.get_exact() == K\n", "def test_FracField_as_expr():\n    F, x, y = field(\"x,y\", ZZ)\n\n    f = F(x**2/y + 1)\n    g = F(y**2/x - 1)\n\n    assert f.as_expr() == x**2/y + 1\n    assert g.as_expr() == y**2/x - 1\n    assert (f*g).as_expr() == (x**2/y + 1)*(y**2/x - 1)\n"], "sample_1186": ["def test_array_construction_from_matrix():\n    M = Matrix([[1, 2], [3, 4]])\n    for ArrayType in array_types:\n        A = ArrayType(M)\n        assert A.shape == (2, 2)\n        assert A.tolist() == [[1, 2], [3, 4]]\n        assert A == Array([[1, 2], [3, 4]])\n", "def test_ndim_array_applyfunc():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2], [3, 4]])\n        result_array = test_array.applyfunc(lambda x: x**2)\n        assert result_array == ArrayType([[1, 4], [9, 16]])\n        assert test_array.applyfunc(lambda x: x + 1) == ArrayType([[2, 3], [4, 5]])\n\n    # Test with SparseNDimArray\n    sparse_array = MutableSparseNDimArray({0: 1, 2: 3, 3: 4}, (2, 2))\n    result_sparse = sparse_array.applyfunc(lambda x: x**2)\n    assert result_sparse == MutableSparseNDimArray({0: 1, 2: 9, 3: 16}, (2, 2))\n", "def test_array_equality():\n    for ArrayType in array_types:\n        array1 = ArrayType([1, 2, 3, 4], (2, 2))\n        array2 = ArrayType([1, 2, 3, 4], (2, 2))\n        array3 = ArrayType([1, 2, 3, 5], (2, 2))\n        array4 = ArrayType([1, 2, 3, 4], (1, 4))\n\n        assert array1 == array2\n        assert array1 != array3\n        assert array1 != array4\n\n        # Test equality with different types\n        if ArrayType in mutable_array_types:\n            array5 = ArrayType([1, 2, 3, 4], (2, 2)).as_immutable()\n            assert array1 == array5\n        else:\n            array5 = ArrayType([1, 2, 3, 4], (2, 2)).as_mutable()\n            assert array1 == array5\n\n    # Test equality with non-array types\n    array = Array([1, 2, 3, 4], (2, 2))\n    assert array != 1\n    assert array != \"array\"\n    assert array != [1, 2, 3, 4]\n", "def test_NDimArray_transpose():\n    for ArrayType in array_types:\n        arr = ArrayType([[1, 2], [3, 4]])\n        assert arr.transpose().shape == (2, 2)\n        assert arr.transpose() == ArrayType([[1, 3], [2, 4]])\n        raises(ValueError, lambda: ArrayType([1, 2, 3]).transpose())\n", "def test_array_negation():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2], [3, 4]])\n        negated_array = -test_array\n        assert negated_array == ArrayType([[-1, -2], [-3, -4]])\n"], "sample_1188": ["def test_issue_16293():\n    from sympy import Symbol, Function\n\n    class MyFunction(Function):\n        @classmethod\n            if x.is_Number:\n                return x**2\n\n    x = Symbol('x')\n    f = MyFunction(x)\n    assert pretty(f) == pretty(x**2)\n    assert upretty(f) == upretty(x**2)\n", "def test_pretty_print_MatAdd():\n    from sympy import MatrixSymbol\n    A, B = MatrixSymbol('A', 2, 2), MatrixSymbol('B', 2, 2)\n    assert upretty(A + B) == 'A + B'\n    assert upretty(A - B) == 'A - B'\n    assert upretty(A + B + A + B) == '2*A + 2*B'\n    assert upretty(A + B - A - B) == '0'\n", "def test_pretty_printing_tensor():\n    from sympy.tensor.tensor import TensorIndexType, TensorHead, TensorIndex\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    mu, nu = TensorIndex('mu', Lorentz), TensorIndex('nu', Lorentz)\n    R = TensorHead('R', [Lorentz]*4, [[1, 3], [2, 4]])\n    expr = R(mu, nu, mu, nu)\n    assert upretty(expr) == 'R^{\u03bc \u03bd}_{  \u03bc \u03bd}'\n    assert pretty(expr) == 'R^{mu nu}_{  mu nu}'\n    expr = R(nu, mu, mu, nu)\n    assert upretty(expr) == 'R^{\u03bd \u03bc}_{  \u03bc \u03bd}'\n    assert pretty(expr) == 'R^{nu mu}_{  mu nu}'\n    expr = R(mu, nu, -mu, -nu)\n    assert upretty(expr) == 'R^{\u03bc \u03bd}_{  \u03bc \u03bd}'\n    assert pretty(expr) == 'R^{mu nu}_{  mu nu}'\n    expr = R(nu, mu, -mu, -nu)\n    assert upretty(expr) == 'R^{\u03bd \u03bc}_{  \u03bc \u03bd}'\n    assert pretty(expr) == 'R^{nu mu}_{  mu nu}'\n", "def test_pretty_printing_productset():\n    from sympy import Interval, S\n    assert upretty(Interval(0, 1) * Interval(0, 1)) == \"(0 \u2264 x \u2264 1) \u00d7 (0 \u2264 x \u2264 1)\"\n    assert pretty(Interval(0, 1) * Interval(0, 1)) == \"(0 <= x <= 1) x (0 <= x <= 1)\"\n    assert upretty(Interval(0, 1) * S.Reals) == \"\u211d \u00d7 \u211d\"\n    assert pretty(Interval(0, 1) * S.Reals) == \"R x R\"\n", "def test_pretty_printing_mod():\n    from sympy import symbols, Mod\n    a, m = symbols('a m')\n    assert upretty(Mod(a, m)) == 'a mod m'\n"], "sample_1189": ["def test_lambdify_vector_argument():\n    if not numpy:\n        skip(\"numpy not installed\")\n    x_vec = IndexedBase('x')\n    n = symbols('n')\n    i = symbols('i', integer=True)\n\n    # Using a vector argument\n    f = lambdify((x_vec,), x_vec[i], modules='numpy')\n    vec = numpy.array([1, 2, 3])\n    result = f(vec)\n    assert result == vec[0]  # i=0 by default\n\n    # Using a vector argument with a custom dtype\n    f = lambdify((x_vec,), x_vec[i], modules='numpy', printer_options={\n        'numpy_dtype': 'float64'\n    })\n    vec = numpy.array([1, 2, 3])\n    result = f(vec)\n    assert result == vec[0]  # i=0 by default\n    assert result.dtype == numpy.float64\n\n    # Using a vector argument with broadcasting\n    f = lambdify((x_vec,), x_vec[i] + x_vec[i+1], modules='numpy')\n    vec = numpy.array([1, 2, 3])\n    result = f(vec)\n    assert result == vec[0] + vec[1]  # i=0 by default\n\n    # Using a vector argument with shape information\n    f = lambdify((x_vec,), x_vec[i] + x_vec[i+1], modules='numpy',\n                 printer_options={'numpy_shape': (3,)})\n    vec = numpy.array([1, 2, 3])\n    result = f(vec)\n    assert result == vec[0] + vec[1]  # i=0 by default\n\n    # Using a vector argument with a custom dtype and shape information\n    f = lambdify((x_vec,), x_vec[i] + x_vec[i+1], modules='numpy', printer_options={\n        'numpy_dtype': 'float64',\n        'numpy_shape': (3,)\n    })\n    vec = numpy.array([1, 2, 3])\n    result = f(vec)\n    assert result == vec[0] + vec[1]  # i=0 by default\n    assert result.dtype == numpy.float64\n", "compilation error", "def test_lambdify_SparseMatrix():\n    if not scipy:\n        skip(\"scipy not installed.\")\n    x = symbols('x')\n    A = SparseMatrix([[x, 0], [0, x]])\n    f = lambdify(x, A, modules=\"scipy\")\n    B = f(1)\n    assert isinstance(B, scipy.sparse.coo_matrix)\n", "def test_lambdify_lambda():\n    f = Lambda(x, x**2)\n    g = lambdify(x, f, \"sympy\")\n    assert g(2) == 4\n", "def test_issue_18905():\n    if not numpy:\n        skip(\"numpy not installed\")\n    # numpy module should be able to handle complex128 input\n    f = lambdify(x, sin(x), 'numpy')\n    result = f(numpy.array([1 + 2j], dtype=numpy.complex128))\n    assert isinstance(result, numpy.ndarray)\n    assert result.dtype == numpy.complex128\n"], "sample_1187": ["def test_norm():\n    assert norm(Point(2, 7)) == sqrt(53)\n    assert norm(Point(-2, 7)) == sqrt(53)\n    assert norm(Point(0, 0)) == 0\n    assert norm(Point(1, 0)) == 1\n    assert norm(Point(0, 1)) == 1\n    assert norm((2, 7)) == sqrt(53)\n    assert norm((0, 0, 1)) == 1\n    assert norm((0, 1, 0)) == 1\n    assert norm((1, 0, 0)) == 1\n", "def test_left_integral2D():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    facets = triangle.sides\n    assert left_integral2D(3, 0, facets, facets[0].points[0], 1, (x, y)) == 5\n    assert left_integral2D(3, 0, facets, facets[0].points[0], 0, (x, y)) == 0\n", "def test_hyperplane_parameters():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    assert hyperplane_parameters(triangle) == [((0, 1), 3), ((1, -2), -1), ((-2, -1), -3)]\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    assert hyperplane_parameters(faces, vertices) == [([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5),\n                                                      ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)]\n", "def test_gradient_terms():\n    assert gradient_terms(binomial_power=0) == [[1, 0, 0, 0]]\n    assert gradient_terms(binomial_power=1) == [[1, 0, 0, 0], [y, 0, 1, 0], [x, 1, 0, 0]]\n    assert gradient_terms(binomial_power=2) == [[1, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [x, 1, 0, 0],\n                                                [x*y, 1, 1, 0], [x**2, 2, 0, 0]]\n    assert gradient_terms(binomial_power=2, no_of_gens=3) == \\\n        [[[[1, 0, 0, 0, 0, 0, 0, 0]]], [[[y, 0, 1, 0, 1, 0, 0, 0],\n                                          [z, 0, 0, 1, 1, 0, 1, 0]],\n                                         [[x, 1, 0, 0, 1, 1, 0, 0]]],\n                                         [[[y**2, 0, 2, 0, 2, 0, 0, 0],\n                                           [y*z, 0, 1, 1, 2, 0, 1, 0],\n                                           [z**2, 0, 0, 2, 2, 0, 2, 0]],\n                                          [[x*y, 1, 1, 0, 2, 1, 0, 0],\n                                           [x*z, 1, 0, 1, 2, 1, 1, 0]],\n                                          [[x**2, 2, 0, 0, 2, 2, 0, 0]]]]\n", "def test_gradient_terms():\n    assert gradient_terms(2) == [[1, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [x, 1, 0, 0],\n                                [x*y, 1, 1, 0], [x**2, 2, 0, 0]]\n    assert gradient_terms(2, 3) == [[[[1, 0, 0, 0, 0, 0, 0, 0]]], [[[y, 0, 1, 0, 1, 0, 0, 0],\n    [z, 0, 0, 1, 1, 0, 1, 0]], [[x, 1, 0, 0, 1, 1, 0, 0]]],\n    [[[y**2, 0, 2, 0, 2, 0, 0, 0], [y*z, 0, 1, 1, 2, 0, 1, 0],\n    [z**2, 0, 0, 2, 2, 0, 2, 0]], [[x*y, 1, 1, 0, 2, 1, 0, 0],\n    [x*z, 1, 0, 1, 2, 1, 1, 0]], [[x**2, 2, 0, 0, 2, 2, 0, 0]]]]\n"], "sample_1190": ["def test_get_units_non_prefixed():\n    assert meter in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n", "def test_get_units_non_prefixed():\n    assert SI.get_units_non_prefixed() == {\n        meter, second, joule, kilogram, day, coulomb, volt, ohm,\n        elementary_charge, vacuum_permittivity, molar_gas_constant,\n        amount_of_substance, area, energy, volume, length, mass,\n        pressure, temperature, time\n    }\n", "def test_units_non_prefixed():\n    assert len(SI.get_units_non_prefixed()) == 15  # non-prefixed units\n    assert meter in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert day in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n", "def test_get_units_non_prefixed():\n    assert meter in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert day in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n", "def test_UnitSystem_get_units_non_prefixed():\n    assert set(SI.get_units_non_prefixed()) == {meter, second, joule, day, volt, ohm, kilogram, \n                                               amount_of_substance, area, volume, speed_of_light, \n                                               molar_gas_constant, vacuum_permittivity, \n                                               elementary_charge}\n    derived_units = {meter, second, joule, day, volt, ohm, kilogram}\n    us = UnitSystem(base_units=list(derived_units), derived_units=dict((u.dimension, u) for u in derived_units))\n    assert set(us.get_units_non_prefixed()) == derived_units\n"], "sample_1192": ["def test_disambiguate():\n    x1, x2 = symbols('x x')\n    assert disambiguate(x1, x2) == (x1, Symbol('x_1'))\n    x = Symbol('x')\n    y = Symbol('y')\n    # x and y are in the same order in the output as they are in the input\n    assert disambiguate(x, y, x) == (x, y, Symbol('x_1'))\n    assert disambiguate(y, x, x) == (y, x, Symbol('x_1'))\n    d = Dummy('x')\n    assert disambiguate(d) == (Symbol('x'),)\n    assert disambiguate(x, d) == (x, Symbol('x_1'))\n    assert disambiguate(x, x + 1, x + 2) == (x, x + 1, x + 2)\n", "def test_disambiguate():\n    x = Symbol('x')\n    x1 = Symbol('x')\n    y = Symbol('y')\n    assert disambiguate(x, x1, 2*x, 2*x1, y) == (x, x_1, 2*x, 2*x_1, y)\n    assert disambiguate(x, x1, x, 2*x, 2*x1) == (x, x_1, x, 2*x, 2*x_1)\n    assert disambiguate(x, x1, x, x1) == (x, x_1, x, x_1)\n    assert disambiguate(x, x1, x, x1, x) == (x, x_1, x, x_1, x)\n    assert disambiguate(x, x1, x, x1, x1) == (x, x_1, x, x_1, x_1)\n    x2 = Dummy('x')\n    assert disambiguate(x, x1, x1, x2, x) == (x, x_1, x_1, x_2, x)\n    assert disambiguate(x, x1, x1, x2, x2) == (x, x_1, x_1, x_2, x_2)\n    assert disambiguate(x, x1, x1, x2, x, x) == (x, x_1, x_1, x_2, x, x)\n", "def test_disambiguate():\n    x = Symbol('x')\n    y = Symbol('y')\n    x1 = Symbol('x1')\n    x_ = Symbol('x_')\n    _x = Symbol('_x')\n    _x1 = Symbol('_x1')\n    xx = Symbol('xx')\n    xdummy1 = Dummy('x')\n    xdummy2 = Dummy('x')\n\n    assert disambiguate(x, xdummy1) == (x, xdummy1)\n    assert disambiguate(x, xdummy1, xdummy2) == (x, x_1, x_2)\n    assert disambiguate(x, xdummy1, xdummy2, x1, _x) == (x, x_1, x_2, x1, _x)\n    assert disambiguate(x, xdummy1, xdummy2, x1, _x, xx) == (x, x_1, x_2, x1, _x, xx)\n    assert disambiguate(x, xdummy1, xdummy2, x1, _x, xx, _x1) == (x, x_1, x_2, x1, _x, xx, _x1)\n    assert disambiguate(x, xdummy1, xdummy2, x1, _x, xx, _x1, x_) == (x, x_1, x_2, x1, _x, xx, _x1, x__)\n", "def test_disambiguate():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    xdummy = Dummy('x')\n    ydummy = Dummy('y')\n    assert disambiguate(x, xdummy) == (x, xdummy)\n\n    assert disambiguate(xdummy, xdummy) == (xdummy, xdummy)\n    assert disambiguate(*([x] + [xdummy]*5)) == (\n        x, xdummy, Symbol('x_1'), Symbol('x_2'), Symbol('x_3'), Symbol('x_4'))\n    assert disambiguate(x, xdummy, Symbol('x_1')) == (\n        x, xdummy, Symbol('x_2'))\n    assert disambiguate(x, xdummy, Symbol('x_1'), Symbol('x_2')) == (\n        x, xdummy, Symbol('x_3'), Symbol('x_4'))\n    assert disambiguate(\n        x + 2, y + 2, Symbol('x_1') + 2, Symbol('x_2') + 2) == (\n        x + 2, y + 2, Symbol('x_3') + 2, Symbol('x_4') + 2)\n    assert disambiguate(x, xdummy, y, ydummy) == (x, xdummy, y, ydummy)\n    assert disambiguate(x, xdummy, xdummy, ydummy, ydummy) == (\n        x, xdummy, Symbol('x_1'), ydummy, Symbol('y_1'))\n    assert disambiguate(x, xdummy, xdummy, ydummy, ydummy, z, z) == (\n        x, xdummy, Symbol('x_1'), ydummy, Symbol('y_1'), z, Symbol('z_1'))\n    assert disambiguate(x, y, z) == (x, y, z)\n", "def test_symbols_with_properties():\n    x, y = symbols('x,y', real=True, integer=True)\n    assert x.is_real and x.is_integer\n    assert y.is_real and y.is_integer\n\n    a, b = symbols('a,b', real=True, integer=False)\n    assert a.is_real and not a.is_integer\n    assert b.is_real and not b.is_integer\n\n    p, q = symbols('p,q', positive=True)\n    assert p.is_positive\n    assert q.is_positive\n\n    r, s = symbols('r,s', positive=False)\n    assert not r.is_positive\n    assert not s.is_positive\n\n    t, u = symbols('t,u', real=True, positive=True)\n    assert t.is_real and t.is_positive\n    assert u.is_real and u.is_positive\n"], "sample_1191": ["def test_hermite_normal_form():\n    # test that HNF is in the correct form\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = hermite_normal_form(m).to_dense()\n    assert all(hnf[i][j] == 0 for i in range(hnf.shape[0]) for j in range(i))\n    assert all(hnf[i][i] >= 0 for i in range(hnf.shape[0]))\n\n    # test that HNF is not changed by elementary row operations\n    assert hermite_normal_form(m).to_dense() == hermite_normal_form(m).to_dense()\n\n    # test that HNF raises an error for non-ZZ domains\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], QQ)))\n\n    # test that HNF raises an error for invalid modulus\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], ZZ), D=0))\n\n    # test that HNF raises an error for non-square matrices\n    raises(DMShapeError, lambda: hermite_normal_form(DM([[1, 2]], ZZ), D=2))\n\n    # test that HNF is computed correctly for a simple case\n    m = DM([[2, 1], [1, 1]], ZZ)\n    hnf = DM([[1, 0], [0, 1]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n", "def test_hermite_normal_form():\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], QQ)))\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], ZZ), D=0))\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], ZZ), D=1.0))\n\n    raises(DMShapeError, lambda: hermite_normal_form(DM([[1, 2], [3, 4], [5, 6]], ZZ), D=6))\n    raises(DMShapeError, lambda: hermite_normal_form(DM([[1, 2, 3], [4, 5, 6]], ZZ), D=6, check_rank=True))\n\n    hnf_mod_d = DM([[1, 0, -1], [0, 3, 0], [0, 0, 6]], ZZ)\n    assert hermite_normal_form(DM([[1, 0, -1], [0, 3, 0], [0, 0, 6]], ZZ), D=18).to_dense() == hnf_mod_d\n    assert hermite_normal_form(DM([[1, 0, -1], [0, 3, 0], [0, 0, 6]], ZZ), D=18, check_rank=True).to_dense() == hnf_mod_d\n", "def test_hermite_normal():\n    # Test case 1: Simple matrix over ZZ\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Test case 2: Matrix with zero rows\n    m = DM([[0, 0, 0], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[1, 0, 0], [0, 2, 0], [0, 0, 30]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Test case 3: Matrix with zero columns\n    m = DM([[12, 0, 0], [3, 0, 0], [2, 0, 0]], ZZ)\n    hnf = DM([[1, 0, 0], [0, 0, 0], [0, 0, 0]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    # Test case 4: Test the _hermite_normal_form function directly\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert _hermite_normal_form(m).to_dense() == hnf\n\n    # Test case 5: Test the _hermite_normal_form_modulo_D function directly\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert _hermite_normal_form_modulo_D(m, 30).to_dense() == hnf\n\n    # Test case 6: Test that the function", "def test_hermite_normal():\n    A = DM([[2, 7, 12, -7, 25], [3, 9, 10, -2, 31]], ZZ)\n    B = DM([[1, 0, 50, -11, 13], [0, 1, -12, 3, -4]], ZZ)\n    assert hermite_normal_form(A).to_dense() == B\n\n    A = DM([[2, 3, 6, 2], [4, 5, 6, 4], [6, 13, 0, 0]], ZZ)\n    B = DM([[1, 0, 18, 3], [0, 1, -6, -1], [0, 0, 30, 5]], ZZ)\n    assert hermite_normal_form(A).to_dense() == B\n\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], QQ)))\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1]], ZZ), D=QQ(1)))\n    raises(DMShapeError, lambda: hermite_normal_form(DM([[1, 2], [3, 4], [5, 6]], ZZ), D=1))\n", "def test_hermite_normal_form():\n    m = DM([[2, 7, 12], [-2, -9, -20]], ZZ)\n    hnf = DM([[10, 9, 8], [0, 1, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1, 2]], QQ)))\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1, 2]], ZZ), D=3.5))\n    raises(DMDomainError, lambda: hermite_normal_form(DM([[1, 2]], ZZ), D=-3))\n\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m).to_dense() == hnf\n    assert hermite_normal_form(m, check_rank=True).to_dense() == hnf\n\n    m = DM([[12, 6, 4], [3, 9, 6]], ZZ)\n    raises(DMShapeError, lambda: hermite_normal_form(m, D=42))\n"], "sample_1193": ["def test_are_coplanar():\n    p1 = Point3D(1, 1, 1)\n    p2 = Point3D(1, 1, 0)\n    p3 = Point3D(1, 0, 0)\n    p4 = Point3D(0, 0, 0)\n\n    assert are_coplanar(p1, p2, p3, p4) is True\n    assert are_coplanar(p1, p2, p3) is True\n    assert are_coplanar(p1, p2) is False\n\n    assert are_coplanar(Point3D(1, 1, 1), [Point3D(1, 1, 0), Point3D(0, 0, 0)]) is True\n\n    from sympy.geometry.line import Line3D\n    l1 = Line3D(p1, p2)\n    l2 = Line3D(p3, p4)\n\n    assert are_coplanar(l1, l2) is True\n\n    from sympy.geometry.plane import Plane\n    pl = Plane(p1, p2, p3)\n    assert are_coplanar(pl, l1) is True\n    assert are_coplanar(pl, p4) is True\n\n    raises(NotImplementedError, lambda: are_coplanar(1, p1, p2))\n", "def test_are_coplanar():\n    p1, p2, p3, p4 = Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2), Point3D(3, 3, 3)\n    assert are_coplanar(p1, p2, p3, p4) is True\n\n    p1, p2, p3, p4 = Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2), Point3D(3, 3, 4)\n    assert are_coplanar(p1, p2, p3, p4) is False\n\n    l1, l2 = Line3D(p1, p2), Line3D(p3, p4)\n    assert are_coplanar(l1, l2) is True\n\n    p1, p2, p3, p4 = Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2), Point3D(3, 3, 4)\n    l1, l2 = Line3D(p1, p2), Line3D(p3, p4)\n    assert are_coplanar(l1, l2) is False\n\n    p1, p2, p3 = Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2)\n    assert are_coplanar(p1, p2) is False\n", "def test_are_coplanar():\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert are_coplanar(a, b, c) is False\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2)) is True\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(1, 1, 0)) is True\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 1, 0), Point3D(1, 1, 1)) is False\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 1, 0), Point3D(0, 1, 1)) is False\n", "def test_are_coplanar():\n    a, b, c = Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2)\n    assert are_coplanar(a, b, c) is True\n    assert are_coplanar(a, b, Point3D(0, 1, 0)) is False\n    assert are_coplanar(a, b, Point3D(0, 1, 0), c) is False\n    from sympy.geometry.line import Line3D\n    assert are_coplanar(Line3D(a, b), Line3D(a, c)) is True\n    assert are_coplanar(Line3D(a, b), Line3D(c, Point3D(0, 1, 0))) is False\n    assert are_coplanar(Line3D(a, b), Line3D(c, Point3D(0, 1, 0)), c) is False\n    from sympy.geometry.plane import Plane\n    assert are_coplanar(Plane(a, b, c), Line3D(a, b)) is True\n    assert are_coplanar(Plane(a, b, c), Line3D(a, Point3D(0, 1, 0))) is False\n", "def test_are_coplanar():\n    a = Point3D(5, 0, 0)\n    b = Point3D(1, -1, 1)\n    c = Point3D(0, -2, 0)\n    d = Point3D(3, 1, 1)\n    e = Point3D(0, -1, 0)\n    f = Point3D(5, -1, 9)\n    assert are_coplanar(a, b, c) is False\n    assert are_coplanar(a, b, e) is True\n    assert are_coplanar(a, b, d) is True\n    assert are_coplanar(c, f, e) is True\n    assert are_coplanar(a, f, e) is False\n    assert are_coplanar(a, b, c, d) is False\n    assert are_coplanar(a, b, e, f) is False\n    assert are_coplanar(c, e, f, a) is False\n"], "sample_1194": ["def test_MatMul():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    D = Matrix([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n    assert julia_code(A*B) == \"A * B\"\n    assert julia_code(B*A) == \"B * A\"\n    assert julia_code(A*C) == \"A * [1 2 3;\\n4 5 6;\\n7 8 9]\"\n    assert julia_code(A*D) == \"A * [9 8 7;\\n6 5 4;\\n3 2 1]\"\n    assert julia_code(C*A) == \"[1 2 3;\\n4 5 6;\\n7 8 9] * A\"\n    assert julia_code(D*A) == \"[9 8 7;\\n6 5 4;\\n3 2 1] * A\"\n", "def test_julia_MatMul():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert julia_code(A*B) == \"A * B\"\n    assert julia_code(B*A) == \"B * A\"\n    assert julia_code(2*A*B) == \"2 * A * B\"\n    assert julia_code(B*2*A) == \"2 * B * A\"\n    assert julia_code(A*(B + 3*Identity(2))) == \"A * (3 * eye(2) + B)\"\n    assert julia_code(A**(x**2)) == \"A ^ (x .^ 2)\"\n    assert julia_code(A**3) == \"A ^ 3\"\n    assert julia_code(A**S.Half) == \"A ^ (1 // 2)\"\n    assert julia_code(A*x) == \"x * A\"\n    assert julia_code(A*y*x) == \"x .* y * A\"\n    assert julia_code(x*A*x) == \"x * A * x\"\n", "def test_julia_user_functions():\n    f = Function('f')\n    g = Function('g')\n    u1 = implemented_function('u1', Lambda(x, x**2))\n    u2 = implemented_function('u2', Lambda(x, x**3))\n    custom_functions = {\n        \"f\": \"existing_julia_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")],\n        u1: \"user1_func\",\n        u2: [(lambda x: x.is_Matrix, \"user2_mat_func\"),\n             (lambda x: not x.is_Matrix, \"user2_func\")]\n    }\n    assert julia_code(f(x) + g(x) + g(Matrix([[x, y]])) +\n                      u1(x) + u2(x) + u2(Matrix([[x, y]])),\n                      user_functions=custom_functions) == (\n        \"existing_julia_fcn(x) + my_fcn(x) + my_mat_fcn([x y]) + \"\n        \"user1_func(x) + user2_func(x) + user2_mat_func([x y])\"\n    )\n", "def test_MatMul():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert julia_code(Mul(A, B)) == \"A * B\"\n    assert julia_code(Mul(2, A, B)) == \"2 * A * B\"\n    assert julia_code(Mul(A, B, evaluate=False)) == \"A * B\"\n", "def test_julia_code_contract():\n    n, m = symbols('n m', integer=True)\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    C = MatrixSymbol('C', n, n)\n    assign_to = MatrixSymbol('r', n, n)\n    expr = A*B*C\n    result = julia_code(expr, assign_to, contract=False)\n    assert result == \"r = A * B * C\"\n    result = julia_code(expr, assign_to, contract=True)\n    assert \"for i = 1:n\" in result\n"], "sample_1196": ["def test_eval_with_non_set():\n    x = Symbol('x')\n    raises(TypeError, lambda: Contains(x, x**2))\n    raises(TypeError, lambda: Contains(x, [1, 2, 3]))\n    raises(TypeError, lambda: Contains(x, 'string'))\n", "def test_eval_with_sets():\n    x = Symbol('x', integer=True)\n    assert Contains(x, S.Naturals).eval(x, S.Naturals) == Contains(x, S.Naturals)\n    assert Contains(1, S.Naturals).eval(1, S.Naturals) is S.true\n    assert Contains(-1, S.Naturals).eval(-1, S.Naturals) is S.false\n    assert Contains(x, S.Integers).eval(x, S.Integers) == Contains(x, S.Integers)\n    assert Contains(1, S.Integers).eval(1, S.Integers) is S.true\n    assert Contains(-1, S.Integers).eval(-1, S.Integers) is S.true\n", "def test_contains_eval():\n    x = Symbol('x')\n    s = S.Integers\n    ret = Contains.eval(x, s)\n    assert isinstance(ret, Contains)\n\n    s = S.Naturals\n    x = -2\n    ret = Contains.eval(x, s)\n    assert ret is S.false\n\n    s = S.Reals\n    x = 1\n    ret = Contains.eval(x, s)\n    assert ret is S.true\n", "def test_eval_type_error():\n    x = Symbol('x')\n    raises(TypeError, lambda: Contains(x, 'not a Set'))\n    raises(TypeError, lambda: Contains(x, 1))\n    raises(TypeError, lambda: Contains(x, Eq(x, 1)))\n", "def test_eval_with_non_set():\n    x = Symbol('x')\n    raises(TypeError, lambda: Contains(x, x))\n    raises(TypeError, lambda: Contains(x, Eq(x, 1)))\n"], "sample_1195": ["def test_simplify_gpgp():\n    i0,i1,i2,i3,i4,i5 = tensor_indices('i0:6', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n    t = ps*qs*qs\n    st = simplify_gpgp(t)\n    assert _is_tensor_eq(st, G(-LorentzIndex(0))*p(LorentzIndex(0))*q(LorentzIndex(1))*q(-LorentzIndex(1)))\n", "def test_gamma_trace():\n    i, j, k = tensor_indices('i,j,k', LorentzIndex)\n    A = TensorHead('A', [LorentzIndex])\n\n    t = gamma_trace(G(i)*G(j))\n    assert _is_tensor_eq(t, 4*LorentzIndex.metric(i, j))\n\n    t = gamma_trace(G(i)*A(k)*G(j))\n    assert _is_tensor_eq(t, 4*A(k)*LorentzIndex.metric(i, j))\n\n    t = gamma_trace(G(i)*G(j)*G(k)*G(-j))\n    assert _is_tensor_eq(t, 4*G(k)*G(i) - 4*G(i)*G(k))\n", "def test_gamma_trace():\n    mu, nu = tensor_indices(\"mu nu\", LorentzIndex)\n    g = LorentzIndex.metric\n    t = G(mu)*G(nu)\n    r = gamma_trace(t)\n    assert r.equals(4*g(mu, nu))\n\n    t = G(mu)*G(-mu)\n    r = gamma_trace(t)\n    assert r.equals(4*4)\n\n    t = G(mu)*G(nu)*G(-mu)\n    r = gamma_trace(t)\n    assert r.equals(-2*G(nu).trace())\n", "def test_simplify_gpgp():\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n    t = ps*qs*qs\n    ts = simplify_gpgp(t)\n    assert _is_tensor_eq(ts, G(-i0)*p(i0)*q(i1)*q(-i1))\n    ts = simplify_gpgp(ps*qs)\n    assert _is_tensor_eq(ts, G(-i0)*p(i0)*q(i1)*G(-i1))\n", "def test_simplify_gpgp():\n    i0,i1,i2,i3,i4,i5 = tensor_indices('i0:6', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n    t = ps*qs*qs\n    st = simplify_gpgp(t)\n    assert _is_tensor_eq(st, G(-LorentzIndex(0))*p(LorentzIndex(0))*q(LorentzIndex(1))*q(-LorentzIndex(1)))\n"], "sample_1197": ["def test_UnitSystem_extend():\n    base_units = (meter, second, kilogram)\n    units = (centimeter, millisecond, gram)\n    derived_units = {length: meter, time: second, mass: kilogram}\n    us = UnitSystem(base_units, units, \"SI\", \"International System of Units\", derived_units=derived_units)\n\n    extended_base_units = (ampere,)\n    extended_units = (milliampere,)\n    extended_derived_units = {current: ampere}\n    extended_us = us.extend(extended_base_units, extended_units, \"SI_electrical\", \"International System of Units with electrical units\", derived_units=extended_derived_units)\n\n    assert extended_us._base_units == base_units + extended_base_units\n    assert extended_us._units == units + extended_units\n    assert extended_us._derived_units == {**derived_units, **extended_derived_units}\n", "def test_UnitSystem_extend():\n    base = (meter, second)\n    units = (kilometer, hour)\n    name = \"Extended SI\"\n    description = \"Extended SI system\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {}\n\n    extended_si = SI.extend(base, units, name, description, dimension_system, derived_units)\n\n    assert extended_si.name == name\n    assert extended_si.descr == description\n    assert extended_si._base_units == SI._base_units + base\n    assert extended_si._units == SI._units + units\n    assert extended_si._derived_units == {**SI._derived_units, **derived_units}\n", "def test_get_unit_system():\n    assert UnitSystem.get_unit_system(SI) == SI\n    assert UnitSystem.get_unit_system(\"SI\") == SI\n\n    raises(ValueError, lambda: UnitSystem.get_unit_system(\"Invalid Unit System\"))\n", "def test_get_units_non_prefixed():\n    assert meter in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert day in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n", "def test_get_units_non_prefixed():\n    assert set(SI.get_units_non_prefixed()) == {\n        meter, second, kilogram, kelvin, mole, ampere, candela\n    }\n    celsius = Quantity('celsius')\n    celsius.set_global_relative_scale_factor(1, kelvin)\n    SI._units += (celsius,)\n    assert set(SI.get_units_non_prefixed()) == {\n        meter, second, kilogram, kelvin, mole, ampere, candela, celsius\n    }\n"], "sample_1199": ["def test_tensor_product_simpdensity():\n    # tests for density matrix\n    q1 = Qubit('1')\n    q2 = Qubit('0')\n    q3 = Qubit('1')\n    state = q1*q2\n    mixedstate = 0.5*(state*state.dual + q3*q3.dual)\n    assert tensor_product_simp(Density(mixedstate)) == Density(mixedstate)\n    assert Tr(tensor_product_simp(Density(mixedstate)).doit()).doit() == 1\n", "def test_tensor_product_trace():\n    # Test Tr() for simple TensorProduct\n    assert Tr(TP(A, B)) == Tr(A) * Tr(B)\n\n    # Test Tr() for TensorProduct with one non-traceable matrix\n    assert Tr(TP(A, mat1)) == Tr(A) * mat1.trace()\n\n    # Test Tr() for TensorProduct with multiple non-traceable matrices\n    assert Tr(TP(mat1, mat2)) == mat1.trace() * mat2.trace()\n\n    # Test Tr() with indices for TensorProduct\n    assert Tr(TP(A, B), indices=[0]) == Tr(A) * B\n    assert Tr(TP(A, B), indices=[1]) == A * Tr(B)\n\n    # Test Tr() with Density object\n    dens = Density([TP(Qubit(0), Qubit(0)), TP(Qubit(1), Qubit(1))], [0.5, 0.5])\n    assert Tr(dens) == 1\n\n    # Test Tr() with Density object and indices\n    assert Tr(dens, indices=[0]) == Density([Qubit(0), Qubit(1)], [0.5, 0.5])\n", "def test_tensor_product_trace():\n    # tests for Tr with no indices\n    assert Tr(TP(A, B)).doit() == Tr(A).doit()*Tr(B).doit()\n    assert Tr(TP(A, B)*TP(B, C)).doit() == Tr(A*B).doit()*Tr(B*C).doit()\n    assert Tr(TP(A, B)**x).doit() == Tr(A**x).doit()*Tr(B**x).doit()\n    # tests for Tr with indices\n    assert Tr(TP(A, B), [0]).doit() == Tr(A).doit()*B\n    assert Tr(TP(A, B), [1]).doit() == A*Tr(B).doit()\n    assert Tr(TP(A, B)*TP(B, C), [0]).doit() == Tr(A*B).doit()*B*C\n    assert Tr(TP(A, B)*TP(B, C), [1]).doit() == A*B*Tr(B*C).doit()\n    assert Tr(TP(A, B)**x, [0]).doit() == Tr(A**x).doit()*B**x\n    assert Tr(TP(A, B)**x, [1]).doit() == A**x*Tr(B**x).doit()\n", "def test_tensor_product_trace():\n    # Testing Tr for TensorProduct with no indices specified\n    assert Tr(TP(A, B)).doit() == Tr(A)*Tr(B)\n    # Testing Tr for TensorProduct with indices specified\n    assert Tr(TP(A, B), indices=[0]).doit() == Tr(A)*B\n    assert Tr(TP(A, B), indices=[1]).doit() == A*Tr(B)\n", "def test_tensor_product_trace():\n    # Test the trace of a tensor product\n    assert Tr(TP(A, B)) == Tr(A)*Tr(B)\n    # Test the trace with indices specified\n    assert Tr(TP(A, B), indices=[0]) == Tr(A)*B\n    assert Tr(TP(A, B), indices=[1]) == A*Tr(B)\n    # Test the trace of a tensor product of density operators\n    assert Tr(TP(Density([A]), Density([B]))) == Tr(Density([A]))*Tr(Density([B]))\n"], "sample_1198": ["def test_invalid_mathematica_expression():\n    parser = MathematicaParser()\n    raises(ValueError, lambda: parser.parse(\"1 + \"))\n    raises(ValueError, lambda: parser.parse(\"(1 + 2\"))\n    raises(ValueError, lambda: parser.parse(\"1 + 2*\"))\n    raises(ValueError, lambda: parser.parse(\"1 + *2\"))\n    raises(ValueError, lambda: parser.parse(\"1 + 2^\"))\n    raises(ValueError, lambda: parser.parse(\"Sin[\"))\n    raises(ValueError, lambda: parser.parse(\"Sin[x\"))\n    raises(ValueError, lambda: parser.parse(\"F[1,2,\"))\n    raises(ValueError, lambda: parser.parse(\"F[1,2\"))\n    raises(ValueError, lambda: parser.parse(\"a[b,c]\"))\n    raises(ValueError, lambda: parser.parse(\"a[b,c\"))\n    raises(ValueError, lambda: parser.parse(\"{1,2,\"))\n    raises(ValueError, lambda: parser.parse(\"{1,2\"))\n    raises(ValueError, lambda: parser.parse(\"[1,2]\"))\n    raises(ValueError, lambda: parser.parse(\"(1,2]\"))\n    raises(ValueError, lambda: parser.parse(\"(1,2\"))\n    raises(ValueError, lambda: parser.parse(\"Mod[1,2\"))\n    raises(ValueError, lambda: parser.parse(\"Hold[1,2\"))\n    raises(ValueError, lambda: parser.parse(\"#1^2#2^\"))\n    raises(ValueError, lambda: parser.parse(\"#1^2#2\"))\n    raises(ValueError, lambda: parser.parse(\"#1^2 #2^3& \"))\n", "def test_mathematica_parser_symbol_conversions():\n    parser = MathematicaParser()\n\n    # Test some basic symbol conversions\n    assert parser._from_fullformlist_to_sympy([\"Pi\"]) == sympy.pi\n    assert parser._from_fullformlist_to_sympy([\"I\"]) == sympy.I\n    assert parser._from_fullformlist_to_sympy([\"E\"]) == sympy.E\n    assert parser._from_fullformlist_to_sympy([\"GoldenRatio\"]) == sympy.GoldenRatio\n    assert parser._from_fullformlist_to_sympy([\"Catalan\"]) == sympy.Catalan\n    assert parser._from_fullformlist_to_sympy([\"Infinity\"]) == sympy.oo\n", "def test_mathematica_to_sympy_xlatetofullform():\n    parser = MathematicaParser()\n\n        return parser._from_fullformlist_to_fullformsympy(parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr)))\n\n    assert chainback(\"x\") == x\n    assert chainback(\"(x)\") == x\n    assert chainback(\"(x + y)\") == x + y\n    assert chainback(\"(a + b) + c\") == (a + b) + c\n    assert chainback(\"a(b+c)\") == a * (b + c)\n    assert chainback(\"a*b*c\") == a * b * c\n    assert chainback(\"(x+y)*z\") == (x + y) * z\n    assert chainback(\"(a+b)*(c+d)\") == (a + b) * (c + d)\n    assert chainback(\"(x+y)*(z+w)\") == (x + y) * (z + w)\n\n    f = Function(\"f\")\n    g = Function(\"g\")\n    assert chainback(\"f[x]\") == f(x)\n    assert chainback(\"f[x,y]\") == f(x, y)\n    assert chainback(\"f[g[x]]\") == f(g(x))\n    assert chainback(\"(x + y) f[x]\") == (x + y) * f(x)\n    assert chainback(\"(x + y)*f[x]\") == (x + y) * f(x)\n\n    lambda_ = Function(\"Lambda\")\n    assert chainback(\"f[#, #]&\") == lambda_(x, f(x, x))\n    assert chainback(\"f[##]&\") == lambda_((x, y), f(x, y))\n    assert chainback(\"f[x,##]&\") == lambda_((y, z), f(x, y, z))\n    assert chainback(\"(x + #)&\") == lambda_(y, x + y)\n    assert chainback(\"(x + ##)&\") == lambda_((y, z), x + y + z)\n\n    raises(SyntaxError, lambda: chainback(\"x[\"))\n    raises(SyntaxError, lambda: chainback(\"x]\"))\n    raises(SyntaxError, lambda: chainback(\"(\"))\n    raises(SyntaxError, lambda: chainback(\")\"))\n    raises(SyntaxError, lambda: chainback(\"[\"))\n    raises(SyntaxError, lambda: chainback(\"]\"))\n    raises", "def test_parser_mathematica_fullformsympy():\n    parser = MathematicaParser()\n\n    # FullForm-to-Sympy parser tests\n    fullform = parser._from_fullformlist_to_fullformsympy\n\n    assert fullform(\"x\") == symbols(\"x\")\n    assert fullform(\"Pi\") == sympy.pi\n    assert fullform(\"Sin[x]\") == sympy.sin(symbols(\"x\"))\n    assert fullform(\"List[x, y, z]\") == (symbols(\"x\"), symbols(\"y\"), symbols(\"z\"))\n    assert fullform(\"Plus[x, y, z]\") == symbols(\"x\") + symbols(\"y\") + symbols(\"z\")\n    assert fullform(\"Times[x, y, z]\") == symbols(\"x\") * symbols(\"y\") * symbols(\"z\")\n    assert fullform(\"Power[x, y]\") == symbols(\"x\") ** symbols(\"y\")\n    assert fullform(\"Mod[x, y]\") == sympy.Mod(symbols(\"x\"), symbols(\"y\"))\n\n    # Tests for parsing lambda functions\n    assert fullform(\"Function[x, x^2]\") == sympy.Lambda(symbols(\"x\"), symbols(\"x\") ** 2)\n    assert fullform(\"Function[{x, y}, x + y]\") == sympy.Lambda((symbols(\"x\"), symbols(\"y\")), symbols(\"x\") + symbols(\"y\"))\n\n    # Tests for parsing lambda functions with slots\n    assert fullform(\"#1 + #2&\") == sympy.Lambda((symbols(\"x\"), symbols(\"y\")), symbols(\"x\") + symbols(\"y\"))\n    assert fullform(\"#1^2#2^3&\") == sympy.Lambda((symbols(\"x\"), symbols(\"y\")), symbols(\"x\") ** 2 * symbols(\"y\") ** 3)\n", "compilation error"], "sample_1200": ["def test_get_units_non_prefixed():\n    # Test that base units are returned\n    assert meter in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n\n    # Test that derived units are returned\n    assert speed_of_light not in SI.get_units_non_prefixed()\n    assert inch not in SI.get_units_non_prefixed()\n\n    # Test that physical constants are not returned\n    assert gravitational_constant not in SI.get_units_non_prefixed()\n    assert elementary_charge not in SI.get_units_non_prefixed()\n", "def test_unit_system_get_dimension_system():\n    dim_sys = SI.get_dimension_system()\n    assert dim_sys is not None\n    assert isinstance(dim_sys, Dimension.System)\n", "def test_get_units_non_prefixed():\n    from sympy.physics.units import meter, second, kilometer, kilogram, joule\n    from sympy.physics.units.systems import SI\n    assert meter in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n", "def test_get_units_non_prefixed():\n    assert set(SI.get_units_non_prefixed()) == {\n        meter, gram, second, joule, coulomb, volt, ohm\n    }\n", "def test_unit_systems():\n    us = UnitSystem(base_units=(meter, second), name='test_unit_system')\n    assert us.name == 'test_unit_system'\n    assert us.get_dimension_system() is None\n    assert us.get_quantity_dimension(meter) == length\n    assert us.get_quantity_scale_factor(meter) == 1\n    assert us.get_unit_system('SI') is SI\n\n    us2 = us.extend(base=(kilogram,), name='extended_unit_system')\n    assert us2.name == 'extended_unit_system'\n    assert us2.get_dimension_system() is None\n    assert us2.get_quantity_dimension(kilogram) == mass\n\n    assert UnitSystem.get_default_unit_system() is SI\n    assert us.dim == 2\n    assert us.is_consistent\n    assert us2.derived_units == {}\n    assert us.get_units_non_prefixed() == {meter, second}\n"], "sample_1203": ["def test_group_homomorphism():\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    A = AlternatingGroup(4)\n    T = homomorphism(G, A, [a, b], [Permutation(3)(0, 1, 2), Permutation(3)(1, 2, 3)])\n\n    # Test the kernel property of a homomorphism\n    K = T.kernel()\n    assert K.order() == 1\n\n    # Test the image property of a homomorphism\n    Im = T.image()\n    assert Im.order() == 12\n\n    # Test the composition of homomorphisms\n    E, c, d = free_group(\"c, d\")\n    H = FpGroup(E, [c**3, d**3, (c*d)**2])\n    S = homomorphism(H, G, [c, d], [a, b])\n    U = T.compose(S)\n    assert U(c*d) == T(a*b)\n\n    # Test the restriction of a homomorphism to a subgroup\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    P = PermutationGroup(p)\n    T = homomorphism(P, D, [p], [p])\n    H = P.subgroup([p**2])\n    U = T.restrict_to(H)\n    assert U(p**2) == p**2\n\n    # Test the invert_subgroup method of a homomorphism\n    H = D.subgroup([p**2])\n    K = T.invert_subgroup(H)\n    assert K.order() == 4\n", "def test_block_and_orbit_homomorphism():\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    P = PermutationGroup(p)\n    B = block_homomorphism(D, [0]*8)\n    assert B.is_trivial()\n    O = orbit_homomorphism(D, [0, 1])\n    assert O.is_surjective()\n    assert O.image().degree == 2\n    assert O.kernel().order() == 4\n", "def test_compose_homomorphisms():\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = AlternatingGroup(4)\n\n    c = Permutation(3)(0, 1, 2)\n    d = Permutation(3)(1, 2, 3)\n    T1 = homomorphism(G, H, [a, b], [c, d])\n\n    E, e = free_group(\"e\")\n    K = FpGroup(E, [e**8])\n    f = Permutation(0, 1, 2, 3)\n    T2 = homomorphism(H, K, [c, d], [f, f**2])\n\n    T = T2.compose(T1)\n    assert T(a*b) == f**3\n    assert T.domain == G\n    assert T.codomain == K\n", "def test_restrict_to():\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    c = Permutation(3)(0, 1, 2)\n    d = Permutation(3)(1, 2, 3)\n    A = AlternatingGroup(4)\n    T = homomorphism(G, A, [a, b], [c, d])\n    H = G.subgroup([a])\n    T_restrict = T.restrict_to(H)\n    assert T_restrict.domain == H\n    assert T_restrict.codomain == A\n    assert T_restrict.is_homomorphism\n    assert T_restrict(a**2) == T(a**2)\n", "def test_block_homomorphism():\n    D = DihedralGroup(6)\n    blocks = [0, 0, 0, 1, 1, 1]\n    T = block_homomorphism(D, blocks)\n    assert T.codomain.degree == 2\n    assert T.kernel().order() == 3\n    assert T.is_surjective()\n\n    G = AlternatingGroup(4)\n    blocks = [0, 0, 1, 1]\n    T = block_homomorphism(G, blocks)\n    assert T.image().is_subgroup(AlternatingGroup(2))\n    assert T(G.generators[0]) == Permutation(2)(0, 1)\n"], "sample_1201": ["def test_cgs_gauss_system():\n    # Test base units\n    assert cgs_gauss._base_units == (centimeter, gram, second)\n\n    # Test quantity scale factors\n    assert cgs_gauss.get_quantity_scale_factor(coulombs_constant) == 1\n    assert cgs_gauss.get_quantity_scale_factor(statcoulomb) == centimeter**(S(3)/2)*gram**(S.Half)/second\n    assert cgs_gauss.get_quantity_scale_factor(statampere) == statcoulomb/second\n    assert cgs_gauss.get_quantity_scale_factor(statvolt) == erg/statcoulomb\n    assert cgs_gauss.get_quantity_scale_factor(gauss) == sqrt(gram/centimeter)/second\n    assert cgs_gauss.get_quantity_scale_factor(maxwell) == sqrt(centimeter**3*gram)/second\n\n    # Test quantity dimensions\n    assert cgs_gauss.get_quantity_dimension(statcoulomb) == charge\n    assert cgs_gauss.get_quantity_dimension(statampere) == current\n    assert cgs_gauss.get_quantity_dimension(statvolt) == voltage\n    assert cgs_gauss.get_quantity_dimension(gauss) == magnetic_density\n    assert cgs_gauss.get_quantity_dimension(maxwell) == magnetic_flux\n\n    # Test SI units in CGS-gaussian system\n    assert cgs_gauss.get_quantity_scale_factor(coulomb) == 10*speed_of_light*statcoulomb\n    assert cgs_gauss.get_quantity_scale_factor(ampere) == 10*speed_of_light*statcoulomb/second\n    assert cgs_gauss.get_quantity_scale_factor(volt) == 10**6/speed_of_light*statvolt\n    assert cgs_gauss.get_quantity_scale_factor(weber) == 10**8*maxwell\n    assert cgs_gauss.get_quantity_scale_factor(tesla) == 10**4*gauss\n    assert cgs_gauss.get_quantity_scale_factor(debye) == One/10**18*statcoulomb*centimeter\n    assert cgs_gauss.get_quantity_scale_factor(oersted) == sqrt(gram/centimeter)/second\n    assert cgs_gauss.get_quantity_scale_factor(ohm) == 10**5/speed_of_light**2*second/centimeter\n    assert cgs_gauss.get_quantity_scale_factor(farad) == One/10**", "def test_cgs_gauss_units():\n    # Test base units\n    assert convert_to(centimeter, meter, cgs_gauss) == meter/100\n    assert convert_to(gram, kilogram, cgs_gauss) == kilogram/1000\n    assert convert_to(second, second, cgs_gauss) == second\n\n    # Test derived units\n    assert convert_to(statvolt, volt, cgs_gauss) == 2997924580*volt\n    assert convert_to(statampere, ampere, cgs_gauss) == 2997924580*ampere\n    assert convert_to(farad, farad, cgs_gauss) == farad\n    assert convert_to(henry, henry, cgs_gauss) == henry\n    assert convert_to(ohm, ohm, cgs_gauss) == ohm\n\n    # Test physical constants\n    assert convert_to(coulomb_constant, coulomb_constant, cgs_gauss) == coulomb_constant\n    assert convert_to(speed_of_light, speed_of_light, cgs_gauss) == speed_of_light\n\n    # Test conversions involving erg\n    assert convert_to(erg, joule, cgs_gauss) == joule/10**7\n    assert convert_to(joule, erg, cgs_gauss) == 10**7*erg\n\n    # Test conversions involving magnetic units\n    assert convert_to(gauss, tesla, cgs_gauss) == tesla/10000\n    assert convert_to(tesla, gauss, cgs_gauss) == 10000*gauss\n    assert convert_to(maxwell, weber, cgs_gauss) == weber/10**8\n    assert convert_to(weber, maxwell, cgs_gauss) == 10**8*maxwell\n", "def test_conversion_of_units_derived_from_em_units():\n    assert convert_to(statvolt, volt, cgs_gauss) == volt/299792458\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == erg/statcoulomb\n    assert convert_to(volt, statvolt, cgs_gauss) == 299792458*statvolt\n    assert convert_to(farad, 1/statvolt, cgs_gauss) == 1/(299792458**2*statvolt)\n\n    assert convert_to(ohm, statvolt/statampere, cgs_gauss) == statvolt/statampere\n    assert convert_to(statampere, ampere, cgs_gauss) == ampere/2997924580\n    assert convert_to(statampere, statcoulomb/second, cgs_gauss) == statcoulomb/second\n    assert convert_to(ampere, statampere, cgs_gauss) == 2997924580*statampere\n\n    assert convert_to(henry, statvolt*second/statampere, cgs_gauss) == statvolt*second/statampere\n    assert convert_to(henry, statvolt/statampere*second, cgs_gauss) == statvolt/statampere*second\n", "def test_conversion_of_units_derived_from_coulombs_constant():\n    assert convert_to(statvolt, volt, cgs_gauss) == volt/29979245800\n    assert convert_to(statvolt, erg/statcoulomb, cgs_gauss) == erg/statcoulomb\n    assert convert_to(volt, statvolt, cgs_gauss) == 29979245800*statvolt\n    assert convert_to(volt, erg/statcoulomb, cgs_gauss) == 29979245800*erg/statcoulomb\n\n    assert convert_to(farad, (second/centimeter)**2/statcoulomb, cgs_gauss) == (second/centimeter)**2/statcoulomb/8987551788\n    assert convert_to(ohm, centimeter/second, cgs_gauss) == 8987551788*centimeter/second\n\n    assert convert_to(henry, centimeter/second**2, cgs_gauss) == centimeter/second**2/8987551788\n", "def test_cgs_gauss_units():\n    # Test the scale factors of cgs_gauss units\n    assert cgs_gauss.get_quantity_scale_factor(statcoulomb) == centimeter**(S(3)/2)*gram**(S.Half)/second\n    assert cgs_gauss.get_quantity_scale_factor(statampere) == statcoulomb/second\n    assert cgs_gauss.get_quantity_scale_factor(statvolt) == erg/statcoulomb\n    assert cgs_gauss.get_quantity_scale_factor(gauss) == sqrt(gram/centimeter)/second\n    assert cgs_gauss.get_quantity_scale_factor(maxwell) == sqrt(centimeter**3*gram)/second\n\n    # Test the scale factors of SI units in cgs_gauss\n    assert cgs_gauss.get_quantity_scale_factor(coulomb) == 10*speed_of_light*statcoulomb\n    assert cgs_gauss.get_quantity_scale_factor(ampere) == 10*speed_of_light*statcoulomb/second\n    assert cgs_gauss.get_quantity_scale_factor(volt) == 10**6/speed_of_light*statvolt\n    assert cgs_gauss.get_quantity_scale_factor(weber) == 10**8*maxwell\n    assert cgs_gauss.get_quantity_scale_factor(tesla) == 10**4*gauss\n    assert cgs_gauss.get_quantity_scale_factor(debye) == One/10**18*statcoulomb*centimeter\n    assert cgs_gauss.get_quantity_scale_factor(oersted) == sqrt(gram/centimeter)/second\n    assert cgs_gauss.get_quantity_scale_factor(ohm) == 10**5/speed_of_light**2*second/centimeter\n    assert cgs_gauss.get_quantity_scale_factor(farad) == One/10**5*speed_of_light**2*centimeter\n    assert cgs_gauss.get_quantity_scale_factor(henry) == 10**5/speed_of_light**2/centimeter*second**2\n\n    # Test the dimensions of cgs_gauss units\n    assert cgs_gauss.get_quantity_dimension(statcoulomb) == charge\n    assert cgs_gauss.get_quantity_dimension(statampere) == current\n    assert cgs_gauss.get_quantity_dimension(statvolt) == voltage\n    assert cgs_gauss.get_quantity_dimension(gauss) == magnetic_density\n"], "sample_1202": ["def test_comp2():\n    assert comp(sqrt(2).n(2), 1.5, '')\n    assert not comp(sqrt(2).n(2), 1.4, '')\n    assert comp(sqrt(2).n(2), '1.4', '')\n    assert comp(sqrt(2).n(2), Float(1.4, 2), '')\n    assert comp(sqrt(2).n(2), 1.4, .1)\n    assert comp(sqrt(2).n(2), Float(1.4, 2), .1)\n    assert comp(sqrt(2).n(2), '1.4', .1)\n    assert comp(sqrt(2).n(2)*I, '1.4*I', .1)\n    assert not comp(sqrt(2).n(2), 1.3, .1)\n    assert not comp(sqrt(2).n(2), 2, '')\n    assert not comp(sqrt(2).n(2), 2, .1)\n    assert not comp(sqrt(2).n(2)*I, 2, .1)\n    assert not comp(sqrt(2).n(2)*I, '2*I', .1)\n    assert not comp(sqrt(2).n(2)*I, 2, '')\n    assert not comp(sqrt(2).n(2)*I, Float(2, 2), '')\n    assert not comp(sqrt(2).n(2)*I, Float(2, 2), .1)\n", "def test_issue_20895():\n    assert sympify(-Rational(1, 3), rational=True) == -Rational(1, 3)\n    assert sympify(Float('-1/3'), rational=True) == -Rational(1, 3)\n    assert sympify(Float('-1/3'), rational=False) == Float('-1/3')\n    assert sympify(Float('-1/3')) == Float('-1/3')\n    assert sympify(Float('-1/3', 3), rational=True) == -Rational(1, 3)\n    assert sympify(Float('-1/3', 3), rational=False) == Float('-1/3', 3)\n    assert sympify(Float('-1/3', 3)) == Float('-1/3', 3)\n    assert sympify(Float('-1/3', 5), rational=True) == -Rational(1, 3)\n    assert sympify(Float('-1/3', 5), rational=False) == Float('-1/3', 5)\n    assert sympify(Float('-1/3', 5)) == Float('-1/3', 5)\n    assert sympify(Float('-0.333333', 5), rational=True) == -Rational(333333, 1000000)\n    assert sympify(Float('-0.333333', 5), rational=False) == Float('-0.333333', 5)\n    assert sympify(Float('-0.333333', 5)) == Float('-0.333333', 5)\n", "def test_issue_21999():\n    x = S(0)\n    assert x.is_integer\n    assert x.is_comparable\n    assert x.is_real\n    assert x.is_complex\n    assert x.is_number\n    assert x.is_finite\n    assert x.is_zero\n    assert x.is_commutative\n    assert x.is_integer\n    assert x.is_rational\n    assert x.is_algebraic\n    assert x.is_transcendental is False\n    assert x.is_nonpositive\n    assert x.is_nonnegative\n    assert x.is_even\n    assert x.is_odd is False\n", "def test_issue_19578():\n    n = Float(mpq(1, 2), 53)\n    assert n.is_finite\n    assert n.is_positive\n    assert n.is_real\n    assert n.is_number\n    assert n.is_Rational is False\n    assert n.is_Integer is False\n    assert n.is_nonzero\n    assert n.is_nonpositive is False\n    assert n.is_nonnegative\n    assert n.is_zero is False\n    assert n.is_infinite is False\n    assert n.is_comparable\n    assert n.is_positive is not False\n    assert n.is_negative is False\n", "def test_powers_Rational_issue_20462():\n    assert (S(-2)/S(3))**S.Half == I*sqrt(6)/3\n    assert (S(-2)/S(3))**-S.Half == -3/(I*sqrt(6))\n    assert (S(2)/S(-3))**S.Half == sqrt(6)/(3*I)\n    assert (S(2)/S(-3))**-S.Half == 3*I/sqrt(6)\n"], "sample_1205": ["def test_PolyElement_sturm():\n    R, x = ring(\"x\", QQ)\n    assert (x**3 - 2).sturm() == [x**3 - 2, 3*x**2, -6*x, -6]\n    assert (x**3 - 2*x**2 - 8*x + 3).sturm() == [x**3 - 2*x**2 - 8*x + 3, 3*x**2 - 4*x - 8, 20*x/9 - 16/9, -416/45]\n    assert (x**2 - 2*x + 1).sturm() == [x**2 - 2*x + 1, 2*x - 2, 0]\n", "def test_PolyElement_drop_to_ground():\n    R, x,y,z = ring(\"x,y,z\", ZZ)\n    Ryz = PolyRing(\"y,z\", ZZ, lex)\n\n    assert R(1).drop_to_ground(x).ring == Ryz\n    assert R(1).drop_to_ground(y).ring == PolyRing(\"x,z\", ZZ, lex)\n    assert R(1).drop_to_ground(z).ring == PolyRing(\"x,y\", ZZ, lex)\n\n    assert x.drop_to_ground(x) == Ryz(1)\n    assert x.drop_to_ground(y) == PolyRing(\"x,z\", ZZ, lex)(x)\n    assert x.drop_to_ground(z) == PolyRing(\"x,y\", ZZ, lex)(x)\n\n    assert y.drop_to_ground(x) == Ryz(y)\n    assert y.drop_to_ground(y) == PolyRing(\"x,z\", ZZ, lex)(1)\n    assert y.drop_to_ground(z) == PolyRing(\"x,y\", ZZ, lex)(y)\n\n    assert z.drop_to_ground(x) == Ryz(z)\n    assert z.drop_to_ground(y) == PolyRing(\"x,z\", ZZ, lex)(z)\n    assert z.drop_to_ground(z) == PolyRing(\"x,y\", ZZ, lex)(1)\n", "def test_PolyElement_set_ring():\n    R, x, y = ring(\"x,y\", ZZ)\n    Ruv, u, v = ring(\"u,v\", ZZ)\n\n    f = x**2 + 2*y\n    g = f.set_ring(Ruv)\n\n    assert g.ring == Ruv\n    assert g == u**2 + 2*v\n\n    raises(CoercionFailed, lambda: (u + v).set_ring(R))\n", "def test_PolyElement_is_squarefree():\n    R, x = ring(\"x\", ZZ)\n\n    f = x**2 + 2*x + 1\n    g = x**2 + 3*x + 1\n\n    assert f.is_squarefree is False\n    assert g.is_squarefree is True\n", "def test_PolyElement_imul_num():\n    R, x,y,z = ring(\"x,y,z\", ZZ)\n\n    f = x + y**2\n\n    assert f.imul_num(3) == 3*x + 3*y**2\n    assert f.imul_num(3) is f\n\n    g = f.imul_num(3)\n    assert g is not f\n\n    assert g.imul_num(0) == 0\n    assert g.imul_num(0) is g\n\n    assert R(3).imul_num(2) == 6\n    assert R(3).imul_num(2) is not R(3)\n"], "sample_1204": ["def test_symmetric_permutation_group():\n    G = SymmetricPermutationGroup(4)\n    assert G.degree == 4\n    assert G.order() == factorial(4)\n\n    assert Permutation(0, 1, 2, 3) in G\n    assert Permutation(3)(0, 2, 1) in G\n    assert Permutation(4) not in G\n    assert Permutation(1, 2, 3, 4) not in G\n\n    assert G.identity == Permutation(3)\n", "def test_symmetric_permutation_group():\n    G = SymmetricPermutationGroup(3)\n    assert G.degree == 3\n    assert G.order() == 6\n    assert Permutation(0, 1, 2) in G\n    assert Permutation(3) not in G\n    assert G.identity == Permutation(2)\n", "def test_sylow_alt_sym():\n    for p in [2, 3]:\n        assert PermutationGroup(Permutation([1, 2, 0]))._sylow_alt_sym(p) == []\n    for p in [2, 3, 5]:\n        assert PermutationGroup(Permutation([1, 2, 3, 4, 0]))._sylow_alt_sym(p) == []\n    assert PermutationGroup(Permutation([1, 2, 0])).sylow_subgroup(2).order() == 4\n", "def test_polycyclic_group():\n    G = AlternatingGroup(4)\n    P = G.polycyclic_group()\n    assert P.is_polycyclic\n    assert P.generators == G.strong_gens\n    assert P.relative_order == [3, 2, 2]\n    assert P.pc_series == G.derived_series()\n", "def test_symmetric_permutation_group():\n    G = SymmetricPermutationGroup(5)\n    assert G.order() == 120\n    assert G.degree == 5\n    assert G.identity == Permutation(4)\n"], "sample_1206": ["def test_mpf_norm_negative_sign():\n    mpf = (1, 0, 1, 0)\n    assert mpf_norm(mpf, 10) == mpf_norm(mpf, 10)\n    mpf = (1, 0, -1, 0)\n    assert mpf_norm(mpf, 10) == mpf_norm(mpf, 10)\n    mpf = (-1, 0, 1, 0)\n    assert mpf_norm(mpf, 10) == mpf_norm(mpf, 10)\n    mpf = (-1, 0, -1, 0)\n    assert mpf_norm(mpf, 10) == mpf_norm(mpf, 10)\n", "def test_AlgebraicNumber():\n    minpoly, root = x**2 + 1, I\n    a = AlgebraicNumber((minpoly, root), [1, 0])\n    assert a.is_AlgebraicNumber\n    assert a.is_real is False\n    assert a.is_number is True\n    assert a.is_Rational is False\n    assert a.is_Integer is False\n\n    b = AlgebraicNumber((minpoly, root), [2, 1])\n    assert b.is_real is False\n    assert b.is_number is True\n    assert b.is_Rational is False\n    assert b.is_Integer is False\n\n    assert a == a\n    assert a != b\n    assert a != 1\n    assert a != I\n    assert b != 1\n    assert b != I\n\n    assert +a == a\n    assert -a != a\n    assert a + b == AlgebraicNumber((minpoly, root), [3, 1])\n    assert a - b == AlgebraicNumber((minpoly, root), [-1, -1])\n    assert a * b == AlgebraicNumber((minpoly, root), [-1, 2])\n    assert a / b == AlgebraicNumber((minpoly, root), [S.Half, -S.Half])\n    assert a + 2 == AlgebraicNumber((minpoly, root), [3, 0])\n    assert 2 + a == AlgebraicNumber((minpoly, root), [3, 0])\n    assert a - 2 == AlgebraicNumber((minpoly, root), [-1, 0])\n    assert 2 - a == AlgebraicNumber((minpoly, root), [1, 0])\n    assert a * 2 == AlgebraicNumber((minpoly, root), [2, 0])\n    assert 2 * a == AlgebraicNumber((minpoly, root), [2, 0])\n    assert a / 2 == AlgebraicNumber((minpoly, root), [S.Half, 0])\n    assert 2 / a == AlgebraicNumber((minpoly, root), [0, -2])\n", "def test_issue_17806():\n    assert Rational(2, 3) != Float(2/3)\n    assert Rational(2, 3) != Float(2, 3)\n    assert Rational(1, 2) != Float(1, 2)\n    assert Rational(1, 2) != Float(1/2)\n    assert Rational(1, 2) != Float('0.5')\n    assert Rational(1, 2) != 0.5\n", "def test_getitem_Float():\n    f = Float(1.23)\n    raises(TypeError, lambda: f[0])\n", "def test_issue_18273():\n    # Test that Float.__format__ properly handles NaN and infinity\n    assert format(Float('nan'), '.0f') == 'nan'\n    assert format(Float('inf'), '.0f') == 'inf'\n    assert format(Float('-inf'), '.0f') == '-inf'\n    assert format(Float('nan'), '+.0f') == 'nan'\n    assert format(Float('inf'), '+.0f') == '+inf'\n    assert format(Float('-inf'), '+.0f') == '-inf'\n"], "sample_1207": ["def test_stringify_expr():\n    transformations = standard_transformations\n    x = Symbol('x')\n    local_dict = {}\n    global_dict = {}\n    s = \"2*x\"\n    result = \"Integer (2)*Symbol ('x')\"\n    assert stringify_expr(s, local_dict, global_dict, transformations) == result\n", "def test_implicit_multiplication_with_exponent():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application,)\n\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert parse_expr(\"2x**2\", transformations=transformations) == 2*x**2\n    assert parse_expr(\"x2**3\", transformations=transformations) == x*2**3\n    assert parse_expr(\"xy**2\", transformations=transformations) == x*y**2\n", "def test_T():\n    assert T[0] == _transformation[0]\n    assert T[1:3] == (_transformation[1], _transformation[2])\n    assert T[:5] == (_transformation[0], _transformation[1], _transformation[2], _transformation[3], _transformation[4])\n    assert T[5:] == tuple(_transformation[i] for i in range(5, len(_transformation)))\n    assert T[:] == tuple(_transformation[i] for i in range(len(_transformation)))\n    raises(TypeError, lambda: T['abc'])\n    raises(TypeError, lambda: T[1, 'abc'])\n    raises(TypeError, lambda: T[1, 2, 'abc'])\n    raises(TypeError, lambda: T[None])\n    raises(TypeError, lambda: T[1, None])\n    raises(TypeError, lambda: T[1, 2, None])\n", "def test_parse_expr_evaluate_false_on_complex_expressions():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test expression with multiple terms\n    expr1 = parse_expr('2*x + 3*y', evaluate=False)\n    assert expr1 == 2*x + 3*y\n\n    # Test expression with nested parentheses\n    expr2 = parse_expr('(2*x + 3*y)*4', evaluate=False)\n    assert expr2 == (2*x + 3*y)*4\n\n    # Test expression with exponents and implicit multiplication\n    expr3 = parse_expr('2*x^2 + 3*y^2', transformations='all', evaluate=False)\n    assert expr3 == 2*x**2 + 3*y**2\n", "def test_T_class_repr():\n    repr_T = repr(T)\n    assert repr_T.startswith('<_T object at')\n    assert str(T) == transformations\n"], "sample_1209": ["def test_prefix_repr():\n    y = PREFIXES['y']\n    Y = PREFIXES['Y']\n    assert repr(y) == \"Prefix('yocto', 'y', -24)\"\n    assert repr(Y) == \"Prefix('yotta', 'Y', 24)\"\n    assert repr(kibi) == \"Prefix('kibi', 'Y', 10, 2)\"\n", "def test_prefix_repr_latex():\n    assert repr(kilo) == \"Prefix('kilo', 'k', 3)\"\n    assert repr(kibi) == \"Prefix('kibi', 'Y', 10, 2)\"\n    assert kilo._latex(None) == r'\\text{k}'\n    assert micro._latex(None) == r'\\mu'\n", "def test_prefix_repr():\n    yotta = Prefix('yotta', 'Y', 24)\n    kibi = Prefix('kibi', 'Ki', 10, 2)\n\n    assert repr(yotta) == \"Prefix('yotta', 'Y', 24)\"\n    assert repr(kibi) == \"Prefix('kibi', 'Ki', 10, 2)\"\n", "def test_prefix_repr():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n    kibi_inst = Prefix('kibi', 'Ki', 10, 2)\n\n    assert repr(m) == \"Prefix('milli', 'm', -3)\"\n    assert repr(k) == \"Prefix('kilo', 'k', 3)\"\n    assert repr(M) == \"Prefix('mega', 'M', 6)\"\n    assert repr(kibi_inst) == \"Prefix('kibi', 'Ki', 10, 2)\"\n", "def test_prefix_representation():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n    kibi = PREFIXES['Ki']\n\n    assert repr(m) == \"Prefix('milli', 'm', -3)\"\n    assert str(m) == 'm'\n    assert repr(k) == \"Prefix('kilo', 'k', 3)\"\n    assert str(k) == 'k'\n    assert repr(M) == \"Prefix('mega', 'M', 6)\"\n    assert str(M) == 'M'\n    assert repr(kibi) == \"Prefix('kibi', 'Y', 10, 2)\"\n    assert str(kibi) == 'Y'\n"], "sample_1208": ["def test_sample_numpy():\n    distribs_numpy = [\n        #MatrixNormal('M', [[5, 6]], [4], [[2, 1], [1, 2]]),\n        #Wishart('W', 5, [[1, 0], [0, 1]])\n    ]\n    size = 5\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy not installed. Abort tests for _sample_numpy.')\n    else:\n        for X in distribs_numpy:\n            raises(NotImplementedError, lambda: sample(X, size=size, library='numpy'))\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n", "def test_sample_numpy():\n    distribs_numpy = [\n        # Add distributions as they are implemented in numpy_rv_map\n    ]\n    size = 5\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy not installed. Abort tests for _sample_numpy.')\n    else:\n        for X in distribs_numpy:\n            samps = sample(X, size=size, library='numpy')\n            for sam in samps:\n                assert Matrix(sam) in X.pspace.distribution.set\n        M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n        raises(NotImplementedError, lambda: sample(M, size=3, library='numpy'))\n", "def test_sample_numpy():\n    distribs_numpy = [\n        # Add distributions as they are implemented in SampleMatrixNumpy\n    ]\n    size = 5\n    numpy = import_module('numpy')\n    if not numpy:\n        skip('NumPy not installed. Abort tests for _sample_numpy.')\n    else:\n        for X in distribs_numpy:\n            raises(NotImplementedError, lambda: sample(X, size=size, library='numpy'))\n", "def test_MatrixDistribution():\n    M = MatrixGammaDistribution(1, 2, [[2, 1], [1, 2]])\n    assert M.dimension == (2, 2)\n    raises(ValueError, lambda: MatrixGammaDistribution(1, 2, [[1, 0], [2, 1]]))\n    raises(ValueError, lambda: MatrixGammaDistribution(1, 2, [[1, 0], [0]]))\n    raises(ValueError, lambda: MatrixGammaDistribution(-1, 2, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixGammaDistribution(1, -2, [[1, 0], [0, 1]]))\n    W = WishartDistribution(5, [[1, 0], [0, 1]])\n    assert W.dimension == (2, 2)\n    raises(ValueError, lambda: WishartDistribution(-1, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: WishartDistribution(2, [[1, 0], [2, 1]]))\n    raises(ValueError, lambda: WishartDistribution(2, [[1, 0], [0]]))\n    MN = MatrixNormalDistribution([[5, 6]], [4], [[2, 1], [1, 2]])\n    assert MN.dimension == (1, 2)\n    raises(ValueError, lambda: MatrixNormalDistribution([1, 2], [[1, 0], [0, 1]], [[1, 0], [2, 1]]))\n    raises(ValueError, lambda: MatrixNormalDistribution([1, 2], [[1, 0], [2, 1]], [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixNormalDistribution([1, 2], [[1, 0], [0, 1]], [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixNormalDistribution([1, 2], [[1, 0], [2]], [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixNormalDistribution([1, 2], [[1, 0], [2, 1]], [[1], [2]]))\n    raises(ValueError, lambda: MatrixNormalDistribution([[1, 2]], [[1, 0], [0, 1]],", "def test_MatrixStudentT_sample_scipy():\n    M = MatrixStudentT('M', 2, [[5, 6]], [[2, 1], [1, 2]], [4])\n    size = 5\n    scipy = import_module('scipy')\n    if not scipy:\n        skip('Scipy not installed. Abort tests for MatrixStudentT.sample')\n    else:\n        samps = sample(M, size=size)\n        for sam in samps:\n            assert Matrix(sam) in M.pspace.distribution.set\n"]}, "raw_scored_generations": {"sample_2": [false, true, true, false, true], "sample_3": [false, false, true, true, true], "sample_5": [false, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_0": [false, false, false, false, false], "sample_1": [true, false, false, false, false], "sample_6": [false, true, true, true, false], "sample_7": [true, false, false, false, false], "sample_8": [true, false, false, false, true], "sample_9": [false, false, false, false, false], "sample_10": [false, true, true, true, false], "sample_11": [true, true, false, false, false], "sample_12": [true, false, true, false, false], "sample_13": [true, false, false, true, false], "sample_14": [true, true, true, false, true], "sample_15": [true, true, true, true, true], "sample_16": [false, false, false, false, false], "sample_17": [false, false, false, false, false], "sample_18": [false, false, false, false, false], "sample_19": [false, true, true, true, false], "sample_20": [true, true, false, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [true, false, true, true, false], "sample_24": [false, false, false, true, false], "sample_25": [false, true, false, true, false], "sample_26": [false, true, true, false, false], "sample_27": [true, false, true, true, true], "sample_28": [false, false, false, false, false], "sample_29": [false, false, false, false, false], "sample_30": [false, true, true, false, false], "sample_31": [false, false, false, false, false], "sample_32": [false, false, false, false, false], "sample_33": [false, true, false, false, false], "sample_34": [false, false, false, false, true], "sample_35": [false, true, true, false, false], "sample_36": [false, true, false, false, true], "sample_37": [true, false, false, false, false], "sample_38": [false, false, false, false, true], "sample_39": [false, false, false, false, false], "sample_40": [false, false, false, false, false], "sample_41": [false, true, false, false, true], "sample_42": [false, false, false, false, false], "sample_43": [false, true, true, true, true], "sample_44": [false, false, true, true, false], "sample_45": [true, true, false, false, true], "sample_46": [false, false, false, false, false], "sample_47": [true, false, true, false, false], "sample_48": [true, true, true, true, true], "sample_49": [false, true, true, true, true], "sample_50": [false, false, false, false, false], "sample_51": [true, true, true, true, true], "sample_52": [true, true, true, false, false], "sample_54": [true, true, true, true, true], "sample_53": [true, true, true, true, true], "sample_55": [false, false, false, false, false], "sample_58": [true, true, true, false, true], "sample_56": [true, true, true, true, true], "sample_57": [false, true, false, false, false], "sample_59": [true, true, true, true, true], "sample_60": [false, true, false, false, true], "sample_61": [false, true, true, false, true], "sample_62": [true, false, false, false, false], "sample_63": [false, true, true, true, false], "sample_64": [false, false, false, true, false], "sample_65": [true, true, true, true, true], "sample_67": [false, false, true, false, true], "sample_66": [false, false, true, true, false], "sample_68": [false, true, true, false, true], "sample_69": [false, true, true, true, false], "sample_70": [true, true, true, true, true], "sample_71": [true, true, true, true, true], "sample_72": [false, false, false, false, false], "sample_73": [false, false, false, false, false], "sample_75": [false, false, false, false, false], "sample_74": [false, true, false, true, false], "sample_76": [true, true, true, true, true], "sample_77": [true, true, true, true, true], "sample_78": [true, true, true, true, true], "sample_79": [false, false, false, false, false], "sample_80": [true, true, true, true, true], "sample_82": [true, true, true, true, true], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_85": [false, false, false, false, true], "sample_84": [false, false, false, false, false], "sample_86": [true, true, true, true, false], "sample_88": [true, true, true, true, true], "sample_87": [true, false, false, false, false], "sample_89": [true, true, false, false, false], "sample_90": [false, false, false, false, false], "sample_91": [true, true, true, true, true], "sample_92": [false, true, false, false, false], "sample_93": [true, true, true, true, true], "sample_94": [false, false, false, true, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, true], "sample_96": [false, false, true, true, false], "sample_99": [false, false, false, false, true], "sample_97": [true, false, false, true, false], "sample_100": [false, false, false, true, false], "sample_102": [true, true, true, true, true], "sample_101": [true, true, true, true, true], "sample_103": [true, true, true, true, true], "sample_104": [false, true, false, false, false], "sample_107": [false, true, false, false, true], "sample_106": [false, false, true, true, false], "sample_105": [false, true, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [true, true, true, true, true], "sample_111": [true, true, true, true, true], "sample_110": [false, false, false, false, false], "sample_112": [true, true, true, true, true], "sample_113": [true, true, true, false, true], "sample_114": [true, true, true, true, true], "sample_115": [false, true, false, false, false], "sample_116": [true, true, true, true, true], "sample_117": [true, false, false, false, false], "sample_118": [true, true, true, true, true], "sample_119": [true, true, true, true, true], "sample_120": [false, false, false, false, false], "sample_121": [false, false, true, true, true], "sample_122": [true, false, true, true, false], "sample_123": [false, false, false, false, true], "sample_124": [false, false, false, true, true], "sample_125": [false, true, true, true, true], "sample_126": [true, true, true, true, true], "sample_127": [true, true, true, true, true], "sample_128": [true, true, true, true, true], "sample_129": [false, false, false, false, false], "sample_130": [true, true, true, true, true], "sample_131": [false, false, false, true, true], "sample_132": [false, false, true, false, false], "sample_133": [true, true, true, true, true], "sample_135": [true, true, true, true, true], "sample_134": [false, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [true, true, true, true, true], "sample_137": [true, true, true, true, true], "sample_138": [false, false, false, false, false], "sample_140": [false, false, false, false, true], "sample_141": [true, false, false, false, false], "sample_142": [true, true, true, true, true], "sample_143": [true, true, true, true, true], "sample_144": [true, true, false, true, true], "sample_145": [false, true, true, false, true], "sample_146": [true, true, true, true, true], "sample_147": [true, true, true, true, true], "sample_148": [true, true, true, true, true], "sample_151": [true, true, true, true, true], "sample_149": [false, true, true, true, true], "sample_152": [true, true, false, true, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [true, true, true, true, true], "sample_155": [true, true, true, true, true], "sample_156": [true, false, true, false, true], "sample_157": [true, false, false, false, false], "sample_158": [true, true, false, false, true], "sample_159": [true, true, true, true, true], "sample_160": [true, true, true, true, true], "sample_161": [true, false, true, true, true], "sample_162": [true, true, true, true, true], "sample_163": [true, false, true, true, false], "sample_164": [false, false, true, false, false], "sample_165": [true, false, false, false, false], "sample_166": [true, true, true, true, true], "sample_167": [false, false, false, true, false], "sample_168": [true, true, true, true, true], "sample_169": [false, true, false, true, true], "sample_171": [true, true, true, true, false], "sample_170": [true, false, true, false, false], "sample_172": [true, false, true, true, true], "sample_173": [false, true, false, false, false], "sample_174": [false, false, false, false, false], "sample_175": [true, false, true, true, true], "sample_176": [true, true, true, true, true], "sample_177": [true, true, true, true, true], "sample_178": [true, true, true, true, true], "sample_180": [false, true, false, false, true], "sample_179": [false, false, false, true, false], "sample_182": [true, true, true, true, true], "sample_181": [true, true, true, true, true], "sample_183": [false, false, false, false, false], "sample_184": [false, true, false, false, false], "sample_185": [false, false, false, true, false], "sample_186": [true, true, true, true, true], "sample_187": [true, true, true, true, true], "sample_188": [true, false, true, true, false], "sample_189": [false, false, true, false, false], "sample_190": [true, true, false, true, true], "sample_191": [false, false, false, false, false], "sample_192": [false, false, true, false, true], "sample_193": [true, true, true, true, false], "sample_194": [true, true, true, true, true], "sample_195": [false, false, false, true, false], "sample_196": [false, false, false, false, false], "sample_198": [false, true, true, false, true], "sample_197": [true, true, true, true, true], "sample_199": [true, true, true, true, true], "sample_200": [true, true, true, true, true], "sample_201": [true, true, true, true, true], "sample_202": [true, true, true, true, true], "sample_203": [true, true, true, true, false], "sample_204": [true, true, true, true, true], "sample_205": [true, true, true, true, true], "sample_206": [true, true, true, true, true], "sample_207": [false, true, false, true, true], "sample_208": [true, true, true, true, true], "sample_209": [true, true, false, false, true], "sample_210": [true, false, true, false, false], "sample_211": [false, false, false, false, false], "sample_213": [false, false, false, false, false], "sample_212": [true, false, false, true, false], "sample_214": [false, false, false, false, true], "sample_215": [true, false, false, false, true], "sample_216": [true, true, true, true, true], "sample_217": [true, true, true, true, false], "sample_218": [false, true, false, false, false], "sample_219": [false, false, true, false, false], "sample_220": [true, true, false, false, false], "sample_221": [true, true, true, true, true], "sample_222": [true, true, true, true, false], "sample_223": [false, true, false, false, false], "sample_224": [true, true, true, true, true], "sample_225": [false, false, false, false, false], "sample_226": [true, false, true, true, true], "sample_227": [true, false, true, true, true], "sample_228": [true, true, true, true, true], "sample_229": [true, true, true, true, true], "sample_230": [true, true, true, true, true], "sample_231": [false, false, true, false, false], "sample_232": [true, false, true, false, false], "sample_233": [true, true, true, true, true], "sample_234": [true, true, true, true, true], "sample_235": [true, true, true, true, false], "sample_236": [false, true, true, true, true], "sample_237": [true, true, true, false, true], "sample_238": [true, true, true, true, false], "sample_239": [true, true, true, true, true], "sample_240": [true, true, true, true, true], "sample_241": [false, false, false, false, false], "sample_242": [true, true, true, true, true], "sample_243": [true, true, true, true, true], "sample_244": [true, true, true, true, true], "sample_245": [true, true, true, true, true], "sample_246": [true, true, true, true, true], "sample_247": [true, true, true, true, true], "sample_248": [true, true, true, true, true], "sample_249": [true, false, true, false, false], "sample_250": [true, true, true, true, true], "sample_251": [true, true, true, true, true], "sample_252": [true, true, false, true, true], "sample_253": [false, false, false, false, false], "sample_254": [false, true, true, false, true], "sample_256": [true, true, true, false, false], "sample_255": [true, false, true, true, true], "sample_257": [true, true, false, false, true], "sample_258": [false, false, false, false, false], "sample_259": [true, true, true, true, true], "sample_260": [true, true, true, true, true], "sample_261": [true, true, true, true, true], "sample_262": [false, true, false, false, false], "sample_263": [true, true, true, true, true], "sample_264": [true, true, true, true, true], "sample_265": [true, true, true, true, true], "sample_266": [true, true, true, true, true], "sample_267": [false, false, false, false, false], "sample_268": [false, false, false, false, false], "sample_269": [true, true, true, true, true], "sample_270": [false, false, false, true, false], "sample_271": [false, false, false, false, true], "sample_272": [true, true, true, true, true], "sample_273": [false, false, false, false, false], "sample_274": [false, false, false, false, false], "sample_275": [false, true, false, false, false], "sample_276": [true, true, true, true, true], "sample_277": [true, true, true, true, true], "sample_278": [false, false, false, true, false], "sample_279": [false, true, true, true, true], "sample_280": [true, true, true, true, true], "sample_281": [true, true, true, true, true], "sample_282": [true, true, true, true, true], "sample_283": [true, true, true, true, true], "sample_284": [false, true, false, false, false], "sample_285": [true, true, true, true, true], "sample_286": [false, false, false, true, false], "sample_287": [true, true, true, true, true], "sample_288": [false, false, false, true, true], "sample_289": [false, true, false, true, false], "sample_290": [true, true, false, false, false], "sample_291": [false, true, true, false, false], "sample_292": [true, true, true, true, true], "sample_293": [false, false, false, false, false], "sample_294": [true, true, true, true, true], "sample_295": [false, false, false, false, false], "sample_296": [true, true, true, true, true], "sample_297": [false, false, false, false, false], "sample_298": [true, true, true, true, true], "sample_299": [true, true, true, true, false], "sample_300": [true, true, true, true, true], "sample_301": [false, false, false, false, true], "sample_302": [false, false, false, false, false], "sample_303": [true, true, true, true, true], "sample_304": [false, false, true, true, false], "sample_305": [false, true, true, false, true], "sample_306": [true, true, true, true, true], "sample_307": [true, true, true, true, true], "sample_308": [true, true, true, true, true], "sample_309": [false, false, false, true, true], "sample_310": [true, true, true, true, true], "sample_312": [true, true, true, true, true], "sample_311": [false, false, false, false, false], "sample_313": [true, true, true, false, true], "sample_314": [false, false, false, false, true], "sample_315": [true, true, false, false, true], "sample_316": [true, true, true, true, true], "sample_317": [true, true, true, true, true], "sample_318": [false, false, false, true, false], "sample_319": [false, true, true, true, true], "sample_320": [true, false, true, true, true], "sample_321": [true, true, true, true, true], "sample_322": [true, true, true, true, true], "sample_323": [true, true, true, true, true], "sample_324": [true, true, true, true, true], "sample_325": [true, false, true, true, false], "sample_326": [true, true, true, true, true], "sample_327": [true, true, true, true, true], "sample_328": [true, true, true, true, true], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, false, false], "sample_331": [true, true, true, true, true], "sample_332": [true, true, true, true, true], "sample_333": [true, false, true, true, true], "sample_334": [true, true, true, true, true], "sample_335": [true, true, true, true, true], "sample_336": [true, false, false, false, true], "sample_337": [true, true, true, false, true], "sample_338": [true, true, true, true, true], "sample_339": [false, true, true, true, true], "sample_340": [true, true, true, true, true], "sample_341": [true, true, false, true, true], "sample_342": [false, true, true, true, true], "sample_343": [true, true, true, true, true], "sample_344": [true, true, true, true, true], "sample_345": [false, false, false, true, false], "sample_346": [false, false, false, false, false], "sample_347": [true, true, true, true, true], "sample_348": [false, false, false, false, true], "sample_349": [true, true, true, true, true], "sample_350": [true, true, true, true, true], "sample_351": [true, true, false, true, true], "sample_352": [true, true, true, true, false], "sample_353": [false, false, true, false, false], "sample_354": [false, false, true, true, false], "sample_355": [false, false, false, false, false], "sample_356": [true, true, true, true, true], "sample_357": [true, true, true, true, true], "sample_358": [false, false, false, false, false], "sample_359": [true, true, true, true, true], "sample_360": [false, false, false, true, false], "sample_361": [true, true, true, true, true], "sample_362": [true, true, true, true, true], "sample_363": [true, true, true, true, true], "sample_364": [false, false, true, false, false], "sample_365": [false, false, false, true, false], "sample_366": [true, true, true, true, true], "sample_367": [false, false, false, false, false], "sample_368": [true, true, true, true, true], "sample_369": [true, true, false, true, true], "sample_370": [false, false, false, false, false], "sample_371": [false, true, false, false, false], "sample_372": [false, false, false, false, false], "sample_373": [true, true, true, true, true], "sample_374": [false, false, false, false, false], "sample_375": [true, true, true, true, true], "sample_376": [true, true, true, true, true], "sample_377": [true, false, false, false, false], "sample_378": [true, true, true, true, true], "sample_379": [true, true, true, true, true], "sample_380": [true, true, true, true, true], "sample_381": [true, true, true, true, true], "sample_382": [true, true, true, true, true], "sample_383": [true, false, false, false, false], "sample_384": [true, true, true, true, true], "sample_385": [true, true, true, true, true], "sample_386": [true, true, true, true, true], "sample_387": [true, false, true, false, true], "sample_388": [false, false, false, false, false], "sample_389": [true, false, false, false, false], "sample_390": [true, true, true, true, true], "sample_391": [true, true, true, true, true], "sample_392": [true, true, false, true, false], "sample_393": [true, true, true, true, true], "sample_394": [false, false, false, false, false], "sample_395": [true, true, true, true, true], "sample_396": [false, false, true, false, false], "sample_397": [true, true, true, true, true], "sample_398": [false, true, true, false, false], "sample_399": [true, true, true, true, true], "sample_400": [true, true, false, true, true], "sample_401": [true, true, false, false, true], "sample_402": [true, true, true, true, true], "sample_403": [true, true, true, true, true], "sample_404": [true, true, true, true, true], "sample_405": [false, true, true, true, true], "sample_406": [false, true, false, false, true], "sample_407": [true, true, true, true, true], "sample_408": [true, true, true, true, true], "sample_409": [true, true, false, true, false], "sample_410": [false, false, false, false, false], "sample_411": [true, true, true, true, true], "sample_412": [true, true, true, true, true], "sample_413": [true, false, false, false, false], "sample_414": [true, true, false, false, true], "sample_415": [true, true, true, true, false], "sample_416": [true, true, true, true, true], "sample_417": [false, false, false, false, false], "sample_418": [true, true, true, true, false], "sample_419": [true, false, false, true, false], "sample_420": [false, false, true, true, false], "sample_421": [false, false, false, false, false], "sample_422": [false, false, false, false, false], "sample_423": [true, true, true, true, false], "sample_424": [true, true, true, true, false], "sample_425": [false, false, true, false, true], "sample_426": [true, true, true, true, true], "sample_427": [true, true, false, true, true], "sample_428": [false, false, false, false, false], "sample_429": [true, true, true, false, false], "sample_430": [true, true, true, true, true], "sample_431": [false, true, false, false, true], "sample_432": [true, true, true, true, true], "sample_433": [true, true, true, true, true], "sample_434": [false, false, false, false, false], "sample_435": [false, false, false, false, false], "sample_436": [false, false, false, false, false], "sample_437": [false, false, false, false, true], "sample_438": [true, true, true, true, true], "sample_439": [false, true, true, true, true], "sample_440": [true, true, true, false, true], "sample_441": [false, true, false, false, false], "sample_442": [true, true, true, true, true], "sample_443": [true, false, false, false, false], "sample_444": [false, false, false, false, true], "sample_445": [true, true, true, true, true], "sample_446": [false, false, false, false, false], "sample_447": [true, true, true, true, true], "sample_448": [true, true, true, true, true], "sample_449": [false, false, false, true, true], "sample_450": [true, true, true, true, true], "sample_451": [true, true, true, false, true], "sample_453": [true, true, true, true, true], "sample_452": [true, true, true, true, true], "sample_454": [false, false, false, false, false], "sample_455": [true, true, true, true, true], "sample_456": [true, true, false, true, true], "sample_457": [true, true, true, false, true], "sample_458": [false, false, false, false, false], "sample_459": [false, false, false, false, false], "sample_460": [false, false, false, false, false], "sample_461": [true, true, true, true, true], "sample_462": [true, true, true, true, true], "sample_463": [true, false, true, true, true], "sample_464": [true, true, true, true, false], "sample_465": [true, false, true, true, true], "sample_466": [false, false, false, false, false], "sample_467": [true, true, true, true, true], "sample_469": [true, true, true, true, true], "sample_468": [true, true, true, true, true], "sample_470": [false, false, false, false, false], "sample_471": [true, true, true, true, true], "sample_472": [true, true, true, true, false], "sample_473": [false, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [true, false, true, true, true], "sample_476": [false, false, false, false, false], "sample_477": [true, true, true, true, true], "sample_478": [true, false, true, false, true], "sample_479": [true, true, true, true, true], "sample_480": [false, true, false, false, true], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [true, true, true, true, true], "sample_484": [false, false, false, false, false], "sample_485": [false, true, true, true, true], "sample_486": [true, true, true, true, true], "sample_487": [true, false, true, true, true], "sample_488": [true, true, true, false, true], "sample_489": [true, true, true, true, true], "sample_490": [true, false, true, true, true], "sample_491": [true, true, true, true, true], "sample_492": [false, true, false, false, false], "sample_493": [true, true, true, true, true], "sample_494": [true, false, false, false, false], "sample_495": [true, false, true, true, true], "sample_496": [false, false, false, false, false], "sample_497": [true, false, true, false, true], "sample_498": [true, true, true, true, true], "sample_499": [false, true, false, false, true], "sample_500": [false, true, true, false, true], "sample_501": [false, false, false, true, true], "sample_502": [true, false, true, false, true], "sample_503": [false, false, false, false, true], "sample_504": [false, true, false, false, false], "sample_505": [false, false, true, false, false], "sample_506": [true, false, false, false, false], "sample_507": [true, true, true, false, true], "sample_508": [true, true, false, false, false], "sample_509": [false, true, true, false, false], "sample_510": [true, false, false, true, true], "sample_511": [true, true, false, false, false], "sample_512": [false, false, true, false, false], "sample_513": [true, false, false, true, false], "sample_514": [false, false, false, false, false], "sample_515": [false, true, true, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, true, true, false, true], "sample_518": [true, false, true, true, true], "sample_519": [false, true, false, false, true], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [false, false, false, false, true], "sample_523": [true, false, false, false, true], "sample_524": [false, false, true, true, false], "sample_525": [false, false, false, false, false], "sample_526": [false, false, true, false, true], "sample_527": [false, false, false, false, false], "sample_528": [true, false, true, true, true], "sample_529": [false, true, false, true, true], "sample_530": [false, false, false, false, false], "sample_531": [false, false, false, false, true], "sample_532": [false, false, true, false, false], "sample_533": [false, true, false, true, true], "sample_534": [false, false, false, true, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, true, false, false], "sample_537": [false, false, false, false, false], "sample_538": [false, true, true, true, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, true, false], "sample_541": [true, false, true, false, true], "sample_542": [true, false, false, true, false], "sample_543": [false, false, false, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [true, false, true, false, true], "sample_547": [true, false, false, false, false], "sample_548": [true, false, false, false, false], "sample_549": [false, false, false, true, false], "sample_550": [false, true, false, false, false], "sample_551": [false, false, false, false, false], "sample_552": [true, false, false, true, false], "sample_553": [false, false, false, false, false], "sample_554": [false, true, false, true, true], "sample_555": [false, false, false, true, false], "sample_556": [false, true, false, false, false], "sample_557": [true, false, false, false, false], "sample_558": [false, false, false, false, true], "sample_559": [false, false, false, false, false], "sample_560": [false, true, true, false, true], "sample_561": [true, true, true, false, true], "sample_562": [false, true, false, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [true, false, false, false, false], "sample_566": [false, false, true, false, false], "sample_567": [false, true, true, true, false], "sample_568": [false, false, false, true, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, true, false], "sample_571": [false, false, false, false, false], "sample_572": [true, false, true, true, true], "sample_573": [false, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, true], "sample_576": [false, false, false, false, false], "sample_577": [true, false, false, true, false], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [false, false, true, true, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [false, false, false, true, false], "sample_584": [false, false, true, true, false], "sample_585": [false, false, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, false, false, false], "sample_588": [true, false, true, false, false], "sample_589": [false, false, false, false, true], "sample_590": [false, false, false, false, false], "sample_591": [false, true, false, false, false], "sample_592": [false, false, false, true, false], "sample_593": [false, false, false, false, false], "sample_594": [false, false, false, false, false], "sample_595": [false, false, true, false, false], "sample_596": [true, false, false, false, false], "sample_597": [false, false, false, false, false], "sample_598": [true, false, true, false, true], "sample_599": [false, false, false, false, true], "sample_600": [false, false, false, false, true], "sample_601": [false, false, false, true, false], "sample_602": [true, false, false, true, false], "sample_603": [false, false, false, false, false], "sample_604": [false, false, true, false, false], "sample_605": [false, false, true, false, false], "sample_606": [false, false, false, false, false], "sample_607": [true, false, false, false, false], "sample_608": [false, false, false, false, false], "sample_609": [true, false, true, false, true], "sample_610": [true, false, true, true, false], "sample_611": [false, false, false, false, false], "sample_612": [true, false, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [false, false, false, false, false], "sample_615": [true, false, true, true, false], "sample_616": [false, false, false, false, false], "sample_617": [true, true, false, false, true], "sample_618": [true, false, false, true, true], "sample_619": [false, false, false, true, true], "sample_620": [true, false, false, true, false], "sample_621": [false, false, true, false, false], "sample_622": [false, false, false, false, false], "sample_623": [false, false, false, false, false], "sample_624": [true, true, false, false, true], "sample_625": [false, false, false, false, true], "sample_626": [false, false, false, false, false], "sample_627": [true, false, false, false, false], "sample_628": [false, false, false, false, false], "sample_629": [false, true, false, false, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [true, true, false, true, true], "sample_633": [false, true, false, false, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, false, false, false], "sample_639": [false, false, false, true, false], "sample_640": [false, true, false, false, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [true, false, false, false, false], "sample_646": [false, false, false, false, false], "sample_647": [true, false, true, true, false], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, false, true, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, false, false, false, false], "sample_656": [false, true, false, false, false], "sample_657": [false, false, false, true, false], "sample_658": [false, false, false, false, false], "sample_659": [false, false, false, false, false], "sample_660": [false, false, true, false, true], "sample_661": [false, true, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, true, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, true], "sample_666": [false, false, false, false, false], "sample_667": [true, false, true, false, false], "sample_668": [false, true, false, false, false], "sample_669": [false, true, false, false, true], "sample_670": [false, false, false, false, false], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, false, true, false], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [false, false, false, false, false], "sample_678": [false, true, true, true, true], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, true, true, false, true], "sample_684": [false, false, false, false, false], "sample_685": [true, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [true, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [true, false, true, false, false], "sample_693": [false, false, false, true, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [false, true, false, true, false], "sample_698": [false, true, true, false, false], "sample_699": [true, true, true, true, false], "sample_700": [false, false, true, false, false], "sample_701": [false, false, false, false, false], "sample_702": [true, true, false, false, false], "sample_703": [false, false, false, false, false], "sample_704": [false, false, false, false, false], "sample_705": [true, false, false, true, true], "sample_706": [false, false, false, false, false], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, true, false, true, false], "sample_710": [true, false, false, true, false], "sample_711": [false, false, true, false, false], "sample_712": [false, false, true, false, false], "sample_713": [false, false, false, true, false], "sample_714": [false, false, false, false, false], "sample_715": [false, false, false, false, false], "sample_716": [false, false, false, false, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, false, true, false], "sample_720": [false, false, false, false, true], "sample_721": [false, false, false, false, false], "sample_722": [false, false, false, false, false], "sample_723": [true, true, true, true, false], "sample_724": [true, false, true, true, false], "sample_725": [false, false, false, false, false], "sample_726": [false, false, false, true, true], "sample_727": [true, true, true, true, false], "sample_728": [true, false, false, true, false], "sample_729": [true, false, true, true, true], "sample_730": [false, false, false, false, true], "sample_731": [true, true, true, false, true], "sample_732": [false, true, true, true, true], "sample_733": [true, false, true, true, false], "sample_734": [true, true, true, false, true], "sample_735": [true, true, true, true, true], "sample_736": [true, true, true, false, false], "sample_737": [false, false, false, false, false], "sample_738": [false, false, false, false, false], "sample_739": [false, true, true, false, false], "sample_740": [false, true, true, false, true], "sample_741": [false, true, true, true, false], "sample_742": [false, true, false, true, true], "sample_743": [false, true, true, true, false], "sample_744": [true, false, true, false, false], "sample_745": [false, true, false, false, true], "sample_746": [false, false, false, false, false], "sample_747": [false, false, false, true, true], "sample_748": [false, false, true, true, false], "sample_749": [false, false, false, true, false], "sample_750": [false, false, false, false, false], "sample_751": [false, false, false, false, false], "sample_752": [true, false, true, false, false], "sample_753": [false, true, false, false, true], "sample_754": [false, false, false, false, false], "sample_755": [true, true, true, true, true], "sample_756": [true, false, true, false, false], "sample_757": [false, true, true, false, true], "sample_758": [false, true, false, true, true], "sample_759": [true, false, false, true, true], "sample_760": [true, true, true, false, true], "sample_761": [false, false, false, false, false], "sample_762": [false, false, false, false, false], "sample_763": [false, false, true, false, false], "sample_764": [true, true, true, false, false], "sample_765": [false, true, false, true, true], "sample_766": [false, true, true, true, true], "sample_767": [false, false, false, false, true], "sample_768": [false, false, false, false, false], "sample_769": [false, false, false, false, false], "sample_770": [false, false, true, false, true], "sample_771": [false, true, true, false, true], "sample_772": [true, true, true, true, false], "sample_773": [false, false, false, false, false], "sample_774": [false, false, false, true, false], "sample_775": [false, false, false, false, true], "sample_776": [false, false, false, false, true], "sample_777": [false, false, true, false, true], "sample_778": [false, false, false, true, false], "sample_779": [false, false, false, false, false], "sample_780": [true, true, true, false, true], "sample_781": [false, true, false, true, true], "sample_782": [true, true, false, false, false], "sample_783": [true, true, false, false, false], "sample_784": [true, false, false, false, true], "sample_785": [false, false, false, false, false], "sample_786": [true, false, true, true, false], "sample_787": [false, true, true, false, true], "sample_788": [true, false, true, false, false], "sample_789": [false, true, false, true, true], "sample_790": [true, false, false, false, false], "sample_791": [false, true, false, false, false], "sample_792": [true, true, false, true, false], "sample_793": [true, false, true, true, true], "sample_794": [true, false, false, true, true], "sample_795": [false, false, false, false, false], "sample_796": [true, false, true, false, true], "sample_797": [true, true, true, true, false], "sample_798": [false, true, false, false, false], "sample_799": [false, true, false, false, false], "sample_800": [false, false, false, false, false], "sample_801": [false, false, false, false, false], "sample_802": [true, true, false, false, true], "sample_803": [false, false, false, true, false], "sample_804": [false, true, false, false, false], "sample_805": [true, false, true, false, false], "sample_806": [true, false, false, true, false], "sample_807": [false, false, false, false, false], "sample_808": [false, true, false, false, false], "sample_809": [false, false, true, false, false], "sample_810": [true, false, false, false, false], "sample_811": [false, true, true, false, false], "sample_812": [false, false, false, false, false], "sample_813": [true, true, false, true, true], "sample_814": [false, true, false, false, true], "sample_815": [true, true, true, true, true], "sample_816": [true, false, false, false, false], "sample_817": [false, false, false, true, false], "sample_818": [true, true, true, true, true], "sample_819": [true, true, true, true, false], "sample_820": [true, true, false, true, true], "sample_821": [true, false, false, true, false], "sample_822": [false, false, false, true, false], "sample_823": [false, false, false, false, true], "sample_824": [true, false, false, true, true], "sample_825": [true, false, true, true, true], "sample_826": [false, true, false, true, false], "sample_827": [false, false, false, false, false], "sample_828": [false, false, true, false, true], "sample_829": [true, true, true, false, true], "sample_830": [false, false, false, false, false], "sample_831": [false, false, false, false, false], "sample_832": [true, true, false, false, false], "sample_833": [false, true, false, false, false], "sample_834": [true, true, true, false, false], "sample_835": [false, true, false, true, false], "sample_836": [false, false, false, false, false], "sample_837": [false, false, false, false, false], "sample_838": [false, false, false, false, false], "sample_839": [true, true, false, true, false], "sample_840": [true, false, false, false, false], "sample_841": [true, false, false, false, true], "sample_842": [false, true, false, false, false], "sample_843": [false, true, false, true, false], "sample_844": [false, false, false, false, true], "sample_845": [false, true, false, true, true], "sample_846": [false, true, true, false, true], "sample_847": [true, true, true, true, false], "sample_848": [true, false, false, true, true], "sample_849": [true, false, false, false, false], "sample_850": [false, true, false, false, false], "sample_851": [false, false, true, false, false], "sample_852": [true, false, false, false, true], "sample_853": [true, true, true, true, true], "sample_854": [false, true, true, false, false], "sample_855": [false, true, true, false, false], "sample_856": [false, false, false, false, false], "sample_857": [false, false, false, false, false], "sample_858": [false, true, false, true, false], "sample_859": [true, true, false, false, true], "sample_860": [true, true, false, true, false], "sample_861": [true, true, false, false, true], "sample_862": [false, false, true, true, true], "sample_863": [false, false, false, false, true], "sample_864": [true, true, true, true, true], "sample_865": [true, true, true, false, false], "sample_866": [true, true, true, true, true], "sample_867": [false, false, true, false, true], "sample_868": [false, false, false, false, false], "sample_869": [true, true, false, false, false], "sample_870": [true, true, true, true, false], "sample_871": [false, false, false, false, false], "sample_872": [false, false, false, false, false], "sample_873": [true, false, false, false, false], "sample_874": [false, false, false, false, false], "sample_875": [false, false, true, true, true], "sample_876": [false, true, false, false, true], "sample_877": [true, true, false, true, false], "sample_878": [false, true, true, false, true], "sample_879": [false, false, true, true, false], "sample_880": [false, false, false, false, false], "sample_881": [true, false, true, false, false], "sample_882": [true, true, false, true, false], "sample_883": [true, false, true, true, false], "sample_884": [true, true, true, true, false], "sample_885": [false, false, false, false, false], "sample_886": [false, false, false, true, true], "sample_887": [false, false, false, false, true], "sample_888": [false, true, true, true, false], "sample_889": [false, false, false, true, false], "sample_890": [false, false, false, false, false], "sample_891": [false, true, false, true, false], "sample_892": [false, true, false, true, false], "sample_893": [false, false, false, false, false], "sample_894": [false, false, false, false, false], "sample_895": [true, false, true, true, true], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, true, false], "sample_898": [false, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [true, true, false, true, false], "sample_901": [false, true, false, false, true], "sample_902": [false, false, false, true, true], "sample_903": [true, true, true, true, false], "sample_904": [false, false, true, false, true], "sample_905": [true, true, true, false, true], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [true, false, false, true, false], "sample_909": [false, false, false, false, false], "sample_910": [false, false, false, false, false], "sample_911": [false, false, false, false, false], "sample_912": [false, true, false, false, true], "sample_913": [false, false, false, true, true], "sample_914": [false, false, false, false, false], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [true, true, true, false, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [true, false, false, false, true], "sample_923": [false, false, false, false, false], "sample_924": [false, false, false, false, false], "sample_925": [true, false, false, true, true], "sample_926": [false, false, false, false, false], "sample_927": [false, false, true, false, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, true, false], "sample_930": [false, false, false, false, false], "sample_931": [false, true, false, true, false], "sample_932": [false, false, true, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [false, false, false, false, false], "sample_937": [false, false, true, true, true], "sample_938": [false, false, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, false, true], "sample_941": [true, true, false, true, true], "sample_942": [false, true, false, false, false], "sample_943": [false, false, false, false, false], "sample_944": [true, false, true, false, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, true, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, false, false], "sample_951": [false, true, true, true, true], "sample_952": [true, false, true, true, false], "sample_953": [false, false, true, false, false], "sample_954": [false, false, false, false, false], "sample_955": [true, false, false, true, false], "sample_956": [false, false, false, false, false], "sample_957": [true, true, false, true, true], "sample_958": [false, false, false, false, false], "sample_959": [false, true, false, false, false], "sample_960": [false, true, false, false, false], "sample_961": [true, false, false, false, true], "sample_962": [false, false, false, false, false], "sample_963": [true, true, true, true, true], "sample_964": [false, false, false, false, false], "sample_965": [true, true, true, true, true], "sample_966": [false, false, false, false, true], "sample_967": [false, false, false, false, false], "sample_968": [false, false, false, true, false], "sample_969": [false, false, false, false, false], "sample_970": [true, false, false, false, false], "sample_971": [false, false, false, false, false], "sample_972": [true, false, false, false, false], "sample_973": [true, true, true, false, false], "sample_974": [false, false, false, false, false], "sample_975": [false, false, false, false, false], "sample_976": [true, false, false, false, false], "sample_977": [true, true, false, true, true], "sample_978": [false, false, false, false, false], "sample_979": [true, false, true, true, false], "sample_980": [false, false, false, true, false], "sample_981": [false, false, false, false, false], "sample_982": [false, false, false, false, false], "sample_983": [true, true, false, true, false], "sample_984": [false, false, true, false, false], "sample_985": [false, false, true, false, false], "sample_986": [false, false, false, false, false], "sample_987": [false, false, false, false, false], "sample_988": [true, false, true, false, false], "sample_989": [false, true, true, true, false], "sample_990": [false, false, false, false, false], "sample_991": [true, false, false, false, true], "sample_992": [false, true, true, false, false], "sample_993": [false, true, false, false, false], "sample_994": [false, false, true, false, false], "sample_995": [false, false, false, false, false], "sample_996": [true, false, false, false, true], "sample_997": [false, false, false, false, false], "sample_998": [false, false, false, false, false], "sample_999": [false, true, false, false, false], "sample_1000": [true, true, true, true, false], "sample_1001": [false, false, true, false, false], "sample_1002": [false, true, false, false, false], "sample_1003": [true, true, true, true, true], "sample_1004": [false, false, false, true, false], "sample_1005": [false, true, false, true, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, true, false, true], "sample_1009": [false, false, false, false, false], "sample_1010": [false, false, false, true, false], "sample_1011": [false, true, false, false, false], "sample_1012": [true, true, false, false, true], "sample_1013": [false, true, false, false, false], "sample_1014": [true, true, false, true, true], "sample_1015": [true, true, false, true, true], "sample_1016": [false, false, false, true, false], "sample_1017": [true, false, false, false, false], "sample_1018": [true, true, true, false, true], "sample_1019": [true, false, false, false, false], "sample_1020": [true, true, true, true, true], "sample_1021": [false, false, true, false, false], "sample_1022": [false, true, false, false, false], "sample_1023": [false, false, true, false, false], "sample_1024": [false, false, true, true, false], "sample_1025": [false, false, true, true, true], "sample_1026": [false, false, false, true, false], "sample_1027": [true, false, false, false, false], "sample_1028": [false, false, false, false, true], "sample_1029": [true, false, false, false, false], "sample_1030": [false, false, true, false, true], "sample_1031": [false, false, false, false, false], "sample_1032": [true, false, false, false, false], "sample_1033": [false, false, true, true, false], "sample_1034": [false, true, true, false, false], "sample_1035": [false, false, false, false, false], "sample_1036": [false, true, false, false, true], "sample_1037": [true, false, true, false, false], "sample_1038": [true, true, false, true, true], "sample_1039": [false, false, false, false, false], "sample_1040": [false, false, false, false, false], "sample_1041": [false, false, false, true, true], "sample_1042": [false, true, false, false, true], "sample_1043": [true, false, true, false, false], "sample_1044": [false, false, false, false, false], "sample_1045": [false, true, true, false, false], "sample_1046": [false, false, false, true, false], "sample_1047": [false, false, true, false, true], "sample_1048": [false, false, false, false, false], "sample_1049": [false, true, false, false, false], "sample_1050": [true, false, true, false, true], "sample_1051": [true, false, false, true, false], "sample_1052": [false, false, false, false, false], "sample_1053": [false, false, true, false, true], "sample_1054": [true, false, false, false, false], "sample_1055": [true, false, true, true, true], "sample_1056": [false, false, true, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, true, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, true, true, false, false], "sample_1061": [false, false, false, true, false], "sample_1062": [true, false, true, false, false], "sample_1063": [true, false, true, false, false], "sample_1064": [true, true, true, true, true], "sample_1065": [false, false, false, false, false], "sample_1066": [false, true, true, false, false], "sample_1067": [false, false, false, true, false], "sample_1068": [false, false, false, false, true], "sample_1069": [false, false, false, false, false], "sample_1070": [false, true, false, false, false], "sample_1071": [false, false, false, false, true], "sample_1072": [false, true, false, false, false], "sample_1073": [false, false, false, false, false], "sample_1074": [true, true, false, false, false], "sample_1075": [true, true, true, true, true], "sample_1076": [true, false, false, false, false], "sample_1077": [false, false, true, false, false], "sample_1078": [false, true, false, false, false], "sample_1079": [true, true, true, false, false], "sample_1080": [true, true, true, true, false], "sample_1081": [true, true, true, true, true], "sample_1082": [false, false, false, false, false], "sample_1083": [false, false, false, true, true], "sample_1084": [false, false, false, false, false], "sample_1085": [false, true, false, false, false], "sample_1086": [false, false, true, false, false], "sample_1087": [true, true, true, true, true], "sample_1088": [false, false, false, false, false], "sample_1089": [false, false, false, false, true], "sample_1090": [false, false, false, false, false], "sample_1091": [false, true, true, true, true], "sample_1092": [false, false, false, false, false], "sample_1093": [false, false, false, false, false], "sample_1094": [true, false, false, false, true], "sample_1095": [false, false, false, false, false], "sample_1096": [true, false, true, true, true], "sample_1097": [true, false, false, true, true], "sample_1098": [false, false, true, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, false, false, false, false], "sample_1101": [false, false, false, false, false], "sample_1102": [false, false, false, false, true], "sample_1103": [false, false, false, false, true], "sample_1104": [false, false, false, true, false], "sample_1105": [true, false, false, false, true], "sample_1106": [false, false, false, false, false], "sample_1107": [false, true, false, false, false], "sample_1108": [false, false, false, false, false], "sample_1109": [false, true, false, false, false], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [true, true, true, true, true], "sample_1113": [false, false, false, false, false], "sample_1114": [true, false, false, false, false], "sample_1115": [false, false, false, false, false], "sample_1116": [false, false, false, true, false], "sample_1117": [true, false, true, false, false], "sample_1118": [false, false, false, false, true], "sample_1119": [false, false, false, false, false], "sample_1120": [false, false, true, false, false], "sample_1121": [true, false, false, false, false], "sample_1122": [false, false, false, true, false], "sample_1123": [false, false, false, true, true], "sample_1124": [false, false, false, false, false], "sample_1125": [false, true, false, false, false], "sample_1126": [true, true, true, true, true], "sample_1127": [false, false, true, false, false], "sample_1128": [false, true, true, false, false], "sample_1129": [false, false, false, false, false], "sample_1130": [true, false, false, true, true], "sample_1131": [true, true, false, false, false], "sample_1132": [false, false, false, false, false], "sample_1133": [false, false, false, false, false], "sample_1134": [true, true, false, true, false], "sample_1135": [false, false, false, true, false], "sample_1136": [true, true, false, false, true], "sample_1137": [false, false, true, false, false], "sample_1138": [false, false, false, false, false], "sample_1139": [false, true, false, false, true], "sample_1140": [false, false, false, false, false], "sample_1141": [false, true, false, false, false], "sample_1142": [false, false, false, false, false], "sample_1143": [true, false, true, false, true], "sample_1144": [false, true, true, true, false], "sample_1145": [false, true, true, true, true], "sample_1146": [false, false, false, false, false], "sample_1147": [false, false, false, true, false], "sample_1148": [false, false, false, false, false], "sample_1149": [true, false, false, false, false], "sample_1150": [false, true, false, false, false], "sample_1151": [false, false, false, false, true], "sample_1152": [true, true, false, false, true], "sample_1153": [false, false, true, false, false], "sample_1154": [false, false, false, false, true], "sample_1155": [false, false, true, false, false], "sample_1156": [true, false, false, false, false], "sample_1157": [false, false, false, false, false], "sample_1158": [false, false, true, false, false], "sample_1159": [true, true, true, false, false], "sample_1160": [false, true, false, false, true], "sample_1161": [false, false, false, false, false], "sample_1162": [true, false, false, false, true], "sample_1163": [false, false, true, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, false, false, false], "sample_1166": [false, false, false, false, false], "sample_1167": [false, false, true, true, true], "sample_1168": [false, false, false, false, false], "sample_1169": [true, false, false, false, false], "sample_1170": [false, false, false, true, false], "sample_1171": [false, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, false, false, false], "sample_1174": [false, false, false, true, true], "sample_1175": [false, false, true, false, true], "sample_1176": [true, false, false, true, false], "sample_1177": [true, false, false, false, false], "sample_1178": [true, false, false, false, false], "sample_1179": [true, true, false, false, false], "sample_1180": [false, true, true, false, false], "sample_1181": [true, true, true, true, true], "sample_1182": [false, true, false, false, true], "sample_1183": [false, true, false, false, false], "sample_1184": [false, true, true, false, true], "sample_1185": [true, true, true, true, true], "sample_1186": [true, true, true, true, true], "sample_1187": [false, false, true, false, false], "sample_1188": [false, false, false, false, true], "sample_1189": [true, false, true, false, true], "sample_1190": [true, false, false, false, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, false, false, false, true], "sample_1193": [false, false, false, false, false], "sample_1194": [true, false, false, true, false], "sample_1195": [false, false, false, false, false], "sample_1196": [true, false, false, false, true], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [true, false, true, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, false, true, false, false], "sample_1203": [false, false, false, false, false], "sample_1204": [false, true, false, false, true], "sample_1205": [false, false, false, true, false], "sample_1206": [true, false, false, true, false], "sample_1207": [false, true, false, false, false], "sample_1208": [true, true, true, false, true], "sample_1209": [true, false, true, true, false]}}